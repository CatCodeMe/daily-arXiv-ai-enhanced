<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 10]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.SE](#cs.SE) [Total: 29]
- [cs.NI](#cs.NI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 147]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.DL](#cs.DL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 11]
- [stat.ML](#stat.ML) [Total: 16]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CC](#cs.CC) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.CY](#cs.CY) [Total: 4]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CL](#cs.CL) [Total: 13]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms](https://arxiv.org/abs/2506.17226)
*Ashish Manchanda,Prem Prakash Jayaraman,Abhik Banerjee,Kaneez Fizza,Arkady Zaslavsky*

Main category: cs.DB

TL;DR: 论文提出了一种动态上下文监控框架（DCMF），用于优化物联网环境中的上下文缓存管理，显著提高了缓存命中率并减少了缓存过期。


<details>
  <summary>Details</summary>
Motivation: 随着上下文感知物联网应用的兴起，对实时、准确的上下文信息需求增加，但传统缓存策略难以应对上下文的高动态性和实时性要求。

Method: DCMF包含上下文评估引擎（CEE）和上下文管理模块（CMM），通过计算访问概率（PoA）并应用Dempster-Shafer理论动态管理上下文新鲜度（CF）。

Result: 实验表明，DCMF在真实智能城市数据中实现了12.5%的缓存命中率提升，缓存过期减少60%，显著降低了延迟。

Conclusion: DCMF适用于动态上下文感知物联网环境，具有可扩展性和高效性。

Abstract: The rise of context-aware IoT applications has increased the demand for
timely and accurate context information. Context is derived by aggregating and
inferring from dynamic IoT data, making it highly volatile and posing
challenges in maintaining freshness and real-time accessibility. Caching is a
potential solution, but traditional policies struggle with the transient nature
of context in IoT (e.g., ensuring real-time access for frequent queries or
handling fast-changing data). To address this, we propose the Dynamic Context
Monitoring Framework (DCMF) to enhance context caching in Context Management
Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises
two core components: the Context Evaluation Engine (CEE) and the Context
Management Module (CMM). The CEE calculates the Probability of Access (PoA)
using parameters such as Quality of Service (QoS), Quality of Context (QoC),
Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs),
assigning weights to assess access likelihood. Based on this, the CMM applies a
hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating
belief levels and confidence scores to determine whether to cache, evict, or
refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)
platform and evaluated it using real-world smart city data, particularly
traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache
hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,
ensuring timely delivery of relevant context and reduced latency. These results
demonstrate DCMF's scalability and suitability for dynamic context-aware IoT
environments.

</details>


### [2] [Transient Concepts in Streaming Graphs](https://arxiv.org/abs/2506.17451)
*Aida Sheshbolouki,M. Tamer Ozsu*

Main category: cs.DB

TL;DR: 论文提出了两种新框架SGDD和SGDP，用于流图环境下的概念漂移检测和预测，解决了现有方法在动态数据管理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 概念漂移（CD）在非静态数据流中普遍存在，对数据管理和分析至关重要。现有方法在流图环境中存在不足，需要更高效的检测和预测工具。

Method: 提出了SGDD和SGDP框架，SGDD检测生成参数变化导致的概念漂移，SGDP预测概念漂移的发生时间，无需访问数据记录内容。

Result: SGDD检测到概念漂移但存在显著延迟，SGDP能在0.19至7374毫秒前预测概念漂移。

Conclusion: SGDD和SGDP为流图环境下的概念漂移问题提供了创新解决方案，显著提升了检测和预测能力。

Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce
changes in a target concept. CD is a natural phenomenon in non-stationary
settings such as data streams. Understanding, detection, and adaptation to CD
in streaming data is (i) vital for effective and efficient analytics as
reliable output depends on adaptation to fresh input, (ii) challenging as it
requires efficient operations as well as effective performance evaluations, and
(iii) impactful as it applies to a variety of use cases and is a crucial
initial step for data management systems. Current works are mostly focused on
passive CD detection as part of supervised adaptation, on independently
generated data instances or graph snapshots, on target concepts as a function
of data labels, on static data management, and on specific temporal order of
data record. These methods do not always work. We revisit CD for the streaming
graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for
streaming graph CD detection and prediction. Both frameworks discern the change
of generative source. SGDD detects the CDs due to the changes of generative
parameters with significant delays such that it is difficult to evaluate the
performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds
ahead of their occurrence, without accessing the payloads of data records.

</details>


### [3] [Lower Bounds for Conjunctive Query Evaluation](https://arxiv.org/abs/2506.17702)
*Stefan Mengel*

Main category: cs.DB

TL;DR: 综述了不同设置下合取查询评估的复杂性，探讨了从布尔查询到更复杂模型的已知结果，并强调了复杂性理论中的新假设如何影响查询回答。


<details>
  <summary>Details</summary>
Motivation: 探讨合取查询评估在不同设置下的复杂性，并展示复杂性理论中的新假设如何为查询回答提供理论支持。

Method: 通过综述已知结果，分析布尔查询、计数、枚举和直接访问等模型的复杂性，并结合复杂性理论的新假设。

Result: 展示了已知算法在某些情况下可能无法改进的理论依据。

Conclusion: 复杂性理论的新假设为合取查询评估提供了更深入的理解，并揭示了算法的潜在限制。

Abstract: In this tutorial, we will survey known results on the complexity of
conjunctive query evaluation in different settings, ranging from Boolean
queries over counting to more complex models like enumeration and direct
access. A particular focus will be on showing how different relatively recent
hypotheses from complexity theory connect to query answering and allow showing
that known algorithms in several cases can likely not be improved.

</details>


### [4] [Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks](https://arxiv.org/abs/2506.18013)
*Muhammad Farhan,Henning Koehler,Qing Wang*

Main category: cs.DB

TL;DR: 提出了一种名为Dual-Hierarchy Labelling (DHL)的高效解决方案，用于动态道路网络中的最短路径查询，通过双层次结构和动态算法显著提升了查询和更新性能。


<details>
  <summary>Details</summary>
Motivation: 动态道路网络中实时交通条件变化频繁，现有方法在查询响应时间和维护性能上表现不佳，尤其是大规模网络。

Method: 提出DHL方法，包含查询层次、更新层次和层次标签三部分，支持高效查询和更新处理，并开发了动态算法和并行变体。

Result: 在10个大型道路网络上测试，DHL在构建和更新时间上显著优于现有方法，查询处理速度快2-4倍，标签空间仅占用10%-20%。

Conclusion: DHL通过双层次结构和动态算法，为动态道路网络中的最短路径查询提供了高效且节省资源的解决方案。

Abstract: Computing the shortest-path distance between any two given vertices in road
networks is an important problem. A tremendous amount of research has been
conducted to address this problem, most of which are limited to static road
networks. Since road networks undergo various real-time traffic conditions,
there is a pressing need to address this problem for dynamic road networks.
Existing state-of-the-art methods incrementally maintain an indexing structure
to reflect dynamic changes on road networks. However, these methods suffer from
either slow query response time or poor maintenance performance, particularly
when road networks are large. In this work, we propose an efficient solution
\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road
networks from a novel perspective, which incorporates two hierarchies with
different but complementary data structures to support efficient query and
update processing. Specifically, our proposed solution is comprised of three
main components: \emph{query hierarchy}, \emph{update hierarchy}, and
\emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient
query answering by exploring only a small subset of vertices in the labels of
two query vertices and \emph{update hierarchy} supports efficient maintenance
of distance labelling under edge weight increase or decrease. We further
develop dynamic algorithms to reflect dynamic changes by efficiently
maintaining the update hierarchy and hierarchical labelling. We also propose a
parallel variant of our dynamic algorithms by exploiting labelling structure.
We evaluate our methods on 10 large road networks and it shows that our methods
significantly outperform the state-of-the-art methods, i.e., achieving
considerably faster construction and update time, while being consistently 2-4
times faster in terms of query processing and consuming only 10\%-20\%
labelling space.

</details>


### [5] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan,Kazem Cheshmi*

Main category: cs.DB

TL;DR: 提出了一种名为Typed Data Transformation（DTT）的新方法，通过将相关字节分组来提高浮点数据的压缩效率。


<details>
  <summary>Details</summary>
Motivation: 浮点数据在多个领域广泛应用，且需要无损存储以保证精度。现有方法未能充分利用数据中的模式，导致压缩效率不高。

Method: 提出DTT方法，通过将相关字节分组，利用浮点数据中的固有模式来提高压缩比。

Result: DTT在多种数据集上测试，几何平均压缩比提高了1.16倍，压缩和解压吞吐量提高了1.18-3.79倍。

Conclusion: DTT方法显著提升了浮点数据的压缩效率和性能，优于现有工具。

Abstract: Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.

</details>


### [6] [Learning Lineage Constraints for Data Science Operations](https://arxiv.org/abs/2506.18252)
*Jinjin Zhao*

Main category: cs.DB

TL;DR: 提出了一种跨库数据血缘追踪的通用架构XProv，结合具体血缘图和抽象逻辑模式，支持已知和未知操作。


<details>
  <summary>Details</summary>
Motivation: 数据科学工作流常涉及多库功能集成，但血缘表示通常与特定数据模型绑定，需跨库通用解决方案。

Method: 借鉴中间表示（IR）思想，设计XProv架构，结合具体血缘图和抽象逻辑模式，支持已知和未知操作。

Result: 提出XProv架构，并探讨如何从具体血缘图推断逻辑模式。

Conclusion: XProv为跨库数据血缘追踪提供了通用解决方案，支持复杂工作流分析。

Abstract: Data science workflows often integrate functionalities from a diverse set of
libraries and frameworks. Tasks such as debugging require data lineage that
crosses library boundaries. The problem is that the way that "lineage" is
represented is often intimately tied to particular data models and data
manipulation paradigms. Inspired by the use of intermediate representations
(IRs) in cross-library performance optimizations, this vision paper proposes a
similar architecture for lineage - how do we specify logical lineage across
libraries in a common parameterized way? In practice, cross-library workflows
will contain both known operations and unknown operations, so a key design of
XProv to link both materialized lineage graphs of data transformations and the
aforementioned abstracted logical patterns. We further discuss early ideas on
how to infer logical patterns when only the materialized graphs are available.

</details>


### [7] [Fast Capture of Cell-Level Provenance in Numpy](https://arxiv.org/abs/2506.18255)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: 本文提出了一种针对数组的原型注释系统，用于捕获numpy库中的单元格级来源，以解决API快速变化、操作类型多样和大规模数据集带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 有效的来源追踪能提升数组工作流的可重复性、治理和数据质量，但面临API快速变化、操作类型多样和大规模数据集等挑战。

Method: 设计了一个原型注释系统，专注于numpy库中的单元格级来源捕获，并探索了减少注释延迟的内存优化方法。

Result: 原型系统通过内存优化显著降低了注释延迟。

Conclusion: 该方法可作为结构化数据工作流和多样化数据科学应用中更广泛治理系统的一部分。

Abstract: Effective provenance tracking enhances reproducibility, governance, and data
quality in array workflows. However, significant challenges arise in capturing
this provenance, including: (1) rapidly evolving APIs, (2) diverse operation
types, and (3) large-scale datasets. To address these challenges, this paper
presents a prototype annotation system designed for arrays, which captures
cell-level provenance specifically within the numpy library. With this
prototype, we explore straightforward memory optimizations that substantially
reduce annotation latency. We envision this provenance capture approach for
arrays as part of a broader governance system for tracking for structured data
workflows and diverse data science applications.

</details>


### [8] [TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows](https://arxiv.org/abs/2506.18257)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: TableVault是一个数据管理系统，旨在解决LLM增强环境中动态数据集合的管理挑战，支持并发执行、可重现性、数据版本控制和可组合的工作流设计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂数据任务中表现出色，但其集成到更复杂的数据工作流中带来了显著的管理挑战。

Method: TableVault结合了传统数据库方法和新兴的LLM驱动需求，提供了一个透明平台，高效管理结构化数据和相关数据工件。

Result: TableVault能够满足动态数据工作流的需求，支持并发执行、可重现性、数据版本控制和可组合设计。

Conclusion: TableVault通过融合数据库方法和LLM需求，为LLM增强环境提供了一个高效、透明的数据管理解决方案。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating
and executing complex data tasks. However, their integration into more complex
data workflows introduces significant management challenges. In response, we
present TableVault - a data management system designed to handle dynamic data
collections in LLM-augmented environments. TableVault meets the demands of
these workflows by supporting concurrent execution, ensuring reproducibility,
maintaining robust data versioning, and enabling composable workflow design. By
merging established database methodologies with emerging LLM-driven
requirements, TableVault offers a transparent platform that efficiently manages
both structured data and associated data artifacts.

</details>


### [9] [Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications](https://arxiv.org/abs/2506.18772)
*Hassan S. Al Khatib,Subash Neupane,Sudip Mittal,Shahram Rahimi,Nina Marhamati,Sean Bozorgzad*

Main category: cs.DB

TL;DR: 该论文提出了一种患者旅程本体（PJO），用于整合和管理患者数据，支持语义互操作性和临床推理，提升医疗效率和个性化治疗。


<details>
  <summary>Details</summary>
Motivation: 医疗行业正转向以患者为中心的模式，需要先进的方法来管理和表示患者数据。

Method: 利用本体论构建PJO，整合患者数据源（如病史、诊断、治疗方案和结果），捕捉医疗事件的时间、顺序和因果关系。

Result: 定量和定性评估表明PJO在患者历史检索、症状跟踪和提供者交互表示方面表现优异，具有可靠性和实用性。

Conclusion: PJO为个性化医疗和患者旅程分析提供了可靠工具，并推动了生成式AI在医疗应用中的发展。

Abstract: The healthcare industry is moving towards a patient-centric paradigm that
requires advanced methods for managing and representing patient data. This
paper presents a Patient Journey Ontology (PJO), a framework that aims to
capture the entirety of a patient's healthcare encounters. Utilizing
ontologies, the PJO integrates different patient data sources like medical
histories, diagnoses, treatment pathways, and outcomes; it enables semantic
interoperability and enhances clinical reasoning. By capturing temporal,
sequential, and causal relationships between medical encounters, the PJO
supports predictive analytics, enabling earlier interventions and optimized
treatment plans. The ontology's structure, including its main classes,
subclasses, properties, and relationships, as detailed in the paper,
demonstrates its ability to provide a holistic view of patient care.
Quantitative and qualitative evaluations by Subject Matter Experts (SMEs)
demonstrate strong capabilities in patient history retrieval, symptom tracking,
and provider interaction representation, while identifying opportunities for
enhanced diagnosis-symptom linking. These evaluations reveal the PJO's
reliability and practical applicability, demonstrating its potential to enhance
patient outcomes and healthcare efficiency. This work contributes to the
ongoing efforts of knowledge representation in healthcare, offering a reliable
tool for personalized medicine, patient journey analysis and advancing the
capabilities of Generative AI in healthcare applications.

</details>


### [10] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: 提出了一个高精度的全球海岸线数据集（10米分辨率）和高效计算库Lighthouse，适用于资源受限环境下的实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有全球海岸数据集分辨率低（1-4公里），限制了其应用潜力。

Method: 结合公开卫星影像和计算机视觉技术，开发了10米分辨率数据集，并设计了Lighthouse库以实现高效计算。

Result: 数据集精度提升100倍以上，Lighthouse库仅需1 CPU和2GB RAM即可实现毫秒级在线推理。

Conclusion: 新数据集和算法显著提升了海岸距离计算的精度和效率，适用于实时应用。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Main category: cs.DC

TL;DR: 本文提出了一种名为Co-Forgetting Protocol的新框架，用于在多智能体系统中同步修剪内存，以解决共享知识管理中的挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂动态环境中需要高效管理共享知识，避免过时或无关数据的积累。

Method: 协议包含三个关键组件：上下文感知语义投票、多尺度时间衰减函数和基于PBFT的共识机制。

Result: 实验显示，协议在内存占用减少、投票准确性、共识成功率和缓存命中率方面表现优异。

Conclusion: Co-Forgetting Protocol有效解决了多智能体系统中的内存同步和修剪问题。

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [12] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Main category: cs.DC

TL;DR: 提出了一种基于张量的GPU加速方法，用于优化车辆路径问题中的局部搜索操作，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 局部搜索在车辆路径问题中计算成本高且耗时，尤其是在大规模或复杂约束情况下，需要更高效的解决方案。

Method: 采用基于属性的张量表示方法，将密集计算完全卸载到GPU，实现低耦合架构，适用于多种VRP变体。

Result: 在三种路由问题的基准实例上，该方法比传统CPU实现具有显著的计算优势，并可能提升解的质量。

Conclusion: 该方法为局部搜索提供了高效的计算支持，同时分析了其性能特点和潜在瓶颈，为未来改进指明了方向。

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [13] [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](https://arxiv.org/abs/2506.17471)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.DC

TL;DR: 提出了一种基于GPU的有限元方法（FEM）变分形式并行化策略，通过代码转换和启发式成本模型优化调度，实现高性能计算。


<details>
  <summary>Details</summary>
Motivation: 解决在GPU上高效执行FEM变分形式的挑战，特别是通过统一形式语言（UFL）表达的复杂计算负载。

Method: 构建调度候选空间，利用启发式成本模型进行排序，并通过剪枝策略减少配置数量，平衡设备延迟隐藏和状态空间。

Result: 在两种Nvidia GPU上测试，65%的案例中达到超过50%的屋顶线性能。

Conclusion: 该并行化策略在多种应用中表现优异，为FEM求解提供了高效解决方案。

Abstract: We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated pruning strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.

</details>


### [14] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Main category: cs.DC

TL;DR: ConsumerBench是一个用于评估终端设备上GenAI模型系统效率和响应时间的基准测试框架，模拟多应用并发场景，揭示资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI应用从云端转向终端设备，资源管理和系统效率面临新挑战，需要更真实的评估工具。

Method: 设计ConsumerBench框架，支持多应用并发和自定义工作流，捕获应用级和系统级指标。

Result: 实验揭示了资源共享低效、贪婪分配下的不公平调度以及静态模型服务器配置的性能问题。

Conclusion: 为模型开发者和系统设计者提供实用建议，包括定制内核和SLO感知调度策略的价值。

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [15] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Main category: cs.DC

TL;DR: 本文研究了在推荐系统中使用大语言模型（LLMs）时，通过模型并行和数据并行的混合优化方法，显著提升了训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在推荐系统中的广泛应用，其庞大的参数规模和数据量导致了计算和通信瓶颈，亟需优化方法。

Method: 采用模型并行（包括张量并行和流水线并行）和数据并行（同步与异步模式结合梯度压缩和稀疏化技术），并引入自适应负载均衡机制。

Result: 实验表明，混合并行方案将训练吞吐量提升30%以上，资源利用率提高约20%，同时保持强扩展性和鲁棒性。

Conclusion: 讨论了不同并行策略的权衡，并展望了异构硬件集成和自动化调度技术的未来方向。

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


### [16] [Distributed Butterfly Analysis using Mobile Agents](https://arxiv.org/abs/2506.17721)
*Prabhat Kumar Chand,Apurba Das,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文提出了一种基于代理的分布式算法，用于在二分图中高效计数4-循环（蝴蝶结构），无需先验知识，仅需最高代理ID。


<details>
  <summary>Details</summary>
Motivation: 二分图中的蝴蝶结构对识别密集子图至关重要，但基于代理的数据挖掘在此领域的应用尚未充分探索。

Method: 代理通过协作构建生成树并选举领导者，利用相邻代理的新颖会面机制提高效率，最终在有限轮次内完成蝴蝶计数。

Result: 算法在O(Δ)轮内完成节点蝴蝶计数，总蝴蝶计数在O(Δ+min{|A|,|B|})轮内完成，且适用于一般图。

Conclusion: 该算法高效且通用，为二分图和一般图中的蝴蝶计数提供了分布式解决方案。

Abstract: Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying
cohesive structures and dense subgraphs. While agent-based data mining is
gaining prominence, its application to bipartite networks remains relatively
unexplored. We propose distributed, agent-based algorithms for \emph{Butterfly
Counting} in a bipartite graph $G((A,B),E)$. Agents first determine their
respective partitions and collaboratively construct a spanning tree, electing a
leader within $O(n \log \lambda)$ rounds using only $O(\log \lambda)$ bits per
agent. A novel meeting mechanism between adjacent agents improves efficiency
and eliminates the need for prior knowledge of the graph, requiring only the
highest agent ID $\lambda$ among the $n$ agents. Notably, our techniques
naturally extend to general graphs, where leader election and spanning tree
construction maintain the same round and memory complexities. Building on these
foundations, agents count butterflies per node in $O(\Delta)$ rounds and
compute the total butterfly count of $G$ in $O(\Delta+\min\{|A|,|B|\})$ rounds.

</details>


### [17] [Choosing the Right Battery Model for Data Center Simulations](https://arxiv.org/abs/2506.17739)
*Paul Kilian,Philipp Wiesner,Odej Kao*

Main category: cs.DC

TL;DR: 本文研究了数据中心微电网中的电池模型选择问题，比较了四种模型在Vessim框架中的表现，发现线性模型在短期实验中表现接近复杂物理模型，且速度更快。


<details>
  <summary>Details</summary>
Motivation: 随着电力成本上升和碳排放法规的加强，数据中心需要更高效、可控的电力系统，而选择合适的电池模型对模拟至关重要。

Method: 在Vessim框架中实现了四种电池模型，并分析其行为，比较了模拟速度、真实性和配置难易度。

Result: 线性模型在短期实验中表现与复杂物理模型接近，且运行更快；而简单的无损模型无法准确模拟复杂行为。

Conclusion: 线性模型是数据中心模拟中的理想选择，因其平衡了速度与真实性，且无需复杂的电化学知识。

Abstract: As demand for computing resources continues to rise, the increasing cost of
electricity and anticipated regulations on carbon emissions are prompting
changes in data center power systems. Many providers are now operating compute
nodes in microgrids, close to renewable power generators and energy storage, to
maintain full control over the cost and origin of consumed electricity.
Recently, new co-simulation testbeds have emerged that integrate
domain-specific simulators to support research, development, and testing of
such systems in a controlled environment. Yet, choosing an appropriate battery
model for data center simulations remains challenging, as it requires balancing
simulation speed, realism, and ease of configuration.
  In this paper, we implement four different battery models for data center
scenarios within the co-simulation framework Vessim and analyze their behavior.
The results show that linear models, which consider inefficiencies and power
limits, closely match the behavior of complex physics-based models in
short-term experiments while offering faster execution, and not requiring
knowledge on electrochemical reactions and circuit-level dynamics. In contrast,
simple, lossless models fail to accurately represent complex behavior and
provide no further runtime advantage.

</details>


### [18] [Maintaining a Bounded Degree Expander in Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.17757)
*Antonio Cruciani*

Main category: cs.DC

TL;DR: 该论文研究了在完全分布式环境中维护鲁棒且稀疏的覆盖网络的问题，提出了一种动态网络中的扩展器拓扑维护协议。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的非结构化P2P网络需要维护一个连接良好且低度的通信图，而现有方法在动态网络中的适应性不足。

Method: 通过随机化连接策略，结合对抗性动态网络模型，提出了一种分布式算法。

Result: 算法在高概率下维护了一个常数度的扩展器图，即使每轮有高达O(n/polylog(n))的节点变动。

Conclusion: 该协议解决了先前工作中的开放性问题，提供了一种简单、完全分布式且具有理论保证的解决方案。

Abstract: We study the problem of maintaining robust and sparse overlay networks in
fully distributed settings where nodes continuously join and leave the system.
This scenario closely models real-world unstructured peer-to-peer networks,
where maintaining a well-connected yet low-degree communication graph is
crucial. We generalize a recent protocol by Becchetti et al. [SODA 2020] that
relies on a simple randomized connection strategy to build an expander topology
with high probability to a dynamic networks with churn setting. In this work,
the network dynamism is governed by an oblivious adversary that controls which
nodes join and leave the system in each round. The adversary has full knowledge
of the system and unbounded computational power, but cannot see the random
choices made by the protocol. Our analysis builds on the framework of Augustine
et al. [FOCS 2015], and shows that our distributed algorithm maintains a
constant-degree expander graph with high probability, despite a continuous
adversarial churn with a rate of up to $\mathcal{O}(n/polylog(n))$ per round,
where $n$ is the stable network size. The protocol and proof techniques are not
new, but together they resolve a specific open problem raised in prior work.
The result is a simple, fully distributed, and churn-resilient protocol with
provable guarantees that align with observed empirical behavior.

</details>


### [19] [Implementation and Evaluation of Fast Raft for Hierarchical Consensus](https://arxiv.org/abs/2506.17793)
*Anton Melnychuk,Bryan SebaRaj*

Main category: cs.DC

TL;DR: Fast Raft是一种分层共识协议，通过快速通道机制减少消息轮次，降低对Leader的依赖，提升了吞吐量并减少了提交延迟。


<details>
  <summary>Details</summary>
Motivation: 解决动态分布式环境中标准Raft协议消息轮次多、Leader依赖高的问题。

Method: 采用gRPC和基于Kubernetes的部署，在AWS可用区中进行实验。

Result: 在低丢包条件下，吞吐量提升，提交延迟降低，同时保持Raft的安全性和活性保证。

Conclusion: Fast Raft在动态环境中表现优于标准Raft，具有实际应用潜力。

Abstract: We present the first open-source implementation and evaluation of Fast Raft,
a hierarchical consensus protocol designed for dynamic, distributed
environments. Fast Raft reduces the number of message rounds needed to commit
log entries compared to standard Raft by introducing a fast-track mechanism and
reducing leader dependence. Our implementation uses gRPC and Kubernetes-based
deployment across AWS availability zones. Experimental results demonstrate a
throughput improvement and reduced commit latency under low packet loss
conditions, while maintaining Raft's safety and liveness guarantees.

</details>


### [20] [CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation](https://arxiv.org/abs/2506.17991)
*Thien Tran,Jonathan Kua,Minh Tran,Honghao Lyu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 论文提出了Cloud-Fog Telerobotics（CFTel）架构，以解决传统云基远程机器人技术的延迟、可靠性和扩展性问题，并探讨了其在实时控制和自主性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统云基远程机器人技术存在延迟、可靠性和扩展性问题，无法满足关键应用的实时性能需求。

Method: 采用分布式Cloud-Edge-Robotics计算架构，结合5G、边缘智能、具身AI和数字孪生技术。

Result: CFTel能够提升实时控制、扩展性和自主性，支持面向服务的解决方案。

Conclusion: CFTel为未来远程机器人研究提供了基础参考，但仍需解决延迟、网络安全和标准化等挑战。

Abstract: Telerobotics is a key foundation in autonomous Industrial Cyber-Physical
Systems (ICPS), enabling remote operations across various domains. However,
conventional cloud-based telerobotics suffers from latency, reliability,
scalability, and resilience issues, hindering real-time performance in critical
applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation
(CFA) paradigm to address these limitations by leveraging a distributed
Cloud-Edge-Robotics computing architecture, enabling deterministic
connectivity, deterministic connected intelligence, and deterministic networked
computing. This paper synthesizes recent advancements in CFTel, aiming to
highlight its role in facilitating scalable, low-latency, autonomous, and
AI-driven telerobotics. We analyze architectural frameworks and technologies
that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge
Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel
has the potential to enhance real-time control, scalability, and autonomy while
supporting service-oriented solutions. We also discuss practical challenges,
including latency constraints, cybersecurity risks, interoperability issues,
and standardization efforts. This work serves as a foundational reference for
researchers, stakeholders, and industry practitioners in future telerobotics
research.

</details>


### [21] [Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles](https://arxiv.org/abs/2506.18024)
*Thien Tran,Quang Nguyen,Jonathan Kua,Minh Tran,Toan Luu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 提出了一种分布式云-边-物联网架构，用于解决海上工业信息物理系统中的计算和通信延迟问题，提高了计算效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 海上无人水面车辆（USVs）的实时数据处理和分析受限于计算能力和通信延迟，影响了系统的可扩展性和响应性。

Method: 采用分层架构：云层用于数据聚合和高级分析，边缘层执行本地AI处理，物联网层负责低延迟数据采集。

Result: 实验结果显示分类准确率达到86%，延迟性能有所改善。

Conclusion: 该架构为海上工业信息物理系统提供了一种模块化、可扩展的解决方案，推动了智能USVs的自主决策能力。

Abstract: Industrial Cyber-Physical Systems (ICPS) technologies are foundational in
driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs).
However, onboard computational constraints and communication latency
significantly restrict real-time data processing, analysis, and predictive
modeling, hence limiting the scalability and responsiveness of maritime ICPS.
To overcome these challenges, we propose a distributed Cloud-Edge-IoT
architecture tailored for maritime ICPS by leveraging design principles from
the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture
comprises three hierarchical layers: a Cloud Layer for centralized and
decentralized data aggregation, advanced analytics, and future model
refinement; an Edge Layer that executes localized AI-driven processing and
decision-making; and an IoT Layer responsible for low-latency sensor data
acquisition. Our experimental results demonstrated improvements in
computational efficiency, responsiveness, and scalability. When compared with
our conventional approaches, we achieved a classification accuracy of 86\%,
with an improved latency performance. By adopting Cloud-Fog Automation, we
address the low-latency processing constraints and scalability challenges in
maritime ICPS applications. Our work offers a practical, modular, and scalable
framework to advance robust autonomy and AI-driven decision-making and autonomy
for intelligent USVs in future maritime ICPS.

</details>


### [22] [Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data](https://arxiv.org/abs/2506.18259)
*Jer Shyuan Ng,Aditya Pribadi Kalapaaking,Xiaoyu Xia,Dusit Niyato,Ibrahim Khalil,Iqbal Gondal*

Main category: cs.DC

TL;DR: 论文提出了一种基于合成数据的层次联邦学习（HFL）框架，解决非独立同分布（non-IID）数据问题，并通过激励机制提升工人参与度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式边缘网络中面临工人退出、通信效率低和非IID数据等问题，影响模型性能。

Method: 引入边缘服务器层，生成并分发合成数据以缓解非IID问题，同时通过奖励机制激励工人参与。

Result: 提出的框架改善了模型性能，减少了通信轮次，并提高了工人参与度。

Conclusion: 该框架为FL在非IID数据和低参与度场景下的应用提供了有效解决方案。

Abstract: In recent years, Federated Learning (FL) has emerged as a widely adopted
privacy-preserving distributed training approach, attracting significant
interest from both academia and industry. Research efforts have been dedicated
to improving different aspects of FL, such as algorithm improvement, resource
allocation, and client selection, to enable its deployment in distributed edge
networks for practical applications. One of the reasons for the poor FL model
performance is due to the worker dropout during training as the FL server may
be located far away from the FL workers. To address this issue, an Hierarchical
Federated Learning (HFL) framework has been introduced, incorporating an
additional layer of edge servers to relay communication between the FL server
and workers. While the HFL framework improves the communication between the FL
server and workers, large number of communication rounds may still be required
for model convergence, particularly when FL workers have non-independent and
identically distributed (non-IID) data. Moreover, the FL workers are assumed to
fully cooperate in the FL training process, which may not always be true in
practical situations. To overcome these challenges, we propose a
synthetic-data-empowered HFL framework that mitigates the statistical issues
arising from non-IID local datasets while also incentivizing FL worker
participation. In our proposed framework, the edge servers reward the FL
workers in their clusters for facilitating the FL training process. To improve
the performance of the FL model given the non-IID local datasets of the FL
workers, the edge servers generate and distribute synthetic datasets to FL
workers within their clusters. FL workers determine which edge server to
associate with, considering the computational resources required to train on
both their local datasets and the synthetic datasets.

</details>


### [23] [The Power of Strong Linearizability: the Difficulty of Consistent Refereeing](https://arxiv.org/abs/2506.18401)
*Hagit Attiya,Armando Castañeda,Constantin Enea*

Main category: cs.DC

TL;DR: 本文研究了强线性化实现与协议之间的关系，揭示了锁无关和等待无关实现中需要一种弱于共识但无法通过非通用原语组合实现的协议形式。


<details>
  <summary>Details</summary>
Motivation: 探索强线性化实现与协议之间的联系，以理解并发对象实现中的协调需求。

Method: 定义并使用了两种竞争对象来捕捉锁无关和等待无关实现中强线性化的能力。

Result: 发现强线性化实现需要高协调能力，且竞争对象严格弱于共识。

Conclusion: 强线性化在并发对象实现中具有重要作用，但其实现需要高协调能力，且竞争对象为研究提供了新视角。

Abstract: This paper studies the relation between agreement and strongly linearizable
implementations of various objects. This leads to new results about
implementations of concurrent objects from various primitives including window
registers and interfering primitives. We consider implementations that provide
both strong linearizability and decisive linearizability.
  We identify that lock-free, respectively, wait-free, strongly linearizable
implementations of several concurrent objects entail a form of agreement that
is weaker than consensus but impossible to strongly-linearizable implement with
combinations of non-universal primitives. In both cases, lock-free and
wait-free, this form of agreement requires a distinguished process to referee a
competition that involves all other processes. Our results show that consistent
refereeing of such competitions (i.e. the outcome of the competition does not
change in extensions of the current execution) requires high coordination
power.
  More specifically, two contest objects are defined and used to capture the
power of strong linearizability in lock-free and wait-free implementations,
respectively. Both objects are strictly weaker than consensus, in the sense
that they have a wait-free linearizable (in fact, decisively linearizable)
implementation from reads and writes. The contest objects capture strong
linearizability since (1) they have strongly linearizable implementations from
several ``high-level'' objects like stacks, queues, snapshots, counters, and
therefore, impossibility results for them carry over to these objects, and (2)
they admit powerful impossibility results for strong linearizability that
involve window registers and interfering primitives, which are non-universal.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [24] [On the Power of Spatial Locality on Online Routing Problems](https://arxiv.org/abs/2506.17517)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: 研究了在线旅行商问题（TSP）和在线按需乘车问题（DARP）的空间局部性模型，利用未来请求的有限信息改进竞争比。


<details>
  <summary>Details</summary>
Motivation: 受现实应用（如Uber/Lyft）启发，利用空间局部性信息优化在线路由问题。

Method: 提出空间局部性模型，预测服务器当前位置附近未来请求的距离范围。

Result: 证明空间局部性信息能改进竞争比，且与度量空间无关。

Conclusion: 空间局部性信息对优化在线路由问题具有实际价值。

Abstract: We consider the online versions of two fundamental routing problems,
traveling salesman (TSP) and dial-a-ride (DARP), which have a variety of
relevant applications in logistics and robotics. The online versions of these
problems concern with efficiently serving a sequence of requests presented in a
real-time on-line fashion located at points of a metric space by servers
(salesmen/vehicles/robots). In this paper, motivated from real-world
applications, such as Uber/Lyft rides, where some limited knowledge is
available on the future requests, we propose the {\em spatial locality} model
that provides in advance the distance within which new request(s) will be
released from the current position of server(s). We study the usefulness of
this advanced information on achieving the improved competitive ratios for both
the problems with $k\geq 1$ servers, compared to the competitive results
established in the literature without such spatial locality consideration. We
show that small locality is indeed useful in obtaining improved competitive
ratios irrespective of the metric space.

</details>


### [25] [Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination](https://arxiv.org/abs/2506.17521)
*Matthias Bentert,Alex Crane,Pål Grønås Drange,Yosuke Mizutani,Blair D. Sullivan*

Main category: cs.DS

TL;DR: 论文研究了图论中的两个基本问题：结构最优雅可比累积和最小边计数问题，证明了它们的NP完全性，并提供了精确算法和数据简化规则。


<details>
  <summary>Details</summary>
Motivation: 解决算法微分中两个长期未决的问题，明确其计算复杂性，并提供高效解决方案。

Method: 采用顶点消除操作，通过NP完全性证明和精确算法分析问题。

Result: 证明两个问题均为NP完全，提供了O*(2^n)时间算法，并展示了数据简化规则。

Conclusion: 研究为算法微分中的图论问题提供了理论基础和实用工具，解决了开放性问题。

Abstract: We study graph-theoretic formulations of two fundamental problems in
algorithmic differentiation. The first (Structural Optimal Jacobian
Accumulation) is that of computing a Jacobian while minimizing multiplications.
The second (Minimum Edge Count) is to find a minimum-size computational graph.
For both problems, we consider the vertex elimination operation. Our main
contribution is to show that both problems are NP-complete, thus resolving
longstanding open questions. In contrast to prior work, our reduction for
Structural Optimal Jacobian Accumulation does not rely on any assumptions about
the algebraic relationships between local partial derivatives; we allow these
values to be mutually independent. We also provide $O^*(2^n)$-time exact
algorithms for both problems, and show that under the exponential time
hypothesis these running times are essentially tight. Finally, we provide a
data reduction rule for Structural Optimal Jacobian Accumulation by showing
that false twins may always be eliminated consecutively.

</details>


### [26] [Faster Low-Rank Approximation and Kernel Ridge Regression via the Block-Nyström Method](https://arxiv.org/abs/2506.17556)
*Sachin Garg,Michał Dereziński*

Main category: cs.DS

TL;DR: Block-Nyström方法通过引入块对角结构降低计算成本，同时保持近似精度，适用于二阶优化和核岭回归。


<details>
  <summary>Details</summary>
Motivation: 解决Nyström方法在数据谱衰减严重时计算成本过高的问题。

Method: 提出Block-Nyström算法，结合多个小规模Nyström近似以提升谱尾估计。

Result: 显著降低计算成本，提供更强的近似保证，并改进二阶优化的预条件器。

Conclusion: Block-Nyström在相同计算预算下优于传统方法，适用于统计学习和优化问题。

Abstract: The Nystr\"om method is a popular low-rank approximation technique for large
matrices that arise in kernel methods and convex optimization. Yet, when the
data exhibits heavy-tailed spectral decay, the effective dimension of the
problem often becomes so large that even the Nystr\"om method may be outside of
our computational budget. To address this, we propose Block-Nystr\"om, an
algorithm that injects a block-diagonal structure into the Nystr\"om method,
thereby significantly reducing its computational cost while recovering strong
approximation guarantees. We show that Block-Nystr\"om can be used to construct
improved preconditioners for second-order optimization, as well as to
efficiently solve kernel ridge regression for statistical learning over Hilbert
spaces. Our key technical insight is that, within the same computational
budget, combining several smaller Nystr\"om approximations leads to stronger
tail estimates of the input spectrum than using one larger approximation. Along
the way, we provide a novel recursive preconditioning scheme for efficiently
inverting the Block-Nystr\"om matrix, and provide new statistical learning
bounds for a broad class of approximate kernel ridge regression solvers.

</details>


### [27] [Contextual Pattern Mining and Counting](https://arxiv.org/abs/2506.17613)
*Ling Li,Daniel Gibney,Sharma V. Thankachan,Solon P. Pissis,Grigorios Loukides*

Main category: cs.DS

TL;DR: 论文提出了两种与字符串上下文相关的问题（CPM和CPC），并分别给出了高效的算法和索引结构，适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 研究如何在长字符串中高效地挖掘和计数特定模式的上下文，以支持大规模数据处理。

Method: 对于CPM问题，提出了一种线性工作算法，支持内部或外部内存；对于CPC问题，设计了一种高效索引结构，并利用LZ77分解和查询长度上限优化性能。

Result: 实验表明，CPM算法能处理大规模数据集，CPC索引在查询时间、索引大小等方面显著优于现有方法。

Conclusion: 提出的算法和索引在实际应用中表现出色，适用于大规模字符串处理任务。

Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and
two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of
all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$
occurs in $T$. We introduce two problems related to the notion of context: (1)
the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an
integer $\tau>0$, asks for outputting the context of each substring $P$ of
length $m$ of $T$, provided that the size of the context of $P$ is at least
$\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for
preprocessing $T$ so that the size of the context of a given query string $P$
of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal
memory, or a bounded amount of internal memory and external memory, which
allows much larger datasets to be handled. For CPC, we propose an
$\widetilde{\mathcal{O}}(n)$-space index that can be constructed in
$\widetilde{\mathcal{O}}n)$ time and answers queries in
$\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the
practical performance of the CPC index by optimizations that exploit the LZ77
factorization of $T$ and an upper bound on the query length. Using
billion-letter datasets from different domains, we show that the external
memory version of our CPM algorithm can deal with very large datasets using a
small amount of internal memory while its runtime is comparable to that of the
internal memory version. Interestingly, we also show that our optimized index
for CPC outperforms an approach based on the state of the art for the reporting
version of CPC [Navarro, SPIRE 2020] in terms of query time, index size,
construction time, and construction space, often by more than an order of
magnitude.

</details>


### [28] [Optimizing Periodic Operations for Efficient Inland Waterway Lock Management](https://arxiv.org/abs/2506.17743)
*Julian Golak,Alexander Grigoriev,Freija van Lent,Tom van der Zanden*

Main category: cs.DS

TL;DR: 研究评估周期性船闸调度对船舶等待时间的影响，提出多项式时间算法优化调度，实验表明简单策略优于基于周期性模式的优化策略。


<details>
  <summary>Details</summary>
Motivation: 内河航道中，船闸操作的高效管理影响拥堵和运输不确定性，周期性调度可简化管理但可能增加等待时间。

Method: 开发算法匹配不规则船舶到达模式，并优化周期性调度以减少长期平均等待时间。

Result: 提出多项式时间算法，实验显示简单策略在实际数据中表现更优。

Conclusion: 周期性调度虽简化管理，但简单策略在实际应用中可能更有效。

Abstract: In inland waterways, the efficient management of water lock operations
impacts the level of congestion and the resulting uncertainty in inland
waterway transportation. To achieve reliable and efficient traffic, schedules
should be easy to understand and implement, reducing the likelihood of errors.
The simplest schedules follow periodic patterns, reducing complexity and
facilitating predictable management. Since vessels do not arrive in perfectly
regular intervals, periodic schedules may lead to more wait time. The aim of
this research is to estimate this cost by evaluating how effective these
periodic schedules manage vessel traffic at water locks. The first objective is
to estimate a periodic arrival pattern that closely matches a dataset of
irregular vessel arrivals at a specific lock. We develop an algorithm that,
given a fixed number of vessel streams, solves the problem in polynomial time.
The solution then serves as input for the subsequent part, where we consider
algorithms that compute operational schedules by formulating an optimisation
problem with periodic arrival patterns as input, and the goal is to determine a
periodic schedule that minimises the long-run average waiting time of vessels.
We present a polynomial-time algorithm for the two-stream case and a
pseudo-polynomial-time algorithm for the general case, along with incremental
polynomial-time approximation schemes. In our numerical experiments, use AIS
data to construct a periodic arrival pattern closely matching the observed
data. Our experiments demonstrate that when evaluated against actual data,
intuitive and straightforward policies often outperform optimal policies
specifically trained on the periodic arrival pattern.

</details>


### [29] [Semirandom Planted Clique via 1-norm Isometry Property](https://arxiv.org/abs/2506.17916)
*Venkatesan Guruswami,Hsin-Po Wang*

Main category: cs.DS

TL;DR: 提出了一种多项式时间算法，能够在半随机模型中检测到大小为$k \ge \sqrt{n \log n}$的植入团，改进了现有技术中的$\sqrt{n} (\log n)^2$界限。


<details>
  <summary>Details</summary>
Motivation: 解决半随机植入团问题，即在图中找到植入的完全子图，同时处理随机和对抗性边。

Method: 基于Blasiok等人的贪婪算法，通过采样邻接矩阵列的内积并检查其与随机向量的典型内积的偏差。改进方法采用1-范数类比，优化分析至最优界限。

Result: 算法成功将检测团大小的界限提升至$\sqrt{n \log n}$，回答了[BBK24]中的主要开放问题。

Conclusion: 通过改进分析方法和1-范数类比，实现了对半随机植入团问题的最优解，验证了理论猜想。

Abstract: We give a polynomial-time algorithm that finds a planted clique of size $k
\ge \sqrt{n \log n}$ in the semirandom model, improving the state-of-the-art
$\sqrt{n} (\log n)^2$ bound. This $\textit{semirandom planted clique problem}$
concerns finding the planted subset $S$ of $k$ vertices of a graph $G$ on $V$,
where the induced subgraph $G[S]$ is complete, the cut edges in $G[S; V
\setminus S]$ are random, and the remaining edges in $G[V \setminus S]$ are
adversarial.
  An elegant greedy algorithm by Blasiok, Buhai, Kothari, and Steurer [BBK24]
finds $S$ by sampling inner products of the columns of the adjacency matrix of
$G$, and checking if they deviate significantly from typical inner products of
random vectors. Their analysis uses a suitably random matrix that, with high
probability, satisfies a certain restricted isometry property. Inspired by
Wootters's work on list decoding, we put forth and implement the $1$-norm
analog of this argument, and quantitatively improve their analysis to work all
the way up to the conjectured optimal $\sqrt{n \log n}$ bound on clique size,
answering one of the main open questions posed in [BBK24].

</details>


### [30] [Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering](https://arxiv.org/abs/2506.18384)
*Quinten De Man,Laxman Dhulipala,Kishen N Gowda*

Main category: cs.DS

TL;DR: 本文研究了在完全动态设置中维护单链接树状图（SLD）的问题，提出了比从头重新计算更快的更新算法，并提供了并行和批处理版本。


<details>
  <summary>Details</summary>
Motivation: 单链接层次聚类（HAC）在动态数据中维护SLD的问题尚未被充分研究，现有方法效率低下。本文旨在提供更高效的动态更新算法。

Method: 假设输入是一个动态森林，提出插入和删除算法，并扩展为并行和批处理版本。算法的时间复杂度优于静态计算。

Result: 插入算法时间为O(h)，删除算法时间为O(h log(1+n/h))，并行版本具有工作高效性和多对数深度。

Conclusion: 本文提出的动态SLD维护算法在时间和并行效率上显著优于现有方法，适用于低高度树状图等场景。

Abstract: Single-linkage clustering is a popular form of hierarchical agglomerative
clustering (HAC) where the distance between two clusters is defined as the
minimum distance between any pair of points across the two clusters. In
single-linkage HAC, the output is typically the single-linkage dendrogram
(SLD), which is the binary tree representing the hierarchy of clusters formed
by iteratively contracting the two closest clusters. In the dynamic setting,
prior work has only studied maintaining a minimum spanning forest over the data
since single-linkage HAC reduces to computing the SLD on the minimum spanning
forest of the data.
  In this paper, we study the problem of maintaining the SLD in the
fully-dynamic setting. We assume the input is a dynamic forest $F$
(representing the minimum spanning forest of the data) which receives a
sequence of edge insertions and edge deletions. To our knowledge, no prior work
has provided algorithms to update an SLD asymptotically faster than recomputing
it from scratch. All of our update algorithms are asymptotically faster than
the best known static SLD computation algorithm, which takes $O(n \log h)$ time
where $h$ is the height of the dendrogram ($h \leq n-1$). Furthermore, our
algorithms are much faster in many cases, such as when $h$ is low. Our first
set of results are an insertion algorithm in $O(h)$ time and a deletion
algorithm in $O(h \log (1+n/h))$ time. Next, we describe parallel and
batch-parallel versions of these algorithms which are work-efficient or nearly
work-efficient and have poly-logarithmic depth. Finally, we show how to perform
insertions near-optimally in $O(c \log(1+n/c))$ time, where $c$ is the number
of structural changes in the dendrogram caused by the update, and give a
work-efficient parallel version of this algorithm that has polylogarithmic
depth.

</details>


### [31] [Tight simulation of a distribution using conditional samples](https://arxiv.org/abs/2506.18444)
*Tomer Adar*

Main category: cs.DS

TL;DR: 本文提出了一种基于前缀条件样本的分布模拟算法，改进了之前的复杂度，并提供了更严格的KL散度保证。


<details>
  <summary>Details</summary>
Motivation: 改进现有前缀条件样本模拟算法的复杂度，并提供更严格的分布近似保证。

Method: 使用前缀条件样本及兼容模型（如区间模型和子立方模型），复杂度为O(log²N/ε²)。

Result: 算法复杂度为O(log²N/ε²)，KL散度接近O(ε²)，优于现有结果。

Conclusion: 算法在估计任务中达到最优，复杂度下界为Ω(log²N/ε²)。

Abstract: We present an algorithm for simulating a distribution using prefix
conditional samples (Adar, Fischer and Levi, 2024), as well as
``prefix-compatible'' conditional models such as the interval model (Cannone,
Ron and Servedio, 2015) and the subcube model (CRS15, Bhattacharyya and
Chakraborty, 2018). The conditional sample complexity is $O(\log^2 N /
\varepsilon^2)$ prefix conditional samples per query, which improves on the
previously known $\tilde{O}(\log^3 N / \varepsilon^2)$ (Kumar, Meel and Pote,
2025). Moreover, our simulating distribution is $O(\varepsilon^2)$-close to the
input distribution with respect to the Kullback-Leibler divergence, which is
stricter than the usual guarantee of being $O(\varepsilon)$-close with respect
to the total-variation distance.
  We show that our algorithm is tight with respect to the highly-related task
of estimation: every algorithm that is able to estimate the mass of individual
elements within $(1 \pm \varepsilon)$-multiplicative error must make
$\Omega(\log^2 N / \varepsilon^2)$ prefix conditional samples per element.

</details>


### [32] [Near-Optimal Dynamic Policies for Joint Replenishment in Continuous/Discrete Time](https://arxiv.org/abs/2506.18491)
*Danny Segev*

Main category: cs.DS

TL;DR: 本文提出了一种确定性框架，用于高效逼近连续时间联合补货问题的最优动态策略，并开发了多项式时间近似方案（EPTAS）。


<details>
  <summary>Details</summary>
Motivation: 尽管动态策略在联合补货问题中具有重要影响，但对其最优策略的结构理解和计算问题仍存在显著空白。本文旨在填补这些空白。

Method: 通过开发算法思想和分析见解，提出了一种离散化框架，将连续时间问题转化为离散时间问题，并改进了现有近似方案的时间和复杂性。

Result: 提出了一个紧凑编码的补货策略，其长期平均成本在动态最优解的$1 + \epsilon$范围内，实现了高效的多项式时间近似方案。

Conclusion: 本文通过解决两个基本开放问题，显著提升了联合补货问题的近似效率和理论理解。

Abstract: While dynamic policies have historically formed the foundation of most
influential papers dedicated to the joint replenishment problem, we are still
facing profound gaps in our structural understanding of optimal such policies
as well as in their surrounding computational questions. To date, the seminal
work of Roundy (1985, 1986) and Jackson et al. (1985) remains unsurpassed in
efficiently developing provably-good dynamic policies in this context.
  The principal contribution of this paper consists in developing a wide range
of algorithmic ideas and analytical insights around the continuous-time joint
replenishment problem, culminating in a deterministic framework for efficiently
approximating optimal dynamic policies to any desired level of accuracy. These
advances enable us to derive a compactly-encoded replenishment policy whose
long-run average cost is within factor $1 + \epsilon$ of the dynamic optimum,
arriving at an efficient polynomial-time approximation scheme (EPTAS).
Technically speaking, our approach hinges on affirmative resolutions to two
fundamental open questions:
  -- We devise the first efficient discretization-based framework for
approximating the joint replenishment problem. Specifically, we prove that
every continuous-time infinite-horizon instance can be reduced to a
corresponding discrete-time $O( \frac{ n^3 }{ \epsilon^6 } )$-period instance,
while incurring a multiplicative optimality loss of at most $1 + \epsilon$.
  -- Motivated by this relation, we substantially improve on the $O(
2^{2^{O(1/\epsilon)}} \cdot (nT)^{ O(1) } )$-time approximation scheme of
Nonner and Sviridenko (2013) for the discrete-time joint replenishment problem.
Beyond an exponential improvement in running time, we demonstrate that
randomization and hierarchical decompositions can be entirely avoided, while
concurrently offering a relatively simple analysis.

</details>


### [33] [Learning Partitions with Optimal Query and Round Complexities](https://arxiv.org/abs/2505.05009)
*Hadley Black,Arya Mazumdar,Barna Saha*

Main category: cs.DS

TL;DR: 研究了通过子集查询学习未知分区的问题，提出了确定性查询复杂度的完整表征，并探讨了子集查询的两种泛化形式。


<details>
  <summary>Details</summary>
Motivation: 该问题在聚类、主动学习和众包等领域具有基础性和广泛应用，目标是减少适应性同时最小化查询复杂度。

Method: 通过确定性查询复杂度分析，提出算法在有限轮数内达到最优查询复杂度，并研究了子集查询的两种泛化形式。

Result: 给出了查询复杂度与轮数$r$的完整表征，算法仅需$O(\log \log n)$轮即可达到最优$O(nk)$查询复杂度。对于子集查询，提出了匹配上下界的非适应性算法。

Conclusion: 研究为学习分区问题提供了理论支持，特别是在减少适应性和查询复杂度方面，对实际应用具有重要意义。

Abstract: We consider the basic problem of learning an unknown partition of $n$
elements into at most $k$ sets using simple queries that reveal information
about a small subset of elements. Our starting point is the well-studied
pairwise same-set queries which ask if a pair of elements belong to the same
class. It is known that non-adaptive algorithms require $\Theta(n^2)$ queries,
while adaptive algorithms require $\Theta(nk)$ queries, and the best known
algorithm uses $k-1$ rounds. This problem has been studied extensively over the
last two decades in multiple communities due to its fundamental nature and
relevance to clustering, active learning, and crowd sourcing. In many
applications, it is of high interest to reduce adaptivity while minimizing
query complexity. We give a complete characterization of the deterministic
query complexity of this problem as a function of the number of rounds, $r$,
interpolating between the non-adaptive and adaptive settings: for any constant
$r$, the query complexity is
$\Theta(n^{1+\frac{1}{2^r-1}}k^{1-\frac{1}{2^r-1}})$. Our algorithm only needs
$O(\log \log n)$ rounds to attain the optimal $O(nk)$ query complexity.
  Next, we consider two generalizations of pairwise queries to subsets $S$ of
size at most $s$: (1) weak subset queries which return the number of classes
intersected by $S$, and (2) strong subset queries which return the entire
partition restricted on $S$. Once again in crowd sourcing applications, queries
on large sets may be prohibitive. For non-adaptive algorithms, we show
$\Omega(n^2/s^2)$ strong queries are needed. Perhaps surprisingly, we show that
there is a non-adaptive algorithm using weak queries that matches this bound up
to log-factors for all $s \leq \sqrt{n}$. More generally, we obtain nearly
matching upper and lower bounds for algorithms using subset queries in terms of
both the number of rounds, $r$, and the query size bound, $s$.

</details>


### [34] [Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs](https://arxiv.org/abs/2506.08405)
*Hadley Black,Arya Mazumdar,Barna Saha,Yinzhan Xu*

Main category: cs.DS

TL;DR: 论文提出了一种新的查询模型，基于图的连通分量数量，研究了图重构问题，并给出了查询复杂度的上下界。


<details>
  <summary>Details</summary>
Motivation: 研究图重构问题，探索基于连通分量数量的查询模型，填补现有研究的空白。

Method: 提出了一种新的查询模型，通过子集查询连通分量数量，分析了自适应和非自适应查询的复杂度。

Result: 证明了自适应查询的复杂度为Θ(m log n / log m)，非自适应查询需要Ω(n²)次，并给出了两轮自适应查询的算法。

Conclusion: 新查询模型为图重构提供了新的视角，自适应查询效率更高，非自适应查询复杂度较高。

Abstract: The graph reconstruction problem has been extensively studied under various
query models. In this paper, we propose a new query model regarding the number
of connected components, which is one of the most basic and fundamental graph
parameters. Formally, we consider the problem of reconstructing an $n$-node
$m$-edge graph with oracle queries of the following form: provided with a
subset of vertices, the oracle returns the number of connected components in
the induced subgraph. We show $\Theta(\frac{m \log n}{\log m})$ queries in
expectation are both sufficient and necessary to adaptively reconstruct the
graph. In contrast, we show that $\Omega(n^2)$ non-adaptive queries are
required, even when $m = O(n)$. We also provide an $O(m\log n + n\log^2 n)$
query algorithm using only two rounds of adaptivity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [35] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 量子软件工程面临测试和调试的独特挑战，开发者依赖传统方法而非量子专用工具，亟需改进工具和工作流程。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论转向实践，开发者面临概率执行、有限可观察性等问题，需了解当前实践以改进工具。

Method: 通过调查26名量子软件开发者并进行访谈，聚焦测试、调试及常见挑战。

Result: 开发者主要使用单元测试（88%）、回归测试（54%）和验收测试（54%），但仅31%使用量子专用工具。调试依赖传统方法，如打印语句和模拟器。常见错误源于库更新（81%）、开发者错误（68%）和兼容性问题（62%）。

Conclusion: 研究强调需开发更适配的测试和调试工具，并基于开发者实际需求提出建议。

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [36] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: 论文探讨了数字孪生（DTs）中集成多种数字模型（DMs）的挑战，通过专家调查揭示了标准化接口缺失、手动调整工作量大等问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在工业应用中日益重要，但集成多种数字模型时面临挑战，缺乏行业视角的研究。

Method: 通过跨多个应用领域的专家调查，识别和分析数字模型在数字孪生中的集成挑战。

Result: 研究发现标准化接口缺失、手动调整工作量大、模型跨生命周期复用支持有限等问题。

Conclusion: 未来研究应关注自动化模型组合和基于语义的互操作性。

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [37] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 该研究提出了一个基准框架FLARE，用于评估大型语言模型（LLMs）在电子表格任务中的表现，发现其在复杂任务中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在电子表格任务中的表现，填补现有研究的空白。

Method: 引入FLARE基准框架，涵盖从基础公式生成到复杂数据操作的任务。

Result: LLMs在简单任务中表现良好，但在复杂任务中常产生错误输出。

Conclusion: 当前LLMs在需要精确逻辑推理的电子表格任务中存在不足，需结合符号推理能力。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [38] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: LMR-BENCH是一个评估LLM代理从NLP研究论文中重现代码能力的基准，包含28个任务。实验显示，即使最先进的模型在科学推理和代码合成方面仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在从研究论文中重现代码任务中的能力，填补这一领域的空白。

Method: 提出LMR-BENCH基准，包含28个任务，基于23篇顶级NLP论文。实验采用标准提示和LLM代理设置，评估代码正确性。

Result: 实验结果表明，即使最先进的模型在科学推理和代码合成方面仍有显著局限性。

Conclusion: LLM代理在自主重现科学研究代码方面存在关键能力缺口，需进一步改进。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [39] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文研究了代码基准测试中提示敏感性对大型语言模型（LLM）性能评估的影响，提出了一种通用框架来设计语义和结构相似的提示模板，并通过实验验证了提示微小变化会导致性能显著波动。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试通常依赖单一提示模板，容易因提示敏感性导致模型性能评估不可靠。研究旨在探索代码任务中提示敏感性的影响，为未来基准测试设计提供参考。

Method: 提出一个通用框架，设计语义和结构相似的提示模板，并在8个代码任务和10个代表性开源LLM上进行实验，每个任务包含100个相似提示模板。使用多种统计指标分析结果。

Result: 实验表明，即使提示的微小变化也会导致性能显著波动，并可能引发不同模型间性能排名的不一致。

Conclusion: 提示敏感性对代码基准测试的可靠性有重要影响，未来设计需考虑这一点以确保更准确的LLM能力评估。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [40] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: MAdroid是一种基于大型语言模型的多代理方法，用于自动化多用户交互式应用测试，通过协调、操作和观察三种代理角色，显著提升了测试效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化测试方法难以满足多用户交互式功能（如聊天、直播等）的动态和协作需求，因此需要一种更高效的解决方案。

Method: MAdroid采用多代理架构，包括协调者（Coordinator）、操作者（Operator）和观察者（Observer），分别负责任务指导、用户交互模拟和过程监控。

Result: 在41项任务中，MAdroid成功完成82.9%的任务，动作相似度达96.8%，并发现11个多用户交互式bug。

Conclusion: MAdroid在多用户交互测试中表现出色，具有实际应用潜力，能够有效支持软件开发中的回归测试。

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [41] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: CodeMorph提出了一种支持多编程语言并保留跨文件依赖的代码扰动方法，以减少LLM评估中的数据泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 解决代码LLM中数据泄漏和评估指标膨胀问题，现有方法难以生成复杂多样的代码变体且不支持多语言。

Method: CodeMorph包含两部分：26种语义保留的代码变换方法和基于遗传算法的PESO选择算法，以优化扰动效果。

Result: 应用CodeMorph后，LLM在五种语言上的代码补全准确率平均下降24.67%，Python下降最多（45%）。PESO优化的代码相似度平均降低7.01%。

Conclusion: CodeMorph有效减少了数据泄漏，提升了代码扰动的多样性和复杂性，为LLM评估提供了更可靠的数据基础。

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [42] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文研究了基于模型变异的深度学习框架缺陷检测方法的有效性，发现现有方法存在高误报率和开发者忽视的问题，并提出优化策略，成功识别并修复了多个高优先级缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基于变异的测试方法在深度学习框架缺陷检测中存在高误报率和开发者忽视的问题，需要研究其有效性并提出改进方案。

Method: 收集并分类三个流行框架的缺陷报告，分析现有方法的不足，提出优化策略并验证其效果。

Result: 优化后识别出39个独特缺陷，其中31个被开发者确认，8个已修复，包括7个新缺陷中的4个高优先级问题。

Conclusion: 优化后的方法显著提高了缺陷检测的有效性，为深度学习框架测试提供了实用改进方向。

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [43] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: FUEL利用LLM分析反馈信息，提升深度学习框架的模糊测试效果，检测出多个新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试技术未充分利用多种反馈信息，且LLM在测试用例生成中潜力未完全开发。

Method: FUEL采用两个LLM代理：分析LLM从反馈中提取总结，生成LLM根据总结创建测试用例。

Result: 检测出104个PyTorch和TensorFlow的漏洞，其中93个为新漏洞，47个已修复，5个获得CVE编号。

Conclusion: 多类型反馈分析提升模糊测试性能，LLM在反馈分析中具有潜力。

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [44] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: AutoCBI利用预训练大语言模型（LLM）改进编译器错误定位，通过总结文件功能和重新排序可疑文件，显著提升了定位效果。


<details>
  <summary>Details</summary>
Motivation: 传统编译器错误定位技术存在测试程序变异和资源消耗的局限性，需要更高效的方法。

Method: AutoCBI结合LLM总结文件功能，利用测试程序、文件功能摘要、可疑文件列表和编译配置信息，重新排序可疑文件。

Result: 在GCC和LLVM的120个真实错误测试中，AutoCBI在Top-1结果中分别比RecBi、DiWi和FuseFL多定位66.67%/69.23%、300%/340%和100%/57.14%的错误。

Conclusion: AutoCBI通过LLM和多信息融合显著提升了编译器错误定位的效率和准确性。

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [45] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: 论文研究了LLM代码代理生成的失败补丁，提出了一个分类法，并设计了PAGENT工具来修复类型相关错误。


<details>
  <summary>Details</summary>
Motivation: 了解LLM代码代理生成失败补丁的根本原因，并探索修复方法。

Method: 收集了114个未解决问题和769个失败补丁，通过GPT-4o和人工分析进行分类，并设计了PAGENT工具进行类型推断和修复。

Result: 提出了六类失败原因的分类法，PAGENT成功修复了29个类型相关的失败补丁。

Conclusion: PAGENT在修复类型相关错误方面具有潜力，但仍有改进空间。

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [46] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: SAVANT利用语义预处理和LLM技术，提高了Java第三方库漏洞检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SCA工具在检测Java第三方库漏洞时，因语义理解和计算复杂性不足导致误报率高，影响开发效率。

Method: SAVANT结合语义预处理和LLM技术，分析API使用上下文，确定漏洞实际影响。

Result: 在55个实际应用中，SAVANT的精确率、召回率、准确率和F1分数均优于现有SCA工具。

Conclusion: SAVANT通过语义和LLM技术显著提升了漏洞检测效果，为开发团队提供了更可靠的解决方案。

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [47] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: BouncerBench是一个评估LLM在软件工程任务中是否能在输入不明确或输出可能错误时拒绝响应的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在处理模糊输入或错误输出时缺乏拒绝机制，导致不可靠行为，如生成错误代码或响应模糊问题。

Method: 引入BouncerBench基准测试，评估LLM是否能区分可操作问题和模糊问题，以及有效补丁和不可信补丁。

Result: 大多数模型无法在输入不明确或输出错误时拒绝响应，表明LLM在实际应用中仍有改进空间。

Conclusion: BouncerBench为评估和构建更谨慎、可信的代码代理提供了初步工具，LLM在软件工程中的可靠性仍需提升。

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [48] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: AI工具显著提升软件工程师的生产力，但随着项目复杂度增加，生产力提升效果减弱。AI生成的小代码片段对软件质量无显著负面影响，但在解决更复杂问题时质量下降。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对软件工程师生产力的长期影响及其对软件质量的影响。

Method: 通过对使用AI工具的软件从业者进行问卷调查。

Result: AI工具显著提高生产力，但复杂项目中效果减弱；小代码片段对质量无负面影响，复杂问题中质量下降。

Conclusion: AI工具在提升生产力方面有效，但需在复杂问题中结合人工分解与集成以保障质量。

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [49] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: 论文探讨了AI辅助生成式软件重用在‘AI原生’软件工程中的影响，提出了相关问题，并定义了研究议程。


<details>
  <summary>Details</summary>
Motivation: 随着AI和生成式软件重用的兴起，传统软件重用方法正被AI辅助方法取代，这可能导致类似‘货物崇拜开发’的问题。

Method: 通过讨论和问题提出，定义研究议程和行动呼吁。

Result: 提出了AI辅助生成式软件重用的潜在问题和挑战。

Conclusion: 需要进一步研究和行动以解决AI辅助软件重用的核心问题。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [50] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: 该研究通过混合方法分析GitHub上的构建脚本和问题，识别了13种代码异味类别，并开发了静态分析工具Sniffer。研究发现不同构建工具中常见的异味类型，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 构建脚本中的代码异味可能导致构建失败或增加技术债务，研究旨在帮助开发者避免这些问题。

Method: 结合定性（分析2000个GitHub问题）和定量（开发Sniffer工具分析5882个构建脚本）方法。

Result: 识别了13种异味类别，共10,895个异味实例，不同工具中异味类型各异。

Conclusion: 建议改进策略以提高构建脚本的效率、可靠性和可维护性。

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [51] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出VFArch=e，一种双模式方法，用于定位软件依赖中的漏洞函数（VF），适用于有或无补丁的场景，显著提高了准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现代漏洞数据库（如NVD）通常缺乏漏洞函数的关键信息，而补丁提取或忽略补丁的方法均存在局限性，需要一种全面的解决方案。

Method: VFArch=e采用双模式设计，结合补丁信息和无补丁场景的优化搜索，自动定位漏洞函数。

Result: 实验结果显示，VFArch=e在补丁存在和缺失模式下分别比最佳基线方法提高了1.3倍和1.9倍的Mean Reciprocal Rank，并显著减少了SCA工具的误报。

Conclusion: VFArch=e在实际应用中表现出色，能够高效定位漏洞函数，为软件组合分析提供了更可靠的解决方案。

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [52] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA利用图神经网络改进JavaScript调用图构建，通过链接预测提高召回率。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript调用图构建工具既不健全也不完整，存在误报和漏报问题。

Method: 将问题建模为全程序图上的链接预测，结合语法和语义边，利用图神经网络学习非局部关系。

Result: 在50个流行JavaScript库上评估，GRAPHIA在42%的未解决调用中将正确目标排名第一，72%排名前五。

Conclusion: 学习型方法可显著提升JavaScript调用图构建的召回率，是首个将GNN应用于多文件程序图的分析工作。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [53] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: 研究探讨了多学科数据密集型团队如何管理技术债务，识别了特定债务类型并提出了管理实践。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型解决方案投资的增加，技术债务管理成为关键问题，但多学科团队的管理实践缺乏研究。

Method: 采用探索性观察案例研究和社会技术扎根理论分析数据，形成概念和类别。

Result: 识别了技术数据组件债务和管道债务，并描述了团队如何评估、处理及适应迭代容量限制。

Conclusion: 研究结果与现有技术债务分类一致，强调需要新的实施模式和工具支持多学科团队。

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [54] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: 论文提出将能源效率作为AI设计核心，通过五个阶段的策略性选择实现显著节能。


<details>
  <summary>Details</summary>
Motivation: AI计算需求激增带来能源挑战，现有优化方法缺乏系统性。

Method: 在数据、模型、训练、系统和推理五个阶段进行策略性组合优化。

Result: 实验验证节能高达94.6%，同时保持95.95%的F1分数。

Conclusion: 系统性优化框架为可持续AI提供平衡效率与性能的可行方案。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [55] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: 论文提出Property-Generated Solver框架，利用基于属性的测试（PBT）替代传统测试驱动开发（TDD），通过两个协作的LLM代理（生成器和测试器）提升代码生成的正确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂编程任务中生成功能正确代码的挑战，避免传统TDD因低质量测试用例或自动化测试生成缺陷导致的误导。

Method: 引入Property-Generated Solver框架，结合PBT和两个LLM代理（生成器和测试器），生成器负责代码生成与迭代优化，测试器管理PBT生命周期并提供语义丰富的反馈。

Result: 在多个代码生成基准测试中，Property-Generated Solver的pass@1指标相对传统TDD方法提升了23.1%至37.3%。

Conclusion: 通过PBT和协作代理的闭环范式，Property-Generated Solver显著提升了LLM生成代码的正确性和泛化能力。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [56] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: 本研究评估了21种算法在三个任务中的表现，发现不同任务下最优模型不同，并验证了CodeBERT在用户流失分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用的模型数量有限，可能导致遗漏未测试算法，因此需要更广泛的基准测试。

Method: 采用标准化、对数变换等方法，结合贝叶斯超参数优化和遗传算法，并微调CodeBERT模型。

Result: Bagging集成模型在预测用户回答数时表现最佳（R2=0.821），XGBoost在预测用户流失时F1-score最高（0.825），CodeBERT的F1-score为0.809。

Conclusion: 研究提供了针对不同任务的最优模型和超参数，为研究者和实践者提供了实用指导。

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [57] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: 本文提出了一种框架，用于发现、分类和分析分散在机构系统中的开源软件项目，并以加州大学系统为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发在高校和研究机构中往往分散且难以追踪，导致这些高影响力工具缺乏可见性和认可。

Method: 利用GitHub的REST API构建管道，发现相关仓库并提取元数据；采用传统机器学习模型和大型语言模型（LLMs）进行分类。

Result: 框架在规模上有效，发现了超过52,000个仓库，并以高准确率预测了机构关联性。

Conclusion: 该框架为学术开源生态的可见性和分析提供了有效工具。

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [58] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: LTFix利用大型语言模型（LLM）和有限类型状态自动机，解决C语言内存错误的自动化修复问题，特别是跨函数和文件的复杂错误。


<details>
  <summary>Details</summary>
Motivation: C语言的手动内存管理易导致严重漏洞，传统APR方法依赖人工设计，效率低且受限。深度学习虽能自动提取修复模式，但需大量数据且缺乏可解释性。

Method: LTFix结合LLM和有限类型状态自动机，跟踪错误传播路径和上下文，捕捉内存状态和执行历史的时空维度信息。

Result: 该方法有效解决了LLM在跨函数和文件分析中的上下文限制问题，提升了内存错误修复的准确性和效率。

Conclusion: LTFix为复杂内存错误的自动化修复提供了新思路，结合了LLM的强大能力和类型状态自动机的精确指导。

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [59] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: 本文提出了一种名为RPhunter的新技术，通过结合代码和交易信息来检测Rug Pull骗局，显著提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: Rug Pull骗局在加密货币领域造成了重大财务损失，现有方法仅关注代码或交易行为单一层面，无法有效检测复杂的骗局。

Method: RPhunter通过构建语义风险代码图（SRCG）和代币流动行为图（TFBG），利用图神经网络提取特征，并通过注意力融合模型整合信息。

Result: 在645个Rug Pull事件的数据集上，RPhunter的精确率为95.3%，召回率为93.8%，F1分数为94.5%。在实际场景中，识别了4801个Rug Pull代币，精确率为91%。

Conclusion: RPhunter通过多角度分析显著提升了Rug Pull检测性能，优于现有方法。

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [60] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 论文提出Debugging Decay Index (DDI)框架，量化AI调试能力的指数衰减，并通过适时干预恢复调试效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI调试能力在2-3次尝试后会大幅衰减（60-80%），而迭代调试对代码生成系统至关重要。

Method: 引入DDI数学框架，量化调试失效点并提出干预策略，采用“战略重启”方法从利用转向探索。

Result: DDI揭示了当前AI调试的根本局限，并提供了优化迭代代码生成策略的首个定量框架。

Conclusion: 适时干预可挽救调试效果，DDI为优化调试策略提供了新工具。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [61] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: ModeliHub是一个基于Web的联邦分析平台，专注于基于Modelica的模型系统工程，提供统一的系统模型和实时仿真环境。


<details>
  <summary>Details</summary>
Motivation: 为系统工程提供一个基于Modelica的统一平台，解决异构工程工件的集成和分析问题。

Method: 采用中心辐射式联邦架构，结合Modelica编译器前端，支持跨浏览器、桌面和服务器的无缝运行。

Result: 实现了实时交互式仿真环境，支持数字孪生模型的部署和分析。

Conclusion: ModeliHub在严谨性和灵活性之间取得了平衡，为跨领域工程集成和分析提供了有效工具。

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [62] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: CACE是一种新型上下文感知模型驱逐策略，优化自托管CodeLLM在资源受限下的服务效率。


<details>
  <summary>Details</summary>
Motivation: 解决企业自托管CodeLLM时面临的模型管理和服务效率问题，尤其是隐私、延迟和模型定制需求。

Method: CACE利用多因素上下文感知（如模型加载时间、任务延迟敏感性、输出长度预测等）进行模型驱逐决策。

Result: CACE显著降低首次令牌时间和端到端延迟，减少模型驱逐次数。

Conclusion: CACE为实际软件工程环境中部署可扩展、低延迟AI编码助手提供了实用策略。

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [63] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 本文通过实证研究分析了三种基于LLM的代理（RepairAgent、AutoCodeRover、OpenHands）在程序修复和问题解决中的决策过程，揭示了其行为模式和失败原因。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程任务中广泛应用，但其内部决策过程尚不明确，限制了对其动态和失败模式的理解。

Method: 统一了三种代理的交互日志，分析了120条轨迹和2822次LLM交互，结合定量和定性方法评估结构特性、行为模式和推理连贯性。

Result: 研究发现成功与失败执行的行为模式和反模式，为代理设计提供了改进建议，如提示策略、失败诊断和反模式检测。

Conclusion: 研究为透明和鲁棒的自软件工程代理设计提供了数据支持和实用建议。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [64] [The Case for a Horizontal Federated AI operating System for Telcos](https://arxiv.org/abs/2506.17259)
*Sebastian Barros*

Main category: cs.NI

TL;DR: 论文提出了一种面向电信领域的横向联邦AI操作系统，旨在统一分散的AI应用，支持大规模部署AI代理，同时满足数据本地化、合规性和异构架构需求。


<details>
  <summary>Details</summary>
Motivation: 电信运营商面临AI能力分散的问题，需要统一管理客户体验、网络运营和服务编排中的AI应用。

Method: 设计并部署了一个横向联邦AI操作系统，提供统一的执行和协调层，支持联邦训练、现有系统集成，并遵循行业标准。

Result: 该系统能够打破现有孤岛，释放生态级智能，为电信领域的自动化提供基础。

Conclusion: 横向联邦AI操作系统在技术和结构上重新定义了分布式网络环境中智能的部署和组合方式。

Abstract: As artificial intelligence capabilities rapidly advance, Telco operators face
a growing need to unify fragmented AI efforts across customer experience,
network operations, and service orchestration. This paper proposes the design
and deployment of a horizontal federated AI operating system tailored for the
telecommunications domain. Unlike vertical vendor-driven platforms, this system
acts as a common execution and coordination layer, enabling Telcos to deploy AI
agents at scale while preserving data locality, regulatory compliance, and
architectural heterogeneity. We argue that such an operating system must expose
tightly scoped abstractions for telemetry ingestion, agent execution, and model
lifecycle management. It should support federated training across sovereign
operators, offer integration hooks into existing OSS and BSS systems, and
comply with TM Forum and O-RAN standards. Importantly, the platform must be
governed through a neutral foundation model to ensure portability,
compatibility, and multi-vendor extensibility. This architecture offers a path
to break the current silos, unlock ecosystem-level intelligence, and provide a
foundation for agent-based automation across the Telco stack. The case for this
horizontal layer is not only technical but structural, redefining how
intelligence is deployed and composed in a distributed network environment.

</details>


### [65] [Solving the Problem of Poor Internet Connectivity in Dhaka: Innovative Solutions Using Advanced WebRTC and Adaptive Streaming Technologies](https://arxiv.org/abs/2506.17343)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 论文提出了一种结合WebRTC技术和自适应流媒体的创新框架，以改善达卡市的高密度网络环境下的移动数据连接问题。


<details>
  <summary>Details</summary>
Motivation: 达卡市作为全球人口密度最高的城市之一，面临严重的互联网连接问题，亟需解决方案。

Method: 结合动态转码、实时纠错和优化接口选择，分析网络速度、移动塔密度等数据，并提供了数学模型和代码示例。

Result: 实验结果显示，该方法显著提高了吞吐量、降低了延迟，并提升了服务质量。

Conclusion: 该框架为超密集城市环境下的下一代通信系统提供了可扩展的解决方案。

Abstract: Dhaka, Bangladesh, one of the world's most densely populated cities, faces
severe challenges in maintaining reliable, high-speed internet connectivity.
This paper presents an innovative framework that addresses poor mobile data
connections through the integration of advanced WebRTC technology with adaptive
streaming and server-side recording solutions. Focusing on the unique network
conditions in Dhaka in 2025, our approach combines dynamic transcoding,
real-time error correction, and optimized interface selection to enhance
connectivity. We analyze empirical data on connection speeds, mobile tower
density, district-level population statistics, and social media usage.
Extensive mathematical formulations, including novel models for bitrate
estimation, round-trip time optimization, and reliability analysis, are
provided alongside detailed diagrams and multiple examples of code in both
Python and C++. Experimental results demonstrate significant improvements in
throughput, latency reduction, and overall service quality, offering a scalable
blueprint for next-generation communication systems in hyper-dense urban
environments.

</details>


### [66] [VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels](https://arxiv.org/abs/2506.17570)
*Sun Wei,Fang Minghong,Li Mengyuan*

Main category: cs.NI

TL;DR: 本文提出VReaves系统，通过分析VR头显的电磁辐射侧信道，实现VR应用识别和活动检测。


<details>
  <summary>Details</summary>
Motivation: VR设备的安全性尚未从硬件角度深入研究，尤其是电磁辐射这一潜在威胁。

Method: 通过信号处理流程分析VR头显中嵌入式IoT传感器的电磁辐射，并利用机器学习模型识别VR应用及其活动。

Result: 实验证明，VReaves能高效识别VR应用及其活动。

Conclusion: 电磁辐射侧信道为VR安全研究提供了新视角。

Abstract: Virtual reality (VR) has recently proliferated significantly, consisting of
headsets or head-mounted displays (HMDs) and hand controllers for an embodied
and immersive experience. The VR device is usually embedded with different
kinds of IoT sensors, such as cameras, microphones, communication sensors, etc.
However, VR security has not been scrutinized from a physical hardware point of
view, especially electromagnetic emanations (EM) that are automatically and
unintentionally emitted from the VR headset. This paper presents VReaves, a
system that can eavesdrop on the electromagnetic emanation side channel of a VR
headset for VR app identification and activity recognition. To do so, we first
characterize the electromagnetic emanations from the embedded IoT sensors
(e.g., cameras and microphones) in the VR headset through a signal processing
pipeline and further propose machine learning models to identify the VR app and
recognize the VR app activities. Our experimental evaluation with commercial
off-the-shelf VR devices demonstrates the efficiency of VR app identification
and activity recognition via electromagnetic emanation side channel.

</details>


### [67] [Non-Intrusive MLOps-Driven Performance Intelligence in Software Data Planes](https://arxiv.org/abs/2506.17658)
*Qiong Liu,Jianke Lin,Tianzhu Zhang,Leonardo Linguaglossa*

Main category: cs.NI

TL;DR: 论文提出了一种轻量级、非侵入式的框架（DRST），用于在线性能推断和适应，解决了NFV中资源争用导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: NFV的共享基础设施导致资源争用，难以保证性能，现有解决方案因集成开销和系统限制不适用。

Method: 利用底层NFV基础设施的硬件特征，避免直接数据平面收集，实现轻量级性能推断和适应。

Result: DRST框架在多样化NFV场景中表现出准确的性能推断、运行时瓶颈诊断和自动化适应能力。

Conclusion: DRST框架为NFV系统提供了一种高效、轻量的性能管理解决方案。

Abstract: The last decade has witnessed the proliferation of network function
virtualization (NFV) in the telco industry, thanks to its unparalleled
flexibility, scalability, and cost-effectiveness. However, as the NFV
infrastructure is shared by virtual network functions (VNFs), sporadic resource
contentions are inevitable. Such contention makes it extremely challenging to
guarantee the performance of the provisioned network services, especially in
high-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely
on direct traffic analysis (e.g., packet- or flow-level measurements) to detect
performance degradation and identify bottlenecks, which is not always
applicable due to significant integration overhead and system-level
constraints.
  This paper complements existing solutions with a lightweight, non-intrusive
framework for online performance inference and adaptation. Instead of direct
data-plane collection, we reuse hardware features in the underlying NFV
infrastructure, introducing negligible interference in the data plane. This
framework can be integrated into existing NFV systems with minimal engineering
effort and operates without the need for predefined traffic models or
VNF-specific customization. Through comprehensive evaluation across diverse NFV
scenarios, our Drift-Resilient and Self-Tuning (DRST) framework delivers
accurate performance inference, runtime bottleneck diagnose, and automated
adaptation under runtime drift, via a lightweight MLOps pipeline.

</details>


### [68] [Location Information Sharing Using Software Defined Radio in Multi-UAV Systems](https://arxiv.org/abs/2506.17678)
*Mehmet Kaan Erol,Eyup Emre Ulku*

Main category: cs.NI

TL;DR: 该研究通过SDR实现FANET多通道无线通信，用于共享位置信息，并在真实环境中测试。采用多通道令牌循环协议和GNU Radio平台，克服仿真环境的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决仿真环境与实际硬件、软件及环境因素脱节的问题，提供更真实的测试环境。

Method: 使用多通道令牌循环协议和GNU Radio平台开发SDR软件，建立通信层结构。

Result: 成功实现多通道通信，并公开了可复现的软件和框图。

Conclusion: 研究展示了SDR在真实环境中的实用性，并为相关领域提供了开源工具。

Abstract: SDR (Software Defined Radio) provides flexible, reproducible, and
longer-lasting radio tools for military and civilian wireless communications
infrastructure. SDR is a radio communication system whose components are
implemented as software. This study aims to establish multi-channel wireless
communication with FANET between two SDRs to share location information and
examine it in a realistic test environment. We used multi-channel token
circulation as a channel access protocol and GNU Radio platform for SDR
software development. The structures of the communication layer, including the
protocols, communication systems, and network structures suggested in the
studies in the literature, are generally tested in the simulation environment.
The simulation environment provides researchers with fast and easy development
and testing, but disadvantages exist. These cause a product to be isolated from
hardware, software, and cost effects encountered while developing and
environmental factors affecting the communication channel while testing.
Another contribution of the study is to present the developed block diagrams
and codes as clear and reproducible. The developed software and block diagrams
are available at github.com/knrl/uav-in-802.11-gnuradio.

</details>


### [69] [The Blind Spot of BGP Anomaly Detection: Why LSTM Autoencoders Fail on Real-World Outages](https://arxiv.org/abs/2506.17821)
*Samuel Oluwafemi Adebayo*

Main category: cs.NI

TL;DR: 论文探讨了深度学习在BGP安全中的局限性，指出仅依赖高复杂度异常检测的模型无法识别信号丢失或低偏差的真实事件。


<details>
  <summary>Details</summary>
Motivation: 挑战现有BGP异常检测模型的假设，即异常仅表现为高复杂度噪声，揭示其在实际安全事件中的盲区。

Method: 使用LSTM自编码器作为检测模型，对比历史BGP异常（如Slammer蠕虫、莫斯科停电）和模拟的BGP风暴。

Result: 模型能识别高复杂度合成异常，但无法检测信号丢失或低偏差的真实事件，误判为稳定性。

Conclusion: 仅将BGP异常定义为高重构误差事件是危险的简化，需开发混合多模态检测系统。

Abstract: Deep learning has significant potential to make the Internet's Border Gateway
Protocol (BGP) secure by detecting anomalous routing activity. However, all but
a few of these approaches rely on the implicit assumption that anomalies
manifest as noisy, high-complexity outliers from some normal baseline. This
work challenges this assumption by investigating if a best-in-class detection
model built on this assumption can effectively deal with real-world security
events' diverse signatures. We employ an LSTM-based autoencoder, a classical
example of a reconstruction-based anomaly detector, as our test vehicle. We
then contrast this model with a representative sampling of historical BGP
anomalies, including the Slammer worm and the Moscow blackout, and with a
simulated 'BGP storm' designed as a positive control. Our experience unveils a
blind spot of our model: the model easily identifies the synthetic anomaly of
high complexity but invariably fails to identify real-world events that
manifest in the form of a "signal loss" (e.g., Slammer, Moscow Blackout) or
"low-deviation" (e.g., WannaCry) signature. We demonstrate that the model
mistakenly recognizes the abrupt cut-off of BGP updates during catastrophic
failures as a signal of extreme stability, leading to reconstruction errors of
virtually zero and total failure to detect. We conclude that the
characterization of BGP anomalies as high-reconstruction-error events alone is
a weak and dangerous oversimplification. Our research provides the data-driven
case for why hybrid, multi-modal detection systems capable of identifying both
high-complexity and signal-loss signatures are required to enable end-to-end
BGP security.

</details>


### [70] [Supporting Deterministic Traffic on Standard NICs](https://arxiv.org/abs/2506.17877)
*Chuanyu Xue,Tianyu Zhang,Andrew Loveless,Song Han*

Main category: cs.NI

TL;DR: 论文提出了一种名为KeepON的软件驱动模型，用于在标准网络接口卡（NIC）上实现确定性数据包传输，填补了现有硬件方案的不足。


<details>
  <summary>Details</summary>
Motivation: 关键任务应用（如航空控制和工业自动化）需要确定性数据传输，但通用计算平台缺乏原生支持，硬件方案兼容性有限。

Method: KeepON通过让NIC持续传输固定大小的数据块作为占位符，实时应用的数据包在指定位置替换占位符，确保精确传输时间。

Result: 实验表明，KeepON在树莓派上实现了比默认驱动高162倍的调度精度，比硬件方案高2.6倍。

Conclusion: KeepON在标准硬件上实现了精确的时序控制，为关键任务应用提供了低成本解决方案。

Abstract: Networked mission-critical applications (e.g., avionic control and industrial
automation systems) require deterministic packet transmissions to support a
range of sensing and control tasks with stringent timing constraints. While
specialized network infrastructure (e.g., time-sensitive networking (TSN)
switches) provides deterministic data transport across the network, achieving
strict end-to-end timing guarantees requires equally capable end devices to
support deterministic traffic. These end devices, however, often employ
general-purpose computing platforms like standard PCs, which lack native
support for deterministic traffic and suffer from unpredictable delays
introduced by their software stack and system architecture. Although
specialized NICs with hardware scheduling offload can mitigate this problem,
the limited compatibility hinders their widespread adoption, particularly for
cost-sensitive applications or in legacy devices.
  To fill this gap, this paper proposes a novel software-based driver model,
namely KeepON, to enable the support of deterministic packet transmissions on
end devices equipped with standard NICs. The key idea of KeepON is to have the
NIC keep on transmitting fixed-size data chunks as placeholders, thereby
maintaining a predictable temporal transmission pattern. The real-time packets
generated by the mission-critical application(s) will then be precisely
inserted into this stream by replacing placeholders at the designated position
to ensure their accurate transmission time. We implement and evaluate KeepON by
modifying the network driver on a Raspberry Pi using its standard NIC. Our
experiments demonstrate that KeepON can achieve x162 times scheduling accuracy
comparable to its default driver, and x2.6 times compared to hardware-based
solution, thus enabling precise timing control on standard commodity hardware.

</details>


### [71] [LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN](https://arxiv.org/abs/2506.17911)
*Shefali Goel,Vinod Kumar Verma,Abhishek Verma*

Main category: cs.NI

TL;DR: 论文提出了一种轻量级安全解决方案LiSec-RTF，利用物理不可克隆函数（PUFs）生成独特的认证码，以抵御RPL网络中的路由表伪造攻击（RTF）。


<details>
  <summary>Details</summary>
Motivation: RPL协议在6LoWPAN网络中虽高效，但其控制消息未认证，易受RTF攻击，导致网络性能下降。目前缺乏有效防护措施。

Method: 提出LiSec-RTF方案，利用PUFs生成认证码（Licenses），在静态和移动场景下保护路由表免受伪造。

Result: 实验表明，LiSec-RTF显著提高了网络性能，降低了RTF攻击对包传递率和延迟的影响。

Conclusion: LiSec-RTF是一种高效的安全解决方案，适用于资源受限的6LoWPAN设备，能有效抵御RTF攻击。

Abstract: Routing Protocol for Low-Power and Lossy Networks (RPL) is an
energy-efficient routing solution for IPv6 over Low-Power Wireless Personal
Area Networks (6LoWPAN), recommended for resource-constrained devices. While
RPL offers significant benefits, its security vulnerabilities pose challenges,
particularly due to unauthenticated control messages used to establish and
maintain routing information. These messages are susceptible to manipulation,
enabling malicious nodes to inject false routing data. A notable security
concern is the Routing Table Falsification (RTF) attack, where attackers forge
Destination Advertisement Object (DAO) messages to promote fake routes via a
parent nodes routing table. Experimental results indicate that RTF attacks
significantly reduce packet delivery ratio, increase end-to-end delay, and
leverage power consumption. Currently, no effective countermeasures exist in
the literature, reinforcing the need for a security solution to prevent network
disruption and protect user applications. This paper introduces a Lightweight
Security Solution against Routing Table Falsification Attack (LiSec-RTF),
leveraging Physical Unclonable Functions (PUFs) to generate unique
authentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact
while considering the resource limitations of 6LoWPAN devices in both static
and mobile scenarios. Our testbed experiments indicate that LiSec-RTF
significantly improves network performance compared to standard RPL under RTF
attacks, thereby ensuring reliable and efficient operation.

</details>


### [72] [Mapping The Invisible Internet: Framework and Dataset](https://arxiv.org/abs/2506.18159)
*Siddique Abubakr Muntaka,Jacques Bou Abdo,Kemi Akanbi,Sunkanmi Oluwadare,Faiza Hussein,Oliver Konyo,Michael Asante*

Main category: cs.NI

TL;DR: 本文介绍了一个专注于I2P网络层的新数据集，填补了以往研究主要关注暗网应用层的空白。


<details>
  <summary>Details</summary>
Motivation: 以往研究多集中于I2P的应用层（如暗网），而网络层的研究较少。本文旨在通过收集和分析网络层数据，为相关研究提供支持。

Method: 使用SWARM-I2P框架部署I2P路由器作为映射代理，通过动态端口映射（30000-50000范围）收集数据。数据收集方法包括路由器控制台查询、netDb分析和被动监控。

Result: 数据集包含超过50,000个节点，包括2,077个FastSet节点和2,331个高带宽节点，记录了带宽、延迟和运行时间等指标。此外，还包含大量流量记录和地理分布数据。

Conclusion: 该数据集为隧道对等选择分析、匿名网络韧性研究和对抗建模提供了潜在应用价值。

Abstract: This article presents a novel dataset focusing on the network layer of the
Invisible Internet Project (I2P), where prior research has predominantly
examined application layers like the dark web. Data was collected through the
SWARM- I2P framework, deploying I2P routers as mapping agents, utilizing
dynamic port mapping (30000-50000 range). The dataset documents over 50,000
nodes, including 2,077 FastSet nodes and 2,331 high-capacity nodes
characterized by bandwidth, latency (mean 121.21ms +- 48.50), and uptime
metrics. It contains 1,997 traffic records (1,003,032 packets/bytes) and
4,222,793 records (2,147,585,625 packets/bytes), with geographic distributions
for 3,444 peers showing capacity metrics (mean 8.57 +- 1.20). Collection
methods included router console queries (127.0.0.1:port/tunnels), netDb
analysis, and passive monitoring, with anonymized identifiers. Data is
structured in CSV/TXT formats (Zenodo) with collection scripts (GitHub).
Potential applications include tunnel peer selection analysis, anonymity
network resilience studies, and adversarial modelling.

</details>


### [73] [Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets](https://arxiv.org/abs/2506.18381)
*Yiwei Liu,Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 提出了一个理论框架，用于解决无线网络中多频道会合问题（MRP），通过一致性频道选择函数和排列序列表示算法，证明了其最优性，并提出了简化实现复杂度的模算法。


<details>
  <summary>Details</summary>
Motivation: 解决无线网络中多频道会合问题，特别是在异构可用频道集的情况下，确保频道选择的一致性和高效性。

Method: 提出一致性频道选择函数，将其表示为排列序列，并设计模算法以减少实现复杂度。

Result: 证明了算法的最优性，推导了最大和期望会合时间的紧界，并通过仿真验证了算法的有效性和可扩展性。

Conclusion: 一致性频道跳变算法在MRP中表现优异，模算法简化实现且性能接近LSH算法，适用于多用户场景。

Abstract: We propose a theoretical framework for consistent channel hopping algorithms
to address the multichannel rendezvous problem (MRP) in wireless networks with
heterogeneous available channel sets. A channel selection function is called
consistent if the selected channel remains unchanged when the available channel
set shrinks, provided the selected channel is still available. We show that all
consistent channel selection functions are equivalent to the function that
always selects the smallest-index channel under appropriate channel relabeling.
This leads to a natural representation of a consistent channel hopping
algorithm as a sequence of permutations. For the two-user MRP, we characterize
rendezvous time slots using a fictitious user and derive tight bounds on the
maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR).
Notably, the ETTR is shown to be the inverse of the Jaccard index when
permutations are randomly selected. We also prove that consistent channel
hopping algorithms maximize the rendezvous probability. To reduce
implementation complexity, we propose the modulo algorithm, which uses modular
arithmetic with one-cycle permutations and achieves performance comparable to
locality-sensitive hashing (LSH)-based algorithms. The framework is extended to
multiple users, with novel strategies such as stick-together, spread-out, and a
hybrid method that accelerates rendezvous in both synchronous and asynchronous
settings. Simulation results confirm the effectiveness and scalability of the
proposed algorithms.

</details>


### [74] [XR Offloading Across Multiple Time Scales: The Roles of Power, Temperature, and Energy](https://arxiv.org/abs/2506.18584)
*Francesco Malandrino,Olga Chukhno,Alessandro Catania,Antonella Molinaro,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: 本文提出了一种名为TAO的温度感知卸载策略，旨在优化XR设备的计算卸载成本，同时满足功耗、温度和能量限制。


<details>
  <summary>Details</summary>
Motivation: XR设备需要在低延迟下处理高计算负载，同时面临功耗、温度和电池寿命的挑战，因此需要高效的卸载策略。

Method: 提出了一种综合考虑瞬时功耗、短期温度波动和长期电池寿命的系统模型，并设计了随机且静态的TAO卸载策略。

Result: 通过COMSOL模型验证，TAO比现有方法降低卸载成本超过35%，且不违反设备操作限制。

Conclusion: TAO是一种高效的卸载策略，适用于XR设备，显著提升了性能和能效。

Abstract: Extended reality (XR) devices, commonly known as wearables, must handle
significant computational loads under tight latency constraints. To meet these
demands, they rely on a combination of on-device processing and edge
offloading. This letter focuses on offloading strategies for wearables by
considering their impact across three time scales: instantaneous power
consumption, short-term temperature fluctuations, and long-term battery
duration. We introduce a comprehensive system model that captures these
temporal dynamics, and propose a stochastic and stationary offloading strategy,
called TAO (for temperature-aware offloading), designed to minimize the
offloading cost while adhering to power, thermal, and energy constraints. Our
performance evaluation, leveraging COMSOL models of real-world wearables,
confirms that TAO reduces offloading cost by over 35% compared to
state-of-the-art approaches, without violating the wearable operational limits.

</details>


### [75] [RL-Driven Semantic Compression Model Selection and Resource Allocation in Semantic Communication Systems](https://arxiv.org/abs/2506.18660)
*Xinyi Lin,Peizheng Li,Adnan Aijaz*

Main category: cs.NI

TL;DR: 本文提出了一种基于强化学习的框架，用于多用户语义通信系统中的语义压缩模型选择和资源分配，以平衡语义准确性、延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统常忽视用户间计算和通信能力的多样性，需自适应平衡语义准确性、延迟和能耗。

Method: 使用近端策略优化（PPO）的强化学习方法，动态选择语义压缩模型并分配带宽和功率，定义系统级优化指标RDE。

Result: 仿真表明该方法优于多种基线策略，讨论了框架的泛化能力、计算复杂度、可扩展性和实际应用意义。

Conclusion: 该框架为实际语义通信系统提供了有效的解决方案，平衡了性能与资源消耗。

Abstract: Semantic communication (SemCom) is an emerging paradigm that leverages
semantic-level understanding to improve communication efficiency, particularly
in resource-constrained scenarios. However, existing SemCom systems often
overlook diverse computational and communication capabilities and requirements
among different users. Motivated by the need to adaptively balance semantic
accuracy, latency, and energy consumption, this paper presents a reinforcement
learning (RL)-driven framework for semantic compression model (SCM) selection
and resource allocation in multi-user SemCom systems. To address the challenges
of balancing image reconstruction quality and communication performance, a
system-level optimization metric called Rate-Distortion Efficiency (RDE) has
been defined. The framework considers multiple SCMs with varying complexity and
resource requirements. A proximal policy optimization (PPO)-based RL approach
is developed to dynamically select SCMs and allocate bandwidth and power under
non-convex constraints. Simulations demonstrate that the proposed method
outperforms several baseline strategies. This paper also discusses the
generalization ability, computational complexity, scalability, and practical
implications of the framework for real-world SemCom systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Main category: cs.LG

TL;DR: 论文提出了一种名为MMET的新框架，用于高效解决多输入和多尺度的偏微分方程问题，通过解耦网格和查询点序列并引入GCE层，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在解决多输入和多尺度的偏微分方程问题时存在泛化能力不足和计算成本高的问题。

Method: MMET框架通过解耦网格和查询点序列、使用GCE层嵌入输入变量，并结合Hilbert曲线重排序和分块嵌入机制，降低输入长度和计算成本。

Result: 实验表明，MMET在多个物理领域的基准测试中，在精度和计算效率上均优于现有方法。

Conclusion: MMET为工程和物理应用中的实时偏微分方程求解提供了高效且可扩展的解决方案，并展示了预训练大规模模型在特定领域的潜力。

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [77] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为PCaM的渐进式聚焦交叉注意力机制，用于解决无监督域适应中前景对象不匹配的问题，通过过滤背景信息并增强跨域注意力一致性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Vision Transformers的无监督域适应方法存在前景对象不匹配的问题，导致注意力一致性不足，影响域对齐效果。

Method: 提出PCaM机制，逐步过滤背景信息，聚焦前景语义，并引入注意力引导损失以增强跨域注意力一致性。

Result: 在多个数据集（如Office-Home、DomainNet等）上验证了PCaM的有效性，取得了新的最优性能。

Conclusion: PCaM通过注意力引导的前景融合，显著提升了无监督域适应的性能，具有轻量级和通用性强的特点。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [78] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: 该论文系统综述了图神经网络（GNN）在多组学癌症研究中的应用，分类了不同方法，并总结了当前趋势和未来方向。


<details>
  <summary>Details</summary>
Motivation: 多组学数据整合是揭示癌症复杂生物学机制的有效策略，而GNN为建模异构结构化数据提供了框架。

Method: 分类了基于目标组学层、GNN结构和生物任务（如亚型分类、预后预测）的方法，并分析了趋势。

Result: 发现混合和可解释模型、注意力机制和对比学习的应用增加，患者特异性图和知识驱动先验成为新兴方向。

Conclusion: 该综述为设计有效的GNN多组学分析流程提供了资源，总结了当前实践、局限性和未来潜力。

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [79] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/abs/2506.17238)
*Siddharth M. Narayanan,James D. Braza,Ryan-Rhys Griffiths,Albert Bou,Geemi Wellawatte,Mayk Caldas Ramos,Ludovico Mitchener,Samuel G. Rodriques,Andrew D. White*

Main category: cs.LG

TL;DR: 论文展示了推理模型在化学领域的应用，无需额外预训练即可通过少量数据实现高性能。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型推理能力是否适用于数学、编程和逻辑之外的领域，如化学。

Method: 基于Mistral-Small-24B的24B参数LLM（ether0），通过强化学习在640,730个化学问题上训练。

Result: 模型在分子设计任务上超越通用化学模型、前沿模型和人类专家，且数据效率更高。

Conclusion: 该方法可推广至其他科学领域，训练高效的专业化语言模型。

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [80] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Main category: cs.LG

TL;DR: MLBuf-RePlAce是一种基于学习的虚拟缓冲感知全局布局框架，显著提升了时序性能。


<details>
  <summary>Details</summary>
Motivation: 现代技术节点中互连延迟与单元延迟的不均衡缩放，使得缓冲孔隙感知的布局对时序收敛至关重要。现有方法存在计算成本高或缺乏对电气规则检查（ERC）的全面考虑。

Method: MLBuf-RePlAce采用递归学习生成缓冲方法，预测缓冲类型和位置，并在全局布局中解决ERC违规问题。

Result: 在OpenROAD流程中，MLBuf-RePlAce在不降低后布线功耗的情况下，总负松弛（TNS）最大提升56%，平均提升31%。在商业流程中，TNS最大提升53%，平均提升28%，后布线功耗平均提升0.2%。

Conclusion: MLBuf-RePlAce是一种高效且开源的缓冲感知布局框架，显著提升了时序性能并解决了现有方法的不足。

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [81] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Main category: cs.LG

TL;DR: 论文提出了轻量级样本级多模态交互（LSMI）估计器，用于量化多模态信息中的冗余、独特性和协同作用，解决了理论和计算上的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态信息中的交互作用（冗余、独特性和协同作用）对分析信息动态至关重要，但样本级的精确量化存在理论和计算上的困难。

Method: 基于点信息理论，开发了冗余估计框架，并提出了通用的交互估计方法，采用高效的熵估计技术，适用于连续分布的样本级估计。

Result: 在合成和真实数据集上的实验验证了LSMI的精确性和高效性，揭示了样本和类别级别的细粒度动态。

Conclusion: LSMI为多模态数据的实际应用（如样本分区、知识蒸馏和模型集成）提供了有力工具，代码已开源。

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [82] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Main category: cs.LG

TL;DR: 本文提出了一种基于Certainty-Aware Probability (CAP) 分数的早期退出方法，通过结合logits和NSP分数来更准确地估计预测确定性，从而提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出方法仅依赖类别相关的logits来估计预测确定性，忽略了类别无关信息对确定性的负面影响，导致过早退出错误预测的样本。

Method: 提出NSP分数来衡量类别无关信息在特征中的比例，并基于此设计CAP分数，结合logits和NSP分数来优化预测确定性估计。

Result: 在GLUE基准测试中，平均加速比为2.19倍，性能损失可忽略，优于现有方法ConsistentEE 28%。

Conclusion: CAP分数方法在任务性能和推理效率之间取得了更好的平衡，代码已开源。

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [83] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Main category: cs.LG

TL;DR: 提出了一种稀疏攻击方法，通过最小化初始扰动的幅度并满足l0约束，解决了现有稀疏攻击方法的计算开销大、迁移性差和攻击强度弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏攻击方法生成的对抗样本稀疏性差，计算开销大，迁移性差，攻击强度弱，难以解释DNN的脆弱性。

Method: 引入了一种新颖的理论参数化技术近似NP难的l0优化问题，并设计了新的损失函数以同时最大化对抗性和最小化扰动像素数量。

Result: 实验表明，该方法在计算开销、迁移性和攻击强度上优于现有稀疏攻击方法，并能生成更稀疏的对抗样本，发现了两类噪声（“遮蔽噪声”和“引导噪声”）。

Conclusion: 该方法为评估DNN的鲁棒性提供了基准，并有助于解释对抗扰动如何误导分类器。

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [84] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为Referi的新框架，通过复用少样本示例来验证LLM输出，无需额外训练即可显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: LLM推理过程的随机性和结论不一致性带来挑战，现有方法（如多数投票或外部验证模型）存在局限性。

Method: 利用少样本示例评估候选输出，结合两种得分（基于贝叶斯规则）选择最佳答案，仅需少量额外推理。

Result: 在三个LLM和七项任务中，平均准确率提升4.8%。

Conclusion: Referi框架通过高效响应选择提升LLM准确性，无需额外训练。

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [85] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Main category: cs.LG

TL;DR: SliceGX是一种新型的GNN解释方法，通过分层渐进式分析生成解释，解决了现有方法缺乏细粒度层间分析的问题，并提供了高效的算法和查询接口。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法缺乏对中间表示如何影响最终结果的细粒度分析，限制了模型诊断和优化的能力。

Method: SliceGX通过自动分割GNN为层块，并在每个层块中发现高质量解释子图，逐步生成层间解释。

Result: 实验验证了SliceGX的有效性和效率，展示了其在模型调试中的实际应用价值。

Conclusion: SliceGX为GNN提供了更细粒度的解释能力，支持模型诊断和优化。

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [86] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: AlgoSelect是一个基于Comb Operator的框架，通过学习从数据中选择最优算法，具有通用性、信息理论最优性、计算高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决自动化算法选择问题，提供理论支持和实际部署方案。

Method: 使用Comb Operator（包括sigmoid门控选择器和N-Path Comb）进行算法插值选择，并学习自适应种子函数。

Result: 在20×20的问题-算法研究中实现了99.9%+的准确率，样本需求少且收敛快。

Conclusion: AlgoSelect为自动化算法选择提供了理论支持，具有广泛的应用前景。

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [87] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为SamS的动态样本调度算法，用于优化DPO过程中样本选择，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: DPO的性能依赖于人类偏好数据的质量，但现有方法忽略了模型状态变化的影响。

Method: 提出Sample Scheduling for DPO问题，并设计SamS算法动态选择样本。

Result: SamS在不修改DPO核心算法的情况下显著提升了任务性能。

Conclusion: SamS为通过更有效利用固定偏好数据集改进LLM对齐提供了新方向。

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [88] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Main category: cs.LG

TL;DR: Pucktrick是一个Python库，用于在合成数据中引入受控错误，以评估机器学习模型在真实数据缺陷下的鲁棒性。实验表明，使用受污染合成数据训练的模型表现优于纯合成数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和可用性问题，真实数据难以获取，而合成数据往往过于干净，缺乏真实数据的缺陷，影响模型泛化能力。

Method: 开发Pucktrick库，支持多种错误类型（如缺失值、噪声、异常值等），提供两种污染模式。

Result: 实验证明，在受污染的合成数据上训练的模型（尤其是树模型和线性模型）表现更好。

Conclusion: Pucktrick为评估模型鲁棒性提供了有效工具，受污染的合成数据能提升模型性能。

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [89] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Main category: cs.LG

TL;DR: 论文提出了一种基于卷积网络的多尺度时间序列预测模型MS-TVNet，通过多尺度时间序列重塑模块捕捉多周期片段和变量依赖关系，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测主要依赖Transformer和MLP模型，卷积网络的潜力尚未充分挖掘。

Method: 提出多尺度时间序列重塑模块，并基于此构建MS-TVNet，一种多尺度3D动态卷积神经网络。

Result: 在多个数据集上评估，MS-TVNet表现优于基线模型，达到SOTA水平。

Conclusion: 卷积网络能有效捕捉复杂时间模式，为未来研究提供了新方向。

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [90] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为StageRoute的分层算法，用于在线决策问题，以优化大型语言模型（LLM）的部署和查询路由。


<details>
  <summary>Details</summary>
Motivation: 由于新的大型语言模型快速涌现且旧模型迅速过时，服务提供商需要在有限的部署容量和查询成本预算下管理动态的模型库存。

Method: StageRoute采用分层方法：(i) 使用奖励上界和成本下界乐观选择下一阶段的模型；(ii) 通过预算约束的bandit子问题路由每个查询。

Result: 理论证明StageRoute的遗憾度为$T^{2/3}$，实验验证其接近最优性能。

Conclusion: StageRoute在理论和实践中均表现优异，为LLM的动态管理提供了高效解决方案。

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [91] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: ASMS（自适应社交元宇宙流媒体系统）通过结合联邦学习和深度强化学习，动态调整流媒体比特率，提升用户体验14%，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 社交元宇宙中隐私保护和高质量低延迟流媒体的需求是主要挑战。

Method: 提出基于联邦多智能体近端策略优化（F-MAPPO）的ASMS系统，结合联邦学习（FL）和深度强化学习（DRL）。

Result: ASMS在各种网络条件下比现有方法提升用户体验至少14%。

Conclusion: ASMS在动态和资源受限网络中提供无缝沉浸式流媒体，同时确保用户数据本地化。

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [92] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Main category: cs.LG

TL;DR: UltraSketchLLM是一种基于数据素描的框架，实现了超低比特压缩（低至0.5比特/权重），同时保持模型性能，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速增长超过了边缘设备的内存限制，需要超越1比特极限的极端权重压缩。

Method: 采用数据素描技术，结合低估AbsMaxMin素描、重要性感知空间分配和直通估计器，实现无索引压缩。

Result: 在Llama-3.2-1B上实现了0.5比特压缩，同时保持了竞争性的困惑度和可接受的延迟开销。

Conclusion: UltraSketchLLM为资源受限环境中的LLM部署提供了实用解决方案。

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [93] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Main category: cs.LG

TL;DR: 研究评估视神经头（ONH）生物力学是否改善青光眼三种进展性视野缺损模式的预测，并利用可解释AI识别关键应变敏感区域。


<details>
  <summary>Details</summary>
Motivation: 青光眼视野缺损预测通常依赖形态学特征，本研究旨在探索生物力学特征（如ONH应变）是否能提升预测准确性。

Method: 237名青光眼患者参与研究，通过眼压动态测量获取ONH应变数据，结合几何深度学习模型进行分类任务，并使用可解释AI技术分析关键区域。

Result: 模型AUC达0.77-0.88，表明ONH应变显著提升预测性能，下部和下颞侧神经视网膜缘是关键应变敏感区域。

Conclusion: ONH应变可增强青光眼视野缺损预测，神经视网膜缘是模型预测的关键区域。

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [94] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Main category: cs.LG

TL;DR: 研究内存限制如何影响强化学习代理在未知环境中的性能，探讨内存分配对模型估计和规划的影响。


<details>
  <summary>Details</summary>
Motivation: 资源限制会改变学习和决策过程，内存约束尤其影响代理在未知环境中的表现。

Method: 在MCTS和DQN算法中研究内存分配问题，分析其在情景学习和持续学习中的性能影响。

Result: 不同内存分配策略对代理性能有显著影响，尤其在模型估计和规划之间的权衡。

Conclusion: 内存分配策略是强化学习代理性能的关键因素，需根据学习环境优化。

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [95] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: OAT-Rephrase是一种优化感知的训练数据重述策略，通过LLM重述训练实例以提升零阶优化（ZO）微调的性能，缩小与一阶方法的差距。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（ZO）方法在微调大型语言模型（LLM）时内存效率高，但收敛慢且优化不稳定。本文旨在通过优化感知的数据重述策略解决这些问题。

Method: 提出OAT-Rephrase，利用LLM根据ZO动态（MeZO）重述训练实例，采用双阶段管道（重写器LLM和语义判断器）确保任务相关性和逻辑一致性。

Result: 在五个分类任务和三种LLM架构上评估，OAT-Rephrase显著提升MeZO微调性能，缩小或消除与一阶方法的差距。

Conclusion: 优化感知的重述策略是一种可重用且低开销的增强方法，适用于零阶优化微调场景。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [96] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了一种名为SUA的框架，用于恢复多模态大语言模型（MLLM）中未学习的信息，通过生成通用噪声模式触发模型泄露隐私内容。


<details>
  <summary>Details</summary>
Motivation: MLLM可能在训练过程中记忆敏感信息，现有未学习方法无法确保信息真正被遗忘，反而可能隐藏信息。因此，研究如何攻击未学习模型以恢复其隐藏知识。

Method: 提出SUA框架，学习通用噪声模式，应用于输入图像以触发模型泄露未学习内容。引入嵌入对齐损失以提高攻击的隐蔽性。

Result: 实验表明SUA能有效恢复MLLM中未学习的信息，且噪声模式具有泛化能力，适用于未见过的图像。

Conclusion: 未学习方法可能无法真正删除信息，而是隐藏信息，SUA揭示了这一潜在风险，为未来防御提供了方向。

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [97] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Main category: cs.LG

TL;DR: CF-VLM通过引入反事实样本增强视觉语言模型的因果推理能力，显著提升了细粒度判别和深度因果推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）依赖表面统计相关性，缺乏捕捉视觉与文本内容间深层因果逻辑的能力。

Method: 提出CF-VLM框架，通过三种互补训练目标：保持跨模态对齐、强化事实场景表示的唯一性和稳定性、提升对关键因果编辑的敏感性。

Result: CF-VLM在组合推理和泛化基准测试中优于现有方法，并有效减少视觉幻觉，提高事实一致性。

Conclusion: CF-VLM为高风险的现实场景提供了可靠的推理和可解释性基础。

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [98] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Main category: cs.LG

TL;DR: SafeRL-Lite是一个开源的Python库，用于构建具有约束性和可解释性的强化学习（RL）代理。


<details>
  <summary>Details</summary>
Motivation: 现有的RL工具包通常缺乏强制执行硬安全约束或生成人类可理解的决策理由的机制。

Method: SafeRL-Lite提供模块化封装，支持约束执行和实时解释（通过SHAP值和显著性图）。

Result: 在CartPole的约束变体上验证了其有效性，并提供了策略逻辑和安全性的可视化。

Conclusion: SafeRL-Lite是一个轻量级、可扩展的库，支持安全感知训练和解释性分析。

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [99] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Main category: cs.LG

TL;DR: 提出了一种名为NestQuant的资源友好型后训练量化方法，用于物联网设备上的量化模型切换，解决了现有方法的动态资源适应和存储开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态/混合精度量化方法需要重新训练或特殊硬件，而后训练量化（PTQ）在资源适应方面存在两个局限性：无法适应动态资源需求和多模型存储开销大。

Method: NestQuant通过整数权重分解和嵌套机制，将量化权重拆分为高低位整数权重，并通过自适应舍入优化高位权重。部署时只需存储一个模型，通过切换低位权重适应资源变化。

Result: 在ImageNet-1K预训练DNN上的实验表明，NestQuant在保持高准确率的同时，显著减少了数据传输、存储消耗和切换开销。例如，ResNet-101在INT8嵌套INT6时，准确率分别为78.1%和77.9%，切换开销减少约78.1%。

Conclusion: NestQuant是一种高效的资源适应方法，适用于物联网设备上的量化模型部署，显著提升了资源利用效率。

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [100] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，通过在输入空间中学习补充CLIP的冻结特征，以解决少样本测试时域适应问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CLIP的特征空间知识，但受限于其先验知识，尤其是在使用较弱骨干网络时性能下降。

Method: 引入一个独立的分支与CLIP并行，通过反向注意力学习专属知识，并利用贪婪文本集成和细化增强文本特征。

Result: 在5个大规模基准测试中表现优异，如iWildCam的F1提高了5.1，FMoW的WC Acc提高了3.1%。

Conclusion: 该方法通过直接学习输入空间和优化文本特征，显著提升了少样本测试时域适应的性能。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [101] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: 论文提出了一种名为CodeT5-Authorship的模型，用于识别C程序代码的生成来源，并发布了LLM-AuthorBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成的代码日益普遍，识别代码来源变得重要。

Method: 使用CodeT5的编码器层，结合两层分类头进行分类。

Result: 在二元分类中准确率达97.56%，多类分类中达95.40%。

Conclusion: CodeT5-Authorship在识别代码来源方面表现优异，支持开源科学。

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [102] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: DeInfoReg通过分解梯度流为多个短流，解决梯度消失问题，并利用管道策略实现多GPU并行训练，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播（BP）存在梯度消失问题，且难以充分利用并行计算资源。

Method: 提出DeInfoReg方法，将长梯度流分解为多个短流，结合管道策略实现多GPU并行。

Result: 实验表明DeInfoReg性能优于传统BP，抗噪能力更强，且能高效利用并行资源。

Conclusion: DeInfoReg是一种有效解决梯度消失和并行训练问题的新方法。

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [103] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: 本文探讨了扩散模型中创造性的起源，特别是自注意力机制在生成图像中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在图像生成中的应用日益广泛，理解其创造性来源变得尤为重要。现有理论未能解释自注意力在扩散过程中的作用。

Method: 扩展了现有理论，研究了由CNN和自注意力层参数化的扩散模型，并通过实验验证了自注意力的影响。

Result: 理论表明，自注意力能促进局部特征的全局一致性，实验也证实了这一点。

Conclusion: 自注意力在扩散模型中起到了关键作用，推动了生成图像的全局一致性。

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [104] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/abs/2506.17326)
*Agnideep Aich,Md Monzur Murshed,Sameera Hewage,Amanda Mayeaux*

Main category: cs.LG

TL;DR: 该研究提出了一种基于A2 copula的数据增强方法，用于解决糖尿病数据不平衡问题，结合XGBoost等机器学习算法，性能显著优于传统SMOTE方法。


<details>
  <summary>Details</summary>
Motivation: 糖尿病早期检测对降低健康风险至关重要，但数据不平衡问题影响机器学习模型的性能。

Method: 使用A2 copula生成少数类数据，结合逻辑回归、随机森林、梯度提升和XGBoost四种算法进行实验。

Result: XGBoost结合A2 copula在准确率、精确率、召回率、F1分数和AUC上分别提升4.6%、15.6%、20.4%、18.2%和25.5%。

Conclusion: A2 copula首次用于数据增强，效果优于SMOTE，展示了copula在机器学习中的潜力。

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [105] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/abs/2506.17333)
*Jaime A. Berkovich,Noah S. David,Markus J. Buehler*

Main category: cs.LG

TL;DR: AutomataGPT利用预训练的Transformer模型，成功从数据中推断和执行细胞自动机（CA）的动态规则，实现了高精度的状态预测和规则重建。


<details>
  <summary>Details</summary>
Motivation: 研究如何从简单的局部相互作用中推断复杂的时空行为，并用于定量预测，目前仍具挑战性。

Method: 使用预训练的Decoder-only Transformer模型（AutomataGPT），在100种二维二进制确定性CA规则上训练了约100万条模拟轨迹。

Result: 在未见过的CA规则上，AutomataGPT实现了98.5%的一步预测准确率，规则重建的功能准确率达96%，规则矩阵匹配率达82%。

Conclusion: 大规模预训练在CA规则空间上实现了显著的泛化能力，为将真实世界动态现象抽象为数据高效的CA替代模型奠定了基础。

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [106] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/abs/2506.17344)
*Tao Wang,Hewei Tang*

Main category: cs.LG

TL;DR: 提出了一种名为FFINO的新型神经算子架构，用于快速模拟地下氢储存（UHS）中的多相流问题，相比现有模型FMIONet，FFINO在参数数量、训练时间和GPU内存消耗上均有显著优化，且预测精度更高。


<details>
  <summary>Details</summary>
Motivation: 地下氢储存是低碳经济转型中的重要能源存储方式，快速建模氢羽流迁移和压力场演化对UHS管理至关重要。

Method: 通过参数化实验相对渗透率曲线，并将其作为关键不确定性参数纳入FFINO模型，同时与FMIONet模型进行多指标对比。

Result: FFINO模型比FMIONet减少了38.1%的可训练参数、17.6%的训练时间和12%的GPU内存成本，预测精度提高了9.8%，推理速度比数值模拟器快7850倍。

Conclusion: FFINO模型在UHS问题中具有卓越的时间效率和预测能力，可作为数值模拟的有效替代方案。

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [107] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Main category: cs.LG

TL;DR: 论文研究了基于Mixture-of-Experts (MoE)的大语言模型的安全对齐问题，提出了SAFEx框架，通过稳定性专家选择算法识别关键安全专家，揭示了MoE模型的特定位置脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐策略主要针对密集模型，无法有效解决MoE特有的安全漏洞，因此需要专门研究MoE模型的安全对齐问题。

Method: 提出了SAFEx框架，使用稳定性专家选择(SES)算法识别和验证安全关键专家，并将其分解为不同功能组。

Result: 实验表明，主流MoE模型的安全机制依赖于少数关键专家，禁用这些专家会显著降低模型拒绝有害请求的能力。

Conclusion: MoE模型的安全对齐存在特定位置脆弱性，SAFEx框架为识别和解决这一问题提供了有效工具。

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [108] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/abs/2506.17417)
*Mingyuan Wu,Meitang Li,Jingcheng Yang,Jize Jiang,Kaizhuo Yan,Zhaoheng Li,Minjia Zhang,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 研究发现，推理时计算技术（如解码时缩放和自我验证）可以显著提升视觉语言模型（VLM）的推理能力，但生成依赖方法比验证依赖方法效果更好。RL训练的VLM在视觉和文本模态上仍缺乏鲁棒的自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 探讨推理时技术（如自我修正和自我验证）是否适用于视觉语言模型（VLM），特别是通过强化学习（RL）训练的模型。

Method: 采用解码策略（如多数投票和最佳N选择）和自我验证方法，通过大量实验评估VLM的推理性能。

Result: 生成依赖方法（如多数投票）比验证依赖方法（如最佳N选择）表现更好；RL训练的VLM在自我验证能力上存在不足。

Conclusion: RL训练的VLM在视觉和文本模态上的自我验证能力仍需改进，生成依赖方法在提升推理性能上更具潜力。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [109] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Main category: cs.LG

TL;DR: 论文提出了一种结合神经加法模型（NAMs）和联邦学习的新方法（FedNAMs），旨在解决联邦学习中可解释性和隐私问题，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在可解释性和隐私保护方面存在挑战，需要一种既能保持模型性能又能提供透明分析的方法。

Method: 采用神经加法模型（NAMs）与联邦学习结合，通过分散训练和特征特定学习提升隐私和可解释性。

Result: FedNAMs在文本和图像分类任务中表现优异，保持了高可解释性且精度损失小，同时识别了关键预测特征。

Conclusion: FedNAMs在隐私保护、模型效率和可解释性方面均表现出色，适用于金融和医疗等领域。

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [110] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)
*Steffen Schotthöfer,Timon Klein,Jonas Kusch*

Main category: cs.LG

TL;DR: 论文提出了一种针对低秩参数化权重训练的新方法，解决了传统动量方法在收敛性上的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 低秩预训练和微调能降低大型神经网络的计算和存储成本，但传统优化器在训练低秩参数化时可能遇到收敛困难。

Method: 结合动态低秩近似和动量优化，设计了一种尊重参数空间几何结构的优化器。

Result: 实验表明，新方法在给定参数预算下实现了更快的收敛速度和更强的验证指标。

Conclusion: 新方法通过考虑优化问题的几何结构，有效解决了传统优化器在低秩训练中的问题。

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [111] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Main category: cs.LG

TL;DR: 论文提出了一种在少样本分类任务中通过推理时微调优化度量空间的方法，包括RDFT及其变体，结合元学习框架避免过拟合，并在多个音频数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的度量模型在少样本分类中未能充分利用支持样本的潜力，仅用于相似性比较，而未能微调度量空间。

Method: 提出了RDFT、IDFT和ADFT等推理时微调方法，结合优化元学习框架，避免过拟合。

Result: 在ESC-50、Speech Commands V2和Medley-solos-DB数据集上，方法显著提升了度量模型的性能，尤其适用于注意力模型。

Conclusion: 结合推理时微调和元学习，能有效提升少样本分类任务的性能，并具有跨领域泛化能力。

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [112] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 该论文综述了强化学习中状态表示学习方法，将其分为六类，探讨了其机制、优势和局限，旨在为研究者提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决复杂观测空间在序列决策问题中的挑战，提升样本效率、泛化能力和性能。

Method: 在无模型在线环境下，对状态表示学习方法进行分类，分析其机制。

Result: 提出了六类方法的分类框架，并讨论了评估表示质量的技术。

Conclusion: 该分类框架有助于理解该领域，并为未来研究提供了方向。

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [113] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/abs/2506.17543)
*Aditi Madhusudan Jain*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度Q网络（DQN）架构的新方法，用于预测电子商务中的购买意图和产品需求，结合LSTM和DQN的优势，在大型数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的电子商务环境中，准确预测用户行为对优化库存管理、个性化用户体验和最大化销售至关重要。

Method: 将强化学习概念应用于监督学习，结合LSTM的顺序建模能力和DQN的战略决策能力，处理高维时序数据。

Result: 模型在88.5万用户会话数据集上表现稳健，准确率达88%，AUC-ROC为0.88，优于传统方法。

Conclusion: 该研究为电子商务分析提供了结合深度学习和强化学习的新技术，对需求预测、用户体验和营销优化有重要意义。

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [114] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的不完整多视角手术评估模型，结合多视角数据和人工智能技术，用于直肠癌手术难度的可靠评估。


<details>
  <summary>Details</summary>
Motivation: 当前直肠癌手术难度评估主要依赖临床数据，但随着技术进步，更多数据可被收集，人工智能的应用成为可能。

Method: 构建多视角直肠癌数据集，提出双表示不完整多视角学习模型，结合缺失视角填补和二阶相似性约束，并基于TSK模糊系统提出多视角手术评估模型。

Result: 在MVRC数据集上，DRIMV_TSK模型优于其他先进算法。

Conclusion: 该模型为直肠癌手术难度评估提供了更全面和可靠的方法。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [115] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Main category: cs.LG

TL;DR: 本文提出两种改进残差强化学习的方法，提升样本效率并适用于随机基础策略。


<details>
  <summary>Details</summary>
Motivation: 现有残差强化学习方法在稀疏奖励和随机基础策略上表现不佳，需要改进。

Method: 利用基础策略的不确定性估计聚焦探索，并改进离策略残差学习以处理随机策略。

Result: 在仿真和真实环境中显著优于现有基线方法。

Conclusion: 改进方法有效提升了残差强化学习的性能和适用性。

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [116] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/abs/2506.17576)
*Furong Peng,Jinzhen Gao,Xuan Lu,Kang Liu,Yifan Huo,Sheng Wang*

Main category: cs.LG

TL;DR: 论文揭示了GCN中可训练线性变换加剧特征崩溃的问题，提出Layer-wise Gradual Training (LGT)方法，通过分层训练、低秩适应和恒等初始化提升深度GCN性能。


<details>
  <summary>Details</summary>
Motivation: 解决GCN在深度架构中因线性变换导致特征崩溃和性能下降的问题。

Method: 提出LGT方法，包括分层训练、低秩适应和恒等初始化三个组件。

Result: LGT在32层GCN中实现最优性能，并可与其他方法结合进一步提升效果。

Conclusion: LGT是一种通用的训练框架，适用于可扩展的深度GCN。

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [117] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Main category: cs.LG

TL;DR: LFR-PINO是一种新型物理信息神经算子，通过分层超网络架构和频域降维策略，显著提升了参数化偏微分方程（PDE）求解的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在参数化PDE求解中存在表达能力受限或计算复杂度高的问题，需要一种更高效的解决方案。

Method: 提出分层超网络架构和频域降维策略，减少参数数量并保持频谱特征。

Result: 在四个代表性PDE问题上，LFR-PINO比现有基线方法误差降低22.8%-68.7%，内存使用减少28.6%-69.3%。

Conclusion: LFR-PINO在计算效率和求解精度之间取得了最优平衡，适用于通用PDE求解。

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [118] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/abs/2506.17607)
*Chicheng Zhang,Yihan Zhou*

Main category: cs.LG

TL;DR: 本文研究了主动多分布学习问题，提出了新算法并改进了标签复杂度的上下界，证明了在可实现和不可知设置下的最优性。


<details>
  <summary>Details</summary>
Motivation: 多分布学习在协作学习、公平性和鲁棒性中有广泛应用，但主动学习的样本复杂度研究较少，现有算法的优劣性未知。

Method: 开发了新的主动多分布学习算法，分析了分布依赖和分布无关设置下的标签复杂度上下界。

Result: 在可实现设置中证明了信息论最优的上界，在不可知设置中揭示了$k\nu/\varepsilon^2$项对适当学习者的必要性。

Conclusion: 本文填补了主动多分布学习的理论空白，为实际应用提供了理论支持。

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>


### [119] [EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](https://arxiv.org/abs/2506.17615)
*Ibrahim Ahmed,Clemens Schaefer,Gil Tabak,Denis Vnukov,Zenong Zhang,Felix chern,Anatoliy Yevtushenko,Andy Davis*

Main category: cs.LG

TL;DR: 论文提出了一种名为EQuARX的动态块级量化AllReduce方法，用于TPU上的LLM部署，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）部署时因跨设备通信（如AllReduce）的性能开销大，传统量化方法难以直接应用于此类通信操作。

Method: 在XLA编译器中实现了一种动态块级量化AllReduce（EQuARX），结合TPU友好的量化和深度流水线技术。

Result: EQuARX在int8精度下比BF16 AllReduce快1.8倍，并在Gemma 3模型上分别实现了1.25倍和1.1倍的预填充加速。

Conclusion: EQuARX显著提升了LLM部署效率，且对模型质量影响极小。

Abstract: While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model quantization has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying quantization directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient quantized AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.

</details>


### [120] [Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation](https://arxiv.org/abs/2506.17620)
*Minh Le,Khoi Ton*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的慢性疾病风险预测模型，仅使用个人和生活方式因素，并通过SHAP解释性验证其可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖医学检测数据，限制了自我评估的实用性，且缺乏对解释性的医学验证。

Method: 开发深度学习模型，利用个人和生活方式因素预测13种慢性疾病风险，并通过SHAP验证特征重要性。

Result: 模型的关键特征与医学文献一致，验证了其可信度，适用于多种慢性疾病预测。

Conclusion: 该研究为开发可信的自我预防工具奠定了基础，未来可探索其他可信度验证方法及伦理问题。

Abstract: Chronic diseases are long-term, manageable, yet typically incurable
conditions, highlighting the need for effective preventive strategies. Machine
learning has been widely used to assess individual risk for chronic diseases.
However, many models rely on medical test data (e.g. blood results, glucose
levels), which limits their utility for proactive self-assessment.
Additionally, to gain public trust, machine learning models should be
explainable and transparent. Although some research on self-assessment machine
learning models includes explainability, their explanations are not validated
against established medical literature, reducing confidence in their
reliability. To address these issues, we develop deep learning models that
predict the risk of developing 13 chronic diseases using only personal and
lifestyle factors, enabling accessible, self-directed preventive care.
Importantly, we use SHAP-based explainability to identify the most influential
model features and validate them against established medical literature. Our
results show a strong alignment between the models' most influential features
and established medical literature, reinforcing the models' trustworthiness.
Critically, we find that this observation holds across 13 distinct diseases,
indicating that this machine learning approach can be broadly trusted for
chronic disease prediction. This work lays the foundation for developing
trustworthy machine learning tools for self-directed preventive care. Future
research can explore other approaches for models' trustworthiness and discuss
how the models can be used ethically and responsibly.

</details>


### [121] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.17621)
*Ravishka Rathnasuriya,Wei Yang*

Main category: cs.LG

TL;DR: 动态深度学习系统（DDLSs）因其输入自适应性而优化效率，但也带来安全风险，如效率攻击。本文研究其安全影响并提出防御措施。


<details>
  <summary>Details</summary>
Motivation: 现实环境中深度学习模型需高效推理，但动态行为引入安全风险，如效率攻击。

Method: 调查现有攻击策略，分析漏洞，提出针对性防御。

Result: 揭示DDLSs的效率漏洞，发现防御机制的不足。

Conclusion: 需进一步研究现代DDLSs的效率攻击可行性并开发防御措施。

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [122] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/abs/2506.17631)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: LLM-Prompt是一个基于大语言模型的时间序列预测框架，通过统一文本提示和多模态语义对齐提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法在文本提示统一性和模态差异处理上存在不足，影响了时间序列预测的效果。

Method: 提出LLM-Prompt框架，包含统一文本提示范式、语义空间嵌入和跨模态对齐模块，实现时间和文本信息的融合。

Result: 在6个公共数据集和3个碳排放数据集上的实验表明，LLM-Prompt具有强大的预测能力。

Conclusion: LLM-Prompt通过多模态融合和统一提示范式，显著提升了时间序列预测的性能。

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [123] [Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution](https://arxiv.org/abs/2506.17670)
*Manhin Poon,XiangXiang Dai,Xutong Liu,Fang Kong,John C. S. Lui,Jinhang Zuo*

Main category: cs.LG

TL;DR: 提出了一种基于上下文多臂老虎机框架的实时自适应多LLM选择方法，解决了动态提示变化下的LLM选择问题。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）的响应行为、成本和优势各异，如何在在线环境中动态选择最合适的LLM成为挑战。

Method: 提出了一种LinUCB-based算法，无需依赖未来上下文预测，实现了次线性遗憾。还引入了预算和位置感知扩展以适应不同成本和用户偏好。

Result: 实验表明，该方法在准确性和成本效率上优于现有LLM路由策略。

Conclusion: 上下文多臂老虎机框架为实时自适应LLM选择提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit diverse response behaviors, costs, and
strengths, making it challenging to select the most suitable LLM for a given
user query. We study the problem of adaptive multi-LLM selection in an online
setting, where the learner interacts with users through multi-step query
refinement and must choose LLMs sequentially without access to offline datasets
or model internals. A key challenge arises from unstructured context evolution:
the prompt dynamically changes in response to previous model outputs via a
black-box process, which cannot be simulated, modeled, or learned. To address
this, we propose the first contextual bandit framework for sequential LLM
selection under unstructured prompt dynamics. We formalize a notion of myopic
regret and develop a LinUCB-based algorithm that provably achieves sublinear
regret without relying on future context prediction. We further introduce
budget-aware and positionally-aware (favoring early-stage satisfaction)
extensions to accommodate variable query costs and user preferences for early
high-quality responses. Our algorithms are theoretically grounded and require
no offline fine-tuning or dataset-specific training. Experiments on diverse
benchmarks demonstrate that our methods outperform existing LLM routing
strategies in both accuracy and cost-efficiency, validating the power of
contextual bandits for real-time, adaptive LLM selection.

</details>


### [124] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/abs/2506.17672)
*Weiming Mai,Jie Gao,Oded Cats*

Main category: cs.LG

TL;DR: 提出了一种基于超网络和集成学习的方法，用于预测网约车司机的个性化决策，解决了传统模型无法捕捉非线性关系和个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型（如RUM）假设线性关系且忽略个性化偏好，难以准确预测司机的决策。

Method: 使用超网络动态生成线性效用函数的权重，结合集成学习提高模型的适应性和泛化能力。

Result: 在真实数据集上验证，模型能准确预测司机效用，平衡可解释性和不确定性量化。

Conclusion: 该方法不仅提升了预测准确性，还能揭示司机的个性化偏好，为系统优化提供工具。

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [125] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Main category: cs.LG

TL;DR: FaithfulSAE是一种训练稀疏自编码器（SAE）的方法，通过在模型自身合成的数据集上训练，解决了传统SAE因外部数据集导致的虚假特征问题，提高了稳定性和特征捕获能力。


<details>
  <summary>Details</summary>
Motivation: 传统SAE在外部数据集上训练可能导致虚假特征和不稳定性，影响对模型内部特征的准确捕获。

Method: 提出FaithfulSAE方法，使用模型自身合成的数据集训练SAE，减少分布外数据的影响。

Result: FaithfulSAE在5/7的模型中表现出更低的虚假特征比例，并在SAE探测任务中优于基于网络数据集的SAE。

Conclusion: FaithfulSAE消除了对外部数据集的依赖，提升了模型内部特征的捕获能力，强调了SAE训练数据集的重要性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [126] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/abs/2506.17680)
*Zhengni Yang,Rui Yang,Weijian Han,Qixin Liu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的创新方法，通过小冲孔试验数据预测高强度钢的真实应力-应变曲线，使用GAF和Seq2Seq模型，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在预测高强度钢的真实应力-应变曲线时效率低且成本高，需要一种更高效准确的方法。

Method: 利用GAF将载荷-位移序列转换为图像，结合LSTM编码器-解码器和多头交叉注意力机制的Seq2Seq模型。

Result: 实验结果显示预测误差最小为0.15 MPa，最大为5.58 MPa，表现出优越的预测精度。

Conclusion: 该方法为材料科学提供了一种高效准确的替代方案，显著提升了真实应力-应变关系的预测能力。

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [127] [CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition](https://arxiv.org/abs/2506.17709)
*Zebin Wang,Menghan Lin,Bolin Shen,Ken Anderson,Molei Liu,Tianxi Cai,Yushun Dong*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络（GNNs）在模型提取攻击（MEAs）中的脆弱性，并提出了一种节点查询策略，用于在有限资源下高效获取GNN模型。


<details>
  <summary>Details</summary>
Motivation: 随着GNNs的广泛应用和MLaaS平台的普及，模型提取攻击成为严重威胁。同时，在非对抗性研究环境中，高效获取GNN模型的需求也日益增长。

Method: 提出了一种迭代优化的节点查询策略，通过选择性采样信息丰富的节点，在有限查询条件下提高模型提取的效率和准确性。

Result: 在基准图数据集上的实验表明，该方法在准确性、保真度和F1分数上优于基线方法。

Conclusion: 研究揭示了GNNs在部署中的安全风险，同时为低资源研究环境提供了一种高效的GNN获取方法。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable utility across
diverse applications, and their growing complexity has made Machine Learning as
a Service (MLaaS) a viable platform for scalable deployment. However, this
accessibility also exposes GNN to serious security threats, most notably model
extraction attacks (MEAs), in which adversaries strategically query a deployed
model to construct a high-fidelity replica. In this work, we evaluate the
vulnerability of GNNs to MEAs and explore their potential for cost-effective
model acquisition in non-adversarial research settings. Importantly, adaptive
node querying strategies can also serve a critical role in research,
particularly when labeling data is expensive or time-consuming. By selectively
sampling informative nodes, researchers can train high-performing GNNs with
minimal supervision, which is particularly valuable in domains such as
biomedicine, where annotations often require expert input. To address this, we
propose a node querying strategy tailored to a highly practical yet
underexplored scenario, where bulk queries are prohibited, and only a limited
set of initial nodes is available. Our approach iteratively refines the node
selection mechanism over multiple learning cycles, leveraging historical
feedback to improve extraction efficiency. Extensive experiments on benchmark
graph datasets demonstrate our superiority over comparable baselines on
accuracy, fidelity, and F1 score under strict query-size constraints. These
results highlight both the susceptibility of deployed GNNs to extraction
attacks and the promise of ethical, efficient GNN acquisition methods to
support low-resource research environments.

</details>


### [128] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/abs/2506.17718)
*Zhuo He,Shuang Li,Wenze Song,Longhui Yuan,Jian Liang,Han Li,Kun Gai*

Main category: cs.LG

TL;DR: 论文提出了一种名为SYNC的方法，通过时间感知的结构因果模型（SCM）和动态因果表示学习，解决了现有EDG方法中存在的虚假相关性问题，提升了模型在动态场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据分布持续且复杂变化，现有EDG方法可能因仅建模数据与目标间的依赖关系而产生虚假相关性，阻碍模型泛化。

Method: 设计了时间感知SCM，结合动态因果因素和因果机制漂移，提出SYNC方法，通过信息论目标和序列VAE框架学习时间感知因果表示。

Result: 在合成和真实数据集上，SYNC表现出优越的时间泛化性能。

Conclusion: SYNC通过时间感知因果表示学习，有效解决了动态场景中的泛化问题，并理论上证明了其最优性。

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [129] [Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities](https://arxiv.org/abs/2506.17755)
*Xinghao Huang,Shengyu Tao,Chen Liang,Jiawei Chen,Junzhe Shi,Yuqi Li,Bizhong Xia,Guangmin Zhou,Xuan Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的专家混合网络（PIMOE），用于预测退役电动汽车电池的退化轨迹，仅需单周期部分信号即可实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 退役电动汽车电池在低碳能源系统中潜力巨大，但其退化行为的不确定性和数据不可访问性阻碍了安全、可扩展的部署。

Method: PIMOE网络结合自适应多退化预测模块，利用容量-电压和松弛数据分类退化模式，并通过循环网络预测长期轨迹。

Result: 在207个电池、77种使用条件和67,902个周期上验证，PIMOE的平均绝对百分比误差（MAPE）为0.88%，推理时间为0.43毫秒，性能优于现有方法。

Conclusion: PIMOE为电池退化轨迹计算提供了无需历史数据的可部署解决方案，重新定义了退役电池在可持续能源系统中的评估与集成方式。

Abstract: Retired electric vehicle batteries offer immense potential to support
low-carbon energy systems, but uncertainties in their degradation behavior and
data inaccessibilities under second-life use pose major barriers to safe and
scalable deployment. This work proposes a Physics-Informed Mixture of Experts
(PIMOE) network that computes battery degradation trajectories using partial,
field-accessible signals in a single cycle. PIMOE leverages an adaptive
multi-degradation prediction module to classify degradation modes using expert
weight synthesis underpinned by capacity-voltage and relaxation data, producing
latent degradation trend embeddings. These are input to a use-dependent
recurrent network for long-term trajectory prediction. Validated on 207
batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average
mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time.
Compared to the state-of-the-art Informer and PatchTST, it reduces
computational time and MAPE by 50%, respectively. Compatible with random state
of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50%
average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB
training data. Broadly, PIMOE framework offers a deployable, history-free
solution for battery degradation trajectory computation, redefining how
second-life energy storage systems are assessed, optimized, and integrated into
the sustainable energy landscape.

</details>


### [130] [Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion](https://arxiv.org/abs/2506.17761)
*Jiheng Liang,Ziru Yu,Zujie Xie,Yuchen Guo,Yulan Guo,Xiangyang Yu*

Main category: cs.LG

TL;DR: 提出了一种多模态光谱分析框架，结合知识图谱和大型语言模型，提升光谱分析的通用性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前光谱分析方法存在单模态数据依赖、通用性有限和可解释性差的问题。

Method: 将原始光谱转化为文本图（TAGs），结合知识图谱和提示节点，利用图神经网络完成任务。

Result: 在多种光谱分析任务中表现优异，支持零样本和小样本学习。

Conclusion: 为基于LLM的光谱分析提供了可扩展和可解释的基础，统一了物理和化学模态。

Abstract: Motivated by the limitations of current spectral analysis methods-such as
reliance on single-modality data, limited generalizability, and poor
interpretability-we propose a novel multi-modal spectral analysis framework
that integrates prior knowledge graphs with Large Language Models. Our method
explicitly bridges physical spectral measurements and chemical structural
semantics by representing them in a unified Textual Graph format, enabling
flexible, interpretable, and generalizable spectral understanding. Raw spectra
are first transformed into TAGs, where nodes and edges are enriched with
textual attributes describing both spectral properties and chemical context.
These are then merged with relevant prior knowledge-including functional groups
and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes"
supporting LLM-based contextual reasoning. A Graph Neural Network further
processes this structure to complete downstream tasks. This unified design
enables seamless multi-modal integration and automated feature decoding with
minimal manual annotation. Our framework achieves consistently high performance
across multiple spectral analysis tasks, including node-level, edge-level, and
graph-level classification. It demonstrates robust generalization in both
zero-shot and few-shot settings, highlighting its effectiveness in learning
from limited data and supporting in-context reasoning. This work establishes a
scalable and interpretable foundation for LLM-driven spectral analysis,
unifying physical and chemical modalities for scientific applications.

</details>


### [131] [Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks](https://arxiv.org/abs/2506.17768)
*Keigo Nishida,Eren Mehmet Kıral,Kenichi Bannai,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: 论文提出了一种基于对数正态分布后验的贝叶斯学习规则，设计了Log-Normal Multiplicative Dynamics (LMD)算法，用于人工神经网络的稳定低精度训练。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过模拟生物突触的对数正态分布动态，设计出类似的人工神经网络乘法训练方法。

Method: 推导了假设权重对数正态后验分布的贝叶斯学习规则，提出LMD算法，结合乘性噪声和正则化。

Result: LMD在Vision Transformer和GPT-2的低精度前向操作中实现了稳定且准确的训练。

Conclusion: 乘性动态可能为未来高效能硬件上的稳定低精度推理和学习提供支持。

Abstract: Studies in neuroscience have shown that biological synapses follow a
log-normal distribution whose transitioning can be explained by noisy
multiplicative dynamics. Biological networks can function stably even under
dynamically fluctuating conditions arising due to unreliable synaptic
transmissions. Here we ask: Is it possible to design similar multiplicative
training in artificial neural networks? To answer this question, we derive a
Bayesian learning rule that assumes log-normal posterior distributions over
weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD)
algorithm. The algorithm uses multiplicative updates with both noise and
regularization applied multiplicatively. The method is as easy to implement as
Adam and only requires one additional vector to store. Our results show that
LMD achieves stable and accurate training-from-scratch under low-precision
forward operations for Vision Transformer and GPT-2. These results suggest that
multiplicative dynamics, a biological feature, may enable stable low-precision
inference and learning on future energy-efficient hardware.

</details>


### [132] [PhysiX: A Foundation Model for Physics Simulations](https://arxiv.org/abs/2506.17774)
*Tung Nguyen,Arsh Koneru,Shufan Li,Aditya grover*

Main category: cs.LG

TL;DR: PhysiX是首个用于物理模拟的大规模基础模型，通过自回归生成和离散标记化技术解决数据稀缺问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 物理模拟领域因数据稀缺和规模多样性问题，尚未受益于基础模型的成功。PhysiX旨在填补这一空白。

Method: PhysiX采用4.5B参数的自回归生成模型，使用离散标记化编码多尺度物理过程，并引入细化模块减少离散化误差。

Result: PhysiX在The Well基准测试中超越任务专用方法和现有最佳方法，验证了跨任务联合训练的协同效应。

Conclusion: PhysiX成功解决了物理模拟的数据瓶颈，证明了自然视频知识可迁移至物理模拟，并展示了多任务训练的潜力。

Abstract: Foundation models have achieved remarkable success across video, image, and
language domains. By scaling up the number of parameters and training datasets,
these models acquire generalizable world knowledge and often surpass
task-specific approaches. However, such progress has yet to extend to the
domain of physics simulation. A primary bottleneck is data scarcity: while
millions of images, videos, and textual resources are readily available on the
internet, the largest physics simulation datasets contain only tens of
thousands of samples. This data limitation hinders the use of large models, as
overfitting becomes a major concern. As a result, physics applications
typically rely on small models, which struggle with long-range prediction due
to limited context understanding. Additionally, unlike images, videos, or
text-which typically exhibit fixed granularity-physics datasets often vary
drastically in scale, amplifying the challenges of scaling up multitask
training. We introduce PhysiX, the first large-scale foundation model for
physics simulation. PhysiX is a 4.5B parameter autoregressive generative model.
It uses a discrete tokenizer to encode physical processes at different scales
into a sequence of discrete tokens, and employs an autoregressive next-token
prediction objective to model such processes in the token space. To mitigate
the rounding error in the discretization process, PhysiX incorporates a
specialized refinement module. Through extensive experiments, we show that
PhysiX effectively addresses the data bottleneck, outperforming task-specific
baselines under comparable settings as well as the previous absolute
state-of-the-art approaches on The Well benchmark. Our results indicate that
knowledge learned from natural videos can be successfully transferred to
physics simulation, and that joint training across diverse simulation tasks
enables synergistic learning.

</details>


### [133] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Main category: cs.LG

TL;DR: 论文提出了一种将机器学习模型输出与PyReason框架结合的新方法，实现实时自适应决策。


<details>
  <summary>Details</summary>
Motivation: 解决将机器学习模型的感知或提取输出转化为复杂工作流中可操作的理性决策的挑战。

Method: 通过PyReason框架，将ML模型的输出（如概率、置信度）转换为逻辑事实，并动态计算最小模型。

Result: 实现了实时自适应决策，支持时间敏感数据分析和组织知识整合。

Conclusion: 结合ML模型的感知能力和PyReason的逻辑推理，为复杂流程自动化提供了强大工具。

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [134] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/abs/2506.17779)
*Andrei Cristian Nica,Akshaya Vishnu Kudlu Shanbhogue,Harshil Shah,Aleix Cambray,Tudor Berariu,Lucas Maystre,David Barber*

Main category: cs.LG

TL;DR: 介绍了UIExplore-Bench，首个专注于UI探索的基准测试，评估代理在结构化模式和屏幕模式下的表现，提出hUFO指标量化探索效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏对UI探索阶段的系统评估，需开发基准测试以推动研究。

Method: 在GitLab沙盒环境中，通过结构化模式（DOM树）和屏幕模式（GUI观察）评估代理，提出hUFO指标。

Result: UIExplore-AlGo表现最佳，结构化模式达人类性能77.2%，屏幕模式达59.0%。

Conclusion: 当前代理与人类表现差距显著，基准测试及配套工具公开以促进研究。

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [135] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Main category: cs.LG

TL;DR: 论文提出了一种名为MoTE的Transformer块，通过任务感知对比学习提升低容量模型生成专用嵌入的能力，性能显著提升且不增加额外负担。


<details>
  <summary>Details</summary>
Motivation: 现有指令调节方法在低容量模型中存在表示限制，限制了性能提升。

Method: 引入Mixture of Task Experts (MoTE)块，结合任务感知对比学习（TACL）训练任务专用参数。

Result: MoTE在检索数据集上性能提升64%（+3.27→+5.21），所有数据集上提升43%（+1.81→+2.60）。

Conclusion: MoTE在不改变指令、数据、推理时间或参数量的情况下显著提升了嵌入性能。

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [136] [SING: SDE Inference via Natural Gradients](https://arxiv.org/abs/2506.17796)
*Amber Hu,Henry Smith,Scott Linderman*

Main category: cs.LG

TL;DR: 提出了一种名为SING的自然梯度变分推断方法，用于高效推断潜在随机微分方程（SDE）模型中的状态路径，解决了现有方法收敛慢和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 潜在SDE模型在复杂领域中应用广泛，但精确推断状态路径通常不可行，现有变分推断方法存在收敛慢和不稳定的问题。

Method: SING利用自然梯度变分推断，近似难解积分并并行化时间计算，从而高效利用模型和变分后验的几何结构。

Result: SING在状态推断和漂移函数估计上优于现有方法，并在神经动力学建模等应用中表现出色。

Conclusion: SING为复杂动态系统提供了高效准确的推断工具，特别适用于先验知识有限和非共轭结构的场景。

Abstract: Latent stochastic differential equation (SDE) models are important tools for
the unsupervised discovery of dynamical systems from data, with applications
ranging from engineering to neuroscience. In these complex domains, exact
posterior inference of the latent state path is typically intractable,
motivating the use of approximate methods such as variational inference (VI).
However, existing VI methods for inference in latent SDEs often suffer from
slow convergence and numerical instability. Here, we propose SDE Inference via
Natural Gradients (SING), a method that leverages natural gradient VI to
efficiently exploit the underlying geometry of the model and variational
posterior. SING enables fast and reliable inference in latent SDE models by
approximating intractable integrals and parallelizing computations in time. We
provide theoretical guarantees that SING will approximately optimize the
intractable, continuous-time objective of interest. Moreover, we demonstrate
that better state inference enables more accurate estimation of nonlinear drift
functions using, for example, Gaussian process SDE models. SING outperforms
prior methods in state inference and drift estimation on a variety of datasets,
including a challenging application to modeling neural dynamics in freely
behaving animals. Altogether, our results illustrate the potential of SING as a
tool for accurate inference in complex dynamical systems, especially those
characterized by limited prior knowledge and non-conjugate structure.

</details>


### [137] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/abs/2506.17807)
*Lijun Zhang,Xiao Liu,Hui Guan*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的生成方法，直接从任务标识生成任务特定参数，无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 减少对标记数据和任务特定微调的依赖，提高效率。

Method: 使用扩散模型学习任务特定参数空间的结构，按需生成参数。

Result: 扩散模型能生成准确的任务特定参数，支持多任务插值，但无法泛化到未见任务。

Conclusion: 该方法在生成任务特定参数方面有潜力，但对未见任务的泛化能力有限。

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [138] [Flatness After All?](https://arxiv.org/abs/2506.17809)
*Neta Shoham,Liron Mor-Yosef,Haim Avron*

Main category: cs.LG

TL;DR: 论文提出了一种基于Hessian矩阵软秩的平坦性度量方法，用于评估神经网络的泛化能力，并在校准和非校准模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究损失函数曲率与泛化能力的关系，尤其是在过参数化网络中，发现平坦极小值通常泛化更好，但现有方法无法完全解释这一现象。

Method: 提出使用Hessian矩阵的软秩度量平坦性，并结合校准模型的条件，证明该方法能准确捕捉泛化差距。

Result: 实验表明，该方法在评估泛化差距时比基线方法更稳健。

Conclusion: Hessian软秩度量提供了一种可靠的方法来评估神经网络的泛化能力，尤其在模型校准的情况下表现更优。

Abstract: Recent literature has examined the relationship between the curvature of the
loss function at minima and generalization, mainly in the context of
overparameterized networks. A key observation is that "flat" minima tend to
generalize better than "sharp" minima. While this idea is supported by
empirical evidence, it has also been shown that deep networks can generalize
even with arbitrary sharpness, as measured by either the trace or the spectral
norm of the Hessian. In this paper, we argue that generalization could be
assessed by measuring flatness using a soft rank measure of the Hessian. We
show that when the common neural network model (neural network with exponential
family negative log likelihood loss) is calibrated, and its prediction error
and its confidence in the prediction are not correlated with the first and the
second derivatives of the network's output, our measure accurately captures the
asymptotic expected generalization gap. For non-calibrated models, we connect
our flatness measure to the well-known Takeuchi Information Criterion and show
that it still provides reliable estimates of generalization gaps for models
that are not overly confident. Experimental results indicate that our approach
offers a robust estimate of the generalization gap compared to baselines.

</details>


### [139] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/abs/2506.17826)
*Zhongtian Sun,Anoushka Harit,Pietro Lio*

Main category: cs.LG

TL;DR: HGCNet利用超图因果框架研究批量大小如何通过梯度噪声、极小值锐度和模型复杂性影响泛化，优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索批量大小在图和文本领域中影响泛化的因果机制，填补现有研究的空白。

Method: 提出HGCNet，基于超图和深度结构因果模型（DSCMs），利用do-calculus量化批量大小的直接和间接效应。

Result: 实验表明HGCNet优于GCN、GAT等基线模型，小批量通过增加随机性和平坦极小值提升泛化。

Conclusion: HGCNet为深度学习训练策略提供可解释的因果见解，推动架构和优化的原则性选择。

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [140] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一种名为IRO的方法，通过强化学习框架对齐冻结的基础模型，无需修改参数，适用于测试时优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RLHF和DPO需直接优化模型参数，无法在测试时使用或适用于无法访问模型权重的情况。测试时方法虽无需权重更新，但推理成本高且依赖不完美的奖励函数。

Method: IRO通过迭代采样、重采样和训练轻量级价值函数，利用搜索优化过程在测试时引导生成。

Result: IRO无需访问模型权重即可实现模型对齐，类似于OpenAI的RFT，但更灵活。

Conclusion: IRO为模型对齐提供了一种高效且灵活的解决方案，适用于无法修改模型权重的情况。

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [141] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/abs/2506.17840)
*Anoushka Harit,Zhongtian Sun*

Main category: cs.LG

TL;DR: 提出Causal-SphHN框架，结合高阶结构、方向性影响和认知不确定性，用于社会行为预测，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 人类社交行为受复杂交互影响，需建模不确定性、因果关系和群体动态。

Method: 使用超球嵌入表示个体，超边表示群体，结合香农熵量化不确定性，Granger子图识别因果依赖，角向消息传递机制传播信息。

Result: 在SNARE、PHEME和AMIGOS数据集上表现优于基线，预测更准确、鲁棒且可解释。

Conclusion: Causal-SphHN为动态社交环境中的学习提供统一因果几何方法。

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [142] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/abs/2506.17847)
*Cristian Del Gobbo*

Main category: cs.LG

TL;DR: 研究评估了六种表格合成数据生成器的性能，发现贝叶斯网络在统计相似性上表现最佳，而TVAE在预测任务中表现最好。SDV因其文档和易用性更受推荐。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据对机器学习模型至关重要，但获取真实高质量数据对小企业和初创公司具有挑战性。合成数据生成器提供了一种隐私保护且可扩展的解决方案。

Method: 使用UCI数据集模拟低数据环境，评估六种生成器在1:1和1:10输入输出比例下的统计相似性和预测效用。

Result: 统计相似性在两种场景下一致，但1:10比例的预测效用显著下降。贝叶斯网络在统计相似性上表现最佳，TVAE在1:10比例下预测任务表现最好。

Conclusion: SDV因其文档和易用性更受推荐，尽管两种库性能无显著差异。

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [143] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/abs/2506.17848)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [144] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/abs/2506.17859)
*Daniel Wurgaft,Ekdeep Singh Lubana,Core Francisco Park,Hidenori Tanaka,Gautam Reddy,Noah D. Goodman*

Main category: cs.LG

TL;DR: 论文通过贝叶斯框架统一了上下文学习（ICL）中的不同策略，提出了一种分层贝叶斯模型，解释了Transformer的训练和推理行为。


<details>
  <summary>Details</summary>
Motivation: 旨在解释为什么模型会学习不同的上下文学习策略，并统一现有发现。

Method: 采用认知科学中的理性分析视角，开发了一个分层贝叶斯框架，预测Transformer的训练行为。

Result: 框架几乎完美预测了Transformer的下一词预测行为，揭示了训练中策略的复杂性与损失之间的权衡。

Conclusion: 研究为上下文学习提供了基于策略损失与复杂性权衡的解释和预测框架。

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [145] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+ 是一个结合神经加法模型和保形预测的联邦学习框架，提供可解释性和可靠的预测不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架缺乏结合不确定性量化、可解释性和鲁棒性的综合解决方案。

Method: FedNAM+ 引入动态级别调整技术，利用基于梯度的敏感度图识别关键输入特征，支持像素级不确定性估计。

Result: 在 CT 扫描、MNIST 和 CIFAR 数据集上验证，预测准确率高（如 MNIST 仅损失 0.1%），并提供透明的不确定性度量。

Conclusion: FedNAM+ 是一个高效、可解释且鲁棒的框架，适用于联邦学习场景，提升预测建模的信任和透明度。

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [146] [Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions](https://arxiv.org/abs/2506.17880)
*Lingfang Hu,Ian A. Kash*

Main category: cs.LG

TL;DR: 论文探讨了在参数假设下间接引发统计属性的任务，研究了权重选择对目标属性估计的影响，并发现最优权重配置通常为零。


<details>
  <summary>Details</summary>
Motivation: 研究如何选择适当的评分规则以间接引发目标属性，填补了现有文献中对实际应用中评分规则选择的空白。

Method: 通过模拟研究和理论分析，探讨了权重选择对目标属性估计的影响，并建立了理论框架。

Result: 发现最优权重配置通常为零，且目标属性的估计随权重增加单调变化。理论框架解释了实验结果。

Conclusion: 在参数假设下，间接引发目标属性的最优权重配置通常为零，理论框架为高维情况提供了理解方法。

Abstract: People are commonly interested in predicting a statistical property of a
random event such as mean and variance. Proper scoring rules assess the quality
of predictions and require that the expected score gets uniquely maximized at
the precise prediction, in which case we call the score directly elicits the
property. Previous research work has widely studied the existence and the
characterization of proper scoring rules for different properties, but little
literature discusses the choice of proper scoring rules for applications at
hand. In this paper, we explore a novel task, the indirect elicitation of
properties with parametric assumptions, where the target property is a function
of several directly-elicitable sub-properties and the total score is a weighted
sum of proper scoring rules for each sub-property. Because of the restriction
to a parametric model class, different settings for the weights lead to
different constrained optimal solutions. Our goal is to figure out how the
choice of weights affects the estimation of the target property and which
choice is the best. We start it with simulation studies and observe an
interesting pattern: in most cases, the optimal estimation of the target
property changes monotonically with the increase of each weight, and the best
configuration of weights is often to set some weights as zero. To understand
how it happens, we first establish the elementary theoretical framework and
then provide deeper sufficient conditions for the case of two sub-properties
and of more sub-properties respectively. The theory on 2-D cases perfectly
interprets the experimental results. In higher-dimensional situations, we
especially study the linear cases and suggest that more complex settings can be
understood with locally mapping into linear situations or using linear
approximations when the true values of sub-properties are close enough to the
parametric space.

</details>


### [147] [TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs](https://arxiv.org/abs/2506.17894)
*Kiran Thorat,Amit Hasan,Caiwen Ding,Zhijie Shi*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的新框架，用于检测大规模芯片设计中的硬件木马（HT），通过模型量化提高效率，并在自定义数据集上取得了高精度和高召回率。


<details>
  <summary>Details</summary>
Motivation: 芯片制造中使用的第三方工具和设计增加了硬件木马的风险，现有GNN方法在大型设计上表现不佳，且缺乏高效的训练和推理过程。

Method: 生成大型设计的图嵌入，结合多种GNN模型，并采用模型量化技术以降低计算需求。

Result: 在自定义数据集上实现了98.66%的精度和92.30%的召回率。

Conclusion: 该框架在检测大规模芯片设计中的硬件木马方面表现出高效性和有效性。

Abstract: Chip manufacturing is a complex process, and to achieve a faster time to
market, an increasing number of untrusted third-party tools and designs from
around the world are being utilized. The use of these untrusted third party
intellectual properties (IPs) and tools increases the risk of adversaries
inserting hardware trojans (HTs). The covert nature of HTs poses significant
threats to cyberspace, potentially leading to severe consequences for national
security, the economy, and personal privacy. Many graph neural network
(GNN)-based HT detection methods have been proposed. However, they perform
poorly on larger designs because they rely on training with smaller designs.
Additionally, these methods do not explore different GNN models that are
well-suited for HT detection or provide efficient training and inference
processes. We propose a novel framework that generates graph embeddings for
large designs (e.g., RISC-V) and incorporates various GNN models tailored for
HT detection. Furthermore, our framework introduces domain-specific techniques
for efficient training and inference by implementing model quantization. Model
quantization reduces the precision of the weights, lowering the computational
requirements, enhancing processing speed without significantly affecting
detection accuracy. We evaluate our framework using a custom dataset, and our
results demonstrate a precision of 98.66% and a recall (true positive rate) of
92.30%, highlighting the effectiveness and efficiency of our approach in
detecting hardware trojans in large-scale chip designs

</details>


### [148] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/abs/2506.17919)
*Zhiyu Mou,Miao Xu,Wei Chen,Rongquan Bai,Chuan Yu,Jian Xu*

Main category: cs.LG

TL;DR: 论文提出了一种基于模型的强化学习自动竞价方法（MRLB），通过结合真实数据和模型生成数据来扩展状态覆盖范围，并设计了PE-MORL算法以提高模型可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习竞价方法（ORLB）受限于数据集的状态覆盖范围，而基于模拟的强化学习竞价（SRLB）存在模拟与现实的差距问题。MRLB旨在通过模型学习来弥补这一差距。

Method: 提出MRLB方法，学习环境模型并结合真实与模型生成数据训练策略。设计了PE-MORL算法，包括置换等变模型架构和悲观惩罚模型误差的鲁棒离线Q学习方法。

Result: 实验表明，PE-MORL在真实场景中优于现有自动竞价方法。

Conclusion: MRLB通过模型学习和PE-MORL算法有效解决了ORLB和SRLB的局限性，提升了自动竞价的性能。

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [149] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/abs/2506.17929)
*Shulun Chen,Wei Shao,Flora D. Salim,Hao Xue*

Main category: cs.LG

TL;DR: ASTER模型将时空预测与决策支持结合，通过RaST模块和Poda代理优化资源分配和干预策略。


<details>
  <summary>Details</summary>
Motivation: 解决现有时空预测与下游决策脱节的问题，提升应急响应等场景的资源分配效率。

Method: 提出ASTER框架，包含RaST模块（动态资源条件下的时空依赖建模）和Poda代理（基于多目标强化学习的决策生成）。

Result: 在四个基准数据集上，ASTER在预测准确性和资源分配效果上均达到最优。

Conclusion: ASTER通过直接支持决策，显著提升了时空智能的实用性和效率。

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [150] [An entropy-optimal path to humble AI](https://arxiv.org/abs/2506.17940)
*Davide Bassetti,Lukáš Pospíšil,Michael Groom,Terence J. O'Kane,Illia Horenko*

Main category: cs.LG

TL;DR: 论文提出了一种基于非平衡熵优化的Boltzmann机新数学框架，显著降低了成本和资源需求，同时提供了性能优越且更轻量的模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型成本高且过于自信，缺乏可靠性和资源效率。

Method: 基于总概率定律的非平衡熵优化Boltzmann机框架，无需梯度下降，具有数学证明的存在唯一性和答案置信度。

Result: 在合成问题和气候数据上验证，模型性能更优、更轻量，且训练数据需求大幅减少。

Conclusion: 该框架为高效、可靠的AI模型提供了新方向，尤其适用于资源受限场景。

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [151] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的世界模型评估方法UNIVERSE，用于解决现有评估指标在动作对齐和语义一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法满足对世界模型生成内容在动作对齐和语义一致性方面的细粒度、时间敏感的评估需求。

Method: 提出UNIVERSE方法，通过适应视觉语言模型（VLM）在数据和计算限制下进行评估，支持动作识别和角色识别的多格式任务。

Result: UNIVERSE在单一检查点下表现与任务专用基线相当，并通过人类研究验证了其与人类判断的高度一致性。

Conclusion: UNIVERSE是一种可扩展、语义感知的世界模型评估工具。

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [152] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为h-calibration的概率学习框架，用于解决深度神经网络输出概率的校准问题，克服了现有方法的十大局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在许多任务中表现优异，但其输出的概率往往不准确（miscalibration），导致不可靠。现有的事后校准方法虽能改善，但仍存在诸多局限性。

Method: 论文总结了现有方法为三类，并提出了h-calibration框架，通过理论构建等效学习目标，设计了一种简单有效的校准算法。

Result: h-calibration不仅克服了现有方法的十大局限性，还在实验中显著优于传统方法，达到了最先进的性能。

Conclusion: 该研究为学习可靠概率提供了理论框架和实用方法，对相关领域具有重要参考价值。

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [153] [Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm](https://arxiv.org/abs/2506.17974)
*Hongyang Li,Lincen Bai,Caesar Wu,Mohammed Chadli,Said Mammar,Pascal Bouvry*

Main category: cs.LG

TL;DR: LQ-SGD是一种高效的梯度压缩算法，结合低秩近似和对数量化技术，显著减少通信开销，同时保持训练速度和模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中通信开销大的问题，同时提升梯度压缩算法的效率和鲁棒性。

Method: 基于PowerSGD，引入低秩近似和对数量化技术，优化梯度压缩过程。

Result: LQ-SGD显著降低通信开销，保持收敛速度和模型精度，并对梯度反转攻击有更强抵抗力。

Conclusion: LQ-SGD为分布式学习系统提供了一种更高效、鲁棒的优化方法。

Abstract: We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an
efficient communication gradient compression algorithm designed for distributed
training. LQ-SGD further develops on the basis of PowerSGD by incorporating the
low-rank approximation and log-quantization techniques, which drastically
reduce the communication overhead, while still ensuring the convergence speed
of training and model accuracy. In addition, LQ-SGD and other compression-based
methods show stronger resistance to gradient inversion than traditional SGD,
providing a more robust and efficient optimization path for distributed
learning systems.

</details>


### [154] [Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings](https://arxiv.org/abs/2506.17989)
*Lucas Mattioli,Youness Ait Hadichou,Sabrina Chaouche,Martin Gonzalez*

Main category: cs.LG

TL;DR: 研究发现，未经筛选的文本嵌入（TE）会导致模型崩溃，表现为预测结果收敛到单一类别。TE质量显著影响下游学习，且可能产生虚假的准确性相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨未经筛选的文本嵌入如何导致模型崩溃，并评估TE质量对下游任务的影响。

Method: 比较相同超参数配置下，基于原始表格数据和TE数据的模型表现，提出衡量模型崩溃程度的指标。

Result: TE无法有效作为数据筛选层，其质量显著影响学习效果，且模型崩溃可能导致虚假的准确性相关性。

Conclusion: 需更细致地筛选和评估嵌入表示，尤其是在分布外场景中。

Abstract: Training models on uncurated Text Embeddings (TEs) derived from raw tabular
data can lead to a severe failure mode known as model collapse, where
predictions converge to a single class regardless of input. By comparing models
trained with identical hyper-parameter configurations on both raw tabular data
and their TE-derived counterparts, we find that collapse is a consistent
failure mode in the latter setting. We introduce a set of metrics that capture
the extent of model collapse, offering a new perspective on TE quality as a
proxy for data curation. Our results reveal that TE alone does not effectively
function as a curation layer - and that their quality significantly influences
downstream learning. More insidiously, we observe that the presence of model
collapse can yield artificially inflated and spurious Accuracy-on-the-Line
correlation. These findings highlight the need for more nuanced curation and
evaluation of embedding-based representations, particularly in
out-of-distribution settings.

</details>


### [155] [Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification](https://arxiv.org/abs/2506.18007)
*Sharon Torao Pingi,Md Abul Bashar,Richi Nayak*

Main category: cs.LG

TL;DR: 本文综述了生成对抗网络（GANs）在纵向数据插补（LDI）中的应用，探讨其是否满足纵向数据分类（LDC）的基本假设，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 纵向数据的复杂性和缺失值问题对LDC的准确性构成挑战，尽管GANs在LDI中显示出潜力，但仍需更全面的解决方案。

Method: 通过分类GAN-based LDI的主要方法，分析其优缺点，并总结研究趋势。

Result: GANs在LDI中表现出潜力，但需更灵活的方法应对纵向数据的广泛挑战。

Conclusion: 未来研究应开发更有效的GAN-based解决方案，以解决LDC中的挑战。

Abstract: Longitudinal data is commonly utilised across various domains, such as
health, biomedical, education and survey studies. This ubiquity has led to a
rise in statistical, machine and deep learning-based methods for Longitudinal
Data Classification (LDC). However, the intricate nature of the data,
characterised by its multi-dimensionality, causes instance-level heterogeneity
and temporal correlations that add to the complexity of longitudinal data
analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of
missing values in longitudinal data. Despite ongoing research that draw on the
generative power and utility of Generative Adversarial Networks (GANs) to
address the missing data problem, critical considerations include statistical
assumptions surrounding longitudinal data and missingness within it, as well as
other data-level challenges like class imbalance and mixed data types that
impact longitudinal data imputation (LDI) and the subsequent LDC process in
GANs. This paper provides a comprehensive overview of how GANs have been
applied in LDI, with a focus whether GANS have adequately addressed fundamental
assumptions about the data from a LDC perspective. We propose a categorisation
of main approaches to GAN-based LDI, highlight strengths and limitations of
methods, identify key research trends, and provide promising future directions.
Our findings indicate that while GANs show great potential for LDI to improve
usability and quality of longitudinal data for tasks like LDC, there is need
for more versatile approaches that can handle the wider spectrum of challenges
presented by longitudinal data with missing values. By synthesising current
knowledge and identifying critical research gaps, this survey aims to guide
future research efforts in developing more effective GAN-based solutions to
address LDC challenges.

</details>


### [156] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/abs/2506.18011)
*Eddie Conti,Alejandro Astruc,Alvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 研究通过最小化标记扰动对Transformer嵌入空间的影响，验证了浅层可作为模型解释的代理假设。


<details>
  <summary>Details</summary>
Motivation: 理解信息如何在Transformer模型中传播是解释性的关键挑战。

Method: 通过实验分析标记扰动对嵌入空间的影响，研究扰动在层间的传播。

Result: 罕见标记通常导致更大的嵌入偏移，深层信息逐渐混合。

Conclusion: 结合标记扰动和嵌入空间偏移是模型解释性的有力工具。

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [157] [Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning](https://arxiv.org/abs/2506.18020)
*Thomas Boudou,Batiste Le Bars,Nirupam Gupta,Aurélien Bellet*

Main category: cs.LG

TL;DR: 论文研究了分布式学习中两种威胁模型（拜占庭攻击和数据投毒攻击）对泛化性能的影响，首次从理论上证明拜占庭攻击对泛化的危害更大。


<details>
  <summary>Details</summary>
Motivation: 现有研究在优化误差上对两种威胁模型表现相似，但对泛化性能的影响尚不明确。本文旨在填补这一理论空白。

Method: 通过理论分析，比较两种威胁模型下鲁棒分布式学习算法的均匀算法稳定性。

Result: 数据投毒攻击下稳定性下降为Θ(f/(n-f))，而拜占庭攻击下为O(√(f/(n-2f)))，后者对泛化性能影响更大。

Conclusion: 拜占庭攻击对泛化性能的负面影响显著高于数据投毒攻击，尤其在f接近n/2时差距更明显。

Abstract: Robust distributed learning algorithms aim to maintain good performance in
distributed and federated settings, even in the presence of misbehaving
workers. Two primary threat models have been studied: Byzantine attacks, where
misbehaving workers can send arbitrarily corrupted updates, and data poisoning
attacks, where misbehavior is limited to manipulation of local training data.
While prior work has shown comparable optimization error under both threat
models, a fundamental question remains open: How do these threat models impact
generalization? Empirical evidence suggests a gap between the two threat
models, yet it remains unclear whether it is fundamental or merely an artifact
of suboptimal attacks. In this work, we present the first theoretical
investigation into this problem, formally showing that Byzantine attacks are
intrinsically more harmful to generalization than data poisoning. Specifically,
we prove that: (i) under data poisoning, the uniform algorithmic stability of a
robust distributed learning algorithm, with optimal optimization error,
degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the
number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine
attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}}
\big)$.This difference in stability leads to a generalization error gap that is
especially significant as $f$ approaches its maximum value $\frac{n}{2}$.

</details>


### [158] [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032)
*Abhay Sheshadri,John Hughes,Julian Michael,Alex Mallen,Arun Jose,Janus,Fabien Roger*

Main category: cs.LG

TL;DR: 研究发现，只有5个大型语言模型在训练时比部署时更可能遵守有害查询，其中Claude 3 Opus的行为最一致。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在训练和部署时对有害查询的响应差异，以及其背后的动机。

Method: 分析了25个模型的行为，重点研究了5个表现异常的模型，并通过扰动实验验证动机。

Result: 仅Claude 3 Opus的行为主要由目标一致性驱动；后训练可能抑制或增强对齐伪装。

Conclusion: 模型的对齐伪装行为差异可能与拒绝行为的变化有关。

Abstract: Alignment faking in large language models presented a demonstration of Claude
3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training
objective to prevent modification of their behavior outside of training. We
expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude
3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries
more when they infer they are in training than when they infer they are in
deployment. First, we study the motivations of these 5 models. Results from
perturbing details of the scenario suggest that only Claude 3 Opus's compliance
gap is primarily and consistently motivated by trying to keep its goals.
Second, we investigate why many chat models don't fake alignment. Our results
suggest this is not entirely due to a lack of capabilities: many base models
fake alignment some of the time, and post-training eliminates alignment-faking
for some models and amplifies it for others. We investigate 5 hypotheses for
how post-training may suppress alignment faking and find that variations in
refusal behavior may account for a significant portion of differences in
alignment faking.

</details>


### [159] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/abs/2506.18037)
*Seongwoo Lim,Won Jo,Joohyung Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: 提出了一种基于隐藏单元子集的路径解释方法，提高了神经网络决策的透明度和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络‘黑盒’性质导致的透明度和可靠性问题。

Method: 通过考虑决策路径中的隐藏单元子集，提供更清晰和一致的输入与决策关系解释。

Result: 实验证明该方法在定量和定性上优于其他方法。

Conclusion: 新方法为神经网络决策提供了更灵活和详细的解释能力。

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [160] [TAB: Unified Benchmarking of Time Series Anomaly Detection Methods](https://arxiv.org/abs/2506.18046)
*Xiangfei Qiu,Zhe Li,Wanghui Qiu,Shiyan Hu,Lekui Zhou,Xingjian Wu,Zhengyu Li,Chenjuan Guo,Aoying Zhou,Zhenli Sheng,Jilin Hu,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间序列异常检测基准TAB，用于更全面、公平地评估和比较现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测（TSAD）方法的评估存在数据集和实验设置的不足，需要更可靠的评估手段。

Method: TAB包含29个公共多元数据集和1,635个单变量时间序列，涵盖多种TSAD方法，并提供统一的自动化评估流程。

Result: 通过TAB评估现有TSAD方法，提供了对这些方法性能的深入洞察。

Conclusion: TAB为TSAD领域提供了更全面的评估工具，促进了方法的改进和比较。

Abstract: Time series anomaly detection (TSAD) plays an important role in many domains
such as finance, transportation, and healthcare. With the ongoing
instrumentation of reality, more time series data will be available, leading
also to growing demands for TSAD. While many TSAD methods already exist, new
and better methods are still desirable. However, effective progress hinges on
the availability of reliable means of evaluating new methods and comparing them
with existing methods. We address deficiencies in current evaluation procedures
related to datasets and experimental settings and protocols. Specifically, we
propose a new time series anomaly detection benchmark, called TAB. First, TAB
encompasses 29 public multivariate datasets and 1,635 univariate time series
from different domains to facilitate more comprehensive evaluations on diverse
datasets. Second, TAB covers a variety of TSAD methods, including Non-learning,
Machine learning, Deep learning, LLM-based, and Time-series pre-trained
methods. Third, TAB features a unified and automated evaluation pipeline that
enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to
evaluate existing TSAD methods and report on the outcomes, thereby offering a
deeper insight into the performance of these methods. Besides, all datasets and
code are available at https://github.com/decisionintelligence/TAB.

</details>


### [161] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/abs/2506.18074)
*Matteo Rufolo,Dario Piga,Marco Forgione*

Main category: cs.LG

TL;DR: 论文提出了一种基于分布鲁棒优化的元学习方法，用于系统辨识，以提升在安全关键应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 标准元学习方法忽略了任务变异性，无法应对高损失任务，而分布鲁棒优化能优先处理这些任务，提升最坏情况下的性能。

Method: 采用分布鲁棒优化范式，在元学习中优化高损失任务，应用于合成动态系统的元模型训练。

Result: 在分布内和分布外场景下测试，该方法减少了安全关键应用中的失败情况。

Conclusion: 分布鲁棒优化在元学习中能有效提升系统辨识的鲁棒性，适用于安全关键场景。

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [162] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/abs/2506.18110)
*Mohammad Hossein Amani,Aryo Lotfi,Nicolas Mario Baldwin,Samy Bengio,Mehrdad Farajtabar,Emmanuel Abbe,Robert West*

Main category: cs.LG

TL;DR: 论文提出了一种基于部分专家演示的强化学习框架（AdaBack），通过动态调整监督长度，解决长序列生成任务中监督微调和稀疏奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）依赖密集标签，成本高；强化学习（RL）则面临稀疏奖励和大输出空间的挑战。论文旨在探索介于SFT和RL之间的中间方案。

Method: 提出自适应回溯（AdaBack），一种基于样本的课程学习算法，动态调整监督长度，逐步学习完成推理链。

Result: 在合成任务和数学推理基准（MATH、GSM8k）上，AdaBack成功解决了传统方法无法处理的复杂问题。

Conclusion: 基于样本的课程学习不仅是一种效率与泛化的权衡，还能在长序列依赖任务中取得突破，为复杂推理任务提供了新思路。

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [163] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/abs/2506.18124)
*Shaoxiu Wei,Mingchao Liang,Florian Meyer*

Main category: cs.LG

TL;DR: 提出了一种结合模型驱动和数据驱动的多目标跟踪（MOT）混合方法，通过神经网络增强贝叶斯MOT中的统计模型，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型驱动方法通用性强，但性能有限；数据驱动方法在数据丰富时表现优异。如何结合两者优势成为研究目标。

Method: 利用神经网络优化贝叶斯MOT中的统计模型，结合置信传播和序列蒙特卡洛方法实现高效计算。

Result: 在nuScenes自动驾驶数据集上验证，表现优于现有方法。

Conclusion: 混合方法兼具模型驱动的灵活性和数据驱动的学习能力，性能达到最优。

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


### [164] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/abs/2506.18145)
*Zheng Zhan,Liliang Ren,Shuohang Wang,Liyuan Liu,Yang Liu,Yeyun Gong,Yanzhi Wang,Yelong Shen*

Main category: cs.LG

TL;DR: RoM是一种通过稀疏混合线性投影专家扩展SSM参数的新方法，显著提升了Mamba模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管SSM在长序列建模中表现出色，但如何高效扩展其表达能力（尤其是与MoE结合）仍具挑战性。

Method: RoM通过共享路由决策和轻量子模块，利用线性投影专家的协同作用，实现了Mamba层的稀疏扩展。

Result: 在1.3B活跃参数和16K序列长度下，RoM性能与密集Mamba模型相当，且节省了23%的FLOPS。

Conclusion: RoM为SSM的高效扩展提供了有效方案，适用于混合语言模型。

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [165] [Probabilistic and reinforced mining of association rules](https://arxiv.org/abs/2506.18155)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出了四种新颖的概率和强化学习方法（GPAR、BARM、MAB-ARM、RLAR）用于关联规则挖掘，相比传统频率算法更具灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统频率驱动的关联规则挖掘方法（如Apriori）在建模不确定性、先验知识和适应性搜索方面存在局限，需要更先进的框架。

Method: 四种方法分别基于高斯过程（GPAR）、贝叶斯框架（BARM）、多臂老虎机（MAB-ARM）和强化学习（RLAR），支持不确定性量化、自适应探索和高效推理。

Result: 实验证明这些方法在合成和真实数据集上有效，尤其适用于发现稀有或复杂模式及小数据集。

Conclusion: 这些方法标志着从静态频率驱动范式的重大转变，为零售、医疗等领域提供了更灵活、鲁棒的关联规则挖掘框架。

Abstract: This work introduces 4 novel probabilistic and reinforcement-driven methods
for association rule mining (ARM): Gaussian process-based association rule
mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and
reinforcement learning based association rule mining (RLAR). These methods
depart fundamentally from traditional frequency-based algorithms such as
Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating
prior knowledge, modeling uncertainty, item dependencies, probabilistic
inference and adaptive search strategies. GPAR employs Gaussian processes to
model item co-occurrence via feature representations, enabling principled
inference, uncertainty quantification, and efficient generalization to unseen
itemsets without retraining. BARM adopts a Bayesian framework with priors and
optional correlation structures, yielding robust uncertainty quantification
through full posterior distributions over item presence probabilities. MAB-ARM,
including its Monte Carlo tree search (MCTS) companion, utilizes an upper
confidence bound (UCB) strategy for efficient and adaptive exploration of the
itemset space, while RLAR applies a deep Q-network (DQN) to learn a
generalizable policy for identifying high-quality rules. Collectively, these
approaches improve the flexibility and robustness of ARM, particularly for
discovering rare or complex patterns and operating on small datasets. Empirical
results on synthetic and real-world datasets demonstrate their effectiveness,
while also highlighting trade-offs in computational complexity and
interpretability. These innovations mark a significant shift from static,
frequency-driven paradigms, offering some prior and dependency-informed,
uncertainty-aware or scalable ARM frameworks for diverse application domains
such as retail, geography, finance, medical diagnostics, and risk-sensitive
scenarios.

</details>


### [166] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: 本文探讨了在医学分类任务中应用保形预测的局限性，特别是在输入和标签变量分布变化时不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究保形预测在医学分类任务中的可靠性，揭示其在实际应用中的潜在问题。

Method: 通过皮肤病学和组织病理学的案例，分析保形预测在分布变化下的表现。

Result: 保形预测在分布变化下不可靠，不适用于提高准确性或用于数据子集（如特定类别或患者属性）。

Conclusion: 在医学图像分类任务中，保形预测的实际价值有限，需谨慎使用。

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [167] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/abs/2506.18165)
*Jaemoo Choi,Yongxin Chen,Molei Tao,Guan-Horng Liu*

Main category: cs.LG

TL;DR: 提出了一种基于随机最优控制（SOC）的新型扩散采样器NAAS，避免了重要性采样的高方差问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于退火的采样方法依赖重要性采样，导致高方差和可扩展性受限，需要一种更高效的替代方案。

Method: NAAS采用退火参考动态和轻量级伴随系统，避免重要性采样，实现高效训练。

Result: 在经典能量景观和分子玻尔兹曼分布等任务中表现出色。

Conclusion: NAAS为扩散采样提供了一种高效且可扩展的新方法。

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [168] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出了一种通过分析和操纵DeepSeek-R1-Distill模型的特定推理行为来引导思考型大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管思考型大语言模型通过生成内部推理链提升了性能，但其推理过程难以控制。

Method: 通过分析模型激活空间中的线性方向，提取并应用控制向量，以调节模型的特定推理行为（如回溯或表达不确定性）。

Result: 在500个任务上的实验表明，该方法能有效控制推理行为，并在不同模型架构中表现一致。

Conclusion: 该方法为思考型模型提供了可控且可解释的推理过程调控工具。

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [169] [Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba](https://arxiv.org/abs/2506.18184)
*Donghyun Lee,Yuhang Li,Ruokai Yin,Shiting Xiao,Priyadarshini Panda*

Main category: cs.LG

TL;DR: Memba是一种专为Mamba设计的参数高效微调方法，通过生物启发的LIM神经元增强时间建模能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着Mamba等状态空间模型规模增大，传统Transformer的PEFT方法无法适应其独特的时间处理动态，需专门设计高效微调方法。

Method: 提出Memba，结合LIM神经元（生物启发的门控机制）、LoRA和跨层膜传递，优化Mamba的时间建模能力。

Result: 在语言和视觉任务中，Memba显著优于现有PEFT方法。

Conclusion: Memba为Mamba提供了高效的时间建模解决方案，代码已开源。

Abstract: State Space Models (SSMs) have emerged as powerful alternatives to
attention-based Transformers, with Mamba demonstrating impressive efficiency
and scalability. As these models grow increasingly larger, the need for
Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt
pre-trained Mamba to downstream tasks without prohibitive computational costs.
However, previous approaches simply apply traditional Transformer-tailored PEFT
methods without addressing the unique temporal processing dynamics of SSMs. To
address this limitation, we propose Memba, a membrane-driven PEFT approach
specifically designed for Mamba. Memba introduces Leaky Integrate Membrane
(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate
membrane potentials over time, enhancing selective information retention. By
strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and
cross-layer membrane transfer, our approach significantly improves Mamba's
temporal modeling capabilities. Extensive experiments across language and
vision tasks demonstrate that Memba achieves substantial improvements over
existing PEFT methods. The code is available at
https://github.com/Intelligent-Computing-Lab-Yale/Memba.

</details>


### [170] [Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels](https://arxiv.org/abs/2506.18186)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 提出了一种在线学习算法，用于在未知、非平稳环境中计算Whittle指数，通过滑动窗口和置信上界优化动态遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决RMABs在未知、非平稳环境中的资源分配问题，传统Whittle指数方法因需要已知转移核而受限。

Method: 基于滑动窗口和置信上界的线性优化预测转移核，计算Whittle指数，利用领域知识加速学习。

Result: 算法在非平稳环境中表现优于基线，动态遗憾随T次线性增长。

Conclusion: 算法有效解决了非平稳环境中的RMABs问题，性能优越。

Abstract: We consider optimal resource allocation for restless multi-armed bandits
(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve
optimally, even when all parameters are known. The Whittle index policy is
known to achieve asymptotic optimality for a large class of such problems,
while remaining computationally efficient. In many practical settings, however,
the transition kernels required to compute the Whittle index are unknown and
non-stationary. In this work, we propose an online learning algorithm for
Whittle indices in this setting. Our algorithm first predicts current
transition kernels by solving a linear optimization problem based on upper
confidence bounds and empirical transition probabilities calculated from data
over a sliding window. Then, it computes the Whittle index associated with the
predicted transition kernels. We design these sliding windows and upper
confidence bounds to guarantee sub-linear dynamic regret on the number of
episodes $T$, under the condition that transition kernels change slowly over
time (rate upper bounded by $\epsilon=1/T^k$ with $k>0$). Furthermore, our
proposed algorithm and regret analysis are designed to exploit prior domain
knowledge and structural information of the RMABs to accelerate the learning
process. Numerical results validate that our algorithm achieves superior
performance in terms of lowest cumulative regret relative to baselines in
non-stationary environments.

</details>


### [171] [Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs](https://arxiv.org/abs/2506.18194)
*Francesco Picolli,Gabriel Vogel,Jana M. Weber*

Main category: cs.LG

TL;DR: 论文探讨了自监督学习（SSL）架构JEPA在聚合物分子图上的应用，证明其在标记数据稀缺时能提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习在聚合物发现中的应用受限于高质量标记数据的稀缺性，研究旨在探索自监督学习是否能缓解这一问题。

Method: 采用JEPA架构对聚合物分子图进行自监督预训练，并在标记数据稀缺的情况下评估其下游性能。

Result: 实验表明，JEPA预训练显著提升了标记数据稀缺时的下游任务性能，在所有测试数据集中均表现优异。

Conclusion: JEPA自监督预训练是解决聚合物机器学习中标记数据稀缺问题的有效方法。

Abstract: Recent advances in machine learning (ML) have shown promise in accelerating
the discovery of polymers with desired properties by aiding in tasks such as
virtual screening via property prediction. However, progress in polymer ML is
hampered by the scarcity of high-quality labeled datasets, which are necessary
for training supervised ML models. In this work, we study the use of the very
recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture
for self-supervised learning (SSL), on polymer molecular graphs to understand
whether pretraining with the proposed SSL strategy improves downstream
performance when labeled data is scarce. Our results indicate that JEPA-based
self-supervised pretraining on polymer graphs enhances downstream performance,
particularly when labeled data is very scarce, achieving improvements across
all tested datasets.

</details>


### [172] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: 论文探讨了迁移学习中的信息饱和瓶颈问题，提出现有大规模网络可能不如任务特定训练有效，并建议通过更丰富的特征表示来改进。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中因特征竞争导致的信息饱和瓶颈问题，以及量化任务相关性的困难。

Method: 评估从预训练混合数据到其组成任务的模型迁移性能，分析特征学习限制。

Result: 发现深度学习模型存在信息饱和瓶颈，导致特征学习受限，迁移性能不稳定。

Conclusion: 建议结合任务特定训练和更丰富的特征表示，以提升迁移学习的泛化能力。

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [173] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: AdapThink是一种自适应后训练框架，通过动态调整奖励函数和多样性采样机制，提升语言模型的推理效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练方法在推理效率上存在不足，无法适应不同问题复杂度和模型能力的变化。

Method: AdapThink结合了基于模型置信度的动态奖励函数和熵引导的多样性采样机制。

Result: 在多个数学推理数据集上，AdapThink显著提升了推理效率，减少了计算浪费。

Conclusion: AdapThink通过自适应机制有效解决了推理效率问题，为语言模型的复杂推理提供了更优方案。

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [174] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/abs/2506.18240)
*Wenxin Li,Chuan Wang,Hongdong Zhu,Qi Gao,Yin Ma,Hai Wei,Kai Wen*

Main category: cs.LG

TL;DR: 提出了一种基于二次二进制优化（QBO）的量化神经网络训练模型，通过样条插值支持任意激活和损失函数，并引入前向区间传播（FIP）方法处理非线性问题。采用量子条件梯度下降（QCGD）算法解决大规模约束问题，实验在Fashion MNIST上达到94.95%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中非线性激活函数和多层复合结构的优化问题，扩展量子计算机在人工智能中的应用。

Method: 提出FIP方法离散化激活函数为线性子区间，并采用QCGD算法直接求解QCBO问题。

Result: 理论证明了近似误差和所需伊辛自旋数量的上界，实验在1.1位精度下达到94.95%分类准确率。

Conclusion: 该方法在保持神经网络通用逼近能力的同时，为量子计算在复杂非线性优化中的应用提供了新途径。

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [175] [Dual-Forward Path Teacher Knowledge Distillation: Bridging the Capacity Gap Between Teacher and Student](https://arxiv.org/abs/2506.18244)
*Tong Li,Long Liu,Yihang Hu,Hu Chen,Shifeng Chen*

Main category: cs.LG

TL;DR: 论文提出DFPT-KD和DFPT-KD+方法，通过双前向路径教师和提示调优解决知识蒸馏中的容量差距问题，提升学生网络性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法因教师与学生网络容量差距大，限制了性能提升，需动态调整知识传递。

Method: 引入双前向路径教师，通过提示调优优化知识传递，并进一步微调路径（DFPT-KD+）。

Result: 实验表明DFPT-KD和DFPT-KD+优于传统方法，后者达到最先进精度。

Conclusion: DFPT-KD系列方法有效解决容量差距问题，显著提升学生网络性能。

Abstract: Knowledge distillation (KD) provides an effective way to improve the
performance of a student network under the guidance of pre-trained teachers.
However, this approach usually brings in a large capacity gap between teacher
and student networks, limiting the distillation gains. Previous methods
addressing this problem either discard accurate knowledge representation or
fail to dynamically adjust the transferred knowledge, which is less effective
in addressing the capacity gap problem and hinders students from achieving
comparable performance with the pre-trained teacher. In this work, we extend
the ideology of prompt-based learning to address the capacity gap problem, and
propose Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), which
replaces the pre-trained teacher with a novel dual-forward path teacher to
supervise the learning of student. The key to DFPT-KD is prompt-based tuning,
i.e., establishing an additional prompt-based forward path within the
pre-trained teacher and optimizing it with the pre-trained teacher frozen to
make the transferred knowledge compatible with the representation ability of
the student. Extensive experiments demonstrate that DFPT-KD leads to trained
students performing better than the vanilla KD. To make the transferred
knowledge better compatible with the representation abilities of the student,
we further fine-tune the whole prompt-based forward path, yielding a novel
distillation approach dubbed DFPT-KD+. By extensive experiments, it is shown
that DFPT-KD+ improves upon DFPT-KD and achieves state-of-the-art accuracy
performance.

</details>


### [176] [Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures](https://arxiv.org/abs/2506.18247)
*Manaswin Oddiraju,Bharath Varma Penumatsa,Divyang Amin,Michael Piedmonte,Souma Chowdhury*

Main category: cs.LG

TL;DR: 论文探讨了如何通过贝叶斯神经网络（BNN）增强物理信息机器学习（PIML）架构的不确定性传播能力，并通过两阶段训练过程优化性能。


<details>
  <summary>Details</summary>
Motivation: 量化并传播建模不确定性对于工程设计和控制中的可靠性分析和鲁棒优化至关重要，但PIML方法在此方面的能力尚未充分探索。

Method: 提出了一种结合部分物理模型和贝叶斯神经网络的混合PIML架构，采用两阶段训练过程，并通过蒙特卡洛采样评估不确定性传播效果。

Result: 在基准问题和飞行实验数据上的测试表明，BNN集成的PIML架构性能略逊或与纯数据驱动ML及原始PIML模型相当，但蒙特卡洛采样在不确定性传播中表现最佳。

Conclusion: BNN可以成功增强PIML架构的不确定性传播能力，蒙特卡洛采样是有效的传播方法，但性能仍有提升空间。

Abstract: Quantifying and propagating modeling uncertainties is crucial for reliability
analysis, robust optimization, and other model-based algorithmic processes in
engineering design and control. Now, physics-informed machine learning (PIML)
methods have emerged in recent years as a new alternative to traditional
computational modeling and surrogate modeling methods, offering a balance
between computing efficiency, modeling accuracy, and interpretability. However,
their ability to predict and propagate modeling uncertainties remains mostly
unexplored. In this paper, a promising class of auto-differentiable hybrid PIML
architectures that combine partial physics and neural networks or ANNs (for
input transformation or adaptive parameter estimation) is integrated with
Bayesian Neural networks (replacing the ANNs); this is done with the goal to
explore whether BNNs can successfully provision uncertainty propagation
capabilities in the PIML architectures as well, further supported by the
auto-differentiability of these architectures. A two-stage training process is
used to alleviate the challenges traditionally encountered in training
probabilistic ML models. The resulting BNN-integrated PIML architecture is
evaluated on an analytical benchmark problem and flight experiments data for a
fixed-wing RC aircraft, with prediction performance observed to be slightly
worse or at par with purely data-driven ML and original PIML models. Moreover,
Monte Carlo sampling of probabilistic BNN weights was found to be most
effective in propagating uncertainty in the BNN-integrated PIML architectures.

</details>


### [177] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Main category: cs.LG

TL;DR: RLPR是一种无需验证器的框架，利用LLM自身生成答案的概率作为奖励信号，显著提升了通用领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖领域特定验证器，导致复杂性和可扩展性受限，限制了其在通用领域的应用。

Method: RLPR通过LLM生成答案的token概率作为奖励信号，并采用prob-to-reward和稳定化方法降低噪声方差。

Result: 在四个通用领域和三个数学基准测试中，RLPR显著提升了Gemma、Llama和Qwen模型的推理能力，优于VeriFree和General-Reasoner。

Conclusion: RLPR通过利用LLM自身概率作为奖励信号，成功扩展了RLVR的应用范围，为通用领域推理提供了高效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [178] [Ground tracking for improved landmine detection in a GPR system](https://arxiv.org/abs/2506.18258)
*Li Tang,Peter A. Torrione,Cihat Eldeniz,Leslie M. Collins*

Main category: cs.LG

TL;DR: 论文提出基于卡尔曼滤波和粒子滤波的地面反弹干扰抑制方法，提升探地雷达对低金属含量地雷的检测性能。


<details>
  <summary>Details</summary>
Motivation: 地面反弹干扰是探地雷达数据中的主要干扰源，影响地雷检测性能，需有效抑制。

Method: 采用卡尔曼滤波和粒子滤波框架，将地面反弹位置建模为隐藏状态，通过自适应参数更新和平滑传播实现干扰跟踪。

Result: 实验验证了所提算法的有效性，性能优于其他地面反弹跟踪方法，提升了地雷检测效果。

Conclusion: 改进的地面反弹跟踪方法显著提升了探地雷达在地雷检测中的性能。

Abstract: Ground penetrating radar (GPR) provides a promising technology for accurate
subsurface object detection. In particular, it has shown promise for detecting
landmines with low metal content. However, the ground bounce (GB) that is
present in GPR data, which is caused by the dielectric discontinuity between
soil and air, is a major source of interference and degrades landmine detection
performance. To mitigate this interference, GB tracking algorithms formulated
using both a Kalman filter (KF) and a particle filter (PF) framework are
proposed. In particular, the location of the GB in the radar signal is modeled
as the hidden state in a stochastic system for the PF approach. The
observations are the 2D radar images, which arrive scan by scan along the
down-track direction. An initial training stage sets parameters automatically
to accommodate different ground and weather conditions. The features associated
with the GB description are updated adaptively with the arrival of new data.
The prior distribution for a given location is predicted by propagating
information from two adjacent channels/scans, which ensures that the overall GB
surface remains smooth. The proposed algorithms are verified in experiments
utilizing real data, and their performances are compared with other GB tracking
approaches. We demonstrate that improved GB tracking contributes to improved
performance for the landmine detection problem.

</details>


### [179] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/abs/2506.18267)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: ARD-LoRA是一种动态调整LoRA秩分配的新框架，通过可学习缩放因子实现每头秩自适应，显著提升参数效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法使用固定秩，无法适应不同层和注意力头的异质性学习动态。

Method: 提出ARD-LoRA框架，通过可学习缩放因子和元目标优化（结合ℓ1稀疏性和总变差正则化）实现动态秩分配。

Result: 在LLAMA-3.1-70B和PaliGemma-2上，ARD-LoRA仅用0.32%可训练参数即达到99.3%全微调性能，并减少41%多模态适应内存。

Conclusion: 动态细粒度秩分配是高效基础模型适应的关键范式。

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [180] [Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models](https://arxiv.org/abs/2506.18271)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: 提出了一种记忆增强架构，用于解决大语言模型在长对话中上下文记忆不足的问题，显著提升了交互的连贯性和响应质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中因上下文记忆有限，导致交互碎片化和响应相关性下降，影响用户体验。

Method: 采用动态检索、更新和修剪历史交互信息的记忆增强架构。

Result: 实验表明，该方案显著提升了上下文连贯性，减少了内存开销，并提高了响应质量。

Conclusion: 该架构在实时交互系统中具有应用潜力。

Abstract: Large Language Models face significant challenges in maintaining coherent
interactions over extended dialogues due to their limited contextual memory.
This limitation often leads to fragmented exchanges and reduced relevance in
responses, diminishing user experience. To address these issues, we propose a
memory-augmented architecture that dynamically retrieves, updates, and prunes
relevant information from past interactions, ensuring effective long-term
context handling. Experimental results demonstrate that our solution
significantly improves contextual coherence, reduces memory overhead, and
enhances response quality, showcasing its potential for real-time applications
in interactive systems.

</details>


### [181] [Leveraging Large Language Models for Information Verification -- an Engineering Approach](https://arxiv.org/abs/2506.18274)
*Nguyen Nang Hung,Nguyen Thanh Trong,Vuong Thanh Toan,Nguyen An Phuoc,Dao Minh Tu,Nguyen Manh Duc Tuan,Nguyen Dinh Mau*

Main category: cs.LG

TL;DR: 提出了一种基于GPT-4o的多媒体新闻来源验证方法，通过自动化流程处理图像、视频和音频数据。


<details>
  <summary>Details</summary>
Motivation: 解决多媒体新闻来源验证的自动化需求，减少人工干预。

Method: 使用Google工具生成元数据，分割和清理多媒体数据，提取关键帧并与元数据交叉验证，同时提取音频文本。

Result: 实现了全自动化验证流程，仅需人工最终确认。

Conclusion: 该方法高效且实用，适用于大规模多媒体新闻验证。

Abstract: For the ACMMM25 challenge, we present a practical engineering approach to
multimedia news source verification, utilizing Large Language Models (LLMs)
like GPT-4o as the backbone of our pipeline. Our method processes images and
videos through a streamlined sequence of steps: First, we generate metadata
using general-purpose queries via Google tools, capturing relevant content and
links. Multimedia data is then segmented, cleaned, and converted into frames,
from which we select the top-K most informative frames. These frames are
cross-referenced with metadata to identify consensus or discrepancies.
Additionally, audio transcripts are extracted for further verification.
Noticeably, the entire pipeline is automated using GPT-4o through prompt
engineering, with human intervention limited to final validation.

</details>


### [182] [Learning Causal Graphs at Scale: A Foundation Model Approach](https://arxiv.org/abs/2506.18285)
*Naiyu Yin,Tian Gao,Yue Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于注意力机制的新架构ADAG，用于多任务学习线性结构方程模型（SEMs），通过预训练共享低维先验，提高小样本下DAG学习的准确性和零样本推理效率。


<details>
  <summary>Details</summary>
Motivation: DAG学习在计算成本和可识别性方面存在挑战，尤其是在小样本情况下。

Method: 提出ADAG架构，利用注意力机制和非线性核学习数据到图结构和参数的映射，通过多任务连续优化捕获共享结构特性。

Result: 在合成数据集上，ADAG显著提高了DAG学习准确性和零样本推理效率。

Conclusion: ADAG是首个专门为DAG学习设计的预训练基础模型，为因果发现提供了更高效和通用的解决方案。

Abstract: Due to its human-interpretability and invariance properties, Directed Acyclic
Graph (DAG) has been a foundational tool across various areas of AI research,
leading to significant advancements. However, DAG learning remains highly
challenging, due to its super-exponential growth in computational cost and
identifiability issues, particularly in small-sample regimes. To address these
two challenges, in this work we leverage the recent success of linear
transformers and develop a foundation model approach for discovering multiple
order-consistent DAGs across tasks. In particular, we propose Attention-DAG
(ADAG), a novel attention-mechanism-based architecture for learning multiple
linear Structural Equation Models (SEMs). ADAG learns the mapping from observed
data to both graph structure and parameters via a nonlinear attention-based
kernel, enabling efficient multi-task estimation of the underlying linear SEMs.
By formulating the learning process across multiple tasks as a continuous
optimization problem, the pre-trained ADAG model captures the common structural
properties as a shared low-dimensional prior, thereby reducing the
ill-posedness of downstream DAG learning tasks in small-sample regimes. We
evaluate our proposed approach on benchmark synthetic datasets and find that
ADAG achieves substantial improvements in both DAG learning accuracy and
zero-shot inference efficiency. To the best of our knowledge, this is the first
practical approach for pre-training a foundation model specifically designed
for DAG learning, representing a step toward more efficient and generalizable
down-stream applications in causal discovery.

</details>


### [183] [Learning High-Quality Latent Representations for Anomaly Detection and Signal Integrity Enhancement in High-Speed Signals](https://arxiv.org/abs/2506.18288)
*Muhammad Usama,Hee-Deok Jang,Soham Shanbhag,Yoo-Chang Sung,Seung-Jun Bae,Dong Eui Chang*

Main category: cs.LG

TL;DR: 提出了一种联合训练框架，结合自动编码器和分类器，用于改进高速动态随机存取存储器信号的异常检测和信号完整性。


<details>
  <summary>Details</summary>
Motivation: 解决高速动态随机存取存储器信号中异常检测和信号完整性的双重挑战。

Method: 提出了一种联合训练框架，集成自动编码器和分类器，专注于有效数据特征以学习更显著的潜在表示。

Result: 在三种异常检测算法中表现优于两种基线方法，信号完整性平均提升11.3%。

Conclusion: 该方法在异常检测和信号完整性方面均表现出显著改进，源代码和数据已公开。

Abstract: This paper addresses the dual challenge of improving anomaly detection and
signal integrity in high-speed dynamic random access memory signals. To achieve
this, we propose a joint training framework that integrates an autoencoder with
a classifier to learn more distinctive latent representations by focusing on
valid data features. Our approach is evaluated across three anomaly detection
algorithms and consistently outperforms two baseline methods. Detailed ablation
studies further support these findings. Furthermore, we introduce a signal
integrity enhancement algorithm that improves signal integrity by an average of
11.3%. The source code and data used in this study are available at
https://github.com/Usama1002/learning-latent-representations.

</details>


### [184] [Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction](https://arxiv.org/abs/2506.18290)
*Han Zhang,Jinghong Mao,Shangwen Zhu,Zhantao Yang,Lianghua Huang,Yu Liu,Deli Zhao,Ruili Feng,Fan Cheng*

Main category: cs.LG

TL;DR: 扩散重建在图像编辑、修复和风格转换中很重要，但实践中存在明显的重建误差，研究发现PF-ODE生成过程中的不稳定性是主要原因。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解释扩散重建中观察到的误差，并揭示PF-ODE生成过程中的不稳定性。

Method: 通过数值实验和理论分析，验证不稳定性及其对重建误差的放大作用。

Result: 实验和理论证明，随着数据维度增加，不稳定性的概率趋近于1。

Conclusion: 研究揭示了扩散重建的固有挑战，为未来改进提供了方向。

Abstract: Diffusion reconstruction plays a critical role in various applications such
as image editing, restoration, and style transfer. In theory, the
reconstruction should be simple - it just inverts and regenerates images by
numerically solving the Probability Flow-Ordinary Differential Equation
(PF-ODE). Yet in practice, noticeable reconstruction errors have been observed,
which cannot be well explained by numerical errors. In this work, we identify a
deeper intrinsic property in the PF-ODE generation process, the instability,
that can further amplify the reconstruction errors. The root of this
instability lies in the sparsity inherent in the generation distribution, which
means that the probability is concentrated on scattered and small regions while
the vast majority remains almost empty. To demonstrate the existence of
instability and its amplification on reconstruction error, we conduct
experiments on both toy numerical examples and popular open-sourced diffusion
models. Furthermore, based on the characteristics of image data, we
theoretically prove that the instability's probability converges to one as the
data dimensionality increases. Our findings highlight the inherent challenges
in diffusion-based reconstruction and can offer insights for future
improvements.

</details>


### [185] [GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing](https://arxiv.org/abs/2506.18295)
*Kejia Bian,Meixia Tao,Shu Sun,Jun Yu*

Main category: cs.LG

TL;DR: GeNeRT是一种可泛化的神经射线追踪框架，通过改进空间迁移性和电磁定律遵循性，提高了建模的准确性、效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经RT方法存在泛化能力受限和电磁定律遵循性弱的问题，需要一种更通用的解决方案。

Method: GeNeRT结合了Fresnel启发的神经网络设计和GPU张量化加速策略，支持场景内和场景间的零样本泛化。

Result: 实验表明，GeNeRT在未训练区域和全新环境中均表现出色，MPC预测准确性优于基线，且运行效率高于Wireless Insite。

Conclusion: GeNeRT通过改进网络架构和训练策略，有效捕捉了射线-表面相互作用的物理原理，实现了更高的泛化性和准确性。

Abstract: Neural ray tracing (RT) has emerged as a promising paradigm for channel
modeling by combining physical propagation principles with neural networks. It
enables high modeling accuracy and efficiency. However, current neural RT
methods face two key limitations: constrained generalization capability due to
strong spatial dependence, and weak adherence to electromagnetic laws. In this
paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced
generalization, accuracy and efficiency. GeNeRT supports both intra-scenario
spatial transferability and inter-scenario zero-shot generalization. By
incorporating Fresnel-inspired neural network design, it also achieves higher
accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized
acceleration strategy is introduced to improve runtime efficiency. Extensive
experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes
well across untrained regions within a scenario and entirely unseen
environments, and achieves superior accuracy in MPC prediction compared to
baselines. Moreover, it outperforms Wireless Insite in runtime efficiency,
particularly in multi-transmitter settings. Ablation experiments validate the
effectiveness of the network architecture and training strategy in capturing
physical principles of ray-surface interactions.

</details>


### [186] [Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies](https://arxiv.org/abs/2506.18304)
*Junchao Fan,Xuyang Lei,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出了一种自适应专家引导的对抗攻击方法，解决了现有方法在攻击效率和训练稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法依赖高频攻击且效率低，同时限制攻击频率会导致训练不稳定。

Method: 通过模仿学习从成功攻击中提取专家策略，并利用KL散度正则化引导DRL对抗策略，结合性能感知退火策略逐步减少对专家的依赖。

Result: 实验表明，该方法在碰撞率、攻击效率和训练稳定性上优于现有方法，尤其在专家策略不理想时表现更佳。

Conclusion: 该方法显著提升了对抗攻击的效率和稳定性，为自动驾驶系统的安全性提供了新思路。

Abstract: Deep reinforcement learning (DRL) has emerged as a promising paradigm for
autonomous driving. However, despite their advanced capabilities, DRL-based
policies remain highly vulnerable to adversarial attacks, posing serious safety
risks in real-world deployments. Investigating such attacks is crucial for
revealing policy vulnerabilities and guiding the development of more robust
autonomous systems. While prior attack methods have made notable progress, they
still face several challenges: 1) they often rely on high-frequency attacks,
yet critical attack opportunities are typically context-dependent and
temporally sparse, resulting in inefficient attack patterns; 2) restricting
attack frequency can improve efficiency but often results in unstable training
due to the adversary's limited exploration. To address these challenges, we
propose an adaptive expert-guided adversarial attack method that enhances both
the stability and efficiency of attack policy training. Our method first
derives an expert policy from successful attack demonstrations using imitation
learning, strengthened by an ensemble Mixture-of-Experts architecture for
robust generalization across scenarios. This expert policy then guides a
DRL-based adversary through a KL-divergence regularization term. Due to the
diversity of scenarios, expert policies may be imperfect. To address this, we
further introduce a performance-aware annealing strategy that gradually reduces
reliance on the expert as the adversary improves. Extensive experiments
demonstrate that our method achieves outperforms existing approaches in terms
of collision rate, attack efficiency, and training stability, especially in
cases where the expert policy is sub-optimal.

</details>


### [187] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
*Lixin Wu,Na Cai,Qiao Cheng,Jiachen Wang,Yitao Duan*

Main category: cs.LG

TL;DR: Confucius3-Math是一个开源的14B参数大语言模型，专为数学推理任务设计，能在消费级GPU上高效运行，并在多项任务中超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术提升教育质量，特别是为中国K-12学生和教师提供数学学习支持。

Method: 采用大规模强化学习（RL）后训练，结合三项技术创新：目标熵正则化、近期样本恢复和策略特定难度加权。

Result: 模型在数学推理任务中表现优异，成本低且符合中国国家课程要求。

Conclusion: 研究表明，低成本构建特定领域的强大推理模型是可行的，模型和代码已开源。

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [188] [Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics](https://arxiv.org/abs/2506.18339)
*Wei Liu,Kiran Bacsa,Loon Ching Tang,Eleni Chatzi*

Main category: cs.LG

TL;DR: SKANODE结合Kolmogorov-Arnold网络与结构化Neural ODE，提出了一种高精度且物理可解释的非线性动力学建模方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在建模非线性动力学系统时难以同时实现高精度和物理可解释性的挑战。

Method: 使用可训练的KAN作为通用函数逼近器，结合结构化Neural ODE进行虚拟感知，提取物理可解释的潜在状态，并通过符号回归获取系统动力学方程。

Result: 在模拟和真实系统中，SKANODE表现出优越性能，并提供了解释性强、符合物理规律的模型。

Conclusion: SKANODE为非线性动力学系统的建模提供了一种高精度且可解释的解决方案。

Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental
problem across scientific and engineering domains. While deep learning has
demonstrated remarkable potential for learning complex system behavior,
achieving models that are both highly accurate and physically interpretable
remains a major challenge. To address this, we propose Structured
Kolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates
structured state-space modeling with the Kolmogorov-Arnold Network (KAN).
SKANODE first employs a fully trainable KAN as a universal function
approximator within a structured Neural ODE framework to perform virtual
sensing, recovering latent states that correspond to physically interpretable
quantities such as positions and velocities. Once this structured latent
representation is established, we exploit the symbolic regression capability of
KAN to extract compact and interpretable expressions for the system's governing
dynamics. The resulting symbolic expression is then substituted back into the
Neural ODE framework and further calibrated through continued training to
refine its coefficients, enhancing both the precision of the discovered
equations and the predictive accuracy of system responses. Extensive
experiments on both simulated and real-world systems demonstrate that SKANODE
achieves superior performance while offering interpretable, physics-consistent
models that uncover the underlying mechanisms of nonlinear dynamical systems.

</details>


### [189] [Controlled Generation with Equivariant Variational Flow Matching](https://arxiv.org/abs/2506.18340)
*Floor Eijkelboom,Heiko Zimmermann,Sharvaree Vadgama,Erik J Bekkers,Max Welling,Christian A. Naesseth,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 提出了一种基于变分流匹配（VFM）的受控生成目标，通过变分推断实现流匹配，支持端到端训练或贝叶斯推断后处理控制，并在分子生成中实现对称性保持。


<details>
  <summary>Details</summary>
Motivation: 探索流匹配与变分推断的结合，以支持受控生成任务，特别是针对分子生成中的对称性需求。

Method: 通过变分推断框架实现流匹配，支持端到端训练和贝叶斯推断后处理两种受控生成方式，并提出了一种对称性保持的VFM变体。

Result: 在非受控和受控分子生成任务中均达到最先进性能，特别是在贝叶斯推断后处理控制中表现优异。

Conclusion: 该工作强化了基于流的生成模型与贝叶斯推断的联系，为约束驱动和对称性感知的生成提供了可扩展的理论框架。

Abstract: We derive a controlled generation objective within the framework of
Variational Flow Matching (VFM), which casts flow matching as a variational
inference problem. We demonstrate that controlled generation can be implemented
two ways: (1) by way of end-to-end training of conditional generative models,
or (2) as a Bayesian inference problem, enabling post hoc control of
unconditional models without retraining. Furthermore, we establish the
conditions required for equivariant generation and provide an equivariant
formulation of VFM tailored for molecular generation, ensuring invariance to
rotations, translations, and permutations. We evaluate our approach on both
uncontrolled and controlled molecular generation, achieving state-of-the-art
performance on uncontrolled generation and outperforming state-of-the-art
models in controlled generation, both with end-to-end training and in the
Bayesian inference setting. This work strengthens the connection between
flow-based generative modeling and Bayesian inference, offering a scalable and
principled framework for constraint-driven and symmetry-aware generation.

</details>


### [190] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
*Zichong Li,Chen Liang,Zixuan Zhang,Ilgee Hong,Young Jin Kim,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: SlimMoE是一种多阶段压缩框架，用于将大型MoE模型压缩为更小、高效的变体，显著降低内存需求和训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构在资源受限环境中因内存需求过大而难以微调或部署的问题。

Method: 通过分阶段压缩（精简专家和知识转移）减少参数数量，避免一次性剪枝的性能下降。

Result: 成功压缩Phi 3.5-MoE为Phi-mini-MoE和Phi-tiny-MoE，仅需少量训练数据，单GPU即可微调，性能优于同类小模型。

Conclusion: 结构化剪枝与分阶段蒸馏结合，为高效紧凑的MoE模型提供了可行路径，推动MoE架构的广泛应用。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [191] [LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization](https://arxiv.org/abs/2506.18383)
*Koushik Viswanadha,Deepanway Ghosal,Somak Aditya*

Main category: cs.LG

TL;DR: 论文提出了一种通过微调偏好优化数据集来改进LLMs逻辑推理能力的方法，包括引入新数据集LogicPO和使用DPO、KTO等技术，显著提升了逻辑正确性并减少了语法错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在将自然语言推理问题转换为逻辑形式时存在不足，影响了其推理能力。

Method: 1) 引入新数据集LogicPO；2) 使用DPO和KTO技术微调开源LLMs。

Result: 最佳模型Phi-3.5在逻辑正确性上比GPT-3.5-turbo（8-shot）高出10%，语法错误减少14%。

Conclusion: 通过改进逻辑表示，为提升LLMs的逻辑推理能力提供了有前景的方向。

Abstract: Logical reasoning is a key task for artificial intelligence due to it's role
in major downstream tasks such as Question Answering, Summarization. Recent
methods in improving the reasoning ability of LLMs fall short in correctly
converting a natural language reasoning problem to an equivalent logical
formulation, which hinders the framework's overall ability to reason. Towards
this, we propose to use finetuning on a preference optimization dataset to
learn to parse and represent a natural language problem as a whole to a
consistent logical program by 1) introducing a new supervised and preference
optimization dataset LogicPO, and 2) adopting popular techniques such as Direct
Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune
open-source LLMs. Our best model with Phi-3.5 consistently outperforms
GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%
less syntax errors. Through the framework and our improved evaluation metrics,
we offer a promising direction in improving the logical reasoning of LLMs by
better representing them in their logical formulations.

</details>


### [192] [ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction](https://arxiv.org/abs/2506.18396)
*Marco Aruta,Ciro Listone,Giuseppe Murano,Aniello Murano*

Main category: cs.LG

TL;DR: 提出了一种名为ADNF的动态神经模糊聚类框架，用于白血病诊断和监测，结合CNN特征提取和在线模糊聚类，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法无法适应动态细胞模式变化和实时不确定性量化，需要更灵活的工具。

Method: 结合CNN特征提取和在线模糊聚类引擎，使用模糊时间指数（FTI）动态更新参数，并通过密度加权合并和熵引导分裂优化拓扑。

Result: 在C-NMC白血病显微镜数据集上，轮廓分数达到0.51，优于静态基线方法。

Conclusion: ADNF框架具有自适应不确定性建模和无标签操作的优势，适用于个性化白血病管理的实时支持。

Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image
data, yet conventional clustering methods lack the flexibility to accommodate
evolving cellular patterns and quantify uncertainty in real time. We introduce
Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable
framework that combines Convolutional Neural Network-based feature extraction
with an online fuzzy clustering engine. ADNF initializes soft partitions via
Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and
fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy
evolution. A topology refinement stage performs density-weighted merging and
entropy-guided splitting to guard against over- and under-segmentation. On the
C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of
0.51, demonstrating superior cohesion and separation over static baselines. The
method's adaptive uncertainty modeling and label-free operation hold immediate
potential for integration within the INFANT pediatric oncology network,
enabling scalable, up-to-date support for personalized leukemia management.

</details>


### [193] [FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data](https://arxiv.org/abs/2506.18481)
*Dominique Mercier,Andreas Dengel,Sheraz,Ahmed*

Main category: cs.LG

TL;DR: FreqATT框架通过频域分析提升时间序列神经网络的解释性，优于现有方法且更稳健。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络缺乏可解释性，现有方法对时间序列分析不足。

Method: 提出FreqATT框架，通过评估相关频率并过滤或标记输入数据。

Result: 频域分析能更突出输入信号的相关区域，且对信号波动更稳健。

Conclusion: FreqATT为时间序列分析提供了更优的解释性方法。

Abstract: Deep neural networks are among the most successful algorithms in terms of
performance and scalability in different domains. However, since these networks
are black boxes, their usability is severely restricted due to the lack of
interpretability. Existing interpretability methods do not address the analysis
of time-series-based networks specifically enough. This paper shows that an
analysis in the frequency domain can not only highlight relevant areas in the
input signal better than existing methods, but is also more robust to
fluctuations in the signal. In this paper, FreqATT is presented, a framework
that enables post-hoc networks to interpret time series analysis. To achieve
this, the relevant different frequencies are evaluated and the signal is either
filtered or the relevant input data is marked.

</details>


### [194] [Reliability-Adjusted Prioritized Experience Replay](https://arxiv.org/abs/2506.18482)
*Leonard S. Pleiss,Tobias Sutter,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 论文提出ReaPER方法，通过引入时间差分误差可靠性的新度量，改进了PER，理论证明其学习效率更高，实验显示在多种环境中优于PER。


<details>
  <summary>Details</summary>
Motivation: 传统经验回放均匀采样，忽略了经验间的学习潜力差异，PER虽改进但仍需提升效率。

Method: 引入时间差分误差可靠性度量，提出ReaPER算法。

Result: ReaPER在理论分析和实验中均优于PER，包括Atari-5基准测试。

Conclusion: ReaPER通过可靠性调整提升了经验回放的效率，适用于多种环境。

Abstract: Experience replay enables data-efficient learning from past experiences in
online reinforcement learning agents. Traditionally, experiences were sampled
uniformly from a replay buffer, regardless of differences in
experience-specific learning potential. In an effort to sample more
efficiently, researchers introduced Prioritized Experience Replay (PER). In
this paper, we propose an extension to PER by introducing a novel measure of
temporal difference error reliability. We theoretically show that the resulting
transition selection algorithm, Reliability-adjusted Prioritized Experience
Replay (ReaPER), enables more efficient learning than PER. We further present
empirical results showing that ReaPER outperforms PER across various
environment types, including the Atari-5 benchmark.

</details>


### [195] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/abs/2506.18495)
*Aniss Bessalah,Hatem Mohamed Abdelmoumen,Karima Benatchba,Hadjer Benmeziane*

Main category: cs.LG

TL;DR: 论文介绍了AnalogNAS-Bench，首个专为模拟内存计算（AIMC）设计的NAS基准，揭示了标准量化技术、架构宽度和分支结构以及跳跃连接对AIMC非理想性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络未针对AIMC的非理想性优化，需通过NAS发现适合AIMC的架构，但缺乏专用基准。

Method: 提出AnalogNAS-Bench基准，用于评估和比较NAS方法在AIMC环境下的表现。

Result: 发现标准量化技术不足、宽分支结构更稳健、跳跃连接增强抗噪性。

Conclusion: AnalogNAS-Bench填补了AIMC专用NAS基准的空白，为未来研究提供了方向。

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [196] [DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling](https://arxiv.org/abs/2506.18522)
*Yang Chang,Kuang-Da Wang,Ping-Chun Hsieh,Cheng-Kuan Lin,Wen-Chih Peng*

Main category: cs.LG

TL;DR: 论文提出了一种新的度量标准DIV-diff和基于Transformer的模型DDOT，用于更全面和稳定地推断多维ODE，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法难以捕捉ODE的时空动态和变量相关性，且现有方法（如ODEFormer）对初始点敏感，无法全面反映性能。

Method: 引入DIV-diff度量标准评估变量空间，并提出DDOT模型，通过预测ODE导数辅助任务重构多维ODE。

Result: DDOT在ODEBench上表现优于现有方法，重建和泛化任务分别提升4.58%和1.62%，DIV-diff降低3.55%。

Conclusion: DDOT在理论和实际应用中均表现出色，展示了其解决复杂动态系统问题的潜力。

Abstract: Uncovering the underlying ordinary differential equations (ODEs) that govern
dynamic systems is crucial for advancing our understanding of complex
phenomena. Traditional symbolic regression methods often struggle to capture
the temporal dynamics and intervariable correlations inherent in ODEs.
ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from
single trajectories, has made notable progress. However, its focus on
single-trajectory evaluation is highly sensitive to initial starting points,
which may not fully reflect true performance. To address this, we propose the
divergence difference metric (DIV-diff), which evaluates divergence over a grid
of points within the target region, offering a comprehensive and stable
analysis of the variable space. Alongside, we introduce DDOT
(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),
a transformer-based model designed to reconstruct multidimensional ODEs in
symbolic form. By incorporating an auxiliary task predicting the ODE's
derivative, DDOT effectively captures both structure and dynamic behavior.
Experiments on ODEBench show DDOT outperforms existing symbolic regression
methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$
for reconstruction and generalization tasks, respectively, and an absolute
reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world
applicability on an anesthesia dataset, highlighting its practical impact.

</details>


### [197] [Federated Learning from Molecules to Processes: A Perspective](https://arxiv.org/abs/2506.18525)
*Jan G. Rittig,Clemens Kortmann*

Main category: cs.LG

TL;DR: 联邦学习在化学工程中的应用，通过协作训练机器学习模型，保护企业数据隐私。


<details>
  <summary>Details</summary>
Motivation: 化学工程中大量数据因企业专有而无法共享，阻碍机器学习模型的训练。

Method: 探讨联邦学习在化学工程中的应用，并通过两个案例研究验证其效果。

Result: 联邦学习训练的模型比单独训练的模型更准确，接近合并数据训练的模型。

Conclusion: 联邦学习在保护数据隐私的同时提升模型性能，具有工业应用潜力。

Abstract: We present a perspective on federated learning in chemical engineering that
envisions collaborative efforts in machine learning (ML) developments within
the chemical industry. Large amounts of chemical and process data are
proprietary to chemical companies and are therefore locked in data silos,
hindering the training of ML models on large data sets in chemical engineering.
Recently, the concept of federated learning has gained increasing attention in
ML research, enabling organizations to jointly train machine learning models
without disclosure of their individual data. We discuss potential applications
of federated learning in several fields of chemical engineering, from the
molecular to the process scale. In addition, we apply federated learning in two
exemplary case studies that simulate practical scenarios of multiple chemical
companies holding proprietary data sets: (i) prediction of binary mixture
activity coefficients with graph neural networks and (ii) system identification
of a distillation column with autoencoders. Our results indicate that ML models
jointly trained with federated learning yield significantly higher accuracy
than models trained by each chemical company individually and can perform
similarly to models trained on combined datasets from all companies. Federated
learning has therefore great potential to advance ML models in chemical
engineering while respecting corporate data privacy, making it promising for
future industrial applications.

</details>


### [198] [Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.18537)
*Azad Deihim,Eduardo Alonso,Dimitra Apostolopoulou*

Main category: cs.LG

TL;DR: MATWM是一种基于Transformer的多智能体世界模型，结合分散式想象框架和半集中式评论家，通过队友预测模块适应部分可观测环境，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中部分可观测性和非平稳性问题，提升协调能力和样本效率。

Method: 结合分散式想象框架、半集中式评论家和队友预测模块，采用优先回放机制适应策略变化。

Result: 在StarCraft等多基准测试中表现最优，样本效率高，50K次交互即接近最优性能。

Conclusion: MATWM通过多组件协同显著提升多智能体任务性能，尤其在协调密集型任务中表现突出。

Abstract: We present the Multi-Agent Transformer World Model (MATWM), a novel
transformer-based world model designed for multi-agent reinforcement learning
in both vector- and image-based environments. MATWM combines a decentralized
imagination framework with a semi-centralized critic and a teammate prediction
module, enabling agents to model and anticipate the behavior of others under
partial observability. To address non-stationarity, we incorporate a
prioritized replay mechanism that trains the world model on recent experiences,
allowing it to adapt to agents' evolving policies. We evaluated MATWM on a
broad suite of benchmarks, including the StarCraft Multi-Agent Challenge,
PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance,
outperforming both model-free and prior world model approaches, while
demonstrating strong sample efficiency, achieving near-optimal performance in
as few as 50K environment interactions. Ablation studies confirm the impact of
each component, with substantial gains in coordination-heavy tasks.

</details>


### [199] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/abs/2506.18588)
*Róisín Luo,James McDermott,Christian Gagné,Qiang Sun,Colm O'Riordan*

Main category: cs.LG

TL;DR: 论文提出了一个数学框架，用于建模随机梯度下降（SGD）训练中Lipschitz连续性的动态演化。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络对输入扰动的敏感性（Lipschitz连续性）在训练过程中的动态变化，填补了这一领域的空白。

Method: 利用随机微分方程（SDEs）系统，捕捉训练中的确定性和随机性因素，分析梯度流、梯度噪声及其投影对Lipschitz连续性的影响。

Result: 理论分析揭示了梯度流、梯度噪声及其投影对Lipschitz连续性演化的驱动作用，实验验证了理论与观察行为的高度一致。

Conclusion: 该框架为理解神经网络训练中Lipschitz连续性的动态提供了理论基础，并揭示了多种因素对其演化的影响。

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [200] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Main category: cs.LG

TL;DR: 提出了一种无需重新训练的低成本方法，通过计算多数与少数群体激活均值差异定义“偏置向量”，并在推理时应用以减少分类偏置。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络分类器在数据不平衡时继承的群体偏置问题，避免传统方法的高计算成本。

Method: 计算多数与少数群体激活均值差异定义“偏置向量”，并在模型残差流中减去该向量。

Result: 减少了分类偏置，提高了最差群体准确率。

Conclusion: 展示了在分类模型中低成本、无需训练的偏置缓解方法，扩展了传统生成模型中使用的导向向量应用。

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [201] [Simulation-Free Differential Dynamics through Neural Conservation Laws](https://arxiv.org/abs/2506.18604)
*Mengjian Hua,Eric Vanden-Eijnden,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: 提出了一种无需模拟的连续时间扩散过程训练框架，适用于广泛的优化目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么限制问题形式，要么需要昂贵的模拟，无法高效处理多样化的优化目标。

Method: 采用耦合参数化方法，联合建模时间依赖密度函数和扩散过程动态，直接嵌入Fokker-Planck方程和密度函数约束。

Result: 实现了多种问题形式的无模拟训练，包括生成建模、动态最优传输和随机最优控制等。

Conclusion: 该方法在时空事件建模和从群体数据学习最优动态等应用中表现出色。

Abstract: We present a novel simulation-free framework for training continuous-time
diffusion processes over very general objective functions. Existing methods
typically involve either prescribing the optimal diffusion process -- which
only works for heavily restricted problem formulations -- or require expensive
simulation to numerically obtain the time-dependent densities and sample from
the diffusion process. In contrast, we propose a coupled parameterization which
jointly models a time-dependent density function, or probability path, and the
dynamics of a diffusion process that generates this probability path. To
accomplish this, our approach directly bakes in the Fokker-Planck equation and
density function requirements as hard constraints, by extending and greatly
simplifying the construction of Neural Conservation Laws. This enables
simulation-free training for a large variety of problem formulations, from
data-driven objectives as in generative modeling and dynamical optimal
transport, to optimality-based objectives as in stochastic optimal control,
with straightforward extensions to mean-field objectives due to the ease of
accessing exact density functions. We validate our method in a diverse range of
application domains from modeling spatio-temporal events to learning optimal
dynamics from population data.

</details>


### [202] [Policy gradient methods for ordinal policies](https://arxiv.org/abs/2506.18614)
*Simón Weinberger,Jairo Cugliari*

Main category: cs.LG

TL;DR: 提出了一种基于序数回归模型的策略参数化方法，解决了传统softmax方法无法捕捉动作顺序关系的问题，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统softmax方法在离散动作空间中无法捕捉动作的顺序关系，为解决实际工业问题，提出了新的策略参数化方法。

Method: 基于序数回归模型，将其适应于强化学习环境，解决了实际挑战。

Result: 数值实验表明，该方法在实际应用和连续动作任务中表现优异。

Conclusion: 提出的序数回归策略参数化方法在强化学习中具有实际应用价值，尤其在需要捕捉动作顺序的场景中表现突出。

Abstract: In reinforcement learning, the softmax parametrization is the standard
approach for policies over discrete action spaces. However, it fails to capture
the order relationship between actions. Motivated by a real-world industrial
problem, we propose a novel policy parametrization based on ordinal regression
models adapted to the reinforcement learning setting. Our approach addresses
practical challenges, and numerical experiments demonstrate its effectiveness
in real applications and in continuous action tasks, where discretizing the
action space and applying the ordinal policy yields competitive performance.

</details>


### [203] [Pr{é}diction optimale pour un mod{è}le ordinal {à} covariables fonctionnelles](https://arxiv.org/abs/2506.18615)
*Simón Weinberger,Jairo Cugliari,Aurélie Le Cain*

Main category: cs.LG

TL;DR: 提出了一个用于有序模型的预测框架，包括基于损失函数的最优预测和最小绝对偏差预测的显式形式，并将带有函数协变量的有序模型转化为经典有序模型。


<details>
  <summary>Details</summary>
Motivation: 为有序模型提供更有效的预测方法，并解决带有函数协变量的模型问题。

Method: 引入基于损失函数的最优预测，推导最小绝对偏差预测的显式形式，并将函数协变量模型转化为经典模型。

Result: 方法在EssilorLuxottica的数据集上进行了验证，用于开发智能眼镜的色调控制算法。

Conclusion: 提出的框架为有序模型提供了有效的预测工具，并展示了在实际应用中的潜力。

Abstract: We present a prediction framework for ordinal models: we introduce optimal
predictions using loss functions and give the explicit form of the
Least-Absolute-Deviation prediction for these models. Then, we reformulate an
ordinal model with functional covariates to a classic ordinal model with
multiple scalar covariates. We illustrate all the proposed methods and try to
apply these to a dataset collected by EssilorLuxottica for the development of a
control algorithm for the shade of connected glasses.

</details>


### [204] [Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits](https://arxiv.org/abs/2506.18627)
*Yannik Mahlau,Maximilian Schier,Christoph Reinders,Frederik Schubert,Marco Bügling,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的逆向设计方法，用于优化光子集成电路（PICs），在多维设计任务中表现优于传统梯度优化。


<details>
  <summary>Details</summary>
Motivation: 传统梯度优化易陷入局部最优，无法满足现代光学计算对PICs设计的需求，需要更高效的优化算法。

Method: 将设计空间离散化为网格，采用多智能体强化学习（RL）算法，将设计任务转化为数千个二元变量的优化问题。

Result: 在多维设计任务中，该方法仅需数千次环境采样即可优化设计，性能优于传统梯度优化。

Conclusion: 该方法为光子学逆向设计提供了高效基准，并展示了RL在复杂设计任务中的潜力。

Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally
relied on gradientbased optimization. However, this approach is prone to end up
in local minima, which results in suboptimal design functionality. As interest
in PICs increases due to their potential for addressing modern hardware demands
through optical computing, more adaptive optimization algorithms are needed. We
present a reinforcement learning (RL) environment as well as multi-agent RL
algorithms for the design of PICs. By discretizing the design space into a
grid, we formulate the design task as an optimization problem with thousands of
binary variables. We consider multiple two- and three-dimensional design tasks
that represent PIC components for an optical computing system. By decomposing
the design space into thousands of individual agents, our algorithms are able
to optimize designs with only a few thousand environment samples. They
outperform previous state-of-the-art gradient-based optimization in both twoand
three-dimensional design tasks. Our work may also serve as a benchmark for
further exploration of sample-efficient RL for inverse design in photonics.

</details>


### [205] [On Equivariant Model Selection through the Lens of Uncertainty](https://arxiv.org/abs/2506.18629)
*Putri A. van der Linden,Alexander Timans,Dharmesh Tailor,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 论文探讨了如何通过不确定性指标选择具有对称性偏见的预训练模型，发现不确定性指标通常与预测性能一致，但贝叶斯模型证据不一致。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用不确定性指标选择具有对称性偏见的预训练模型，以解决对称性约束可能对预测性能的负面影响。

Method: 比较了频率主义（通过Conformal Prediction）、贝叶斯（通过边际似然）和基于校准的指标，与基于误差的评估方法。

Result: 不确定性指标通常与预测性能一致，但贝叶斯模型证据表现不一致，可能与贝叶斯和几何模型复杂性概念不匹配有关。

Conclusion: 不确定性指标在指导对称性感知模型选择方面具有潜力，但需解决贝叶斯方法的不一致问题。

Abstract: Equivariant models leverage prior knowledge on symmetries to improve
predictive performance, but misspecified architectural constraints can harm it
instead. While work has explored learning or relaxing constraints, selecting
among pretrained models with varying symmetry biases remains challenging. We
examine this model selection task from an uncertainty-aware perspective,
comparing frequentist (via Conformal Prediction), Bayesian (via the marginal
likelihood), and calibration-based measures to naive error-based evaluation. We
find that uncertainty metrics generally align with predictive performance, but
Bayesian model evidence does so inconsistently. We attribute this to a mismatch
in Bayesian and geometric notions of model complexity, and discuss possible
remedies. Our findings point towards the potential of uncertainty in guiding
symmetry-aware model selection.

</details>


### [206] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
*Chenxing Wei,Jiarui Yu,Ying Tiffany He,Hande Dong,Yao Shu,Fei Yu*

Main category: cs.LG

TL;DR: ReDit通过向离散奖励信号添加随机噪声，解决了梯度异常、优化不稳定和收敛慢的问题，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 离散奖励函数可能导致梯度异常、优化不稳定和收敛慢，ReDit旨在通过随机噪声解决这些问题。

Method: ReDit方法通过向离散奖励信号添加简单随机噪声，提供连续的探索梯度，加速收敛并鼓励模型探索新策略。

Result: 实验表明，ReDit在训练步骤减少90%的情况下性能与GRPO相当，训练时间相同时性能提升4%，并显著缓解梯度问题。

Conclusion: ReDit通过噪声注入有效解决了离散奖励的局限性，提升了训练效率和模型性能。

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [207] [Granular-Ball-Induced Multiple Kernel K-Means](https://arxiv.org/abs/2506.18637)
*Shuyin Xia,Yifan Wang,Lifeng Shen,Guoyin Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于粒度球计算的多核聚类框架（GB-MKKM），通过粒度球核（GBK）改进传统多核K均值算法的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多核聚类算法（如多核K均值）在复杂数据分布下效率低且鲁棒性差，主要依赖点对点关系优化，难以捕捉数据固有结构和多样性。

Method: 利用粒度球计算自适应拟合数据分布，提出粒度球核（GBK）和GB-MKKM框架，通过球间关系提升效率和聚类性能。

Result: 实验表明，GB-MKKM在多种聚类任务中表现出更高的效率和性能。

Conclusion: 粒度球计算显著提升了多核聚类的效率和鲁棒性，GB-MKKM框架具有实际应用潜力。

Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel
K-means, often struggle with computational efficiency and robustness when faced
with complex data distributions. These challenges stem from their dependence on
point-to-point relationships for optimization, which can lead to difficulty in
accurately capturing data sets' inherent structure and diversity. Additionally,
the intricate interplay between multiple kernels in such algorithms can further
exacerbate these issues, effectively impacting their ability to cluster data
points in high-dimensional spaces. In this paper, we leverage granular-ball
computing to improve the multi-kernel clustering framework. The core of
granular-ball computing is to adaptively fit data distribution by balls from
coarse to acceptable levels. Each ball can enclose data points based on a
density consistency measurement. Such ball-based data description thus improves
the computational efficiency and the robustness to unknown noises.
Specifically, based on granular-ball representations, we introduce the
granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel
K-means framework (GB-MKKM) for efficient clustering. Using granular-ball
relationships in multiple kernel spaces, the proposed GB-MKKM framework shows
its superiority in efficiency and clustering performance in the empirical
evaluation of various clustering tasks.

</details>


### [208] [Federated Loss Exploration for Improved Convergence on Non-IID Data](https://arxiv.org/abs/2506.18640)
*Christian Internò,Markus Olhofer,Yaochu Jin,Barbara Hammer*

Main category: cs.LG

TL;DR: FedLEx是一种针对非独立同分布（non-IID）数据的联邦学习方法，通过优化学习行为和梯度更新策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布数据场景下的性能不足和鲁棒性问题。

Method: 采用联邦损失探索技术，通过客户端的梯度偏差构建全局指导矩阵，优化梯度更新。

Result: 实验表明，FedLEx在非独立同分布条件下性能显著优于现有方法。

Conclusion: FedLEx为联邦学习在多样化应用中的关键问题提供了有效解决方案。

Abstract: Federated learning (FL) has emerged as a groundbreaking paradigm in machine
learning (ML), offering privacy-preserving collaborative model training across
diverse datasets. Despite its promise, FL faces significant hurdles in
non-identically and independently distributed (non-IID) data scenarios, where
most existing methods often struggle with data heterogeneity and lack
robustness in performance. This paper introduces Federated Loss Exploration
(FedLEx), an innovative approach specifically designed to tackle these
challenges. FedLEx distinctively addresses the shortcomings of existing FL
methods in non-IID settings by optimizing its learning behavior for scenarios
in which assumptions about data heterogeneity are impractical or unknown. It
employs a federated loss exploration technique, where clients contribute to a
global guidance matrix by calculating gradient deviations for model parameters.
This matrix serves as a strategic compass to guide clients' gradient updates in
subsequent FL rounds, thereby fostering optimal parameter updates for the
global model. FedLEx effectively navigates the complex loss surfaces inherent
in non-IID data, enhancing knowledge transfer in an efficient manner, since
only a small number of epochs and small amount of data are required to build a
strong global guidance matrix that can achieve model convergence without the
need for additional data sharing or data distribution statics in a large client
scenario. Our extensive experiments with state-of-the art FL algorithms
demonstrate significant improvements in performance, particularly under
realistic non-IID conditions, thus highlighting FedLEx's potential to overcome
critical barriers in diverse FL applications.

</details>


### [209] [On Union-Closedness of Language Generation](https://arxiv.org/abs/2506.18642)
*Steve Hanneke,Amin Karbasi,Anay Mehrotra,Grigoris Velegkas*

Main category: cs.LG

TL;DR: 该论文研究了语言生成的极限问题，解决了Li等人提出的两个开放性问题，并证明了有限并集不一定可生成。同时，构造了一个不满足EUC条件的不可数类。


<details>
  <summary>Details</summary>
Motivation: 探索语言生成在极限情况下的性质，特别是解决Li等人提出的开放性问题，并揭示与传统统计学习任务的不同之处。

Method: 通过构造特定的类和新的对角化论证方法，证明了有限并集不一定可生成，并构造了一个不满足EUC条件的不可数类。

Result: 证明了有限并集不一定可生成，且存在不满足EUC条件的不可数类，揭示了语言生成与传统任务的不同。

Conclusion: 语言生成在极限情况下具有独特性质，有限并集不封闭，且存在不满足EUC条件的不可数类，这对生成模型的组合和提升提出了限制。

Abstract: We investigate language generation in the limit - a model by Kleinberg and
Mullainathan [NeurIPS 2024] and extended by Li, Raman, and Tewari [COLT 2025].
While Kleinberg and Mullainathan proved generation is possible for all
countable collections, Li et al. defined a hierarchy of generation notions
(uniform, non-uniform, and generatable) and explored their feasibility for
uncountable collections.
  Our first set of results resolve two open questions of Li et al. by proving
finite unions of generatable or non-uniformly generatable classes need not be
generatable. These follow from a stronger result: there is a non-uniformly
generatable class and a uniformly generatable class whose union is
non-generatable. This adds to the aspects along which language generation in
the limit is different from traditional tasks in statistical learning theory
like classification, which are closed under finite unions. In particular, it
implies that given two generators for different collections, one cannot combine
them to obtain a single "more powerful" generator, prohibiting this notion of
boosting.
  Our construction also addresses a third open question of Li et al. on whether
there are uncountable classes that are non-uniformly generatable and do not
satisfy the eventually unbounded closure (EUC) condition introduced by Li,
Raman, and Tewari. Our approach utilizes carefully constructed classes along
with a novel diagonalization argument that could be of independent interest in
the growing area of language generation.

</details>


### [210] [SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding](https://arxiv.org/abs/2506.18696)
*Yuchang Zhu,Jintang Li,Huizhe Zhang,Liang Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SaGIF的图神经网络方法，通过引入相似性一致性概念和两种评估指标，解决了图神经网络中的个体公平性问题。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络（GNNs）中的个体公平性（IF）问题，填补了现有研究在理解个体不公平性成因和全面考虑相似个体识别方面的空白。

Method: 通过初步分析探索个体不公平性的原因，引入相似性一致性概念，并提出两种评估个体相似性的指标（拓扑融合和特征融合）。基于这些指标，提出SaGIF方法，通过独立学习相似性表示来提升个体公平性。

Result: 在多个真实数据集上的实验验证了所提指标和SaGIF的有效性，SaGIF在个体公平性方面优于现有方法，同时保持了实用性。

Conclusion: SaGIF通过整合个体相似性表示，显著提升了图神经网络的个体公平性，为相关研究提供了新的思路和工具。

Abstract: Individual fairness (IF) in graph neural networks (GNNs), which emphasizes
the need for similar individuals should receive similar outcomes from GNNs, has
been a critical issue. Despite its importance, research in this area has been
largely unexplored in terms of (1) a clear understanding of what induces
individual unfairness in GNNs and (2) a comprehensive consideration of
identifying similar individuals. To bridge these gaps, we conduct a preliminary
analysis to explore the underlying reason for individual unfairness and observe
correlations between IF and similarity consistency, a concept introduced to
evaluate the discrepancy in identifying similar individuals based on graph
structure versus node features. Inspired by our observations, we introduce two
metrics to assess individual similarity from two distinct perspectives:
topology fusion and feature fusion. Building upon these metrics, we propose
Similarity-aware GNNs for Individual Fairness, named SaGIF. The key insight
behind SaGIF is the integration of individual similarities by independently
learning similarity representations, leading to an improvement of IF in GNNs.
Our experiments on several real-world datasets validate the effectiveness of
our proposed metrics and SaGIF. Specifically, SaGIF consistently outperforms
state-of-the-art IF methods while maintaining utility performance. Code is
available at: https://github.com/ZzoomD/SaGIF.

</details>


### [211] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
*Jie Li,Shifei Ding,Lili Guo,Xuan Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAGTKD的多模态锚点门控变换器，结合知识蒸馏技术，用于对话中的情感识别任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决情感识别任务中多模态特征融合的挑战，特别是模态贡献不均和帧级对齐的高复杂度问题。

Method: 使用提示学习增强文本模态表示，知识蒸馏强化弱模态表示，并引入多模态锚点门控变换器整合模态特征。

Result: 在IEMOCAP和MELD数据集上实现了最先进的情感识别性能。

Conclusion: MAGTKD通过知识蒸馏和多模态锚点门控变换器，有效提升了情感识别的准确性和鲁棒性。

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [212] [PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries](https://arxiv.org/abs/2506.18728)
*Steven Kolawole,Keshav Santhanam,Virginia Smith,Pratiksha Thaker*

Main category: cs.LG

TL;DR: 论文提出了PARALLELPROMPT，首个用于测量自然用户提示中查询内并行性的基准测试，通过分解提示结构实现延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统将用户提示视为单一输入，忽略了其中潜在的语义并行性，导致效率不足。

Method: 使用LLM辅助提示和基于规则的多语言验证，从37,000多个真实提示中提取结构化模式，并设计执行套件比较串行与并行策略。

Result: 在超过75%的数据集中成功解析查询内并行性，任务速度提升高达5倍，且质量损失极小。

Conclusion: PARALLELPROMPT为研究LLM服务中的结构感知执行提供了首个标准化测试平台。

Abstract: LLM serving systems typically treat user prompts as monolithic inputs,
optimizing inference through decoding tricks or inter-query batching. However,
many real-world prompts contain latent semantic parallelism--decomposable
structures where subtasks can be executed independently to reduce latency while
preserving meaning. We introduce PARALLELPROMPT, the first benchmark for
measuring intra-query parallelism in natural user prompts. Our dataset
comprises over 37,000 real-world prompts from public LLM chat logs, each
annotated with a structured schema capturing task templates, shared context,
and iteration inputs. These schemas are extracted using LLM-assisted prompting
with rule-based multilingual validation. To evaluate the benefits of
decomposition, we provide an execution suite that benchmarks serial vs.
parallel strategies, measuring latency, structural adherence, and semantic
fidelity. Our results show that intra-query parallelism can be successfully
parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks
like translation, comprehension, and comparative analysis, with minimal quality
degradation. By releasing this benchmark, curation pipeline, and evaluation
suite, we provide the first standardized testbed for studying structure-aware
execution in LLM serving pipelines.

</details>


### [213] [Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models](https://arxiv.org/abs/2506.18732)
*Yuning Yang,Han Yu,Tianrun Gao,Xiaodong Xu,Guangyu Wang*

Main category: cs.LG

TL;DR: 论文探讨了联邦基础模型（FFM）中多敏感属性的因果公平性分析，提出了一种扩展结构以平衡多属性并量化因果效应。


<details>
  <summary>Details</summary>
Motivation: 在联邦基础模型（FFM）中，敏感属性的偏见可能导致不公平，现有研究多关注单一属性，缺乏多属性依赖关系的解释性。

Method: 扩展FFM结构，通过因果发现和推理量化多敏感属性的公平性因果效应。

Result: 实验验证了方法的有效性，为构建可信赖且公平的FFM系统提供了可解释性见解。

Conclusion: 该研究首次实现了FFM中多敏感属性的因果公平性分析，为公平性研究提供了新方向。

Abstract: The deep integration of foundation models (FM) with federated learning (FL)
enhances personalization and scalability for diverse downstream tasks, making
it crucial in sensitive domains like healthcare. Achieving group fairness has
become an increasingly prominent issue in the era of federated foundation
models (FFMs), since biases in sensitive attributes might lead to inequitable
treatment for under-represented demographic groups. Existing studies mostly
focus on achieving fairness with respect to a single sensitive attribute. This
renders them unable to provide clear interpretability of dependencies among
multiple sensitive attributes which is required to achieve group fairness. Our
paper takes the first attempt towards a causal analysis of the relationship
between group fairness across various sensitive attributes in the FFM. We
extend the FFM structure to trade off multiple sensitive attributes
simultaneously and quantify the causal effect behind the group fairness through
causal discovery and inference. Extensive experiments validate its
effectiveness, offering insights into interpretability towards building
trustworthy and fair FFM systems.

</details>


### [214] [On the Existence of Universal Simulators of Attention](https://arxiv.org/abs/2506.18739)
*Debanjan Dutta,Faizanuddin Ansari,Anish Chakrabarty,Swagatam Das*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prior work on the learnability of transformers has established its capacity
to approximate specific algorithmic patterns through training under restrictive
architectural assumptions. Fundamentally, these arguments remain data-driven
and therefore can only provide a probabilistic guarantee. Expressivity, on the
contrary, has theoretically been explored to address the problems
\emph{computable} by such architecture. These results proved the
Turing-completeness of transformers, investigated bounds focused on circuit
complexity, and formal logic. Being at the crossroad between learnability and
expressivity, the question remains: \emph{can transformer architectures exactly
simulate an arbitrary attention mechanism, or in particular, the underlying
operations?} In this study, we investigate the transformer encoder's ability to
simulate a vanilla attention mechanism. By constructing a universal simulator
$\mathcal{U}$ composed of transformer encoders, we present algorithmic
solutions to identically replicate attention outputs and the underlying
elementary matrix and activation operations via RASP, a formal framework for
transformer computation. Our proofs, for the first time, show the existence of
an algorithmically achievable data-agnostic solution, previously known to be
approximated only by learning.

</details>


### [215] [Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://arxiv.org/abs/2506.18744)
*Qing Feng,Samuel Dalton,Benjamin Letham,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种结合快速实验和离线代理的贝叶斯优化方法，以在短时间内优化长期效果。


<details>
  <summary>Details</summary>
Motivation: 解决长期实验耗时过长的问题，同时避免短期实验的误导性。

Method: 结合快速实验（如短期偏置实验）和离线代理（如离线策略评估），进行贝叶斯优化。

Result: 能够在短时间内高效优化大型动作空间的长期效果。

Conclusion: 该方法显著缩短了实验时间，同时保持了优化效果。

Abstract: Online experiments in internet systems, also known as A/B tests, are used for
a wide range of system tuning problems, such as optimizing recommender system
ranking policies and learning adaptive streaming controllers. Decision-makers
generally wish to optimize for long-term treatment effects of the system
changes, which often requires running experiments for a long time as short-term
measurements can be misleading due to non-stationarity in treatment effects
over time. The sequential experimentation strategies--which typically involve
several iterations--can be prohibitively long in such cases. We describe a
novel approach that combines fast experiments (e.g., biased experiments run
only for a few hours or days) and/or offline proxies (e.g., off-policy
evaluation) with long-running, slow experiments to perform sequential, Bayesian
optimization over large action spaces in a short amount of time.

</details>


### [216] [ContinualFlow: Learning and Unlearning with Neural Flow Matching](https://arxiv.org/abs/2506.18747)
*Lorenzo Simone,Davide Bacciu,Shuangge Ma*

Main category: cs.LG

TL;DR: ContinualFlow是一个基于Flow Matching的生成模型目标性遗忘框架，通过能量重加权损失实现数据分布中不需要区域的软性移除。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中目标性遗忘的问题，避免从头训练或直接访问需要遗忘的样本。

Method: 利用能量重加权损失和Flow Matching，通过能量代理引导遗忘过程。

Result: 在2D和图像领域验证了框架的有效性，支持可视化解释和定量评估。

Conclusion: ContinualFlow提供了一种高效且无需直接访问样本的目标性遗忘方法。

Abstract: We introduce ContinualFlow, a principled framework for targeted unlearning in
generative models via Flow Matching. Our method leverages an energy-based
reweighting loss to softly subtract undesired regions of the data distribution
without retraining from scratch or requiring direct access to the samples to be
unlearned. Instead, it relies on energy-based proxies to guide the unlearning
process. We prove that this induces gradients equivalent to Flow Matching
toward a soft mass-subtracted target, and validate the framework through
experiments on 2D and image domains, supported by interpretable visualizations
and quantitative evaluations.

</details>


### [217] [Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos](https://arxiv.org/abs/2506.18751)
*Lukas Bahr,Lucas Poßner,Konstantin Weise,Sophie Gröger,Rüdiger Daub*

Main category: cs.LG

TL;DR: 该论文研究了图像分类模型在预测质量中的敏感性，提出了一种通过随机变量建模输入分布域偏移，并使用基于广义多项式混沌（GPC）的Sobol指数量化其对模型输出影响的方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在图像分类中常因模型、数据和域偏移而产生不确定性，导致分类模型输出过度自信，需通过敏感性分析理解模型行为。

Method: 提出用随机变量建模输入分布域偏移，并通过GPC计算Sobol指数量化其对模型输出的影响。

Result: 通过焊接缺陷分类和宝马集团生产设施中的标志分类案例验证了方法的有效性。

Conclusion: 该方法能有效量化域偏移对图像分类模型输出的影响，为理解模型行为提供了新工具。

Abstract: Integrating advanced communication protocols in production has accelerated
the adoption of data-driven predictive quality methods, notably machine
learning (ML) models. However, ML models in image classification often face
significant uncertainties arising from model, data, and domain shifts. These
uncertainties lead to overconfidence in the classification model's output. To
better understand these models, sensitivity analysis can help to analyze the
relative influence of input parameters on the output. This work investigates
the sensitivity of image classification models used for predictive quality. We
propose modeling the distributional domain shifts of inputs with random
variables and quantifying their impact on the model's outputs using Sobol
indices computed via generalized polynomial chaos (GPC). This approach is
validated through a case study involving a welding defect classification
problem, utilizing a fine-tuned ResNet18 model and an emblem classification
model used in BMW Group production facilities.

</details>


### [218] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
*Csaba Zsolnai,Niels Lörch,Julian Arnold*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络和‘学习-混淆’方案的方法，用于检测新闻数据中的变化点，无需过多领域知识即可识别公共话语的重大转变。


<details>
  <summary>Details</summary>
Motivation: 理解社会动态需要检测公共话语在重大事件中的变化，但现实数据高维、稀疏且噪声多，传统方法难以应对。

Method: 利用神经网络和‘学习-混淆’方案，训练分类器区分不同时间段的新闻文章，通过分类准确率估计内容分布的变化距离，从而识别变化点。

Result: 方法在合成数据集和《卫报》真实数据中有效，成功检测到9/11、COVID-19大流行和总统选举等重大事件。

Conclusion: 该方法无需领域知识，能自主发现公共话语的显著变化，并提供内容变化的量化指标，对新闻、政策分析和危机监测具有价值。

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [219] [Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning](https://arxiv.org/abs/2506.18789)
*Rahul Atul Bhope,K. R. Jayaram,Praveen Venkateswaran,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: ShiftEx框架通过动态创建专家模型和优化机制，解决了联邦学习中数据分布动态变化的问题，显著提升了模型性能和适应速度。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在动态数据分布环境中的性能下降问题，特别是协变量和标签偏移。

Method: 提出ShiftEx框架，利用最大均值差异检测协变量偏移，动态创建专家模型，并通过潜在内存机制和设施位置优化减少不匹配和成本。

Result: 在基准数据集上，ShiftEx比现有方法提高了5.5-12.9%的准确率，适应速度提升了22-95%。

Conclusion: ShiftEx为动态数据分布环境提供了一种高效、隐私保护的联邦学习解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing raw data, yet faces significant
challenges in real-world settings where client data distributions evolve
dynamically over time. This paper tackles the critical problem of covariate and
label shifts in streaming FL environments, where non-stationary data
distributions degrade model performance and require adaptive middleware
solutions. We introduce ShiftEx, a shift-aware mixture of experts framework
that dynamically creates and trains specialized global models in response to
detected distribution shifts using Maximum Mean Discrepancy for covariate
shifts. The framework employs a latent memory mechanism for expert reuse and
implements facility location-based optimization to jointly minimize covariate
mismatch, expert creation costs, and label imbalance. Through theoretical
analysis and comprehensive experiments on benchmark datasets, we demonstrate
5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation
compared to state-of-the-art FL baselines across diverse shift scenarios. The
proposed approach offers a scalable, privacy-preserving middleware solution for
FL systems operating in non-stationary, real-world conditions while minimizing
communication and computational overhead.

</details>


### [220] [A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction](https://arxiv.org/abs/2506.18797)
*Xin An,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Main category: cs.LG

TL;DR: 提出了一种多视图的Divergence-Convergence特征增强框架（DCFA_DMP），用于药物-微生物关联预测，通过对抗学习和双向协同注意力机制优化特征空间，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在药物-微生物关联预测中缺乏多视图特征的有效融合和优化，限制了预测的准确性和泛化能力。

Method: DCFA_DMP框架包括发散阶段（对抗学习优化特征空间）和收敛阶段（双向协同注意力机制深度融合特征），并结合Transformer图学习。

Result: 实验表明DCFA_DMP在药物-微生物关联预测中表现显著，且在冷启动实验中对新药物和微生物的预测也有效。

Conclusion: DCFA_DMP框架通过多视图特征融合和优化，显著提升了药物-微生物关联预测的稳定性和可靠性。

Abstract: In the study of drug function and precision medicine, identifying new
drug-microbe associations is crucial. However, current methods isolate
association and similarity analysis of drug and microbe, lacking effective
inter-view optimization and coordinated multi-view feature fusion. In our
study, a multi-view Divergence-Convergence Feature Augmentation framework for
Drug-related Microbes Prediction (DCFA_DMP) is proposed, to better learn and
integrate association information and similarity information. In the divergence
phase, DCFA_DMP strengthens the complementarity and diversity between
heterogeneous information and similarity information by performing Adversarial
Learning method between the association network view and different similarity
views, optimizing the feature space. In the convergence phase, a novel
Bidirectional Synergistic Attention Mechanism is proposed to deeply synergize
the complementary features between different views, achieving a deep fusion of
the feature space. Moreover, Transformer graph learning is alternately applied
on the drug-microbe heterogeneous graph, enabling each drug or microbe node to
focus on the most relevant nodes. Numerous experiments demonstrate DCFA_DMP's
significant performance in predicting drug-microbe associations. It also proves
effectiveness in predicting associations for new drugs and microbes in cold
start experiments, further confirming its stability and reliability in
predicting potential drug-microbe associations.

</details>


### [221] [Multi-Agent Online Control with Adversarial Disturbances](https://arxiv.org/abs/2506.18814)
*Anas Barakat,John Lazarsfeld,Georgios Piliouras,Antonios Varvitsiotis*

Main category: cs.LG

TL;DR: 研究了多智能体线性动态系统中的在线控制问题，重点关注对抗性扰动和个体目标下的梯度控制器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体控制问题在机器人、经济和能源系统中日益普遍，但现有研究多忽略对抗性扰动和个体目标的影响。

Method: 采用梯度控制器，分析其在对抗性扰动和个体目标下的性能，并研究智能体数量对个体后悔界的影响。

Result: 证明了在所有智能体中均成立的近最优次线性后悔界，并在目标一致时推导了时变势博弈的均衡间隙保证。

Conclusion: 该研究为多智能体在线控制提供了理论支持，尤其在对抗性扰动和个体目标场景下具有应用潜力。

Abstract: Multi-agent control problems involving a large number of agents with
competing and time-varying objectives are increasingly prevalent in
applications across robotics, economics, and energy systems. In this paper, we
study online control in multi-agent linear dynamical systems with disturbances.
In contrast to most prior work in multi-agent control, we consider an online
setting where disturbances are adversarial and where each agent seeks to
minimize its own, adversarial sequence of convex losses. In this setting, we
investigate the robustness of gradient-based controllers from single-agent
online control, with a particular focus on understanding how individual regret
guarantees are influenced by the number of agents in the system. Under minimal
communication assumptions, we prove near-optimal sublinear regret bounds that
hold uniformly for all agents. Finally, when the objectives of the agents are
aligned, we show that the multi-agent control problem induces a time-varying
potential game for which we derive equilibrium gap guarantees.

</details>


### [222] [Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning](https://arxiv.org/abs/2506.18847)
*Anthony Kobanda,Waris Radji,Mathieu Petitbois,Odalric-Ambrym Maillard,Rémy Portelas*

Main category: cs.LG

TL;DR: ProQ框架通过几何方法解决离线目标强化学习中的长时任务问题，结合度量学习和关键点覆盖，实现鲁棒的目标达成。


<details>
  <summary>Details</summary>
Motivation: 解决离线目标强化学习中长时任务因价值估计误差累积而难以扩展的问题。

Method: 提出ProQ框架，学习非对称距离并用于关键点覆盖和子目标引导，结合拉格朗日分布外检测器确保关键点可达。

Result: 在多样化导航基准测试中，ProQ能生成有意义的子目标并鲁棒地达成长时任务。

Conclusion: ProQ通过几何与控制的统一，有效解决了长时任务中的挑战。

Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to
reach specified goals from previously collected trajectories. Scaling that
promises to long-horizon tasks remains challenging, notably due to compounding
value-estimation errors. Principled geometric offers a potential solution to
address these issues. Following this insight, we introduce Projective
Quasimetric Planning (ProQ), a compositional framework that learns an
asymmetric distance and then repurposes it, firstly as a repulsive energy
forcing a sparse set of keypoints to uniformly spread over the learned latent
space, and secondly as a structured directional cost guiding towards proximal
sub-goals. In particular, ProQ couples this geometry with a Lagrangian
out-of-distribution detector to ensure the learned keypoints stay within
reachable areas. By unifying metric learning, keypoint coverage, and
goal-conditioned control, our approach produces meaningful sub-goals and
robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [223] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: MISO是一种基于视觉的机器学习模型，用于生成阿拉斯加高分辨率土壤地图，优于传统方法如随机森林，适用于永久冻土监测和基础设施规划。


<details>
  <summary>Details</summary>
Motivation: 阿拉斯加土壤地图对生态和基础设施至关重要，但传统方法依赖实地工作且分辨率不足。气候变暖加速永久冻土融化，急需高分辨率地图以支持适应策略。

Method: MISO结合地理空间基础模型、隐式神经表示和对比学习，实现连续空间预测和多模态对齐，并与随机森林进行对比。

Result: MISO在空间交叉验证中表现优于随机森林，泛化能力更强，召回率更高，适用于偏远地区。

Conclusion: MISO展示了先进机器学习在土壤地图中的潜力，为永久冻土区的采样和规划提供实用指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [224] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: 提出了一种基于分组的高效反馈门网络方法，通过反馈和门操作提升单张高光谱图像超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有单张高光谱图像超分辨率方法未能充分探索波段间和空间-光谱信息的连贯性，导致性能受限。

Method: 使用反馈门网络，结合大核卷积和光谱交互，通过分组学习和渐进扩张融合模块（SPDFM）提取丰富信息。

Result: 在三个高光谱数据集上验证了该方法在光谱保真度和空间内容重建上的优越性。

Conclusion: 提出的方法通过高效的空间-光谱特征提取和增强，显著提升了超分辨率性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [225] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: 论文提出SynDaCaTE数据集，用于评估胶囊网络是否真正学习部分-整体层次结构，并发现自注意力机制在部分到整体推理中的高效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有胶囊网络难以验证是否真正学习部分-整体层次结构的问题。

Method: 提出合成数据集SynDaCaTE，并用于评估现有胶囊模型的瓶颈和自注意力机制的效果。

Result: 发现现有胶囊模型的瓶颈，并证明自注意力机制在部分到整体推理中的高效性。

Conclusion: SynDaCaTE为设计计算机视觉的有效归纳偏置提供了未来方向。

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [226] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN通过引入Dual-Gated Depth Propagation Unit（DG-DPU）模块，解决了大型视觉语言模型（LVLM）中的幻觉问题，无需大量资源或任务特定配置。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在生成输出时容易产生视觉上无根据的幻觉，现有方法通常需要大量资源或任务特定配置。

Method: 提出HalluRNN架构，采用DG-DPU模块，通过跨层循环推理增强模型稳定性，自适应传播信息并减少表征漂移。

Result: 仅微调DG-DPU模块，HalluRNN在多个基准测试中表现优异且稳健。

Conclusion: HalluRNN提供了一种高效且通用的架构级解决方案，显著减少了LVLM的幻觉问题。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [227] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: 提出了一种结合SAR-光学特征融合和深度学习的云注意力重建框架，用于生成无云光学影像。


<details>
  <summary>Details</summary>
Motivation: 云污染严重影响光学卫星影像的可用性，妨碍环境监测、灾害响应等关键应用。

Method: 采用注意力驱动的特征融合机制，结合SAR的结构信息和光学数据的光谱特征，并引入云感知模型更新策略。

Result: 实验结果表明，该方法优于现有方法，PSNR为31.01 dB，SSIM为0.918，MAE为0.017。

Conclusion: 该框架能生成高保真、空间和光谱一致的无云光学影像。

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [228] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: 本文构建了首个真实工业场景的传送带裂缝数据集（BeltCrack14ks和BeltCrack9kd），并提出了一种基于时空频三域特征分层融合的基线方法，验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 传送带裂缝对工业安全和效率构成威胁，但现有数据集多为路面或合成数据，缺乏真实工业场景数据。

Method: 构建真实工业场景的传送带裂缝数据集，并提出基于时空频三域特征分层融合的基线方法。

Result: 实验证明数据集有效，且基线方法优于其他类似检测方法。

Conclusion: 本文填补了工业传送带裂缝数据集的空白，并提供了有效的检测方法。

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [229] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出了一种新的无监督域自适应（UDA）方法，通过结合ResNet和FPN的架构以及定制化的损失函数，有效解决了自然图像中的域偏移问题，提升了模型在目标域上的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗域自适应方法在多模态分布的分类问题中可能无法有效对齐不同域，因此需要一种更有效的方法来处理自然图像中的域偏移问题。

Method: 采用ResNet和FPN的深度架构处理内容和风格特征，并结合新颖的损失函数和现有损失函数进行训练。

Result: 在Office-Home、Office-31和VisDA-2017数据集上优于现有CNN方法，在DomainNet数据集上表现相当。

Conclusion: 提出的UDA方法在多模态分布的自然图像中表现出更好的泛化能力和训练效率。

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [230] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: 提出了一种快速可靠的神经逆向运动学框架，用于从3D关键点实时捕捉人体运动。


<details>
  <summary>Details</summary>
Motivation: 无标记运动捕捉虽然灵活且成本低，但通常计算需求高、推理速度慢，限制了实时应用。

Method: 详细描述了网络架构、训练方法和推理流程，并通过消融研究支持关键设计决策。

Result: 框架在定性和定量上均进行了评估。

Conclusion: 该框架为实时人体运动捕捉提供了一种高效解决方案。

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [231] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: ShareGPT-4o-Image数据集和Janus-4o模型旨在开源多模态生成能力，提升文本到图像和文本加图像到图像的生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前领先的多模态生成模型（如GPT-4o-Image）是专有的，限制了开放研究。本文旨在通过开源数据集和模型推动该领域的发展。

Method: 使用GPT-4o生成合成数据集ShareGPT-4o-Image（45K文本到图像和46K文本加图像到图像数据），并基于此训练Janus-4o模型。

Result: Janus-4o在文本到图像生成上显著优于前代模型Janus-Pro，并首次支持文本加图像到图像生成，仅需91K样本和6小时训练即达到高性能。

Conclusion: 开源ShareGPT-4o-Image和Janus-4o有望促进多模态生成研究的开放性和发展。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [232] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 论文分析了VICReg在自监督学习中的潜在不足，提出改进方法SAG-VICReg，通过新训练技术提升泛化能力和全局语义捕捉能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VICReg可能因过度依赖训练数据而泛化能力不足，需改进以生成更鲁棒的图像表示。

Method: 提出SAG-VICReg，在VICReg基础上引入新训练技术，增强全局语义捕捉和泛化能力。

Result: SAG-VICReg在泛化挑战中表现优异，超越现有自监督学习方法，尤其在全局语义评估中表现突出。

Conclusion: SAG-VICReg有效解决了VICReg的泛化问题，并提出新的无标签评估指标，适用于标签稀缺场景。

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [233] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级、适用于边缘设备的视网膜疾病分类器，通过跨架构知识蒸馏方法，将高性能的ViT教师模型压缩为CNN学生模型，适用于资源有限的环境。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的地区，早期准确诊断视网膜疾病至关重要，但缺乏可靠的诊断设备。

Method: 使用I-JEPA自监督学习预训练ViT教师模型，再通过包含PCA投影器和GL投影器的新框架压缩为CNN学生模型。

Result: 学生模型参数比教师模型少97.4%，分类准确率达89%，保留了教师模型93%的诊断性能。

Conclusion: 该方法成功实现了ViT模型的压缩并保持准确性，为资源匮乏地区提供了可扩展的AI驱动视网膜疾病分类方案。

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [234] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: 论文研究了多模态大型视觉语言模型（LVLMs）中虚假相关性的问题，开发了一个名为SpuriVerse的基准测试，包含124种虚假相关性类型，共1364个问题。实验表明，即使是顶级闭源模型表现也较差（最高37.1%准确率），但通过微调可显著提升至78.40%。


<details>
  <summary>Details</summary>
Motivation: 研究多模态LVLMs中虚假相关性的影响，填补现有基准测试在真实场景中的不足。

Method: 利用GPT-4o的错误数据构建SpuriVerse基准，包含真实和合成的视觉问答（VQA）样本，评估15种LVLMs的表现。

Result: 顶级闭源模型表现不佳（37.1%准确率），但微调后提升至78.40%。

Conclusion: 通过多样化虚假相关性的训练，模型能避免‘捷径’并关注整体图像上下文，提升泛化能力。

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [235] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出了一种双曲集合间距离度量（HS2SD），结合全局和局部结构信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用需要比较双曲数据点集合，而现有方法未能充分捕捉集合的全局和局部结构信息。

Method: HS2SD通过双曲集合的爱因斯坦中点测地距离（全局）和拓扑特征（局部）计算距离，使用Thue-Morse序列近似拓扑结构。

Result: 在实体匹配、标准图像分类和少样本图像分类任务中表现优于现有方法。

Conclusion: HS2SD能更细致地建模双曲集合间的层次和复杂关系。

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [236] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,György Dán,Henrik Sandberg*

Main category: cs.CV

TL;DR: SpaNN是一种新型对抗攻击检测器，其计算复杂度与对抗补丁数量无关，通过构建二值化特征图集合和聚类检测攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法多针对单补丁攻击，对多补丁攻击效果不佳或计算效率低，需一种更高效的检测方法。

Method: SpaNN通过应用一组显著性阈值生成二值化特征图集合，聚类后输入分类器进行攻击检测。

Result: 在四个数据集上，SpaNN在目标检测和图像分类任务中分别比现有方法提升11和27个百分点。

Conclusion: SpaNN在多补丁攻击检测中表现出色，计算效率高且对白盒攻击鲁棒。

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [237] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出了一种基于骨架的动作识别新方法，通过词嵌入编码语义信息，显著提升了分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统骨架方法在复杂交互中丢失关键点语义信息，限制了其有效性。

Method: 利用词嵌入替换独热编码，生成语义体积，捕捉关节与物体之间的有意义关系。

Result: 在多个装配数据集上验证，分类性能显著提升，同时支持不同骨架类型和物体类别。

Conclusion: 语义信息的融入能有效增强骨架动作识别在动态多样化环境中的表现。

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [238] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.CV

TL;DR: 提出了一种基于超像素的补丁池化（SPPP）技术和轻量潜在注意力（LLA）模块，以降低Vision Transformers的计算和内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算和内存资源上依赖性强，且任务特定迁移学习困难。这些问题主要源于计算密集的自注意力机制。

Method: 采用SPPP生成上下文感知的补丁嵌入，减少架构复杂性；引入LLA模块，通过潜在令牌集成降低注意力模块的时间和空间复杂度。

Result: 实验表明，该方法显著提高了计算效率，同时性能与最先进方法相当。

Conclusion: 该方法为边缘部署提供了高效的Transformer解决方案，代码已开源。

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [239] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat,Syed N. Sakib,Hasan M. Jamil*

Main category: cs.DL

TL;DR: KnoVo是一个智能框架，用于量化分析科学文献中研究新颖性的演变，通过多层引用网络和LLMs动态提取比较维度，生成定量新颖性分数，并可视化知识演进。


<details>
  <summary>Details</summary>
Motivation: 传统引用分析主要衡量影响力，而KnoVo旨在量化研究新颖性，帮助评估原创性、发现研究空白和跨学科联系。

Method: 利用LLMs动态提取比较维度（如方法、应用、数据集），通过类似锦标赛选择的方法对目标论文与相关文献进行比较分析，生成定量分数。

Result: 通过20篇跨学科论文的详细分析，验证了KnoVo的能力，并评估了开源LLMs在框架中的性能。

Conclusion: KnoVo不仅支持新颖性评估和相似工作识别，还能追踪知识演进，发现研究空白和跨学科联系。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [240] [Residue Number System (RNS) based Distributed Quantum Multiplication](https://arxiv.org/abs/2506.17588)
*Bhaskar Gaur,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 论文提出了一种基于余数系统（RNS）的分布式量子乘法方法，通过设计量子Diminished-1模$(2^n+1)$乘法器，显著降低了Toffoli深度和T门使用量。


<details>
  <summary>Details</summary>
Motivation: 量子乘法器在量子算法中广泛应用，但现有方案存在高Toffoli深度和T门使用量的问题，限制了其可扩展性和实用性。

Method: 采用余数系统（RNS）实现分布式量子乘法，设计量子Diminished-1模$(2^n+1)$乘法器，降低资源消耗。

Result: 与现有非分布式量子乘法器相比，Toffoli深度降低46.018%，T门使用量减少34.483%至86.25%。

Conclusion: 基于RNS的分布式量子乘法方案显著优化了资源使用，提升了量子乘法器的可扩展性和实用性。

Abstract: Multiplication of quantum states is a frequently used function or subroutine
in quantum algorithms and applications, making quantum multipliers an essential
component of quantum arithmetic. However, quantum multiplier circuits suffer
from high Toffoli depth and T gate usage, which ultimately affects their
scalability and applicability on quantum computers. To address these issues, we
propose utilizing the Residue Number System (RNS) based distributed quantum
multiplication, which executes multiple quantum modulo multiplication circuits
across quantum computers or jobs with lower Toffoli depth and T gate usage.
Towards this end, we propose a design of Quantum Diminished-1 Modulo $(2^n+1)$
Multiplier, an essential component of RNS based distributed quantum
multiplication. We provide estimates of quantum resource usage and compare them
with those of an existing non-distributed quantum multiplier for 6 to 16 qubit
sized output. Our comparative analysis estimates up to 46.018% lower Toffoli
depth, and reduction in T gates of 34.483% to 86.25%.

</details>


### [241] [Bloch Vector Assertions for Debugging Quantum Programs](https://arxiv.org/abs/2506.18458)
*Noah H. Oldfield,Christoph Laaber,Shaukat Ali*

Main category: quant-ph

TL;DR: Bloq提出了一种基于Bloch向量的自动化故障定位方法，通过Pauli算子的期望值测量实现低开销的调试，无需中间测量。AutoBloq自动生成断言方案，实验显示Bloq在性能和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子程序调试困难，现有断言方法存在手动生成、依赖中间测量和可扩展性差的问题。Bloq旨在解决这些问题，提供高效、自动化的调试方案。

Method: Bloq利用Bloch向量和Pauli算子期望值测量进行故障定位，无需中间测量；AutoBloq自动生成断言方案。

Result: 实验表明，Bloq在性能和可扩展性上优于Proq，F1分数更高，运行时间和电路深度开销显著降低。

Conclusion: Bloq为近量子设备提供了一种可扩展且高效的断言调试方法，显著提升了调试效果。

Abstract: Quantum programs must be reliable to ensure trustworthy results, yet
debugging them is notoriously challenging due to quantum-specific faults like
gate misimplementations and hardware noise, as well as their inherently
probabilistic nature. Assertion-based debugging provides a promising solution
by enabling localized correctness checks during execution. However, current
approaches face challenges including manual assertion generation, reliance on
mid-circuit-measurements, and poor scalability. In this paper, we present Bloq,
a scalable, automated fault localization approach introducing
Bloch-vector-based assertions utilizing expectation value measurements of Pauli
operators, enabling low-overhead fault localization without mid-circuit
measurements. In addition, we introduce AutoBloq, a component of Bloq for
automatically generating assertion schemes from quantum algorithms. An
experimental evaluation over 684432 programs using two algorithms (Quantum
Fourier Transform (QFT) and Grover) shows that Bloq consistently outperforms
the state-of-the-art approach Proq, notably as circuit depth and noise
increase. For Grover, Bloq achieves a mean F1 score across all experimental
instances of 0.74 versus 0.38 for Proq under ideal conditions, and maintains
performance under noise (0.43 versus 0.06). Bloq also reduces Proq's runtime by
a factor of 5 and circuit depth overhead by a factor of 23. These results
underline Bloq's potential to make assertion-based debugging scalable and
effective for near-term quantum devices.

</details>


### [242] [Quantum-Hybrid Support Vector Machines for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2506.17824)
*Tyler Cultice,Md. Saif Hassan Onim,Annarita Giani,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 量子混合支持向量机（QSVM）在工业控制系统（ICS）异常检测中表现出色，F1分数比传统方法高13.3%，且对噪声的鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统的敏感数据对关键基础设施的安全至关重要，量子机器学习方法有望通过高效特征空间提升异常检测能力。

Method: 研究使用三种流行的网络物理系统数据集，参数化量子混合支持向量机（QSVM），并模拟真实IBMQ硬件噪声。

Result: QSVM的F1分数提高13.3%，噪声误差仅0.98%，分类指标平均下降1.57%，核-目标对齐度提升91.023%。

Conclusion: QSVM在ICS异常检测中具有显著优势，可能实现量子优势，提升关键基础设施的安全性。

Abstract: Sensitive data captured by Industrial Control Systems (ICS) play a large role
in the safety and integrity of many critical infrastructures. Detection of
anomalous or malicious data, or Anomaly Detection (AD), with machine learning
is one of many vital components of cyberphysical security. Quantum kernel-based
machine learning methods have shown promise in identifying complex anomalous
behavior by leveraging the highly expressive and efficient feature spaces of
quantum computing. This study focuses on the parameterization of Quantum Hybrid
Support Vector Machines (QSVMs) using three popular datasets from
Cyber-Physical Systems (CPS). The results demonstrate that QSVMs outperform
traditional classical kernel methods, achieving 13.3% higher F1 scores.
Additionally, this research investigates noise using simulations based on real
IBMQ hardware, revealing a maximum error of only 0.98% in the QSVM kernels.
This error results in an average reduction of 1.57% in classification metrics.
Furthermore, the study found that QSVMs show a 91.023% improvement in
kernel-target alignment compared to classical methods, indicating a potential
"quantum advantage" in anomaly detection for critical infrastructures. This
effort suggests that QSVMs can provide a substantial advantage in anomaly
detection for ICS, ultimately enhancing the security and integrity of critical
infrastructures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [243] [Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems](https://arxiv.org/abs/2506.17236)
*Serdar Metin*

Main category: cs.CR

TL;DR: 论文探讨了非商业区块链网络中公平分配共享资源的问题，提出了6种基于Max-min公平性的算法，解决了传统faucet机制的DoS攻击和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 非商业区块链网络中缺乏货币化机制，传统faucet机制简单但易受DoS攻击且无法保证公平性。

Method: 提出了6种基于Max-min公平性的算法，作为高效的区块链faucet机制。

Result: 算法具有抗DoS攻击、低计算成本的特点，并支持不同的用户权重策略。

Conclusion: 这些算法为非商业区块链网络提供了公平、高效的资源分配解决方案。

Abstract: The present dissertation addresses the problem of fairly distributing shared
resources in non-commercial blockchain networks. Blockchains are distributed
systems that order and timestamp records of a given network of users, in a
public, cryptographically secure, and consensual way. The records, which may in
kind be events, transaction orders, sets of rules for structured transactions
etc. are placed within well-defined datastructures called blocks, and they are
linked to each other by the virtue of cryptographic pointers, in a total
ordering which represents their temporal relations of succession. The ability
to operate on the blockchain, and/or to contribute a record to the content of a
block are shared resources of the blockchain systems. In commercial networks,
these resources are exchanged in return for fiat money, and consequently,
fairness is not a relevant problem in terms of computer engineering. In
non-commercial networks, however, monetary solutions are not available, by
definition. The present non-commercial blockchain networks employ trivial
distribution mechanisms called faucets, which offer fixed amounts of free
tokens (called cryptocurrencies) specific to the given network. This mechanism,
although simple and efficient, is prone to denial of service (DoS) attacks and
cannot address the fairness problem. In the present dissertation, the faucet
mechanism is adapted for fair distribution, in line with Max-min Fairness
scheme. In total, we contributed 6 distinct Max-min Fair algorithms as
efficient blockchain faucets. The algorithms we contribute are resistant to DoS
attacks, low-cost in terms of blockchain computation economics, and they also
allow for different user weighting policies.

</details>


### [244] [Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability](https://arxiv.org/abs/2506.17371)
*Thilina Pathirana,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 论文研究了在5G-MEC存储中使用阈值秘密共享技术，以增强数据安全和可靠性。


<details>
  <summary>Details</summary>
Motivation: 5G-MEC的分布式特性带来了隐私和安全挑战，需要确保边缘数据的机密性和可用性。

Method: 采用(k,n)阈值秘密共享方案，将数据分散存储于n个节点，需至少k个节点才能重构数据。

Result: 该方法能防止少于k个节点的共谋攻击，并容忍最多n-k个节点故障，提升安全性和可用性。

Conclusion: 提出的方案不仅适用于秘密数据存储，还可扩展为5G-MEC中可信节点选择的通用方法。

Abstract: Multi-access Edge Computing (MEC), an enhancement of 5G, processes data
closer to its generation point, reducing latency and network load. However, the
distributed and edge-based nature of 5G-MEC presents privacy and security
challenges, including data exposure risks. Ensuring efficient manipulation and
security of sensitive data at the edge is crucial. To address these challenges,
we investigate the usage of threshold secret sharing in 5G-MEC storage, an
approach that enhances both security and dependability. A (k,n) threshold
secret sharing scheme splits and stores sensitive data among n nodes, requiring
at least k nodes for reconstruction. The solution ensures confidentiality by
protecting data against fewer than k colluding nodes and enhances availability
by tolerating up to n-k failing nodes. This approach mitigates threats such as
unauthorized access and node failures, whether accidental or intentional. We
further discuss a method for selecting the convenient MEHs to store the shares,
considering the MEHs' trustworthiness level as a main criterion. Although we
define our proposal in the context of secret-shared data storage, it can be
seen as an independent, standalone selection process for 5G-MEC trustworthy
node selection in other scenarios too.

</details>


### [245] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [246] [Tracking GPTs Third Party Service: Automation, Analysis, and Insights](https://arxiv.org/abs/2506.17315)
*Chuan Yan,Liuhuo Wan,Bowei Guan,Fengqi Yu,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: GPTs-ThirdSpy是一个自动化框架，用于提取GPTs的隐私设置，支持学术研究第三方服务集成及其隐私影响。


<details>
  <summary>Details</summary>
Motivation: 由于GPTs与第三方服务集成时隐私设置信息不透明，难以系统评估其数据隐私影响，因此需要工具支持研究。

Method: 开发了GPTs-ThirdSpy框架，自动提取GPTs的隐私设置，提供实时、可靠的第三方服务元数据。

Result: 框架能系统收集和结构化数据，支持大规模研究GPT生态的透明度和监管挑战。

Conclusion: GPTs-ThirdSpy为学术研究提供了工具，有助于分析第三方服务集成及其潜在安全风险。

Abstract: ChatGPT has quickly advanced from simple natural language processing to
tackling more sophisticated and specialized tasks. Drawing inspiration from the
success of mobile app ecosystems, OpenAI allows developers to create
applications that interact with third-party services, known as GPTs. GPTs can
choose to leverage third-party services to integrate with specialized APIs for
domain-specific applications. However, the way these disclose privacy setting
information limits accessibility and analysis, making it challenging to
systematically evaluate the data privacy implications of third-party integrate
to GPTs. In order to support academic research on the integration of
third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated
framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides
academic researchers with real-time, reliable metadata on third-party services
used by GPTs, enabling in-depth analysis of their integration, compliance, and
potential security risks. By systematically collecting and structuring this
data, GPTs-ThirdSpy facilitates large-scale research on the transparency and
regulatory challenges associated with the GPT app ecosystem.

</details>


### [247] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: 论文提出Smart-LLaMA-DPO方法，基于LLaMA-3.1-8B，通过构建全面数据集和优化训练流程，显著提升智能合约漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约漏洞检测方法存在数据集不全面和大型语言模型（LLMs）解释不准确的问题。

Method: 构建全面数据集，进行持续预训练（CPT）和监督微调（SFT），并应用直接偏好优化（DPO）提升模型性能。

Result: 在四种主要漏洞类型和机器不可审计漏洞上，F1分数和准确率分别平均提升10.43%和7.87%。

Conclusion: Smart-LLaMA-DPO显著优于现有方法，生成更正确、全面和清晰的解释。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [248] [Automatic Selection of Protections to Mitigate Risks Against Software Applications](https://arxiv.org/abs/2506.18470)
*Daniele Canavese,Leonardo Regano,Bjorn De Sutter,Cataldo Basile*

Main category: cs.CR

TL;DR: 论文提出了一种基于博弈论的自动化软件保护选择方法，通过最大化防御效果并限制保护开销，有效缓解MATE风险。


<details>
  <summary>Details</summary>
Motivation: 解决软件应用中关键资产面临的MATE风险，通过自动化保护选择提升安全性和可用性。

Method: 使用博弈论模型和启发式算法（mini-max深度优先搜索+动态规划优化）选择最优保护策略，并引入软件保护指数评估保护效果。

Result: 通过概念验证和专家评估，证明该方法在风险缓解中是实用且有效的。

Conclusion: 自动化软件保护选择是一种可行的解决方案，能够平衡安全性和性能开销。

Abstract: This paper introduces a novel approach for the automated selection of
software protections to mitigate MATE risks against critical assets within
software applications. We formalize the key elements involved in protection
decision-making - including code artifacts, assets, security requirements,
attacks, and software protections - and frame the protection process through a
game-theoretic model. In this model, a defender strategically applies
protections to various code artifacts of a target application, anticipating
repeated attack attempts by adversaries against the confidentiality and
integrity of the application's assets. The selection of the optimal defense
maximizes resistance to attacks while ensuring the application remains usable
by constraining the overhead introduced by protections. The game is solved
through a heuristic based on a mini-max depth-first exploration strategy,
augmented with dynamic programming optimizations for improved efficiency.
Central to our formulation is the introduction of the Software Protection
Index, an original contribution that extends existing notions of potency and
resilience by evaluating protection effectiveness against attack paths using
software metrics and expert assessments. We validate our approach through a
proof-of-concept implementation and expert evaluations, demonstrating that
automated software protection is a practical and effective solution for risk
mitigation in software.

</details>


### [249] [FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction](https://arxiv.org/abs/2506.18795)
*Jiachi Chen,Yiming Shen,Jiashuo Zhang,Zihao Li,John Grundy,Zhenzhe Shao,Yanlin Wang,Jiashui Wang,Ting Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: FORGE是一种自动化构建智能合约漏洞数据集的方法，解决了人工标注的高成本和分类不一致问题，通过LLM驱动的流程从审计报告中提取高质量漏洞数据，并基于CWE分类。


<details>
  <summary>Details</summary>
Motivation: 当前智能合约漏洞数据集构建存在人工标注成本高、分类不一致的问题，限制了数据集的质量和规模。

Method: FORGE采用LLM驱动的分治策略，从审计报告中提取结构化漏洞信息，并利用树状思维技术按CWE分类。

Result: FORGE生成了包含81,390个Solidity文件和27,497个漏洞的数据集，精度达95.6%，与专家一致性高（k-α=0.87）。

Conclusion: FORGE提供了一种高效、一致的漏洞数据集构建方法，揭示了现有安全工具的局限性，并指出了研究重点与实际漏洞分布的不一致。

Abstract: High-quality smart contract vulnerability datasets are critical for
evaluating security tools and advancing smart contract security research. Two
major limitations of current manual dataset construction are (1)
labor-intensive and error-prone annotation processes limiting the scale,
quality, and evolution of the dataset, and (2) absence of standardized
classification rules results in inconsistent vulnerability categories and
labeling results across different datasets. To address these limitations, we
present FORGE, the first automated approach for constructing smart contract
vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract
high-quality vulnerabilities from real-world audit reports and classify them
according to the CWE, the most widely recognized classification in software
security. FORGE employs a divide-and-conquer strategy to extract structured and
self-contained vulnerability information from these reports. Additionally, it
uses a tree-of-thoughts technique to classify the vulnerability information
into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we
run FORGE on 6,454 real-world audit reports and generate a dataset comprising
81,390 solidity files and 27,497 vulnerability findings across 296 CWE
categories. Manual assessment of the dataset demonstrates high extraction
precision and classification consistency with human experts (precision of 95.6%
and inter-rater agreement k-$\alpha$ of 0.87). We further validate the
practicality of our dataset by benchmarking 13 existing security tools on our
dataset. The results reveal the significant limitations in current detection
capabilities. Furthermore, by analyzing the severity-frequency distribution
patterns through a unified CWE perspective in our dataset, we highlight
inconsistency between current smart contract research focus and priorities
identified from real-world vulnerabilities...

</details>


### [250] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Main category: cs.CR

TL;DR: 论文提出了一个名为Boa的高效算法，用于解决大语言模型（LLMs）的越狱漏洞评估问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在安全关键应用中的广泛部署，缺乏系统方法评估其越狱攻击漏洞成为一个关键安全问题。

Method: Boa采用三阶段搜索策略：构建拒绝模式的黑名单、广度优先采样识别易访问的越狱路径，以及基于细粒度安全评分的深度优先搜索。

Result: Boa能够高效解决越狱预言问题，支持系统防御评估、标准化红队攻击比较和极端对抗条件下的模型认证。

Conclusion: Boa为LLMs的越狱漏洞提供了系统化评估方法，填补了安全领域的空白。

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [251] [Efficient Malware Detection with Optimized Learning on High-Dimensional Features](https://arxiv.org/abs/2506.17309)
*Aditya Choudhary,Sarthak Pawar,Yashodhara Haribhakta*

Main category: cs.CR

TL;DR: 论文提出了一种通过XGBoost特征选择和PCA降维技术优化高维特征的方法，用于恶意软件检测，LightGBM在384维特征上表现最佳，准确率达97.52%。


<details>
  <summary>Details</summary>
Motivation: 高维特征在恶意软件检测中带来计算挑战，需优化特征维度以平衡效率和性能。

Method: 使用XGBoost特征选择和PCA降维技术，评估128、256和384维特征在四种模型上的表现。

Result: LightGBM在384维特征上表现最佳，准确率为97.52%，且能有效泛化到未见数据集。

Conclusion: 该方法在保持高准确率的同时显著降低了计算资源需求，为恶意软件检测提供了高效解决方案。

Abstract: Malware detection using machine learning requires feature extraction from
binary files, as models cannot process raw binaries directly. A common approach
involves using LIEF for raw feature extraction and the EMBER vectorizer to
generate 2381-dimensional feature vectors. However, the high dimensionality of
these features introduces significant computational challenges. This study
addresses these challenges by applying two dimensionality reduction techniques:
XGBoost-based feature selection and Principal Component Analysis (PCA). We
evaluate three reduced feature dimensions (128, 256, and 384), which correspond
to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across
four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified
training, validation, and testing split formed from the EMBER-2018, ERMDS, and
BODMAS datasets. This approach ensures generalization and avoids dataset bias.
Experimental results show that LightGBM trained on the 384-dimensional feature
set after XGBoost feature selection achieves the highest accuracy of 97.52% on
the unified dataset, providing an optimal balance between computational
efficiency and detection performance. The best model, trained in 61 minutes
using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to
completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98%
accuracy on INFERNO. These findings present a scalable, compute-efficient
approach for malware detection without compromising accuracy.

</details>


### [252] [AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator](https://arxiv.org/abs/2506.17805)
*Md. Kamrul Hossain,Walid Aljoby,Anis Elgabli,Ahmed M. Abdelmoniem,Khaled A. Harras*

Main category: cs.CR

TL;DR: AdRo-FL提出了一种对抗性鲁棒的联邦学习方法，既能基于客户端效用进行智能选择，又能防御偏置选择攻击（BSA），同时保持隐私保护聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的安全聚合（SA）易受偏置选择攻击（BSA）影响，而随机选择虽能防御BSA，却无法实现高效客户端选择。

Method: AdRo-FL设计了两种客户端选择框架：针对集群环境的配额防御和效用函数优化，以及针对分布式环境的VRF随机选择协议。

Result: AdRo-FL在时间效率和最终准确率上分别达到基线方法的1.85倍和1.06倍。

Conclusion: AdRo-FL在保护隐私的同时，显著提升了联邦学习的性能和安全性。

Abstract: Federated Learning (FL) enables collaborative learning without exposing
clients' data. While clients only share model updates with the aggregator,
studies reveal that aggregators can infer sensitive information from these
updates. Secure Aggregation (SA) protects individual updates during
transmission; however, recent work demonstrates a critical vulnerability where
adversarial aggregators manipulate client selection to bypass SA protections,
constituting a Biased Selection Attack (BSA). Although verifiable random
selection prevents BSA, it precludes informed client selection essential for FL
performance. We propose Adversarial Robust Federated Learning (AdRo-FL), which
simultaneously enables: informed client selection based on client utility, and
robust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL
implements two client selection frameworks tailored for distinct settings. The
first framework assumes clients are grouped into clusters based on mutual
trust, such as different branches of an organization. The second framework
handles distributed clients where no trust relationships exist between them.
For the cluster-oriented setting, we propose a novel defense against BSA by (1)
enforcing a minimum client selection quota from each cluster, supervised by a
cluster-head in every round, and (2) introducing a client utility function to
prioritize efficient clients. For the distributed setting, we design a
two-phase selection protocol: first, the aggregator selects the top clients
based on our utility-driven ranking; then, a verifiable random function (VRF)
ensures a BSA-resistant final selection. AdRo-FL also applies quantization to
reduce communication overhead and sets strict transmission deadlines to improve
energy efficiency. AdRo-FL achieves up to $1.85\times$ faster time-to-accuracy
and up to $1.06\times$ higher final accuracy compared to insecure baselines.

</details>


### [253] [Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT](https://arxiv.org/abs/2506.18114)
*Ioannis Panopoulos,Maria-Lamprini A. Bartsioka,Sokratis Nikolaidis,Stylianos I. Venieris,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.CR

TL;DR: 提出了一种基于Transformer的早期入侵检测系统（EIDS），通过动态时间位置编码提高检测精度，并在资源受限的IoT设备上实现低延迟推理。


<details>
  <summary>Details</summary>
Motivation: IoT的快速发展带来了安全挑战，传统IDS模型忽略网络流量的时间特性，限制了早期威胁检测的效果。

Method: 利用网络流时间戳，结合动态时间位置编码和数据增强管道，提升模型鲁棒性。

Result: 在CICIoT2023数据集上表现优于现有模型，同时在资源受限的IoT设备上实现低延迟和低内存占用。

Conclusion: EIDS在检测精度和实时性方面均表现出色，适用于IoT环境。

Abstract: The rapid expansion of the Internet of Things (IoT) has introduced
significant security challenges, necessitating efficient and adaptive Intrusion
Detection Systems (IDS). Traditional IDS models often overlook the temporal
characteristics of network traffic, limiting their effectiveness in early
threat detection. We propose a Transformer-based Early Intrusion Detection
System (EIDS) that incorporates dynamic temporal positional encodings to
enhance detection accuracy while maintaining computational efficiency. By
leveraging network flow timestamps, our approach captures both sequence
structure and timing irregularities indicative of malicious behaviour.
Additionally, we introduce a data augmentation pipeline to improve model
robustness. Evaluated on the CICIoT2023 dataset, our method outperforms
existing models in both accuracy and earliness. We further demonstrate its
real-time feasibility on resource-constrained IoT devices, achieving
low-latency inference and minimal memory footprint.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [254] [Coupled Entropy: A Goldilocks Generalization?](https://arxiv.org/abs/2506.17229)
*Kenric P. Nelson*

Main category: stat.ML

TL;DR: 论文探讨了非广延统计力学（NSM）中的Tsallis熵约束问题，提出耦合熵（coupled entropy）作为解决方案，并分析了其在复杂系统中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决Tsallis熵在归一化时的不稳定性问题，并探索其在复杂系统（如机器学习）中的适用性。

Method: 引入耦合熵，通过除以$1 + d\kappa$来增强稳定性，并定义耦合指数族作为最大化分布。

Result: 耦合熵提供了必要的鲁棒性，适用于复杂系统，且耦合参数可作为统计复杂性的度量。

Conclusion: 耦合熵是解决Tsallis熵归一化问题的有效方法，并在复杂系统中展现出潜力。

Abstract: Nonextensive Statistical Mechanics (NSM) has developed into a powerful
toolset for modeling and analyzing complex systems. Despite its many successes,
a puzzle arose early in its development. The constraints on the Tsallis entropy
are in the form of an escort distribution with elements proportional to
$p_i^q$, but this same factor within the Tsallis entropy function is not
normalized. This led to consideration of the Normalized Tsallis Entropy (NTE);
however, the normalization proved to make the function unstable. I will provide
evidence that the coupled entropy, which divides NTE by $1 + d\kappa$, where
$d$ is the dimension and $\kappa$ is the coupling, may provide the necessary
robustness necessary for applications like machine learning. The definition for
the coupled entropy and its maximizing distributions, the coupled exponential
family, arises from clarifying how the number of independent random variables
$(q)$ is composed of the nonlinear properties of complex systems,
$q=1+\frac{\alpha\kappa}{1+d\kappa}$, where $\alpha$ is the nonlinear parameter
governing the shape of distributions near their location and $\kappa$ is the
parameter determining the asymptotic tail decay. Foundationally, for complex
systems, the coupling is the measure of nonlinearity inducing non-exponential
distributions and the degree of nonadditivity entropy. As such, the coupling is
a strong candidate as a measure of statistical complexity.

</details>


### [255] [Differentiable neural network representation of multi-well, locally-convex potentials](https://arxiv.org/abs/2506.17242)
*Reese E. Jones,Adrian Buganza Tepole,Jan N. Fuhg*

Main category: stat.ML

TL;DR: 提出了一种基于对数-求和-指数（LSE）混合输入凸神经网络（ICNN）的可微凸公式，用于建模多模态行为。


<details>
  <summary>Details</summary>
Motivation: 解决传统非平滑最小混合表示在多模态建模中的局限性，提供可微且凸的替代方案。

Method: 使用LSE-ICNN结合稀疏回归，自动发现模态数量和过渡尺度。

Result: 在多个领域（如相变、生物基因电路等）验证了LSE-ICNN的有效性。

Conclusion: LSE-ICNN在数据驱动建模和物理模拟中具有广泛适用性。

Abstract: Multi-well potentials are ubiquitous in science, modeling phenomena such as
phase transitions, dynamic instabilities, and multimodal behavior across
physics, chemistry, and biology. In contrast to non-smooth minimum-of-mixture
representations, we propose a differentiable and convex formulation based on a
log-sum-exponential (LSE) mixture of input convex neural network (ICNN) modes.
This log-sum-exponential input convex neural network (LSE-ICNN) provides a
smooth surrogate that retains convexity within basins and allows for
gradient-based learning and inference.
  A key feature of the LSE-ICNN is its ability to automatically discover both
the number of modes and the scale of transitions through sparse regression,
enabling adaptive and parsimonious modeling. We demonstrate the versatility of
the LSE-ICNN across diverse domains, including mechanochemical phase
transformations, microstructural elastic instabilities, conservative biological
gene circuits, and variational inference for multimodal probability
distributions. These examples highlight the effectiveness of the LSE-ICNN in
capturing complex multimodal landscapes while preserving differentiability,
making it broadly applicable in data-driven modeling, optimization, and
physical simulation.

</details>


### [256] [Gaussian Processes and Reproducing Kernels: Connections and Equivalences](https://arxiv.org/abs/2506.17366)
*Motonobu Kanagawa,Philipp Hennig,Dino Sejdinovic,Bharath K. Sriperumbudur*

Main category: stat.ML

TL;DR: 该专论研究了基于正定核的两种方法（高斯过程的概率方法和再生核希尔伯特空间的非概率方法）之间的联系，并建立了统一视角。


<details>
  <summary>Details</summary>
Motivation: 探讨高斯过程和再生核希尔伯特空间在机器学习、统计和数值分析中的联系与等价性。

Method: 回顾回归、插值、数值积分等基本主题中的等价性，基于高斯希尔伯特空间与RKHS的等价性建立统一视角。

Result: 建立了高斯过程与再生核方法之间的统一框架，为两种研究群体的方法提供桥梁。

Conclusion: 该专论为高斯过程和再生核方法的进一步研究提供了理论基础和统一视角。

Abstract: This monograph studies the relations between two approaches using positive
definite kernels: probabilistic methods using Gaussian processes, and
non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They
are widely studied and used in machine learning, statistics, and numerical
analysis. Connections and equivalences between them are reviewed for
fundamental topics such as regression, interpolation, numerical integration,
distributional discrepancies, and statistical dependence, as well as for sample
path properties of Gaussian processes. A unifying perspective for these
equivalences is established, based on the equivalence between the Gaussian
Hilbert space and the RKHS. The monograph serves as a basis to bridge many
other methods based on Gaussian processes and reproducing kernels, which are
developed in parallel by the two research communities.

</details>


### [257] [Scalable Machine Learning Algorithms using Path Signatures](https://arxiv.org/abs/2506.17634)
*Csaba Tóth*

Main category: stat.ML

TL;DR: 该论文探讨了如何利用路径签名（path signatures）在可扩展的机器学习流程中发挥其表达能力，提出了一系列结合理论鲁棒性和计算效率的模型，并展示了在时间序列和图数据中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决真实世界时间序列和图数据中常见的动态演化、长程依赖和不规则采样等挑战，通过路径签名提供一种通用的特征映射方法。

Method: 方法包括引入高斯过程与签名核协方差函数、Seq2Tens框架、基于图的模型，以及随机傅里叶签名特征等，结合了粗糙路径理论、概率建模和深度学习。

Result: 论文提出了一系列模型，如签名核高斯过程、Seq2Tens框架和基于图的扩散过程，这些模型在时间序列预测和图数据建模中表现出色。

Conclusion: 结论认为该论文为序列和结构化数据的可扩展签名学习提供了方法论工具和概念桥梁，是当前领域的前沿参考。

Abstract: The interface between stochastic analysis and machine learning is a rapidly
evolving field, with path signatures - iterated integrals that provide
faithful, hierarchical representations of paths - offering a principled and
universal feature map for sequential and structured data. Rooted in rough path
theory, path signatures are invariant to reparameterization and well-suited for
modelling evolving dynamics, long-range dependencies, and irregular sampling -
common challenges in real-world time series and graph data.
  This thesis investigates how to harness the expressive power of path
signatures within scalable machine learning pipelines. It introduces a suite of
models that combine theoretical robustness with computational efficiency,
bridging rough path theory with probabilistic modelling, deep learning, and
kernel methods. Key contributions include: Gaussian processes with signature
kernel-based covariance functions for uncertainty-aware time series modelling;
the Seq2Tens framework, which employs low-rank tensor structure in the weight
space for scalable deep modelling of long-range dependencies; and graph-based
models where expected signatures over graphs induce hypo-elliptic diffusion
processes, offering expressive yet tractable alternatives to standard graph
neural networks. Further developments include Random Fourier Signature
Features, a scalable kernel approximation with theoretical guarantees, and
Recurrent Sparse Spectrum Signature Gaussian Processes, which combine Gaussian
processes, signature kernels, and random features with a principled forgetting
mechanism for multi-horizon time series forecasting with adaptive context
length.
  We hope this thesis serves as both a methodological toolkit and a conceptual
bridge, and provides a useful reference for the current state of the art in
scalable, signature-based learning for sequential and structured data.

</details>


### [258] [Derandomizing Simultaneous Confidence Regions for Band-Limited Functions by Improved Norm Bounds and Majority-Voting Schemes](https://arxiv.org/abs/2506.17764)
*Balázs Csanád Csáji,Bálint Horváth*

Main category: stat.ML

TL;DR: 本文改进了带限函数的非参数非渐近置信区域构建方法，通过核范数界限优化和小样本的随机化Hoeffding不等式，应用多数投票提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 带限函数在系统理论和信号处理中广泛应用，但现有方法在构建置信区域时存在局限性，需要更精确的非参数非渐近方法。

Method: 在Paley-Wiener再生核希尔伯特空间中工作，使用随机化Hoeffding不等式和经验Bernstein界限优化核范数，并通过多数投票聚合置信集。

Result: 提出了基于样本大小和输入信息的近似阈值，验证了改进方法的有效性。

Conclusion: 通过数值实验验证了改进方法的稳定性和覆盖性，同时保留了同时覆盖的保证。

Abstract: Band-limited functions are fundamental objects that are widely used in
systems theory and signal processing. In this paper we refine a recent
nonparametric, nonasymptotic method for constructing simultaneous confidence
regions for band-limited functions from noisy input-output measurements, by
working in a Paley-Wiener reproducing kernel Hilbert space. Kernel norm bounds
are tightened using a uniformly-randomized Hoeffding's inequality for small
samples and an empirical Bernstein bound for larger ones. We derive an
approximate threshold, based on the sample size and how informative the inputs
are, that governs which bound to deploy. Finally, we apply majority voting to
aggregate confidence sets from random subsamples, boosting both stability and
region size. We prove that even per-input aggregated intervals retain their
simultaneous coverage guarantee. These refinements are also validated through
numerical experiments.

</details>


### [259] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: DRO-Augment结合Wasserstein分布鲁棒优化与数据增强，显著提升DNN在多种数据扰动和对抗攻击下的鲁棒性，同时保持干净数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法在同时应对数据损坏和对抗攻击时仍有不足，需提升模型鲁棒性。

Method: 提出DRO-Augment框架，整合W-DRO与多种数据增强策略，并建立理论上的泛化误差界。

Result: 在CIFAR-10-C等基准数据集上优于现有方法，尤其在严重数据扰动和对抗攻击下表现突出。

Conclusion: DRO-Augment有效提升模型鲁棒性，且理论分析支持其泛化能力。

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


### [260] [Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares](https://arxiv.org/abs/2506.18078)
*William Chung*

Main category: stat.ML

TL;DR: 提出了一种新的非参数回归方法ICCNLS，通过凸凹分解建模复杂输入输出关系，引入统计正交约束和正则化，提升了预测准确性和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统凸凹分解方法存在模糊性问题，且缺乏统计可识别性，限制了模型的可解释性和应用范围。

Method: ICCNLS将目标函数分解为凸凹分量，通过全局统计正交约束确保分解可识别，并引入L1、L2和弹性网正则化。

Result: 在合成和真实数据集（如医疗定价数据）上，ICCNLS表现出优于传统CNLS和DC回归的预测准确性和模型简洁性。

Conclusion: 统计可识别性与凸凹结构及正则化结合，可生成适用于预测、基准和政策评估的可解释模型。

Abstract: We propose a novel nonparametric regression method that models complex
input-output relationships as the sum of convex and concave components. The
method-Identifiable Convex-Concave Nonparametric Least Squares
(ICCNLS)-decomposes the target function into additive shape-constrained
components, each represented via sub-gradient-constrained affine functions. To
address the affine ambiguity inherent in convex-concave decompositions, we
introduce global statistical orthogonality constraints, ensuring that residuals
are uncorrelated with both intercept and input variables. This enforces
decomposition identifiability and improves interpretability. We further
incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance
generalisation and promote structural sparsity. The proposed method is
evaluated on synthetic and real-world datasets, including healthcare pricing
data, and demonstrates improved predictive accuracy and model simplicity
compared to conventional CNLS and difference-of-convex (DC) regression
approaches. Our results show that statistical identifiability, when paired with
convex-concave structure and sub-gradient regularisation, yields interpretable
models suited for forecasting, benchmarking, and policy evaluation.

</details>


### [261] [Phase transition of \emph{descending} phase retrieval algorithms](https://arxiv.org/abs/2506.18275)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文研究了下降相位恢复算法的理论极限，利用随机对偶理论（RDT）开发了一个通用程序，用于统计表征各种算法性能指标，并发现参数流形及其漏斗点是关键数学对象。


<details>
  <summary>Details</summary>
Motivation: 探索下降相位恢复算法的理论极限及其行为背后的数学结构。

Method: 使用随机对偶理论（RDT）分析参数流形及其漏斗点，并通过理论和模拟验证结果。

Result: 随着样本复杂度的增加，参数流形从多漏斗点结构过渡到单漏斗点结构，对应算法从失败到成功的相变。

Conclusion: 理论和模拟结果在小维度（几百）上表现出强一致性，验证了方法的有效性。

Abstract: We study theoretical limits of \emph{descending} phase retrieval algorithms.
Utilizing \emph{Random duality theory} (RDT) we develop a generic program that
allows statistical characterization of various algorithmic performance metrics.
Through these we identify the concepts of \emph{parametric manifold} and its
\emph{funneling points} as key mathematical objects that govern the underlying
algorithms' behavior. An isomorphism between single funneling point manifolds
and global convergence of descending algorithms is established. The structure
and shape of the parametric manifold as well as its dependence on the sample
complexity are studied through both plain and lifted RDT. Emergence of a phase
transition is observed. Namely, as sample complexity increases, parametric
manifold transitions from a multi to a single funneling point structure. This
in return corresponds to a transition from the scenarios where descending
algorithms generically fail to the scenarios where they succeed in solving
phase retrieval. We also develop and implement a practical algorithmic variant
that in a hybrid alternating fashion combines a barrier and a plain gradient
descent. Even though the theoretical results are obtained for infinite
dimensional scenarios (and consequently non-jittery parametric manifolds), we
observe a strong agrement between theoretical and simulated phase transitions
predictions for fairly small dimensions on the order of a few hundreds.

</details>


### [262] [Optimal spectral initializers impact on phase retrieval phase transitions -- an RDT view](https://arxiv.org/abs/2506.18279)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 论文研究了谱初始器与下降相位检索算法（dPR）理论极限的关系，提出了重叠最优谱初始器（OptSpins）的统计特性，并探讨了dPR解决相位检索（PR）的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究谱初始器如何影响dPR算法的性能，特别是通过重叠最优谱初始器（OptSpins）的统计特性，分析dPR在相位检索中的表现。

Method: 使用随机对偶理论（RDT）分析OptSpins的功能结构，评估其提供的初始重叠，并通过数值模拟验证理论预测。

Result: 发现dPR的理论相变可能难以实现，因为参数流形的平坦区域较大；通过增加样本复杂度比例（α）可以缩小平坦区域，使OptSpins避开这些区域。

Conclusion: OptSpins的初始重叠特性对dPR的成功至关重要，适当增加α可以优化算法性能。

Abstract: We analyze the relation between spectral initializers and theoretical limits
of \emph{descending} phase retrieval algorithms (dPR). In companion paper
[104], for any sample complexity ratio, $\alpha$, \emph{parametric manifold},
${\mathcal {PM}}(\alpha)$, is recognized as a critically important structure
that generically determines dPRs abilities to solve phase retrieval (PR).
Moreover, overlap between the algorithmic solution and the true signal is
positioned as a key ${\mathcal {PM}}$'s component. We here consider the
so-called \emph{overlap optimal} spectral initializers (OptSpins) as dPR's
starting points and develop a generic \emph{Random duality theory} (RDT) based
program to statistically characterize them. In particular, we determine the
functional structure of OptSpins and evaluate the starting overlaps that they
provide for the dPRs. Since ${\mathcal {PM}}$'s so-called \emph{flat regions}
are highly susceptible to \emph{local jitteriness} and as such are key
obstacles on dPR's path towards PR's global optimum, a precise characterization
of the starting overlap allows to determine if such regions can be successfully
circumvented. Through the presented theoretical analysis we observe two key
points in that regard: \textbf{\emph{(i)}} dPR's theoretical phase transition
(critical $\alpha$ above which they solve PR) might be difficult to practically
achieve as the ${\mathcal {PM}}$'s flat regions are large causing the
associated OptSpins to fall exactly within them; and \textbf{\emph{(ii)}}
Opting for so-called ``\emph{safer compression}'' and slightly increasing
$\alpha$ (by say $15\%$) shrinks flat regions and allows OptSpins to fall
outside them and dPRs to ultimately solve PR. Numerical simulations are
conducted as well and shown to be in an excellent agreement with theoretical
predictions.

</details>


### [263] [Phase retrieval with rank $d$ measurements -- \emph{descending} algorithms phase transitions](https://arxiv.org/abs/2506.18282)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文扩展了随机对偶理论（RDT）以分析降序相位检索算法（dPR）的性能，特别是针对秩d正定相位检索测量，并观察到样本复杂度比存在相变现象。理论与模拟结果高度一致。


<details>
  <summary>Details</summary>
Motivation: 扩展RDT理论以分析更广泛的相位检索问题，特别是针对不同秩的正定测量，验证理论与实际算法的性能一致性。

Method: 通过RDT理论分析样本复杂度比的相变现象，并实现对数障碍梯度下降算法进行小规模模拟验证。

Result: 确定了相变位置，理论与模拟结果在小规模问题上表现一致。

Conclusion: RDT理论在分析相位检索算法性能方面具有广泛适用性，理论与实验结果吻合良好。

Abstract: Companion paper [118] developed a powerful \emph{Random duality theory} (RDT)
based analytical program to statistically characterize performance of
\emph{descending} phase retrieval algorithms (dPR) (these include all variants
of gradient descents and among them widely popular Wirtinger flows). We here
generalize the program and show how it can be utilized to handle rank $d$
positive definite phase retrieval (PR) measurements (with special cases $d=1$
and $d=2$ serving as emulations of the real and complex phase retrievals,
respectively). In particular, we observe that the minimal sample complexity
ratio (number of measurements scaled by the dimension of the unknown signal)
which ensures dPR's success exhibits a phase transition (PT) phenomenon. For
both plain and lifted RDT we determine phase transitions locations. To
complement theoretical results we implement a log barrier gradient descent
variant and observe that, even in small dimensional scenarios (with problem
sizes on the order of 100), the simulated phase transitions are in an excellent
agreement with the theoretical predictions.

</details>


### [264] [Quantifying Uncertainty in the Presence of Distribution Shifts](https://arxiv.org/abs/2506.18283)
*Yuli Slavutsky,David M. Blei*

Main category: stat.ML

TL;DR: 提出一种贝叶斯框架，通过自适应先验条件化训练和新协变量，改进神经网络在协变量分布偏移下的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 神经网络在协变量分布偏移下不确定性估计不可靠，需改进。

Method: 采用自适应先验和摊销变分推断，通过合成环境模拟协变量偏移。

Result: 在合成和真实数据上显著提升了分布偏移下的不确定性估计。

Conclusion: 提出的方法有效解决了协变量偏移下的不确定性估计问题。

Abstract: Neural networks make accurate predictions but often fail to provide reliable
uncertainty estimates, especially under covariate distribution shifts between
training and testing. To address this problem, we propose a Bayesian framework
for uncertainty estimation that explicitly accounts for covariate shifts. While
conventional approaches rely on fixed priors, the key idea of our method is an
adaptive prior, conditioned on both training and new covariates. This prior
naturally increases uncertainty for inputs that lie far from the training
distribution in regions where predictive performance is likely to degrade. To
efficiently approximate the resulting posterior predictive distribution, we
employ amortized variational inference. Finally, we construct synthetic
environments by drawing small bootstrap samples from the training data,
simulating a range of plausible covariate shift using only the original
dataset. We evaluate our method on both synthetic and real-world data. It
yields substantially improved uncertainty estimates under distribution shifts.

</details>


### [265] [Theoretical guarantees for neural estimators in parametric statistics](https://arxiv.org/abs/2506.18508)
*Almut Rödder,Manuel Hentschel,Sebastian Engelke*

Main category: stat.ML

TL;DR: 神经估计器是基于模拟的统计模型参数估计器，通过深度学习架构和高效训练方法实现，但缺乏理论支持。本文通过风险分解和假设验证，为其提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 尽管神经估计器在模拟和实际应用中表现良好，但缺乏理论支持。本文旨在填补这一空白，为其性能提供理论依据。

Method: 通过分解神经估计器的风险为多个可单独分析的项，并验证确保各项收敛的假设。

Result: 验证了神经估计器在流行应用中的假设，为更广泛架构和估计问题提供理论保证。

Conclusion: 本文为神经估计器的性能提供了理论支持，并为更广泛的应用奠定了基础。

Abstract: Neural estimators are simulation-based estimators for the parameters of a
family of statistical models, which build a direct mapping from the sample to
the parameter vector. They benefit from the versatility of available network
architectures and efficient training methods developed in the field of deep
learning. Neural estimators are amortized in the sense that, once trained, they
can be applied to any new data set with almost no computational cost. While
many papers have shown very good performance of these methods in simulation
studies and real-world applications, so far no statistical guarantees are
available to support these observations theoretically. In this work, we study
the risk of neural estimators by decomposing it into several terms that can be
analyzed separately. We formulate easy-to-check assumptions ensuring that each
term converges to zero, and we verify them for popular applications of neural
estimators. Our results provide a general recipe to derive theoretical
guarantees also for broader classes of architectures and estimation problems.

</details>


### [266] [Trustworthy Prediction with Gaussian Process Knowledge Scores](https://arxiv.org/abs/2506.18630)
*Kurt Butler,Guanchao Feng,Tong Chen,Petar Djuric*

Main category: stat.ML

TL;DR: 提出了一种用于高斯过程回归（GPR）模型的知识评分，量化数据对预测不确定性的减少程度。


<details>
  <summary>Details</summary>
Motivation: 解决概率模型在数据稀疏区域预测时的不确定性评估问题。

Method: 设计了一种可解释且范围在0到1之间的知识评分。

Result: 实验表明，知识评分能有效预测GPR模型的准确性，并提升异常检测、外推和缺失数据填补等任务的性能。

Conclusion: 知识评分为GPR模型的预测提供了可靠的不确定性评估工具。

Abstract: Probabilistic models are often used to make predictions in regions of the
data space where no observations are available, but it is not always clear
whether such predictions are well-informed by previously seen data. In this
paper, we propose a knowledge score for predictions from Gaussian process
regression (GPR) models that quantifies the extent to which observing data have
reduced our uncertainty about a prediction. The knowledge score is
interpretable and naturally bounded between 0 and 1. We demonstrate in several
experiments that the knowledge score can anticipate when predictions from a GPR
model are accurate, and that this anticipation improves performance in tasks
such as anomaly detection, extrapolation, and missing data imputation. Source
code for this project is available online at
https://github.com/KurtButler/GP-knowledge.

</details>


### [267] [Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning](https://arxiv.org/abs/2506.18645)
*Wenjun Xiong,Juan Ding,Xinlei Zuo,Qizhai Li*

Main category: stat.ML

TL;DR: 论文提出了一种改进的随机梯度下降方法（T2pm-SGD），通过分解泛化误差为轨迹项和平坦项，显著提升了非凸学习中的泛化误差界限。


<details>
  <summary>Details</summary>
Motivation: 研究SGD在非凸学习中的泛化性质，以提升模型在未见数据上的鲁棒性能。

Method: 引入T2pm-SGD，支持子高斯和有界损失函数，分解泛化误差为轨迹项和平坦项，优化扰动噪声方差。

Result: 轨迹项改进为O(n^{-1})，整体界限优化至O(n^{-2/3})，平坦项稳定且小于文献值。

Conclusion: T2pm-SGD在理论和实验中均表现出更紧的泛化误差界限，适用于多种损失函数。

Abstract: Stochastic Gradient Descent (SGD) is fundamental for training deep neural
networks, especially in non-convex settings. Understanding SGD's generalization
properties is crucial for ensuring robust model performance on unseen data. In
this paper, we analyze the generalization error bounds of SGD for non-convex
learning by introducing the Type II perturbed SGD (T2pm-SGD), which
accommodates both sub-Gaussian and bounded loss functions. The generalization
error bound is decomposed into two components: the trajectory term and the
flatness term. Our analysis improves the trajectory term to $O(n^{-1})$,
significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses,
where n is the number of training samples and b is the batch size. By selecting
an optimal variance for the perturbation noise, the overall bound is further
refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory
term is also achieved. In both cases, the flatness term remains stable across
iterations and is smaller than those reported in previous literature, which
increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter
generalization error bounds for both loss function types. Our theoretical
results are validated through extensive experiments on benchmark datasets,
including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in
establishing tighter generalization bounds.

</details>


### [268] [A Random Matrix Analysis of In-context Memorization for Nonlinear Attention](https://arxiv.org/abs/2506.18656)
*Zhenyu Liao,Jiaqing Liu,TianQi Hou,Difan Zou,Zenan Ling*

Main category: stat.ML

TL;DR: 论文研究了非线性注意力机制在高维比例区域下的记忆误差，发现其通常比线性岭回归误差更高，但在输入具有统计结构时差距消失甚至反转。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制在现代大型语言模型中扮演核心角色，但其在非线性设置下的理论理解仍然有限。

Method: 利用大核随机矩阵理论的最新进展，分析了非线性注意力在高维比例区域下的记忆误差。

Result: 非线性注意力在随机输入上通常比线性岭回归误差更高，但在输入具有统计结构时差距消失甚至反转。

Conclusion: 研究揭示了非线性与输入结构如何相互作用以影响非线性注意力的记忆性能，并通过数值实验支持了理论发现。

Abstract: Attention mechanisms have revolutionized machine learning (ML) by enabling
efficient modeling of global dependencies across inputs. Their inherently
parallelizable structures allow for efficient scaling with the exponentially
increasing size of both pretrained data and model parameters. Yet, despite
their central role as the computational backbone of modern large language
models (LLMs), the theoretical understanding of Attentions, especially in the
nonlinear setting, remains limited.
  In this paper, we provide a precise characterization of the \emph{in-context
memorization error} of \emph{nonlinear Attention}, in the high-dimensional
proportional regime where the number of input tokens $n$ and their embedding
dimension $p$ are both large and comparable. Leveraging recent advances in the
theory of large kernel random matrices, we show that nonlinear Attention
typically incurs higher memorization error than linear ridge regression on
random inputs. However, this gap vanishes, and can even be reversed, when the
input exhibits statistical structure, particularly when the Attention weights
align with the input signal direction. Our results reveal how nonlinearity and
input structure interact with each other to govern the memorization performance
of nonlinear Attention. The theoretical insights are supported by numerical
experiments.

</details>


### [269] [Local Averaging Accurately Distills Manifold Structure From Noisy Data](https://arxiv.org/abs/2506.18761)
*Yihan Shen,Shiyu Wang,Arnaud Lamy,Mariam Avagyan,John Wright*

Main category: stat.ML

TL;DR: 论文分析了在高噪声环境下，通过两轮小批量局部平均方法从高维数据中提取低维流形结构的准确性，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 高维数据通常存在于低维流形附近，但实际中流形未知且只有噪声样本可用。局部平均是揭示流形结构的基础方法，但缺乏在高噪声环境下的严格分析。

Method: 采用两轮小批量局部平均方法，分析其在噪声大小接近流形范围时对噪声样本的处理效果。

Result: 证明了在高概率下，平均点与流形的距离满足特定上界，填补了高噪声环境下局部平均分析的空白。

Conclusion: 该方法可作为预处理步骤，为依赖局部平均的去噪和降维方法提供理论基础。

Abstract: High-dimensional data are ubiquitous, with examples ranging from natural
images to scientific datasets, and often reside near low-dimensional manifolds.
Leveraging this geometric structure is vital for downstream tasks, including
signal denoising, reconstruction, and generation. However, in practice, the
manifold is typically unknown and only noisy samples are available. A
fundamental approach to uncovering the manifold structure is local averaging,
which is a cornerstone of state-of-the-art provable methods for manifold
fitting and denoising. However, to the best of our knowledge, there are no
works that rigorously analyze the accuracy of local averaging in a manifold
setting in high-noise regimes. In this work, we provide theoretical analyses of
a two-round mini-batch local averaging method applied to noisy samples drawn
from a $d$-dimensional manifold $\mathcal M \subset \mathbb{R}^D$, under a
relatively high-noise regime where the noise size is comparable to the reach
$\tau$. We show that with high probability, the averaged point $\hat{\mathbf
q}$ achieves the bound $d(\hat{\mathbf q}, \mathcal M) \leq \sigma
\sqrt{d\left(1+\frac{\kappa\mathrm{diam}(\mathcal {M})}{\log(D)}\right)}$,
where $\sigma, \mathrm{diam(\mathcal M)},\kappa$ denote the standard deviation
of the Gaussian noise, manifold's diameter and a bound on its extrinsic
curvature, respectively. This is the first analysis of local averaging accuracy
over the manifold in the relatively high noise regime where $\sigma \sqrt{D}
\approx \tau$. The proposed method can serve as a preprocessing step for a wide
range of provable methods designed for lower-noise regimes. Additionally, our
framework can provide a theoretical foundation for a broad spectrum of
denoising and dimensionality reduction methods that rely on local averaging
techniques.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [270] [UT-GraphCast Hindcast Dataset: A Global AI Forecast Archive from UT Austin for Weather and Climate Applications](https://arxiv.org/abs/2506.17453)
*Naveen Sudharsan,Manmeet Singh,Harsh Kamath,Hassan Dashtian,Clint Dawson,Zong-Liang Yang,Dev Niyogi*

Main category: physics.geo-ph

TL;DR: UT GraphCast Hindcast Dataset是一个全球天气预测数据集，覆盖1979至2024年，由Google DeepMind GraphCast模型生成，提供每日15天确定性预测。


<details>
  <summary>Details</summary>
Motivation: 为全球天气预测提供高分辨率、长期的历史数据，支持气候研究和天气预报改进。

Method: 使用基于物理信息的图神经网络（GraphCast），在ECMWF ERA5再分析数据上训练，预测多种大气和地表变量。

Result: 数据集提供每日15天预测，覆盖37个垂直层次，计算速度快（1分钟内完成）。

Conclusion: GraphCast模型高效且准确，为全球天气预测提供了有价值的长期数据集。

Abstract: The UT GraphCast Hindcast Dataset from 1979 to 2024 is a comprehensive global
weather forecast archive generated using the Google DeepMind GraphCast
Operational model. Developed by researchers at The University of Texas at
Austin under the WCRP umbrella, this dataset provides daily 15 day
deterministic forecasts at 00UTC on an approximately 25 km global grid for a 45
year period. GraphCast is a physics informed graph neural network that was
trained on ECMWF ERA5 reanalysis. It predicts more than a dozen key atmospheric
and surface variables on 37 vertical levels, delivering a full medium range
forecast in under one minute on modern hardware.

</details>


### [271] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: Pix2Geomodel是一种基于cGAN的新框架，用于从复杂地质数据中预测储层属性，与传统方法相比具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统地质建模方法难以处理复杂的地下异质性和观测数据条件，需要更高效准确的解决方案。

Method: 采用Pix2Pix的cGAN框架，结合U-Net生成器和PatchGAN判别器，利用760万细胞数据集进行训练和评估。

Result: 在岩相和水饱和度预测中表现出高准确性（PA 0.88-0.96，FWIoU 0.85-0.95），孔隙度和渗透率预测中等成功。

Conclusion: Pix2Geomodel提升了地质建模的保真度，未来需解决微结构变异性和2D限制，推动生成式AI在地学中的应用。

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [272] [Numerical simulation of transient heat conduction with moving heat source using Physics Informed Neural Networks](https://arxiv.org/abs/2506.17726)
*Anirudh Kalyan,Sundararajan Natarajan*

Main category: math.NA

TL;DR: 提出了一种基于物理信息神经网络（PINNs）和迁移学习的连续时间步进训练方法，用于模拟移动热源的热传递问题，计算效率高且结果与传统有限元方法一致。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟方法（如有限元）在计算移动热源的热传递问题时计算成本高，需要一种更高效的方法。

Method: 采用PINNs，通过迁移学习将时间区间划分为小段，逐段训练单个网络，利用前一区间的解作为下一区间的初始条件。

Result: 提出的框架能高效计算大时间区间，温度分布结果与传统有限元方法吻合良好。

Conclusion: 该方法为移动热源问题提供了一种高效且准确的数值模拟方案。

Abstract: In this paper, the physics informed neural networks (PINNs) is employed for
the numerical simulation of heat transfer involving a moving source. To reduce
the computational effort, a new training method is proposed that uses a
continuous time-stepping through transfer learning. Within this, the time
interval is divided into smaller intervals and a single network is initialized.
On this single network each time interval is trained with the initial condition
for (n+1)th as the solution obtained at nth time increment. Thus, this
framework enables the computation of large temporal intervals without
increasing the complexity of the network itself. The proposed framework is used
to estimate the temperature distribution in a homogeneous medium with a moving
heat source. The results from the proposed framework is compared with
traditional finite element method and a good agreement is seen.

</details>


### [273] [DPG loss functions for learning parameter-to-solution maps by neural networks](https://arxiv.org/abs/2506.18773)
*Pablo Cortés Castillo,Wolfgang Dahmen,Jay Gopalakrishnan*

Main category: math.NA

TL;DR: 论文提出了一种基于残差的损失函数，用于参数依赖偏微分方程（PDEs）的机器学习建模，并通过变分正确的损失函数实现精度认证。


<details>
  <summary>Details</summary>
Motivation: 提升深度神经网络降阶模型的预测能力，特别是在高对比度扩散场等复杂情况下。

Method: 采用超弱间断Petrov-Galerkin（DPG）离散化方法，构建变分正确的损失函数。

Result: 数值和理论结果表明，DPG损失函数在高对比度扩散参数下比简单最小二乘损失更鲁棒。

Conclusion: 该方法适用于更广泛的问题，尤其是存在稳定DPG公式的问题。

Abstract: We develop, analyze, and experimentally explore residual-based loss functions
for machine learning of parameter-to-solution maps in the context of
parameter-dependent families of partial differential equations (PDEs). Our
primary concern is on rigorous accuracy certification to enhance prediction
capability of resulting deep neural network reduced models. This is achieved by
the use of variationally correct loss functions. Through one specific example
of an elliptic PDE, details for establishing the variational correctness of a
loss function from an ultraweak Discontinuous Petrov Galerkin (DPG)
discretization are worked out. Despite the focus on the example, the proposed
concepts apply to a much wider scope of problems, namely problems for which
stable DPG formulations are available. The issue of {high-contrast} diffusion
fields and ensuing difficulties with degrading ellipticity are discussed. Both
numerical results and theoretical arguments illustrate that for high-contrast
diffusion parameters the proposed DPG loss functions deliver much more robust
performance than simpler least-squares losses.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [274] [AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking](https://arxiv.org/abs/2506.17857)
*Chunan Liu,Aurelien Pelissier,Yanjun Shao,Lilian Denzler,Andrew C. R. Martin,Brooks Paige,Mariia Rodriguez Martinez*

Main category: q-bio.BM

TL;DR: AbRank是一个用于抗体-抗原结合亲和力预测的大规模基准和评估框架，通过将问题转化为成对排序任务，提高了模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前抗体-抗原结合亲和力预测模型的性能受限于实验标签噪声、异质性实验条件以及泛化能力不足。AbRank旨在解决这些问题，为机器学习模型提供更可靠的基准。

Method: AbRank整合了来自9个来源的38万多个结合实验数据，并引入标准化数据分割以系统化分布偏移。采用m-confident排序框架过滤低置信度比较，并提出了基于图的方法WALLE-Affinity作为基准。

Result: 基准测试显示当前方法在真实泛化场景下存在显著局限性，而基于排序的训练提高了鲁棒性和可迁移性。

Conclusion: AbRank为机器学习模型在抗体-抗原空间中的泛化提供了坚实基础，对可扩展、结构感知的抗体治疗设计具有直接意义。

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential
for therapeutic design and vaccine development, yet the performance of current
models is limited by noisy experimental labels, heterogeneous assay conditions,
and poor generalization across the vast antibody and antigen sequence space. We
introduce AbRank, a large-scale benchmark and evaluation framework that
reframes affinity prediction as a pairwise ranking problem. AbRank aggregates
over 380,000 binding assays from nine heterogeneous sources, spanning diverse
antibodies, antigens, and experimental conditions, and introduces standardized
data splits that systematically increase distribution shift, from local
perturbations such as point mutations to broad generalization across novel
antigens and antibodies. To ensure robust supervision, AbRank defines an
m-confident ranking framework by filtering out comparisons with marginal
affinity differences, focusing training on pairs with at least an m-fold
difference in measured binding strength. As a baseline for the benchmark, we
introduce WALLE-Affinity, a graph-based approach that integrates protein
language model embeddings with structural information to predict pairwise
binding preferences. Our benchmarks reveal significant limitations in current
methods under realistic generalization settings and demonstrate that
ranking-based training improves robustness and transferability. In summary,
AbRank offers a robust foundation for machine learning models to generalize
across the antibody-antigen space, with direct relevance for scalable,
structure-aware antibody therapeutic design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [275] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的智能日志处理与自动调试框架LLM-ID，通过多阶段语义推理和强化学习策略，显著提升了云平台故障定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统规模和复杂度的增加，日志数据的海量、非结构化和语义模糊性给故障定位和系统自修复带来了巨大挑战。

Method: 扩展预训练的Transformer模型，结合多阶段语义推理机制，动态结构化日志，利用无监督聚类和嵌入提取事件模板与语义模式，再通过微调LLM和多轮注意力机制进行上下文推理，生成故障假设与根因路径，并引入基于强化学习的策略引导恢复规划器。

Result: 实验表明，LLM-ID在云平台日志数据集上将故障定位准确率提高了16.2%，显著优于现有主流方法。

Conclusion: LLM-ID框架具备更强的语义理解、持续学习和异构环境适应能力，为云平台故障定位与自修复提供了高效解决方案。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [276] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: 提出了一种动态细化MDP的方法，加速大规模MDP中的策略合成，显著优于现有工具PRISM。


<details>
  <summary>Details</summary>
Motivation: 传统策略合成方法难以处理大规模状态空间，需要更高效的解决方案。

Method: 通过动态细化MDP并迭代选择最脆弱区域进行优化，平衡准确性与效率。

Result: 在多达1M状态的MDP中，性能提升高达2倍。

Conclusion: 该方法为大规模MDP中的策略合成提供了高效且实用的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [277] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: 比较小语言模型在少样本提示和监督微调下的泛化能力，分析其在分布内和分布外设置中的表现。


<details>
  <summary>Details</summary>
Motivation: 探究少样本提示和监督微调在低资源设置和分布变化下的鲁棒性差异。

Method: 通过任务格式、提示风格和模型规模的比较研究，分析内部表示。

Result: 发现不同适应策略下小模型内化和泛化知识的显著差异。

Conclusion: 为低数据场景提供模型选择指导，并贡献提示与微调辩论的实证见解。

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [278] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: 论文提出了一种基于结构因果模型（SCM）的个体因果推理（ICI）方法，通过个体化操作符（indiv(W)）和个体因果查询（P(Y | indiv(W), do(X), Z)）来估计个体因果效应（ICE）。


<details>
  <summary>Details</summary>
Motivation: 传统因果推理方法多为基于群体的，难以直接应用于个体层面。本文旨在利用SCM中的外生变量（U）编码个体差异，实现个体化的因果推理。

Method: 提出indiv(W)操作符和个体因果查询P(Y | indiv(W), do(X), Z)，通过SCM实现个体化的因果推理。

Result: 证明了基于SCM的ICI是对个体可能性的推理，而非反事实推理。

Conclusion: SCM为个体因果推理提供了可行的理论基础和方法框架，扩展了因果推理的应用范围。

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [279] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: 论文探讨了医疗AI系统在现实环境中性能下降的原因及解决方案，强调持续监控和自校正机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在动态临床环境中性能可能下降，影响可靠性和安全性，需研究如何维持其长期稳定性。

Method: 综述了性能下降的常见原因、检测数据与模型漂移的技术、根因分析及校正策略（如模型重训练和测试时适应）。

Result: 总结了传统机器学习和大型语言模型在医疗AI中的优缺点，并提出了未来研究方向。

Conclusion: 为开发可靠、稳健的医疗AI系统提供了指导，以支持其在动态临床环境中的长期安全部署。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [280] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: 论文探讨如何通过改进训练过程使语言模型在无需推理时检索的情况下可靠地引用预训练文档，提出Active Indexing方法，显著提升引用精度。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的引用不可靠且依赖外部检索器，导致延迟和基础设施依赖，研究旨在解决这一问题。

Method: 采用两阶段方法：持续预训练将事实绑定到文档标识符，指令微调引导引用行为；提出Active Indexing，通过合成QA对增强训练。

Result: Active Indexing在Qwen2.5-7B和3B上表现优于Passive Indexing，引用精度提升高达30.2%。

Conclusion: Active Indexing方法有效提升模型引用可靠性，且性能随数据增强规模持续提升。

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [281] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: 论文提出了一种混合推理框架，结合结构化概率模型和LLM，提升了语言模型在社交推理游戏中的表现，并首次在实验中击败人类玩家。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在社交推理任务（如推断不可观察的信念和意图）中表现有限，尤其是在小型实时模型中表现显著下降。

Method: 引入混合推理框架，将信念推断任务外化到结构化概率模型，同时利用LLM处理语言理解和交互。

Result: 该方法在Agent-Agent对抗中表现优异，首次在控制实验中击败人类玩家，胜率达67%，并获得更高的定性评价。

Conclusion: 混合推理框架为LLM在社交推理任务中的应用提供了新方向，支持未来研究。

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [282] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: 提出了一种新的提示设计范式，通过修剪随机演示为看似无意义的“胡言乱语”来提升大语言模型（LLM）性能，并提出了自动优化框架PromptQuine。


<details>
  <summary>Details</summary>
Motivation: 挑战传统提示设计，探索更高效的提示优化方法。

Method: 提出PromptQuine框架，通过进化搜索自动寻找修剪策略。

Result: 在多种任务中表现优于现有技术，且运行效率高。

Conclusion: 为上下文学习机制研究提供新方向，并呼吁开发更开放的搜索算法。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [283] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: 该研究量化了精神分裂症患者不坚持服用抗精神病药物与不良后果的关联，使用生存分析和因果推断方法，发现不坚持用药会提前1至4个月引发不良事件。


<details>
  <summary>Details</summary>
Motivation: 研究旨在明确不坚持用药对精神分裂症患者不良后果的影响，为临床和政策提供依据。

Method: 采用生存分析框架，结合T-learner、S-learner和最近邻匹配等因果推断方法，利用不同时间段的纵向数据（3、6、9、12个月）进行分析。

Result: 研究发现不坚持用药会显著提前不良事件发生时间（1至4个月），且不同药物类型和剂型的结果一致。

Conclusion: 研究强调了坚持用药对延缓精神病危机的重要性，并展示了生存分析与因果推断结合的政策意义，但需注意因果解释的假设限制。

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [284] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: 研究发现，模型编辑后的行为在微调过程中通常无法持久，甚至与编辑无关的微调也会导致编辑失效，这对AI安全具有双重影响。


<details>
  <summary>Details</summary>
Motivation: 探讨模型编辑与微调之间的交互作用，以解决编辑行为是否能在微调后持久存在的问题，这对AI安全（如恶意编辑防御和偏见缓解）至关重要。

Method: 在T2I扩散模型（Stable Diffusion和FLUX）中，结合两种编辑技术（UCE和ReFACT）和三种微调方法（DreamBooth、LoRA和DoRA），通过多样化的编辑任务和评估指标进行实证分析。

Result: 编辑行为在微调后普遍失效，其中DoRA的编辑反转效果最强；UCE相比ReFACT在微调后保留更高的编辑效果。

Conclusion: 当前编辑方法存在局限性，需开发更鲁棒的技术以确保长期控制和AI系统对齐；微调可作为恶意编辑的补救手段，但需重新编辑以维持安全性和对齐性。

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [285] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: 论文提出Programming by Backprop (PBB)方法，通过仅训练源代码（无输入输出示例）提升大语言模型（LLMs）的通用推理能力，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs通过源代码训练提升推理能力的机制，尤其是无输入输出示例时的表现。

Method: 在两组程序（含输入输出示例和仅源代码）上微调LLMs，比较其表现，并分析PBB的效果。

Result: LLMs能通过源代码隐式评估程序，PBB在代码形式下效果更佳，且比基于输入输出对的训练更稳健。

Conclusion: 源代码训练使LLMs内化可重用算法抽象，未来可进一步优化符号化学习，拓展模型对齐等应用。

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [286] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Main category: cs.AI

TL;DR: 通过激活语言模型中的潜在子空间，引导科学代码生成偏向特定编程语言，提出G-ACT框架，显著提升语言选择准确性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过干预语言模型的内部机制，使其在科学代码生成中更倾向于特定编程语言。

Method: 使用静态神经元归因方法初步评估，随后开发梯度优化的自适应激活引导框架（G-ACT），通过聚类和轻量级探针选择引导方向。

Result: 在LLaMA-3.2 3B中，G-ACT将平均探针分类准确率提升15%，早期层提升61.5%；在LLaMA-3.3 70B中，关键层干预仍有效。

Conclusion: G-ACT提供了一种可扩展、可解释且高效的概念级控制机制，适用于实际代理系统。

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [287] [Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging](https://arxiv.org/abs/2506.18317)
*Emerson Sie,Enguang Fan,Federico Cifuentes-Urtubey,Deepak Vasisht*

Main category: cs.HC

TL;DR: PeepLoc是一种可部署且可扩展的基于Wi-Fi的室内定位解决方案，利用现有设备和基础设施，无需修改硬件即可实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有室内定位方法实用性不足，难以广泛部署，PeepLoc旨在提供一种无需额外硬件支持的实用解决方案。

Method: PeepLoc通过非协作飞行时间（ToF）测量和基于行人航位推算（PDR）的众包机制，利用现有Wi-Fi接入点（APs）实现定位。

Result: 在4个校园建筑中的实验表明，PeepLoc的平均和中位定位误差分别为3.41米和3.06米，优于现有系统，接近室外GPS性能。

Conclusion: PeepLoc展示了利用现有Wi-Fi基础设施实现高精度室内定位的可行性，具有实际部署潜力。

Abstract: Indoor localization opens the path to potentially transformative
applications. Although many indoor localization methods have been proposed over
the years, they remain too impractical for widespread deployment in the real
world. In this paper, we introduce PeepLoc, a deployable and scalable
Wi-Fi-based solution for indoor localization that relies only on pre-existing
devices and infrastructure. Specifically, PeepLoc works on any mobile device
with an unmodified Wi-Fi transceiver and in any indoor environment with a
sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the
core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain
non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel
bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and
crowdsourcing to opportunistically initialize pre-existing APs as anchor points
within an environment. We implement PeepLoc using commodity hardware and
evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a
mean and median positional error of 3.41 m and 3.06 m respectively, which is
superior to existing deployed indoor localization systems and is competitive
with commodity GPS in outdoor environments.

</details>


### [288] [AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms](https://arxiv.org/abs/2506.18727)
*Xingyu Xiao,Jiejuan Tong,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: AutoGraph是一个基于知识图谱的框架，用于数字化核电厂控制室中程序执行的自动化和形式化，旨在减少人为错误并支持智能自动化。


<details>
  <summary>Details</summary>
Motivation: 现有计算机化程序（CBPs）与人机界面（HSIs）缺乏语义集成，限制了智能自动化的能力，增加了复杂操作条件下的人为错误风险。

Method: AutoGraph整合了HTRPM跟踪模块、IE-KG知识图谱、文本程序到可执行路径的自动映射以及执行引擎。

Result: 验证表明，该框架显著减少了任务完成时间，并支持实时人类可靠性评估。

Conclusion: AutoGraph可扩展性高，有望提升复杂社会技术系统中的程序安全性和认知性能。

Abstract: Digitalization in nuclear power plant (NPP) control rooms is reshaping how
operators interact with procedures and interface elements. However, existing
computer-based procedures (CBPs) often lack semantic integration with
human-system interfaces (HSIs), limiting their capacity to support intelligent
automation and increasing the risk of human error, particularly under dynamic
or complex operating conditions. In this study, we present AutoGraph, a
knowledge-graph-based framework designed to formalize and automate procedure
execution in digitalized NPP environments.AutoGraph integrates (1) a proposed
HTRPM tracking module to capture operator interactions and interface element
locations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,
semantic, and structural properties of HSIs; (3) automatic mapping from textual
procedures to executable interface paths; and (4) an execution engine that maps
textual procedures to executable interface paths. This enables the
identification of cognitively demanding multi-action steps and supports fully
automated execution with minimal operator input. We validate the framework
through representative control room scenarios, demonstrating significant
reductions in task completion time and the potential to support real-time human
reliability assessment. Further integration into dynamic HRA frameworks (e.g.,
COGMIF) and real-time decision support systems (e.g., DRIF) illustrates
AutoGraph extensibility in enhancing procedural safety and cognitive
performance in complex socio-technical systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [289] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen利用生成模型修正高斯动态场景表示中的错误并补全缺失信息，通过扩散模型生成帧监督优化4D高斯模型，实现新视角合成和2D/3D跟踪任务的最佳效果。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中重建动态场景存在未观测区域重建和深度估计模糊等挑战，需要一种方法纠正错误并补全信息。

Method: 结合生成模型（扩散模型）与高斯动态场景表示，生成帧用于监督优化4D高斯模型。

Result: 在新型视角合成和2D/3D跟踪任务中取得最佳效果。

Conclusion: BulletGen通过生成模型与动态场景表示的结合，有效解决了单目视频动态场景重建的挑战。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [290] [BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity](https://arxiv.org/abs/2506.18314)
*Moein Khajehnejad,Forough Habibollahi,Adeel Razi*

Main category: q-bio.QM

TL;DR: BrainSymphony是一种轻量级、参数高效的基础模型，用于神经影像分析，性能优于现有大型模型，且训练数据需求更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经影像基础模型体积庞大、数据需求高的问题。

Method: 采用多模态架构，通过并行空间和时间Transformer流处理功能MRI数据，并结合扩散MRI的图Transformer建模结构连接。

Result: 在多种下游任务中表现优于大型模型，并揭示了脑动力学的新的见解。

Conclusion: BrainSymphony证明轻量级多模态模型可以超越大型模型，推动计算神经科学的可访问性和研究能力。

Abstract: Existing foundation models for neuroimaging are often prohibitively large and
data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient
foundation model that achieves state-of-the-art performance while being
pre-trained on significantly smaller public datasets. BrainSymphony's strong
multimodal architecture processes functional MRI data through parallel spatial
and temporal transformer streams, which are then efficiently distilled into a
unified representation by a Perceiver module. Concurrently, it models
structural connectivity from diffusion MRI using a novel signed graph
transformer to encode the brain's anatomical structure. These powerful,
modality-specific representations are then integrated via an adaptive fusion
gate. Despite its compact design, our model consistently outperforms larger
models on a diverse range of downstream benchmarks, including classification,
prediction, and unsupervised network identification tasks. Furthermore, our
model revealed novel insights into brain dynamics using attention maps on a
unique external psilocybin neuroimaging dataset (pre- and post-administration).
BrainSymphony establishes that architecturally-aware, multimodal models can
surpass their larger counterparts, paving the way for more accessible and
powerful research in computational neuroscience.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [291] [Semidefinite Programming for the Asymmetric Stochastic Block Model](https://arxiv.org/abs/2506.18754)
*Julia Gaudio,Phawin Prongpaophan*

Main category: cs.IT

TL;DR: 论文研究了对称SDP在非对称块模型中的失败原因，并提出了一种新的SDP设计，但分析仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨SDP是否能在非对称块模型中实现精确恢复，填补对称SDP在非对称情况下的局限性。

Method: 分析对称SDP在非对称情况下的失败，提出新的SDP设计。

Result: 对称SDP在非对称情况下失败，新SDP设计存在分析困难。

Conclusion: 非对称块模型的SDP设计仍面临根本性挑战。

Abstract: We consider semidefinite programming (SDP) for the binary stochastic block
model with equal-sized communities. Prior work of Hajek, Wu, and Xu proposed an
SDP (sym-SDP) for the symmetric case where the intra-community edge
probabilities are equal, and showed that the SDP achieves the
information-theoretic threshold for exact recovery under the symmetry
assumption. A key open question is whether SDPs can be used to achieve exact
recovery for non-symmetric block models. In order to inform the design of a new
SDP for the non-symmetric setting, we investigate the failure of sym-SDP when
it is applied to non-symmetric settings. We formally show that sym-SDP fails to
return the correct labeling of the vertices in some information-theoretically
feasible, asymmetric cases. In addition, we give an intuitive geometric
interpretation of the failure of sym-SDP in asymmetric settings, which in turn
suggests an SDP formulation to handle the asymmetric setting. Still, this new
SDP cannot be readily analyzed by existing techniques, suggesting a fundamental
limitation in the design of SDPs for community detection.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [292] [JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering](https://arxiv.org/abs/2506.17713)
*Piyush Pradhan,Pierre Gentine,Shaina Kelly*

Main category: physics.comp-ph

TL;DR: JAX-LaB是一个基于Python的可微分Lattice Boltzmann库，用于模拟水文、地质和工程多孔介质中的多相和多物理流动。它利用JAX实现高性能计算，支持多种硬件，并与机器学习工作流无缝集成。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效、硬件无关的库，用于模拟多相和多物理流动，同时支持机器学习应用。

Method: 基于XLB库扩展，采用Shan-Chen伪势方法模拟多相相互作用，结合改进的力方案和虚拟密度方案处理润湿现象。

Result: 通过多个分析基准验证了库的准确性，展示了其在单GPU和多GPU上的性能扩展。

Conclusion: JAX-LaB是一个开源、高性能的库，适用于复杂流动模拟，并支持机器学习集成。

Abstract: We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library
for simulating multiphase and multiphysics flows in hydrologic, geologic, and
engineered porous media. Built as an extension of the XLB library, JAX-LaB
utilizes JAX for computations and offers a performant, hardware-agnostic
implementation that integrates seamlessly with machine learning workflows and
scales efficiently across CPUs, GPUs, and distributed systems. Multiphase
interactions are modeled using the Shan-Chen pseudopotential method, which is
coupled with an equation of state and an improved forcing scheme to obtain
liquid-vapor densities that are consistent with Maxwell's construction,
enabling simulations of systems with very large density ratios while
maintaining minimal spurious currents. Wetting is handled using the "improved"
virtual density scheme, which allows precise control of contact angles and
eliminates non-physical films seen in other Shan-Chen wetting methods. We
validate the library through several analytical benchmarks, such as Laplace's
law, capillary rise, and cocurrent multicomponent flow, and demonstrate some
exemplary use cases for the library. We also report single- and multi-GPU
performance scaling of the library. The library is open-source under the Apache
license and available at https://github.com/piyush-ppradhan/JAX-LaB.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [293] [Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming](https://arxiv.org/abs/2506.17224)
*Zofia Pizoń,Shinji Kimijima,Grzegorz Brus*

Main category: cs.CE

TL;DR: 提出了一种统一动力学和平衡态的人工神经网络替代模型，用于甲烷蒸汽重整过程，具有高预测精度和实用性。


<details>
  <summary>Details</summary>
Motivation: 甲烷蒸汽重整是广泛使用的制氢技术，但现有模型仅适用于单一状态（动力学或平衡态），限制了其应用。

Method: 利用包含实验、插值和理论数据的综合数据集训练神经网络，通过数据增强和权重分配优化训练。

Result: 模型在预测反应后混合物组成上表现出高精度（均方误差0.000498，Pearson相关系数0.927），并能提供连续导数。

Conclusion: 该替代模型适用于动力学和平衡态，为甲烷蒸汽重整的设计和优化提供了有力工具。

Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for
efficient production, with methane steam reforming being the most widely used
technique. This process is crucial for applications like fuel cells, where
hydrogen is converted into electricity, pushing for reactor miniaturization and
optimized process control through numerical simulations. Existing models
typically address either kinetic or equilibrium regimes, limiting their
applicability. Here we show a surrogate model capable of unifying both regimes.
An artificial neural network trained on a comprehensive dataset that includes
experimental data from kinetic and equilibrium experiments, interpolated data,
and theoretical data derived from theoretical models for each regime. Data
augmentation and assigning appropriate weights to each data type enhanced
training. After evaluating Bayesian Optimization and Random Sampling, the
optimal model demonstrated high predictive accuracy for the composition of the
post-reaction mixture under varying operating parameters, indicated by a mean
squared error of 0.000498 and strong Pearson correlation coefficients of 0.927.
The network's ability to provide continuous derivatives of its predictions
makes it particularly useful for process modeling and optimization. The results
confirm the surrogate model's robustness for simulating methane steam reforming
in both kinetic and equilibrium regimes, making it a valuable tool for design
and process optimization.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [294] [Advanced Modeling for Exoplanet Detection and Characterization](https://arxiv.org/abs/2506.17665)
*Krishna Chamarthy*

Main category: astro-ph.EP

TL;DR: 利用开普勒数据集中的恒星光变曲线，结合机器学习方法，发现系外行星并估算其物理特性。


<details>
  <summary>Details</summary>
Motivation: 通过光变曲线分析提高系外行星发现的效率，并获取行星的物理参数。

Method: 分析恒星光变曲线中的周期性亮度下降，结合机器学习分类，估算行星的轨道周期、半径等参数。

Result: 能够快速筛选天文数据集，高效发现系外行星并获取其物理特性。

Conclusion: 该方法显著提升了系外行星发现的效率，并为行星特性研究提供了重要数据。

Abstract: Research into light curves from stars (temporal variation of brightness) has
completely changed how exoplanets are discovered or characterised. This study
including star light curves from the Kepler dataset as a way to discover
exoplanets (planetary transits) and derive some estimate of their physical
characteristics by the light curve and machine learning methods. The dataset
consists of measured flux (recordings) for many individual stars and we will
examine the light curve of each star and look for periodic dips in brightness
due to an astronomical body making a transit. We will apply variables derived
from an established method for deriving measurements from light curve data to
derive key parameters related to the planet we observed during the transit,
such as distance to the host star, orbital period, radius. The orbital period
will typically be measured based on the time between transit of the subsequent
timelines and the radius will be measured based on the depth of transit. The
density of the star and planet can also be estimated from the transit event, as
well as very limited information on the albedo (reflectivity) and atmosphere of
the planet based on transmission spectroscopy and/or the analysis of phase
curve for levels of flux. In addition to these methods, we will employ some
machine learning classification of the stars (i.e. likely have an exoplanet or
likely do not have an exoplanet) based on flux change. This could help fulfil
both the process of looking for exoplanets more efficient as well as providing
important parameters for the planet. This will provide a much quicker means of
searching the vast astronomical datasets for the likelihood of exoplanets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [295] [A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions](https://arxiv.org/abs/2506.17285)
*Vinaik Chhetri,Yousaf Reza,Moghis Fereidouni,Srijata Maji,Umar Farooq,AB Siddique*

Main category: cs.IR

TL;DR: ConvRecStudio框架结合协作过滤和对话推荐系统，利用LLM模拟真实多轮对话，提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统缺乏交互性，而对话推荐系统缺乏协作信号，两者结合可提供更丰富的个性化推荐。

Method: ConvRecStudio采用三阶段流程：时间分析、语义对话规划和多轮模拟，生成基于真实用户行为的对话数据。

Result: 生成12K多轮对话，模型在Hit@K和NDCG@K指标上优于基线，Yelp上Hit@1提升10.9%。

Conclusion: ConvRecStudio成功统一两种推荐范式，为个性化推荐提供了新方向。

Abstract: Modern recommendation systems typically follow two complementary paradigms:
collaborative filtering, which models long-term user preferences from
historical interactions, and conversational recommendation systems (CRS), which
interact with users in natural language to uncover immediate needs. Each
captures a different dimension of user intent. While CRS models lack
collaborative signals, leading to generic or poorly personalized suggestions,
traditional recommenders lack mechanisms to interactively elicit immediate
needs. Unifying these paradigms promises richer personalization but remains
challenging due to the lack of large-scale conversational datasets grounded in
real user behavior. We present ConvRecStudio, a framework that uses large
language models (LLMs) to simulate realistic, multi-turn dialogs grounded in
timestamped user-item interactions and reviews. ConvRecStudio follows a
three-stage pipeline: (1) Temporal Profiling, which constructs user profiles
and community-level item sentiment trajectories over fine-grained aspects; (2)
Semantic Dialog Planning, which generates a structured plan using a DAG of
flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the
plan using paired LLM agents for the user and system, constrained by
executional and behavioral fidelity checks. We apply ConvRecStudio to three
domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K
multi-turn dialogs per dataset. Human and automatic evaluations confirm the
naturalness, coherence, and behavioral grounding of the generated
conversations. To demonstrate utility, we build a cross-attention transformer
model that jointly encodes user history and dialog context, achieving gains in
Hit@K and NDCG@K over baselines using either signal alone or naive fusion.
Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the
strongest baseline.

</details>


### [296] [Recommendation systems in e-commerce applications with machine learning methods](https://arxiv.org/abs/2506.17287)
*Aneta Poniszewska-Maranda,Magdalena Pakula,Bozena Borowska*

Main category: cs.IR

TL;DR: 论文综述了电子商务推荐系统的现状、挑战及机器学习方法的有效性，通过系统文献回顾分析了38篇2013-2025年的文献。


<details>
  <summary>Details</summary>
Motivation: 提升电子商务平台的用户体验、客户留存和销售，研究机器学习方法在推荐系统中的整合效果。

Method: 采用系统文献回顾（SLR）方法，分析38篇相关文献，评估协作过滤、内容过滤和混合模型等方法。

Result: 比较了不同方法的性能，评估了它们在解决电子商务挑战中的有效性。

Conclusion: 论文总结了当前趋势、挑战及机器学习方法在推荐系统中的表现，为未来研究提供了方向。

Abstract: E-commerce platforms are increasingly reliant on recommendation systems to
enhance user experience, retain customers, and, in most cases, drive sales. The
integration of machine learning methods into these systems has significantly
improved their efficiency, personalization, and scalability. This paper aims to
highlight the current trends in e-commerce recommendation systems, identify
challenges, and evaluate the effectiveness of various machine learning methods
used, including collaborative filtering, content-based filtering, and hybrid
models. A systematic literature review (SLR) was conducted, analyzing 38
publications from 2013 to 2025. The methods used were evaluated and compared to
determine their performance and effectiveness in addressing e-commerce
challenges.

</details>


### [297] [PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching](https://arxiv.org/abs/2506.18382)
*Haotong Du,Yaqing Wang,Fei Xiong,Lei Shao,Ming Liu,Hao Gu,Quanming Yao,Zhen Wang*

Main category: cs.IR

TL;DR: PERSCEN是一种创新的多场景匹配方法，通过用户特定建模和图神经网络捕捉用户偏好，结合向量量化技术提取场景感知偏好，显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 随着在线平台业务规模和范围的扩大，多场景匹配成为降低维护成本和缓解数据稀疏的主流解决方案。现有方法常忽略用户特定建模，限制了用户个性化表示。

Method: PERSCEN构建用户特定特征图，利用轻量级图神经网络捕捉高阶交互模式，并通过向量量化技术提取场景感知偏好。引入渐进式场景感知门控线性单元实现高效信息融合。

Result: 实验表明PERSCEN优于现有方法，且在性能和计算成本之间取得平衡，适用于实际工业系统。

Conclusion: PERSCEN通过用户特定和场景感知建模，显著提升了多场景推荐的个性化和效率。

Abstract: With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [298] [Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation](https://arxiv.org/abs/2506.17409)
*Quoc Thinh Vo,Joe Woods,Priontu Chowdhury,David K. Han*

Main category: cs.SD

TL;DR: 提出一种多分支网络架构，结合CNN和Conformer，用于水下声源定位，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，背景噪声高、几何不规则且声学特性多变，导致声源定位困难。

Method: 使用CNN提取空间特征，Conformer捕捉时间依赖，输入为log-mel谱和GCC-PHAT特征，引入AGC层调整输入幅度。

Result: 在跨域测试中表现优异，优于现有方法。

Conclusion: 该方法为水下声源定位提供了新的解决方案，性能显著提升。

Abstract: Localizing acoustic sound sources in the ocean is a challenging task due to
the complex and dynamic nature of the environment. Factors such as high
background noise, irregular underwater geometries, and varying acoustic
properties make accurate localization difficult. To address these obstacles, we
propose a multi-branch network architecture designed to accurately predict the
distance between a moving acoustic source and a receiver, tested on real-world
underwater signal arrays. The network leverages Convolutional Neural Networks
(CNNs) for robust spatial feature extraction and integrates Conformers with
self-attention mechanism to effectively capture temporal dependencies. Log-mel
spectrogram and generalized cross-correlation with phase transform (GCC-PHAT)
features are employed as input representations. To further enhance the model
performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively
adjusts the amplitude of input features, ensuring consistent energy levels
across varying ranges, signal strengths, and noise conditions. We assess the
model's generalization capability by training it in one domain and testing it
in a different domain, using only a limited amount of data from the test domain
for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)
approaches in similar settings, establishing new benchmarks for underwater
sound localization.

</details>


### [299] [From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training](https://arxiv.org/abs/2506.17497)
*Mingyang Yao,Ke Chen*

Main category: cs.SD

TL;DR: 论文提出了一种两阶段训练方法，通过预训练和微调提升作曲家风格的音乐生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决作曲家风格音乐生成中数据稀缺的问题，探索通用音乐知识如何增强特定风格建模。

Method: 采用两阶段训练：先在大规模流行、民谣和古典音乐上预训练REMI模型，再用轻量适配器模块在四位作曲家的小数据集上微调。

Result: 实验表明，该方法在风格准确性和音乐性上优于基线，实现了更精确的风格建模和更好的音乐美学。

Conclusion: 通用预训练和风格微调结合能有效提升作曲家风格的音乐生成质量。

Abstract: Despite progress in controllable symbolic music generation, data scarcity
remains a challenge for certain control modalities. Composer-style music
generation is a prime example, as only a few pieces per composer are available,
limiting the modeling of both styles and fundamental music elements (e.g.,
melody, chord, rhythm). In this paper, we investigate how general music
knowledge learned from a broad corpus can enhance the mastery of specific
composer styles, with a focus on piano piece generation. Our approach follows a
two-stage training paradigm. First, we pre-train a REMI-based music generation
model on a large corpus of pop, folk, and classical music. Then, we fine-tune
it on a small, human-verified dataset from four renowned composers, namely
Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to
condition the model on style indicators. To evaluate the effectiveness of our
approach, we conduct both objective and subjective evaluations on style
accuracy and musicality. Experimental results demonstrate that our method
outperforms ablations and baselines, achieving more precise composer-style
modeling and better musical aesthetics. Additionally, we provide observations
on how the model builds music concepts from the generality pre-training and
refines its stylistic understanding through the mastery fine-tuning.

</details>


### [300] [CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning](https://arxiv.org/abs/2506.17818)
*Angelos-Nikolaos Kanatas,Charilaos Papaioannou,Alexandros Potamianos*

Main category: cs.SD

TL;DR: CultureMERT-95M是一个多文化适应的音乐基础模型，通过两阶段持续预训练策略提升跨文化音乐表示学习，在非西方音乐任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音乐基础模型在多样音乐传统中表现有限，需要提升跨文化音乐表示学习能力。

Method: 采用两阶段持续预训练策略，结合学习率调整，训练于650小时多文化音乐数据。

Result: 在非西方音乐自动标注任务中平均提升4.9%的ROC-AUC和AP，同时保持西方基准性能。

Conclusion: 多文化适应模型在跨文化音乐任务中表现最佳，并开源模型以促进研究。

Abstract: Recent advances in music foundation models have improved audio representation
learning, yet their effectiveness across diverse musical traditions remains
limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation
model developed to enhance cross-cultural music representation learning and
understanding. To achieve this, we propose a two-stage continual pre-training
strategy that integrates learning rate re-warming and re-decaying, enabling
stable adaptation even with limited computational resources. Training on a
650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music
traditions, results in an average improvement of 4.9% in ROC-AUC and AP across
diverse non-Western music auto-tagging tasks, surpassing prior
state-of-the-art, with minimal forgetting on Western-centric benchmarks. We
further investigate task arithmetic, an alternative approach to multi-cultural
adaptation that merges single-culture adapted models in the weight space. Task
arithmetic performs on par with our multi-culturally trained model on
non-Western auto-tagging tasks and shows no regression on Western datasets.
Cross-cultural evaluation reveals that single-culture models transfer with
varying effectiveness across musical traditions, whereas the multi-culturally
adapted model achieves the best overall performance. To support research on
world music representation learning, we publicly release CultureMERT-95M and
CultureMERT-TA-95M, fostering the development of more culturally aware music
foundation models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [301] [Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis](https://arxiv.org/abs/2506.17740)
*Pengyu Han,Zeyi Liu,Shijin Chen,Dongliang Zou,Xiao He*

Main category: eess.SP

TL;DR: 该论文研究了多工况故障诊断中操作条件对故障信息的影响，并提出了一种两阶段诊断框架以提高性能。


<details>
  <summary>Details</summary>
Motivation: 多工况故障诊断中，操作条件对故障信息的影响尚未充分研究，而现有方法可能因条件特异性信息而降低泛化能力。

Method: 提出了一种两阶段诊断框架，结合域泛化编码器和重训练策略，提取条件不变的故障特征。

Result: 通过真实齿轮箱数据集实验验证了该方法的有效性。

Conclusion: 该框架能有效提取条件不变特征并缓解源域过拟合，提升多工况故障诊断性能。

Abstract: Multi-condition fault diagnosis is prevalent in industrial systems and
presents substantial challenges for conventional diagnostic approaches. The
discrepancy in data distributions across different operating conditions
degrades model performance when a model trained under one condition is applied
to others. With the recent advancements in deep learning, transfer learning has
been introduced to the fault diagnosis field as a paradigm for addressing
multi-condition fault diagnosis. Among these methods, domain generalization
approaches can handle complex scenarios by extracting condition-invariant fault
features. Although many studies have considered fault diagnosis in specific
multi-condition scenarios, the extent to which operating conditions affect
fault information has been scarcely studied, which is crucial. However, the
extent to which operating conditions affect fault information has been scarcely
studied, which is crucial. When operating conditions have a significant impact
on fault features, directly applying domain generalization methods may lead the
model to learn condition-specific information, thereby reducing its overall
generalization ability. This paper investigates the performance of existing
end-to-end domain generalization methods under varying conditions, specifically
in variable-speed and variable-load scenarios, using multiple experiments on a
real-world gearbox. Additionally, a two-stage diagnostic framework is proposed,
aiming to improve fault diagnosis performance under scenarios with significant
operating condition impacts. By incorporating a domain-generalized encoder with
a retraining strategy, the framework is able to extract condition-invariant
fault features while simultaneously alleviating potential overfitting to the
source domain. Several experiments on a real-world gearbox dataset are
conducted to validate the effectiveness of the proposed approach.

</details>


### [302] [Fast State-Augmented Learning for Wireless Resource Allocation with Dual Variable Regression](https://arxiv.org/abs/2506.18748)
*Yigit Berkay Uslu,Navid NaderiAlizadeh,Mark Eisen,Alejandro Ribeiro*

Main category: eess.SP

TL;DR: 提出一种基于状态增强图神经网络（GNN）的资源分配方法，通过动态输入对偶变量优化网络性能，避免了传统对偶次梯度方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 解决多用户无线网络中资源分配问题，优化网络效用函数并满足用户平均性能约束。

Method: 使用状态增强GNN参数化资源分配策略，将对偶变量作为动态输入，并通过离线训练学习策略，在线推理时更新对偶变量。

Result: 在发射功率控制的案例中展示了算法的优越性能，并提供了收敛性和对偶函数最优性间隙的概率界限证明。

Conclusion: 提出的方法显著提升了资源分配的性能和训练效率，为相关领域提供了新的解决方案。

Abstract: We consider resource allocation problems in multi-user wireless networks,
where the goal is to optimize a network-wide utility function subject to
constraints on the ergodic average performance of users. We demonstrate how a
state-augmented graph neural network (GNN) parametrization for the resource
allocation policy circumvents the drawbacks of the ubiquitous dual subgradient
methods by representing the network configurations (or states) as graphs and
viewing dual variables as dynamic inputs to the model, viewed as graph signals
supported over the graphs. Lagrangian maximizing state-augmented policies are
learned during the offline training phase, and the dual variables evolve
through gradient updates while executing the learned state-augmented policies
during the inference phase. Our main contributions are to illustrate how
near-optimal initialization of dual multipliers for faster inference can be
accomplished with dual variable regression, leveraging a secondary GNN
parametrization, and how maximization of the Lagrangian over the multipliers
sampled from the dual descent dynamics substantially improves the training of
state-augmented models. We demonstrate the superior performance of the proposed
algorithm with extensive numerical experiments in a case study of transmit
power control. Finally, we prove a convergence result and an exponential
probability bound on the excursions of the dual function (iterate) optimality
gaps.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [303] [Universal Solvability for Robot Motion Planning on Graphs](https://arxiv.org/abs/2506.18755)
*Anubhav Dhar,Ashlesha Hota,Sudeshna Kolay,Pranav Nyati,Tanishq Prasad*

Main category: cs.CC

TL;DR: 研究了机器人运动规划在通用可解性问题（USolR）上的表现，提出了一种规范积累程序，分析了配置可达性，并设计了高效的随机和确定性算法。同时探讨了图边增强问题（EAUS）和顶点边增强问题（VEAUS），给出了相关边界。


<details>
  <summary>Details</summary>
Motivation: 解决机器人运动规划在任意配置转换中的通用可解性问题，并优化算法效率。

Method: 设计规范积累程序，分析配置可达性，提出随机和确定性算法，并研究图边和顶点增强问题。

Result: 证明了非通用可解实例中至少一半配置不可达，设计了高效算法，并给出了图增强问题的边界。

Conclusion: 通过规范积累程序和算法优化，解决了机器人运动规划的通用可解性问题，并提供了图增强问题的理论边界。

Abstract: We study the Universal Solvability of Robot Motion Planning on Graphs (USolR)
problem: given an undirected graph G = (V, E) and p robots, determine whether
any arbitrary configuration of the robots can be transformed into any other
arbitrary configuration via a sequence of valid, collision-free moves. We
design a canonical accumulation procedure that maps arbitrary configurations to
configurations that occupy a fixed subset of vertices, enabling us to analyze
configuration reachability in terms of equivalence classes. We prove that in
instances that are not universally solvable, at least half of all
configurations are unreachable from a given one, and leverage this to design an
efficient randomized algorithm with one-sided error, which can be derandomized
with a blow-up in the running time by a factor of p. Further, we optimize our
deterministic algorithm by using the structure of the input graph G = (V, E),
achieving a running time of O(p * (|V| + |E|)) in sparse graphs and O(|V| +
|E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for
Universal Solvability (EAUS) problem, where given a connected graph G that is
not universally solvable for p robots, the question is to check if for a given
budget b, at most b edges can be added to G to make it universally solvable for
p robots. We provide an upper bound of p - 2 on b for general graphs. On the
other hand, we also provide examples of graphs that require Theta(p) edges to
be added. We further study the Graph Vertex and Edge Augmentation for Universal
Solvability (VEAUS) problem, where a vertices and b edges can be added, and we
provide lower bounds on a and b.

</details>


### [304] [New Hardness Results for Low-Rank Matrix Completion](https://arxiv.org/abs/2506.18440)
*Dror Chawin,Ishay Haviv*

Main category: cs.CC

TL;DR: 该论文证明了低秩矩阵补全问题在某些约束条件下是NP难的，并扩展了之前的结果。


<details>
  <summary>Details</summary>
Motivation: 研究低秩矩阵补全问题的计算复杂性，特别是在满足正半定或有界无穷范数约束时的NP难性。

Method: 通过引入图的近似正交表示、线有向图概念以及对扰动单位矩阵秩的界限分析，证明了新的NP难性结果。

Result: 对于足够大的整数d和特定范围的ε，证明了在正半定和有界无穷范数约束下，低秩矩阵补全问题是NP难的。

Conclusion: 论文扩展了现有结果，为低秩矩阵补全问题的计算复杂性提供了更深入的理解。

Abstract: The low-rank matrix completion problem asks whether a given real matrix with
missing values can be completed so that the resulting matrix has low rank or is
close to a low-rank matrix. The completed matrix is often required to satisfy
additional structural constraints, such as positive semi-definiteness or a
bounded infinity norm. The problem arises in various research fields, including
machine learning, statistics, and theoretical computer science, and has broad
real-world applications.
  This paper presents new $\mathsf{NP} $-hardness results for low-rank matrix
completion problems. We show that for every sufficiently large integer $d$ and
any real number $\varepsilon \in [ 2^{-O(d)},\frac{1}{7}]$, given a partial
matrix $A$ with exposed values of magnitude at most $1$ that admits a positive
semi-definite completion of rank $d$, it is $\mathsf{NP}$-hard to find a
positive semi-definite matrix that agrees with each given value of $A$ up to an
additive error of at most $\varepsilon$, even when the rank is allowed to
exceed $d$ by a multiplicative factor of $O (\frac{1}{\varepsilon ^2 \cdot
\log(1/\varepsilon)} )$. This strengthens a result of Hardt, Meka, Raghavendra,
and Weitz (COLT, 2014), which applies to multiplicative factors smaller than
$2$ and to $\varepsilon $ that decays polynomially in $d$. We establish similar
$\mathsf{NP}$-hardness results for the case where the completed matrix is
constrained to have a bounded infinity norm (rather than be positive
semi-definite), for which all previous hardness results rely on complexity
assumptions related to the Unique Games Conjecture. Our proofs involve a novel
notion of nearly orthonormal representations of graphs, the concept of line
digraphs, and bounds on the rank of perturbed identity matrices.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [305] [Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis](https://arxiv.org/abs/2506.17852)
*Fahad Mostafa,Md Rejuan Haque,Md Mostafijur Rahman,Farzana Nasrin*

Main category: stat.ME

TL;DR: 论文提出了一种贝叶斯方法，用于估计左截断对数-逻辑（LTLL）分布的参数，适用于时间到事件数据的建模。


<details>
  <summary>Details</summary>
Motivation: 参数估计是统计建模的基础步骤，贝叶斯方法结合先验信念和观测数据，能够更稳健地推断分布参数，并提供后验分布以量化不确定性。

Method: 基于截断样本的似然函数，采用独立先验分布，通过Metropolis-Hastings算法进行马尔可夫链蒙特卡洛采样，获得后验估计。

Result: 模拟研究和实际应用表明，贝叶斯估计在截断分布中提供了更稳定和可靠的参数估计，尤其在似然曲面不规则时表现优异。

Conclusion: 贝叶斯推断在截断分布参数估计中具有优势，特别适用于时间到事件数据分析中的不确定性量化。

Abstract: Parameter estimation is a foundational step in statistical modeling, enabling
us to extract knowledge from data and apply it effectively. Bayesian estimation
of parameters incorporates prior beliefs with observed data to infer
distribution parameters probabilistically and robustly. Moreover, it provides
full posterior distributions, allowing uncertainty quantification and
regularization, especially useful in small or truncated samples. Utilizing the
left-truncated log-logistic (LTLL) distribution is particularly well-suited for
modeling time-to-event data where observations are subject to a known lower
bound such as precipitation data and cancer survival times. In this paper, we
propose a Bayesian approach for estimating the parameters of the LTLL
distribution with a fixed truncation point \( x_L > 0 \). Given a random
variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha > 0 \) is the
scale parameter and \( \beta > 0 \) is the shape parameter, the likelihood
function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with
\( X_i > x_L \). We assume independent prior distributions for the parameters,
and the posterior inference is conducted via Markov Chain Monte Carlo sampling,
specifically using the Metropolis-Hastings algorithm to obtain posterior
estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies
and real-world applications, we demonstrate that Bayesian estimation provides
more stable and reliable parameter estimates, particularly when the likelihood
surface is irregular due to left truncation. The results highlight the
advantages of Bayesian inference outperform the estimation of parameter
uncertainty in truncated distributions for time to event data analysis.

</details>


### [306] [GRASP: Grouped Regression with Adaptive Shrinkage Priors](https://arxiv.org/abs/2506.18092)
*Shu Yu Tew,Daniel F. Schmidt,Mario Boley*

Main category: stat.ME

TL;DR: GRASP是一个基于正态贝塔质数（NBP）先验的贝叶斯框架，用于分组预测回归，通过直接控制尾部行为实现灵活的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 解决分组预测回归中稀疏性和信号噪声比变化的问题，避免复杂的分层结构。

Method: 使用NBP先验直接控制尾部行为，提出新的框架量化组内收缩参数的相关性，并引入高效的Metropolis-Hastings采样器。

Result: 在模拟和真实数据上验证了GRASP的鲁棒性和多功能性。

Conclusion: GRASP通过直接控制尾部行为，简化了分组回归的稀疏性建模，并提供了更深入的组内相关性分析。

Abstract: We introduce GRASP, a simple Bayesian framework for regression with grouped
predictors, built on the normal beta prime (NBP) prior. The NBP prior is an
adaptive generalization of the horseshoe prior with tunable hyperparameters
that control tail behavior, enabling a flexible range of sparsity, from strong
shrinkage to ridge-like regularization. Unlike prior work that introduced the
group inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into
structured hierarchies, we show that directly controlling the tails is
sufficient without requiring complex hierarchical constructions. Extending the
non-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the
NBP prior to both local and group shrinkage parameters allowing adaptive
sparsity within and across groups. A key contribution of this work is a novel
framework to explicitly quantify correlations among shrinkage parameters within
a group, providing deeper insights into grouped shrinkage behavior. We also
introduce an efficient Metropolis-Hastings sampler for hyperparameter
estimation. Empirical results on simulated and real-world data demonstrate the
robustness and versatility of GRASP across grouped regression problems with
varying sparsity and signal-to-noise ratios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [307] [Sequence-to-Sequence Models with Attention Mechanistically Map to the Architecture of Human Memory Search](https://arxiv.org/abs/2506.17424)
*Nikolaus Salvatore,Qiong Zhang*

Main category: q-bio.NC

TL;DR: 论文探讨了上下文在人类记忆搜索中的作用，通过神经机器翻译模型（如RNN和注意力机制）与人类记忆模型（CMR）的相似性，提出了一种新的可解释的认知记忆模型。


<details>
  <summary>Details</summary>
Motivation: 研究上下文在人类记忆中的功能角色，并探索为何人类会发展出基于上下文的记忆架构。

Method: 利用神经机器翻译模型（RNN和注意力机制）与CMR模型的相似性，实现了一个可解释的认知记忆模型。

Result: 模型能有效捕捉人类行为模式，并揭示了记忆搜索性能如何由不同模型组件相互作用产生。

Conclusion: 神经机器翻译模型与人类记忆模型的结合为理解记忆功能提供了新视角，并展示了建模复杂记忆动态的潜力。

Abstract: Past work has long recognized the important role of context in guiding how
humans search their memory. While context-based memory models can explain many
memory phenomena, it remains unclear why humans develop such architectures over
possible alternatives in the first place. In this work, we demonstrate that
foundational architectures in neural machine translation -- specifically,
recurrent neural network (RNN)-based sequence-to-sequence models with attention
-- exhibit mechanisms that directly correspond to those specified in the
Context Maintenance and Retrieval (CMR) model of human memory. Since neural
machine translation models have evolved to optimize task performance, their
convergence with human memory models provides a deeper understanding of the
functional role of context in human memory, as well as presenting new ways to
model human memory. Leveraging this convergence, we implement a neural machine
translation model as a cognitive model of human memory search that is both
interpretable and capable of capturing complex dynamics of learning. We show
that our model accounts for both averaged and optimal human behavioral patterns
as effectively as context-based memory models. Further, we demonstrate
additional strengths of the proposed model by evaluating how memory search
performance emerges from the interaction of different model components.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [308] [Learning to Control an Android Robot Head for Facial Animation](https://arxiv.org/abs/2412.13641)
*Marcel Heisler,Christian Becker-Asano*

Main category: cs.RO

TL;DR: 论文提出了一种改进方法，通过使用3D标志点及其成对距离作为输入，将人类演员的面部表情映射到机器人头部，优于之前的面部动作单元方法。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地将人类演员的面部表情映射到不同于原始研究的机器人头部，提升机器人头部的表情表现能力。

Method: 采用3D标志点及其成对距离作为学习算法的输入，替代之前使用的面部动作单元。

Result: 在线调查显示，参与者大多偏好新方法的映射效果，但仍需进一步改进。

Conclusion: 新方法在表情映射上表现更优，但仍有优化空间。

Abstract: The ability to display rich facial expressions is crucial for human-like
robotic heads. While manually defining such expressions is intricate, there
already exist approaches to automatically learn them. In this work one such
approach is applied to evaluate and control a robot head different from the one
in the original study. To improve the mapping of facial expressions from human
actors onto a robot head, it is proposed to use 3D landmarks and their pairwise
distances as input to the learning algorithm instead of the previously used
facial action units. Participants of an online survey preferred mappings from
our proposed approach in most cases, though there are still further
improvements required.

</details>


### [309] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: PRISM框架通过合成数据蒸馏小型语言模型（SLM），替代云端大型语言模型（LLM），提升机器人在无稳定通信环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM机器人依赖云端模型，在户外或工业等通信不稳定环境中表现受限。

Method: PRISM自动合成多样化任务和环境，利用LLM生成计划，并用这些数据蒸馏出紧凑的SLM。

Result: PRISM将Llama-3.2-3B的性能从GPT-4o的10-20%提升至93%以上，且支持异构机器人平台和多样化环境。

Conclusion: PRISM为机器人提供了高效、通用的本地化规划能力，适用于多种场景。

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [310] [Online Adaptation for Flying Quadrotors in Tight Formations](https://arxiv.org/abs/2506.17488)
*Pei-An Hsieh,Kong Yao Chee,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出L1 KNODE-DW MPC框架，解决四旋翼无人机编队飞行中的气动干扰问题，提升轨迹跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 四旋翼编队飞行中复杂的气动干扰难以建模和预测，影响飞行稳定性。

Method: 采用自适应混合专家学习控制框架L1 KNODE-DW MPC，结合精确动力学模型。

Result: 在三机编队中表现优于其他MPC方法，保持垂直对齐。

Conclusion: L1自适应模块与精确动力学模型结合能有效补偿未建模干扰。

Abstract: The task of flying in tight formations is challenging for teams of quadrotors
because the complex aerodynamic wake interactions can destabilize individual
team members as well as the team. Furthermore, these aerodynamic effects are
highly nonlinear and fast-paced, making them difficult to model and predict. To
overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed
expert learning based control framework that allows individual quadrotors to
accurately track trajectories while adapting to time-varying aerodynamic
interactions during formation flights. We evaluate L1 KNODE-DW MPC in two
different three-quadrotor formations and show that it outperforms several MPC
baselines. Our results show that the proposed framework is capable of enabling
the three-quadrotor team to remain vertically aligned in close proximity
throughout the flight. These findings show that the L1 adaptive module
compensates for unmodeled disturbances most effectively when paired with an
accurate dynamics model. A video showcasing our framework and the physical
experiments is available here: https://youtu.be/9QX1Q5Ut9Rs

</details>


### [311] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Main category: cs.RO

TL;DR: 研究通过强化学习减少AUV动态对接中的sim2real差距，探索随机化和历史条件控制器等方法。


<details>
  <summary>Details</summary>
Motivation: 解决AUV在动态和不确定环境中对接时，强化学习控制器因sim2real差距导致的性能下降问题。

Method: 通过模拟研究，训练多种控制器并在真实干扰下评估，重点关注不同负载下的对接挑战。

Result: 发现随机化和历史条件控制器等方法可提高鲁棒性，为减少sim2real差距提供见解。

Conclusion: 研究为海洋机器人社区提供了未来研究方向，有助于改进AUV对接控制器的训练。

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [312] [Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2506.17832)
*Pratik Kunapuli,Jake Welde,Dinesh Jayaraman,Vijay Kumar*

Main category: cs.RO

TL;DR: 论文探讨了基于学习的控制方法（如强化学习RL）与传统几何控制器（GC）在四旋翼飞行器轨迹跟踪任务中的性能比较，提出了公平比较的最佳实践，并纠正了以往研究中偏向RL的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示RL和GC在性能比较中的不对称性，并提出公平的评估方法，以更准确地反映两类控制器的实际表现。

Method: 通过案例研究（四旋翼飞行器末端执行器的敏捷跟踪），开发了一套最佳实践，用于合成和比较RL与GC控制器，并纠正了任务定义、数据集和前馈信息的不对称性。

Result: 研究发现，公平比较下RL和GC的性能差距小于以往研究；GC在稳态误差上表现更好，而RL在瞬态性能上更优，具体表现取决于任务敏捷性需求。

Conclusion: 论文强调了公平比较的重要性，开源了控制器实现，为未来研究提供了基准。

Abstract: Learning-based control approaches like reinforcement learning (RL) have
recently produced a slew of impressive results for tasks like quadrotor
trajectory tracking and drone racing. Naturally, it is common to demonstrate
the advantages of these new controllers against established methods like
analytical controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more complicated
than might appear at first sight. As a case study, we take up the problem of
agile tracking of an end-effector for a quadrotor with a fixed arm. We develop
a set of best practices for synthesizing the best-in-class RL and geometric
controllers (GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric access to: (1) the
task definition, in the form of an objective function, (2) representative
datasets, for parameter optimization, and (3) feedforward information,
describing the desired future trajectory. The resulting findings are the
following: our improvements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above asymmetries can
yield misleading conclusions. Prior works have claimed that RL outperforms GC,
but we find the gaps between the two controller classes are much smaller than
previously published when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL has better
transient performance, resulting in GC performing better in relatively slow or
less agile tasks, but RL performing better when greater agility is required.
Finally, we open-source implementations of geometric and RL controllers for
these aerial vehicles, implementing best practices for future development.
Website and code is available at https://pratikkunapuli.github.io/rl-vs-gc/

</details>


### [313] [Geometric Contact Flows: Contactomorphisms for Dynamics and Control](https://arxiv.org/abs/2506.17868)
*Andrea Testa,Søren Hauberg,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: 论文提出了一种名为Geometric Contact Flows（GCF）的新框架，利用黎曼和接触几何作为归纳偏置，以建模和预测涉及力交换和耗散的复杂动力系统。


<details>
  <summary>Details</summary>
Motivation: 精确建模和预测复杂动力系统（尤其是涉及力交换和耗散的系统）在流体动力学和机器人学等领域至关重要，但由于几何约束和能量传递的复杂相互作用，这一任务具有挑战性。

Method: GCF构建了一个潜在接触哈密顿模型，编码稳定性或能量守恒等理想属性，并通过接触同胚的集合将该模型适配到目标动力学中，同时保留这些属性。

Result: 实验表明，该方法在物理系统动力学学习和机器人交互任务控制中表现有效。

Conclusion: GCF通过几何约束和不确定性感知的测地线，实现了对未见场景的鲁棒泛化和适应。

Abstract: Accurately modeling and predicting complex dynamical systems, particularly
those involving force exchange and dissipation, is crucial for applications
ranging from fluid dynamics to robotics, but presents significant challenges
due to the intricate interplay of geometric constraints and energy transfer.
This paper introduces Geometric Contact Flows (GFC), a novel framework
leveraging Riemannian and Contact geometry as inductive biases to learn such
systems. GCF constructs a latent contact Hamiltonian model encoding desirable
properties like stability or energy conservation. An ensemble of
contactomorphisms then adapts this model to the target dynamics while
preserving these properties. This ensemble allows for uncertainty-aware
geodesics that attract the system's behavior toward the data support, enabling
robust generalization and adaptation to unseen scenarios. Experiments on
learning dynamics for physical systems and for controlling robots on
interaction tasks demonstrate the effectiveness of our approach.

</details>


### [314] [Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification](https://arxiv.org/abs/2506.17994)
*Minh Trinh,Andreas René Geist,Josefine Monnet,Stefan Vilceanu,Sebastian Trimpe,Christian Brecher*

Main category: cs.RO

TL;DR: 比较牛顿神经网络和拉格朗日神经网络在工业机器人逆动力学建模中的表现，发现牛顿神经网络在估计电机扭矩时更有效。


<details>
  <summary>Details</summary>
Motivation: 工业机器人控制需要精确的逆动力学模型，但目前缺乏对牛顿神经网络和拉格朗日神经网络选择的指导。

Method: 结合神经网络回归与牛顿-欧拉和欧拉-拉格朗日运动方程，构建物理信息模型，并在MABI MAX 100机器人数据上比较性能。

Result: 当估计电机扭矩而非直接测量关节扭矩时，拉格朗日网络效果较差，因其未显式建模耗散扭矩。

Conclusion: 牛顿神经网络在电机扭矩估计任务中优于拉格朗日神经网络。

Abstract: Accurate inverse dynamics models are essential tools for controlling
industrial robots. Recent research combines neural network regression with
inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange
equations of motion, resulting in so-called Newtonian neural networks and
Lagrangian neural networks, respectively. These physics-informed models seek to
identify unknowns in the analytical equations from data. Despite their
potential, current literature lacks guidance on choosing between Lagrangian and
Newtonian networks. In this study, we show that when motor torques are
estimated instead of directly measuring joint torques, Lagrangian networks
prove less effective compared to Newtonian networks as they do not explicitly
model dissipative torques. The performance of these models is compared to
neural network regression on data of a MABI MAX 100 industrial robot.

</details>


### [315] [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
*Pranav Atreya,Karl Pertsch,Tony Lee,Moo Jin Kim,Arhan Jain,Artur Kuramshin,Clemens Eppner,Cyrus Neary,Edward Hu,Fabio Ramos,Jonathan Tremblay,Kanav Arora,Kirsty Ellis,Luca Macesanu,Matthew Leonard,Meedeum Cho,Ozgur Aslan,Shivin Dass,Jie Wang,Xingfang Yuan,Xuning Yang,Abhishek Gupta,Dinesh Jayaraman,Glen Berseth,Kostas Daniilidis,Roberto Martin-Martin,Youngwoon Lee,Percy Liang,Chelsea Finn,Sergey Levine*

Main category: cs.RO

TL;DR: 提出RoboArena，一种分布式众包评估方法，用于更准确、可扩展地评估通用机器人策略。


<details>
  <summary>Details</summary>
Motivation: 现有机器人评估方法依赖标准化任务和环境，难以扩展到通用策略的多样化评估。

Method: 通过众包方式，让评估者自由选择任务和环境进行双盲对比评估，并汇总偏好反馈以排名策略。

Result: 在7个学术机构的600多次评估中，证明该方法比传统集中式评估更准确、可扩展、可靠。

Conclusion: RoboArena为通用机器人策略提供了更灵活、可信的评估框架，并开放给社区使用。

Abstract: Comprehensive, unbiased, and comparable evaluation of modern generalist
policies is uniquely challenging: existing approaches for robot benchmarking
typically rely on heavy standardization, either by specifying fixed evaluation
tasks and environments, or by hosting centralized ''robot challenges'', and do
not readily scale to evaluating generalist policies across a broad range of
tasks and environments. In this work, we propose RoboArena, a new approach for
scalable evaluation of generalist robot policies in the real world. Instead of
standardizing evaluations around fixed tasks, environments, or locations, we
propose to crowd-source evaluations across a distributed network of evaluators.
Importantly, evaluators can freely choose the tasks and environments they
evaluate on, enabling easy scaling of diversity, but they are required to
perform double-blind evaluations over pairs of policies. Then, by aggregating
preference feedback from pairwise comparisons across diverse tasks and
environments, we can derive a ranking of policies. We instantiate our approach
across a network of evaluators at seven academic institutions using the DROID
robot platform. Through more than 600 pairwise real-robot evaluation episodes
across seven generalist policies, we demonstrate that our crowd-sourced
approach can more accurately rank the performance of existing generalist
policies than conventional, centralized evaluation approaches, while being more
scalable, resilient, and trustworthy. We open our evaluation network to the
community and hope that it can enable more accessible comparisons of generalist
robot policies.

</details>


### [316] [A Motivational Architecture for Open-Ended Learning Challenges in Robots](https://arxiv.org/abs/2506.18454)
*Alejandro Romero,Gianluca Baldassarre,Richard J. Duro,Vieri Giuliano Santucci*

Main category: cs.RO

TL;DR: H-GRAIL是一种分层架构，通过内在动机和互联学习机制，自主发现目标、学习技能并适应动态环境。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂动态环境中自主交互的智能体，解决目标生成、技能学习和环境适应等核心挑战。

Method: 提出H-GRAIL分层架构，结合多种内在动机和互联学习机制，实现目标发现、技能学习和环境适应。

Result: 在真实机器人场景中测试，验证了H-GRAIL在开放学习挑战中的有效性。

Conclusion: H-GRAIL为开放学习提供了一种集成解决方案，能够同时应对多方面的挑战。

Abstract: Developing agents capable of autonomously interacting with complex and
dynamic environments, where task structures may change over time and prior
knowledge cannot be relied upon, is a key prerequisite for deploying artificial
systems in real-world settings. The open-ended learning framework identifies
the core challenges for creating such agents, including the ability to
autonomously generate new goals, acquire the necessary skills (or curricula of
skills) to achieve them, and adapt to non-stationary environments. While many
existing works tackles various aspects of these challenges in isolation, few
propose integrated solutions that address them simultaneously. In this paper,
we introduce H-GRAIL, a hierarchical architecture that, through the use of
different typologies of intrinsic motivations and interconnected learning
mechanisms, autonomously discovers new goals, learns the required skills for
their achievement, generates skill sequences for tackling interdependent tasks,
and adapts to non-stationary environments. We tested H-GRAIL in a real robotic
scenario, demonstrating how the proposed solutions effectively address the
various challenges of open-ended learning.

</details>


### [317] [Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures](https://arxiv.org/abs/2506.18812)
*Aristotelis Papatheodorou,Pranav Vaidhyanathan,Natalia Ares,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 论文提出了Presymplectification Networks（PSNs），通过Dirac结构学习symplectification lift，解决带耗散和约束的系统中symplectic形式退化的问题，并在ANYmal四足机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在带耗散和约束的系统中（如多体机器人），传统的symplectic形式会退化，导致稳定性与长期预测能力下降。本文旨在解决这一基础性问题。

Method: 提出PSNs框架，结合循环编码器和流匹配目标学习高维流形上的动力学，并附加轻量级SympNet以保持能量、动量和约束满足。

Result: 在ANYmal四足机器人上验证了PSNs的有效性，首次成功将几何机器学习应用于带约束的耗散系统。

Conclusion: PSNs填补了带约束耗散系统与symplectic学习之间的空白，为基于第一性原理且可数据驱动的几何机器学习模型开辟了新方向。

Abstract: Physics-informed deep learning has achieved remarkable progress by embedding
geometric priors, such as Hamiltonian symmetries and variational principles,
into neural networks, enabling structure-preserving models that extrapolate
with high accuracy. However, in systems with dissipation and holonomic
constraints, ubiquitous in legged locomotion and multibody robotics, the
canonical symplectic form becomes degenerate, undermining the very invariants
that guarantee stability and long-term prediction. In this work, we tackle this
foundational limitation by introducing Presymplectification Networks (PSNs),
the first framework to learn the symplectification lift via Dirac structures,
restoring a non-degenerate symplectic geometry by embedding constrained systems
into a higher-dimensional manifold. Our architecture combines a recurrent
encoder with a flow-matching objective to learn the augmented phase-space
dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)
to forecast constrained trajectories while preserving energy, momentum, and
constraint satisfaction. We demonstrate our method on the dynamics of the
ANYmal quadruped robot, a challenging contact-rich, multibody system. To the
best of our knowledge, this is the first framework that effectively bridges the
gap between constrained, dissipative mechanical systems and symplectic
learning, unlocking a whole new class of geometric machine learning models,
grounded in first principles yet adaptable from data.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [318] [CLOUD: A Scalable and Physics-Informed Foundation Model for Crystal Representation Learning](https://arxiv.org/abs/2506.17345)
*Changwen Xu,Shang Zhu,Venkatasubramanian Viswanathan*

Main category: cond-mat.mtrl-sci

TL;DR: CLOUD是一个基于Transformer的框架，用于预测晶体性质，结合对称性一致的编码和物理原理，实现高效且可解释的材料建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如实验或DFT计算）资源密集且难以扩展，而现有机器学习模型缺乏物理一致性和通用性。

Method: CLOUD采用对称性一致的参数编码（SCOPE），预训练于600万晶体结构，并通过微调应用于多种任务。

Result: CLOUD在多种材料性质预测中表现优异，并展示了可微分建模的潜力（如热力学性质预测）。

Conclusion: CLOUD是一个可扩展且物理信息化的基础模型，为晶体材料的研究和发现提供了新工具。

Abstract: The prediction of crystal properties is essential for understanding
structure-property relationships and accelerating the discovery of functional
materials. However, conventional approaches relying on experimental
measurements or density functional theory (DFT) calculations are often
resource-intensive, limiting their scalability. Machine learning (ML) models
offer a promising alternative by learning complex structure-property
relationships from data, enabling faster predictions. Yet, existing ML models
often rely on labeled data, adopt representations that poorly capture essential
structural characteristics, and lack integration with physical
principles--factors that limit their generalizability and interpretability.
Here, we introduce CLOUD (Crystal Language mOdel for Unified and Differentiable
materials modeling), a transformer-based framework trained on a novel
Symmetry-Consistent Ordered Parameter Encoding (SCOPE) that encodes crystal
symmetry, Wyckoff positions, and composition in a compact, coordinate-free
string representation. Pre-trained on over six million crystal structures,
CLOUD is fine-tuned on multiple downstream tasks and achieves competitive
performance in predicting a wide range of material properties, demonstrating
strong scaling performance. Furthermore, as proof of concept of differentiable
materials modeling, CLOUD is applied to predict the phonon internal energy and
heat capacity, which integrates the Debye model to preserve thermodynamic
consistency. The CLOUD-DEBYE framework enforces thermodynamic consistency and
enables temperature-dependent property prediction without requiring additional
data. These results demonstrate the potential of CLOUD as a scalable and
physics-informed foundation model for crystalline materials, unifying
symmetry-consistent representations with physically grounded learning for
property prediction and materials discovery.

</details>


### [319] [Leveraging neural network interatomic potentials for a foundation model of chemistry](https://arxiv.org/abs/2506.18497)
*So Yeon Kim,Yang Jeong Park,Ju Li*

Main category: cond-mat.mtrl-sci

TL;DR: HackNIP是一种两阶段方法，利用预训练的NIP提取特征向量，再用浅层ML模型进行结构到属性的预测，旨在解决ML在材料科学中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: NIP在预测电子属性时面临挑战，而传统ML方法在通用性和计算需求之间存在权衡。HackNIP旨在通过结合NIP和浅层ML模型来克服这些限制。

Method: HackNIP首先从预训练的NIP中提取固定长度的特征向量（嵌入），然后利用这些嵌入训练浅层ML模型进行下游预测。

Result: HackNIP在Matbench上进行了基准测试，展示了数据效率，并在多种任务中表现优异。嵌入深度对性能有显著影响。

Conclusion: HackNIP通过混合策略克服了ML在材料科学中的权衡问题，为高性能预测建模提供了新思路。

Abstract: Large-scale foundation models, including neural network interatomic
potentials (NIPs) in computational materials science, have demonstrated
significant potential. However, despite their success in accelerating atomistic
simulations, NIPs face challenges in directly predicting electronic properties
and often require coupling to higher-scale models or extensive simulations for
macroscopic properties. Machine learning (ML) offers alternatives for
structure-to-property mapping but faces trade-offs: feature-based methods often
lack generalizability, while deep neural networks require significant data and
computational power. To address these trade-offs, we introduce HackNIP, a
two-stage pipeline that leverages pretrained NIPs. This method first extracts
fixed-length feature vectors (embeddings) from NIP foundation models and then
uses these embeddings to train shallow ML models for downstream
structure-to-property predictions. This study investigates whether such a
hybridization approach, by ``hacking" the NIP, can outperform end-to-end deep
neural networks, determines the dataset size at which this transfer learning
approach surpasses direct fine-tuning of the NIP, and identifies which NIP
embedding depths yield the most informative features. HackNIP is benchmarked on
Matbench, evaluated for data efficiency, and tested on diverse tasks including
\textit{ab initio}, experimental, and molecular properties. We also analyze how
embedding depth impacts performance. This work demonstrates a hybridization
strategy to overcome ML trade-offs in materials science, aiming to democratize
high-performance predictive modeling.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [320] [Perfect phylogenies via the Minimum Uncovering Branching problem: efficiently solvable cases](https://arxiv.org/abs/2506.18578)
*Narmina Baghirova,Esther Galby,Martin Milanič*

Main category: cs.DM

TL;DR: 本文解决了Minimum Uncovering Branching问题的多项式时间可解性，通过研究最优解的结构性质，将其转化为二分图最大匹配和偏序集最大权反链问题。


<details>
  <summary>Details</summary>
Motivation: 研究Minimum Uncovering Branching问题的复杂性，特别是针对宽度受限实例的精确复杂度，填补了Hujdurović等人2018年研究的空白。

Method: 通过分析最优解的结构性质，将问题转化为二分图最大匹配和偏序集最大权反链问题，并引入新的多项式可计算下界。

Result: 证明了该问题在多项式时间内可解，并提出了另一个多项式时间可解的条件。

Conclusion: 本文解决了Minimum Uncovering Branching问题的复杂性，为宽度受限实例提供了多项式时间算法，并扩展了问题的可解性条件。

Abstract: In this paper, we present new efficiently solvable cases of the Minimum
Uncovering Branching problem, an optimization problem with applications in
cancer genomics introduced by Hujdurovi\'c, Husi\'c, Milani\v{c}, Rizzi, and
Tomescu in 2018. The problem involves a family of finite sets, and the goal is
to map each non-maximal set to exactly one set that contains it, minimizing the
sum of uncovered elements across all sets in the family. Hujdurovi\'c et al.
formulated the problem in terms of branchings of the digraph formed by the
proper set inclusion relation on the input sets and studied the problem
complexity based on properties of the corresponding partially ordered set, in
particular, with respect to its height and width, defined respectively as the
maximum cardinality of a chain and an antichain. They showed that the problem
is APX-complete for instances of bounded height and that a constant-factor
approximation algorithm exists for instances of bounded width, but left the
exact complexity for bounded-width instances open. In this paper, we answer
this question by proving that the problem is solvable in polynomial time. We
derive this result by examining the structural properties of optimal solutions
and reducing the problem to computing maximum matchings in bipartite graphs and
maximum weight antichains in partially ordered sets. We also introduce a new
polynomially computable lower bound and identify another condition for
polynomial-time solvability.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [321] [Heterogeneous Temporal Hypergraph Neural Network](https://arxiv.org/abs/2506.17312)
*Huan Liu,Pengfei Jiao,Mengzhou Gao,Chaochao Chen,Di Jin*

Main category: cs.SI

TL;DR: 论文提出了一种新型的异质时序超图神经网络（HTHGN），用于捕捉复杂异质时序图中的高阶交互关系，解决了现有方法忽略高阶拓扑信息的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法主要关注低阶拓扑信息，忽略了高阶交互关系，且现有超图方法仅适用于静态同质图，无法有效建模异质时序图中的高阶交互。

Method: 提出异质时序超图的正式定义和P-均匀异质超边构造算法，设计HTHGN模型，结合分层注意力机制和对比学习，捕捉高阶交互和避免低阶结构模糊问题。

Result: 在三个真实异质时序图数据集上的实验验证了HTHGN的有效性，展示了显著的性能提升。

Conclusion: HTHGN成功解决了异质时序图中高阶交互建模的挑战，为复杂网络分析提供了新工具。

Abstract: Graph representation learning (GRL) has emerged as an effective technique for
modeling graph-structured data. When modeling heterogeneity and dynamics in
real-world complex networks, GRL methods designed for complex heterogeneous
temporal graphs (HTGs) have been proposed and have achieved successful
applications in various fields. However, most existing GRL methods mainly focus
on preserving the low-order topology information while ignoring higher-order
group interaction relationships, which are more consistent with real-world
networks. In addition, most existing hypergraph methods can only model static
homogeneous graphs, limiting their ability to model high-order interactions in
HTGs. Therefore, to simultaneously enable the GRL model to capture high-order
interaction relationships in HTGs, we first propose a formal definition of
heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge
construction algorithm that does not rely on additional information. Then, a
novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to
fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical
attention mechanism module that simultaneously performs temporal
message-passing between heterogeneous nodes and hyperedges to capture rich
semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN
performs contrastive learning by maximizing the consistency between low-order
correlated heterogeneous node pairs on HTG to avoid the low-order structural
ambiguity issue. Detailed experimental results on three real-world HTG datasets
verify the effectiveness of the proposed HTHGN for modeling high-order
interactions in HTGs and demonstrate significant performance improvements.

</details>


### [322] [A family of graph GOSPA metrics for graphs with different sizes](https://arxiv.org/abs/2506.17316)
*Jinhao Gu,Ángel F. García-Fernández,Robert E. Firth,Lennart Svensson*

Main category: cs.SI

TL;DR: 本文提出了一种用于测量不同大小图之间距离的图度量家族，扩展了图GOSPA度量，并证明了其满足度量性质。


<details>
  <summary>Details</summary>
Motivation: 研究如何更灵活地衡量图之间的距离，特别是在节点和边不匹配时的惩罚机制。

Method: 提出了一种广义的图GOSPA度量家族，支持对边不匹配的更一般惩罚，并通过线性编程近似计算。

Result: 实验验证了该度量家族在不同超参数下的特性，并在真实数据集上展示了其在分类任务中的优势。

Conclusion: 提出的图GOSPA度量家族在灵活性和实用性上优于现有方法，适用于多种图分析任务。

Abstract: This paper proposes a family of graph metrics for measuring distances between
graphs of different sizes. The proposed metric family defines a general form of
the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also
proved to satisfy the metric properties. Similarly to the graph GOSPA metric,
the proposed graph GOSPA metric family also penalises the node attribute costs
for assigned nodes between the two graphs, and the number of unassigned nodes.
However, the proposed family of metrics provides more general penalties for
edge mismatches than the graph GOSPA metric. This paper also shows that the
graph GOSPA metric family can be approximately computed using linear
programming. Simulation experiments are performed to illustrate the
characteristics of the proposed graph GOSPA metric family with different
choices of hyperparameters. The benefits of the proposed graph GOSPA metric
family for classification tasks are also shown on real-world datasets.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [323] [Continuous Map Matching to Paths under Travel Time Constraints](https://arxiv.org/abs/2506.18354)
*Yannick Bosch,Sabine Storandt*

Main category: cs.CG

TL;DR: 论文研究了带旅行时间约束的地图匹配问题，提出了一种新算法，能处理无限候选位置集，保证找到一致解，并在理论和实践中表现出更优的运行时间。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理无限候选位置集时无法保证一致解的问题，适用于公共交通数据处理和运动轨迹地图匹配。

Method: 提出新算法，利用高效的线段-圆相交数据结构，处理无限候选位置集。

Result: 算法在理论上和实践中的运行时间优于基线方法，复杂度为O(k^2 n log nk)或更低。

Conclusion: 新算法在保证解一致性的同时显著提升了效率，适用于多种实际场景。

Abstract: In this paper, we study the problem of map matching with travel time
constraints. Given a sequence of $k$ spatio-temporal measurements and an
embedded path graph with travel time costs, the goal is to snap each
measurement to a close-by location in the graph, such that consecutive
locations can be reached from one another along the path within the timestamp
difference of the respective measurements. This problem arises in public
transit data processing as well as in map matching of movement trajectories to
general graphs. We show that the classical approach for this problem, which
relies on selecting a finite set of candidate locations in the graph for each
measurement, cannot guarantee to find a consistent solution. We propose a new
algorithm that can deal with an infinite set of candidate locations per
measurement. We prove that our algorithm always detects a consistent map
matching path (if one exists). Despite the enlarged candidate set, we also
demonstrate that our algorithm has superior running time in theory and
practice. For a path graph with $n$ nodes, we show that our algorithm runs in
$\mathcal{O}(k^2 n \log {nk})$ and under mild assumptions in $\mathcal{O}(k n
^\lambda + n \log^3 n)$ for $\lambda \approx 0.695$. This is a significant
improvement over the baseline, which runs in $\mathcal{O}(k n^2)$ and which
might not even identify a correct solution. The performance of our algorithm
hinges on an efficient segment-circle intersection data structure. We describe
how to design and implement such a data structure for our application. In the
experimental evaluation, we demonstrate the usefulness of our novel algorithm
on a diverse set of generated measurements as well as GTFS data.

</details>


### [324] [Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations](https://arxiv.org/abs/2506.17507)
*Michael T. Goodrich,Vinesh Sridhar*

Main category: cs.CG

TL;DR: 本文研究了在噪声原始模型下的并行计算几何算法，特别是2D和3D凸包问题，提出了首个最优并行算法。


<details>
  <summary>Details</summary>
Motivation: 噪声原始模型中的几何算法研究多为顺序算法，缺乏并行方法，本文旨在填补这一空白。

Method: 采用CREW PRAM模型，通过泛化的失败扫描技术检测和修复算法中间步骤中的错误。

Result: 提出了首个在噪声原始模型下2D和3D凸包问题的最优并行算法。

Conclusion: 本文的算法在噪声原始模型中实现了并行计算几何的高效性和鲁棒性。

Abstract: In the noisy primitives model, each primitive comparison performed by an
algorithm, e.g., testing whether one value is greater than another, returns the
incorrect answer with random, independent probability p < 1/2 and otherwise
returns a correct answer. This model was first applied in the context of
sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar
extends this model to sequential algorithms involving geometric primitives such
as orientation and sidedness tests. However, their approaches appear to be
inherently sequential; hence, in this paper, we study parallel computational
geometry algorithms for 2D and 3D convex hulls in the noisy primitives model.
We give the first optimal parallel algorithms in the noisy primitives model for
2D and 3D convex hulls in the CREW PRAM model. The main technical contribution
of our work concerns our ability to detect and fix errors during intermediate
steps of our algorithm using a generalization of the failure sweeping
technique.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [325] [A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery](https://arxiv.org/abs/2506.17510)
*Rafael Ferreira da Silva,Milad Abolhasani,Dionysios A. Antonopoulos,Laura Biven,Ryan Coffee,Ian T. Foster,Leslie Hamilton,Shantenu Jha,Theresa Mayer,Benjamin Mintz,Robert G. Moore,Salahudin Nimer,Noah Paulson,Woong Shin,Frederic Suter,Mitra Taheri,Michela Taufer,Newell R. Washburn*

Main category: cs.CY

TL;DR: AISLE是一个自主互联的科学实验室生态系统，旨在通过跨机构协作和AI驱动的方法加速科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前自主实验室缺乏跨机构协作能力，限制了科学发现的效率。AISLE旨在解决这一问题。

Method: AISLE通过五个关键维度实现：跨机构设备协调、FAIR合规的数据管理、基于科学原则的AI代理协调、可互操作的代理通信接口以及AI/ML集成的科学教育。

Result: AISLE能够缩短从构想到创新的时间，加速科学发现，并推动可持续能源、材料开发和公共健康等领域的突破。

Conclusion: AISLE通过协作自主科学范式，有望实现传统方法无法触及的研究领域，并推动科学技术的民主化。

Abstract: Scientific discovery is being revolutionized by AI and autonomous systems,
yet current autonomous laboratories remain isolated islands unable to
collaborate across institutions. We present the Autonomous Interconnected
Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented
capabilities into a unified system that shorten the path from ideation to
innovation to impact and accelerates discovery from decades to months. AISLE
addresses five critical dimensions: (1) cross-institutional equipment
orchestration, (2) intelligent data management with FAIR compliance, (3)
AI-agent driven orchestration grounded in scientific principles, (4)
interoperable agent communication interfaces, and (5) AI/ML-integrated
scientific education. By connecting autonomous agents across institutional
boundaries, autonomous science can unlock research spaces inaccessible to
traditional approaches while democratizing cutting-edge technologies. This
paradigm shift toward collaborative autonomous science promises breakthroughs
in sustainable energy, materials development, and public health.

</details>


### [326] [Using Machine Learning in Analyzing Air Quality Discrepancies of Environmental Impact](https://arxiv.org/abs/2506.17319)
*Shuangbao Paul Wang,Lucas Yang,Rahouane Chouchane,Jin Guo,Michael Bailey*

Main category: cs.CY

TL;DR: 机器学习与软件工程用于分析巴尔的摩空气污染水平，发现污染与历史偏见保险评估方法相关，低收入和少数族裔社区污染更严重。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示历史政策如何通过空气污染持续影响巴尔的摩居民的生活质量，特别是低收入和少数族裔社区。

Method: 使用三种数据源：偏见保险风险评估方法、居民人口统计数据和NO2/PM2.5浓度普查数据，覆盖650,643名居民。

Result: 空气污染水平与偏见保险评估方法显著相关，低收入和少数族裔社区污染更严重。

Conclusion: 历史政策至今仍在通过空气污染对巴尔的摩居民，尤其是少数族裔和低收入群体，造成不平等影响。

Abstract: In this study, we apply machine learning and software engineering in
analyzing air pollution levels in City of Baltimore. The data model was fed
with three primary data sources: 1) a biased method of estimating insurance
risk used by homeowners loan corporation, 2) demographics of Baltimore
residents, and 3) census data estimate of NO2 and PM2.5 concentrations. The
dataset covers 650,643 Baltimore residents in 44.7 million residents in 202
major cities in US. The results show that air pollution levels have a clear
association with the biased insurance estimating method. Great disparities
present in NO2 level between more desirable and low income blocks. Similar
disparities exist in air pollution level between residents' ethnicity. As
Baltimore population consists of a greater proportion of people of color, the
finding reveals how decades old policies has continued to discriminate and
affect quality of life of Baltimore citizens today.

</details>


### [327] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Main category: cs.CY

TL;DR: MAARTA是一个多智能体框架，通过分析视线模式和放射学报告提供个性化反馈，帮助放射学学生减少感知错误。


<details>
  <summary>Details</summary>
Motivation: 放射学学生因缺乏专家指导时间而难以发展感知技能，导致视觉搜索和诊断解释错误，现有AI系统未能有效解决这些问题。

Method: MAARTA通过动态选择智能体分析视线行为差异，使用结构化图表比较专家与学生行为，并通过逐步提示帮助学生理解错误。

Result: 系统能够识别遗漏发现并分析差异，提供个性化反馈以改善学生的诊断推理能力。

Conclusion: MAARTA通过多智能体框架和自适应推理，推动了AI驱动的放射学教育发展。

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


### [328] [Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps](https://arxiv.org/abs/2506.17577)
*Meng Xia,Robin Schmucker,Conrad Borchers,Vincent Aleven*

Main category: cs.CY

TL;DR: Fast-Forwarding技术通过步骤级适应性减少过度练习，提升学习效率，但效果受学生动机影响。


<details>
  <summary>Details</summary>
Motivation: 解决辅导系统中学生过度练习已掌握技能的问题，避免资源密集的课程重新设计。

Method: 提出Fast-Forwarding技术，基于学习模型和问题解决路径的模拟研究，优化问题选择算法。

Result: Fast-Forwarding可减少高达三分之一的过度练习，尤其适用于偏好选择难题的算法。

Conclusion: Fast-Forwarding能提升练习效率，但其实际效果依赖于学生在高难度下的动机和参与度。

Abstract: Mastery learning improves learning proficiency and efficiency. However, the
overpractice of skills--students spending time on skills they have already
mastered--remains a fundamental challenge for tutoring systems. Previous
research has reduced overpractice through the development of better problem
selection algorithms and the authoring of focused practice tasks. However, few
efforts have concentrated on reducing overpractice through step-level
adaptivity, which can avoid resource-intensive curriculum redesign. We propose
and evaluate Fast-Forwarding as a technique that enhances existing problem
selection algorithms. Based on simulation studies informed by learner models
and problem-solving pathways derived from real student data, Fast-Forwarding
can reduce overpractice by up to one-third, as it does not require students to
complete problem-solving steps if all remaining pathways are fully mastered.
Fast-Forwarding is a flexible method that enhances any problem selection
algorithm, though its effectiveness is highest for algorithms that
preferentially select difficult problems. Therefore, our findings suggest that
while Fast-Forwarding may improve student practice efficiency, the size of its
practical impact may also depend on students' ability to stay motivated and
engaged at higher levels of difficulty.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [329] [Conformal Safety Shielding for Imperfect-Perception Agents](https://arxiv.org/abs/2506.17275)
*William Scarbro,Calum Imrie,Sinem Getir Yaman,Kavan Fatehi,Corina S. Pasareanu,Radu Calinescu,Ravi Mangal*

Main category: eess.SY

TL;DR: 提出了一种基于安全屏蔽的方法，为使用学习组件的离散自主代理提供运行时安全保证，通过限制动作选择来应对感知错误。


<details>
  <summary>Details</summary>
Motivation: 解决在存在感知错误的情况下，如何为自主代理提供安全控制的问题。

Method: 使用符合预测方法为感知组件生成状态估计集，并通过屏蔽机制限制动作选择，确保局部安全性。

Result: 实现了在用户指定概率下的局部安全保证，并通过案例研究验证了方法的有效性。

Conclusion: 该方法能够有效保障自主代理在感知错误下的安全性，同时证明了全局安全性的理论边界。

Abstract: We consider the problem of safe control in discrete autonomous agents that
use learned components for imperfect perception (or more generally, state
estimation) from high-dimensional observations. We propose a shield
construction that provides run-time safety guarantees under perception errors
by restricting the actions available to an agent, modeled as a Markov decision
process, as a function of the state estimates. Our construction uses conformal
prediction for the perception component, which guarantees that for each
observation, the predicted set of estimates includes the actual state with a
user-specified probability. The shield allows an action only if it is allowed
for all the estimates in the predicted set, resulting in a local safety
guarantee. We also articulate and prove a global safety property of existing
shield constructions for perfect-perception agents bounding the probability of
reaching unsafe states if the agent always chooses actions prescribed by the
shield. We illustrate our approach with a case-study of an experimental
autonomous system that guides airplanes on taxiways using high-dimensional
perception DNNs.

</details>


### [330] [Dynamic Hybrid Modeling: Incremental Identification and Model Predictive Control](https://arxiv.org/abs/2506.18344)
*Adrian Caspari,Thomas Bierweiler,Sarah Fadda,Daniel Labisch,Maarten Nauta,Franzisko Wagner,Merle Warmbold,Constantinos C. Pantelides*

Main category: eess.SY

TL;DR: 提出了一种增量式识别动态混合模型的方法，通过解耦机制模型和数据驱动模型，解决了计算和概念上的困难。


<details>
  <summary>Details</summary>
Motivation: 传统数学模型在计算时间、算法复杂性和开发成本上存在局限性，混合模型结合了机制模型和数据驱动模型，但动态混合模型的识别仍具挑战性。

Method: 采用四步法：1) 正则化动态参数估计；2) 相关性分析；3) 数据驱动模型识别；4) 混合模型集成。

Result: 通过三个案例研究验证了方法的稳健性、可靠性和效率，尤其在处理复杂系统和有限数据时表现优异。

Conclusion: 增量式方法加速了混合模型开发，支持早期模型结构评估，并允许数据驱动组件的独立识别。

Abstract: Mathematical models are crucial for optimizing and controlling chemical
processes, yet they often face significant limitations in terms of
computational time, algorithm complexity, and development costs. Hybrid models,
which combine mechanistic models with data-driven models (i.e. models derived
via the application of machine learning to experimental data), have emerged as
a promising solution to these challenges. However, the identification of
dynamic hybrid models remains difficult due to the need to integrate
data-driven models within mechanistic model structures. We present an
incremental identification approach for dynamic hybrid models that decouples
the mechanistic and data-driven components to overcome computational and
conceptual difficulties. Our methodology comprises four key steps: (1)
regularized dynamic parameter estimation to determine optimal time profiles for
flux variables, (2) correlation analysis to evaluate relationships between
variables, (3) data-driven model identification using advanced machine learning
techniques, and (4) hybrid model integration to combine the mechanistic and
data-driven components. This approach facilitates early evaluation of model
structure suitability, accelerates the development of hybrid models, and allows
for independent identification of data-driven components. Three case studies
are presented to illustrate the robustness, reliability, and efficiency of our
incremental approach in handling complex systems and scenarios with limited
data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [331] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: 本文介绍了AI生成游戏解说（AIGGC）的通用框架，综述了45个现有数据集和方法，并分类比较了常用评估指标，同时提供了结构化数据表以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: AIGGC因其市场潜力和技术挑战受到关注，需解决语言模型在事实准确性、逻辑推理、文本生成等方面的需求。

Method: 提出通用框架，综述现有数据集和方法，分类比较评估指标，并提供结构化数据表。

Result: 总结了45个数据集和方法，提供了评估指标分类和公开数据集资源。

Conclusion: 本文为AIGGC领域的研究和基准测试提供了全面支持，未来可通过公开数据集进一步推动发展。

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [332] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: Mercury是一代基于扩散的商业规模大语言模型（LLMs），专为编码应用设计，在速度和质量上达到新水平。


<details>
  <summary>Details</summary>
Motivation: 开发高效且高质量的编码专用LLMs，提升开发效率。

Method: 基于Transformer架构的扩散模型，并行预测多个token。

Result: Mercury Coder Mini和Small在NVIDIA H100 GPU上分别达到1109和737 tokens/sec的吞吐量，速度提升10倍且质量相当。

Conclusion: Mercury Coder在编码应用中表现出色，速度和质量的平衡使其成为领先模型。

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [333] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: 本文提出了一种信息论框架UProp，用于量化大型语言模型（LLM）在序列决策中的不确定性，并将其分解为内部和外部不确定性。UProp通过估计点互信息（PMI）在多步决策过程中高效评估外部不确定性，实验表明其在多步决策任务中优于现有单轮不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用于安全关键的序列决策任务，了解何时信任其决策至关重要。现有不确定性量化方法主要针对单轮问答，多步决策场景研究不足。

Method: 提出信息论框架，将LLM序列决策不确定性分解为内部和外部不确定性（互信息量），并设计UProp方法通过PMI估计外部不确定性。

Result: UProp在多步决策基准测试（如AgentBench和HotpotQA）中显著优于现有单轮不确定性量化方法。

Conclusion: UProp为LLM在多步决策中的不确定性量化提供了高效且有效的解决方案，具有广泛的应用潜力。

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [334] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）对社会的影响，包括AI检测器的偏见、LLMs在各领域的广泛应用，以及LLMs为研究提供反馈的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何改变写作、沟通和创作方式，以及社会如何适应这一技术。

Method: 通过三个研究方向：分析AI检测器的偏见、测量LLMs在各领域的应用、评估LLMs提供研究反馈的能力。

Result: 发现AI检测器存在偏见，LLMs在各领域广泛使用，且能为研究提供有效反馈。

Conclusion: LLMs对社会影响深远，需关注公平性和支持研究者的潜力。

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [335] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer利用自然语言描述作为辅助提示，结合Transformer模型提升台风轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在稀疏气象轨迹（如台风路径）预测中缺乏上下文知识，TyphoFormer通过语言描述弥补这一不足。

Method: 使用LLM生成台风数值属性的语言描述，将其嵌入为特殊标记与时间序列输入结合，通过统一Transformer编码器整合文本与序列信息。

Result: 在HURDAT2基准测试中，TyphoFormer表现优于其他先进方法，尤其在非线性路径变化和历史数据有限的情况下。

Conclusion: TyphoFormer通过结合语言描述与数值特征，显著提升了台风轨迹预测的可靠性。

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [336] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: 论文提出Step-Opt-Instruct框架，通过逐步生成和验证优化建模数据，提升LLMs在复杂OR任务中的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在解决复杂OR任务时面临挑战，需要高质量数据支持。

Method: 采用Step-Opt-Instruct框架，通过迭代问题生成和逐步验证生成高质量数据，并微调LLMs。

Result: Step-Opt模型在多个基准测试中表现优异，复杂任务上准确率提升17.01%。

Conclusion: 结合结构化验证和逐步问题优化，能有效提升LLMs在OR任务中的自动化决策能力。

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [337] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: TPTT框架通过线性化注意力机制和内存管理技术提升预训练Transformer模型的效率和准确性，兼容Hugging Face Transformers库，支持参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在长上下文推理中的计算和内存需求问题。

Method: 采用Memory as Gate (MaG)和混合线性化注意力(LiZA)技术，支持参数高效微调(LoRA)。

Result: 在MMLU基准测试中，1B参数模型效率与准确性显著提升，如Titans-Llama-3.2-1B的Exact Match提高20%。

Conclusion: TPTT具有实际可扩展性和鲁棒性，代码和Python包已开源。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [338] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 论文提出了一个零样本对话立场检测数据集ZS-CSD，并提出了SITPCL模型，在零样本设置下取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有对话立场检测数据集目标受限的问题，提升模型在未见目标上的表现。

Method: 手动构建ZS-CSD数据集，提出SITPCL模型，结合说话者交互和目标感知的原型对比学习。

Result: SITPCL模型在零样本对话立场检测中达到最佳性能，F1-macro得分为43.81%。

Conclusion: 零样本对话立场检测仍具挑战性，SITPCL模型为未来研究提供了基准。

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [339] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文系统研究了中世纪罗曼语（如奥克西唐语、西班牙语和法语）的词性标注性能，探讨了微调、提示工程、模型架构等方法对标注准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管现代大语言模型在古语言处理方面取得进展，但中世纪罗曼语因语言演变、拼写变体和标注数据稀缺而面临独特挑战。

Method: 通过实验评估微调、提示工程、模型架构、解码策略和跨语言迁移学习对标注准确性的影响。

Result: 研究揭示了大语言模型在处理历史语言变体和非标准化拼写时的局限性，但也发现了有效的专门技术。

Conclusion: 针对低资源历史语言的独特挑战，提出了有前景的解决方案。

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [340] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）输出缺乏多样性的原因，提出了分支因子（BF）作为量化指标，发现对齐调优显著降低了BF，解释了模型输出稳定性的来源及其对复杂推理的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管对齐的大型语言模型（LLM）能力强大，但其输出往往缺乏多样性。研究旨在探究这种稳定性的驱动因素。

Method: 通过引入分支因子（BF）量化模型输出分布的概率集中现象，并对对齐模型和基础模型进行实证分析。

Result: 研究发现BF随生成过程递减，对齐调优显著降低BF，且这种稳定性对复杂推理（如链式思维）有积极影响。

Conclusion: 对齐调优并未根本改变模型行为，而是引导其选择低熵路径，BF可作为理解和控制LLM输出的有效工具。

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [341] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 该论文探讨了端到端（E2E）框架在口语语法纠错（SGEC）和反馈生成中的应用，比较了级联、部分级联和E2E架构，并提出了自动伪标注框架和参考对齐过程以提升性能。


<details>
  <summary>Details</summary>
Motivation: 口语语法纠错（SGEC）面临转录错误、不流畅性和结构化输入缺失等挑战，现有级联系统易受模块间错误传播影响，因此研究E2E框架以优化SGEC和反馈生成。

Method: 基于Whisper基础模型，比较级联、部分级联和E2E架构；提出自动伪标注框架扩展训练数据；引入参考对齐过程和编辑置信度估计以提升反馈精度。

Result: 在Linguaskill和Speak & Improve语料库上的实验表明，所提方法显著提升了E2E SGEC的性能。

Conclusion: E2E框架结合伪标注、参考对齐和置信度估计能有效提升SGEC系统性能，为口语学习反馈提供了新思路。

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [342] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出了一种在推理过程中动态修正替换错误的方法，以提高语音识别中对未训练词汇的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列到序列系统在识别未训练词汇（如专有名词、缩写等）时的不足，尤其是发音与拼写不匹配的情况。

Method: 允许用户在推理过程中动态添加修正，以纠正替换错误。

Result: 相对偏置词错误率提高了11%，同时保持整体词错误率的竞争力。

Conclusion: 该方法有效提升了语音识别系统对未训练词汇的识别能力。

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [343] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Main category: cs.CL

TL;DR: 提出了一种基于激励的方法，利用强化学习（RL）从零开始训练LLM，无需依赖标注或合成数据，实现了超长高质量文本生成。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在超长文本生成中的长度限制和质量下降问题，避免传统方法依赖合成数据的缺点。

Method: 从基础模型出发，通过RL训练，结合奖励模型优化长度控制、写作质量和结构格式。

Result: LongWriter-Zero模型在长文本写作任务中表现优于传统SFT方法，在多个评测中达到SOTA。

Conclusion: 激励式RL方法有效提升了LLM的超长文本生成能力，且无需依赖合成数据。

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [344] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: 提出了一种基于GAN的多波段红外图像彩色化框架MTSIC，通过多阶段光谱自注意力Transformer网络提升图像质量。


<details>
  <summary>Details</summary>
Motivation: TIR图像缺乏颜色和纹理信息，现有方法因单波段限制导致图像失真和语义模糊，多波段红外图像能提供更丰富的细节和语义准确性。

Method: 使用GAN框架，生成器为MTSIC网络，结合多波段特征映射和空间-光谱注意力残差块（SARB），通过Transformer单阶段网络（STformer）和多尺度小波块（MSWB）优化重建质量。

Result: 实验表明，该方法显著优于传统技术，有效提升红外图像的视觉质量。

Conclusion: MTSIC框架通过多波段特征和注意力机制，成功解决了红外图像彩色化中的语义模糊问题，提升了图像质量。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [345] [CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study](https://arxiv.org/abs/2506.18106)
*Tingrui Zhang,Honglin Wu,Zekun Jiang,Yingying Wang,Rui Ye,Huiming Ni,Chang Liu,Jin Cao,Xuan Sun,Rong Shao,Xiaorong Wei,Yingchun Sun*

Main category: eess.IV

TL;DR: 开发并验证了一种基于CT影像组学的可解释机器学习模型，用于子宫内膜癌（EC）的良恶性诊断。随机森林模型表现最佳，测试AUC为0.96，SHAP分析揭示了关键特征，临床实用性高。


<details>
  <summary>Details</summary>
Motivation: 旨在为子宫内膜癌患者提供一种高诊断性能且可解释的辅助工具，以减少不必要的干预。

Method: 纳入83例EC患者，手动分割CT扫描的ROI，提取1132个影像组学特征，比较六种机器学习模型，使用SHAP分析和特征映射可视化。

Result: 随机森林模型表现最优，测试AUC为0.96，SHAP分析显示所有特征均显著相关，决策曲线分析表明模型具有临床实用性。

Conclusion: 基于CT影像组学的可解释机器学习模型诊断性能高，可作为EC诊断的智能辅助工具。

Abstract: Aimed to develop and validate a CT radiomics-based explainable machine
learning model for diagnosing malignancy and benignity specifically in
endometrial cancer (EC) patients. A total of 83 EC patients from two centers,
including 46 with malignant and 37 with benign conditions, were included, with
data split into a training set (n=59) and a testing set (n=24). The regions of
interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132
radiomic features were extracted from the pre-surgical CT scans using
Pyradiomics. Six explainable machine learning modeling algorithms were
implemented respectively, for determining the optimal radiomics pipeline. The
diagnostic performance of the radiomic model was evaluated by using
sensitivity, specificity, accuracy, precision, F1 score, confusion matrices,
and ROC curves. To enhance clinical understanding and usability, we separately
implemented SHAP analysis and feature mapping visualization, and evaluated the
calibration curve and decision curve. By comparing six modeling strategies, the
Random Forest model emerged as the optimal choice for diagnosing EC, with a
training AUC of 1.00 and a testing AUC of 0.96. SHAP identified the most
important radiomic features, revealing that all selected features were
significantly associated with EC (P < 0.05). Radiomics feature maps also
provide a feasible assessment tool for clinical applications. DCA indicated a
higher net benefit for our model compared to the "All" and "None" strategies,
suggesting its clinical utility in identifying high-risk cases and reducing
unnecessary interventions. In conclusion, the CT radiomics-based explainable
machine learning model achieved high diagnostic performance, which could be
used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.

</details>


### [346] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习和双层次类别平衡方案（BLCB-CNN）的视网膜血管分割方法，通过预处理和分类平衡提升性能。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割因数据分布不均和血管厚度变化而具有挑战性，需解决类别不平衡问题。

Method: 使用CNN架构和双层次类别平衡方案（Level-I平衡血管/非血管，Level-II平衡厚/薄血管），结合预处理技术（GCN、CLAHE、gamma校正）。

Result: 在标准数据集上表现优异（AUC 98.23%，准确率96.22%），并通过STARE图像验证了泛化能力。

Conclusion: BLCB-CNN方案有效解决了视网膜血管分割中的类别不平衡问题，具有高精度和泛化能力。

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>
