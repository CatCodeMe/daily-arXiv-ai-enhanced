{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289", "abs": "https://arxiv.org/abs/2506.09289", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUTGenerator\u548cUTBoost\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u89e3\u51b3SWE-Bench\u4e2d\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u5927\u91cf\u9519\u8bef\u8865\u4e01\u3002", "motivation": "SWE-Bench\u4e2d\u624b\u52a8\u7f16\u5199\u7684\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8865\u4e01\u53ef\u80fd\u901a\u8fc7\u6d4b\u8bd5\u4f46\u672a\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u5f15\u5165UTGenerator\uff08\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\uff09\u548cUTBoost\uff08\u6d4b\u8bd5\u7528\u4f8b\u589e\u5f3a\u6846\u67b6\uff09\uff0c\u81ea\u52a8\u5206\u6790\u4ee3\u7801\u5e93\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728\u8bc4\u4f30\u4e2d\u53d1\u73b036\u4e2a\u4efb\u52a1\u5b9e\u4f8b\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u8db3\uff0c\u5e76\u8bc6\u522b\u51fa345\u4e2a\u9519\u8bef\u8865\u4e01\u3002\u8fd9\u4e9b\u4fee\u6b63\u5f71\u54cd\u4e86SWE-Bench Lite\u548cVerified\u6392\u884c\u699c\u768440.9%\u548c24.4%\u6761\u76ee\uff0c\u5206\u522b\u5bfc\u81f418\u548c11\u4e2a\u6392\u540d\u53d8\u5316\u3002", "conclusion": "UTGenerator\u548cUTBoost\u80fd\u6709\u6548\u63d0\u5347\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u51cf\u5c11\u9519\u8bef\u8865\u4e01\u7684\u8bef\u5224\u3002"}}
{"id": "2506.09370", "pdf": "https://arxiv.org/pdf/2506.09370", "abs": "https://arxiv.org/abs/2506.09370", "authors": ["Rohit Mehra", "Priyavanshi Pathania", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study", "categories": ["cs.SE"], "comment": "3 pages. To be published in the proceedings of 38th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2023),\n  Kirchberg, Luxembourg", "summary": "Advances in technologies like artificial intelligence and metaverse have led\nto a proliferation of software systems in business and everyday life. With this\nwidespread penetration, the carbon emissions of software are rapidly growing as\nwell, thereby negatively impacting the long-term sustainability of our\nenvironment. Hence, optimizing software from a sustainability standpoint\nbecomes more crucial than ever. We believe that the adoption of automated tools\nthat can identify energy-inefficient patterns in the code and guide appropriate\nrefactoring can significantly assist in this optimization. In this extended\nabstract, we present an industry case study that evaluates the sustainability\nimpact of refactoring energy-inefficient code patterns identified by automated\nsoftware sustainability assessment tools for a large application. Preliminary\nresults highlight a positive impact on the application's sustainability\npost-refactoring, leading to a 29% decrease in per-user per-month energy\nconsumption.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u4f18\u5316\u8f6f\u4ef6\u4ee3\u7801\u4ee5\u51cf\u5c11\u78b3\u6392\u653e\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u5143\u5b87\u5b99\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8f6f\u4ef6\u7cfb\u7edf\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bfc\u81f4\u78b3\u6392\u653e\u589e\u52a0\uff0c\u4e9f\u9700\u4ece\u53ef\u6301\u7eed\u6027\u89d2\u5ea6\u4f18\u5316\u8f6f\u4ef6\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u5de5\u5177\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u80fd\u6e90\u4f4e\u6548\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u4f18\u5316\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u91cd\u6784\u540e\u6bcf\u7528\u6237\u6bcf\u6708\u80fd\u8017\u964d\u4f4e\u4e8629%\u3002", "conclusion": "\u81ea\u52a8\u5316\u5de5\u5177\u5728\u63d0\u5347\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2506.09396", "pdf": "https://arxiv.org/pdf/2506.09396", "abs": "https://arxiv.org/abs/2506.09396", "authors": ["Zongjie Li", "Shuai Wang"], "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u63a8\u7406\u6df1\u5ea6\u4f5c\u4e3a\u53ef\u63a7\u8d44\u6e90\uff0c\u4f18\u5316\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u8bbe\u8ba1\uff0c\u4ee5\u5e73\u8861\u5feb\u901f\u76f4\u63a5\u7b54\u6848\u4e0e\u8be6\u7ec6\u63a8\u7406\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u751f\u6210\u6a21\u578b\u672a\u660e\u786e\u7ba1\u7406\u63a8\u7406\u6df1\u5ea6\uff0c\u5bfc\u81f4\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u4e4b\u95f4\u96be\u4ee5\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u63a7\u5236\u63a8\u7406\u6df1\u5ea6\uff0c\u4ece\u5408\u6210\u6570\u636e\u521b\u5efa\u5230\u5b9e\u9645\u90e8\u7f72\uff0c\u4f18\u5316\u6a21\u578b\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u76d1\u7763\u4fe1\u53f7\u3001\u63a8\u52a8\u591a\u7ef4\u5ea6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u652f\u6301\u6210\u672c\u548c\u5b89\u5168\u610f\u8bc6\u7684\u90e8\u7f72\u7b56\u7565\u3002", "conclusion": "\u5c06\u5feb\u901f\u4e0e\u6162\u901f\u63a8\u7406\u89c6\u4e3a\u4e92\u8865\u6a21\u5f0f\uff0c\u53ef\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5728\u5fc5\u8981\u65f6\u6df1\u5165\u601d\u8003\uff0c\u5728\u53ef\u80fd\u65f6\u5feb\u901f\u884c\u52a8\u3002"}}
{"id": "2506.09052", "pdf": "https://arxiv.org/pdf/2506.09052", "abs": "https://arxiv.org/abs/2506.09052", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "7 Pages", "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLlama 3\u7684\u6297\u4f53-\u6297\u539f\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u6a21\u578b\uff08LlamaAffinity\uff09\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6297\u4f53\u4eb2\u548c\u529b\u6d4b\u91cf\u65b9\u6cd5\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u4eba\u5de5\u667a\u80fd\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u6297\u4f53\u8bbe\u8ba1\u548c\u4eb2\u548c\u529b\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u5229\u7528\u5f00\u6e90Llama 3\u6846\u67b6\u548cObserved Antibody Space\uff08OAS\uff09\u6570\u636e\u5e93\u7684\u6297\u4f53\u5e8f\u5217\u6570\u636e\uff0c\u5f00\u53d1\u4e86LlamaAffinity\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u51c6\u786e\u7387\uff080.9640\uff09\u3001F1\u5206\u6570\uff080.9643\uff09\u3001\u7cbe\u786e\u5ea6\uff080.9702\uff09\u3001\u53ec\u56de\u7387\uff080.9586\uff09\u548cAUC-ROC\uff080.9936\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff080.46\u5c0f\u65f6\uff09\u3002", "conclusion": "LlamaAffinity\u6a21\u578b\u5728\u6297\u4f53-\u6297\u539f\u4eb2\u548c\u529b\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6297\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2506.09061", "pdf": "https://arxiv.org/pdf/2506.09061", "abs": "https://arxiv.org/abs/2506.09061", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "categories": ["cs.DC", "cs.AI", "cs.PF"], "comment": "4 figures, 7 pages, IEEE conference template", "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "AI": {"tldr": "EdgeProfiler\u662f\u4e00\u4e2a\u5feb\u901f\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fb9\u7f18\u7cfb\u7edf\u4e0a\u7684\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u6280\u672f\u548c\u5185\u5b58\u7ea6\u675f\u4f18\u5316\u6027\u80fd\u3002", "motivation": "LLMs\u7684\u9ad8\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u529f\u8017\u9700\u6c42\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0cEdgeProfiler\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6846\u67b6\u91c7\u7528\u91cf\u5316\u6280\u672f\u548c\u5185\u5b58\u7ea6\u675f\u5206\u6790\u8f7b\u91cf\u7ea7LLMs\uff0c\u5305\u62ecTinyLLaMA\u7b49\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u4f30\u8ba1\u5ef6\u8fdf\u3001FLOPs\u548c\u80fd\u8017\u3002", "result": "4\u4f4d\u91cf\u5316\u51cf\u5c11\u5185\u5b58\u4f7f\u752860-70%\uff0c\u7cbe\u5ea6\u635f\u59312-5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472-3\u500d\uff0c\u80fd\u8017\u964d\u4f4e35-50%\u3002", "conclusion": "EdgeProfiler\u4e3a\u8f7b\u91cf\u7ea7LLMs\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5e73\u8861\u7cbe\u5ea6\u3001\u80fd\u8017\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09966", "pdf": "https://arxiv.org/pdf/2506.09966", "abs": "https://arxiv.org/abs/2506.09966", "authors": ["Jos\u00e9 Luis Balc\u00e1zar"], "title": "Tight Paths and Tight Pairs in Weighted Directed Graphs", "categories": ["cs.DS", "cs.DM"], "comment": null, "summary": "We state the graph-theoretic computational problem of finding tight paths in\na directed, edge-weighted graph, as well as its simplification of finding tight\npairs. These problems are motivated by the need of algorithms that find\nso-called basic antecedents in closure spaces, in one specific approach to data\nanalysis. We discuss and compare several algorithms to approach these problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6709\u5411\u8fb9\u52a0\u6743\u56fe\u4e2d\u5bfb\u627e\u7d27\u8def\u5f84\u53ca\u5176\u7b80\u5316\u95ee\u9898\uff08\u7d27\u5bf9\uff09\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u6570\u636e\u5206\u6790\u4e2d\u5bfb\u627e\u95ed\u5305\u7a7a\u95f4\u4e2d\u7684\u57fa\u672c\u524d\u9a71\u9700\u6c42\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u6570\u636e\u5206\u6790\u4e2d\u95ed\u5305\u7a7a\u95f4\u7684\u57fa\u672c\u524d\u9a71\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u7b97\u6cd5\u652f\u6301\u3002", "method": "\u8ba8\u8bba\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u7b97\u6cd5\u6765\u89e3\u51b3\u7d27\u8def\u5f84\u548c\u7d27\u5bf9\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u591a\u79cd\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u4e0d\u540c\u7b97\u6cd5\u7684\u9002\u7528\u6027\uff0c\u4e3a\u6570\u636e\u5206\u6790\u4e2d\u7684\u95ed\u5305\u7a7a\u95f4\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09159", "pdf": "https://arxiv.org/pdf/2506.09159", "abs": "https://arxiv.org/abs/2506.09159", "authors": ["Antonio Calagna", "Yenchia Yu", "Paolo Giaccone", "Carla Fabiana Chiasserini"], "title": "MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge", "categories": ["cs.NI"], "comment": null, "summary": "Stateful migration has emerged as the dominant technology to support\nmicroservice mobility at the network edge while ensuring a satisfying\nexperience to mobile end users. This work addresses two pivotal challenges,\nnamely, the implementation and the orchestration of the migration process. We\nfirst introduce a novel framework that efficiently implements stateful\nmigration and effectively orchestrates the migration process by fulfilling both\nnetwork and application KPI targets. Through experimental validation using\nrealistic microservices, we then show that our solution (i) greatly improves\nmigration performance, yielding up to 77% decrease of the migration downtime\nwith respect to the state of the art, and (ii) successfully addresses the\nstrict user QoE requirements of critical scenarios featuring latency-sensitive\nmicroservices. Further, we consider two practical use cases, featuring,\nrespectively, a UAV autopilot microservice and a multi-object tracking task,\nand demonstrate how our framework outperforms current state-of-the-art\napproaches in configuring the migration process and in meeting KPI targets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u5fae\u670d\u52a1\u79fb\u52a8\u6027\u7684\u72b6\u6001\u8fc1\u79fb\u6280\u672f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8fc1\u79fb\u8fc7\u7a0b\u7684\u5b9e\u73b0\u4e0e\u7f16\u6392\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u6ee1\u8db3\u4e86\u4e25\u683c\u7684\u7528\u6237\u4f53\u9a8c\u8981\u6c42\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5fae\u670d\u52a1\u79fb\u52a8\u6027\u7684\u72b6\u6001\u8fc1\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u7f51\u7edc\u548c\u5e94\u7528KPI\u76ee\u6807\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u73b0\u72b6\u6001\u8fc1\u79fb\u5e76\u667a\u80fd\u7f16\u6392\u8fc1\u79fb\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5c06\u8fc1\u79fb\u505c\u673a\u65f6\u95f4\u51cf\u5c1177%\uff0c\u5e76\u6210\u529f\u6ee1\u8db3\u4e86\u5ef6\u8fdf\u654f\u611f\u5fae\u670d\u52a1\u7684\u4e25\u683c\u7528\u6237\u4f53\u9a8c\u8981\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u914d\u7f6e\u8fc1\u79fb\u8fc7\u7a0b\u548c\u6ee1\u8db3KPI\u76ee\u6807\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u81ea\u52a8\u9a7e\u9a76\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2506.09226", "pdf": "https://arxiv.org/pdf/2506.09226", "abs": "https://arxiv.org/abs/2506.09226", "authors": ["Bowen Wu", "Wei Cui", "Carlo Curino", "Matteo Interlandi", "Rathijit Sen"], "title": "Terabyte-Scale Analytics in the Blink of an Eye", "categories": ["cs.DB", "cs.DC", "cs.PF"], "comment": null, "summary": "For the past two decades, the DB community has devoted substantial research\nto take advantage of cheap clusters of machines for distributed data analytics\n-- we believe that we are at the beginning of a paradigm shift. The scaling\nlaws and popularity of AI models lead to the deployment of incredibly powerful\nGPU clusters in commercial data centers. Compared to CPU-only solutions, these\nclusters deliver impressive improvements in per-node compute, memory bandwidth,\nand inter-node interconnect performance. In this paper, we study the problem of\nscaling analytical SQL queries on distributed clusters of GPUs, with the stated\ngoal of establishing an upper bound on the likely performance gains. To do so,\nwe build a prototype designed to maximize performance by leveraging ML/HPC best\npractices, such as group communication primitives for cross-device data\nmovements. This allows us to conduct thorough performance experimentation to\npoint our community towards a massive performance opportunity of at least\n60$\\times$. To make these gains more relatable, before you can blink twice, our\nsystem can run all 22 queries of TPC-H at a 1TB scale factor!", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5206\u5e03\u5f0fGPU\u96c6\u7fa4\u4e0a\u6269\u5c55\u5206\u6790\u6027SQL\u67e5\u8be2\u7684\u6027\u80fd\u6f5c\u529b\uff0c\u76ee\u6807\u662f\u786e\u5b9a\u6027\u80fd\u63d0\u5347\u7684\u4e0a\u9650\u3002\u901a\u8fc7\u539f\u578b\u8bbe\u8ba1\u548c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u81f3\u5c1160\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u7684\u666e\u53ca\u548cGPU\u96c6\u7fa4\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u4f18\u5316\u5206\u5e03\u5f0f\u6570\u636e\u5206\u6790\uff0c\u63a2\u7d22\u6027\u80fd\u63d0\u5347\u7684\u6781\u9650\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\uff0c\u91c7\u7528ML/HPC\u6700\u4f73\u5b9e\u8df5\uff08\u5982\u8de8\u8bbe\u5907\u6570\u636e\u79fb\u52a8\u7684\u7ec4\u901a\u4fe1\u539f\u8bed\uff09\uff0c\u4ee5\u6700\u5927\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u63d0\u5347\u81f3\u5c1160\u500d\uff0c\u7cfb\u7edf\u80fd\u5728\u6781\u77ed\u65f6\u95f4\u5185\u5b8c\u6210TPC-H 1TB\u89c4\u6a21\u7684\u6240\u670922\u4e2a\u67e5\u8be2\u3002", "conclusion": "GPU\u96c6\u7fa4\u4e3a\u5206\u5e03\u5f0f\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6027\u80fd\u673a\u4f1a\uff0c\u793e\u533a\u5e94\u5173\u6ce8\u8fd9\u4e00\u6f5c\u529b\u3002"}}
{"id": "2506.09550", "pdf": "https://arxiv.org/pdf/2506.09550", "abs": "https://arxiv.org/abs/2506.09550", "authors": ["Fanpeng Yang", "Xu Ma", "Shuling Wang", "Xiong Xu", "Qinxiang Cao", "Naijun Zhan", "Xiaofeng Li", "Bin Gu"], "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries", "categories": ["cs.SE"], "comment": null, "summary": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u6267\u884c\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210C\u7a0b\u5e8f\u7684\u51fd\u6570\u6458\u8981\uff0c\u652f\u6301\u591a\u79cd\u62bd\u8c61\u7ea7\u522b\u3002", "motivation": "\u4efb\u52a1\u5173\u952e\u578b\u9057\u7559\u4ee3\u7801\u7f3a\u4e4f\u5f62\u5f0f\u5316\u89c4\u8303\uff0c\u4e14C\u7a0b\u5e8f\u7684\u590d\u6742\u7279\u6027\uff08\u5982\u5faa\u73af\u3001\u6307\u9488\u522b\u540d\u7b49\uff09\u4f7f\u5f97\u81ea\u52a8\u751f\u6210\u51fd\u6570\u6458\u8981\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408VST-A\u7684\u7b26\u53f7\u6267\u884c\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u5faa\u73af\u4e0d\u53d8\u91cf\uff0c\u4ee5\u53caFrama-C\u7684\u5f62\u5f0f\u9a8c\u8bc1\uff0c\u751f\u6210\u76f8\u5bf9\u6700\u5f3a\u540e\u6761\u4ef6\uff08RSPs\uff09\u5e76\u6784\u5efa\u51fd\u6570\u6458\u8981\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5b8c\u5168\u6355\u83b7\u7a0b\u5e8f\u884c\u4e3a\u7684\u51fd\u6570\u6458\u8981\uff0c\u5e76\u652f\u6301\u591a\u79cd\u62bd\u8c61\u7ea7\u522b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86C\u7a0b\u5e8f\u51fd\u6570\u6458\u8981\u751f\u6210\u7684\u6311\u6218\uff0c\u9002\u7528\u4e8e\u5f62\u5f0f\u9a8c\u8bc1\u548c\u4eba\u7c7b\u7406\u89e3\u7684\u9700\u6c42\u3002"}}
{"id": "2506.09080", "pdf": "https://arxiv.org/pdf/2506.09080", "abs": "https://arxiv.org/abs/2506.09080", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "comment": null, "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "AI": {"tldr": "FinHEAR\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\u63a8\u7406\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u7f3a\u4e4f\u5bf9\u884c\u4e3a\u6a21\u5f0f\uff08\u5982\u4fe1\u606f\u4e0d\u5bf9\u79f0\u4e0b\u7684\u4e13\u5bb6\u4f9d\u8d56\u3001\u635f\u5931\u538c\u6076\u654f\u611f\u6027\u7b49\uff09\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "FinHEAR\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u5386\u53f2\u8d8b\u52bf\u5206\u6790\u3001\u4e8b\u4ef6\u89e3\u8bfb\u548c\u4e13\u5bb6\u6307\u5bfc\u68c0\u7d22\uff0c\u878d\u5165\u884c\u4e3a\u7ecf\u6d4e\u5b66\u539f\u7406\u3002", "result": "\u5728\u91d1\u878d\u6570\u636e\u96c6\u4e0a\uff0cFinHEAR\u5728\u8d8b\u52bf\u9884\u6d4b\u548c\u4ea4\u6613\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u548c\u98ce\u9669\u8c03\u6574\u6536\u76ca\u66f4\u9ad8\u3002", "conclusion": "FinHEAR\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u548c\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2506.09242", "pdf": "https://arxiv.org/pdf/2506.09242", "abs": "https://arxiv.org/abs/2506.09242", "authors": ["Jonas Latt", "Christophe Coreixas"], "title": "Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism", "categories": ["cs.DC"], "comment": null, "summary": "This article presents the principles, software architecture, and performance\nanalysis of the GPU port of the lattice Boltzmann software library Palabos (J.\nLatt et al., \"Palabos: Parallel lattice Boltzmann solver\", Comput. Math. Appl.\n81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which\nnumerical components are selectively assigned to either the CPU or the GPU,\ndepending on considerations of performance or convenience. This design enables\na progressive porting strategy, allowing most features of the original\nCPU-based codebase to be gradually and seamlessly adapted to GPU execution. The\nnew architecture builds upon two complementary paradigms: a classical\nobject-oriented structure for CPU execution, and a data-oriented counterpart\nfor GPUs, which reproduces the modularity of the original code while\neliminating object-oriented overhead detrimental to GPU performance. Central to\nthis approach is the use of modern C++, including standard parallel algorithms\nand template metaprogramming techniques, which permit the generation of\nhardware-agnostic computational kernels. This facilitates the development of\nuser-defined, GPU-accelerated components such as collision operators or\nboundary conditions, while preserving compatibility with the existing codebase\nand avoiding the need for external libraries or non-standard language\nextensions. The correctness and performance of the GPU-enabled Palabos are\ndemonstrated through a series of three-dimensional multiphysics benchmarks,\nincluding the laminar-turbulent transition in a Taylor-Green vortex, lid-driven\ncavity flow, and pore-scale flow in Berea sandstone. Despite the high-level\nabstraction of the implementation, the single-GPU performance is similar to\nCUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling\nacross all test cases.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Palabos\u5e93\u7684GPU\u79fb\u690d\u7248\u672c\uff0c\u91c7\u7528\u6df7\u5408CPU-GPU\u6267\u884c\u6a21\u578b\uff0c\u901a\u8fc7\u73b0\u4ee3C++\u6280\u672f\u5b9e\u73b0\u786c\u4ef6\u65e0\u5173\u7684\u8ba1\u7b97\u5185\u6838\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u751fCUDA\u6c42\u89e3\u5668\u3002", "motivation": "\u5c06Palabos\u5e93\u4eceCPU\u79fb\u690d\u5230GPU\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u4ee3\u7801\u517c\u5bb9\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408CPU-GPU\u6267\u884c\u6a21\u578b\uff0c\u7ed3\u5408\u9762\u5411\u5bf9\u8c61\uff08CPU\uff09\u548c\u6570\u636e\u5bfc\u5411\uff08GPU\uff09\u7684\u67b6\u6784\uff0c\u5229\u7528\u73b0\u4ee3C++\u6280\u672f\u751f\u6210\u786c\u4ef6\u65e0\u5173\u7684\u8ba1\u7b97\u5185\u6838\u3002", "result": "\u5728\u4e09\u7ef4\u591a\u7269\u7406\u573a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355GPU\u6027\u80fd\u63a5\u8fd1\u539f\u751fCUDA\u6c42\u89e3\u5668\uff0c\u591aGPU\u6d4b\u8bd5\u8868\u73b0\u51fa\u826f\u597d\u7684\u5f31\u6269\u5c55\u548c\u5f3a\u6269\u5c55\u6027\u3002", "conclusion": "GPU\u79fb\u690d\u7684Palabos\u5e93\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4ee3\u7801\u7684\u6e10\u8fdb\u5f0f\u79fb\u690d\u548c\u7528\u6237\u81ea\u5b9a\u4e49\u7ec4\u4ef6\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.09959", "pdf": "https://arxiv.org/pdf/2506.09959", "abs": "https://arxiv.org/abs/2506.09959", "authors": ["Max Lovig", "Conor Sheehan", "Konstantinos Tsirkas", "Ilias Zadik"], "title": "Almost-Optimal Local-Search Methods for Sparse Tensor PCA", "categories": ["math.ST", "cs.DS", "stat.ML", "stat.TH"], "comment": null, "summary": "Local-search methods are widely employed in statistical applications, yet\ninterestingly, their theoretical foundations remain rather underexplored,\ncompared to other classes of estimators such as low-degree polynomials and\nspectral methods. Of note, among the few existing results recent studies have\nrevealed a significant \"local-computational\" gap in the context of a\nwell-studied sparse tensor principal component analysis (PCA), where a broad\nclass of local Markov chain methods exhibits a notable underperformance\nrelative to other polynomial-time algorithms. In this work, we propose a series\nof local-search methods that provably \"close\" this gap to the best known\npolynomial-time procedures in multiple regimes of the model, including and\ngoing beyond the previously studied regimes in which the broad family of local\nMarkov chain methods underperforms. Our framework includes: (1) standard greedy\nand randomized greedy algorithms applied to the (regularized) posterior of the\nmodel; and (2) novel random-threshold variants, in which the randomized greedy\nalgorithm accepts a proposed transition if and only if the corresponding change\nin the Hamiltonian exceeds a random Gaussian threshold-rather that if and only\nif it is positive, as is customary. The introduction of the random thresholds\nenables a tight mathematical analysis of the randomized greedy algorithm's\ntrajectory by crucially breaking the dependencies between the iterations, and\ncould be of independent interest to the community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\uff0c\u65e8\u5728\u586b\u8865\u7a00\u758f\u5f20\u91cf\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u4e2d\u5c40\u90e8\u8ba1\u7b97\u65b9\u6cd5\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u8303\u56f4\u5185\u8fbe\u5230\u5df2\u77e5\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u7684\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u5728\u7edf\u8ba1\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u76f8\u5bf9\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u5f20\u91cfPCA\u4e2d\uff0c\u73b0\u6709\u7814\u7a76\u8868\u660e\u5c40\u90e8\u8ba1\u7b97\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u5305\u62ec\u6807\u51c6\u8d2a\u5a6a\u7b97\u6cd5\u3001\u968f\u673a\u8d2a\u5a6a\u7b97\u6cd5\u53ca\u5176\u53d8\u4f53\uff08\u5982\u968f\u673a\u9608\u503c\u53d8\u4f53\uff09\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u968f\u673a\u9ad8\u65af\u9608\u503c\u6765\u4f18\u5316\u7b97\u6cd5\u6027\u80fd\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u8303\u56f4\u5185\u6210\u529f\u586b\u8865\u4e86\u5c40\u90e8\u8ba1\u7b97\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u5c40\u90e8\u9a6c\u5c14\u53ef\u592b\u94fe\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u968f\u673a\u9608\u503c\u7684\u5f15\u5165\u4e3a\u968f\u673a\u8d2a\u5a6a\u7b97\u6cd5\u7684\u8f68\u8ff9\u5206\u6790\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u53ef\u80fd\u5bf9\u5176\u4ed6\u7814\u7a76\u9886\u57df\u4e5f\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2506.09197", "pdf": "https://arxiv.org/pdf/2506.09197", "abs": "https://arxiv.org/abs/2506.09197", "authors": ["Sushi Anna George", "Vinay Joseph"], "title": "Adaptive Bandwidth Sharing for Optimizing QoE of Real-Time Video", "categories": ["cs.NI"], "comment": "arXiv admin note: text overlap with arXiv:2401.10681", "summary": "The concept of spectrum or bandwidth sharing has gained significant global\nattention as a means to enhance the efficiency of real-time traffic management\nin wireless networks. Effective bandwidth sharing enables optimal utilization\nof available resources, reducing congestion and improving QoE for\ndelay-sensitive applications such as real-time video transmission. In this\npaper, we propose a novel iterative semi-static bandwidth sharing policy that\nbalances the advantages of both static and dynamic sharing approaches. Our\napproach minimizes the frequency of coordination between network operators\nwhile ensuring efficient resource allocation and meeting the stringent QoE\ndemands of real-time traffic. The proposed policy iteratively optimizes both\nthe spectrum sharing between operators and the resource allocation for\nindividual clients. We establish strong theoretical guarantees for the\noptimality of the proposed policy and prove that it converges to the optimal\nstatic sharing policy irrespective of initial conditions or fluctuations in\ntraffic arrival rates. Additionally, we conduct extensive simulations to\nevaluate the impact of key system parameters - including step size, hyperperiod\nlength, and arrival process dynamics - on the performance of our policy. Our\nresults demonstrate the effectiveness of the proposed approach in achieving\nnear-optimal bandwidth allocation with reduced overhead, making it a practical\nsolution for real-time wireless applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fed\u4ee3\u534a\u9759\u6001\u5e26\u5bbd\u5171\u4eab\u7b56\u7565\uff0c\u5e73\u8861\u9759\u6001\u548c\u52a8\u6001\u5171\u4eab\u7684\u4f18\u70b9\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u5e76\u6ee1\u8db3\u5b9e\u65f6\u6d41\u91cf\u9700\u6c42\u3002", "motivation": "\u63d0\u9ad8\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u65f6\u6d41\u91cf\u7ba1\u7406\u7684\u6548\u7387\uff0c\u51cf\u5c11\u62e5\u585e\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c\uff08QoE\uff09\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u534a\u9759\u6001\u5e26\u5bbd\u5171\u4eab\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u8fd0\u8425\u5546\u95f4\u7684\u534f\u8c03\u9891\u7387\uff0c\u540c\u65f6\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u7b56\u7565\u7684\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5b9e\u73b0\u8fd1\u6700\u4f18\u5e26\u5bbd\u5206\u914d\u4e14\u964d\u4f4e\u5f00\u9500\u3002", "conclusion": "\u8be5\u7b56\u7565\u662f\u5b9e\u65f6\u65e0\u7ebf\u5e94\u7528\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u4f4e\u5f00\u9500\u3002"}}
{"id": "2506.09467", "pdf": "https://arxiv.org/pdf/2506.09467", "abs": "https://arxiv.org/abs/2506.09467", "authors": ["Wu Min", "Qiao Yuncong", "Yu Tan", "Chenghu Yang"], "title": "ArcNeural: A Multi-Modal Database for the Gen-AI Era", "categories": ["cs.DB"], "comment": null, "summary": "ArcNeural introduces a novel multimodal database tailored for the demands of\nGenerative AI and Large Language Models, enabling efficient management of\ndiverse data types such as graphs, vectors, and documents. Its storage-compute\nseparated architecture integrates graph technology, advanced vector indexing,\nand transaction processing to support real-time analytics and AI-driven\napplications. Key features include a unified storage layer, adaptive edge\ncollection in MemEngine, and seamless integration of transaction and analytical\nprocessing. Experimental evaluations demonstrate ArcNeural's superior\nperformance and scalability compared to state-of-the-art systems. This system\nbridges structured and unstructured data management, offering a versatile\nsolution for enterprise-grade AI applications.\n  ArcNeural's design addresses the challenges of multimodal data processing,\nproviding a robust framework for intelligent, data-driven solutions in the Gen\nAI era.", "AI": {"tldr": "ArcNeural\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u5e93\uff0c\u4e13\u4e3a\u751f\u6210\u5f0fAI\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\uff0c\u652f\u6301\u56fe\u5f62\u3001\u5411\u91cf\u548c\u6587\u6863\u7b49\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u5177\u6709\u5b58\u50a8\u4e0e\u8ba1\u7b97\u5206\u79bb\u7684\u67b6\u6784\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u751f\u6210\u5f0fAI\u65f6\u4ee3\u63d0\u4f9b\u667a\u80fd\u3001\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5b58\u50a8\u4e0e\u8ba1\u7b97\u5206\u79bb\u7684\u67b6\u6784\uff0c\u7ed3\u5408\u56fe\u5f62\u6280\u672f\u3001\u9ad8\u7ea7\u5411\u91cf\u7d22\u5f15\u548c\u4e8b\u52a1\u5904\u7406\uff0c\u652f\u6301\u5b9e\u65f6\u5206\u6790\u548cAI\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aArcNeural\u5728\u6027\u80fd\u548c\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "ArcNeural\u4e3a\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4f01\u4e1a\u7ea7AI\u5e94\u7528\u3002"}}
{"id": "2506.09601", "pdf": "https://arxiv.org/pdf/2506.09601", "abs": "https://arxiv.org/abs/2506.09601", "authors": ["Sota Nakashima", "Yuta Ishimoto", "Masanari Kondo", "Tao Xiao", "Yasutaka Kamei"], "title": "ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Technical debt refers to suboptimal code that degrades software quality. When\ndevelopers intentionally introduce such debt, it is called self-admitted\ntechnical debt (SATD). Since SATD hinders maintenance, identifying its\ncategories is key to uncovering quality issues. Traditionally, constructing\nsuch taxonomies requires manually inspecting SATD comments and surrounding\ncode, which is time-consuming, labor-intensive, and often inconsistent due to\nannotator subjectivity. This study presents ASTAGEN, an initial step toward\nautomating SATD taxonomy generation using large language models (LLMs). Given a\ncomment and its surrounding code, ASTAGEN first generates a concise explanation\nfor each SATD comment, then incrementally generates and updates categories to\nconstruct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:\nquantum software, smart contracts, and machine learning. It successfully\nrecovers domain-specific categories reported in prior work, such as Layer\nConfiguration in machine learning. Compared to a naive use of an LLM, ASTAGEN\nproduces more consistent category assignments due to its explanation-driven,\niterative design. It also completes taxonomy generation in under two hours and\nfor less than one USD, even on the largest dataset. These results suggest that\nwhile full automation remains challenging, ASTAGEN is able to support\nsemi-automated taxonomy construction. Furthermore, our work opens up avenues\nfor future work, such as automatic taxonomy generation in other areas.", "AI": {"tldr": "ASTAGEN\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210SATD\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u89e3\u91ca\u9a71\u52a8\u7684\u8fed\u4ee3\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u77ed\u65f6\u95f4\u5185\u4f4e\u6210\u672c\u5b8c\u6210\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u6784\u5efaSATD\u5206\u7c7b\u6cd5\u8017\u65f6\u3001\u8d39\u529b\u4e14\u4e0d\u4e00\u81f4\uff0cASTAGEN\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ASTAGEN\u901a\u8fc7\u751f\u6210SATD\u8bc4\u8bba\u7684\u7b80\u660e\u89e3\u91ca\uff0c\u5e76\u8fed\u4ee3\u66f4\u65b0\u5206\u7c7b\u6cd5\uff0c\u5229\u7528LLM\u5b9e\u73b0\u81ea\u52a8\u5316\u3002", "result": "ASTAGEN\u5728\u4e09\u4e2a\u9886\u57df\u7684SATD\u6570\u636e\u96c6\u4e2d\u6210\u529f\u6062\u590d\u5df2\u77e5\u5206\u7c7b\uff0c\u5982\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u201c\u5c42\u914d\u7f6e\u201d\uff0c\u4e14\u6210\u672c\u4f4e\u3001\u65f6\u95f4\u77ed\u3002", "conclusion": "ASTAGEN\u652f\u6301\u534a\u81ea\u52a8\u5206\u7c7b\u6cd5\u6784\u5efa\uff0c\u4e3a\u672a\u6765\u5176\u4ed6\u9886\u57df\u7684\u81ea\u52a8\u5206\u7c7b\u6cd5\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2506.09084", "pdf": "https://arxiv.org/pdf/2506.09084", "abs": "https://arxiv.org/abs/2506.09084", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u7684\u5956\u52b1\u5fae\u8c03\u65b9\u6cd5PageLLM\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6574\u9875\u4f18\u5316\uff08WPO\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u9875\u9762\u7ea7\u548c\u9879\u76ee\u7ea7\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u5982\u6574\u9875\u4f18\u5316\uff08WPO\uff09\u4e2d\u7684\u5fae\u8c03\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u7528\u6237\u53cd\u9988\u867d\u6613\u83b7\u53d6\u4f46\u566a\u58f0\u591a\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u7528\u6237\u53cd\u9988\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faPageLLM\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u7c92\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u7ed3\u5408\u9875\u9762\u7ea7\u5956\u52b1\uff08\u8bc4\u4f30\u6574\u4f53\u8d28\u91cf\uff09\u548c\u9879\u76ee\u7ea7\u5956\u52b1\uff08\u8bc4\u4f30\u5173\u952e\u63a8\u8350\u51c6\u786e\u6027\uff09\uff0c\u4ee5\u7528\u6237\u53cd\u9988\u4e3a\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u516c\u5f00\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPageLLM\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u8d85\u8fc71000\u4e07\u7528\u6237\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u5b9e\u73b00.44%\u7684GMV\u589e\u957f\u3002", "conclusion": "PageLLM\u901a\u8fc7\u53cc\u5956\u52b1\u673a\u5236\u6709\u6548\u5229\u7528\u7528\u6237\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86WPO\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.09275", "pdf": "https://arxiv.org/pdf/2506.09275", "abs": "https://arxiv.org/abs/2506.09275", "authors": ["Jonas Svedas", "Hannah Watson", "Nathan Laubeuf", "Diksha Moolchandani", "Abubakr Nada", "Arjun Singh", "Dwaipayan Biswas", "James Myers", "Debjyoti Bhattacharjee"], "title": "A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO", "categories": ["cs.DC"], "comment": null, "summary": "Distributed deep neural networks (DNNs) have become a cornerstone for scaling\nmachine learning to meet the demands of increasingly complex applications.\nHowever, the rapid growth in model complexity far outpaces CMOS technology\nscaling, making sustainable and efficient system design a critical challenge.\nAddressing this requires coordinated co-design across software, hardware, and\ntechnology layers. Due to the prohibitive cost and complexity of deploying\nfull-scale training systems, simulators play a pivotal role in enabling this\ndesign exploration. This survey reviews the landscape of distributed DNN\ntraining simulators, focusing on three major dimensions: workload\nrepresentation, simulation infrastructure, and models for total cost of\nownership (TCO) including carbon emissions. It covers how workloads are\nabstracted and used in simulation, outlines common workload representation\nmethods, and includes comprehensive comparison tables covering both simulation\nframeworks and TCO/emissions models, detailing their capabilities, assumptions,\nand areas of focus. In addition to synthesizing existing tools, the survey\nhighlights emerging trends, common limitations, and open research challenges\nacross the stack. By providing a structured overview, this work supports\ninformed decision-making in the design and evaluation of distributed training\nsystems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5206\u5e03\u5f0f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u62df\u5668\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u8868\u793a\u3001\u6a21\u62df\u57fa\u7840\u8bbe\u65bd\u548c\u603b\u62e5\u6709\u6210\u672c\uff08TCO\uff09\u6a21\u578b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u590d\u6742\u5ea6\u7684\u5feb\u901f\u589e\u957f\uff0c\u5206\u5e03\u5f0fDNN\u8bad\u7ec3\u7684\u7cfb\u7edf\u8bbe\u8ba1\u9762\u4e34\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u7684\u6311\u6218\uff0c\u6a21\u62df\u5668\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5de5\u4f5c\u8d1f\u8f7d\u62bd\u8c61\u3001\u6a21\u62df\u57fa\u7840\u8bbe\u65bd\u548cTCO\u6a21\u578b\uff0c\u7efc\u8ff0\u4e86\u73b0\u6709\u6a21\u62df\u6846\u67b6\u53ca\u5176\u80fd\u529b\u3001\u5047\u8bbe\u548c\u5173\u6ce8\u70b9\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7684\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u6311\u6218\u3002"}}
{"id": "2506.09245", "pdf": "https://arxiv.org/pdf/2506.09245", "abs": "https://arxiv.org/abs/2506.09245", "authors": ["Muthukrishnan Senthilkumar", "Aresh Dadlani", "Hina Tabassum"], "title": "Age of Information in Unreliable Tandem Queues", "categories": ["cs.NI"], "comment": null, "summary": "Stringent demands for timely information delivery, driven by the widespread\nadoption of real-time applications and the Internet of Things, have established\nthe age of information (AoI) as a critical metric for quantifying data\nfreshness. Existing AoI models often assume multi-hop communication networks\nwith fully reliable nodes, which may not accurately capture scenarios involving\nnode transmission failures. This paper presents an analytical framework for two\nconfigurations of tandem queue systems, where status updates generated by a\nsingle sensor are relayed to a destination monitor through unreliable\nintermediate nodes. Using the probability generating function, we first derive\nthe sojourn time distribution for an infinite-buffer M/M/1 tandem system with\ntwo unreliable nodes. We then extend our analysis to an M/G/1 tandem system\nwith an arbitrary number of unreliable nodes, employing the supplementary\nvariable technique while assuming that only the first node has an infinite\nbuffer. Numerical results demonstrate the impact of key system parameters on\nthe average AoI in unreliable tandem queues with Markovian and non-Markovian\nservice times.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u4e0d\u53ef\u9760\u8282\u70b9\u4e32\u8054\u961f\u5217\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\uff0c\u63a8\u5bfc\u4e86\u4e24\u79cd\u914d\u7f6e\u4e0b\u7684\u505c\u7559\u65f6\u95f4\u5206\u5e03\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u53c2\u6570\u5bf9AoI\u7684\u5f71\u54cd\u3002", "motivation": "\u5b9e\u65f6\u5e94\u7528\u548c\u7269\u8054\u7f51\u5bf9\u4fe1\u606f\u53ca\u65f6\u6027\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709AoI\u6a21\u578b\u5047\u8bbe\u8282\u70b9\u5b8c\u5168\u53ef\u9760\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u8282\u70b9\u4f20\u8f93\u5931\u8d25\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u6982\u7387\u751f\u6210\u51fd\u6570\u63a8\u5bfc\u65e0\u9650\u7f13\u51b2M/M/1\u4e32\u8054\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u9760\u8282\u70b9\u7684\u505c\u7559\u65f6\u95f4\u5206\u5e03\uff0c\u5e76\u6269\u5c55\u5230M/G/1\u4e32\u8054\u7cfb\u7edf\uff0c\u91c7\u7528\u8865\u5145\u53d8\u91cf\u6280\u672f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u5bf9\u9a6c\u5c14\u53ef\u592b\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u670d\u52a1\u65f6\u95f4\u7684\u4e0d\u53ef\u9760\u4e32\u8054\u961f\u5217\u7684\u5e73\u5747AoI\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0d\u53ef\u9760\u8282\u70b9\u4e32\u8054\u961f\u5217\u7cfb\u7edf\u7684AoI\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8282\u70b9\u4e0d\u53ef\u9760\u6027\u5bf9\u6570\u636e\u65b0\u9c9c\u5ea6\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.09186", "pdf": "https://arxiv.org/pdf/2506.09186", "abs": "https://arxiv.org/abs/2506.09186", "authors": ["Aaron Hurst", "Andrey V. Kalinichev", "Klaus Koren", "Daniel E. Lucani"], "title": "Not all those who drift are lost: Drift correction and calibration scheduling for the IoT", "categories": ["eess.SP", "cs.DB"], "comment": null, "summary": "Sensors provide a vital source of data that link digital systems with the\nphysical world. However, as sensors age, the relationship between what they\nmeasure and what they output changes. This is known as sensor drift and poses a\nsignificant challenge that, combined with limited opportunity for\nre-calibration, can severely limit data quality over time. Previous approaches\nto drift correction typically require large volumes of ground truth data and do\nnot consider measurement or prediction uncertainty. In this paper, we propose a\nprobabilistic sensor drift correction method that takes a fundamental approach\nto modelling the sensor response using Gaussian Process Regression. Tested\nusing dissolved oxygen sensors, our method delivers mean squared error (MSE)\nreductions of up to 90% and more than 20% on average. We also propose a novel\nuncertainty-driven calibration schedule optimisation approach that builds on\ntop of drift correction and further reduces MSE by up to 15.7%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u6982\u7387\u4f20\u611f\u5668\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\uff0c\u5e76\u4f18\u5316\u6821\u51c6\u8ba1\u5212\u3002", "motivation": "\u4f20\u611f\u5668\u6f02\u79fb\u548c\u6821\u51c6\u673a\u4f1a\u6709\u9650\u5bfc\u81f4\u6570\u636e\u8d28\u91cf\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u5927\u91cf\u771f\u5b9e\u6570\u636e\u4e14\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5efa\u6a21\u4f20\u611f\u5668\u54cd\u5e94\uff0c\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u6821\u51c6\u8ba1\u5212\u4f18\u5316\u3002", "result": "\u6eb6\u89e3\u6c27\u4f20\u611f\u5668\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747MSE\u964d\u4f4e20%\u4ee5\u4e0a\uff0c\u6700\u9ad890%\uff1b\u6821\u51c6\u4f18\u5316\u8fdb\u4e00\u6b65\u964d\u4f4eMSE\u8fbe15.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6821\u6b63\u4f20\u611f\u5668\u6f02\u79fb\u5e76\u4f18\u5316\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2506.09636", "pdf": "https://arxiv.org/pdf/2506.09636", "abs": "https://arxiv.org/abs/2506.09636", "authors": ["Joe Hare", "Leo Freitas", "Ken Pierce"], "title": "Translating a VDM Model of a Medical Device into Kapture", "categories": ["cs.SE"], "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "summary": "As the complexity of safety-critical medical devices increases, so does the\nneed for clear, verifiable, software requirements. This paper explores the use\nof Kapture, a formal modelling tool developed by D-RisQ, to translate an\nexisting formal VDM model of a medical implant for treating focal epilepsy\ncalled CANDO. The work was undertaken without prior experience in formal\nmethods. The paper assess Kapture's usability, the challenges of formal\nmodelling, and the effectiveness of the translated model. The result is a model\nin Kapture which covers over 90% of the original VDM model, and produces\nmatching traces of results. While several issues were encountered during design\nand implementation, mainly due to the initial learning curve, this paper\ndemonstrates that complex systems can be effectively modelled in Kapture by\ninexperienced users and highlights some difficulties in translating VDM\nspecifications to Kapture.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528Kapture\u5de5\u5177\u5c06VDM\u6a21\u578b\u8f6c\u6362\u4e3aKapture\u6a21\u578b\u7684\u6548\u679c\uff0c\u8bc4\u4f30\u4e86\u5de5\u5177\u53ef\u7528\u6027\u548c\u6311\u6218\uff0c\u7ed3\u679c\u8868\u660eKapture\u9002\u5408\u521d\u5b66\u8005\u5efa\u6a21\u590d\u6742\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u533b\u7597\u8bbe\u5907\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u9700\u8981\u6e05\u6670\u53ef\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u9700\u6c42\uff0c\u7814\u7a76\u63a2\u7d22\u4e86Kapture\u5de5\u5177\u5728\u5efa\u6a21\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528Kapture\u5de5\u5177\u5c06\u73b0\u6709\u7684VDM\u6a21\u578b\uff08CANDO\u533b\u7597\u690d\u5165\u7269\uff09\u8f6c\u6362\u4e3aKapture\u6a21\u578b\uff0c\u8bc4\u4f30\u5de5\u5177\u53ef\u7528\u6027\u548c\u5efa\u6a21\u6311\u6218\u3002", "result": "Kapture\u6a21\u578b\u8986\u76d6\u4e8690%\u4ee5\u4e0a\u7684\u539f\u59cbVDM\u6a21\u578b\uff0c\u5e76\u751f\u6210\u5339\u914d\u7684\u7ed3\u679c\u8f68\u8ff9\u3002", "conclusion": "Kapture\u9002\u5408\u521d\u5b66\u8005\u5efa\u6a21\u590d\u6742\u7cfb\u7edf\uff0c\u4f46VDM\u5230Kapture\u7684\u8f6c\u6362\u5b58\u5728\u4e00\u5b9a\u56f0\u96be\u3002"}}
{"id": "2506.09085", "pdf": "https://arxiv.org/pdf/2506.09085", "abs": "https://arxiv.org/abs/2506.09085", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u548cML\u7684\u56e2\u961f\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u6b65\u9aa4\u5b9e\u73b0\u7279\u5f81\u8f6c\u6362\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u51cf\u5c11\u9519\u8bef\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u4f20\u7edfML\u7684\u4f4e\u6548\u6027\u548cLLMs\u7684\u4e0d\u7a33\u5b9a\u6027\uff09\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u7279\u5f81\u8f6c\u6362\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u6311\u6218\u3002", "method": "\u7ed3\u5408LLMs\u7684\u7b26\u53f7\u751f\u6210\u548cML\u7684\u68af\u5ea6\u4f18\u5316\uff0c\u5305\u62ec\u56db\u4e2a\u6b65\u9aa4\uff1a\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210\u3001\u7279\u5f81\u8f6c\u6362\u5e8f\u5217\u5d4c\u5165\u4e0e\u641c\u7d22\u3001\u5b66\u751fLLM\u77e5\u8bc6\u84b8\u998f\u3001LLM-ML\u89e3\u7801\u5668\u56e2\u961f\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u56e2\u961f\u7b56\u7565\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u53475%\uff0c\u9519\u8bef\u51cf\u5c11\u8fd1\u534a\uff0c\u4e14\u6846\u67b6\u9ad8\u6548\u7a33\u5065\u3002", "conclusion": "\u56e2\u961f\u6846\u67b6\u6709\u6548\u7ed3\u5408LLMs\u548cML\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u8f6c\u6362\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86LLMs\u5bf9\u539f\u59cb\u6570\u636e\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.09280", "pdf": "https://arxiv.org/pdf/2506.09280", "abs": "https://arxiv.org/abs/2506.09280", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "categories": ["cs.DC", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "AI": {"tldr": "TTrace\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u9759\u9ed8\u9519\u8bef\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5355\u8bbe\u5907\u53c2\u8003\u5b9e\u73b0\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e2d\u95f4\u5f20\u91cf\uff0c\u6709\u6548\u8bc6\u522b\u9519\u8bef\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u9759\u9ed8\u9519\u8bef\u96be\u4ee5\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u4f20\u7edf\u8c03\u8bd5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u3002", "method": "TTrace\u6536\u96c6\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e2d\u95f4\u5f20\u91cf\uff0c\u4e0e\u5355\u8bbe\u5907\u53c2\u8003\u5b9e\u73b0\u5bf9\u6bd4\uff0c\u5e76\u63d0\u51fa\u6570\u5b66\u5206\u6790\u4ee5\u533a\u5206\u9519\u8bef\u548c\u6d6e\u70b9\u820d\u5165\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTTrace\u5728Megatron-LM\u6846\u67b6\u4e2d\u68c0\u6d4b\u523011\u4e2a\u73b0\u6709\u9519\u8bef\u548c3\u4e2a\u65b0\u9519\u8bef\uff0c\u4e14\u4ee3\u7801\u6539\u52a8\u5c11\u3002", "conclusion": "TTrace\u5728\u591a\u79cd\u8bad\u7ec3\u573a\u666f\uff08\u5305\u62ec\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\uff09\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u9759\u9ed8\u9519\u8bef\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.09268", "pdf": "https://arxiv.org/pdf/2506.09268", "abs": "https://arxiv.org/abs/2506.09268", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.NI", "cs.AI"], "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u96c6\u6210\u5730\u9762\u4e0e\u975e\u5730\u9762\u7f51\u7edc\uff08TN-NTN\uff09\uff0c\u4ee5\u5e73\u8861\u7f51\u7edc\u5bb9\u91cf\u4e0e\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5730\u9762\u7f51\u7edc\u90e8\u7f72\u5bc6\u5ea6\u7684\u589e\u52a0\uff0c\u63a2\u7d22\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u5728\u51cf\u8f7b\u5730\u9762\u7f51\u7edc\u8d1f\u8f7d\u548c\u5b9e\u73b0\u80fd\u6e90\u9ad8\u6548\u8fd0\u884c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u548cBandit-feedback Constrained Online Mirror Descent\uff08BCOMD\uff09\u7b97\u6cd5\uff0c\u5b9e\u65f6\u4f18\u5316\u5e26\u5bbd\u5206\u914d\u3001\u7528\u6237\u8bbe\u5907\u5173\u8054\u548c\u5b8f\u57fa\u7ad9\u5173\u95ed\u7b49\u53c2\u6570\u3002", "result": "24\u5c0f\u65f6\u7cfb\u7edf\u7ea7\u4eff\u771f\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u5cf0\u65f6\u6bb5\u672a\u6ee1\u8db3\u9700\u6c42\u7684\u7528\u6237\u8bbe\u5907\u6bd4\u4f8b\uff0c\u5e76\u5728\u4f4e\u6d41\u91cf\u65f6\u6bb5\u5b9e\u73b0\u4e8619%\u7684\u541e\u5410\u91cf\u589e\u76ca\u548c5%\u7684\u80fd\u6e90\u8282\u7701\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u96c6\u6210TN-NTN\u67b6\u6784\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u9075\u5faa3GPP\u5efa\u8bae\u7684\u6807\u51c6\u7f51\u7edc\u8bbe\u7f6e\u3002"}}
{"id": "2506.09530", "pdf": "https://arxiv.org/pdf/2506.09530", "abs": "https://arxiv.org/abs/2506.09530", "authors": ["Fakhri Momeni", "Janete Saldanha Bach", "Brigitte Mathiak", "Peter Mutschke"], "title": "Linking Data Citation to Repository Visibility: An Empirical Study", "categories": ["cs.DL", "cs.DB"], "comment": null, "summary": "In today's data-driven research landscape, dataset visibility and\naccessibility play a crucial role in advancing scientific knowledge. At the\nsame time, data citation is essential for maintaining academic integrity,\nacknowledging contributions, validating research outcomes, and fostering\nscientific reproducibility. As a critical link, it connects scholarly\npublications with the datasets that drive scientific progress. This study\ninvestigates whether repository visibility influences data citation rates. We\nhypothesize that repositories with higher visibility, as measured by search\nengine metrics, are associated with increased dataset citations. Using OpenAlex\ndata and repository impact indicators (including the visibility index from\nSistrix, the h-index of repositories, and citation metrics such as mean and\nmedian citations), we analyze datasets in Social Sciences and Economics to\nexplore their relationship. Our findings suggest that datasets hosted on more\nvisible web domains tend to receive more citations, with a positive correlation\nobserved between web domain visibility and dataset citation counts,\nparticularly for datasets with at least one citation. However, when analyzing\ndomain-level citation metrics, such as the h-index, mean, and median citations,\nthe correlations are inconsistent and weaker. While higher visibility domains\ntend to host datasets with greater citation impact, the distribution of\ncitations across datasets varies significantly. These results suggest that\nwhile visibility plays a role in increasing citation counts, it is not the sole\nfactor influencing dataset citation impact. Other elements, such as dataset\nquality, research trends, and disciplinary norms, also contribute significantly\nto citation patterns.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\u5b58\u50a8\u5e93\u7684\u53ef\u89c1\u6027\u662f\u5426\u5f71\u54cd\u6570\u636e\u5f15\u7528\u7387\uff0c\u53d1\u73b0\u9ad8\u53ef\u89c1\u6027\u5b58\u50a8\u5e93\u4e0e\u66f4\u591a\u6570\u636e\u5f15\u7528\u76f8\u5173\uff0c\u4f46\u5176\u4ed6\u56e0\u7d20\u5982\u6570\u636e\u96c6\u8d28\u91cf\u4e5f\u8d77\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u6570\u636e\u5f15\u7528\u5bf9\u5b66\u672f\u8bda\u4fe1\u548c\u79d1\u5b66\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u96c6\u53ef\u89c1\u6027\u5bf9\u5f15\u7528\u7387\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528OpenAlex\u6570\u636e\u548c\u5b58\u50a8\u5e93\u5f71\u54cd\u6307\u6807\uff08\u5982Sistrix\u53ef\u89c1\u6027\u6307\u6570\u3001h\u6307\u6570\u548c\u5f15\u7528\u6307\u6807\uff09\u5206\u6790\u793e\u4f1a\u79d1\u5b66\u548c\u7ecf\u6d4e\u5b66\u6570\u636e\u96c6\u3002", "result": "\u9ad8\u53ef\u89c1\u6027\u5b58\u50a8\u5e93\u7684\u6570\u636e\u96c6\u5f15\u7528\u66f4\u591a\uff0c\u4f46\u5f15\u7528\u6307\u6807\u7684\u5173\u8054\u6027\u4e0d\u4e00\u81f4\uff0c\u5176\u4ed6\u56e0\u7d20\u5982\u6570\u636e\u96c6\u8d28\u91cf\u4e5f\u5f71\u54cd\u5f15\u7528\u3002", "conclusion": "\u53ef\u89c1\u6027\u867d\u6709\u52a9\u4e8e\u589e\u52a0\u5f15\u7528\uff0c\u4f46\u975e\u552f\u4e00\u56e0\u7d20\uff0c\u6570\u636e\u96c6\u8d28\u91cf\u548c\u5b66\u79d1\u89c4\u8303\u540c\u6837\u91cd\u8981\u3002"}}
{"id": "2506.09683", "pdf": "https://arxiv.org/pdf/2506.09683", "abs": "https://arxiv.org/abs/2506.09683", "authors": ["Priyavanshi Pathania", "Nikhil Bamby", "Rohit Mehra", "Samarth Sikand", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead", "categories": ["cs.SE", "cs.CY"], "comment": "8 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "summary": "The proliferation of software and AI comes with a hidden risk: its growing\nenergy and carbon footprint. As concerns regarding environmental sustainability\ncome to the forefront, understanding and optimizing how software impacts the\nenvironment becomes paramount. In this paper, we present a state-of-the-art\nreview of methods and tools that enable the measurement of software and\nAI-related energy and/or carbon emissions. We introduce a taxonomy to\ncategorize the existing work as Monitoring, Estimation, or Black-Box\napproaches. We delve deeper into the tools and compare them across different\ndimensions and granularity - for example, whether their measurement encompasses\nenergy and carbon emissions and the components considered (like CPU, GPU, RAM,\netc.). We present our observations on the practical use (component wise\nconsolidation of approaches) as well as the challenges that we have identified\nacross the current state-of-the-art. As we start an initiative to address these\nchallenges, we emphasize active collaboration across the community in this\nimportant field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6d4b\u91cf\u8f6f\u4ef6\u548cAI\u76f8\u5173\u80fd\u6e90\u53ca\u78b3\u6392\u653e\u7684\u65b9\u6cd5\u4e0e\u5de5\u5177\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\u5e76\u6bd4\u8f83\u73b0\u6709\u5de5\u5177\uff0c\u5f3a\u8c03\u793e\u533a\u5408\u4f5c\u4ee5\u5e94\u5bf9\u6311\u6218\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u548cAI\u7684\u666e\u53ca\uff0c\u5176\u80fd\u6e90\u548c\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e9f\u9700\u4f18\u5316\u5176\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\uff08\u76d1\u6d4b\u3001\u4f30\u7b97\u3001\u9ed1\u7bb1\u65b9\u6cd5\uff09\uff0c\u6bd4\u8f83\u5de5\u5177\u7684\u591a\u7ef4\u5ea6\u548c\u7ec4\u4ef6\u8986\u76d6\u8303\u56f4\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4f18\u7f3a\u70b9\u53ca\u6311\u6218\uff0c\u63d0\u51fa\u7ec4\u4ef6\u6574\u5408\u7684\u5b9e\u8df5\u89c2\u5bdf\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u5408\u4f5c\u89e3\u51b3\u5f53\u524d\u6311\u6218\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2506.09087", "pdf": "https://arxiv.org/pdf/2506.09087", "abs": "https://arxiv.org/abs/2506.09087", "authors": ["Sophie Jaffard", "Giulia Mezzadri", "Patricia Reynaud-Bouret", "Etienne Tanr\u00e9"], "title": "Spiking Neural Models for Decision-Making Tasks with Learning", "categories": ["cs.LG", "math.PR", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In cognition, response times and choices in decision-making tasks are\ncommonly modeled using Drift Diffusion Models (DDMs), which describe the\naccumulation of evidence for a decision as a stochastic process, specifically a\nBrownian motion, with the drift rate reflecting the strength of the evidence.\nIn the same vein, the Poisson counter model describes the accumulation of\nevidence as discrete events whose counts over time are modeled as Poisson\nprocesses, and has a spiking neurons interpretation as these processes are used\nto model neuronal activities. However, these models lack a learning mechanism\nand are limited to tasks where participants have prior knowledge of the\ncategories. To bridge the gap between cognitive and biological models, we\npropose a biologically plausible Spiking Neural Network (SNN) model for\ndecision-making that incorporates a learning mechanism and whose neurons\nactivities are modeled by a multivariate Hawkes process. First, we show a\ncoupling result between the DDM and the Poisson counter model, establishing\nthat these two models provide similar categorizations and reaction times and\nthat the DDM can be approximated by spiking Poisson neurons. To go further, we\nshow that a particular DDM with correlated noise can be derived from a Hawkes\nnetwork of spiking neurons governed by a local learning rule. In addition, we\ndesigned an online categorization task to evaluate the model predictions. This\nwork provides a significant step toward integrating biologically relevant\nneural mechanisms into cognitive models, fostering a deeper understanding of\nthe relationship between neural activity and behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u53ef\u89e3\u91ca\u6027\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u51b3\u7b56\u4efb\u52a1\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u6a21\u578b\uff08\u5982DDM\u548c\u6cca\u677e\u8ba1\u6570\u5668\u6a21\u578b\uff09\u7f3a\u4e4f\u5b66\u4e60\u673a\u5236\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u6a21\u578b\uff08\u5982DDM\u548c\u6cca\u677e\u8ba1\u6570\u5668\u6a21\u578b\uff09\u7f3a\u4e4f\u5b66\u4e60\u673a\u5236\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u53c2\u4e0e\u8005\u5df2\u5177\u5907\u7c7b\u522b\u5148\u9a8c\u77e5\u8bc6\u7684\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u7269\u53ef\u89e3\u91ca\u7684SNN\u6a21\u578b\uff0c\u586b\u8865\u8ba4\u77e5\u6a21\u578b\u4e0e\u751f\u7269\u6a21\u578b\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u53d8\u91cf\u970d\u514b\u65af\u8fc7\u7a0b\u7684SNN\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86DDM\u4e0e\u6cca\u677e\u8ba1\u6570\u5668\u6a21\u578b\u7684\u8026\u5408\u5173\u7cfb\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u5177\u6709\u76f8\u5173\u566a\u58f0\u7684\u7279\u5b9aDDM\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u5206\u7c7b\u4efb\u52a1\u9a8c\u8bc1\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u8bc1\u660e\u4e86DDM\u53ef\u4ee5\u88ab\u8109\u51b2\u6cca\u677e\u795e\u7ecf\u5143\u8fd1\u4f3c\uff0c\u4e14\u970d\u514b\u65af\u7f51\u7edc\u4e2d\u7684\u8109\u51b2\u795e\u7ecf\u5143\u53ef\u4ee5\u901a\u8fc7\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\u751f\u6210\u7279\u5b9aDDM\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06\u751f\u7269\u76f8\u5173\u7684\u795e\u7ecf\u673a\u5236\u6574\u5408\u5230\u8ba4\u77e5\u6a21\u578b\u4e2d\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u6df1\u5316\u4e86\u5bf9\u795e\u7ecf\u6d3b\u52a8\u4e0e\u884c\u4e3a\u5173\u7cfb\u7684\u7406\u89e3\u3002"}}
{"id": "2506.09282", "pdf": "https://arxiv.org/pdf/2506.09282", "abs": "https://arxiv.org/abs/2506.09282", "authors": ["Dhruv Parikh", "Viktor Prasanna"], "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs", "categories": ["cs.DC", "cs.LG"], "comment": "IC3", "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.", "AI": {"tldr": "ScalableHD\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6838CPU\u7684\u9ad8\u901a\u91cf\u8d85\u7ef4\u8ba1\u7b97\u63a8\u7406\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\u6267\u884c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfHDC\u65b9\u6cd5\u5728CPU\u4e0a\u7684\u63a8\u7406\u6548\u7387\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0cScalableHD\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\u5e76\u884c\u5316\u5904\u7406\uff0c\u7ed3\u5408\u5185\u5b58\u5206\u5757\u548cNUMA\u611f\u77e5\u7ed1\u5b9a\uff0c\u9488\u5bf9\u4e0d\u540c\u6279\u91cf\u5927\u5c0f\u4f18\u5316\u6267\u884c\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b010\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4e14\u6269\u5c55\u6027\u826f\u597d\u3002", "conclusion": "ScalableHD\u4e3a\u591a\u6838CPU\u4e0a\u7684\u9ad8\u6548HDC\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.09647", "pdf": "https://arxiv.org/pdf/2506.09647", "abs": "https://arxiv.org/abs/2506.09647", "authors": ["Lei Deng", "Wenhan Xu", "Jingwei Li", "Danny H. K. Tsang"], "title": "Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Real-time network traffic forecasting is crucial for network management and\nearly resource allocation. Existing network traffic forecasting approaches\noperate under the assumption that the network traffic data is fully observed.\nHowever, in practical scenarios, the collected data are often incomplete due to\nvarious human and natural factors. In this paper, we propose a generative model\napproach for real-time network traffic forecasting with missing data. Firstly,\nwe model the network traffic forecasting task as a tensor completion problem.\nSecondly, we incorporate a pre-trained generative model to achieve the low-rank\nstructure commonly associated with tensor completion. The generative model\neffectively captures the intrinsic low-rank structure of network traffic data\nduring pre-training and enables the mapping from a compact latent\nrepresentation to the tensor space. Thirdly, rather than directly optimizing\nthe high-dimensional tensor, we optimize its latent representation, which\nsimplifies the optimization process and enables real-time forecasting. We also\nestablish a theoretical recovery guarantee that quantifies the error bound of\nthe proposed approach. Experiments on real-world datasets demonstrate that our\napproach achieves accurate network traffic forecasting within 100 ms, with a\nmean absolute error (MAE) below 0.002, as validated on the Abilene dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u751f\u6210\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\uff0c\u5904\u7406\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u5f20\u91cf\u8865\u5168\u548c\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u4f4e\u79e9\u7ed3\u6784\uff0c\u4f18\u5316\u6f5c\u5728\u8868\u793a\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u9884\u6d4b\u3002", "motivation": "\u5b9e\u9645\u573a\u666f\u4e2d\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u5e38\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5b8c\u6574\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u5c06\u6d41\u91cf\u9884\u6d4b\u5efa\u6a21\u4e3a\u5f20\u91cf\u8865\u5168\u95ee\u9898\uff0c\u5f15\u5165\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u6355\u6349\u4f4e\u79e9\u7ed3\u6784\uff0c\u4f18\u5316\u6f5c\u5728\u8868\u793a\u800c\u975e\u9ad8\u7ef4\u5f20\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u9884\u6d4b\u8bef\u5deeMAE\u4f4e\u4e8e0.002\uff0c\u54cd\u5e94\u65f6\u95f4100\u6beb\u79d2\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u9884\u6d4b\u3002"}}
{"id": "2506.09938", "pdf": "https://arxiv.org/pdf/2506.09938", "abs": "https://arxiv.org/abs/2506.09938", "authors": ["Aaditaa Vashisht", "Rekha B S"], "title": "Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies", "categories": ["cs.SE", "cs.DB"], "comment": null, "summary": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u73b0\u4ee3\u4e8b\u4ef6\u9a71\u52a8\u548c\u5fae\u670d\u52a1\u67b6\u6784\uff08\u5982Apache Kafka\u3001Spring Boot\u3001MongoDB\u548cKubernetes\uff09\u5728\u96f6\u552e\u548c\u91d1\u878d\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5206\u6790\u3001\u6b3a\u8bc8\u68c0\u6d4b\u548c\u9ad8\u53ef\u7528\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5316\u8f6c\u578b\u7684\u52a0\u901f\uff0c\u96f6\u552e\u884c\u4e1a\u4e9f\u9700\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u4e14\u5f39\u6027\u7684\u7cfb\u7edf\u6765\u7ba1\u7406\u91d1\u878d\u4ea4\u6613\u3001\u5206\u6790\u5ba2\u6237\u884c\u4e3a\u548c\u4f18\u5316\u8ba2\u5355\u5904\u7406\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u8fd1\u5e74\u6765\u7684\u5b66\u672f\u51fa\u7248\u7269\u3001\u6280\u672f\u767d\u76ae\u4e66\u548c\u884c\u4e1a\u62a5\u544a\uff0c\u672c\u7814\u7a76\u7efc\u5408\u4e86\u5173\u952e\u4e3b\u9898\u548c\u5b9e\u65bd\u7b56\u7565\u3002", "result": "\u5206\u6790\u8868\u660e\uff0cKafka\u548cSpring Boot\u652f\u6301\u4f4e\u5ef6\u8fdf\u7684\u4e8b\u4ef6\u9a71\u52a8\u5e94\u7528\uff0cMongoDB\u7ed3\u5408Kubernetes\u786e\u4fdd\u5e93\u5b58\u548c\u4ea4\u6613\u7cfb\u7edf\u7684\u5bb9\u9519\u6027\u548c\u9ad8\u53ef\u7528\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u884c\u4e1a\u4ece\u4e1a\u8005\u8bbe\u8ba1\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u3001\u7814\u7a76\u6df7\u5408\u90e8\u7f72\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u6574\u5408\u73b0\u4ee3\u7cfb\u7edf\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09702", "pdf": "https://arxiv.org/pdf/2506.09702", "abs": "https://arxiv.org/abs/2506.09702", "authors": ["Huu Hung Nguyen", "Duc Manh Tran", "Yiran Cheng", "Thanh Le-Cong", "Hong Jin Kang", "Ratnadira Widyasari", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "title": "Mapping NVD Records to Their VFCs: How Hard is it?", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Mapping National Vulnerability Database (NVD) records to vulnerability-fixing\ncommits (VFCs) is crucial for vulnerability analysis but challenging due to\nsparse explicit links in NVD references.This study explores this mapping's\nfeasibility through an empirical approach. Manual analysis of NVD references\nshowed Git references enable over 86% success, while non-Git references achieve\nunder 14%. Using these findings, we built an automated pipeline extracting\n31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,\nmainly from Git references. To fill gaps, we mined six external security\ndatabases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,\nand GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%\nprecision. Combining these, we mapped 26,710 unique records (11.3% coverage)\nfrom 7,634 projects, with overlap between NVD and external databases, plus\nunique GitHub contributions. Despite success with Git references, 88.7% of\nrecords remain unmapped, highlighting the difficulty without Git links. This\nstudy offers insights for enhancing vulnerability datasets and guiding future\nautomated security research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06NVD\u8bb0\u5f55\u6620\u5c04\u5230\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\uff08VFCs\uff09\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0Git\u5f15\u7528\u6210\u529f\u7387\u8f83\u9ad8\uff0c\u4f46\u4ecd\u6709\u5927\u91cf\u8bb0\u5f55\u65e0\u6cd5\u6620\u5c04\u3002", "motivation": "NVD\u8bb0\u5f55\u4e0e\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\u4e4b\u95f4\u7684\u663e\u5f0f\u94fe\u63a5\u7a00\u758f\uff0c\u5bfc\u81f4\u6f0f\u6d1e\u5206\u6790\u56f0\u96be\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6620\u5c04\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u5206\u6790NVD\u5f15\u7528\uff0c\u53d1\u73b0Git\u5f15\u7528\u6210\u529f\u7387\u8f83\u9ad8\uff0c\u968f\u540e\u6784\u5efa\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5e76\u6316\u6398\u5916\u90e8\u5b89\u5168\u6570\u636e\u5e93\u548cGitHub\u4ed3\u5e93\u4ee5\u586b\u8865\u7a7a\u767d\u3002", "result": "\u6210\u529f\u6620\u5c04\u4e8611.3%\u7684NVD\u8bb0\u5f55\uff0c\u4f4688.7%\u7684\u8bb0\u5f55\u4ecd\u65e0\u6cd5\u6620\u5c04\uff0cGit\u5f15\u7528\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6f0f\u6d1e\u6570\u636e\u96c6\u589e\u5f3a\u548c\u672a\u6765\u81ea\u52a8\u5316\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u4f46\u65e0Git\u94fe\u63a5\u7684\u8bb0\u5f55\u6620\u5c04\u4ecd\u5177\u6311\u6218\u6027\u3002"}}
{"id": "2506.09090", "pdf": "https://arxiv.org/pdf/2506.09090", "abs": "https://arxiv.org/abs/2506.09090", "authors": ["Arthur Oghlukyan", "Nuria Gomez Blas"], "title": "Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a comprehensive analysis of an enhanced asynchronous\nAdaBoost framework for federated learning (FL), focusing on its application\nacross five distinct domains: computer vision on edge devices, blockchain-based\nmodel transparency, on-device mobile personalization, IoT anomaly detection,\nand federated healthcare diagnostics. The proposed algorithm incorporates\nadaptive communication scheduling and delayed weight compensation to reduce\nsynchronization frequency and communication overhead while preserving or\nimproving model accuracy. We examine how these innovations improve\ncommunication efficiency, scalability, convergence, and robustness in each\ndomain. Comparative metrics including training time, communication overhead,\nconvergence iterations, and classification accuracy are evaluated using data\nand estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical\nresults show, for example, training time reductions on the order of 20-35% and\ncommunication overhead reductions of 30-40% compared to baseline AdaBoost, with\nconvergence achieved in significantly fewer boosting rounds. Tables and charts\nsummarize these improvements by domain. Mathematical formulations of the\nadaptive scheduling rule and error-driven synchronization thresholds are\nprovided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency\nand robustness across diverse FL scenarios, suggesting broad applicability of\nthe approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u5f02\u6b65AdaBoost\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u4fe1\u8c03\u5ea6\u548c\u5ef6\u8fdf\u6743\u91cd\u8865\u507f\u51cf\u5c11\u540c\u6b65\u9891\u7387\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u3001\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u533a\u5757\u94fe\u3001\u79fb\u52a8\u8bbe\u5907\u3001\u7269\u8054\u7f51\u548c\u533b\u7597\u8bca\u65ad\u7b49\u4e94\u4e2a\u9886\u57df\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u901a\u4fe1\u8c03\u5ea6\u548c\u5ef6\u8fdf\u6743\u91cd\u8865\u507f\u6280\u672f\uff0c\u51cf\u5c11\u540c\u6b65\u9891\u7387\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u8bc4\u4f30\u8bad\u7ec3\u65f6\u95f4\u3001\u901a\u4fe1\u5f00\u9500\u3001\u6536\u655b\u8fed\u4ee3\u548c\u5206\u7c7b\u51c6\u786e\u6027\u7b49\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebfAdaBoost\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1120-35%\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1130-40%\uff0c\u6536\u655b\u6240\u9700\u7684\u63d0\u5347\u8f6e\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u589e\u5f3a\u7684AdaBoost\u6846\u67b6\u5728\u591a\u79cd\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09397", "pdf": "https://arxiv.org/pdf/2506.09397", "abs": "https://arxiv.org/abs/2506.09397", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "68T07, 68M14", "I.2.6; C.2.4; C.1.4"], "comment": "6 pages, 9 figures, 2 tables", "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSLED\u65b9\u6cd5\uff0c\u5229\u7528\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u4f18\u5316\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u5f02\u6784\u8bbe\u5907\u534f\u540c\u8ba1\u7b97\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u5185\u5b58\u548c\u529f\u8017\u9650\u5236\u5bfc\u81f4\u9ad8\u6548\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95f4\u6743\u8861\u3002", "method": "SLED\u65b9\u6cd5\u5229\u7528\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u8bbe\u5907\u672c\u5730\u751f\u6210\u5019\u9009\u4ee4\u724c\uff0c\u8fb9\u7f18\u670d\u52a1\u5668\u6279\u91cf\u9a8c\u8bc1\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u652f\u6301\u8bbe\u5907\u5f02\u6784\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLED\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3001\u63d0\u9ad8\u80fd\u6548\u5e76\u652f\u6301\u66f4\u591a\u5e76\u53d1\u63a8\u7406\u4f1a\u8bdd\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "SLED\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09703", "pdf": "https://arxiv.org/pdf/2506.09703", "abs": "https://arxiv.org/abs/2506.09703", "authors": ["Huan Lin", "Chenguang Zhu", "Lianghui Ding", "Feng Yang"], "title": "Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks", "categories": ["cs.NI", "68M18", "C.2.1"], "comment": "15 pages. arXiv admin note: text overlap with arXiv:2411.11342", "summary": "Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to\naddress communication network split issues and restore connectivity. However,\nexisting graph learning-based resilient algorithms face over-aggregation and\nnon-convergence problems caused by uneven and sparse topology under massive\ndamage scenarios. To alleviate these problems, we propose a novel Multi-Level\nDamage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery\ntrajectories by mining information from destroyed UAVs. We first introduce a\nMulti-Branch Damage Attention (MBDA) module, which forms a sequence of\nmulti-hop Damage Attentive Graphs (mDAG) with different ranges of receptive\nfields. Each mDAG links only remaining and damaged nodes to ensure a more even\ndegree distribution for mitigating over-aggregation, and utilizes multi-hop\ndilation to establish more links for sparse topology enhancement. To resort to\nthe mDAG, we propose a Dilated Graph Convolution Network (DGCN), which\ngenerates the optimal recovery trajectories with theoretically proven\nconvergence under massive damage cases. Simulation results show that the\nproposed algorithm can guarantee the connectivity restoration under large swarm\nand damage scales, while significantly expediting the recovery time by 75.94%\nand improving the topology uniformity after recovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u635f\u4f24\u611f\u77e5\u56fe\u5b66\u4e60\u7b97\u6cd5\uff08ML-DAGL\uff09\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u7fa4\u7f51\u7edc\u4e2d\u56fe\u5b66\u4e60\u7b97\u6cd5\u7684\u8fc7\u805a\u5408\u548c\u975e\u6536\u655b\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5206\u652f\u635f\u4f24\u6ce8\u610f\u529b\u6a21\u5757\uff08MBDA\uff09\u548c\u6269\u5f20\u56fe\u5377\u79ef\u7f51\u7edc\uff08DGCN\uff09\u4f18\u5316\u6062\u590d\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u56fe\u5b66\u4e60\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u635f\u4f24\u573a\u666f\u4e0b\u56e0\u62d3\u6251\u4e0d\u5747\u5300\u548c\u7a00\u758f\u6027\u5bfc\u81f4\u8fc7\u805a\u5408\u548c\u975e\u6536\u655b\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165MBDA\u6a21\u5757\u751f\u6210\u591a\u8df3\u635f\u4f24\u6ce8\u610f\u529b\u56fe\uff08mDAG\uff09\uff0c\u7ed3\u5408DGCN\u751f\u6210\u6700\u4f18\u6062\u590d\u8f68\u8ff9\uff0c\u786e\u4fdd\u6536\u655b\u6027\u3002", "result": "\u4eff\u771f\u663e\u793a\u7b97\u6cd5\u80fd\u5728\u5927\u89c4\u6a21\u635f\u4f24\u4e0b\u6062\u590d\u7f51\u7edc\u8fde\u63a5\uff0c\u6062\u590d\u65f6\u95f4\u51cf\u5c1175.94%\uff0c\u62d3\u6251\u5747\u5300\u6027\u63d0\u5347\u3002", "conclusion": "ML-DAGL\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u7fa4\u7f51\u7edc\u7684\u8fde\u901a\u6027\u6062\u590d\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.09713", "pdf": "https://arxiv.org/pdf/2506.09713", "abs": "https://arxiv.org/abs/2506.09713", "authors": ["Mugeng Liu", "Siqi Zhong", "Weichen Bi", "Yixuan Zhang", "Zhiyang Chen", "Zhenpeng Chen", "Xuanzhe Liu", "Yun Ma"], "title": "A First Look at Bugs in LLM Inference Engines", "categories": ["cs.SE"], "comment": "Under review", "summary": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5f15\u64ce\u4e2d\u7684\u9519\u8bef\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86929\u4e2a\u771f\u5b9e\u9519\u8bef\uff0c\u63ed\u793a\u4e86\u75c7\u72b6\u3001\u6839\u672c\u539f\u56e0\u53ca\u5171\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1LLM\u63a8\u7406\u5f15\u64ce\u5728\u73b0\u4ee3AI\u57fa\u7840\u8bbe\u65bd\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8d44\u6e90\u9700\u6c42\u548c\u8de8\u5e73\u53f0\u590d\u6742\u6027\uff0c\u5b83\u4eec\u5bb9\u6613\u51fa\u9519\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6316\u63985\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684LLM\u63a8\u7406\u5f15\u64ce\u7684\u5b98\u65b9\u4ed3\u5e93\uff0c\u6784\u5efa\u4e86929\u4e2a\u771f\u5b9e\u9519\u8bef\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u7f16\u7801\u5206\u6790\u75c7\u72b6\u548c\u6839\u672c\u539f\u56e0\u3002", "result": "\u7814\u7a76\u53d1\u73b06\u79cd\u4e3b\u8981\u9519\u8bef\u75c7\u72b6\u548c28\u79cd\u6839\u672c\u539f\u56e0\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u9519\u8bef\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5f15\u64ce\u4f9b\u5e94\u5546\u548c\u5e94\u7528\u5f00\u53d1\u8005\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2506.09091", "pdf": "https://arxiv.org/pdf/2506.09091", "abs": "https://arxiv.org/abs/2506.09091", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "11 pages, 2 figures, AGI-25", "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8026\u5408\u81ea\u7531\u80fd\u7684\u53d8\u5206\u63a8\u65ad\u4f18\u5316\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u53d8\u5206\u63a8\u65ad\u6280\u672f\u4ee5\u5904\u7406\u8026\u5408\u6307\u6570\u65cf\u7684\u66f2\u7387\u51e0\u4f55\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u53d8\u5206\u63a8\u65ad\u6280\u672f\u5728\u5904\u7406\u91cd\u5c3e\u5206\u5e03\uff08\u5982\u5e7f\u4e49Pareto\u548cStudent's t\u5206\u5e03\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6846\u67b6\u3002", "method": "\u5229\u7528\u8026\u5408\u81ea\u7531\u80fd\u548c\u8026\u5408ELBO\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8026\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\uff0c\u901a\u8fc7\u91c7\u6837\u91cd\u5c3e\u6f5c\u5728\u5206\u5e03\u53ca\u5176\u8026\u5408\u6982\u7387\u6765\u6539\u8fdb\u6a21\u578b\u3002", "result": "\u5728CelebA\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0cCVAE\u57285\u4e2a\u8bad\u7ec3\u5468\u671f\u540e\u6bd4VAE\u67093%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8026\u5408\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u91cd\u5c3e\u6570\u636e\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.09463", "pdf": "https://arxiv.org/pdf/2506.09463", "abs": "https://arxiv.org/abs/2506.09463", "authors": ["Soumyajit Chatterjee", "Rahul Utkoor", "Uppu Eshwar", "Sathya Peri", "V. Krishna Nandivada"], "title": "Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP", "categories": ["cs.DC"], "comment": null, "summary": "Efficient task scheduling is paramount in parallel programming on multi-core\narchitectures, where tasks are fundamental computational units. QR\nfactorization is a critical sub-routine in Sequential Least Squares Quadratic\nProgramming (SLSQP) for solving non-linear programming (NLP) problems. QR\nfactorization decomposes a matrix into an orthogonal matrix Q and an upper\ntriangular matrix R, which are essential for solving systems of linear\nequations arising from optimization problems. SLSQP uses an in-place version of\nQR factorization, which requires storing intermediate results for the next\nsteps of the algorithm. Although DAG-based approaches for QR factorization are\nprevalent in the literature, they often lack control over the intermediate\nkernel results, providing only the final output matrices Q and R. This\nlimitation is particularly challenging in SLSQP, where intermediate results of\nQR factorization are crucial for back-substitution logic at each iteration. Our\nwork introduces novel scheduling techniques using a two-queue approach to\nexecute the QR factorization kernel effectively. This approach, implemented in\nhigh-level C++ programming language, facilitates compiler optimizations and\nallows storing intermediate results required by back-substitution logic.\nEmpirical evaluations demonstrate substantial performance gains, including a\n10x improvement over the sequential QR version of the SLSQP algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u8c03\u5ea6\u6280\u672f\uff0c\u91c7\u7528\u53cc\u961f\u5217\u65b9\u6cd5\u9ad8\u6548\u6267\u884cQR\u5206\u89e3\u5185\u6838\uff0c\u89e3\u51b3\u4e86SLSQP\u7b97\u6cd5\u4e2d\u4e2d\u95f4\u7ed3\u679c\u5b58\u50a8\u7684\u6311\u6218\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u5728\u5e76\u884c\u7f16\u7a0b\u4e2d\uff0c\u4efb\u52a1\u8c03\u5ea6\u5bf9\u591a\u6838\u67b6\u6784\u81f3\u5173\u91cd\u8981\u3002SLSQP\u7b97\u6cd5\u4e2d\u7684QR\u5206\u89e3\u9700\u8981\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\uff0c\u800c\u73b0\u6709DAG\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u961f\u5217\u8c03\u5ea6\u6280\u672f\uff0c\u7ed3\u5408\u9ad8\u7ea7C++\u5b9e\u73b0\uff0c\u652f\u6301\u7f16\u8bd1\u5668\u4f18\u5316\u5e76\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6bd4SLSQP\u7b97\u6cd5\u7684\u987a\u5e8fQR\u7248\u672c\u5feb10\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86QR\u5206\u89e3\u4e2d\u95f4\u7ed3\u679c\u5b58\u50a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86SLSQP\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09878", "pdf": "https://arxiv.org/pdf/2506.09878", "abs": "https://arxiv.org/abs/2506.09878", "authors": ["Ryan Barker"], "title": "Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks", "categories": ["cs.NI"], "comment": "12 pages, 4 figures, 8 tables", "summary": "Virtualising the Radio Access Network (RAN) is widely touted as the\ncorner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current\ndiscourse often isolates spectrum policy, cloud engineering and organisational\nreadiness into silos. This paper delivers an integrated analysis that spans\nscience, technology, business strategy and culture. I first review\nspectrum-auction economics and show-via a comparative study of T-Mobile US and\nVerizon-that mid-band contiguity leveraged through software-defined carrier\naggregation outperforms mmWave-centric deployments in both coverage and churn\nmetrics. I then formalise the technical foundations of virtualised and open\nRAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths\nand quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute\nplatforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent\nControllers are examined alongside AI ML pipelines that enable\ndigital-twin-driven optimisation. A security cost model extends recent O-RAN\nmeasurements to show how 256-bit cipher enforcement adds 35-60 us latency\nunless mitigated by inline crypto off-load. Finally, a national automation case\nstudy of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction\nonce cultural change errors are corrected. I conclude with open research\nchallenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust\norchestration, offering actionable recommendations for operators, vendors and\nresearchers.", "AI": {"tldr": "\u672c\u6587\u5bf9\u865a\u62df\u5316\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u8fdb\u884c\u4e86\u7efc\u5408\u5206\u6790\uff0c\u6db5\u76d6\u9891\u8c31\u653f\u7b56\u3001\u4e91\u8ba1\u7b97\u3001\u7ec4\u7ec7\u51c6\u5907\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u63d0\u51fa\u4e865G\u548c6G\u53d1\u5c55\u7684\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u865a\u62df\u5316RAN\u7684\u8ba8\u8bba\u5f80\u5f80\u5c06\u9891\u8c31\u653f\u7b56\u3001\u4e91\u8ba1\u7b97\u548c\u7ec4\u7ec7\u51c6\u5907\u5272\u88c2\u5f00\u6765\uff0c\u7f3a\u4e4f\u6574\u4f53\u89c6\u89d2\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a5G\u548c6G\u7684\u53d1\u5c55\u63d0\u4f9b\u96c6\u6210\u5206\u6790\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\uff08\u5982T-Mobile US\u548cVerizon\uff09\u3001\u6570\u5b66\u5efa\u6a21\uff08\u9891\u8c31\u5bb9\u91cf\u8ba1\u7b97\uff09\u3001\u6280\u672f\u5e73\u53f0\u5206\u6790\uff08\u5982NVIDIA EGX\u3001Samsung vRAN 3.0\uff09\u4ee5\u53ca\u6848\u4f8b\u7814\u7a76\uff08\u5982\u56fd\u5bb6\u81ea\u52a8\u5316\u6848\u4f8b\uff09\uff0c\u5168\u9762\u63a2\u8ba8\u865a\u62df\u5316RAN\u7684\u5404\u4e2a\u65b9\u9762\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e2d\u9891\u6bb5\u8fde\u7eed\u9891\u8c31\u5728\u8986\u76d6\u548c\u7528\u6237\u6d41\u5931\u7387\u4e0a\u4f18\u4e8e\u6beb\u7c73\u6ce2\u90e8\u7f72\uff1b256\u4f4d\u52a0\u5bc6\u4f1a\u589e\u52a0\u5ef6\u8fdf\uff0c\u4f46\u53ef\u901a\u8fc7\u5185\u8054\u52a0\u5bc6\u5378\u8f7d\u7f13\u89e3\uff1b\u6587\u5316\u53d8\u9769\u9519\u8bef\u4fee\u6b63\u540e\uff0c\u81ea\u52a8\u5316\u5468\u671f\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e866G\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\uff08\u5982\u592a\u8d6b\u5179\u9891\u6bb5\u3001\u80fd\u6e90\u4e2d\u6027AI\u52a0\u901f\u5668\uff09\uff0c\u5e76\u4e3a\u8fd0\u8425\u5546\u3001\u4f9b\u5e94\u5546\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2506.09759", "pdf": "https://arxiv.org/pdf/2506.09759", "abs": "https://arxiv.org/abs/2506.09759", "authors": ["Abhijit Paul", "Proma Chowdhury", "Kazi Sakib"], "title": "Towards Bridging Formal Methods and Human Interpretability", "categories": ["cs.SE"], "comment": "Need to improve data annotation process in methodology section", "summary": "Labeled Transition Systems (LTS) are integral to model checking and design\nrepair tools. System engineers frequently examine LTS designs during model\nchecking or design repair to debug, identify inconsistencies, and validate\nsystem behavior. Despite LTS's significance, no prior research has examined\nhuman comprehension of these designs. To address this, we draw on traditional\nsoftware engineering and graph theory, identifying 7 key metrics: cyclomatic\ncomplexity, state space size, average branching factor, maximum depth, Albin\ncomplexity, modularity, and redundancy. We created a dataset of 148 LTS\ndesigns, sampling 48 for 324 paired comparisons, and ranked them using the\nBradley-Terry model. Through Kendall's Tau correlation analysis, we found that\nAlbin complexity ($\\tau = 0.444$), state space size ($\\tau = 0.420$),\ncyclomatic complexity ($\\tau = 0.366$), and redundancy ($\\tau = 0.315$) most\naccurately reflect human comprehension of LTS designs. To showcase the metrics'\nutility, we applied the Albin complexity metric within the Fortis design repair\ntool, ranking system redesigns. This ranking reduced annotators' comprehension\ntime by 39\\%, suggesting that metrics emphasizing human factors can enhance\nformal design interpretability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u5bf9\u6807\u8bb0\u8f6c\u79fb\u7cfb\u7edf\uff08LTS\uff09\u8bbe\u8ba1\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u4e867\u4e2a\u5173\u952e\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e2d4\u4e2a\u6307\u6807\u6700\u80fd\u53cd\u6620\u4eba\u7c7b\u7406\u89e3\u3002", "motivation": "\u5c3d\u7ba1LTS\u5728\u6a21\u578b\u68c0\u67e5\u548c\u8bbe\u8ba1\u4fee\u590d\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u7406\u89e3LTS\u8bbe\u8ba1\u7684\u7814\u7a76\u3002", "method": "\u7ed3\u5408\u8f6f\u4ef6\u5de5\u7a0b\u548c\u56fe\u8bba\uff0c\u63d0\u51fa7\u4e2a\u6307\u6807\uff0c\u521b\u5efa148\u4e2aLTS\u8bbe\u8ba1\u6570\u636e\u96c6\uff0c\u901a\u8fc7Bradley-Terry\u6a21\u578b\u548cKendall's Tau\u5206\u6790\u9a8c\u8bc1\u3002", "result": "Albin\u590d\u6742\u5ea6\u3001\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u3001\u5708\u590d\u6742\u5ea6\u548c\u5197\u4f59\u5ea6\u6700\u80fd\u53cd\u6620\u4eba\u7c7b\u7406\u89e3\u3002\u5e94\u7528Albin\u590d\u6742\u5ea6\u540e\uff0c\u7406\u89e3\u65f6\u95f4\u51cf\u5c1139%\u3002", "conclusion": "\u5f3a\u8c03\u4eba\u7c7b\u56e0\u7d20\u7684\u6307\u6807\u53ef\u63d0\u5347\u5f62\u5f0f\u5316\u8bbe\u8ba1\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.09092", "pdf": "https://arxiv.org/pdf/2506.09092", "abs": "https://arxiv.org/abs/2506.09092", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSR\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u9ad8\u6027\u80fdCUDA\u7a0b\u5e8f\uff0c\u663e\u8457\u63d0\u5347GPU\u5185\u6838\u7684\u6267\u884c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u901a\u7528\u4ee3\u7801\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u751f\u6210\u786c\u4ef6\u7279\u5b9a\u3001\u67b6\u6784\u611f\u77e5\u4e14\u6027\u80fd\u5173\u952e\u7684GPU\u4ee3\u7801\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86FSR\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u7f16\u8bd1\u3001\u529f\u80fd\u6b63\u786e\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645GPU\u5185\u6838\u6267\u884c\u5ef6\u8fdf\u9a8c\u8bc1\u3002", "result": "FSR\u589e\u5f3a\u7684LLMs\u4e0d\u4ec5\u80fd\u751f\u6210\u6b63\u786eCUDA\u4ee3\u7801\uff0c\u8fd8\u80fd\u8fed\u4ee3\u4f18\u5316\u6548\u7387\uff0c\u751f\u6210\u7684\u4ee3\u7801\u6267\u884c\u901f\u5ea6\u6700\u9ad8\u53ef\u8fbe\u4eba\u5de5\u7f16\u5199\u7684179\u500d\u3002", "conclusion": "\u7ed3\u5408LLMs\u4e0e\u6027\u80fd\u5f3a\u5316\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u81ea\u52a8\u5316GPU\u7f16\u7a0b\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u7279\u5b9a\u3001\u67b6\u6784\u654f\u611f\u548c\u6027\u80fd\u5173\u952e\u7684\u5e94\u7528\u3002"}}
{"id": "2506.09505", "pdf": "https://arxiv.org/pdf/2506.09505", "abs": "https://arxiv.org/abs/2506.09505", "authors": ["Dumitrel Loghin", "Shuang Liang", "Shengwei Liu", "Xiong Liu", "Pingcheng Ruan", "Zhigang Ye"], "title": "On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems", "categories": ["cs.DC", "cs.ET", "cs.PF"], "comment": null, "summary": "Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling\nblockchains and bringing Web3 to life. At the same time, ZKP for transactions\nrunning on the Ethereum Virtual Machine require powerful servers with hundreds\nof CPU cores. The current zkProver implementation from Polygon is optimized for\nx86-64 CPUs by vectorizing key operations, such as Merkle tree building with\nPoseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX\nand AVX512). With these optimizations, a ZKP for a batch of transactions is\ngenerated in less than two minutes. With the advent of cloud servers with ARM\nwhich are at least 10% cheaper than x86-64 servers and the implementation of\nARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over\ntheir x86-64 counterparts. Unfortunately, our analysis shows that current ARM\nCPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web\nServices (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X\nslower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX\nand AVX512, respectively, when building a Merkle tree with over four million\nleaves. This low performance is due to (1) smaller vector size in these ARM\nCPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the\nother hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as\npowerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing\nthe vector size to 512 bits will enable higher performance in ARM CPUs compared\nto their x86-64 counterparts while maintaining their price advantage.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86ARM\u670d\u52a1\u5668\u5728\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKP\uff09\u751f\u6210\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524dARM CPU\uff08\u5982AWS Graviton4\u548cGCP Axion\uff09\u5728\u6784\u5efaMerkle\u6811\u65f6\u6bd4x86-64\u670d\u52a1\u5668\u61621.4-1.6\u500d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5411\u91cf\u5927\u5c0f\u548c\u65f6\u949f\u9891\u7387\u8f83\u4f4e\u3002\u4f46ARM SVE/SVE2 ISA\u5177\u6709\u6f5c\u529b\uff0c\u82e5\u5411\u91cf\u5927\u5c0f\u63d0\u5347\u81f3512\u4f4d\uff0c\u6027\u80fd\u53ef\u80fd\u8d85\u8d8ax86-64\u3002", "motivation": "\u63a2\u7d22ARM\u670d\u52a1\u5668\u662f\u5426\u80fd\u5728ZKP\u751f\u6210\u4e2d\u53d6\u4ee3x86-64\u670d\u52a1\u5668\uff0c\u4ee5\u5229\u7528\u5176\u6210\u672c\u4f18\u52bf\u3002", "method": "\u6bd4\u8f83ARM\uff08AWS Graviton4\u548cGCP Axion\uff09\u4e0ex86-64\uff08AMD EPYC\u548cIntel Xeon\uff09\u670d\u52a1\u5668\u5728\u6784\u5efaMerkle\u6811\u65f6\u7684\u6027\u80fd\uff0c\u5206\u6790\u539f\u56e0\u3002", "result": "\u5f53\u524dARM CPU\u6027\u80fd\u8f83\u4f4e\uff08\u61621.4-1.6\u500d\uff09\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8f83\u5c0f\u7684\u5411\u91cf\u5927\u5c0f\uff08128\u4f4d\uff09\u548c\u8f83\u4f4e\u7684\u65f6\u949f\u9891\u7387\u3002", "conclusion": "ARM SVE/SVE2 ISA\u5177\u6709\u6f5c\u529b\uff0c\u82e5\u5411\u91cf\u5927\u5c0f\u63d0\u5347\u81f3512\u4f4d\uff0c\u53ef\u80fd\u8d85\u8d8ax86-64\u670d\u52a1\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u6210\u672c\u4f18\u52bf\u3002"}}
{"id": "2506.09845", "pdf": "https://arxiv.org/pdf/2506.09845", "abs": "https://arxiv.org/abs/2506.09845", "authors": ["Tobias He\u00df", "Lukas Ostheimer", "Tobias Betz", "Simon Karrer", "Tim Jannik Schmidt", "Pierre Coquet", "Sean Semmler", "Thomas Th\u00fcm"], "title": "variability.dev: Towards an Online Toolbox for Feature Modeling", "categories": ["cs.SE"], "comment": "Presented at 6th International Workshop on Languages for Modelling\n  Variability (MODEVAR'24) (arXiv:cs/2402.15511). 5 pages, 3 figures", "summary": "The emergence of feature models as the default to model the variability in\nconfigurable systems fosters a rich diversity in applications, application\ndomains, and perspectives. Independent of their domain, modelers require to\nopen, view, edit, transform, save, and configure models as well as to\ncollaborate with others. However, at the time of writing, the top five results\nwhen googling ``Online Editor Feature Model'' point to editors that either have\nminimal functionality, are unmaintained or defunct, or require an offline\ninstallation, such as FeatureIDE. In this work we present a preview of our\nin-development online toolbox for feature modeling, variability.dev. In\nparticular, we showcase our collaborative feature-model editor and our online\nconfigurator both of which are built on top of the FeatureIDE library.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6b63\u5728\u5f00\u53d1\u7684\u5728\u7ebf\u5de5\u5177\u7bb1variability.dev\uff0c\u7528\u4e8e\u7279\u5f81\u5efa\u6a21\uff0c\u91cd\u70b9\u5c55\u793a\u4e86\u5176\u534f\u4f5c\u5f0f\u7279\u5f81\u6a21\u578b\u7f16\u8f91\u5668\u548c\u5728\u7ebf\u914d\u7f6e\u5668\u3002", "motivation": "\u5f53\u524d\u5728\u7ebf\u7279\u5f81\u6a21\u578b\u7f16\u8f91\u5668\u529f\u80fd\u6709\u9650\u3001\u7ef4\u62a4\u4e0d\u8db3\u6216\u9700\u8981\u79bb\u7ebf\u5b89\u88c5\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u529f\u80fd\u5b8c\u5584\u7684\u5728\u7ebf\u5de5\u5177\u3002", "method": "\u57fa\u4e8eFeatureIDE\u5e93\u5f00\u53d1\u534f\u4f5c\u5f0f\u7279\u5f81\u6a21\u578b\u7f16\u8f91\u5668\u548c\u5728\u7ebf\u914d\u7f6e\u5668\u3002", "result": "\u5c55\u793a\u4e86variability.dev\u5de5\u5177\u7bb1\u7684\u9884\u89c8\u7248\uff0c\u652f\u6301\u534f\u4f5c\u7f16\u8f91\u548c\u5728\u7ebf\u914d\u7f6e\u3002", "conclusion": "variability.dev\u6709\u671b\u586b\u8865\u5f53\u524d\u5728\u7ebf\u7279\u5f81\u5efa\u6a21\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u534f\u4f5c\u548c\u914d\u7f6e\u529f\u80fd\u3002"}}
{"id": "2506.09093", "pdf": "https://arxiv.org/pdf/2506.09093", "abs": "https://arxiv.org/abs/2506.09093", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLwPTV\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u663e\u8457\u6027\u5206\u6570\u6765\u4fee\u526a\u4efb\u52a1\u5411\u91cf\u4e2d\u7684\u5197\u4f59\u53c2\u6570\uff0c\u4ece\u800c\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u57df\u5916\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57df\u5185\u6570\u636e\u96c6\u6027\u80fd\uff0c\u800c\u5ffd\u7565\u4e86\u57df\u5916\u6570\u636e\u96c6\u7684\u8868\u73b0\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLwPTV\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u7ea7\u4fee\u526a\u4efb\u52a1\u5411\u91cf\u4e2d\u7684\u5197\u4f59\u53c2\u6570\uff0c\u751f\u6210\u63a9\u7801\u5411\u91cf\uff0c\u4ec5\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLwPTV\u663e\u8457\u63d0\u5347\u4e86\u57df\u5916\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57df\u5185\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "LwPTV\u65b9\u6cd5\u7075\u6d3b\u4e14\u6709\u6548\uff0c\u53ef\u4e0e\u5176\u4ed6\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u7ed3\u5408\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09554", "pdf": "https://arxiv.org/pdf/2506.09554", "abs": "https://arxiv.org/abs/2506.09554", "authors": ["Mayank Arya", "Yogesh Simmhan"], "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators", "categories": ["cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide\nrange of domains, for tasks as diverse as code generation and robot navigation.\nWhile LLMs are usually served from cloud data centers, mission-critical and\nprivacy-sensitive applications may require local hosting of open LLM models.\nGiven the large GPU memory footprint needed for LLMs, edge accelerators such as\nNvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.\nHowever, the feasibility and performance of LLM inference on edge accelerators\nis under-explored. This study presents a detailed evaluation of LLM inference\non the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B\nparameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We\ninvestigate the impact of varying batch sizes, sequence lengths, and\nquantization levels on latency, throughput, and perplexity, and also explore\nvarious custom power modes on the Orin AGX to perform power and energy\nconsumption analysis. Our findings offer interesting insights on the trade-offs\nbetween efficiency, inference speed and resource use, e.g., increasing the\nsequence length causes a decrease in token throughput and quantization causes\nsmaller LLMs to be slower. These results can help optimize LLM serving on edge\naccelerators for practical applications.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5728NVIDIA Jetson Orin AGX\u8fb9\u7f18\u52a0\u901f\u5668\u4e0a\u8fd0\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53ef\u884c\u6027\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u4e91\u6570\u636e\u4e2d\u5fc3\u65e0\u6cd5\u6ee1\u8db3\u5173\u952e\u4efb\u52a1\u548c\u9690\u79c1\u654f\u611f\u5e94\u7528\u7684\u9700\u6c42\uff0c\u672c\u5730\u6258\u7ba1\u5f00\u6e90LLM\u6210\u4e3a\u5fc5\u8981\uff0c\u4f46\u8fb9\u7f18\u8bbe\u5907\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728NVIDIA Jetson Orin AGX\u4e0a\u6d4b\u8bd5\u4e86\u56db\u79cd\u6a21\u578b\uff082.7B\u81f332.8B\u53c2\u6570\uff09\uff0c\u5206\u6790\u4e86\u6279\u6b21\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u91cf\u5316\u7ea7\u522b\u53ca\u7535\u6e90\u6a21\u5f0f\u5bf9\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u56f0\u60d1\u5ea6\u548c\u80fd\u8017\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u4f1a\u964d\u4f4e\u541e\u5410\u91cf\uff0c\u91cf\u5316\u4f1a\u4f7f\u5c0f\u6a21\u578b\u53d8\u6162\uff0c\u7535\u6e90\u6a21\u5f0f\u5bf9\u80fd\u8017\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f18\u5316LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e73\u8861\u4e86\u6548\u7387\u3001\u901f\u5ea6\u548c\u8d44\u6e90\u4f7f\u7528\u3002"}}
{"id": "2506.09418", "pdf": "https://arxiv.org/pdf/2506.09418", "abs": "https://arxiv.org/abs/2506.09418", "authors": ["Ryan Barker", "Fatemeh Afghah"], "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G", "categories": ["cs.CR", "cs.NI"], "comment": "4 pages, 1 figure", "summary": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86O-RAN\u57285G\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u5206\u6790\u4e86\u5bc6\u7801\u5b66\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "O-RAN\u7684\u6a21\u5757\u5316\u548c\u7075\u6d3b\u6027\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u4fdd\u62a4\u5176\u5206\u6563\u63a5\u53e3\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e8613\u4e2a\u5b66\u672f\u548c\u884c\u4e1a\u6765\u6e90\uff0c\u8bc4\u4f30\u4e86\u5bc6\u7801\u5b66\u5de5\u5177\u7684\u6027\u80fd\u53ca\u5176\u5728O-RAN\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0O-RAN\u63a5\u53e3\u5b58\u5728\u591a\u79cd\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86AI\u9a71\u52a8\u7684\u52a8\u6001\u7f16\u6392\u548c\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u786c\u4ef6\u5378\u8f7d\u3001\u8de8\u5c42\u5bc6\u7801\u9002\u5e94\u548c\u96f6\u4fe1\u4efb\u67b6\u6784\uff0c\u4ee5\u652f\u63016G\u53d1\u5c55\u3002"}}
{"id": "2506.09873", "pdf": "https://arxiv.org/pdf/2506.09873", "abs": "https://arxiv.org/abs/2506.09873", "authors": ["Emma Kallina", "Thomas Bohn\u00e9", "Jat Singh"], "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\uff08SHI\uff09\u5b9e\u8df5\u4e3b\u8981\u53d7\u5546\u4e1a\u76ee\u6807\u9a71\u52a8\uff0c\u4e0e\u8d1f\u8d23\u4efbAI\uff08rAI\uff09\u7684\u76ee\u6807\u5b58\u5728\u8131\u8282\uff0c\u9700\u901a\u8fc7\u5e72\u9884\u63aa\u65bd\u548c\u7814\u7a76\u673a\u4f1a\u52a0\u4ee5\u6539\u8fdb\u3002", "motivation": "\u63a2\u8ba8SHI\u5728\u5546\u4e1aAI\u5f00\u53d1\u4e2d\u7684\u5b9e\u8df5\u4e0erAI\u76ee\u6807\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u5e72\u9884\u63aa\u65bd\u3002", "method": "\u5206\u679056\u4efdrAI\u6307\u5357\uff0c\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\uff08n=130\uff09\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff08n=10\uff09\u3002", "result": "SHI\u5b9e\u8df5\u4e3b\u8981\u670d\u52a1\u4e8e\u5546\u4e1a\u76ee\u6807\uff08\u5982\u5ba2\u6237\u4ef7\u503c\u3001\u5408\u89c4\uff09\uff0c\u4e0erAI\u76ee\u6807\uff08\u5982\u6743\u529b\u518d\u5206\u914d\u3001\u98ce\u9669\u9884\u6d4b\uff09\u8131\u8282\u3002", "conclusion": "\u9700\u63d0\u51fa\u5e72\u9884\u63aa\u65bd\u548c\u7814\u7a76\u673a\u4f1a\uff0c\u4ee5\u63a8\u52a8SHI\u5b9e\u8df5\u4e0erAI\u76ee\u6807\u7684\u5bf9\u9f50\u3002"}}
{"id": "2506.09096", "pdf": "https://arxiv.org/pdf/2506.09096", "abs": "https://arxiv.org/abs/2506.09096", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "title": "Intra-Trajectory Consistency for Reward Modeling", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u6982\u7387\u589e\u5f3a\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u54cd\u5e94\u8f68\u8ff9\u4e2d\u7684\u8fc7\u7a0b\u4e00\u81f4\u6027\u6539\u8fdb\u5956\u52b1\u5b66\u4e60\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u54cd\u5e94\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u54cd\u5e94\u7ea7\u8bc4\u5206\uff0c\u96be\u4ee5\u8bc6\u522b\u4e0e\u8bc4\u5206\u771f\u6b63\u76f8\u5173\u7684\u54cd\u5e94\u8f68\u8ff9\u4e2d\u7684\u5177\u4f53\u90e8\u5206\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u5229\u7528\u751f\u6210\u6982\u7387\u5efa\u7acb\u54cd\u5e94\u8f68\u8ff9\u4e2d\u8fc7\u7a0b\u95f4\u7684\u5956\u52b1\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u8f68\u8ff9\u5185\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u5f3a\u5236\u76f8\u90bb\u8fc7\u7a0b\u5728\u66f4\u9ad8\u751f\u6210\u6982\u7387\u4e0b\u4fdd\u6301\u5956\u52b1\u4e00\u81f4\u6027\u3002", "result": "\u6539\u8fdb\u7684\u5956\u52b1\u6a21\u578b\u5728RewardBench\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u5347\u4e86DPO\u5bf9\u9f50\u7b56\u7565\u548c\u6700\u4f73N\u63a8\u7406\u9a8c\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8fc7\u7a0b\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u5956\u52b1\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u5b66\u4e60\u7ec6\u7c92\u5ea6\u4fe1\u53f7\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2506.09823", "pdf": "https://arxiv.org/pdf/2506.09823", "abs": "https://arxiv.org/abs/2506.09823", "authors": ["Stephen Buttolph", "Andrew Lewis-Pye", "Kevin Sekniqi"], "title": "Frosty for partial synchrony", "categories": ["cs.DC"], "comment": "arXiv admin note: substantial text overlap with arXiv:2404.14250,\n  arXiv:2501.15904", "summary": "Snowman is the consensus protocol used by blockchains on Avalanche. Recent\nwork has shown both how to augment Snowman with a `liveness' module called\n`Frosty' that protects against liveness attacks, and also how to modify Snowman\nso as to be consistent in partial synchrony. Since Frosty assumes (a strong\nform of) synchrony, the aim of this note is to show how to modify Frosty to\ndeal with the partially synchronous version of Snowman.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u5c06Frosty\u6a21\u5757\u4fee\u6539\u4e3a\u9002\u7528\u4e8e\u90e8\u5206\u540c\u6b65\u7248\u672c\u7684Snowman\u5171\u8bc6\u534f\u8bae\u3002", "motivation": "Frosty\u6a21\u5757\u5047\u8bbe\u5f3a\u540c\u6b65\u6027\uff0c\u800cSnowman\u5df2\u4fee\u6539\u4e3a\u90e8\u5206\u540c\u6b65\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u8c03\u6574Frosty\u4ee5\u9002\u5e94\u90e8\u5206\u540c\u6b65\u73af\u5883\u3002", "method": "\u4fee\u6539Frosty\u6a21\u5757\uff0c\u4f7f\u5176\u4e0e\u90e8\u5206\u540c\u6b65\u7248\u672c\u7684Snowman\u517c\u5bb9\u3002", "result": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u90e8\u5206\u540c\u6b65Snowman\u7684Frosty\u6539\u8fdb\u7248\u672c\u3002", "conclusion": "\u6210\u529f\u5c55\u793a\u4e86\u5982\u4f55\u8c03\u6574Frosty\u4ee5\u652f\u6301\u90e8\u5206\u540c\u6b65Snowman\uff0c\u589e\u5f3a\u4e86\u534f\u8bae\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2506.09929", "pdf": "https://arxiv.org/pdf/2506.09929", "abs": "https://arxiv.org/abs/2506.09929", "authors": ["Scott Schnelle", "Francesca Favaro", "Laura Fraade-Blanar", "David Wichner", "Holland Broce", "Justin Miranda"], "title": "Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation", "categories": ["cs.SE", "cs.CY"], "comment": null, "summary": "As Automated Driving Systems (ADS) technology advances, ensuring safety and\npublic trust requires robust assurance frameworks, with safety cases emerging\nas a critical tool toward such a goal. This paper explores an approach to\nassess how a safety case is supported by its claims and evidence, toward\nestablishing credibility for the overall case. Starting from a description of\nthe building blocks of a safety case (claims, evidence, and optional\nformat-dependent entries), this paper delves into the assessment of support of\neach claim through the provided evidence. Two domains of assessment are\noutlined for each claim: procedural support (formalizing process specification)\nand implementation support (demonstrating process application). Additionally,\nan assessment of evidence status is also undertaken, independently from the\nclaims support. Scoring strategies and evaluation guidelines are provided,\nincluding detailed scoring tables for claim support and evidence status\nassessment. The paper further discusses governance, continual improvement, and\ntiming considerations for safety case assessments. Reporting of results and\nfindings is contextualized within its primary use for internal decision-making\non continual improvement efforts. The presented approach builds on state of the\nart auditing practices, but specifically tackles the question of judging the\ncredibility of a safety case. While not conclusive on its own, it provides a\nstarting point toward a comprehensive \"Case Credibility Assessment\" (CCA),\nstarting from the evaluation of the support for each claim (individually and in\naggregate), as well as every piece of evidence provided. By delving into the\ntechnical intricacies of ADS safety cases, this work contributes to the ongoing\ndiscourse on safety assurance and aims to facilitate the responsible\nintegration of ADS technology into society.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u5b89\u5168\u6848\u4f8b\u53ef\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6848\u4f8b\u4e2d\u6bcf\u4e2a\u4e3b\u5f20\u7684\u652f\u6301\u7a0b\u5ea6\u548c\u8bc1\u636e\u72b6\u6001\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u5b89\u5168\u548c\u516c\u4f17\u4fe1\u4efb\u9700\u8981\u5f3a\u6709\u529b\u7684\u4fdd\u969c\u6846\u67b6\uff0c\u5b89\u5168\u6848\u4f8b\u6210\u4e3a\u5173\u952e\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u5b89\u5168\u6848\u4f8b\u7684\u53ef\u4fe1\u5ea6\uff0c\u4ee5\u652f\u6301\u5176\u6574\u4f53\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b89\u5168\u6848\u4f8b\u7684\u6784\u5efa\u6a21\u5757\uff08\u4e3b\u5f20\u3001\u8bc1\u636e\u7b49\uff09\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u4e3b\u5f20\u7684\u7a0b\u5e8f\u652f\u6301\u548c\u5b9e\u65bd\u652f\u6301\uff0c\u5e76\u5bf9\u8bc1\u636e\u72b6\u6001\u8fdb\u884c\u72ec\u7acb\u8bc4\u4f30\u3002\u63d0\u4f9b\u4e86\u8bc4\u5206\u7b56\u7565\u548c\u8bc4\u4f30\u6307\u5357\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u8bc4\u5206\u8868\u548c\u8bc4\u4f30\u6307\u5357\uff0c\u7528\u4e8e\u5224\u65ad\u5b89\u5168\u6848\u4f8b\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u8ba8\u8bba\u4e86\u6cbb\u7406\u3001\u6301\u7eed\u6539\u8fdb\u548c\u65f6\u95f4\u5b89\u6392\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u6848\u4f8b\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8d77\u70b9\uff0c\u6709\u52a9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u5b89\u5168\u6574\u5408\u548c\u793e\u4f1a\u63a5\u53d7\u3002"}}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u6a21\u578b\u5bb9\u91cf\u4e0e\u5b66\u4e60\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u7406\u89e3\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u5728LLMs\u4e2d\u7684\u5173\u7cfb\uff0c\u63a2\u7d22\u6a21\u578b\u5bb9\u91cf\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u5bb9\u91cf\u53d7\u9650\u7684Transformer\u6a21\u578b\uff0c\u5728\u5408\u6210\u5b57\u7b26\u7ea7\u4efb\u52a1\u4e2d\u5206\u522b\u6d4b\u8bd5\u6cdb\u5316\uff08\u7b97\u672f\u5916\u63a8\uff09\u548c\u8bb0\u5fc6\uff08\u4e8b\u5b9e\u56de\u5fc6\uff09\u3002", "result": "\u5c0f\u6a21\u578b\u80fd\u6cdb\u5316\u4f46\u65e0\u6cd5\u8bb0\u5fc6\uff0c\u5927\u6a21\u578b\u80fd\u8bb0\u5fc6\u4f46\u65e0\u6cd5\u6cdb\u5316\uff1b\u8054\u5408\u8bad\u7ec3\u65f6\u6240\u6709\u6a21\u578b\u5747\u65e0\u6cd5\u6cdb\u5316\u3002", "conclusion": "\u9884\u8bad\u7ec3\u53ef\u80fd\u56fa\u6709\u5730\u504f\u5411\u67d0\u79cd\u5b66\u4e60\u6a21\u5f0f\uff0c\u7814\u7a76\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2506.09199", "pdf": "https://arxiv.org/pdf/2506.09199", "abs": "https://arxiv.org/abs/2506.09199", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "21 pages, 12 figures", "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "AI": {"tldr": "FLoRIST\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u5206\u89e3\u672c\u5730\u9002\u914d\u5668\uff0c\u5e73\u8861\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u5ba2\u6237\u7aef\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2dLoRA\u65b9\u6cd5\u5728\u901a\u4fe1\u6548\u7387\u3001\u6a21\u578b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "FLoRIST\u91c7\u7528\u5947\u5f02\u503c\u5206\u89e3\u672c\u5730\u9002\u914d\u5668\uff0c\u901a\u8fc7\u4e2d\u95f4\u7a7a\u95f4\u8868\u793a\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u53ef\u8c03\u9608\u503c\u9009\u62e9\u5168\u5c40\u4f4e\u79e9\u9002\u914d\u5668\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cLLM\u4e0a\u9a8c\u8bc1\uff0cFLoRIST\u5728\u901a\u4fe1\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "FLoRIST\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09101", "pdf": "https://arxiv.org/pdf/2506.09101", "abs": "https://arxiv.org/abs/2506.09101", "authors": ["M\u00edriam Barrab\u00e9s", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "title": "Feature Shift Localization Network", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 2 figures, 4 tables", "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "AI": {"tldr": "FSL-Net\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e2d\u5feb\u901f\u51c6\u786e\u5730\u5b9a\u4f4d\u7279\u5f81\u504f\u79fb\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7279\u5f81\u504f\u79fb\u65f6\u7684\u4e0d\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u6570\u636e\u6e90\u548c\u566a\u58f0\u6570\u636e\u4e2d\u3002", "method": "\u63d0\u51faFSL-Net\uff0c\u901a\u8fc7\u8bad\u7ec3\u5b66\u4e60\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u548c\u504f\u79fb\u3002", "result": "FSL-Net\u80fd\u591f\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5b9a\u4f4d\u7279\u5f81\u504f\u79fb\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u3002", "conclusion": "FSL-Net\u4e3a\u7279\u5f81\u504f\u79fb\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "ComfyUI-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210AI\u5185\u5bb9\u5de5\u4f5c\u6d41\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08CoT\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4f5c\u6d41\u751f\u6210\u7684\u683c\u5f0f\u6709\u6548\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "motivation": "AI\u751f\u6210\u5185\u5bb9\u7684\u5de5\u4f5c\u6d41\u5b9a\u5236\u9700\u8981\u9ad8\u5ea6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0cComfyUI-R1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u75284K\u5de5\u4f5c\u6d41\u6570\u636e\u96c6\u6784\u5efa\u957f\u94fe\u63a8\u7406\u6570\u636e\uff0c\u901a\u8fc7CoT\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u5956\u52b1\u673a\u5236\u786e\u4fdd\u683c\u5f0f\u548c\u7ed3\u6784\u6709\u6548\u6027\u3002", "result": "7B\u53c2\u6570\u6a21\u578b\u5728\u683c\u5f0f\u6709\u6548\u6027\u3001\u8282\u70b9\u548c\u56fe\u5f62\u7ea7\u522bF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude\u7cfb\u5217\u3002", "conclusion": "\u957f\u94fe\u63a8\u7406\u548c\u4ee3\u7801\u5316\u5de5\u4f5c\u6d41\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\uff0cComfyUI-R1\u5c55\u793a\u4e86\u5176\u590d\u6742\u5de5\u4f5c\u6d41\u5408\u6210\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.09104", "pdf": "https://arxiv.org/pdf/2506.09104", "abs": "https://arxiv.org/abs/2506.09104", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUPQ\u7684\u7edf\u4e00\u6e10\u8fdb\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u6307\u4ee4\u8c03\u4f18\u7684LLM\u4eceFP16\u91cf\u5316\u5230INT2\uff0c\u7ed3\u5408\u4e86\u5757\u7ea7\u540e\u8bad\u7ec3\u91cf\u5316\u548c\u84b8\u998f\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u7684INT2\u91cf\u5316\uff0c\u5e76\u5728MMLU\u548cIFEval\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c4\u6a21\u7684\u8fc5\u901f\u6269\u5927\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u5bf9\u6781\u4f4e\u4f4d\u91cf\u5316\uff08\u59822\u4f4d\uff09\u7684\u9700\u6c42\u589e\u52a0\u3002\u6b64\u524d\u5de5\u4f5c\u4ec5\u9488\u5bf9\u9884\u8bad\u7ec3LLM\uff0c\u672a\u6269\u5c55\u5230\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002", "method": "UPQ\u6846\u67b6\u5206\u4e24\u6b65\uff1a1\uff09\u4f7f\u7528\u5757\u7ea7\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5c06FP16\u6a21\u578b\u91cf\u5316\u4e3aINT4\u4ee5\u51cf\u5c11\u540e\u7eedINT2\u91cf\u5316\u7684\u8bef\u5dee\uff1b2\uff09\u901a\u8fc7\u84b8\u998f\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08Distill-QAT\uff09\u6700\u5c0f\u5316\u5e7f\u4e49Jensen-Shannon\u6563\u5ea6\uff08JSD\uff09\uff0c\u4f7fINT2\u6a21\u578b\u8f93\u51fa\u4e0eFP16\u4e00\u81f4\u3002", "result": "UPQ\u9996\u6b21\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u7684INT2\u6307\u4ee4\u8c03\u4f18LLM\u91cf\u5316\uff0c\u5728MMLU\u548cIFEval\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "UPQ\u4e3a\u6307\u4ee4\u8c03\u4f18LLM\u7684\u6781\u4f4e\u4f4d\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.09438", "pdf": "https://arxiv.org/pdf/2506.09438", "abs": "https://arxiv.org/abs/2506.09438", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u8bef\u5dee\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u5f02\u8d28\u6027\u3001\u6a21\u578b\u521d\u59cb\u5316\u548c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u62dc\u5360\u5ead\u653b\u51fb\u5bf9\u6cdb\u5316\u8bef\u5dee\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u4f18\u5316\u8bef\u5dee\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u6cdb\u5316\u8bef\u5dee\u76f8\u5bf9\u8f83\u5c11\u88ab\u63a2\u7d22\u3002\u6cdb\u5316\u8bef\u5dee\u5bf9\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u7406\u89e3\u5176\u5f71\u54cd\u56e0\u7d20\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u7684\u6cdb\u5316\u8bef\u5dee\u5206\u6790\uff0c\u9488\u5bf9\u65e0\u653b\u51fb\u548c\u62dc\u5360\u5ead\u9c81\u68d2\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\uff0c\u8003\u8651\u4e86\u6570\u636e\u5f02\u8d28\u6027\u548c\u6e29\u548c\u5047\u8bbe\uff0c\u800c\u975e\u4e25\u683c\u7684\u968f\u673a\u68af\u5ea6\u5047\u8bbe\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u636e\u5f02\u8d28\u6027\u3001\u6a21\u578b\u521d\u59cb\u5316\u548c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5bf9\u6cdb\u5316\u8bef\u5dee\u6709\u663e\u8457\u5f71\u54cd\uff0c\u62dc\u5360\u5ead\u653b\u51fb\u7684\u8d1f\u9762\u5f71\u54cd\u4e0e\u6570\u636e\u5f02\u8d28\u6027\u76f8\u5173\u800c\u4e0e\u6837\u672c\u91cf\u65e0\u5173\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u6cdb\u5316\u8bef\u5dee\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.09105", "pdf": "https://arxiv.org/pdf/2506.09105", "abs": "https://arxiv.org/abs/2506.09105", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "AI": {"tldr": "MetaTT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684Tensor Train\uff08TT\uff09\u9002\u914d\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3Transformer\u7684\u5168\u5c40\u4f4e\u79e9\u5fae\u8c03\u3002\u4e0eLoRA\u72ec\u7acb\u5fae\u8c03\u6bcf\u4e2a\u6743\u91cd\u77e9\u9635\u4e0d\u540c\uff0cMetaTT\u901a\u8fc7\u5171\u4eabTT\u5206\u89e3\u6240\u6709\u5b50\u6a21\u5757\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u53c2\u6570\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5168\u5c40\u4f4e\u79e9\u5fae\u8c03\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5171\u4eabTT\u5206\u89e3Transformer\u7684\u6240\u6709\u5b50\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u7d22\u5f15\u7ed3\u6784\u8f74\uff08\u5982\u5c42\u548c\u77e9\u9635\u7c7b\u578b\uff09\u5b9e\u73b0\u53c2\u6570\u538b\u7f29\u3002", "result": "\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaTT\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eLoRA\u76f8\u4f3c\u7684\u51c6\u786e\u6027\uff0c\u751a\u81f3\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5f20\u91cf\u7684\u65b9\u6cd5\u3002", "conclusion": "MetaTT\u901a\u8fc7TT\u5206\u89e3\u548c\u6210\u719f\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5fae\u8c03\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u5171\u4eab\u9002\u914d\u5668\u3002"}}
{"id": "2506.09660", "pdf": "https://arxiv.org/pdf/2506.09660", "abs": "https://arxiv.org/abs/2506.09660", "authors": ["Baran Can G\u00fcl", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "categories": ["cs.LG", "cs.DC"], "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "AI": {"tldr": "SyncFed\u662f\u4e00\u4e2a\u65f6\u95f4\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u540c\u6b65\u548c\u65f6\u95f4\u6233\u91cf\u5316\u5ba2\u6237\u7aef\u66f4\u65b0\u7684\u9648\u65e7\u6027\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u4fe1\u606f\u65b0\u9c9c\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7f51\u7edc\u5ef6\u8fdf\u3001\u65f6\u949f\u4e0d\u540c\u6b65\u548c\u5ba2\u6237\u7aef\u66f4\u65b0\u4e0d\u4e00\u81f4\u4f1a\u635f\u5bb3\u6a21\u578b\u53ef\u9760\u6027\u548c\u6536\u655b\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u91cf\u5316\u9648\u65e7\u6027\u7684\u673a\u5236\u3002", "method": "\u5f15\u5165SyncFed\u6846\u67b6\uff0c\u5229\u7528\u65f6\u95f4\u540c\u6b65\u548c\u65f6\u95f4\u6233\uff08\u57fa\u4e8eNTP\u534f\u8bae\uff09\u91cf\u5316\u66f4\u65b0\u9648\u65e7\u6027\uff0c\u5e76\u5728\u805a\u5408\u65f6\u5e94\u7528\u65f6\u95f4\u611f\u77e5\u7684\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSyncFed\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u80fd\u7a33\u5b9a\u6a21\u578b\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u4fe1\u606f\u65b0\u9c9c\u5ea6\u3002", "conclusion": "SyncFed\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "SensorLM\u662f\u4e00\u4e2a\u4f20\u611f\u5668-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u6570\u636e\u4e0e\u8bed\u8a00\u5bf9\u9f50\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u914d\u5bf9\u7684\u3001\u4e30\u5bcc\u6ce8\u91ca\u7684\u4f20\u611f\u5668-\u6587\u672c\u63cf\u8ff0\uff0c\u4f20\u611f\u5668\u6570\u636e\u4e0e\u8bed\u8a00\u7684\u5bf9\u9f50\u548c\u89e3\u91ca\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6807\u9898\u751f\u6210\u6d41\u7a0b\uff0c\u4ece\u4f20\u611f\u5668\u6570\u636e\u4e2d\u63d0\u53d6\u7edf\u8ba1\u3001\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u6269\u5c55\u4e86\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u67b6\u6784\uff08\u5982CLIP\u3001CoCa\uff09\u3002", "result": "\u6784\u5efa\u4e86\u6700\u5927\u7684\u4f20\u611f\u5668-\u8bed\u8a00\u6570\u636e\u96c6\uff0859.7\u767e\u4e07\u5c0f\u65f6\u6570\u636e\uff0c103,000\u4eba\uff09\uff0c\u5728\u96f6\u6837\u672c\u8bc6\u522b\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SensorLM\u5728\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u548c\u533b\u7597\u4fdd\u5065\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u793a\u4e86\u6807\u7b7e\u6548\u7387\u3001\u4f20\u611f\u5668\u6807\u9898\u751f\u6210\u548c\u96f6\u6837\u672c\u6cdb\u5316\u7b49\u80fd\u529b\u3002"}}
{"id": "2506.09870", "pdf": "https://arxiv.org/pdf/2506.09870", "abs": "https://arxiv.org/abs/2506.09870", "authors": ["Maximilian Egger", "Rawad Bitar"], "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "categories": ["cs.LG", "cs.DC", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u79d8\u5bc6\u5171\u4eab\u3001\u5b89\u5168\u805a\u5408\u548c\u5b9a\u5236\u5bf9\u79f0\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\u65b9\u6848\uff0c\u4ee5\u5728\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u5b9e\u73b0\u4fe1\u606f\u8bba\u9690\u79c1\u4fdd\u8bc1\u548c\u62dc\u5360\u5ead\u5f39\u6027\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u62a4\u5ba2\u6237\u7aef\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u62b5\u5fa1\u62dc\u5360\u5ead\u5ba2\u6237\u7aef\u7684\u653b\u51fb\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5f02\u8d28\u6027\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u79d8\u5bc6\u5171\u4eab\u3001\u5b89\u5168\u805a\u5408\u548c\u5b9a\u5236\u5bf9\u79f0\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\u65b9\u6848\u3002", "result": "\u5728\u591a\u79cd\u653b\u51fb\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u96f6\u9636\u4f30\u8ba1\u65b9\u6cd5\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u62dc\u5360\u5ead\u5f39\u6027\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u6210\u672c\u4f7f\u5176\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.09110", "pdf": "https://arxiv.org/pdf/2506.09110", "abs": "https://arxiv.org/abs/2506.09110", "authors": ["Jingying Ma", "Feng Wu", "Qika Lin", "Yucheng Xing", "Chenyu Liu", "Ziyu Jia", "Mengling Feng"], "title": "CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model", "categories": ["cs.LG"], "comment": null, "summary": "Electroencephalography (EEG) provides real-time insights into brain activity\nand is widely used in neuroscience. However, variations in channel\nconfigurations, sequence lengths, and task objectives limit the transferability\nof traditional task-specific models. Although recent EEG foundation models\n(EFMs) aim to learn generalizable representations, they struggle with limited\nheterogeneous representation capacity and inefficiency in capturing multi-scale\nbrain dependencies. To address these challenges, we propose CodeBrain, an\nefficient EFM structurally aligned with brain organization, trained in two\nstages. (1) We introduce a TFDual-Tokenizer that independently tokenizes\nheterogeneous temporal and frequency components, enabling a quadratic expansion\nof the discrete representation space. This also offers a degree of\ninterpretability through cross-domain token analysis. (2) We propose the\nEEGSSM, which combines a structured global convolution architecture and a\nsliding window attention mechanism to jointly model sparse long-range and local\ndependencies. Unlike fully connected Transformer models, EEGSSM better reflects\nthe brain's small-world topology and efficiently captures EEG's inherent\nmulti-scale structure. EEGSSM is trained with a masked self-supervised learning\nobjective to predict token indices obtained in TFDual-Tokenizer. Comprehensive\nexperiments on 10 public EEG datasets demonstrate the generalizability of\nCodeBrain with linear probing. By offering biologically informed and\ninterpretable EEG modeling, CodeBrain lays the foundation for future\nneuroscience research. Both code and pretraining weights will be released in\nthe future version.", "AI": {"tldr": "CodeBrain\u662f\u4e00\u79cd\u9ad8\u6548\u7684EEG\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728EEG\u6570\u636e\u4e2d\u7684\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u4e86TFDual-Tokenizer\u548cEEGSSM\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfEEG\u6a21\u578b\u56e0\u901a\u9053\u914d\u7f6e\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u4efb\u52a1\u76ee\u6807\u7684\u5dee\u5f02\u800c\u6cdb\u5316\u6027\u5dee\uff0c\u73b0\u6709EEG\u57fa\u7840\u6a21\u578b\u5728\u5f02\u6784\u8868\u793a\u548c\u591a\u5c3a\u5ea6\u4f9d\u8d56\u6355\u6349\u4e0a\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTFDual-Tokenizer\u72ec\u7acb\u7f16\u7801\u65f6\u9891\u57df\u7279\u5f81\uff0c\u6269\u5c55\u8868\u793a\u7a7a\u95f4\uff1bEEGSSM\u7ed3\u5408\u5168\u5c40\u5377\u79ef\u548c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u5efa\u6a21\u591a\u5c3a\u5ea6\u4f9d\u8d56\u3002", "result": "\u572810\u4e2a\u516c\u5f00EEG\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CodeBrain\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "CodeBrain\u4e3aEEG\u5efa\u6a21\u63d0\u4f9b\u4e86\u751f\u7269\u5b66\u542f\u53d1\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u672a\u6765\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09114", "pdf": "https://arxiv.org/pdf/2506.09114", "abs": "https://arxiv.org/abs/2506.09114", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "categories": ["cs.LG"], "comment": null, "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "AI": {"tldr": "TRACE\u662f\u4e00\u79cd\u901a\u7528\u7684\u591a\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u4e0e\u5bf9\u9f50\u7684\u6587\u672c\u4e0a\u4e0b\u6587\u7ed3\u5408\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u901a\u9053\u7ea7\u5bf9\u9f50\u548c\u8bed\u4e49\u68c0\u7d22\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u52a8\u6001\u6570\u636e\uff08\u5982\u5929\u6c14\u3001\u533b\u7597\u548c\u80fd\u6e90\u9886\u57df\uff09\u7684\u666e\u904d\u6027\u9700\u8981\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u89e3\u91ca\u548c\u68c0\u7d22\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\uff0c\u96be\u4ee5\u5904\u7406\u5f02\u6784\u6a21\u6001\u548c\u591a\u901a\u9053\u4fe1\u53f7\u3002", "method": "\u63d0\u51faTRACE\uff0c\u901a\u8fc7\u786c\u8d1f\u6837\u672c\u6316\u6398\u5b9e\u73b0\u8bed\u4e49\u68c0\u7d22\uff0c\u652f\u6301\u6587\u672c\u5230\u65f6\u95f4\u5e8f\u5217\u548c\u65f6\u95f4\u5e8f\u5217\u5230\u6587\u672c\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u3002", "result": "TRACE\u5728\u4e0b\u6e38\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4f5c\u4e3a\u901a\u7528\u68c0\u7d22\u5668\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "TRACE\u4e0d\u4ec5\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u72ec\u7acb\u7f16\u7801\u5668\uff0c\u8fd8\u80fd\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u63d0\u5347\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09163", "pdf": "https://arxiv.org/pdf/2506.09163", "abs": "https://arxiv.org/abs/2506.09163", "authors": ["Daniel Jenson", "Jhonathan Navott", "Piotr Grynfelder", "Mengyan Zhang", "Makkunda Sharma", "Elizaveta Semenova", "Seth Flaxman"], "title": "Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nWhile early architectures were developed primarily as a scalable alternative to\nGaussian Processes (GPs), modern NPs tackle far more complex and data hungry\napplications spanning geology, epidemiology, climate, and robotics. These\napplications have placed increasing pressure on the scalability of these\nmodels, with many architectures compromising accuracy for scalability. In this\npaper, we demonstrate that this tradeoff is often unnecessary, particularly\nwhen modeling fully or partially translation invariant processes. We propose a\nversatile new architecture, the Biased Scan Attention Transformer Neural\nProcess (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),\ngroup-invariant attention biases, and memory-efficient Biased Scan Attention\n(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models\nwhile often training in a fraction of the time, (2) exhibit translation\ninvariance, enabling learning at multiple resolutions simultaneously, (3)\ntransparently model processes that evolve in both space and time, (4) support\nhigh dimensional fixed effects, and (5) scale gracefully -- running inference\nwith over 1M test points with 100K context points in under a minute on a single\n24GB GPU.", "AI": {"tldr": "BSA-TNP\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u8fc7\u7a0b\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165KRBlocks\u548cBSA\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u5206\u8fa8\u7387\u5b66\u4e60\u548c\u65f6\u7a7a\u5efa\u6a21\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u8fc7\u7a0b\u6a21\u578b\u5728\u590d\u6742\u5e94\u7528\u4e2d\u9762\u4e34\u7cbe\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u6743\u8861\uff0c\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u79cd\u6743\u8861\u5e76\u975e\u5fc5\u8981\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faBSA-TNP\u67b6\u6784\uff0c\u7ed3\u5408KRBlocks\u3001\u7fa4\u4e0d\u53d8\u6ce8\u610f\u529b\u504f\u5dee\u548c\u5185\u5b58\u9ad8\u6548\u7684BSA\uff0c\u652f\u6301\u591a\u5206\u8fa8\u7387\u5b66\u4e60\u548c\u65f6\u7a7a\u5efa\u6a21\u3002", "result": "BSA-TNP\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u6a21\u578b\uff0c\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\uff0c\u652f\u6301\u9ad8\u7ef4\u56fa\u5b9a\u6548\u5e94\uff0c\u5e76\u80fd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3002", "conclusion": "BSA-TNP\u5728\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5e94\u7528\uff0c\u4e3a\u795e\u7ecf\u8fc7\u7a0b\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u589e\u5f3a\u89c4\u5212\u80fd\u529b\uff0c\u5229\u7528\u539f\u5b50\u4e8b\u5b9e\u589e\u5f3a\u548c\u9012\u5f52\u524d\u77bb\u641c\u7d22\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u7ebf\u6539\u8fdb\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u96be\u4ee5\u9002\u5e94\u65b0\u4fe1\u606f\u6216\u9ad8\u6548\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u539f\u5b50\u4e8b\u5b9e\u589e\u5f3a\u52a8\u6001\u63d0\u793aLLM\u7ec4\u4ef6\uff0c\u7ed3\u5408\u6df1\u5ea6\u53d7\u9650\u7684\u524d\u77bb\u641c\u7d22\u6a21\u62df\u8f68\u8ff9\u5e76\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5728TextFrozenLake\u548cALFWorld\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u4f18\u5316\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4e8b\u5b9e\u62bd\u8c61\u548cLLM\u6a21\u62df\u7684\u7406\u8bba\u52a8\u673a\u3002"}}
{"id": "2506.09172", "pdf": "https://arxiv.org/pdf/2506.09172", "abs": "https://arxiv.org/abs/2506.09172", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "AI": {"tldr": "MultiNet\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u9002\u5e94\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u9886\u57df\u7684\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u901a\u7528\u4ee3\u7406\u7cfb\u7edf\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u7406\u89e3\u3001\u8bed\u8a00\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51faMultiNet\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u3001\u5f00\u6e90\u8f6f\u4ef6\u548c\u590d\u5408\u6570\u636e\u96c6\uff081.3\u4e07\u4ebftoken\uff09\u3002", "result": "MultiNet\u88ab\u7528\u4e8e\u4e0b\u6e38\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u5c40\u9650\u6027\u3002", "conclusion": "MultiNet\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bc4\u4f30\u548c\u9002\u5e94\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u901a\u7528\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09173", "pdf": "https://arxiv.org/pdf/2506.09173", "abs": "https://arxiv.org/abs/2506.09173", "authors": ["Michael Cooper", "Rohan Wadhawan", "John Michael Giorgi", "Chenhao Tan", "Davis Liang"], "title": "The Curious Language Model: Strategic Test-Time Information Acquisition", "categories": ["cs.LG", "cs.CL"], "comment": "39 pages", "summary": "Decision-makers often possess insufficient information to render a confident\ndecision. In these cases, the decision-maker can often undertake actions to\nacquire the necessary information about the problem at hand, e.g., by\nconsulting knowledgeable authorities or by conducting experiments. Importantly,\ndifferent levers of information acquisition come with different costs, posing\nthe challenge of selecting the actions that are both informative and\ncost-effective. In this work, we propose CuriosiTree, a heuristic-based,\ntest-time policy for zero-shot information acquisition in large language models\n(LLMs). CuriosiTree employs a greedy tree search to estimate the expected\ninformation gain of each action and strategically chooses actions based on a\nbalance of anticipated information gain and associated cost. Empirical\nvalidation in a clinical diagnosis simulation shows that CuriosiTree enables\ncost-effective integration of heterogenous sources of information, and\noutperforms baseline action selection strategies in selecting action sequences\nthat enable accurate diagnosis.", "AI": {"tldr": "CuriosiTree\u662f\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u96f6\u6837\u672c\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\uff0c\u901a\u8fc7\u8d2a\u5fc3\u6811\u641c\u7d22\u5e73\u8861\u4fe1\u606f\u589e\u76ca\u4e0e\u6210\u672c\uff0c\u5728\u4e34\u5e8a\u8bca\u65ad\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u51b3\u7b56\u8005\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u9700\u9009\u62e9\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4fe1\u606f\u83b7\u53d6\u65b9\u5f0f\u3002", "method": "\u63d0\u51faCuriosiTree\uff0c\u91c7\u7528\u8d2a\u5fc3\u6811\u641c\u7d22\u8bc4\u4f30\u884c\u52a8\u7684\u4fe1\u606f\u589e\u76ca\u4e0e\u6210\u672c\u3002", "result": "\u5728\u4e34\u5e8a\u8bca\u65ad\u6a21\u62df\u4e2d\uff0cCuriosiTree\u80fd\u6709\u6548\u6574\u5408\u5f02\u6784\u4fe1\u606f\u6e90\uff0c\u8bca\u65ad\u51c6\u786e\u6027\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "CuriosiTree\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4fe1\u606f\u4e0d\u8db3\u7684\u51b3\u7b56\u573a\u666f\u3002"}}
{"id": "2506.09174", "pdf": "https://arxiv.org/pdf/2506.09174", "abs": "https://arxiv.org/abs/2506.09174", "authors": ["Chenheng Xu", "Dan Wu", "Yixin Zhu", "Ying Nian Wu"], "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter", "categories": ["cs.LG"], "comment": null, "summary": "Multivariate long-term time series forecasting has been suffering from the\nchallenge of capturing both temporal dependencies within variables and spatial\ncorrelations across variables simultaneously. Current approaches predominantly\nrepurpose backbones from natural language processing or computer vision (e.g.,\nTransformers), which fail to adequately address the unique properties of time\nseries (e.g., periodicity). The research community lacks a dedicated backbone\nwith temporal-specific inductive biases, instead relying on domain-agnostic\nbackbones supplemented with auxiliary techniques (e.g., signal decomposition).\nWe introduce FNF as the backbone and DBD as the architecture to provide\nexcellent learning capabilities and optimal learning pathways for\nspatio-temporal modeling, respectively. Our theoretical analysis proves that\nFNF unifies local time-domain and global frequency-domain information\nprocessing within a single backbone that extends naturally to spatial modeling,\nwhile information bottleneck theory demonstrates that DBD provides superior\ngradient flow and representation capacity compared to existing unified or\nsequential architectures. Our empirical evaluation across 11 public benchmark\ndatasets spanning five domains (energy, meteorology, transportation,\nenvironment, and nature) confirms state-of-the-art performance with consistent\nhyperparameter settings. Notably, our approach achieves these results without\nany auxiliary techniques, suggesting that properly designed neural\narchitectures can capture the inherent properties of time series, potentially\ntransforming time series modeling in scientific and industrial applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFNF\u7684\u9aa8\u5e72\u7f51\u7edc\u548cDBD\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u53d8\u91cf\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u540c\u65f6\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u7a7a\u95f4\u76f8\u5173\u6027\u7684\u6311\u6218\uff0c\u65e0\u9700\u4f9d\u8d56\u8f85\u52a9\u6280\u672f\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u501f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u9aa8\u5e72\u7f51\u7edc\uff08\u5982Transformer\uff09\uff0c\u672a\u80fd\u5145\u5206\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7684\u72ec\u7279\u6027\u8d28\uff08\u5982\u5468\u671f\u6027\uff09\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u9aa8\u5e72\u7f51\u7edc\u3002", "method": "\u5f15\u5165FNF\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u7edf\u4e00\u5904\u7406\u5c40\u90e8\u65f6\u57df\u548c\u5168\u5c40\u9891\u57df\u4fe1\u606f\uff1bDBD\u67b6\u6784\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u4f18\u5316\u68af\u5ea6\u6d41\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "\u572811\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8f85\u52a9\u6280\u672f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8bbe\u8ba1\u826f\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u53ef\u4ee5\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u7279\u6027\uff0c\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u65b9\u5f0f\u3002"}}
{"id": "2506.09183", "pdf": "https://arxiv.org/pdf/2506.09183", "abs": "https://arxiv.org/abs/2506.09183", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "title": "Multi-Task Reward Learning from Human Ratings", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8003\u8651\u591a\u4e2a\u4efb\u52a1\u6765\u6a21\u4eff\u4eba\u7c7b\u51b3\u7b56\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dRLHF\u65b9\u6cd5\u7b80\u5316\u4e86\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u672a\u80fd\u5145\u5206\u6574\u5408\u591a\u7b56\u7565\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528\u65e0\u5956\u52b1\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u8bc4\u5206\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u6743\u91cd\u5e73\u8861\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u8bc4\u5206RL\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u4f20\u7edfRL\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u4eba\u7c7b\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9002\u5e94\u6027\u5f3a\u3002"}}
{"id": "2506.09193", "pdf": "https://arxiv.org/pdf/2506.09193", "abs": "https://arxiv.org/abs/2506.09193", "authors": ["Yilin Zhuang", "Karthik Duraisamy"], "title": "LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Accurate probabilistic weather forecasting demands both high accuracy and\nefficient uncertainty quantification, challenges that overburden both ensemble\nnumerical weather prediction (NWP) and recent machine-learning methods. We\nintroduce LaDCast, the first global latent-diffusion framework for medium-range\nensemble forecasting, which generates hourly ensemble forecasts entirely in a\nlearned latent space. An autoencoder compresses high-dimensional ERA5\nreanalysis fields into a compact representation, and a transformer-based\ndiffusion model produces sequential latent updates with arbitrary hour\ninitialization. The model incorporates Geometric Rotary Position Embedding\n(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream\nattention mechanism for efficient conditioning, and sinusoidal temporal\nembeddings to capture seasonal patterns. LaDCast achieves deterministic and\nprobabilistic skill close to that of the European Centre for Medium-Range\nForecast IFS-ENS, without any explicit perturbations. Notably, LaDCast\ndemonstrates superior performance in tracking rare extreme events such as\ncyclones, capturing their trajectories more accurately than established models.\nBy operating in latent space, LaDCast reduces storage and compute by orders of\nmagnitude, demonstrating a practical path toward forecasting at kilometer-scale\nresolution in real time. We open-source our code and models and provide the\ntraining and evaluation pipelines at: https://github.com/tonyzyl/ladcast.", "AI": {"tldr": "LaDCast\u662f\u4e00\u79cd\u5168\u7403\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u4e2d\u7a0b\u96c6\u5408\u5929\u6c14\u9884\u62a5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u5c0f\u65f6\u7ea7\u96c6\u5408\u9884\u62a5\uff0c\u6027\u80fd\u63a5\u8fd1\u6b27\u6d32\u4e2d\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u5fc3IFS-ENS\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cLaDCast\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u538b\u7f29\u9ad8\u7ef4ERA5\u518d\u5206\u6790\u6570\u636e\uff0c\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u6f5c\u5728\u66f4\u65b0\uff0c\u7ed3\u5408GeoRoPE\u3001\u53cc\u6d41\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b63\u5f26\u65f6\u95f4\u5d4c\u5165\u3002", "result": "LaDCast\u5728\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u9884\u62a5\u6280\u80fd\u4e0a\u63a5\u8fd1IFS-ENS\uff0c\u4e14\u5728\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u6c14\u65cb\uff09\u8f68\u8ff9\u6355\u6349\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002", "conclusion": "LaDCast\u5c55\u793a\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u5929\u6c14\u9884\u62a5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u65f6\u516c\u91cc\u7ea7\u5206\u8fa8\u7387\u9884\u62a5\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.09200", "pdf": "https://arxiv.org/pdf/2506.09200", "abs": "https://arxiv.org/abs/2506.09200", "authors": ["Val Andrei Fajardo", "David B. Emerson", "Amandeep Singh", "Veronica Chatrath", "Marcelo Lotif", "Ravi Theja", "Alex Cheung", "Izuki Matsubi"], "title": "FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at\n  ICML 2025. Framework code available at\n  https://github.com/VectorInstitute/fed-rag", "summary": "Retrieval-augmented generation (RAG) systems have been shown to be effective\nin addressing many of the drawbacks of relying solely on the parametric memory\nof large language models. Recent work has demonstrated that RAG systems can be\nimproved via fine-tuning of their retriever and generator models. In this work,\nwe introduce FedRAG, a framework for fine-tuning RAG systems across centralized\nand federated architectures. FedRAG supports state-of-the-art fine-tuning\nmethods, offering a simple and intuitive interface and a seamless conversion\nfrom centralized to federated training tasks. FedRAG is also deeply integrated\nwith the modern RAG ecosystem, filling a critical gap in available tools.", "AI": {"tldr": "FedRAG\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5f0f\u67b6\u6784\u4e0a\u5fae\u8c03RAG\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u652f\u6301\u5148\u8fdb\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u8bb0\u5fc6\u7684\u7f3a\u70b9\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u63d0\u5347RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faFedRAG\u6846\u67b6\uff0c\u652f\u6301\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5f0f\u67b6\u6784\u7684\u5fae\u8c03\uff0c\u63d0\u4f9b\u7b80\u5355\u63a5\u53e3\u548c\u4efb\u52a1\u8f6c\u6362\u529f\u80fd\u3002", "result": "FedRAG\u586b\u8865\u4e86\u73b0\u4ee3RAG\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u7a7a\u767d\uff0c\u652f\u6301\u5148\u8fdb\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "FedRAG\u4e3aRAG\u7cfb\u7edf\u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u3002"}}
{"id": "2506.09202", "pdf": "https://arxiv.org/pdf/2506.09202", "abs": "https://arxiv.org/abs/2506.09202", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\uff1a\u4ece\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u4e2d\u805a\u7c7b\u8f68\u8ff9\uff0c\u6bcf\u4e2a\u805a\u7c7b\u4e2d\u5fc3\u4ee3\u8868\u751f\u6210\u5176\u8f68\u8ff9\u7684\u7b56\u7565\u3002\u901a\u8fc7KL\u6563\u5ea6\u548c\u7b56\u7565\u8bf1\u5bfc\u5206\u5e03\u7684\u6df7\u5408\uff0c\u63d0\u51fa\u4e86PG-Kmeans\u548cCAAE\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u4e2d\u7684\u8f68\u8ff9\u805a\u7c7b\u662f\u4e00\u4e2a\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u805a\u7c7b\u4e2d\u5fc3\u63ed\u793a\u751f\u6210\u8f68\u8ff9\u7684\u7b56\u7565\uff0c\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51faPG-Kmeans\uff08\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7b56\u7565\u8fed\u4ee3\u805a\u7c7b\uff09\u548cCAAE\uff08\u7c7b\u4f3cVQ-VAE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u5f15\u5bfc\u805a\u7c7b\uff09\u3002", "result": "\u5728D4RL\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49GridWorld\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5c06\u8f68\u8ff9\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u805a\u7c7b\u3002", "conclusion": "PG-Kmeans\u548cCAAE\u4e3a\u57fa\u4e8e\u7b56\u7565\u7684\u8f68\u8ff9\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09207", "pdf": "https://arxiv.org/pdf/2506.09207", "abs": "https://arxiv.org/abs/2506.09207", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "title": "mLaSDI: Multi-stage latent space dynamics identification", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u964d\u9636\u6a21\u578b\u65b9\u6cd5mLaSDI\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u6765\u63d0\u5347\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfLaSDI\u65b9\u6cd5\u5728\u590d\u6742\u6216\u9ad8\u9891\u573a\u666f\u4e0b\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u6570\u636e\u91cd\u5efa\u548c\u52a8\u6001\u7ea6\u675f\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u81ea\u7f16\u7801\u5668\u8bad\u7ec3\uff0c\u6bcf\u9636\u6bb5\u4fee\u6b63\u524d\u4e00\u9636\u6bb5\u7684\u8bef\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "mLaSDI\u5728\u9884\u6d4b\u548c\u91cd\u5efa\u8bef\u5dee\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "mLaSDI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u964d\u9636\u6a21\u578b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2506.09215", "pdf": "https://arxiv.org/pdf/2506.09215", "abs": "https://arxiv.org/abs/2506.09215", "authors": ["Greyson Brothers"], "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68P30, 68T45 (Secondary)", "E.4; I.2.6; I.2.10"], "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u5d4c\u5165\u6a21\u578b\u7684\u6c60\u5316\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u6c60\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08AvgPool\u3001MaxPool\u3001ClsToken\uff09\uff0c\u5728\u4fe1\u53f7\u566a\u58f0\u6bd4\u6ce2\u52a8\u65f6\u8868\u73b0\u66f4\u7a33\u5065\u3002", "motivation": "\u4e3b\u8981\u52a8\u673a\u662f\u89e3\u51b3\u4f20\u7edf\u6c60\u5316\u65b9\u6cd5\u5728\u4fe1\u53f7\u566a\u58f0\u6bd4\uff08SNR\uff09\u6ce2\u52a8\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u89c9\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u5c06\u6c60\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u5411\u91cf\u91cf\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u81ea\u9002\u5e94\u6c60\u5316\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u4fe1\u53f7\u635f\u5931\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982\u5173\u7cfb\u63a8\u7406\u3001\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u89c9\u4efb\u52a1\uff09\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u6c60\u5316\u65b9\u6cd5\u5728\u4fe1\u53f7\u566a\u58f0\u6bd4\u6ce2\u52a8\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4e3aTransformer\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09227", "pdf": "https://arxiv.org/pdf/2506.09227", "abs": "https://arxiv.org/abs/2506.09227", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "title": "SoK: Machine Unlearning for Large Language Models", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9057\u5fd8\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u9057\u5fd8\u7684\u771f\u5b9e\u6027\u3001\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u5b9e\u9645\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u6280\u672f\u7684\u5206\u7c7b\u5ffd\u89c6\u4e86\u5176\u6839\u672c\u610f\u56fe\uff0c\u5373\u662f\u5426\u771f\u6b63\u79fb\u9664\u5185\u90e8\u77e5\u8bc6\u6216\u4ec5\u6291\u5236\u5176\u884c\u4e3a\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u610f\u56fe\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u91cd\u65b0\u5ba1\u89c6\u9057\u5fd8\u65b9\u6cd5\u7684\u771f\u5b9e\u6027\uff0c\u8c03\u67e5\u8bc4\u4f30\u7b56\u7565\u5e76\u6307\u51fa\u5c40\u9650\u6027\uff0c\u63a2\u8ba8\u5b9e\u9645\u6311\u6218\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u79fb\u9664\u65b9\u6cd5\u53ef\u80fd\u4ec5\u8868\u73b0\u4e3a\u6291\u5236\uff0c\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u987a\u5e8f\u9057\u5fd8\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u548c\u63a8\u8fdb\u751f\u6210\u5f0fAI\u4e2d\u7684\u9057\u5fd8\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u548c\u6570\u636e\u9690\u79c1\u653f\u7b56\u51b3\u7b56\u3002"}}
{"id": "2506.09247", "pdf": "https://arxiv.org/pdf/2506.09247", "abs": "https://arxiv.org/abs/2506.09247", "authors": ["Karl L\u00f6wenmark", "Daniel Str\u00f6mbergsson", "Chang Liu", "Marcus Liwicki", "Fredrik Sandin"], "title": "Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation", "categories": ["cs.LG"], "comment": null, "summary": "Condition monitoring (CM) plays a crucial role in ensuring reliability and\nefficiency in the process industry. Although computerised maintenance systems\neffectively detect and classify faults, tasks like fault severity estimation,\nand maintenance decisions still largely depend on human expert analysis. The\nanalysis and decision making automatically performed by current systems\ntypically exhibit considerable uncertainty and high false alarm rates, leading\nto increased workload and reduced efficiency.\n  This work integrates large language model (LLM)-based reasoning agents with\nCM workflows to address analyst and industry needs, namely reducing false\nalarms, enhancing fault severity estimation, improving decision support, and\noffering explainable interfaces. We propose MindRAG, a modular framework\ncombining multimodal retrieval-augmented generation (RAG) with novel vector\nstore structures designed specifically for CM data. The framework leverages\nexisting annotations and maintenance work orders as surrogates for labels in a\nsupervised learning protocol, addressing the common challenge of training\npredictive models on unlabelled and noisy real-world datasets.\n  The primary contributions include: (1) an approach for structuring industry\nCM data into a semi-structured multimodal vector store compatible with\nLLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM\ndata; (3) developing practical reasoning agents capable of addressing\nreal-world CM queries; and (4) presenting an experimental framework for\nintegrating and evaluating such agents in realistic industrial scenarios.\nPreliminary results, evaluated with the help of an experienced analyst,\nindicate that MindRAG provide meaningful decision support for more efficient\nmanagement of alarms, thereby improving the interpretability of CM systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMindRAG\u6846\u67b6\uff0c\u7ed3\u5408LLM\u548c\u591a\u6a21\u6001RAG\u6280\u672f\uff0c\u4f18\u5316\u5de5\u4e1a\u6761\u4ef6\u76d1\u6d4b\uff08CM\uff09\u4e2d\u7684\u6545\u969c\u4e25\u91cd\u6027\u4f30\u8ba1\u548c\u51b3\u7b56\u652f\u6301\uff0c\u51cf\u5c11\u8bef\u62a5\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u5f53\u524dCM\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u5206\u6790\uff0c\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u4e0d\u786e\u5b9a\u6027\uff0cMindRAG\u65e8\u5728\u901a\u8fc7LLM\u548cRAG\u6280\u672f\u81ea\u52a8\u5316\u5206\u6790\uff0c\u63d0\u5347\u51b3\u7b56\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faMindRAG\u6846\u67b6\uff0c\u6574\u5408\u591a\u6a21\u6001RAG\u548c\u65b0\u578b\u5411\u91cf\u5b58\u50a8\u7ed3\u6784\uff0c\u5229\u7528\u73b0\u6709\u6807\u6ce8\u548c\u7ef4\u62a4\u5de5\u5355\u4f5c\u4e3a\u76d1\u7763\u5b66\u4e60\u6807\u7b7e\uff0c\u5904\u7406\u65e0\u6807\u7b7e\u548c\u566a\u58f0\u6570\u636e\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cMindRAG\u80fd\u6709\u6548\u652f\u6301\u51b3\u7b56\uff0c\u63d0\u9ad8\u62a5\u8b66\u7ba1\u7406\u6548\u7387\uff0c\u5e76\u589e\u5f3aCM\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MindRAG\u901a\u8fc7LLM\u548cRAG\u6280\u672f\u663e\u8457\u6539\u5584\u4e86CM\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09258", "pdf": "https://arxiv.org/pdf/2506.09258", "abs": "https://arxiv.org/abs/2506.09258", "authors": ["Vaidotas Simkus", "Michael U. Gutmann"], "title": "CFMI: Flow Matching for Missing Data Imputation", "categories": ["cs.LG", "stat.ML", "62D10", "I.5.1"], "comment": null, "summary": "We introduce conditional flow matching for imputation (CFMI), a new\ngeneral-purpose method to impute missing data. The method combines continuous\nnormalising flows, flow-matching, and shared conditional modelling to deal with\nintractabilities of traditional multiple imputation. Our comparison with nine\nclassical and state-of-the-art imputation methods on 24 small to\nmoderate-dimensional tabular data sets shows that CFMI matches or outperforms\nboth traditional and modern techniques across a wide range of metrics. Applying\nthe method to zero-shot imputation of time-series data, we find that it matches\nthe accuracy of a related diffusion-based method while outperforming it in\nterms of computational efficiency. Overall, CFMI performs at least as well as\ntraditional methods on lower-dimensional data while remaining scalable to\nhigh-dimensional settings, matching or exceeding the performance of other deep\nlearning-based approaches, making it a go-to imputation method for a wide range\nof data types and dimensionalities.", "AI": {"tldr": "CFMI\u662f\u4e00\u79cd\u65b0\u7684\u901a\u7528\u7f3a\u5931\u6570\u636e\u586b\u8865\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u3001\u6d41\u5339\u914d\u548c\u5171\u4eab\u6761\u4ef6\u5efa\u6a21\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u4ee3\u586b\u8865\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u91cd\u586b\u8865\u65b9\u6cd5\u7684\u8ba1\u7b97\u96be\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u586b\u8865\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u3001\u6d41\u5339\u914d\u548c\u5171\u4eab\u6761\u4ef6\u5efa\u6a21\uff0c\u5904\u7406\u7f3a\u5931\u6570\u636e\u3002", "result": "\u572824\u4e2a\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u4e2d\uff0cCFMI\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u586b\u8865\u4e2d\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "CFMI\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5404\u79cd\u6570\u636e\u7c7b\u578b\u548c\u7ef4\u5ea6\u7684\u9ad8\u6548\u586b\u8865\u65b9\u6cd5\u3002"}}
{"id": "2506.09270", "pdf": "https://arxiv.org/pdf/2506.09270", "abs": "https://arxiv.org/abs/2506.09270", "authors": ["Rodrigo Carrasco-Davis", "Sebastian Lee", "Claudia Clopath", "Will Dabney"], "title": "Uncertainty Prioritized Experience Replay", "categories": ["cs.LG"], "comment": "Accepted at Reinforcement Learning Conference", "summary": "Prioritized experience replay, which improves sample efficiency by selecting\nrelevant transitions to update parameter estimates, is a crucial component of\ncontemporary value-based deep reinforcement learning models. Typically,\ntransitions are prioritized based on their temporal difference error. However,\nthis approach is prone to favoring noisy transitions, even when the value\nestimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which\nexploration-guided agents get stuck by mistaking noise for novelty. To mitigate\nthe disruptive effects of noise in value estimation, we propose using epistemic\nuncertainty estimation to guide the prioritization of transitions from the\nreplay buffer. Epistemic uncertainty quantifies the uncertainty that can be\nreduced by learning, hence reducing transitions sampled from the buffer\ngenerated by unpredictable random processes. We first illustrate the benefits\nof epistemic uncertainty prioritized replay in two tabular toy models: a simple\nmulti-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our\nprioritization scheme on the Atari suite, outperforming quantile regression\ndeep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u7ecf\u9a8c\u56de\u653e\u4f18\u5148\u7ea7\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u65f6\u95f4\u5dee\u5206\u8bef\u5dee\u7684\u4f18\u5148\u7ea7\u65b9\u6cd5\u5bb9\u6613\u53d7\u566a\u58f0\u5e72\u6270\uff0c\u7c7b\u4f3c\u4e8e\u63a2\u7d22\u4e2d\u7684\u566a\u58f0\u7535\u89c6\u95ee\u9898\u3002", "method": "\u5229\u7528\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6307\u5bfc\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u7684\u8f6c\u79fb\u4f18\u5148\u7ea7\uff0c\u51cf\u5c11\u4e0d\u53ef\u9884\u6d4b\u968f\u673a\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u8868\u683c\u6a21\u578b\u548cAtari\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u6df1\u5ea6Q\u5b66\u4e60\u57fa\u51c6\u3002", "conclusion": "\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f18\u5148\u7ea7\u56de\u653e\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u8def\u5f84\u3002"}}
{"id": "2506.09272", "pdf": "https://arxiv.org/pdf/2506.09272", "abs": "https://arxiv.org/abs/2506.09272", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Antonin Berthon", "Mihaela van der Schaar"], "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "categories": ["cs.LG", "stat.ML", "68T05, 68U20, 62F15", "I.2.6; I.6.5; G.3"], "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "AI": {"tldr": "G-Sim\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u7ed3\u6784\u8bbe\u8ba1\u548c\u4e25\u683c\u7684\u5b9e\u8bc1\u6821\u51c6\uff0c\u81ea\u52a8\u5316\u6784\u5efa\u7a33\u5065\u7684\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5728\u533b\u7597\u548c\u7269\u6d41\u7b49\u5173\u952e\u9886\u57df\uff0c\u6784\u5efa\u53ef\u9760\u7684\u6a21\u62df\u5668\u5bf9\u653f\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u6216\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "G-Sim\u901a\u8fc7LLM\u8fed\u4ee3\u8bbe\u8ba1\u6a21\u62df\u5668\u6838\u5fc3\u7ec4\u4ef6\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u7ed3\u5408\u7075\u6d3b\u7684\u6821\u51c6\u6280\u672f\uff08\u5982\u68af\u5ea6\u81ea\u7531\u4f18\u5316\u6216\u6a21\u62df\u63a8\u65ad\uff09\u4f30\u8ba1\u53c2\u6570\u3002", "result": "G-Sim\u751f\u6210\u53ef\u9760\u3001\u56e0\u679c\u611f\u77e5\u7684\u6a21\u62df\u5668\uff0c\u7f13\u89e3\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u652f\u6301\u590d\u6742\u51b3\u7b56\u7684\u7cfb\u7edf\u7ea7\u5e72\u9884\u3002", "conclusion": "G-Sim\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5148\u9a8c\u548c\u5b9e\u8bc1\u8bc1\u636e\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u63d0\u4f9b\u7a33\u5065\u7684\u6a21\u62df\u5668\u3002"}}
{"id": "2506.09276", "pdf": "https://arxiv.org/pdf/2506.09276", "abs": "https://arxiv.org/abs/2506.09276", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "\u00d6zg\u00fcr \u015eim\u015fek", "Anders Jonsson"], "title": "Learning The Minimum Action Distance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4ece\u72b6\u6001\u8f68\u8ff9\u5b66\u4e60MDP\u72b6\u6001\u8868\u793a\u7684\u6846\u67b6\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\u6216\u52a8\u4f5c\u4fe1\u606f\uff0c\u901a\u8fc7\u5b66\u4e60\u6700\u5c0f\u52a8\u4f5c\u8ddd\u79bb\uff08MAD\uff09\u4f5c\u4e3a\u73af\u5883\u7ed3\u6784\u7684\u5ea6\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5956\u52b1\u6216\u52a8\u4f5c\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60MAD\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u901a\u7528\u7684\u72b6\u6001\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5b66\u4e60MAD\u4f5c\u4e3a\u72b6\u6001\u95f4\u8ddd\u79bb\u7684\u5ea6\u91cf\uff0c\u6784\u5efa\u5d4c\u5165\u7a7a\u95f4\u4f7f\u5d4c\u5165\u8ddd\u79bb\u5bf9\u5e94MAD\uff0c\u652f\u6301\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u8fd1\u4f3c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u5b66\u4e60\u51c6\u786e\u7684MAD\u8868\u793a\uff0c\u5e76\u5728\u591a\u79cd\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAD\u662f\u4e00\u79cd\u6709\u6548\u7684\u72b6\u6001\u8868\u793a\u5ea6\u91cf\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6837\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2506.09279", "pdf": "https://arxiv.org/pdf/2506.09279", "abs": "https://arxiv.org/abs/2506.09279", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u63ed\u793aHIV\u611f\u67d3\u8005\u9762\u4e34\u7684\u6c61\u540d\u5316\u3001\u793e\u4f1a\u548c\u884c\u4e3a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u95ee\u5377\u65b9\u6cd5\u5728\u8bc4\u4f30HIV\u76f8\u5173\u6c61\u540d\u5316\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\uff08LDA\uff09\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u7b5b\u9009\u7684\u5173\u952e\u8bcd\u548c\u96ea\u7403\u7b56\u7565\uff0c\u5206\u67909,140\u540dHIV\u611f\u67d3\u8005\u7684\u4e34\u5e8a\u8bb0\u5f55\u3002", "result": "\u8bc6\u522b\u51fa\u591a\u4e2a\u4e3b\u9898\uff0c\u5982\u5fc3\u7406\u5065\u5eb7\u4e0e\u6c61\u540d\u3001\u793e\u4f1a\u652f\u6301\u3001\u533b\u7597\u8d44\u6e90\u9650\u5236\u7b49\uff0c\u5e76\u5728\u4e0d\u540c\u5e74\u9f84\u548c\u6027\u522b\u4e9a\u7ec4\u4e2d\u53d1\u73b0\u5dee\u5f02\u3002", "conclusion": "\u901a\u8fc7\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5206\u6790\uff0c\u53ef\u9ad8\u6548\u8bc4\u4f30HIV\u76f8\u5173\u6c61\u540d\u5316\u95ee\u9898\uff0c\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u3002"}}
{"id": "2506.09286", "pdf": "https://arxiv.org/pdf/2506.09286", "abs": "https://arxiv.org/abs/2506.09286", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ME"], "comment": null, "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff08ASP\uff09\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5b66\u4e60\u56e0\u679c\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u56e0\u91c7\u6837\u9891\u7387\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u56e0\u91c7\u6837\u9891\u7387\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u56e0\u679c\u56fe\u7ed3\u6784\u5b66\u4e60\u56f0\u96be\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4fe1\u606f\u4e22\u5931\u5e26\u6765\u7684\u591a\u89e3\u6027\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff08ASP\uff09\u63a8\u5bfc\u56e0\u679c\u56fe\uff0c\u7ed3\u5408\u56fe\u7406\u8bba\u8fdb\u4e00\u6b65\u526a\u679d\u53ef\u80fd\u7684\u89e3\u96c6\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u5b9e\u9645\u8111\u7ed3\u6784\u8fde\u63a5\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0cF1\u5206\u6570\u5e73\u5747\u63d0\u534712%\uff0c\u4e14\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u91c7\u6837\u4e22\u5931\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.09316", "pdf": "https://arxiv.org/pdf/2506.09316", "abs": "https://arxiv.org/abs/2506.09316", "authors": ["Yeonju Ro", "Zhenyu Zhang", "Souvik Kundu", "Zhangyang Wang", "Aditya Akella"], "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose \\textit{dual-state linear attention} (\\textbf{\\dsla}), a novel\ndesign that maintains two specialized hidden states-one for preserving\nhistorical context and one for tracking recency-thereby mitigating the\nshort-range bias typical of linear-attention architectures. To further balance\nefficiency and accuracy under dynamic workload conditions, we introduce\n\\textbf{\\serve}, an online \\textit{adaptive distillation} framework that\nprogressively replaces Transformer layers with DSLA layers at inference time,\nguided by a sensitivity-based layer ordering. \\serve\\ uses a chained\nfine-tuning strategy to ensure that each newly converted DSLA layer remains\nconsistent with previously replaced layers, preserving the overall quality.\nExtensive evaluations on commonsense reasoning, long-context QA, and text\nsummarization demonstrate that \\serve\\ yields \\textbf{2.3x} faster inference\nthan Llama2-7B and \\textbf{3.0x} faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u72b6\u6001\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff08DSLA\uff09\u548c\u81ea\u9002\u5e94\u84b8\u998f\u6846\u67b6\uff08Serve\uff09\uff0c\u4ee5\u89e3\u51b3\u957f\u8f93\u5165\u4e0bLLMs\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\u9762\u4e34\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u800c\u73b0\u6709\u7684\u6b21\u4e8c\u6b21\u65b9\u6cd5\uff08\u5982\u7ebf\u6027\u6ce8\u610f\u529b\uff09\u56e0\u8fc7\u5ea6\u5f3a\u8c03\u8fd1\u671f\u6807\u8bb0\u800c\u964d\u4f4e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faDSLA\uff0c\u901a\u8fc7\u7ef4\u62a4\u4e24\u4e2a\u4e13\u7528\u9690\u85cf\u72b6\u6001\uff08\u5386\u53f2\u4e0a\u4e0b\u6587\u548c\u8fd1\u671f\u8ddf\u8e2a\uff09\u6765\u7f13\u89e3\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u77ed\u7a0b\u504f\u5dee\uff1b\u5e76\u5f15\u5165Serve\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u84b8\u998f\u5728\u63a8\u7406\u65f6\u9010\u6b65\u66ff\u6362Transformer\u5c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cServe\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4Llama2-7B\u5feb2.3\u500d\uff0c\u6bd4Zamba-7B\u5feb3.0\u500d\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "DSLA\u7684\u53cc\u72b6\u6001\u8bbe\u8ba1\u6709\u6548\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u5386\u53f2\u6807\u8bb0\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "InstructPro\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8bbe\u8ba1\u86cb\u767d\u8d28\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u529f\u80fd\u63cf\u8ff0\u548c\u914d\u4f53\u516c\u5f0f\u751f\u6210\u529f\u80fd\u4e00\u81f4\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u4f9d\u8d56\u7a00\u7f3a\u7684\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u6570\u636e\uff0c\u800c\u4eba\u7c7b\u6574\u7406\u7684\u6587\u672c\u63cf\u8ff0\u8d44\u6e90\u4e30\u5bcc\uff0c\u56e0\u6b64\u63d0\u51fa\u5229\u7528\u6587\u672c\u6307\u4ee4\u6307\u5bfc\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faInstructPro\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u62ec1B\u548c3B\u53c2\u6570\u7248\u672c\uff0c\u5e76\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6InstructProBench\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "InstructPro\u5728\u5bf9\u63a5\u6210\u529f\u7387\u548c\u7ed3\u6784\u504f\u5dee\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c3B\u7248\u672c\u5e73\u5747RMSD\u964d\u81f32.527\u00c5\u3002", "conclusion": "InstructPro\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8bbe\u8ba1\u529f\u80fd\u86cb\u767d\u8d28\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u914d\u4f53\u7ed3\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.09347", "pdf": "https://arxiv.org/pdf/2506.09347", "abs": "https://arxiv.org/abs/2506.09347", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faErrorEraser\uff0c\u4e00\u79cd\u901a\u8fc7\u8bc6\u522b\u548c\u6d88\u9664\u6570\u636e\u504f\u5dee\u5f15\u8d77\u7684\u9519\u8bef\u8bb0\u5fc6\u6765\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u7684\u901a\u7528\u63d2\u4ef6\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u9700\u8981\u9632\u6b62\u9057\u5fd8\u548c\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u6570\u636e\u504f\u5dee\u5bfc\u81f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u4e86CL\u7684\u6548\u679c\u3002", "method": "ErrorEraser\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u9519\u8bef\u8bc6\u522b\uff08\u901a\u8fc7\u65e0\u5148\u9a8c\u5b66\u4e60\u4efb\u52a1\u6570\u636e\u7684\u6982\u7387\u5bc6\u5ea6\u5206\u5e03\u8bc6\u522b\u504f\u5dee\u6837\u672c\uff09\u548c\u9519\u8bef\u64e6\u9664\uff08\u901a\u8fc7\u8c03\u6574\u51b3\u7b56\u7a7a\u95f4\u6d88\u9664\u9519\u8bef\u77e5\u8bc6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cErrorEraser\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u504f\u5dee\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5728\u4e09\u79cdCL\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u9057\u5fd8\u7387\u3002", "conclusion": "ErrorEraser\u901a\u8fc7\u6d88\u9664\u9519\u8bef\u8bb0\u5fc6\uff0c\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u4fdd\u7559\u548c\u8fc1\u79fb\u77e5\u8bc6\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09348", "pdf": "https://arxiv.org/pdf/2506.09348", "abs": "https://arxiv.org/abs/2506.09348", "authors": ["Natalie S. Frank"], "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "37 pages, 2 figures", "summary": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5bf9\u6297\u8bad\u7ec3\u4e2d\u66ff\u4ee3\u98ce\u9669\u4e0e\u5206\u7c7b\u98ce\u9669\u6536\u655b\u901f\u7387\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u66ff\u4ee3\u98ce\u9669\u754c\u9650\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u6297\u8bad\u7ec3\u4e2d\u66ff\u4ee3\u98ce\u9669\u6700\u5c0f\u5316\u5e8f\u5217\u662f\u5426\u80fd\u591f\u5feb\u901f\u6536\u655b\u5230\u6700\u4f18\u5206\u7c7b\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u66ff\u4ee3\u98ce\u9669\u754c\u9650\uff0c\u91cf\u5316\u6536\u655b\u901f\u7387\u3002", "result": "\u63d0\u51fa\u4e86\u66ff\u4ee3\u98ce\u9669\u754c\u9650\uff0c\u5e76\u6269\u5c55\u5230\u6807\u51c6\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5206\u5e03\u4f9d\u8d56\u6027\u754c\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u98ce\u9669\u6536\u655b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5bf9\u6807\u51c6\u5b66\u4e60\u573a\u666f\u6709\u72ec\u7acb\u610f\u4e49\u3002"}}
{"id": "2506.09368", "pdf": "https://arxiv.org/pdf/2506.09368", "abs": "https://arxiv.org/abs/2506.09368", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u751f\u6210\uff08ADGDM\uff09\uff0c\u63a2\u8ba8\u4e86\u5176\u7406\u8bba\u57fa\u7840\u3001\u5b9e\u9645\u5e94\u7528\u53ca\u534f\u540c\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u5728\u591a\u4e2a\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u5206\u5e03\u5b66\u4e60\u7684\u6311\u6218\u3002\u6269\u6563\u6a21\u578b\u56e0\u5176\u5f3a\u5927\u7684\u6570\u636e\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u5206\u7c7bADGDM\u65b9\u6cd5\uff08\u5982\u5f02\u5e38\u8bc4\u5206\u673a\u5236\u3001\u6761\u4ef6\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff09\uff0c\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e0e\u751f\u6210\u4e2d\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u751f\u6210\u5f02\u5e38\u6570\u636e\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u68c0\u6d4b\u65b9\u6cd5\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e24\u8005\u76f8\u4e92\u4fc3\u8fdb\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u521b\u65b0\u5e94\u7528\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u9ad8\u6548\u67b6\u6784\u548c\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6574\u5408\u3002"}}
{"id": "2506.09373", "pdf": "https://arxiv.org/pdf/2506.09373", "abs": "https://arxiv.org/abs/2506.09373", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4f4d\u7f6e\u6570\u636e\u548c\u4fe1\u606f\u71b5\u4f18\u5316GUI\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u7a7a\u95f4\u5b9a\u4f4d\u4e0a\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f46\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u4f4d\u7f6e\u51c6\u786e\u6027\u3002", "method": "LPO\u5229\u7528\u4fe1\u606f\u71b5\u9884\u6d4b\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\u7684\u4ea4\u4e92\u4f4d\u7f6e\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u8ddd\u79bb\u7684\u52a8\u6001\u4f4d\u7f6e\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408GRPO\u652f\u6301\u5e7f\u6cdb\u63a2\u7d22GUI\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLPO\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "LPO\u901a\u8fc7\u4f18\u5316\u4f4d\u7f6e\u504f\u597d\u663e\u8457\u63d0\u5347\u4e86GUI\u4ea4\u4e92\u7684\u7cbe\u786e\u6027\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.09376", "pdf": "https://arxiv.org/pdf/2506.09376", "abs": "https://arxiv.org/abs/2506.09376", "authors": ["Bowen Zheng", "Tianming Yang"], "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Diffusion distillation is a widely used technique to reduce the sampling cost\nof diffusion models, yet it often requires extensive training, and the student\nperformance tends to be degraded. Recent studies show that incorporating a GAN\nobjective may alleviate these issues, yet the underlying mechanism remains\nunclear. In this work, we first identify a key limitation of distillation:\nmismatched step sizes and parameter numbers between the teacher and the student\nmodel lead them to converge to different local minima, rendering direct\nimitation suboptimal. We further demonstrate that a standalone GAN objective,\nwithout relying a distillation loss, overcomes this limitation and is\nsufficient to convert diffusion models into efficient one-step generators.\nBased on this finding, we propose that diffusion training may be viewed as a\nform of generative pre-training, equipping models with capabilities that can be\nunlocked through lightweight GAN fine-tuning. Supporting this view, we create a\none-step generation model by fine-tuning a pre-trained model with 85% of\nparameters frozen, achieving strong performance with only 0.2M images and\nnear-SOTA results with 5M images. We further present a frequency-domain\nanalysis that may explain the one-step generative capability gained in\ndiffusion training. Overall, our work provides a new perspective for diffusion\ntraining, highlighting its role as a powerful generative pre-training process,\nwhich can be the basis for building efficient one-step generation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6269\u6563\u84b8\u998f\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0GAN\u76ee\u6807\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u6269\u6563\u8bad\u7ec3\u53ef\u4f5c\u4e3a\u751f\u6210\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7GAN\u5fae\u8c03\u5b9e\u73b0\u9ad8\u6548\u4e00\u6b65\u751f\u6210\u3002", "motivation": "\u6269\u6563\u84b8\u998f\u6280\u672f\u867d\u5e7f\u6cdb\u7528\u4e8e\u964d\u4f4e\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6210\u672c\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u957f\u548c\u5b66\u751f\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22GAN\u76ee\u6807\u5982\u4f55\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u84b8\u998f\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5355\u72ec\u4f7f\u7528GAN\u76ee\u6807\u53ef\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5e76\u57fa\u4e8e\u6269\u6563\u8bad\u7ec3\u7684\u751f\u6210\u9884\u8bad\u7ec3\u80fd\u529b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7GAN\u5fae\u8c03\u5b9e\u73b0\u4e00\u6b65\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u97000.2M\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c5M\u56fe\u50cf\u65f6\u63a5\u8fd1SOTA\u7ed3\u679c\u3002\u9891\u7387\u57df\u5206\u6790\u89e3\u91ca\u4e86\u6269\u6563\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u4e00\u6b65\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u6269\u6563\u8bad\u7ec3\u53ef\u4f5c\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u9884\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u4e00\u6b65\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.09398", "pdf": "https://arxiv.org/pdf/2506.09398", "abs": "https://arxiv.org/abs/2506.09398", "authors": ["Haiyang Yu", "Yuchao Lin", "Xuan Zhang", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames", "categories": ["cs.LG", "physics.comp-ph"], "comment": "Code available at: https://github.com/divelab/AIRS", "summary": "We consider the task of predicting Hamiltonian matrices to accelerate\nelectronic structure calculations, which plays an important role in physics,\nchemistry, and materials science. Motivated by the inherent relationship\nbetween the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local\nframe, we propose a novel and efficient network, called QHNetV2, that achieves\nglobal SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor\nproducts. This is achieved by introducing a set of new efficient and powerful\nSO(2)-equivariant operations and performing all off-diagonal feature updates\nand message passing within SO(2) local frames, thereby eliminating the need of\nSO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed\nwithin the SO(2) local frame at each node to fuse node features, mimicking the\nsymmetric contraction operation. Extensive experiments on the large QH9 and\nMD17 datasets demonstrate that our model achieves superior performance across a\nwide range of molecular structures and trajectories, highlighting its strong\ngeneralization capability. The proposed SO(2) operations on SO(2) local frames\noffer a promising direction for scalable and symmetry-aware learning of\nelectronic structures. Our code will be released as part of the AIRS library\nhttps://github.com/divelab/AIRS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQHNetV2\u7684\u9ad8\u6548\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u54c8\u5bc6\u987f\u77e9\u9635\u4ee5\u52a0\u901f\u7535\u5b50\u7ed3\u6784\u8ba1\u7b97\u3002\u901a\u8fc7\u5229\u7528SO(2)\u5c40\u90e8\u6846\u67b6\u7684\u5173\u7cfb\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684SO(3)\u5f20\u91cf\u79ef\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40SO(3)\u7b49\u53d8\u6027\u3002", "motivation": "\u7535\u5b50\u7ed3\u6784\u8ba1\u7b97\u5728\u7269\u7406\u3001\u5316\u5b66\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u7f51\u7edc\u9884\u6d4b\u54c8\u5bc6\u987f\u77e9\u9635\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faQHNetV2\u7f51\u7edc\uff0c\u5f15\u5165\u9ad8\u6548\u7684SO(2)\u7b49\u53d8\u64cd\u4f5c\uff0c\u5728SO(2)\u5c40\u90e8\u6846\u67b6\u5185\u5b8c\u6210\u7279\u5f81\u66f4\u65b0\u548c\u4fe1\u606f\u4f20\u9012\uff0c\u907f\u514d\u4e86SO(3)\u5f20\u91cf\u79ef\u3002", "result": "\u5728QH9\u548cMD17\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u591a\u79cd\u5206\u5b50\u7ed3\u6784\u548c\u8f68\u8ff9\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SO(2)\u5c40\u90e8\u6846\u67b6\u7684\u64cd\u4f5c\u4e3a\u7535\u5b50\u7ed3\u6784\u7684\u53ef\u6269\u5c55\u548c\u5bf9\u79f0\u611f\u77e5\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09404", "pdf": "https://arxiv.org/pdf/2506.09404", "abs": "https://arxiv.org/abs/2506.09404", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u7684\u8fdb\u5316\u589e\u5f3a\u673a\u5236\uff08EAM\uff09\uff0c\u901a\u8fc7\u751f\u6210\u548c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3DRL\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u63a2\u7d22\u80fd\u529b\u4e0d\u8db3\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865GA\u6837\u672c\u6548\u7387\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u7f3a\u9677\u3002", "method": "EAM\u6846\u67b6\u901a\u8fc7DRL\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528GA\u7684\u9057\u4f20\u64cd\u4f5c\uff08\u5982\u4ea4\u53c9\u548c\u53d8\u5f02\uff09\u4f18\u5316\u8fd9\u4e9b\u65b9\u6848\uff0c\u5e76\u5c06\u4f18\u5316\u540e\u7684\u65b9\u6848\u91cd\u65b0\u6ce8\u5165\u7b56\u7565\u8bad\u7ec3\u5faa\u73af\u3002", "result": "\u5728TSP\u3001CVRP\u7b49\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cEAM\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "EAM\u662f\u4e00\u79cd\u901a\u7528\u4e14\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7ed3\u5408DRL\u548cGA\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u6548\u679c\u3002"}}
{"id": "2506.09433", "pdf": "https://arxiv.org/pdf/2506.09433", "abs": "https://arxiv.org/abs/2506.09433", "authors": ["Shurui Gui", "Shuiwang Ji"], "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training", "categories": ["cs.LG"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.", "AI": {"tldr": "CAPT\u901a\u8fc7\u56e0\u679c\u611f\u77e5\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u51cf\u5c11LLMs\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u56e0\u9884\u8bad\u7ec3\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5c06\u504f\u5dee\u9884\u6d4b\u5206\u89e3\u4e3a\u4e8b\u4ef6\u4f30\u8ba1\u548c\u4e8b\u4ef6\u5e72\u9884\u4e24\u4e2a\u65e0\u504f\u6b65\u9aa4\uff0c\u907f\u514d\u989d\u5916\u5fae\u8c03\u504f\u5dee\u3002", "result": "\u5728CLadder\u548cPrOntoQA\u6570\u636e\u96c6\u4e0a\uff0c3B\u89c4\u6a21\u7684CAPT\u6a21\u578b\u4ec5\u7528100\u4e2a\u6837\u672c\u5373\u4f18\u4e8e\u4f20\u7edfSFT\u548c\u66f4\u5927LLMs\u3002", "conclusion": "CAPT\u65b9\u6cd5\u6709\u6548\u4e14\u6837\u672c\u9ad8\u6548\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u5206\u5e03\u5185\u5916\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2506.09451", "pdf": "https://arxiv.org/pdf/2506.09451", "abs": "https://arxiv.org/abs/2506.09451", "authors": ["Runxue Bao", "Quanchao Lu", "Yanfu Zhang"], "title": "Safe Screening Rules for Group SLOPE", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by ECML PKDD 2025", "summary": "Variable selection is a challenging problem in high-dimensional sparse\nlearning, especially when group structures exist. Group SLOPE performs well for\nthe adaptive selection of groups of predictors. However, the block\nnon-separable group effects in Group SLOPE make existing methods either invalid\nor inefficient. Consequently, Group SLOPE tends to incur significant\ncomputational costs and memory usage in practical high-dimensional scenarios.\nTo overcome this issue, we introduce a safe screening rule tailored for the\nGroup SLOPE model, which efficiently identifies inactive groups with zero\ncoefficients by addressing the block non-separable group effects. By excluding\nthese inactive groups during training, we achieve considerable gains in\ncomputational efficiency and memory usage. Importantly, the proposed screening\nrule can be seamlessly integrated into existing solvers for both batch and\nstochastic algorithms. Theoretically, we establish that our screening rule can\nbe safely employed with existing optimization algorithms, ensuring the same\nresults as the original approaches. Experimental results confirm that our\nmethod effectively detects inactive feature groups and significantly boosts\ncomputational efficiency without compromising accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Group SLOPE\u6a21\u578b\u7684\u5b89\u5168\u7b5b\u9009\u89c4\u5219\uff0c\u901a\u8fc7\u8bc6\u522b\u96f6\u7cfb\u6570\u7684\u4e0d\u6d3b\u8dc3\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "Group SLOPE\u5728\u81ea\u9002\u5e94\u9009\u62e9\u9884\u6d4b\u53d8\u91cf\u7ec4\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5757\u4e0d\u53ef\u5206\u7ec4\u6548\u5e94\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u65e0\u6548\u6216\u4f4e\u6548\uff0c\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u9ad8\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5b89\u5168\u7b5b\u9009\u89c4\u5219\uff0c\u4e13\u95e8\u89e3\u51b3\u5757\u4e0d\u53ef\u5206\u7ec4\u6548\u5e94\uff0c\u8bc6\u522b\u4e0d\u6d3b\u8dc3\u7ec4\u5e76\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u4e0d\u6d3b\u8dc3\u7279\u5f81\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b5b\u9009\u89c4\u5219\u53ef\u5b89\u5168\u96c6\u6210\u5230\u73b0\u6709\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u786e\u4fdd\u7ed3\u679c\u4e0e\u539f\u59cb\u65b9\u6cd5\u4e00\u81f4\uff0c\u9002\u7528\u4e8e\u6279\u91cf\u548c\u968f\u673a\u7b97\u6cd5\u3002"}}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStained Glass Transform\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u53d8\u6362LLM\u7684\u8bcd\u5d4c\u5165\uff0c\u5728\u4fdd\u62a4\u8f93\u5165\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6210\u672c\u548cLLM\u90e8\u7f72\u7684\u6311\u6218\u5bfc\u81f4\u4f01\u4e1a\u5bf9\u6570\u636e\u9690\u79c1\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u5171\u4eab\u6216\u591a\u79df\u6237\u73af\u5883\u4e2d\u3002", "method": "\u5f15\u5165Stained Glass Transform\uff0c\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e92\u4fe1\u606f\u7406\u8bba\u7684\u5b66\u4e60\u968f\u673a\u53d8\u6362\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86LLM\u7684\u6027\u80fd\u3002", "conclusion": "Stained Glass Transform\u4e3a\u5171\u4eab\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09454", "pdf": "https://arxiv.org/pdf/2506.09454", "abs": "https://arxiv.org/abs/2506.09454", "authors": ["Yuanhao Pu", "Defu Lian", "Xiaolong Chen", "Xu Huang", "Jin Chen", "Enhong Chen"], "title": "NDCG-Consistent Softmax Approximation with Accelerated Convergence", "categories": ["cs.LG"], "comment": "35 pages", "summary": "Ranking tasks constitute fundamental components of extreme similarity\nlearning frameworks, where extremely large corpora of objects are modeled\nthrough relative similarity relationships adhering to predefined ordinal\nstructures. Among various ranking surrogates, Softmax (SM) Loss has been widely\nadopted due to its natural capability to handle listwise ranking via global\nnegative comparisons, along with its flexibility across diverse application\nscenarios. However, despite its effectiveness, SM Loss often suffers from\nsignificant computational overhead and scalability limitations when applied to\nlarge-scale object spaces. To address this challenge, we propose novel loss\nformulations that align directly with ranking metrics: the\nRanking-Generalizable \\textbf{squared} (RG$^2$) Loss and the\nRanking-Generalizable interactive (RG$^\\times$) Loss, both derived through\nTaylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic\nmechanisms underlying weighted squared losses (WSL) in ranking methods and\nuncovers fundamental connections between sampling-based and non-sampling-based\nloss paradigms. Furthermore, we integrate the proposed RG losses with the\nhighly efficient Alternating Least Squares (ALS) optimization method, providing\nboth generalization guarantees and convergence rate analyses. Empirical\nevaluations on real-world datasets demonstrate that our approach achieves\ncomparable or superior ranking performance relative to SM Loss, while\nsignificantly accelerating convergence. This framework offers the similarity\nlearning community both theoretical insights and practically efficient tools,\nwith methodologies applicable to a broad range of tasks where balancing ranking\nquality and computational efficiency is essential.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08RG\u00b2\u548cRG\u00d7\uff09\uff0c\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00Softmax\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5bf9\u8c61\u7a7a\u95f4\u4e2d\u7684\u8ba1\u7b97\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u7ed3\u5408ALS\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6392\u540d\u6027\u80fd\u3002", "motivation": "Softmax\u635f\u5931\u5728\u5927\u89c4\u6a21\u5bf9\u8c61\u7a7a\u95f4\u4e2d\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u548c\u6269\u5c55\u6027\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00Softmax\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51faRG\u00b2\u548cRG\u00d7\u635f\u5931\u51fd\u6570\uff0c\u5e76\u7ed3\u5408ALS\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0eSoftmax\u635f\u5931\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6392\u540d\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u76f8\u4f3c\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u548c\u9ad8\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5e73\u8861\u6392\u540d\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u7684\u4efb\u52a1\u3002"}}
{"id": "2506.09477", "pdf": "https://arxiv.org/pdf/2506.09477", "abs": "https://arxiv.org/abs/2506.09477", "authors": ["Yunhao Tang", "R\u00e9mi Munos"], "title": "On a few pitfalls in KL divergence gradient estimation for RL", "categories": ["cs.LG"], "comment": null, "summary": "We point out a few pitfalls in implementing gradient estimation for KL\ndivergence in RL training for LLM, as seen in a number of open source projects\nand papers. The first major pitfall is to differentiate through the KL estimate\nas loss functions to minimize KL divergence. We show that such implementations\nare generally incorrect and do not produce the desired KL gradient. Secondly,\nwe show that some implementations do not account for the sequential nature of\nthe estimation problem and produce a partial gradient at best. We demonstrate\nthe impact of such issues with illustrative tabular and LLM experiments, and\nshow the correct way to implement the KL gradient.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u4e86\u5728RL\u8bad\u7ec3\u4e2dKL\u6563\u5ea6\u68af\u5ea6\u4f30\u8ba1\u7684\u51e0\u4e2a\u5e38\u89c1\u9677\u9631\uff0c\u5305\u62ec\u9519\u8bef\u5730\u5c06KL\u4f30\u8ba1\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u5ffd\u7565\u95ee\u9898\u7684\u5e8f\u5217\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u6b63\u786e\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u63ed\u793aKL\u6563\u5ea6\u68af\u5ea6\u4f30\u8ba1\u5728RL\u8bad\u7ec3\u4e2d\u7684\u5e38\u89c1\u9519\u8bef\u5b9e\u73b0\uff0c\u5e76\u63d0\u51fa\u6b63\u786e\u65b9\u6cd5\u4ee5\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8868\u683c\u548cLLM\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u9519\u8bef\u548c\u6b63\u786e\u7684KL\u68af\u5ea6\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u9519\u8bef\u5b9e\u73b0\u65e0\u6cd5\u4ea7\u751f\u671f\u671b\u7684KL\u68af\u5ea6\uff0c\u800c\u6b63\u786e\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86KL\u68af\u5ea6\u4f30\u8ba1\u7684\u6b63\u786e\u5b9e\u73b0\u6307\u5357\uff0c\u5f3a\u8c03\u4e86\u907f\u514d\u5e38\u89c1\u9677\u9631\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09496", "pdf": "https://arxiv.org/pdf/2506.09496", "abs": "https://arxiv.org/abs/2506.09496", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09499", "pdf": "https://arxiv.org/pdf/2506.09499", "abs": "https://arxiv.org/abs/2506.09499", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "categories": ["cs.LG", "cs.AI"], "comment": "12 Pages", "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "AI": {"tldr": "OKBEs\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u5956\u52b1MDP\u65b9\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001-\u65f6\u95f4\u9009\u9879\u6838\uff08STOK\uff09\u76f4\u63a5\u4f18\u5316\u76ee\u6807\u5b8c\u6210\u6982\u7387\uff0c\u5177\u6709\u7ec4\u5408\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u6700\u5927\u5316\u4e0e\u7ec4\u5408\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u652f\u6301\u53ef\u9a8c\u8bc1\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u5185\u5728\u52a8\u673a\u3002", "method": "\u4f7f\u7528STOK\u4f5c\u4e3a\u9884\u6d4b\u6620\u5c04\uff0c\u901a\u8fc7Chapman-Kolmogorov\u65b9\u7a0b\u7ec4\u5408\uff0c\u9ad8\u6548\u8868\u793a\u548c\u8ba1\u7b97\u9ad8\u7ef4STOK\uff0c\u5e76\u8bb0\u5f55\u8bed\u4e49\u53ef\u89e3\u91ca\u7684\u4e8b\u4ef6\u6982\u7387\u3002", "result": "\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u7b56\u7565\u5408\u6210\u3001\u8de8\u4efb\u52a1\u89c4\u5212\u8868\u793a\u91cd\u7528\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5185\u5728\u52a8\u673a\u7684\u76ee\u6807\u9a8c\u8bc1\u3002", "conclusion": "OKBEs\u4e3a\u9ad8\u7ef4\u52a8\u6001\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u53ef\u9a8c\u8bc1\u89c4\u5212\u548c\u5185\u5728\u52a8\u673a\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2506.09508", "pdf": "https://arxiv.org/pdf/2506.09508", "abs": "https://arxiv.org/abs/2506.09508", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": null, "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u63a2\u7d22\u7684\u5143\u7b97\u6cd5\uff0c\u89e3\u51b3\u504f\u597d\u67e5\u8be2\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u6539\u8fdb\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u4ece\u8f68\u8ff9\u7ea7\u504f\u597d\u6bd4\u8f83\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u6311\u6218\uff0c\u540c\u65f6\u786e\u4fdd\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u63a2\u7d22\u7684\u5143\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e3a\u6279\u91cf\u6536\u96c6\u8f68\u8ff9\u5bf9\u548c\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u4ee5\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u67e5\u8be2\u3002", "result": "\u5728\u6e29\u548c\u7684\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u9057\u61be\u548c\u6700\u7ec8\u8fed\u4ee3\u4fdd\u8bc1\uff0c\u5b9e\u8bc1\u8868\u660e\u65b9\u6cd5\u5728\u5c11\u91cf\u504f\u597d\u67e5\u8be2\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u67e5\u8be2\u590d\u6742\u5ea6\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5e76\u884c\u53cd\u9988\u6536\u96c6\u3002"}}
{"id": "2506.09526", "pdf": "https://arxiv.org/pdf/2506.09526", "abs": "https://arxiv.org/abs/2506.09526", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "title": "Neural Functions for Learning Periodic Signal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u5468\u671f\u6027\u6a21\u5f0f\u63d0\u5347\u4fe1\u53f7\u8868\u793a\u7684\u5916\u63a8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5750\u6807\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5728\u8bad\u7ec3\u533a\u57df\u5916\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4fe1\u53f7\u5177\u6709\u5468\u671f\u6027\u65f6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7f51\u7edc\u67b6\u6784\uff0c\u4ece\u6d4b\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8868\u793a\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5fae\u5206\u65b9\u7a0b\u5468\u671f\u89e3\u5b66\u4e60\u3001\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u548c\u5916\u63a8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5468\u671f\u6027\u4fe1\u53f7\u7684\u5916\u63a8\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "Athena-PRM\u662f\u4e00\u79cd\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u63a8\u7406\u95ee\u9898\u4e2d\u6bcf\u4e00\u6b65\u7684\u5956\u52b1\u5206\u6570\u3002\u901a\u8fc7\u5229\u7528\u5f31\u548c\u5f3a\u5b8c\u6210\u8005\u4e4b\u95f4\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\u6570\u636e\uff0c\u4ec5\u97005000\u6837\u672c\u5373\u53ef\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff08\u5982\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\uff09\u751f\u6210\u566a\u58f0\u6807\u7b7e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u8fc7\u7a0b\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5f31\u548c\u5f3a\u5b8c\u6210\u8005\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u4f5c\u4e3a\u53ef\u9760\u8fc7\u7a0b\u6807\u7b7e\u7684\u751f\u6210\u6807\u51c6\uff0c\u5e76\u91c7\u7528ORM\u521d\u59cb\u5316\u548c\u8d1f\u6570\u636e\u4e0a\u91c7\u6837\u7b56\u7565\u63d0\u5347PRM\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u573a\u666f\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982WeMath\u548cMathVista\u5206\u522b\u63d0\u534710.2\u548c7.1\u5206\uff0c\u5e76\u5728VisualProcessBench\u4e2d\u8d85\u8d8a\u4e4b\u524dSoTA 3.9 F1\u5206\u6570\u3002", "conclusion": "Athena-PRM\u80fd\u9ad8\u6548\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.09544", "pdf": "https://arxiv.org/pdf/2506.09544", "abs": "https://arxiv.org/abs/2506.09544", "authors": ["Yang Yang", "Du Yin", "Hao Xue", "Flora Salim"], "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network", "categories": ["cs.LG"], "comment": null, "summary": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.", "AI": {"tldr": "STOAT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4-\u65f6\u95f4\u6982\u7387\u56e0\u679c\u63a8\u65ad\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u95f4-\u65f6\u95f4\u56e0\u679c\u65f6\u95f4\u5e8f\u5217\u7684\u6982\u7387\u9884\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u77e9\u9635\u548c\u6df1\u5ea6\u6982\u7387\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5efa\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5ffd\u7565\u4e86\u56e0\u679c\u9a71\u52a8\u7684\u6982\u7387\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u80fd\u529b\u3002", "method": "STOAT\u6269\u5c55\u4e86\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u5f15\u5165\u7a7a\u95f4\u5173\u7cfb\u77e9\u9635\u7f16\u7801\u533a\u57df\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u6982\u7387\u6a21\u578b\u4f30\u8ba1\u5206\u5e03\u53c2\u6570\u3002", "result": "\u5728COVID-19\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTOAT\u5728\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u4f9d\u8d56\u5f3a\u7684\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "STOAT\u4e3a\u590d\u6742\u7a7a\u95f4-\u65f6\u95f4\u4efb\u52a1\uff08\u5982\u75ab\u60c5\u7ba1\u7406\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u56e0\u679c\u63a8\u65ad\u548c\u5730\u7406\u7a7a\u95f4\u6982\u7387\u9884\u6d4b\u3002"}}
{"id": "2506.09574", "pdf": "https://arxiv.org/pdf/2506.09574", "abs": "https://arxiv.org/abs/2506.09574", "authors": ["Gaurav Chaudhary", "Wassim Uddin Mondal", "Laxmidhar Behera"], "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Sample efficiency and exploration remain critical challenges in Deep\nReinforcement Learning (DRL), particularly in complex domains. Offline RL,\nwhich enables agents to learn optimal policies from static, pre-collected\ndatasets, has emerged as a promising alternative. However, offline RL is\nconstrained by issues such as out-of-distribution (OOD) actions that limit\npolicy performance and generalization. To overcome these limitations, we\npropose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework\nthat unifies offline and online RL for efficient and scalable learning. While\nprevious hybrid methods rely on extensive design components and added\ncomputational complexity to utilize offline data effectively, MOORL introduces\na meta-policy that seamlessly adapts across offline and online trajectories.\nThis enables the agent to leverage offline data for robust initialization while\nutilizing online interactions to drive efficient exploration. Our theoretical\nanalysis demonstrates that the hybrid approach enhances exploration by\neffectively combining the complementary strengths of offline and online data.\nFurthermore, we demonstrate that MOORL learns a stable Q-function without added\ncomplexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL\nbenchmarks validate its effectiveness, showing consistent improvements over\nstate-of-the-art offline and hybrid RL baselines. With minimal computational\noverhead, MOORL achieves strong performance, underscoring its potential for\npractical applications in real-world scenarios.", "AI": {"tldr": "MOORL\u662f\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u4e0e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u65e0\u7f1d\u9002\u5e94\u4e24\u79cd\u6570\u636e\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2dOOD\u52a8\u4f5c\u9650\u5236\u6027\u80fd\u4e0e\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u6df7\u5408\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51faMOORL\u6846\u67b6\uff0c\u5229\u7528\u5143\u7b56\u7565\u7ed3\u5408\u79bb\u7ebf\u4e0e\u5728\u7ebf\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u4e0e\u7a33\u5b9aQ\u51fd\u6570\u5b66\u4e60\u3002", "result": "\u572828\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cMOORL\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebf\u4e0e\u6df7\u5408RL\u57fa\u7ebf\uff0c\u6027\u80fd\u7a33\u5b9a\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "MOORL\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u9886\u57df\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09593", "pdf": "https://arxiv.org/pdf/2506.09593", "abs": "https://arxiv.org/abs/2506.09593", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5982ConvNeXt\u3001EVA\u548cBEiT\uff09\u7684\u6821\u51c6\u884c\u4e3a\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5206\u5e03\u5185\u9884\u6d4b\u4e2d\u503e\u5411\u4e8e\u4e0d\u81ea\u4fe1\uff0c\u5bfc\u81f4\u6821\u51c6\u8bef\u5dee\u8f83\u9ad8\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u66f4\u597d\u3002\u540e\u5904\u7406\u6821\u51c6\u6280\u672f\u5bf9\u5206\u5e03\u5185\u6570\u636e\u6709\u6548\uff0c\u4f46\u5728\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u4e0b\u6548\u679c\u4e0b\u964d\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u6821\u51c6\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u7684\u6821\u51c6\u7279\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u57fa\u7840\u6a21\u578b\u7684\u6821\u51c6\u884c\u4e3a\uff0c\u5e76\u6d4b\u8bd5\u540e\u5904\u7406\u6821\u51c6\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u5206\u5e03\u5185\u9884\u6d4b\u4e2d\u4e0d\u81ea\u4fe1\uff0c\u6821\u51c6\u8bef\u5dee\u9ad8\uff1b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6821\u51c6\u8868\u73b0\u66f4\u597d\u3002\u540e\u5904\u7406\u6821\u51c6\u5728\u5206\u5e03\u5185\u6709\u6548\uff0c\u4f46\u5728\u4e25\u91cd\u504f\u79fb\u4e0b\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7684\u6821\u51c6\u884c\u4e3a\u590d\u6742\uff0c\u6311\u6218\u4e86\u6301\u7eed\u6539\u8fdb\u7684\u4f20\u7edf\u89c2\u70b9\u3002"}}
{"id": "2506.09594", "pdf": "https://arxiv.org/pdf/2506.09594", "abs": "https://arxiv.org/abs/2506.09594", "authors": ["Wenjin Qin", "Hailin Wang", "Jingyao Hou", "Jianjun Wang"], "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery", "categories": ["cs.LG"], "comment": null, "summary": "Currently, existing tensor recovery methods fail to recognize the impact of\ntensor scale variations on their structural characteristics. Furthermore,\nexisting studies face prohibitive computational costs when dealing with\nlarge-scale high-order tensor data. To alleviate these issue, assisted by the\nKrylov subspace iteration, block Lanczos bidiagonalization process, and random\nprojection strategies, this article first devises two fast and accurate\nrandomized algorithms for low-rank tensor approximation (LRTA) problem.\nTheoretical bounds on the accuracy of the approximation error estimate are\nestablished. Next, we develop a novel generalized nonconvex modeling framework\ntailored to large-scale tensor recovery, in which a new regularization paradigm\nis exploited to achieve insightful prior representation for large-scale\ntensors. On the basis of the above, we further investigate new unified\nnonconvex models and efficient optimization algorithms, respectively, for\nseveral typical high-order tensor recovery tasks in unquantized and quantized\nsituations. To render the proposed algorithms practical and efficient for\nlarge-scale tensor data, the proposed randomized LRTA schemes are integrated\ninto their central and time-intensive computations. Finally, we conduct\nextensive experiments on various large-scale tensors, whose results demonstrate\nthe practicability, effectiveness and superiority of the proposed method in\ncomparison with some state-of-the-art approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u5feb\u901f\u968f\u673a\u7b97\u6cd5\u89e3\u51b3\u4f4e\u79e9\u5f20\u91cf\u903c\u8fd1\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u51f8\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5f20\u91cf\u6062\u590d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u6062\u590d\u65b9\u6cd5\u672a\u8003\u8651\u5f20\u91cf\u5c3a\u5ea6\u53d8\u5316\u5bf9\u7ed3\u6784\u7279\u6027\u7684\u5f71\u54cd\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u7ed3\u5408Krylov\u5b50\u7a7a\u95f4\u8fed\u4ee3\u3001\u5757Lanczos\u53cc\u5bf9\u89d2\u5316\u8fc7\u7a0b\u548c\u968f\u673a\u6295\u5f71\u7b56\u7565\uff0c\u63d0\u51fa\u968f\u673a\u5316\u7b97\u6cd5\u548c\u975e\u51f8\u5efa\u6a21\u6846\u67b6\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u8fd1\u4f3c\u8bef\u5dee\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5f20\u91cf\u6570\u636e\u4e0a\u5b9e\u7528\u4e14\u9ad8\u6548\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6062\u590d\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.09613", "pdf": "https://arxiv.org/pdf/2506.09613", "abs": "https://arxiv.org/abs/2506.09613", "authors": ["Kaiwen Tuo", "Huan Wang"], "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot", "categories": ["cs.LG"], "comment": null, "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.", "AI": {"tldr": "SparseSSM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u6846\u67b6\uff0c\u9488\u5bf9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8fd1\u4f3c\u4e8c\u9636\u663e\u8457\u6027\u8bc4\u5206\u548c\u7ec4\u4ef6\u654f\u611f\u6027\u5206\u6790\uff0c\u5b9e\u73b0\u9ad8\u6548\u526a\u679d\uff0c50%\u6743\u91cd\u526a\u679d\u540e\u96f6\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u9488\u5bf9\u6ce8\u610f\u529b\u5757\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5904\u7406\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u7684\u65f6\u95f4\u5171\u4eab\u548c\u79bb\u6563\u5316\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u72b6\u6001\u7a7a\u95f4\u67b6\u6784\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSparseSSM\u6846\u67b6\uff0c\u7ed3\u5408\u8fd1\u4f3c\u4e8c\u9636\u663e\u8457\u6027\u8bc4\u5206\uff08Hessian-trace\u4fe1\u606f\uff09\u548c\u7ec4\u4ef6\u654f\u611f\u6027\u5206\u6790\uff0c\u652f\u6301\u534a\u7ed3\u6784\u5316\u548c\u7ed3\u6784\u5316\u7a00\u758f\u5316\u3002", "result": "\u5728Mamba\u6a21\u578b\u4e2d\u526a\u679d50%\u6743\u91cd\u540e\uff0c\u96f6\u7cbe\u5ea6\u635f\u5931\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u5148\u8fdb\u7684\u526a\u679d\u6548\u679c\u3002", "conclusion": "SparseSSM\u662f\u9996\u4e2a\u9488\u5bf9\u72b6\u6001\u7a7a\u95f4\u67b6\u6784\u7684\u65e0\u8bad\u7ec3\u526a\u679d\u6846\u67b6\uff0c\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u4e3aMamba\u7c7b\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09625", "pdf": "https://arxiv.org/pdf/2506.09625", "abs": "https://arxiv.org/abs/2506.09625", "authors": ["Ekaterina Filimoshina", "Dmitry Shirokov"], "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "categories": ["cs.LG", "68T07, 15A66"], "comment": "Accepted to ICML 2025", "summary": "We propose, implement, and compare with competitors a new architecture of\nequivariant neural networks based on geometric (Clifford) algebras: Generalized\nLipschitz Group Equivariant Neural Networks (GLGENN). These networks are\nequivariant to all pseudo-orthogonal transformations, including rotations and\nreflections, of a vector space with any non-degenerate or degenerate symmetric\nbilinear form. We propose a weight-sharing parametrization technique that takes\ninto account the fundamental structures and operations of geometric algebras.\nDue to this technique, GLGENN architecture is parameter-light and has less\ntendency to overfitting than baseline equivariant models. GLGENN outperforms or\nmatches competitors on several benchmarking equivariant tasks, including\nestimation of an equivariant function and a convex hull experiment, while using\nsignificantly fewer optimizable parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\uff08Clifford\uff09\u4ee3\u6570\u7684\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784GLGENN\uff0c\u5176\u53c2\u6570\u8f7b\u91cf\u5316\u4e14\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u4f2a\u6b63\u4ea4\u53d8\u6362\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\u503e\u5411\u3002", "method": "\u5229\u7528\u51e0\u4f55\u4ee3\u6570\u7684\u57fa\u672c\u7ed3\u6784\u548c\u64cd\u4f5c\uff0c\u63d0\u51fa\u6743\u91cd\u5171\u4eab\u53c2\u6570\u5316\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "GLGENN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53c2\u6570\u8f7b\u91cf\u5316\u7684\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002"}}
{"id": "2506.09630", "pdf": "https://arxiv.org/pdf/2506.09630", "abs": "https://arxiv.org/abs/2506.09630", "authors": ["Pol G. Recasens", "Alberto Gutierrez", "Jordi Torres", "Josep. Ll Berral", "Anisa Halimi", "Kieran Fraser"], "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "categories": ["cs.LG"], "comment": "Paper accepted at ICML 2025 workshop DIG-BUG", "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.", "AI": {"tldr": "LLMs\u7528\u4e8e\u751f\u6210\u5408\u6210\u8868\u683c\u6570\u636e\u65f6\uff0c\u53ef\u80fd\u56e0\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u7684\u7edf\u8ba1\u504f\u5dee\u5bfc\u81f4\u5168\u5c40\u6570\u636e\u5931\u771f\uff0c\u751a\u81f3\u88ab\u6076\u610f\u5229\u7528\u6ce8\u5165\u504f\u89c1\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u751f\u6210\u5408\u6210\u8868\u683c\u6570\u636e\u65f6\uff0c\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u7684\u7edf\u8ba1\u504f\u5dee\u5982\u4f55\u4f20\u64ad\u5e76\u5f71\u54cd\u6570\u636e\u5206\u5e03\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u5b89\u5168\u9690\u60a3\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e2d\u7684\u7edf\u8ba1\u504f\u5dee\u5bf9\u5408\u6210\u6570\u636e\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u5e76\u6a21\u62df\u6076\u610f\u6ce8\u5165\u504f\u89c1\u7684\u573a\u666f\u3002", "result": "\u53d1\u73b0\u5373\u4f7f\u8f7b\u5fae\u7684\u4e0a\u4e0b\u6587\u504f\u5dee\u4e5f\u4f1a\u5bfc\u81f4\u5168\u5c40\u6570\u636e\u5931\u771f\uff0c\u6076\u610f\u6ce8\u5165\u504f\u89c1\u4f1a\u663e\u8457\u635f\u5bb3\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "conclusion": "LLM\u57fa\u4e8e\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\u5728\u654f\u611f\u9886\u57df\u5b58\u5728\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2506.09638", "pdf": "https://arxiv.org/pdf/2506.09638", "abs": "https://arxiv.org/abs/2506.09638", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "AI": {"tldr": "FedVLMBench\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u7684\u8054\u90a6\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5fae\u8c03\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u67b6\u6784\u3001\u7b56\u7565\u548c\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u6570\u636e\u5f02\u8d28\u6027\u548c\u591a\u4efb\u52a1\u4f18\u5316\u7684\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709VLM\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u8bad\u7ec3\uff0c\u96be\u4ee5\u6ee1\u8db3\u9690\u79c1\u654f\u611f\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u7684\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u8054\u90a6\u5b66\u4e60\u4e0b\u7684\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faFedVLMBench\uff0c\u6574\u5408\u4e24\u79cdVLM\u67b6\u6784\u3001\u56db\u79cd\u5fae\u8c03\u7b56\u7565\u3001\u4e94\u79cdFL\u7b97\u6cd5\u3001\u516d\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8986\u76d6\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u573a\u666f\u3002", "result": "\u53d1\u73b02\u5c42MLP\u8fde\u63a5\u5668\u4e0e\u5e76\u53d1\u8c03\u4f18\u662fFL\u4e2d\u7f16\u7801\u5668VLM\u7684\u6700\u4f73\u914d\u7f6e\uff0c\u4e14FL\u65b9\u6cd5\u5bf9\u89c6\u89c9\u4efb\u52a1\u7684\u6570\u636e\u5f02\u8d28\u6027\u66f4\u654f\u611f\u3002", "conclusion": "FedVLMBench\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8054\u90a6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\u548c\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2506.09674", "pdf": "https://arxiv.org/pdf/2506.09674", "abs": "https://arxiv.org/abs/2506.09674", "authors": ["Alessandro Licciardi", "Davide Leo", "Davide Carbone"], "title": "Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables the training of machine learning models\nacross decentralized clients while preserving data privacy. However, the\npresence of anomalous or corrupted clients - such as those with faulty sensors\nor non representative data distributions - can significantly degrade model\nperformance. Detecting such clients without accessing raw data remains a key\nchallenge. We propose WAFFLE (Wavelet and Fourier representations for Federated\nLearning) a detection algorithm that labels malicious clients {\\it before\ntraining}, using locally computed compressed representations derived from\neither the Wavelet Scattering Transform (WST) or the Fourier Transform. Both\napproaches provide low-dimensional, task-agnostic embeddings suitable for\nunsupervised client separation. A lightweight detector, trained on a\ndistillated public dataset, performs the labeling with minimal communication\nand computational overhead. While both transforms enable effective detection,\nWST offers theoretical advantages, such as non-invertibility and stability to\nlocal deformations, that make it particularly well-suited to federated\nscenarios. Experiments on benchmark datasets show that our method improves\ndetection accuracy and downstream classification performance compared to\nexisting FL anomaly detection algorithms, validating its effectiveness as a\npre-training alternative to online detection strategies.", "AI": {"tldr": "WAFFLE\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u548c\u5085\u91cc\u53f6\u53d8\u6362\u7684\u8054\u90a6\u5b66\u4e60\u5f02\u5e38\u5ba2\u6237\u7aef\u68c0\u6d4b\u7b97\u6cd5\uff0c\u80fd\u5728\u8bad\u7ec3\u524d\u8bc6\u522b\u6076\u610f\u5ba2\u6237\u7aef\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5f02\u5e38\u5ba2\u6237\u7aef\uff08\u5982\u6570\u636e\u5206\u5e03\u4e0d\u5177\u4ee3\u8868\u6027\u6216\u4f20\u611f\u5668\u6545\u969c\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5982\u4f55\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u68c0\u6d4b\u8fd9\u4e9b\u5ba2\u6237\u7aef\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c0f\u6ce2\u6563\u5c04\u53d8\u6362\uff08WST\uff09\u6216\u5085\u91cc\u53f6\u53d8\u6362\u751f\u6210\u4f4e\u7ef4\u4efb\u52a1\u65e0\u5173\u5d4c\u5165\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWAFFLE\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u9884\u8bad\u7ec3\u66ff\u4ee3\u5728\u7ebf\u68c0\u6d4b\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "WAFFLE\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u5f02\u5e38\u5ba2\u6237\u7aef\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2506.09682", "pdf": "https://arxiv.org/pdf/2506.09682", "abs": "https://arxiv.org/abs/2506.09682", "authors": ["Iulia Duta", "Pietro Li\u00f2"], "title": "Wasserstein Hypergraph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "The ability to model relational information using machine learning has driven\nadvancements across various domains, from medicine to social science. While\ngraph representation learning has become mainstream over the past decade,\nrepresenting higher-order relationships through hypergraphs is rapidly gaining\nmomentum. In the last few years, numerous hypergraph neural networks have\nemerged, most of them falling under a two-stage, set-based framework. The\nmessages are sent from nodes to edges and then from edges to nodes. However,\nmost of the advancement still takes inspiration from the graph counterpart,\noften simplifying the aggregations to basic pooling operations. In this paper\nwe are introducing Wasserstein Hypergraph Neural Network, a model that treats\nthe nodes and hyperedge neighbourhood as distributions and aggregate the\ninformation using Sliced Wasserstein Pooling. Unlike conventional aggregators\nsuch as mean or sum, which only capture first-order statistics, our approach\nhas the ability to preserve geometric properties like the shape and spread of\ndistributions. This enables the learned embeddings to reflect how easily one\nhyperedge distribution can be transformed into another, following principles of\noptimal transport. Experimental results demonstrate that applying Wasserstein\npooling in a hypergraph setting significantly benefits node classification\ntasks, achieving top performance on several real-world datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08WHGNN\uff09\uff0c\u901a\u8fc7\u5c06\u8282\u70b9\u548c\u8d85\u8fb9\u90bb\u57df\u89c6\u4e3a\u5206\u5e03\uff0c\u5e76\u5229\u7528Sliced Wasserstein Pooling\u8fdb\u884c\u4fe1\u606f\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u591a\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u4fe1\u606f\u805a\u5408\u65b9\u5f0f\u7b80\u5355\uff08\u5982\u5747\u503c\u6216\u6c42\u548c\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u9ad8\u9636\u7edf\u8ba1\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165Wasserstein\u8ddd\u79bb\uff0c\u4fdd\u7559\u5206\u5e03\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5efa\u6a21\u8d85\u56fe\u5173\u7cfb\u3002", "method": "\u63d0\u51faWasserstein Hypergraph Neural Network\uff08WHGNN\uff09\uff0c\u5c06\u8282\u70b9\u548c\u8d85\u8fb9\u90bb\u57df\u5efa\u6a21\u4e3a\u5206\u5e03\uff0c\u5e76\u4f7f\u7528Sliced Wasserstein Pooling\u8fdb\u884c\u4fe1\u606f\u805a\u5408\uff0c\u4ee5\u6355\u6349\u5206\u5e03\u7684\u51e0\u4f55\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWHGNN\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u3002", "conclusion": "WHGNN\u901a\u8fc7\u5f15\u5165Wasserstein\u8ddd\u79bb\u548c\u5206\u5e03\u5efa\u6a21\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u8d85\u56fe\u7684\u9ad8\u9636\u5173\u7cfb\uff0c\u4e3a\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.09701", "pdf": "https://arxiv.org/pdf/2506.09701", "abs": "https://arxiv.org/abs/2506.09701", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "AI": {"tldr": "TRIDENT\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u901a\u7528\u63a8\u7406\u7b97\u6cd5\uff0c\u786e\u4fddLLM\u8f93\u51fa\u6ee1\u8db3\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7ea6\u675f\u3002", "motivation": "LLMs\u5728\u751f\u6210\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u8f93\u51fa\u6ee1\u8db3\u65f6\u5e8f\u7ea6\u675f\u3002", "method": "\u5c06LTLf\u516c\u5f0f\u7f16\u8bd1\u4e3aDFA\uff0c\u5f15\u5bfc\u53d7\u9650\u6ce2\u675f\u641c\u7d22\uff0c\u52a8\u6001\u5c4f\u853d\u8fdd\u89c4\u8def\u5f84\u5e76\u91cd\u65b0\u6392\u5e8f\u3002", "result": "TRIDENT\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\uff0c\u5728\u56fe\u50cf\u6d41\u5206\u7c7b\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TRIDENT\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u5730\u6ee1\u8db3\u65f6\u5e8f\u7ea6\u675f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.09714", "pdf": "https://arxiv.org/pdf/2506.09714", "abs": "https://arxiv.org/abs/2506.09714", "authors": ["Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "Auto-Compressing Networks", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks with short residual connections have demonstrated\nremarkable success across domains, but increasing depth often introduces\ncomputational redundancy without corresponding improvements in representation\nquality. In this work, we introduce Auto-Compressing Networks (ACNs), an\narchitectural variant where additive long feedforward connections from each\nlayer to the output replace traditional short residual connections. ACNs\nshowcase a unique property we coin as \"auto-compression\", the ability of a\nnetwork to organically compress information during training with gradient\ndescent, through architectural design alone. Through auto-compression,\ninformation is dynamically \"pushed\" into early layers during training,\nenhancing their representational quality and revealing potential redundancy in\ndeeper ones. We theoretically show that this property emerges from layer-wise\ntraining patterns present in ACNs, where layers are dynamically utilized during\ntraining based on task requirements. We also find that ACNs exhibit enhanced\nnoise robustness compared to residual networks, superior performance in\nlow-data settings, improved transfer learning capabilities, and mitigate\ncatastrophic forgetting suggesting that they learn representations that\ngeneralize better despite using fewer parameters. Our results demonstrate up to\n18% reduction in catastrophic forgetting and 30-80% architectural compression\nwhile maintaining accuracy across vision transformers, MLP-mixers, and BERT\narchitectures. Furthermore, we demonstrate that coupling ACNs with traditional\npruning techniques, enables significantly better sparsity-performance\ntrade-offs compared to conventional architectures. These findings establish\nACNs as a practical approach to developing efficient neural architectures that\nautomatically adapt their computational footprint to task complexity, while\nlearning robust representations.", "AI": {"tldr": "Auto-Compressing Networks (ACNs) \u901a\u8fc7\u957f\u524d\u9988\u8fde\u63a5\u66ff\u4ee3\u77ed\u6b8b\u5dee\u8fde\u63a5\uff0c\u5b9e\u73b0\u81ea\u52a8\u538b\u7f29\u4fe1\u606f\uff0c\u63d0\u5347\u8868\u793a\u8d28\u91cf\u5e76\u51cf\u5c11\u5197\u4f59\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6df1\u5ea6\u589e\u52a0\u5e26\u6765\u7684\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u8868\u793a\u8d28\u91cf\u3002", "method": "\u5f15\u5165ACNs\u67b6\u6784\uff0c\u901a\u8fc7\u957f\u524d\u9988\u8fde\u63a5\u5b9e\u73b0\u81ea\u52a8\u538b\u7f29\u4fe1\u606f\uff0c\u52a8\u6001\u4f18\u5316\u5c42\u95f4\u8bad\u7ec3\u6a21\u5f0f\u3002", "result": "ACNs\u5728\u566a\u58f0\u9c81\u68d2\u6027\u3001\u4f4e\u6570\u636e\u6027\u80fd\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u51cf\u5c1130-80%\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "conclusion": "ACNs\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u81ea\u52a8\u8c03\u6574\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u3002"}}
{"id": "2506.09733", "pdf": "https://arxiv.org/pdf/2506.09733", "abs": "https://arxiv.org/abs/2506.09733", "authors": ["Minjong Cheon"], "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "AI": {"tldr": "AtmosMJ\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u5728\u6807\u51c6\u7ecf\u7eac\u5ea6\u7f51\u683c\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u957f\u671f\u7a33\u5b9a\u7684\u5929\u6c14\u9884\u6d4b\uff0c\u65e0\u9700\u4f9d\u8d56\u975e\u6807\u51c6\u7a7a\u95f4\u57df\u8f6c\u6362\u3002", "motivation": "\u63a2\u8ba8\u662f\u5426\u80fd\u5728\u6807\u51c6\u7ecf\u7eac\u5ea6\u7f51\u683c\u4e0a\u5b9e\u73b0\u957f\u671f\u7a33\u5b9a\u7684\u5929\u6c14\u9884\u6d4b\uff0c\u800c\u975e\u4f9d\u8d56\u975e\u6807\u51c6\u7a7a\u95f4\u57df\u8f6c\u6362\u3002", "method": "\u63d0\u51faAtmosMJ\uff0c\u4e00\u79cd\u76f4\u63a5\u5728ERA5\u6570\u636e\u4e0a\u8fd0\u884c\u7684\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\uff0c\u91c7\u7528\u65b0\u578bGated Residual Fusion\u673a\u5236\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "AtmosMJ\u80fd\u7a33\u5b9a\u9884\u6d4b\u7ea6500\u5929\uff0c10\u5929\u9884\u6d4b\u7cbe\u5ea6\u4e0ePangu-Weather\u548cGraphCast\u76f8\u5f53\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u6bd4\u975e\u6807\u51c6\u6570\u636e\u8868\u793a\u66f4\u80fd\u5b9e\u73b0\u7a33\u5b9a\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u957f\u671f\u5929\u6c14\u9884\u6d4b\u3002"}}
{"id": "2506.09738", "pdf": "https://arxiv.org/pdf/2506.09738", "abs": "https://arxiv.org/abs/2506.09738", "authors": ["Xin Wang", "Zeyang Zhang", "Linxin Xiao", "Haibo Chen", "Chendi Ge", "Wenwu Zhu"], "title": "Towards Multi-modal Graph Large Language Model", "categories": ["cs.LG"], "comment": null, "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u5927\u8bed\u8a00\u6a21\u578b\uff08MG-LLM\uff09\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u548c\u6cdb\u5316\u591a\u6a21\u6001\u56fe\u6570\u636e\u4e0e\u4efb\u52a1\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5173\u952e\u7279\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u591a\u6a21\u6001\u56fe\u6570\u636e\u4e0e\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MG-LLM\u6846\u67b6\uff0c\u5305\u62ec\u591a\u6a21\u6001\u56fe\u6570\u636e\u7684\u7edf\u4e00\u8868\u793a\u3001\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u63a8\u7406\u7b49\u7279\u6027\u3002", "result": "\u603b\u7ed3\u4e86\u591a\u6a21\u6001\u56fe\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0MG-LLM\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "MG-LLM\u6709\u671b\u63a8\u52a8\u591a\u6a21\u6001\u56fe\u6570\u636e\u4e0e\u4efb\u52a1\u7684\u6cdb\u5316\u7814\u7a76\u3002"}}
{"id": "2506.09742", "pdf": "https://arxiv.org/pdf/2506.09742", "abs": "https://arxiv.org/abs/2506.09742", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AAMAS 2025", "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5de5\u7a0b\u539f\u5219\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76d1\u63a7\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff08\u91cd\u6784\u3001\u5206\u89e3\u548c\u7f16\u8bd1\uff09\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76d1\u63a7\u65b9\u6cd5\u8f93\u51fa\u5197\u957f\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u5f71\u54cd\u51b3\u7b56\u6548\u7387\u3002", "method": "\u91c7\u7528\u7279\u5f81\u5de5\u7a0b\u539f\u5219\uff0c\u901a\u8fc7\u91cd\u6784\u3001\u5206\u89e3\u548c\u7f16\u8bd1\u4e09\u4e2a\u6b65\u9aa4\u4f18\u5316LLM\u4ee3\u7406\u7684\u76d1\u63a7\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76d1\u63a7\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\u548c\u9009\u62e9\u6027LLM\u5229\u7528\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u6e05\u6670\u3001\u53ef\u64cd\u4f5c\u7684\u76d1\u63a7\u6d1e\u5bdf\u3002"}}
{"id": "2506.09769", "pdf": "https://arxiv.org/pdf/2506.09769", "abs": "https://arxiv.org/abs/2506.09769", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "AI": {"tldr": "Load-aware Tram-FL\u901a\u8fc7\u5f15\u5165\u8bad\u7ec3\u8c03\u5ea6\u673a\u5236\uff0c\u4f18\u5316\u5206\u6563\u5f0f\u8054\u90a6\u5b66\u4e60\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u8d1f\u8f7d\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u5728\u5206\u6563\u5f0f\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u8ba1\u7b97\u548c\u901a\u4fe1\u8d1f\u8f7d\u7684\u4e0d\u5747\u8861\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u4f18\u5316\u8c03\u5ea6\u3002", "method": "\u5c06\u5168\u5c40\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u8282\u70b9\u7ea7\u5b50\u95ee\u9898\uff0c\u5f15\u5165\u65b9\u5dee\u7ea6\u675f\u4ee5\u5e73\u8861\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u7684\u6570\u636e\u5229\u7528\u7387\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u51fd\u6570\u6700\u5c0f\u5316\u8bad\u7ec3\u5ef6\u8fdf\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cLoad-aware Tram-FL\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "Load-aware Tram-FL\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u6563\u5f0f\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2506.09781", "pdf": "https://arxiv.org/pdf/2506.09781", "abs": "https://arxiv.org/abs/2506.09781", "authors": ["Chungpa Lee", "Sehee Lim", "Kibok Lee", "Jy-yong Sohn"], "title": "On the Similarities of Embeddings in Contrastive Learning", "categories": ["cs.LG", "stat.ML"], "comment": "contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair", "summary": "Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u6b63\u8d1f\u5bf9\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5728\u5b8c\u6574\u6279\u6b21\u548c\u5c0f\u6279\u6b21\u8bbe\u7f6e\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u635f\u5931\u51fd\u6570\u4ee5\u6539\u8fdb\u5c0f\u6279\u6b21\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u89e3\u91ca\u5e7f\u6cdb\u7684\u5bf9\u6bd4\u635f\u5931\u76ee\u6807\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b63\u8d1f\u5bf9\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u5728\u5c0f\u6279\u6b21\u8bad\u7ec3\u4e2d\u5f15\u5165\u8f85\u52a9\u635f\u5931\u4ee5\u51cf\u5c11\u8d1f\u5bf9\u76f8\u4f3c\u6027\u7684\u65b9\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u8f85\u52a9\u635f\u5931\u5728\u5c0f\u6279\u6b21\u8bad\u7ec3\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u7684\u7edf\u4e00\u6846\u67b6\u548c\u8f85\u52a9\u635f\u5931\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6539\u8fdb\u3002"}}
{"id": "2506.09785", "pdf": "https://arxiv.org/pdf/2506.09785", "abs": "https://arxiv.org/abs/2506.09785", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fde\u7eed\u4f9d\u8d56\u6570\u636e\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u548c\u8f6f\u76f8\u4f3c\u6027\u5ea6\u91cf\u6539\u8fdb\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5728\u65f6\u7a7a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u6837\u672c\u95f4\u8bed\u4e49\u72ec\u7acb\uff0c\u4e0d\u9002\u7528\u4e8e\u5177\u6709\u590d\u6742\u76f8\u5173\u6027\u7684\u4f9d\u8d56\u6570\u636e\uff08\u5982\u65f6\u7a7a\u6570\u636e\uff09\u3002", "method": "\u63d0\u51fa\u4f9d\u8d56\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u786c\u548c\u8f6f\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u63a8\u5bfc\u51fa\u9002\u5e94\u6837\u672c\u4f9d\u8d56\u5173\u7cfb\u7684\u76f8\u4f3c\u6027\u77e9\u9635\u548c\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728UEA\u548cUCR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u63d0\u53474.17%\u548c2.08%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5e72\u65f1\u5206\u7c7b\u4efb\u52a1\u4e2dROC-AUC\u5f97\u5206\u63d0\u9ad87%\u3002", "conclusion": "\u4f9d\u8d56\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.09803", "pdf": "https://arxiv.org/pdf/2506.09803", "abs": "https://arxiv.org/abs/2506.09803", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u672c\u5730\u9690\u79c1\u56fe\u5b66\u4e60\u534f\u8bae\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u4e86\u9632\u5fa1\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u56fe\u6570\u636e\u5e38\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u672c\u5730\u9690\u79c1\u56fe\u5b66\u4e60\u534f\u8bae\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u53ef\u80fd\u9762\u4e34\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u9700\u7814\u7a76\u5176\u5b89\u5168\u6027\u548c\u9632\u5fa1\u63aa\u65bd\u3002", "method": "\u653b\u51fb\u8005\u901a\u8fc7\u6ce8\u5165\u865a\u5047\u7528\u6237\u5e76\u64cd\u7eb5\u5176\u4e0e\u771f\u5b9e\u7528\u6237\u5efa\u7acb\u94fe\u63a5\uff0c\u53d1\u9001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u4ee5\u7834\u574f\u56fe\u5b66\u4e60\u7684\u6548\u7528\u3002", "result": "\u653b\u51fb\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u73b0\u6709\u9632\u5fa1\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u4fdd\u969c\u9690\u79c1\u56fe\u5b66\u4e60\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2506.09810", "pdf": "https://arxiv.org/pdf/2506.09810", "abs": "https://arxiv.org/abs/2506.09810", "authors": ["Minoh Jeong", "Alfred Hero"], "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProjNCE\uff0c\u4e00\u79cd\u7edf\u4e00\u76d1\u7763\u4e0e\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5176\u4e3a\u6709\u6548\u7684\u4e92\u4fe1\u606f\u4e0b\u754c\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff08SupCon\uff09\u5728\u4e92\u4fe1\u606f\u89c6\u89d2\u4e0b\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u66f4\u7075\u6d3b\u7684\u635f\u5931\u51fd\u6570\u3002", "method": "\u5f15\u5165ProjNCE\uff0c\u901a\u8fc7\u6295\u5f71\u51fd\u6570\u548c\u8d1f\u5bf9\u8c03\u6574\u9879\u7edf\u4e00\u76d1\u7763\u4e0e\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u5e76\u63a2\u7d22\u57fa\u4e8e\u8d28\u5fc3\u7684\u6295\u5f71\u65b9\u6cd5\u3002", "result": "ProjNCE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8eSupCon\u548c\u6807\u51c6\u4ea4\u53c9\u71b5\u8bad\u7ec3\u3002", "conclusion": "ProjNCE\u4ece\u4e92\u4fe1\u606f\u89e3\u91ca\u548c\u6295\u5f71\u8bbe\u8ba1\u4e24\u65b9\u9762\u6539\u8fdb\u4e86SupCon\uff0c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u9002\u7528\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2506.09813", "pdf": "https://arxiv.org/pdf/2506.09813", "abs": "https://arxiv.org/abs/2506.09813", "authors": ["Ariel Procaccia", "Benjamin Schiffer", "Serena Wang", "Shirley Zhang"], "title": "Metritocracy: Representative Metrics for Lite Benchmarks", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u7684\u5ea6\u91cf\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff1a\u4f4d\u7f6e\u4ee3\u8868\u6027\u548c\u4f4d\u7f6e\u6bd4\u4f8b\u6027\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u5ea6\u91cf\u5b50\u96c6\u9009\u62e9\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4f4d\u7f6e\u4ee3\u8868\u6027\u548c\u4f4d\u7f6e\u6bd4\u4f8b\u6027\u4e24\u79cd\u5f62\u5f0f\u5316\u6982\u5ff5\uff0c\u5e76\u7814\u7a76\u5176\u7406\u8bba\u754c\u9650\u548c\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u6ee1\u8db3\u8fd9\u4e9b\u6027\u8d28\u6240\u9700\u7684\u6700\u5c0f\u5ea6\u91cf\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5ea6\u91cf\u5b50\u96c6\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u9002\u7528\u4e8eLLM\u8bc4\u4f30\u548c\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2506.09816", "pdf": "https://arxiv.org/pdf/2506.09816", "abs": "https://arxiv.org/abs/2506.09816", "authors": ["Cecilia Casolo", "S\u00f6ren Becker", "Niki Kilbertus"], "title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "categories": ["cs.LG"], "comment": "9 pages, 4 figures", "summary": "Dynamical systems modeling is a core pillar of scientific inquiry across\nnatural and life sciences. Increasingly, dynamical system models are learned\nfrom data, rendering identifiability a paramount concept. For systems that are\nnot identifiable from data, no guarantees can be given about their behavior\nunder new conditions and inputs, or about possible control mechanisms to steer\nthe system. It is known in the community that \"linear ordinary differential\nequations (ODE) are almost surely identifiable from a single trajectory.\"\nHowever, this only holds for dense matrices. The sparse regime remains\nunderexplored, despite its practical relevance with sparsity arising naturally\nin many biological, social, and physical systems. In this work, we address this\ngap by characterizing the identifiability of sparse linear ODEs. Contrary to\nthe dense case, we show that sparse systems are unidentifiable with a positive\nprobability in practically relevant sparsity regimes and provide lower bounds\nfor this probability. We further study empirically how this theoretical\nunidentifiability manifests in state-of-the-art methods to estimate linear ODEs\nfrom data. Our results corroborate that sparse systems are also practically\nunidentifiable. Theoretical limitations are not resolved through inductive\nbiases or optimization dynamics. Our findings call for rethinking what can be\nexpected from data-driven dynamical system modeling and allows for quantitative\nassessments of how much to trust a learned linear ODE.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7a00\u758f\u7ebf\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u4e0e\u5bc6\u96c6\u77e9\u9635\u4e0d\u540c\uff0c\u7a00\u758f\u7cfb\u7edf\u5728\u5b9e\u9645\u76f8\u5173\u7a00\u758f\u5ea6\u4e0b\u5b58\u5728\u4e0d\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6982\u7387\u4e0b\u754c\u3002", "motivation": "\u7a00\u758f\u7ebf\u6027ODE\u5728\u751f\u7269\u3001\u793e\u4f1a\u548c\u7269\u7406\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u4f46\u5176\u53ef\u8bc6\u522b\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u672c\u6587\u7814\u7a76\u4e86\u7a00\u758f\u7ebf\u6027ODE\u7684\u53ef\u8bc6\u522b\u6027\u53ca\u5176\u5728\u5b9e\u9645\u4f30\u8ba1\u65b9\u6cd5\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7a00\u758f\u7cfb\u7edf\u5728\u5b9e\u9645\u76f8\u5173\u7a00\u758f\u5ea6\u4e0b\u5b58\u5728\u4e0d\u53ef\u8bc6\u522b\u6027\uff0c\u4e14\u7406\u8bba\u9650\u5236\u65e0\u6cd5\u901a\u8fc7\u5f52\u7eb3\u504f\u7f6e\u6216\u4f18\u5316\u52a8\u6001\u89e3\u51b3\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u91cd\u65b0\u601d\u8003\u6570\u636e\u9a71\u52a8\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u7684\u9884\u671f\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5b66\u4e60\u5230\u7684\u7ebf\u6027ODE\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u91cf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2506.09824", "pdf": "https://arxiv.org/pdf/2506.09824", "abs": "https://arxiv.org/abs/2506.09824", "authors": ["Johan Erbani", "Sonia Ben Mokhtar", "Pierre-Edouard Portier", "Elod Egyed-Zsigmond", "Diana Nurbakova"], "title": "Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWoLA\u7684\u52a0\u6743\u635f\u5931\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u4e0b\u8bc6\u522b\u62dc\u5360\u5ead\u68af\u5ea6\u7684\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b58\u5728\u62dc\u5360\u5ead\u53c2\u4e0e\u8005\u8d21\u732e\u6709\u5bb3\u68af\u5ea6\u7684\u5b89\u5168\u5a01\u80c1\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u96be\u4ee5\u533a\u5206\u8bda\u5b9e\u68af\u5ea6\u4e0e\u62dc\u5360\u5ead\u68af\u5ea6\u3002", "method": "\u5f15\u5165Worker Label Alignment Loss (WoLA)\uff0c\u901a\u8fc7\u52a0\u6743\u635f\u5931\u5bf9\u9f50\u8bda\u5b9e\u5de5\u4f5c\u8005\u7684\u68af\u5ea6\uff0c\u4fbf\u4e8e\u8bc6\u522b\u62dc\u5360\u5ead\u68af\u5ea6\u3002", "result": "WoLA\u5728\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "WoLA\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u62dc\u5360\u5ead\u653b\u51fb\u7684\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6570\u636e\u5f02\u6784\u6027\u5f3a\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.09862", "pdf": "https://arxiv.org/pdf/2506.09862", "abs": "https://arxiv.org/abs/2506.09862", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarc\u00f3n", "Sofia Vallecorsa", "Michele Grossi"], "title": "Guided Graph Compression for Quantum Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "hep-ex", "quant-ph"], "comment": null, "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGGC\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u538b\u7f29\u56fe\u7684\u8282\u70b9\u548c\u7279\u5f81\u7ef4\u5ea6\uff0c\u4ee5\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u91cf\u5b50\u6216\u7ecf\u5178\u5206\u7c7b\u5668\u4e2d\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5904\u7406\u5927\u56fe\u65f6\u7684\u9ad8\u5185\u5b58\u9700\u6c42\u548c\u7a00\u758f\u77e9\u9635\u64cd\u4f5c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u91cf\u5b50\u8ba1\u7b97\uff08QC\uff09\u786c\u4ef6\u5bf9\u6570\u636e\u7ef4\u5ea6\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u8282\u70b9\u548c\u7279\u5f81\u7ef4\u5ea6\u7684\u538b\u7f29\uff0c\u538b\u7f29\u8fc7\u7a0b\u4ee5\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u4e3a\u76ee\u6807\u3002", "result": "\u5728Jet Tagging\u4efb\u52a1\u4e2d\uff0cGGC\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u6216\u7ecf\u5178GNN\u5206\u7c7b\u5668\uff0c\u5e76\u652f\u6301\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u65b0\u578bQGNN\u7ed3\u6784\u3002", "conclusion": "GGC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u56fe\u5904\u7406\u548c\u91cf\u5b50\u786c\u4ef6\u9650\u5236\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4e3aQGNN\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.09867", "pdf": "https://arxiv.org/pdf/2506.09867", "abs": "https://arxiv.org/abs/2506.09867", "authors": ["Amit Baran Dey", "Wasim Arif", "Rakhesh Singh Kshetrimayum"], "title": "Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing", "categories": ["cs.LG"], "comment": "6 pages, 11 figures, Accepted to IEEE INDISCON 2025", "summary": "This paper proposes a machine learning-based methodology for the\nclassification of various oil samples based on their dielectric properties,\nutilizing a microwave resonant sensor. The dielectric behaviour of oils,\ngoverned by their molecular composition, induces distinct shifts in the\nsensor's resonant frequency and amplitude response. These variations are\nsystematically captured and processed to extract salient features, which serve\nas inputs for multiple machine learning classifiers. The microwave resonant\nsensor operates in a non-destructive, low-power manner, making it particularly\nwell-suited for real-time industrial applications. A comprehensive dataset is\ndeveloped by varying the permittivity of oil samples and acquiring the\ncorresponding sensor responses. Several classifiers are trained and evaluated\nusing the extracted resonant features to assess their capability in\ndistinguishing between oil types. Experimental results demonstrate that the\nproposed approach achieves a high classification accuracy of 99.41% with the\nrandom forest classifier, highlighting its strong potential for automated oil\nidentification. The system's compact form factor, efficiency, and high\nperformance underscore its viability for fast and reliable oil characterization\nin industrial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5fae\u6ce2\u8c10\u632f\u4f20\u611f\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u4ecb\u7535\u7279\u6027\u5206\u7c7b\u6cb9\u6837\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe99.41%\u3002", "motivation": "\u6cb9\u7684\u5206\u5b50\u7ec4\u6210\u51b3\u5b9a\u5176\u4ecb\u7535\u7279\u6027\uff0c\u5bfc\u81f4\u4f20\u611f\u5668\u8c10\u632f\u9891\u7387\u548c\u632f\u5e45\u53d8\u5316\uff0c\u53ef\u7528\u4e8e\u5206\u7c7b\u3002", "method": "\u5229\u7528\u5fae\u6ce2\u8c10\u632f\u4f20\u611f\u5668\u975e\u7834\u574f\u6027\u6355\u83b7\u6cb9\u6837\u4ecb\u7535\u7279\u6027\uff0c\u63d0\u53d6\u7279\u5f81\u540e\u8f93\u5165\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe99.41%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u7d27\u51d1\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u6cb9\u6837\u5206\u7c7b\u3002"}}
{"id": "2506.09887", "pdf": "https://arxiv.org/pdf/2506.09887", "abs": "https://arxiv.org/abs/2506.09887", "authors": ["Nirmit Joshi", "Hugo Koubbi", "Theodor Misiakiewicz", "Nathan Srebro"], "title": "Learning single-index models via harmonic decomposition", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "80 pages", "summary": "We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5355\u6307\u6807\u6a21\u578b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u7528\u7403\u8c10\u51fd\u6570\u800c\u975eHermite\u591a\u9879\u5f0f\u4f5c\u4e3a\u81ea\u7136\u57fa\uff0c\u4ee5\u6355\u6349\u95ee\u9898\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u4f30\u8ba1\u5668\uff0c\u5206\u522b\u4f18\u5316\u6837\u672c\u590d\u6742\u5ea6\u6216\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u63a2\u7d22\u5355\u6307\u6807\u6a21\u578b\u5b66\u4e60\u7684\u7edf\u8ba1\u548c\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u8f93\u5165\u5206\u5e03\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7403\u8c10\u51fd\u6570\u7684\u65b0\u89c6\u89d2\uff0c\u8bbe\u8ba1\u4e24\u79cd\u4f30\u8ba1\u5668\uff1a\u57fa\u4e8e\u5f20\u91cf\u5c55\u5f00\u548c\u5728\u7ebfSGD\u3002", "result": "\u5728\u4efb\u610f\u7403\u5bf9\u79f0\u8f93\u5165\u5206\u5e03\u4e0b\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\u6216\u6700\u4f18\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u53d1\u73b0\u540c\u65f6\u4f18\u5316\u4e24\u8005\u53ef\u80fd\u4e0d\u53ef\u884c\u3002", "conclusion": "\u7403\u8c10\u51fd\u6570\u4e3a\u5355\u6307\u6807\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u57fa\uff0c\u7406\u8bba\u4e0d\u4ec5\u590d\u73b0\u4e86\u9ad8\u65af\u8f93\u5165\u4e0b\u7684\u7ed3\u679c\uff0c\u8fd8\u63ed\u793a\u4e86\u65b0\u73b0\u8c61\u3002"}}
{"id": "2506.09891", "pdf": "https://arxiv.org/pdf/2506.09891", "abs": "https://arxiv.org/abs/2506.09891", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "title": "Causal Climate Emulation with Bayesian Filtering", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.ao-ph"], "comment": "32 pages, 21 figures", "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u6c14\u5019\u6a21\u578b\u6a21\u62df\u5668\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u7a33\u5b9a\u7684\u81ea\u56de\u5f52\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u6c14\u5019\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u56e0\u679c\u5173\u7cfb\u7684\u6574\u5408\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u6a21\u62df\u5668\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u3002", "result": "\u6a21\u62df\u5668\u80fd\u51c6\u786e\u5b66\u4e60\u6c14\u5019\u52a8\u6001\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u5b9e\u9645\u6c14\u5019\u6a21\u578b\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u6c14\u5019\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.09896", "pdf": "https://arxiv.org/pdf/2506.09896", "abs": "https://arxiv.org/abs/2506.09896", "authors": ["Attanasia Garuso", "Silvija Kokalj-Filipovic", "Yagna Kaasaragadda"], "title": "A look at adversarial attacks on radio waveforms from discrete latent space", "categories": ["cs.LG"], "comment": null, "summary": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86VQVAE\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5bf9\u9ad8\u4fe1\u566a\u6bd4\u5c04\u9891\u6570\u636e\u7684\u653b\u51fb\u6291\u5236\u6548\u679c\uff0c\u53d1\u73b0VQVAE\u80fd\u663e\u8457\u964d\u4f4e\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76VQVAE\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5176\u5728\u6570\u5b57\u65e0\u7ebf\u7535\u6ce2\u5f62\u4e2d\u7684\u9632\u5fa1\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u4fdd\u7559\u76f8\u4f4d\u548c\u4e0d\u4fdd\u7559\u76f8\u4f4d\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u6d4b\u8bd5\u5206\u7c7b\u5668\u5728\u539f\u59cb\u6570\u636e\u548cVQVAE\u91cd\u5efa\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u3002", "result": "VQVAE\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\u4e2d\u53d1\u73b0\u4e86\u6709\u52a9\u4e8e\u68c0\u6d4b\u653b\u51fb\u7684\u7279\u6027\u3002", "conclusion": "VQVAE\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u5176\u6f5c\u5728\u7a7a\u95f4\u7684\u7279\u6027\u53ef\u80fd\u6709\u52a9\u4e8e\u653b\u51fb\u68c0\u6d4b\u3002"}}
{"id": "2506.09901", "pdf": "https://arxiv.org/pdf/2506.09901", "abs": "https://arxiv.org/abs/2506.09901", "authors": ["Noel Brindise", "Vijeth Hebbar", "Riya Shah", "Cedric Langbort"], "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDNA\u7684\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u8fd1\u6700\u4f18\u8f68\u8ff9\u9009\u9879\uff0c\u5e2e\u52a9\u4eba\u7c7b\u7528\u6237\u7406\u89e3\u4ee3\u7406\u7684\u9009\u62e9\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u7684\u8f68\u8ff9\u9009\u9879\uff0c\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u652f\u6301\u4eba\u7c7b\u7528\u6237\u7684\u51b3\u7b56\u3002", "method": "\u5229\u7528\u5956\u52b1\u5851\u9020\u548c\u5c40\u90e8\u6539\u8fdb\u7684Q\u5b66\u4e60\u95ee\u9898\uff0c\u6c42\u89e3\u5177\u6709\u4fdd\u8bc1\u7684\u03b5\u6700\u4f18\u6027\u7684\u591a\u6837\u5316\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u6210\u529f\u751f\u6210\u4e86\u5177\u6709\u663e\u8457\u5dee\u5f02\u7684\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DNA\u4e0d\u4ec5\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u548c\u81ea\u9002\u5e94\u89c4\u5212\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.09923", "pdf": "https://arxiv.org/pdf/2506.09923", "abs": "https://arxiv.org/abs/2506.09923", "authors": ["Liou Tang", "James Joshi", "Ashish Kundu"], "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning", "categories": ["cs.LG"], "comment": null, "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u79c1\u653b\u51fb\u65b9\u6cd5Apollo\uff0c\u9488\u5bf9\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u573a\u666f\uff0c\u4ec5\u9700\u8bbf\u95ee\u9057\u5fd8\u6a21\u578b\u7684\u6807\u7b7e\u8f93\u51fa\u5373\u53ef\u63a8\u65ad\u6570\u636e\u6837\u672c\u662f\u5426\u88ab\u9057\u5fd8\uff0c\u653b\u51fb\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\u548c\u5408\u89c4\u6027\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u6a21\u578b\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u8f83\u5f31\u5a01\u80c1\u6a21\u578b\uff08\u9700\u8bbf\u95ee\u539f\u59cb\u548c\u9057\u5fd8\u6a21\u578b\uff09\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faApollo\u653b\u51fb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e25\u683c\u5a01\u80c1\u6a21\u578b\uff08\u4ec5\u9700\u8bbf\u95ee\u9057\u5fd8\u6a21\u578b\u7684\u6807\u7b7e\u8f93\u51fa\uff09\uff0c\u63a8\u65ad\u6570\u636e\u6837\u672c\u662f\u5426\u88ab\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cApollo\u5728\u8bbf\u95ee\u6743\u9650\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u9ad8\u7cbe\u5ea6\u63a8\u65ad\u9057\u5fd8\u6837\u672c\u7684\u6210\u5458\u72b6\u6001\u3002", "conclusion": "Apollo\u653b\u51fb\u65b9\u6cd5\u5728\u4e25\u683c\u5a01\u80c1\u6a21\u578b\u4e0b\u6709\u6548\uff0c\u63ed\u793a\u4e86\u673a\u5668\u9057\u5fd8\u53ef\u80fd\u5e26\u6765\u7684\u65b0\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2506.09928", "pdf": "https://arxiv.org/pdf/2506.09928", "abs": "https://arxiv.org/abs/2506.09928", "authors": ["Ruixuan Xu", "Xiangxiang Weng"], "title": "Bayesian Probabilistic Matrix Factorization", "categories": ["cs.LG", "stat.ML"], "comment": "11 pages, 4 figures", "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6982\u7387\u77e9\u9635\u5206\u89e3\uff08PMF\uff09\u7684\u540e\u9a8c\u5206\u5e03\u8fd1\u4f3c\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86MCMC\u548cVI\u4e24\u79cd\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u77e9\u9635\u5206\u89e3\u65e0\u6cd5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u800cPMF\u867d\u7136\u5f15\u5165\u6982\u7387\u5206\u5e03\uff0c\u4f46\u540e\u9a8c\u5206\u5e03\u8ba1\u7b97\u56f0\u96be\u3002", "method": "\u91c7\u7528MCMC\u548cVI\u4e24\u79cd\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVI\u6536\u655b\u66f4\u5feb\uff0c\u800cMCMC\u7684\u540e\u9a8c\u4f30\u8ba1\u66f4\u51c6\u786e\u3002", "conclusion": "VI\u9002\u5408\u5feb\u901f\u6536\u655b\u573a\u666f\uff0cMCMC\u9002\u5408\u9ad8\u7cbe\u5ea6\u9700\u6c42\u573a\u666f\u3002"}}
{"id": "2506.09940", "pdf": "https://arxiv.org/pdf/2506.09940", "abs": "https://arxiv.org/abs/2506.09940", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u77e5\u8bc6\u8f6c\u79fb\u7684\u80cc\u666f\u4e0b\uff0c\u5b66\u4e60\u7cfb\u7edf\u52a8\u6001\u5e76\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u77e5\u8bc6\u8f6c\u79fb\u662f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7ecf\u6d4e\u5b66\u548c\u793e\u4f1a\u79d1\u5b66\u4e2d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5728\u8fd9\u4e9b\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u5b66\u4e60\u7cfb\u7edf\u52a8\u6001\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-i.i.d.\uff09\u52a8\u4f5c\u5b66\u4e60\u6df7\u6742\u53d8\u91cf\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u6218\u7565\u4ea4\u4e92\u6a21\u578b\u4e2d\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u4ee5\u7d27\u5bc6\u7684\u6837\u672c\u590d\u6742\u5ea6O(1/\u03b5\u00b2)\u5b66\u4e60\u03b5-\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u77e5\u8bc6\u8f6c\u79fb\u7684\u80cc\u666f\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7cfb\u7edf\u52a8\u6001\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2506.09955", "pdf": "https://arxiv.org/pdf/2506.09955", "abs": "https://arxiv.org/abs/2506.09955", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine S\u00fcsstrunk"], "title": "Canonical Latent Representations in Conditional Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "45 pages,41 figures", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLAReps\u7684\u6f5c\u5728\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08CDMs\uff09\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u56e0\u7279\u5f81\u7ea0\u7f20\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7CaDistill\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7c7b\u522b\u77e5\u8bc6\u84b8\u998f\u3002", "motivation": "\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08CDMs\uff09\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5efa\u6a21\u80fd\u529b\u4f1a\u5bfc\u81f4\u7c7b\u522b\u7279\u5f81\u4e0e\u65e0\u5173\u4e0a\u4e0b\u6587\u7684\u7ea0\u7f20\uff0c\u5f71\u54cd\u7279\u5f81\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522bCLAReps\uff08\u4fdd\u7559\u7c7b\u522b\u6838\u5fc3\u4fe1\u606f\u3001\u53bb\u9664\u65e0\u5173\u4fe1\u53f7\u7684\u6f5c\u5728\u8868\u793a\uff09\uff0c\u5e76\u5f00\u53d1CaDistill\u65b9\u6cd5\uff0c\u5229\u7528CLAReps\u8fdb\u884c\u7279\u5f81\u84b8\u998f\u3002", "result": "\u5b66\u751f\u6a21\u578b\u4ec5\u970010%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b66\u4e60\u5230\u6838\u5fc3\u7c7b\u522b\u77e5\u8bc6\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CDMs\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u5668\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u6559\u5e08\u6a21\u578b\uff0c\u63a8\u52a8\u9c81\u68d2\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2506.09991", "pdf": "https://arxiv.org/pdf/2506.09991", "abs": "https://arxiv.org/abs/2506.09991", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "categories": ["cs.LG"], "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.", "AI": {"tldr": "Multiverse\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7MapReduce\u8303\u5f0f\u5b9e\u73b0\u5e76\u884c\u751f\u6210\uff0c\u6027\u80fd\u4e0e\u4e3b\u6d41AR-LLMs\u76f8\u5f53\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u53d7AR-LLMs\u4e2d\u9690\u5f0f\u5e76\u884c\u6027\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u539f\u751f\u652f\u6301\u5e76\u884c\u751f\u6210\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528MapReduce\u8303\u5f0f\uff0c\u5206\u4e3aMap\uff08\u4efb\u52a1\u5206\u89e3\uff09\u3001Process\uff08\u5e76\u884c\u6267\u884c\uff09\u548cReduce\uff08\u7ed3\u679c\u5408\u6210\uff09\u4e09\u9636\u6bb5\uff1b\u8bbe\u8ba1\u4e86Multiverse Attention\u548cMultiverse Engine\u652f\u6301\u5e76\u884c\u63a8\u7406\u3002", "result": "Multiverse-32B\u57283\u5c0f\u65f6\u5fae\u8c03\u540e\uff0c\u6027\u80fd\u4e0e\u540c\u89c4\u6a21AR-LLMs\u76f8\u5f53\uff08AIME24 & 25\u5f97\u520654%\u548c46%\uff09\uff0c\u6548\u7387\u63d0\u53472\u500d\u3002", "conclusion": "Multiverse\u5c55\u793a\u4e86\u5e76\u884c\u751f\u6210\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b8c\u6574\u652f\u6301\u3002"}}
{"id": "2506.09998", "pdf": "https://arxiv.org/pdf/2506.09998", "abs": "https://arxiv.org/abs/2506.09998", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Sch\u00f6lkopf"], "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63cf\u8ff0\u6982\u7387\u5206\u5e03\u4e0e\u751f\u6210\u6837\u672c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVerbalized Rejection Sampling\uff08VRS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6539\u8fdb\u91c7\u6837\u504f\u5dee\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "LLMs\u80fd\u591f\u51c6\u786e\u63cf\u8ff0\u6982\u7387\u5206\u5e03\uff0c\u4f46\u5728\u751f\u6210\u53ef\u9760\u6837\u672c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u968f\u673a\u6027\u7684\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86Verbalized Rejection Sampling\uff08VRS\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793aLLM\u5bf9\u6837\u672c\u8fdb\u884c\u63a8\u7406\u548c\u63a5\u53d7/\u62d2\u7edd\u3002", "result": "VRS\u663e\u8457\u51cf\u5c11\u4e86\u91c7\u6837\u504f\u5dee\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u4f18\u4e8e\u76f4\u63a5\u91c7\u6837\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u5178\u6982\u7387\u5de5\u5177\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5d4c\u5165LLM\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6216\u590d\u6742\u7684\u63d0\u793a\u5de5\u7a0b\u3002"}}
{"id": "2505.14156", "pdf": "https://arxiv.org/pdf/2505.14156", "abs": "https://arxiv.org/abs/2505.14156", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "I.2; H.3.3"], "comment": null, "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSymbolic Graph Ranker (SGR)\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u63d0\u5347\u4f1a\u8bdd\u641c\u7d22\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4f1a\u8bdd\u641c\u7d22\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u987a\u5e8f\u5efa\u6a21\u6216\u901a\u7528\u56fe\u7ed3\u6784\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u7ed3\u6784\u7684\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u7b26\u53f7\u8bed\u6cd5\u89c4\u5219\u5c06\u4f1a\u8bdd\u56fe\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u5e76\u8bbe\u8ba1\u81ea\u76d1\u7763\u4efb\u52a1\uff08\u5982\u94fe\u63a5\u9884\u6d4b\u3001\u8282\u70b9\u5185\u5bb9\u751f\u6210\u7b49\uff09\u589e\u5f3aLLMs\u5bf9\u56fe\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "\u5728AOL\u548cTiangong-ST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86SGR\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SGR\u4e3a\u4f20\u7edf\u641c\u7d22\u7b56\u7565\u4e0e\u73b0\u4ee3LLMs\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u4efb\u52a1\u76f8\u5173\u56fe\u50cf\u7279\u5f81\u751f\u6210\u8f6f\u63d0\u793a\uff0c\u63d0\u5347\u5c0f\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\uff0c\u989d\u5916\u56fe\u50cf\u4fe1\u606f\u53ef\u80fd\u5e72\u6270\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u5f15\u5165\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6620\u5c04\u6a21\u5757\u84b8\u998f\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u751f\u6210\u8f6f\u63d0\u793a\uff0c\u7ed3\u5408LLaVA v1.5\u67b6\u6784\u5b9e\u73b0\u5c11\u6837\u672c\u4efb\u52a1\u9002\u5e94\u3002", "result": "\u5728VL-ICL Bench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8eICL\u548c\u5176\u4ed6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u56fe\u50cf\u6270\u52a8\u4e0b\u4ecd\u80fd\u63d0\u5347\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u89c4\u6a21LMMs\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "RuleReasoner\u662f\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u5c0f\u578b\u63a8\u7406\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u89c4\u5219\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u6837\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u63a8\u7406\u6a21\u578b\uff08SRMs\uff09\u5728\u89c4\u5219\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u3002", "method": "\u63d0\u51faRuleReasoner\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u611f\u77e5\u7684\u52a8\u6001\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u5386\u53f2\u5956\u52b1\u66f4\u65b0\u91c7\u6837\u6743\u91cd\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u5728\u7ebf\u5b66\u4e60\u3002", "result": "RuleReasoner\u5728\u5206\u5e03\u5185\uff08ID\uff09\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u524d\u6cbf\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982OpenAI-o1\uff09\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "RuleReasoner\u4e3a\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89c4\u5219\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09063", "pdf": "https://arxiv.org/pdf/2506.09063", "abs": "https://arxiv.org/abs/2506.09063", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "AI": {"tldr": "CryoSPIRE\u662f\u4e00\u79cd\u65b0\u578b\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5904\u7406\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u4e2d\u7684\u975e\u521a\u6027\u6784\u8c61\u7075\u6d3b\u6027\u548c\u7ec4\u6210\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5b9e\u9a8c\u6570\u636e\u7684\u89e3\u6790\u80fd\u529b\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\uff08Cryo-EM\uff09\u5728\u5206\u5b50\u751f\u7269\u5b66\u4e2d\u5177\u6709\u9769\u547d\u6027\u610f\u4e49\uff0c\u4f46\u5982\u4f55\u5efa\u6a21\u975e\u521a\u6027\u6784\u8c61\u7075\u6d3b\u6027\u548c\u7ec4\u6210\u53d8\u5316\u7684\u5206\u5b50\u7ed3\u6784\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u90e8\u5206\u5206\u5272\u7684\u521d\u59cb\u8fc7\u7a0b\uff0c\u4ee5\u5904\u7406\u6784\u8c61\u548c\u7ec4\u6210\u53d8\u5316\u3002", "result": "CryoSPIRE\u5728\u590d\u6742\u5b9e\u9a8c\u6570\u636e\u4e2d\u63ed\u793a\u4e86\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7ed3\u6784\uff0c\u5e76\u5728CryoBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "CryoSPIRE\u4e3a\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u4e2d\u7684\u5f02\u8d28\u6027\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09065", "pdf": "https://arxiv.org/pdf/2506.09065", "abs": "https://arxiv.org/abs/2506.09065", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "6 pages, 8 figures, and 1 table", "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u8f85\u52a9\u6280\u672f\uff0c\u901a\u8fc7\u773c\u52a8\u53d8\u91cf\u751f\u6210\u7684\u56fe\u50cf\u53d8\u6362\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\uff0c\u7b80\u5316ASD\u8bca\u65ad\u548c\u7ba1\u7406\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "ASD\u60a3\u75c5\u7387\u4e0a\u5347\uff0c\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u66f4\u4fbf\u6377\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u773c\u52a8\u53d8\u91cf\u751f\u6210\u7684\u56fe\u50cf\u53d8\u6362\uff0c\u5f00\u53d1AI\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u3002", "result": "\u5b9e\u73b0\u5bb6\u5ead\u5b9a\u671f\u8bca\u65ad\uff0c\u51cf\u5c11\u538b\u529b\uff0c\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u6539\u5584\u76d1\u62a4\u4eba\u4e0e\u6cbb\u7597\u5e08\u95f4\u7684\u6c9f\u901a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ca\u65f6\u3001\u53ef\u8bbf\u95ee\u7684\u8bca\u65ad\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\uff0c\u6539\u5584ASD\u60a3\u8005\u7684\u751f\u6d3b\u8d28\u91cf\u3002"}}
{"id": "2506.09068", "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "BG-HOP\u662f\u4e00\u79cd\u751f\u6210\u6027\u5148\u9a8c\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a213D\u53cc\u624b-\u7269\u4f53\u4ea4\u4e92\uff0c\u901a\u8fc7\u6269\u5c55\u5355\u624b\u751f\u6210\u5148\u9a8c\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53cc\u624b\u4ea4\u4e92\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u6a21\u53cc\u624b\u4e0e\u7269\u4f53\u7684\u8054\u5408\u5206\u5e03\u3002", "method": "\u6269\u5c55\u73b0\u6709\u7684\u5355\u624b\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0c\u751f\u6210\u53cc\u624b\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u53cc\u624b\u4ea4\u4e92\u52a8\u4f5c\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u53d6\u52a8\u4f5c\u3002", "conclusion": "BG-HOP\u4e3a\u53cc\u624b\u4ea4\u4e92\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.09069", "pdf": "https://arxiv.org/pdf/2506.09069", "abs": "https://arxiv.org/abs/2506.09069", "authors": ["Sahaj Raj Malla"], "title": "Devanagari Digit Recognition using Quantum Machine Learning", "categories": ["quant-ph", "cs.CV", "cs.LG", "68T05, 68Q10, 68T07", "I.2.6; I.4.8; F.1.2"], "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u7528\u4e8eDevanagari\u624b\u5199\u6570\u5b57\u8bc6\u522b\uff0c\u7ed3\u5408CNN\u548c\u91cf\u5b50\u7535\u8def\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "Devanagari\u7b49\u533a\u57df\u811a\u672c\u7684\u624b\u5199\u6570\u5b57\u8bc6\u522b\u5bf9\u591a\u8bed\u8a00\u6587\u6863\u6570\u5b57\u5316\u3001\u6559\u80b2\u5de5\u5177\u548c\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u590d\u6742\u7ed3\u6784\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u5bf9\u4f20\u7edf\u6a21\u578b\u6784\u6210\u6311\u6218\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u7ed3\u540810\u91cf\u5b50\u6bd4\u7279\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u8fdb\u884c\u91cf\u5b50\u589e\u5f3a\u5206\u7c7b\u3002", "result": "\u5728Devanagari\u624b\u5199\u5b57\u7b26\u6570\u636e\u96c6\uff08DHCD\uff09\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe99.80%\uff0c\u635f\u5931\u4e3a0.2893\uff0c\u5e73\u5747\u6bcf\u7c7bF1\u5206\u6570\u4e3a0.9980\uff0c\u4f18\u4e8e\u7ecf\u5178CNN\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u91cf\u5b50\u53e0\u52a0\u548c\u7ea0\u7f20\u539f\u7406\uff0c\u4e3a\u533a\u57df\u811a\u672c\u8bc6\u522b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09075", "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b80\u5355\u6846\u67b6\uff0c\u7528\u4e8e\u8fd0\u52a8\u63d2\u503c\u4efb\u52a1\uff0c\u5f3a\u8c03\u6570\u636e\u5efa\u6a21\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u8fd0\u52a8\u63d2\u503c\u662f\u52a8\u753b\u5e08\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7b80\u5355\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u6570\u636e\u4f18\u5316\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u753b\u3002", "method": "\u91c7\u7528\u5355\u4e00Transformer\u7f16\u7801\u5668\u6846\u67b6\uff0c\u91cd\u70b9\u7814\u7a76\u6570\u636e\u5efa\u6a21\u9009\u62e9\uff08\u5982\u6570\u636e\u91cf\u3001\u59ff\u6001\u8868\u793a\u548c\u901f\u5ea6\u8f93\u5165\u7279\u5f81\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6570\u636e\u91cf\u3001\u4f18\u5316\u59ff\u6001\u8868\u793a\u548c\u5f15\u5165\u901f\u5ea6\u7279\u5f81\u53ef\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u63d2\u503c\u8d28\u91cf\uff0c\u6311\u6218\u4e86\u6a21\u578b\u590d\u6742\u6027\u51b3\u5b9a\u6027\u80fd\u7684\u5047\u8bbe\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u6570\u636e\u4f18\u5316\u6bd4\u6a21\u578b\u590d\u6742\u6027\u66f4\u80fd\u63d0\u5347\u8fd0\u52a8\u63d2\u503c\u8d28\u91cf\uff0c\u4e3a\u52a8\u753b\u9886\u57df\u63d0\u4f9b\u4e86\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09076", "pdf": "https://arxiv.org/pdf/2506.09076", "abs": "https://arxiv.org/abs/2506.09076", "authors": ["Haley Stone", "Jing Du", "Hao Xue", "Matthew Scotch", "David Heslop", "Andreas Z\u00fcfle", "Chandini Raina MacIntyre", "Flora Salim"], "title": "A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models", "categories": ["q-bio.GN", "cs.LG", "q-bio.PE"], "comment": "9 pages, 3 figures", "summary": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u65ad\u672a\u6d4b\u5e8f\u75c5\u4f8b\u4e0e\u5df2\u77e5\u5e8f\u5217\u4e4b\u95f4\u7684\u9057\u4f20\u8ddd\u79bb\uff0c\u652f\u6301\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u3002", "motivation": "\u75c5\u539f\u4f53\u57fa\u56e0\u7ec4\u6570\u636e\u5728\u7a7a\u95f4\u6a21\u578b\u4e2d\u6709\u7528\uff0c\u4f46\u6d4b\u5e8f\u8986\u76d6\u7387\u4e0d\u8db3\u9650\u5236\u4e86\u5176\u6548\u7528\u3002", "method": "\u57fa\u4e8e\u65f6\u95f4\u611f\u77e5\u7684\u8fdb\u5316\u8ddd\u79bb\u5efa\u6a21\uff0c\u4f30\u8ba1\u672a\u6d4b\u5e8f\u75c5\u4f8b\u4e0e\u5df2\u77e5\u5e8f\u5217\u7684\u9057\u4f20\u8ddd\u79bb\uff0c\u65e0\u9700\u5e8f\u5217\u6bd4\u5bf9\u6216\u5df2\u77e5\u4f20\u64ad\u94fe\u3002", "result": "\u5e94\u7528\u4e8e\u9ad8\u81f4\u75c5\u6027\u79bd\u6d41\u611fA/H5\u75c5\u4f8b\uff0c\u652f\u6301\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u548c\u65f6\u7a7a\u5efa\u6a21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u57fa\u56e0\u7ec4\u6570\u636e\u5728\u65f6\u7a7a\u5efa\u6a21\u4e2d\u7684\u6574\u5408\u80fd\u529b\u3002"}}
{"id": "2506.09082", "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "AVA-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u802614\u79cd\u539f\u5b50\u89c6\u89c9\u80fd\u529b\uff08AVAs\uff09\u6765\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VQA\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e24\u4e2a\u76f2\u70b9\u3002", "motivation": "\u73b0\u6709VQA\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u548c\u591a\u80fd\u529b\u8026\u5408\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30VFMs\u7684\u89c6\u89c9\u80fd\u529b\u3002", "method": "\u5f15\u5165AVA-Bench\uff0c\u901a\u8fc7\u89e3\u802614\u79cdAVAs\u5e76\u5728\u6bcf\u79cd\u80fd\u529b\u5185\u5339\u914d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\uff0c\u7cbe\u51c6\u8bc4\u4f30VFMs\u7684\u8868\u73b0\u3002", "result": "AVA-Bench\u63ed\u793a\u4e86VFMs\u7684\u72ec\u7279\u201c\u80fd\u529b\u6307\u7eb9\u201d\uff0c\u5e76\u53d1\u73b00.5B LLM\u5728\u8bc4\u4f30\u6548\u7387\u4e0a\u4f18\u4e8e7B LLM\u3002", "conclusion": "AVA-Bench\u4e3a\u4e0b\u4e00\u4ee3VFMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5168\u9762\u900f\u660e\u7684\u8bc4\u4f30\u57fa\u7840\u3002"}}
{"id": "2506.09097", "pdf": "https://arxiv.org/pdf/2506.09097", "abs": "https://arxiv.org/abs/2506.09097", "authors": ["R\u00e9mi Vaucher", "St\u00e9phane Chr\u00e9tien"], "title": "Detecting malignant dynamics on very few blood sample using signature coefficients", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "comment": "Under review", "summary": "Recent discoveries have suggested that the promising avenue of using\ncirculating tumor DNA (ctDNA) levels in blood samples provides reasonable\naccuracy for cancer monitoring, with extremely low burden on the patient's\nside. It is known that the presence of ctDNA can result from various mechanisms\nleading to DNA release from cells, such as apoptosis, necrosis or active\nsecretion. One key idea in recent cancer monitoring studies is that monitoring\nthe dynamics of ctDNA levels might be sufficient for early multi-cancer\ndetection. This interesting idea has been turned into commercial products, e.g.\nin the company named GRAIL.\n  In the present work, we propose to explore the use of Signature theory for\ndetecting aggressive cancer tumors based on the analysis of blood samples. Our\napproach combines tools from continuous time Markov modelling for the dynamics\nof ctDNA levels in the blood, with Signature theory for building efficient\ntesting procedures. Signature theory is a topic of growing interest in the\nMachine Learning community (see Chevyrev2016 and Fermanian2021), which is now\nrecognised as a powerful feature extraction tool for irregularly sampled\nsignals. The method proposed in the present paper is shown to correctly address\nthe challenging problem of overcoming the inherent data scarsity due to the\nextremely small number of blood samples per patient. The relevance of our\napproach is illustrated with extensive numerical experiments that confirm the\nefficiency of the proposed pipeline.", "AI": {"tldr": "\u5229\u7528\u5faa\u73af\u80bf\u7624DNA\uff08ctDNA\uff09\u6c34\u5e73\u548c\u7b7e\u540d\u7406\u8bba\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u764c\u75c7\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "ctDNA\u6c34\u5e73\u76d1\u6d4b\u4e3a\u764c\u75c7\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4f4e\u8d1f\u62c5\u4e14\u9ad8\u51c6\u786e\u6027\u7684\u53ef\u80fd\uff0c\u4f46\u6570\u636e\u7a00\u758f\u6027\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7b7e\u540d\u7406\u8bba\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5de5\u5177\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u5206\u6790ctDNA\u52a8\u6001\uff0c\u5e76\u5229\u7528\u7b7e\u540d\u7406\u8bba\u6784\u5efa\u9ad8\u6548\u68c0\u6d4b\u6d41\u7a0b\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u514b\u670d\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u8840\u6db2\u6837\u672c\u7684\u764c\u75c7\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09100", "pdf": "https://arxiv.org/pdf/2506.09100", "abs": "https://arxiv.org/abs/2506.09100", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "AI": {"tldr": "LoREIN\u662f\u4e00\u79cd\u65b0\u578b\u65e0\u76d1\u7763\u53cc\u5148\u9a8c\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f3D\u591a\u53c2\u6570\u5b9a\u91cfMRI\u91cd\u5efa\uff0c\u7ed3\u5408\u4f4e\u79e9\u548c\u8fde\u7eed\u6027\u5148\u9a8c\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u91cd\u5efa\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u4e00\u5148\u9a8c\u6216\u7269\u7406\u6a21\u578b\uff0c\u96be\u4ee5\u89e3\u51b3\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u7684\u9006\u95ee\u9898\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "LoREIN\u7ed3\u5408\u4f4e\u79e9\u8868\u793a\uff08LRR\uff09\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\uff0c\u5229\u7528\u4f4e\u79e9\u548c\u8fde\u7eed\u6027\u5148\u9a8c\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u4fdd\u771f\u91cd\u5efa\u52a0\u6743\u56fe\u50cf\uff0c\u5e76\u63d0\u9ad8\u5b9a\u91cf\u53c2\u6570\u56fe\u7684\u91cd\u5efa\u51c6\u786e\u6027\u3002", "conclusion": "LoREIN\u4e3a\u96f6\u6837\u672c\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09106", "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u6a21\u578b\u4e2d\u504f\u89c1\u7684\u673a\u5236\uff0c\u53d1\u73b0\u504f\u89c1\u53d8\u5316\u8f83\u5c0f\uff0c\u4f46\u5bf9\u5c5e\u6027\u5206\u7c7b\u5668\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u5c5e\u6027\u4e0a\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u5bf9\u5176\u6f5c\u5728\u504f\u89c1\u548c\u6b67\u89c6\u6027\u7ed3\u679c\u7684\u62c5\u5fe7\uff0c\u4f46\u504f\u89c1\u4ea7\u751f\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bad\u7ec3\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u5e38\u7528\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u8bad\u7ec3\u4e0e\u751f\u6210\u5206\u5e03\u95f4\u7684\u504f\u89c1\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u504f\u89c1\u53d8\u5316\u8f83\u5c0f\uff0c\u4f46\u5c5e\u6027\u5206\u7c7b\u5668\u7684\u9009\u62e9\u5bf9\u7ed3\u679c\u654f\u611f\uff0c\u5c24\u5176\u662f\u8fde\u7eed\u5c5e\u6027\u3002", "conclusion": "\u9700\u6539\u8fdb\u6807\u7b7e\u5b9e\u8df5\uff0c\u6df1\u5165\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8003\u8651\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u3002"}}
{"id": "2506.09176", "pdf": "https://arxiv.org/pdf/2506.09176", "abs": "https://arxiv.org/abs/2506.09176", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "ICML 2025 Poster", "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "AI": {"tldr": "AIM\u662f\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u4eba\u95e8\u63a7\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5e72\u9884\u8bf7\u6c42\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u7c7b\u76d1\u7763\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u76d1\u7763\u8005\u7684\u8ba4\u77e5\u8981\u6c42\u8fc7\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5e72\u9884\u673a\u5236\u3002", "method": "AIM\u5229\u7528\u4ee3\u7406Q\u51fd\u6570\u6a21\u62df\u4eba\u7c7b\u5e72\u9884\u89c4\u5219\uff0c\u6839\u636e\u4ee3\u7406\u4e0e\u4eba\u7c7b\u52a8\u4f5c\u7684\u5bf9\u9f50\u7a0b\u5ea6\u52a8\u6001\u8c03\u6574\u5e72\u9884\u8bf7\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIM\u5728\u8fde\u7eed\u548c\u79bb\u6563\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u4e13\u5bb6\u76d1\u63a7\u6210\u672c\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5Thrifty-DAgger\u63d0\u534740%\u7684\u6548\u7387\u3002", "conclusion": "AIM\u80fd\u6709\u6548\u8bc6\u522b\u5b89\u5168\u5173\u952e\u72b6\u6001\uff0c\u6536\u96c6\u66f4\u9ad8\u8d28\u91cf\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u51cf\u5c11\u6574\u4f53\u6570\u636e\u9700\u6c42\u548c\u73af\u5883\u4ea4\u4e92\u3002"}}
{"id": "2506.09209", "pdf": "https://arxiv.org/pdf/2506.09209", "abs": "https://arxiv.org/abs/2506.09209", "authors": ["Leandro Anghinoni", "Pablo Zivic", "Jorge Adrian Sanchez"], "title": "Revisiting Graph Projections for Effective Complementary Product Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Complementary product recommendation is a powerful strategy to improve\ncustomer experience and retail sales. However, recommending the right product\nis not a simple task because of the noisy and sparse nature of user-item\ninteractions. In this work, we propose a simple yet effective method to predict\na list of complementary products given a query item, based on the structure of\na directed weighted graph projected from the user-item bipartite graph. We\nrevisit bipartite graph projections for recommender systems and propose a novel\napproach for inferring complementarity relationships from historical user-item\ninteractions. We compare our model with recent methods from the literature and\nshow, despite the simplicity of our approach, an average improvement of +43%\nand +38% over sequential and graph-based recommenders, respectively, over\ndifferent benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237-\u7269\u54c1\u4e8c\u5206\u56fe\u6295\u5f71\u7684\u7b80\u5355\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u8350\u4e92\u8865\u4ea7\u54c1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e92\u8865\u4ea7\u54c1\u63a8\u8350\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u96f6\u552e\u9500\u552e\u989d\uff0c\u4f46\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u7a00\u758f\u4e14\u566a\u58f0\u591a\uff0c\u5bfc\u81f4\u63a8\u8350\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5c06\u7528\u6237-\u7269\u54c1\u4e8c\u5206\u56fe\u6295\u5f71\u4e3a\u6709\u5411\u52a0\u6743\u56fe\uff0c\u63a8\u65ad\u4e92\u8865\u5173\u7cfb\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u5206\u522b\u6bd4\u5e8f\u5217\u548c\u57fa\u4e8e\u56fe\u7684\u63a8\u8350\u65b9\u6cd5\u63d0\u534743%\u548c38%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u4f46\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e92\u8865\u4ea7\u54c1\u63a8\u8350\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09237", "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "PatchGuard\u662f\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u5bf9\u6297\u9c81\u68d2\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u5f02\u5e38\u6837\u672c\u548c\u5b9a\u4f4d\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u8bad\u7ec3\u6570\u636e\u9650\u5236\uff08\u4ec5\u542b\u6b63\u5e38\u6837\u672c\uff09\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0cPatchGuard\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\uff0c\u7ed3\u5408ViT\u67b6\u6784\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u91c7\u7528\u65b0\u578b\u635f\u5931\u51fd\u6570\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cPatchGuard\u5728\u5bf9\u6297\u73af\u5883\u4e0bAD\u6027\u80fd\u63d0\u534753.2%\uff0cAL\u63d0\u534768.5%\uff0c\u4e14\u5728\u975e\u5bf9\u6297\u73af\u5883\u4e0b\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "PatchGuard\u901a\u8fc7\u4f2a\u5f02\u5e38\u6837\u672c\u548c\u7406\u8bba\u6307\u5bfc\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.09250", "pdf": "https://arxiv.org/pdf/2506.09250", "abs": "https://arxiv.org/abs/2506.09250", "authors": ["C. Opus", "A. Lawsen"], "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.LG"], "comment": "Comment on: arXiv:2506.06941", "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "AI": {"tldr": "Shojaee\u7b49\u4eba\uff082025\uff09\u7684\u7814\u7a76\u6307\u51fa\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u89c4\u5212\u8c1c\u9898\u4e0a\u5b58\u5728\u201c\u51c6\u786e\u6027\u5d29\u6e83\u201d\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u5176\u7ed3\u8bba\u6e90\u4e8e\u5b9e\u9a8c\u8bbe\u8ba1\u7f3a\u9677\u800c\u975e\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u63ed\u793aShojaee\u7b49\u4eba\u7814\u7a76\u4e2d\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u91cd\u65b0\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u5206\u6790\u539f\u7814\u7a76\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u95ee\u9898\uff0c\u5305\u62ec\u8f93\u51fa\u4ee4\u724c\u9650\u5236\u3001\u8bc4\u4f30\u6846\u67b6\u7f3a\u9677\u548c\u6570\u5b66\u4e0a\u4e0d\u53ef\u80fd\u7684\u95ee\u9898\u5b9e\u4f8b\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u5b9e\u9a8c\u8bbe\u8ba1\u91cd\u65b0\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u63a7\u5236\u5b9e\u9a8c\u8bbe\u8ba1\u7f3a\u9677\u540e\uff0c\u6a21\u578b\u5728Tower of Hanoi\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u539f\u7814\u7a76\u7684\u5931\u8d25\u7ed3\u8bba\u4e0d\u51c6\u786e\u3002", "conclusion": "\u5b9e\u9a8c\u8bbe\u8ba1\u5bf9\u8bc4\u4f30AI\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u907f\u514d\u8bef\u5bfc\u6027\u7ed3\u8bba\u3002"}}
{"id": "2506.09255", "pdf": "https://arxiv.org/pdf/2506.09255", "abs": "https://arxiv.org/abs/2506.09255", "authors": ["Saeed Hashemi", "Genchang Peng", "Mehrdad Nourani", "Omar Nofal", "Jay Harvey"], "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted to be presented at the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025). This\n  version is submitted to arXiv prior to final IEEE formatting and publication", "summary": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684SEEG\u901a\u9053\u6392\u5e8f\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e34\u5e8a\u9009\u62e9\u548c\u8ba1\u7b97\u53d1\u73b0\uff0c\u901a\u8fc7XGBoost\u548cSHAP\u8bc4\u5206\u63d0\u9ad8\u766b\u75eb\u533a\u57df\u8bc6\u522b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "SEEG\u4fe1\u53f7\u7684\u4eba\u5de5\u68c0\u67e5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u81ea\u52a8\u5316\u65b9\u6cd5\u8f85\u52a9\u8bc6\u522b\u766b\u75eb\u533a\u57df\u3002", "method": "\u4f7f\u7528XGBoost\u5206\u7c7b\u6a21\u578b\u5b66\u4e60\u53d1\u4f5c\u671f\u901a\u9053\u7279\u5f81\uff0c\u7ed3\u5408SHAP\u8bc4\u5206\u6392\u5e8f\u901a\u9053\uff0c\u5e76\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\u4ee5\u53d1\u73b0\u66f4\u591a\u53ef\u7591\u533a\u57df\u3002", "result": "\u5728\u4e94\u540d\u60a3\u8005\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\uff0c\u63d0\u9ad8\u766b\u75eb\u533a\u57df\u8bc6\u522b\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.09278", "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5149\u6d41\u4e0e\u5339\u914d\u6a21\u578b\uff08UFM\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u6570\u636e\uff0c\u4f7f\u7528\u7b80\u5355\u7684Transformer\u67b6\u6784\u76f4\u63a5\u56de\u5f52(u,v)\u5149\u6d41\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5bbd\u57fa\u7ebf\u548c\u5149\u6d41\u4f30\u8ba1\u4e2d\u5206\u522b\u5904\u7406\u5bc6\u96c6\u5bf9\u5e94\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u7edf\u4e00\u8bad\u7ec3\u662f\u5426\u80fd\u8d85\u8d8a\u4e13\u7528\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u548c\u7b80\u5355\u7684Transformer\u67b6\u6784\uff0c\u76f4\u63a5\u56de\u5f52(u,v)\u5149\u6d41\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7c97\u5230\u7ec6\u6210\u672c\u4f53\u79ef\u7684\u590d\u6742\u6027\u3002", "result": "UFM\u6bd4\u6700\u5148\u8fdb\u7684\u5149\u6d41\u65b9\u6cd5\uff08Unimatch\uff09\u51c6\u786e\u7387\u9ad828%\uff0c\u6bd4\u5bc6\u96c6\u5bbd\u57fa\u7ebf\u5339\u914d\u5668\uff08RoMa\uff09\u8bef\u5dee\u4f4e62%\uff0c\u901f\u5ea6\u5feb6.7\u500d\u3002", "conclusion": "UFM\u9996\u6b21\u8bc1\u660e\u7edf\u4e00\u8bad\u7ec3\u53ef\u5728\u4e24\u4e2a\u9886\u57df\u4e2d\u8d85\u8d8a\u4e13\u7528\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u3001\u957f\u8ddd\u79bb\u548c\u5b9e\u65f6\u5bf9\u5e94\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09299", "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u8282\u80fd\u7684\u7a7a\u4e2d\u5e94\u6025\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u65b9\u6848\uff0c\u57fa\u4e8eYOLOv4-Tiny\u6a21\u578b\uff0c\u901a\u8fc7INT8\u91cf\u5316\u4f18\u5316\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u7f3a\u4e4f\u65e0\u4eba\u673a\u89c6\u89d2\u7684\u5e94\u6025\u56fe\u50cf\uff0c\u56e0\u6b64\u4f5c\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,820\u5f20\u6807\u6ce8\u56fe\u50cf\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5e76\u4f18\u5316\u6a21\u578b\u4ee5\u9002\u5e94\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u3002", "method": "\u91c7\u7528YOLOv4-Tiny\u6a21\u578b\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u91cf\u5316\u81f3INT8\u7cbe\u5ea6\uff0c\u5e76\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u91cf\u5316\u540e\u7684YOLOv4-Tiny\u6a21\u578b\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4e0eYOLOv5-small\u76f8\u5f53\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u51cf\u5c1171%\uff0822.5 MB\u81f36.4 MB\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534744%\u3002", "conclusion": "\u91cf\u5316\u540e\u7684YOLOv4-Tiny\u6a21\u578b\u9002\u5408\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u68c0\u6d4b\u5e94\u6025\u573a\u666f\u3002"}}
{"id": "2506.09312", "pdf": "https://arxiv.org/pdf/2506.09312", "abs": "https://arxiv.org/abs/2506.09312", "authors": ["Erik Buchholz", "Natasha Fernandes", "David D. Nguyen", "Alsharif Abuadbba", "Surya Nepal", "Salil S. Kanhere"], "title": "What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u751f\u6210\u5408\u6210\u8f68\u8ff9\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5e94\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5bf9\u6548\u7528-\u9690\u79c1\u6743\u8861\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u65b0\u7684DP\u673a\u5236\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u8f68\u8ff9\u6570\u636e\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u5dee\u5206\u9690\u79c1\u63d0\u4f9b\u4fdd\u62a4\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u6b63\u5f0f\u9690\u79c1\u4fdd\u8bc1\uff0c\u4e14\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u6761\u4ef6\u4fe1\u606f\u3002", "method": "\u8bc4\u4f30DP-SGD\u5bf9\u751f\u6210\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u65b0\u7684DP\u673a\u5236\u7528\u4e8e\u6761\u4ef6\u751f\u6210\uff0c\u5e76\u6bd4\u8f83\u6269\u6563\u6a21\u578b\u3001VAE\u548cGAN\u7684\u8868\u73b0\u3002", "result": "DP-SGD\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u5927\u6570\u636e\u96c6\u4e0b\u4ecd\u4fdd\u7559\u90e8\u5206\u6548\u7528\uff1b\u65b0DP\u673a\u5236\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b\u6269\u6563\u6a21\u578b\u5728\u65e0\u9690\u79c1\u4fdd\u8bc1\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u800cGAN\u5728DP-SGD\u4e0b\u6700\u4f18\u3002", "conclusion": "DP\u8f68\u8ff9\u751f\u6210\u4ecd\u5177\u6311\u6218\u6027\uff0c\u6b63\u5f0f\u9690\u79c1\u4fdd\u8bc1\u76ee\u524d\u4ec5\u9002\u7528\u4e8e\u5927\u6570\u636e\u96c6\u548c\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2506.09313", "pdf": "https://arxiv.org/pdf/2506.09313", "abs": "https://arxiv.org/abs/2506.09313", "authors": ["Angel Yanguas-Gil", "Jeffrey W. Elam"], "title": "Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.plasm-ph"], "comment": null, "summary": "In this work we explore surrogate models to optimize plasma enhanced atomic\nlayer deposition (PEALD) in high aspect ratio features. In plasma-based\nprocesses such as PEALD and atomic layer etching, surface recombination can\ndominate the reactivity of plasma species with the surface, which can lead to\nunfeasibly long exposure times to achieve full conformality inside\nnanostructures like high aspect ratio vias. Using a synthetic dataset based on\nsimulations of PEALD, we train artificial neural networks to predict saturation\ntimes based on cross section thickness data obtained for partially coated\nconditions. The results obtained show that just two experiments in\nundersaturated conditions contain enough information to predict saturation\ntimes within 10% of the ground truth. A surrogate model trained to determine\nwhether surface recombination dominates the plasma-surface interactions in a\nPEALD process achieves 99% accuracy. This demonstrates that machine learning\ncan provide a new pathway to accelerate the optimization of PEALD processes in\nareas such as microelectronics. Our approach can be easily extended to atomic\nlayer etching and more complex structures.", "AI": {"tldr": "\u5229\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u548c\u5408\u6210\u6570\u636e\u9884\u6d4bPEALD\u5de5\u827a\u7684\u9971\u548c\u65f6\u95f4\uff0c\u4ec5\u9700\u4e24\u6b21\u5b9e\u9a8c\u5373\u53ef\u572810%\u8bef\u5dee\u5185\u9884\u6d4b\u7ed3\u679c\uff0c\u540c\u65f6\u6a21\u578b\u5bf9\u8868\u9762\u91cd\u7ec4\u4e3b\u5bfc\u7684\u4ea4\u4e92\u5224\u65ad\u51c6\u786e\u7387\u8fbe99%\u3002", "motivation": "\u89e3\u51b3PEALD\u5de5\u827a\u4e2d\u56e0\u8868\u9762\u91cd\u7ec4\u5bfc\u81f4\u7684\u9ad8\u6df1\u5bbd\u6bd4\u7ed3\u6784\u5185\u9971\u548c\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u9971\u548c\u65f6\u95f4\u5e76\u5224\u65ad\u8868\u9762\u91cd\u7ec4\u662f\u5426\u4e3b\u5bfc\u3002", "result": "\u4ec5\u9700\u4e24\u6b21\u5b9e\u9a8c\u5373\u53ef\u9884\u6d4b\u9971\u548c\u65f6\u95f4\uff08\u8bef\u5dee10%\uff09\uff0c\u6a21\u578b\u5224\u65ad\u51c6\u786e\u7387\u8fbe99%\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e3aPEALD\u5de5\u827a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u590d\u6742\u5de5\u827a\u3002"}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08Mistral-7B\uff09\u7684\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u51c6\u786e\u7387\u8f83\u73b0\u6709\u65b9\u6cd5\u63d0\u53473.33%-6.35%\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5e38\u4f34\u968f\u8bed\u8a00\u80fd\u529b\u4e0b\u964d\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u6307\u4ee4\u8ddf\u968f\u7248\u672c\u7684Mistral-7B\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6269\u5c55\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5206\u6790\u8bed\u8a00\u6a21\u5f0f\u3002", "result": "\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53473.33%\u548c6.35%\uff0c\u4e14\u51b3\u7b56\u8fb9\u754c\u6e05\u6670\u53ef\u89e3\u91ca\u3002\u6a21\u578b\u8fd8\u5b66\u4e60\u4e86AD\u60a3\u8005\u7684\u7279\u6b8a\u8bed\u8a00\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AD\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09338", "pdf": "https://arxiv.org/pdf/2506.09338", "abs": "https://arxiv.org/abs/2506.09338", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u6821\u51c6\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u5b9e\u4f8b\u81ea\u9002\u5e94\u7f29\u653e\uff08IAS\uff09\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u63a8\u7406\u9884\u7b97\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709PRMs\u6821\u51c6\u4e0d\u4f73\u4e14\u5e38\u9ad8\u4f30\u6210\u529f\u6982\u7387\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5206\u4f4d\u6570\u56de\u5f52\u6821\u51c6PRMs\u8f93\u51fa\uff0c\u5e76\u57fa\u4e8e\u6821\u51c6\u540e\u7684\u6210\u529f\u6982\u7387\u548c\u7f6e\u4fe1\u8fb9\u754c\u8bbe\u8ba1IAS\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6821\u51c6\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0cIAS\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u6821\u51c6PRMs\u548cIAS\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e0e\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO\u901a\u8fc7\u5229\u7528\u591a\u6837\u5316\u7684\u56de\u653e\u7b56\u7565\u4ece\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u68c0\u7d22\u79bb\u7b56\u7565\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u6548\u7387\uff0c\u76f8\u6bd4GRPO\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u660e\u663e\u6539\u8fdb\u3002", "motivation": "GRPO\u65b9\u6cd5\u56e0\u4f9d\u8d56\u591a\u4e2a\u540c\u7b56\u7565\u8f93\u51fa\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u636e\u6548\u7387\u4f4e\uff0cRePO\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u79bb\u7b56\u7565\u6837\u672c\u4f18\u5316\u7b56\u7565\u3002", "method": "RePO\u91c7\u7528\u591a\u6837\u5316\u7684\u56de\u653e\u7b56\u7565\u4ece\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u68c0\u7d22\u79bb\u7b56\u7565\u6837\u672c\uff0c\u4ee5\u66f4\u5e7f\u6cdb\u7684\u6837\u672c\u96c6\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRePO\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u589e\u52a015%\uff0c\u4f46\u6709\u6548\u4f18\u5316\u6b65\u9aa4\u589e\u52a0\u4e8648%\u3002", "conclusion": "RePO\u901a\u8fc7\u7ed3\u5408\u79bb\u7b56\u7565\u6837\u672c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344", "abs": "https://arxiv.org/abs/2506.09344", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "AI": {"tldr": "Ming-Omni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u5904\u7406\uff0c\u5e76\u5728\u8bed\u97f3\u548c\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u9700\u8981\u5355\u72ec\u6a21\u578b\u6216\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u6846\u67b6\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u548c\u751f\u6210\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e13\u7528\u7f16\u7801\u5668\u63d0\u53d6\u591a\u6a21\u6001\u6807\u8bb0\uff0c\u7ed3\u5408MoE\u67b6\u6784\u548c\u65b0\u63d0\u51fa\u7684\u6a21\u6001\u7279\u5b9a\u8def\u7531\u5668\uff0c\u96c6\u6210\u97f3\u9891\u89e3\u7801\u5668\u548c\u56fe\u50cf\u751f\u6210\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMing-Omni\u5728\u7edf\u4e00\u611f\u77e5\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u804a\u5929\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u56fe\u50cf\u7f16\u8f91\u3002", "conclusion": "Ming-Omni\u662f\u9996\u4e2a\u5f00\u6e90\u652f\u6301\u591a\u6a21\u6001\u5339\u914dGPT-4o\u7684\u6a21\u578b\uff0c\u4ee3\u7801\u548c\u6743\u91cd\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.09350", "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u6027\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u5668\uff0c\u652f\u6301\u5355\u6b65\u751f\u6210\u548c\u5b9e\u65f6\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u5355\u6b65\u751f\u6210\u6f5c\u5728\u5e27\uff0c\u5e76\u5229\u7528KV\u7f13\u5b58\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u5b66\u751f\u5f3a\u5236\u8bad\u7ec3\u51cf\u5c11\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "8B\u6a21\u578b\u5728\u5355H100\u4e0a\u5b9e\u73b024fps\u7684736x416\u5206\u8fa8\u7387\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u6216\u57288xH100\u4e0a\u652f\u63011280x720\u5206\u8fa8\u7387\u957f\u8fbe1\u5206\u949f\u7684\u89c6\u9891\u751f\u6210\u3002", "conclusion": "AAPT\u65b9\u6cd5\u5728\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.09366", "pdf": "https://arxiv.org/pdf/2506.09366", "abs": "https://arxiv.org/abs/2506.09366", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "AI": {"tldr": "SkillBlender\u662f\u4e00\u4e2a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u4efb\u52a1\u65e0\u5173\u7684\u539f\u59cb\u6280\u80fd\u5e76\u52a8\u6001\u6df7\u5408\u5b83\u4eec\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u7684\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u7e41\u7410\u7684\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0cSkillBlender\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SkillBlender\u9884\u8bad\u7ec3\u76ee\u6807\u6761\u4ef6\u7684\u4efb\u52a1\u65e0\u5173\u539f\u59cb\u6280\u80fd\uff0c\u5e76\u52a8\u6001\u6df7\u5408\u8fd9\u4e9b\u6280\u80fd\u4ee5\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u5de5\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkillBlender\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u884c\u4e3a\u66f4\u51c6\u786e\u4e14\u53ef\u884c\u3002", "conclusion": "SkillBlender\u4e3a\u591a\u6837\u5316\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u57fa\u51c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.09401", "pdf": "https://arxiv.org/pdf/2506.09401", "abs": "https://arxiv.org/abs/2506.09401", "authors": ["Vivek Shripad Borkar"], "title": "A theoretical basis for model collapse in recursive training", "categories": ["math.PR", "cs.LG", "68T01"], "comment": null, "summary": "It is known that recursive training from generative models can lead to the so\ncalled `collapse' of the simulated probability distribution. This note shows\nthat one in fact gets two different asymptotic behaviours depending on whether\nan external source, howsoever minor, is also contributing samples.", "AI": {"tldr": "\u9012\u5f52\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u6a21\u62df\u6982\u7387\u5206\u5e03\u7684\u201c\u5d29\u6e83\u201d\uff0c\u4f46\u7814\u7a76\u8868\u660e\uff0c\u6839\u636e\u662f\u5426\u6709\u5916\u90e8\u6837\u672c\u6e90\u7684\u53c2\u4e0e\uff0c\u4f1a\u51fa\u73b0\u4e24\u79cd\u4e0d\u540c\u7684\u6e10\u8fd1\u884c\u4e3a\u3002", "motivation": "\u63a2\u8ba8\u9012\u5f52\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u65f6\u6982\u7387\u5206\u5e03\u5d29\u6e83\u7684\u73b0\u8c61\uff0c\u5e76\u7814\u7a76\u5916\u90e8\u6837\u672c\u6e90\u5bf9\u6e10\u8fd1\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u9012\u5f52\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u5916\u90e8\u6837\u672c\u6e90\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u6709\u65e0\u5916\u90e8\u6837\u672c\u6e90\u4f1a\u5bfc\u81f4\u4e24\u79cd\u4e0d\u540c\u7684\u6e10\u8fd1\u884c\u4e3a\u3002", "conclusion": "\u5916\u90e8\u6837\u672c\u6e90\u7684\u53c2\u4e0e\u663e\u8457\u5f71\u54cd\u9012\u5f52\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u907f\u514d\u5d29\u6e83\u73b0\u8c61\u3002"}}
{"id": "2506.09406", "pdf": "https://arxiv.org/pdf/2506.09406", "abs": "https://arxiv.org/abs/2506.09406", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u817f\u90e8\u7684\u654f\u6377\u6027\u6536\u96c6\u7269\u4f53\uff0c\u65e0\u9700\u989d\u5916\u6267\u884c\u5668\u3002", "motivation": "\u63a2\u7d22\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u8d85\u8d8a\u9759\u6001\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u7ed3\u6784\uff0c\u5305\u62ec\u4e24\u4e2a\u4e13\u5bb6\u7b56\u7565\uff08\u6293\u53d6\u548c\u629b\u63b7\u3001\u63a5\u8fd1\u7269\u4f53\u4f4d\u7f6e\uff09\u548c\u4e00\u4e2a\u52a8\u6001\u5207\u6362\u7684\u5143\u7b56\u7565\u3002", "result": "\u5c55\u793a\u4e86\u56db\u8db3\u673a\u5668\u4eba\u817f\u90e8\u5728\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u56db\u8db3\u673a\u5668\u4eba\u817f\u90e8\u7684\u529f\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u8fd0\u52a8\u4efb\u52a1\u3002"}}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420", "abs": "https://arxiv.org/abs/2506.09420", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5b8c\u5168\u81ea\u4e3bAI\u4ee3\u7406\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51faLLM-HAS\uff08\u57fa\u4e8eLLM\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff09\uff0c\u5f3a\u8c03\u4eba\u673a\u534f\u4f5c\u6bd4AI\u72ec\u7acb\u5de5\u4f5c\u66f4\u53ef\u9760\u548c\u7075\u6d3b\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u53ef\u9760\u6027\u3001\u900f\u660e\u5ea6\u548c\u7406\u89e3\u4eba\u7c7b\u9700\u6c42\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u3002", "method": "\u63d0\u51faLLM-HAS\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u63d0\u4f9b\u6307\u5bfc\u548c\u4fdd\u6301\u63a7\u5236\uff0c\u7ed3\u5408\u533b\u7597\u3001\u91d1\u878d\u548c\u8f6f\u4ef6\u5f00\u53d1\u6848\u4f8b\u5c55\u793a\u534f\u4f5c\u4f18\u52bf\u3002", "result": "\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u72ec\u7acbAI\uff0c\u540c\u65f6\u66f4\u5177\u4fe1\u4efb\u5ea6\u548c\u9002\u5e94\u6027\u3002", "conclusion": "AI\u53d1\u5c55\u7684\u76ee\u6807\u4e0d\u5e94\u662f\u72ec\u7acb\u6027\uff0c\u800c\u662f\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u672a\u6765AI\u5e94\u901a\u8fc7\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u5b9e\u73b0\u4ef7\u503c\u3002"}}
{"id": "2506.09422", "pdf": "https://arxiv.org/pdf/2506.09422", "abs": "https://arxiv.org/abs/2506.09422", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u7edf\u4e00\u7684\u6269\u6563\u7b56\u7565\uff08TUDP\uff09\uff0c\u901a\u8fc7\u52a8\u4f5c\u8bc6\u522b\u80fd\u529b\u6784\u5efa\u65f6\u95f4\u7edf\u4e00\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u5b9e\u65f6\u6027\u548c\u52a8\u4f5c\u51c6\u786e\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9700\u8981\u5927\u91cf\u65f6\u95f4\u8fed\u4ee3\u53bb\u566a\uff0c\u4e14\u65f6\u95f4\u53d8\u5316\u7684\u53bb\u566a\u8fc7\u7a0b\u589e\u52a0\u4e86\u6a21\u578b\u8bad\u7ec3\u96be\u5ea6\uff0c\u5bfc\u81f4\u52a8\u4f5c\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "TUDP\u6784\u5efa\u4e86\u65f6\u95f4\u7edf\u4e00\u7684\u901f\u5ea6\u573a\uff0c\u5e76\u5f15\u5165\u52a8\u4f5c\u5224\u522b\u5206\u652f\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\uff0c\u901a\u8fc7\u52a8\u4f5c\u7ea7\u8bad\u7ec3\u63d0\u9ad8\u53bb\u566a\u51c6\u786e\u6027\u3002", "result": "\u5728RLBench\u4e0a\uff0cTUDP\u53d6\u5f97\u4e86\u591a\u89c6\u56fe82.6%\u548c\u5355\u89c6\u56fe83.8%\u7684\u6700\u9ad8\u6210\u529f\u7387\uff0c\u4e14\u5728\u8f83\u5c11\u53bb\u566a\u8fed\u4ee3\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "TUDP\u901a\u8fc7\u65f6\u95f4\u7edf\u4e00\u7684\u53bb\u566a\u8fc7\u7a0b\u548c\u52a8\u4f5c\u7ea7\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.09434", "pdf": "https://arxiv.org/pdf/2506.09434", "abs": "https://arxiv.org/abs/2506.09434", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u95ee\u9898\u4e2d\uff0c\u5f02\u8d28\u6027\u56e2\u961f\u4f55\u65f6\u4f18\u4e8e\u540c\u8d28\u6027\u56e2\u961f\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u8bbe\u8ba1\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u4e86\u5f02\u8d28\u6027\u7684\u4f18\u52bf\u3002", "motivation": "\u63a2\u8ba8\u5f02\u8d28\u6027\u56e2\u961f\u5728\u4efb\u52a1\u5206\u914d\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u7814\u7a76\u5956\u52b1\u8bbe\u8ba1\u5982\u4f55\u4fc3\u8fdb\u5f02\u8d28\u6027\u3002", "method": "\u7ed3\u5408\u5e7f\u4e49\u805a\u5408\u7b97\u5b50\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\uff0c\u63d0\u51faHeterogeneous Environment Design\uff08HED\uff09\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u5b50\u66f2\u7387\u51b3\u5b9a\u5f02\u8d28\u6027\u4f18\u52bf\uff0cHED\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u884c\u4e3a\u591a\u6837\u6027\u4f55\u65f6\u5e26\u6765\u663e\u8457\u4f18\u52bf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u652f\u6301\u3002"}}
{"id": "2506.09441", "pdf": "https://arxiv.org/pdf/2506.09441", "abs": "https://arxiv.org/abs/2506.09441", "authors": ["Piyush Mishra", "Philippe Roudot"], "title": "Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Tracking multiple particles in noisy and cluttered scenes remains challenging\ndue to a combinatorial explosion of trajectory hypotheses, which scales\nsuper-exponentially with the number of particles and frames. The transformer\narchitecture has shown a significant improvement in robustness against this\nhigh combinatorial load. However, its performance still falls short of the\nconventional Bayesian filtering approaches in scenarios presenting a reduced\nset of trajectory hypothesis. This suggests that while transformers excel at\nnarrowing down possible associations, they may not be able to reach the\noptimality of the Bayesian approach in locally sparse scenario. Hence, we\nintroduce a hybrid tracking framework that combines the ability of\nself-attention to learn the underlying representation of particle behavior with\nthe reliability and interpretability of Bayesian filtering. We perform\ntrajectory-to-detection association by solving a label prediction problem,\nusing a transformer encoder to infer soft associations between detections\nacross frames. This prunes the hypothesis set, enabling efficient\nmultiple-particle tracking in Bayesian filtering framework. Our approach\ndemonstrates improved tracking accuracy and robustness against spurious\ndetections, offering a solution for high clutter multiple particle tracking\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u7684\u6df7\u5408\u8ddf\u8e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u548c\u6742\u4e71\u573a\u666f\u4e2d\u9ad8\u6548\u8ddf\u8e2a\u591a\u7c92\u5b50\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5728\u8f68\u8ff9\u5047\u8bbe\u8f83\u5c11\u65f6\u8868\u73b0\u4f18\u4e8eTransformer\uff0c\u4f46Transformer\u5728\u51cf\u5c11\u5173\u8054\u53ef\u80fd\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u56e0\u6b64\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u4ee5\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u901a\u8fc7Transformer\u7f16\u7801\u5668\u63a8\u65ad\u5e27\u95f4\u68c0\u6d4b\u7684\u8f6f\u5173\u8054\uff0c\u4fee\u526a\u5047\u8bbe\u96c6\uff0c\u518d\u5728\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u4e2d\u8fdb\u884c\u591a\u7c92\u5b50\u8ddf\u8e2a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5bf9\u865a\u5047\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u9ad8\u6742\u4e71\u573a\u666f\u3002", "conclusion": "\u6df7\u5408\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u591a\u7c92\u5b50\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "POET\u65b9\u6cd5\u901a\u8fc7\u622a\u65ad\u54cd\u5e94\u957f\u5ea6\u89e3\u51b3DAAs\u4e2d\u7684\u5956\u52b1-\u751f\u6210\u5dee\u8ddd\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "DAAs\uff08\u5982DPO\u548cSimPO\uff09\u5728\u8bad\u7ec3\u76ee\u6807\u548c\u751f\u6210\u6027\u80fd\u95f4\u5b58\u5728\u5956\u52b1-\u751f\u6210\u5dee\u8ddd\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "method": "\u63d0\u51faPOET\u65b9\u6cd5\uff0c\u622a\u65ad\u54cd\u5e94\u81f3\u7b49\u957f\uff0c\u4f18\u5316DAAs\u76ee\u6807\uff0c\u5173\u6ce8\u524d\u7f00\u6807\u8bb0\u3002", "result": "POET\u5728DPO\u548cSimPO\u4e0a\u8868\u73b0\u63d0\u5347\uff0cAlpacaEval 2\u5f97\u5206\u63d0\u9ad815.6\u5206\u3002", "conclusion": "\u89e3\u51b3\u5956\u52b1-\u751f\u6210\u5dee\u8ddd\u5bf9\u63d0\u5347DAAs\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.09487", "pdf": "https://arxiv.org/pdf/2506.09487", "abs": "https://arxiv.org/abs/2506.09487", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.LO", "eess.AS", "I.2.6; H.5.5; I.5.1"], "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "AI": {"tldr": "BemaGANv2\u662f\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u9ad8\u4fdd\u771f\u97f3\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u957f\u671f\u97f3\u9891\u751f\u6210\u6548\u679c\u3002", "motivation": "\u63d0\u5347\u97f3\u9891\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u957f\u671f\u4f9d\u8d56\u6027\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u751f\u6210\u5668\u91c7\u7528AMP\u6a21\u5757\u548cSnake\u6fc0\u6d3b\u51fd\u6570\uff0c\u5224\u522b\u5668\u7ed3\u5408MED\u548cMRD\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "conclusion": "BemaGANv2\u5728\u97f3\u9891\u751f\u6210\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9e\u73b0\u6307\u5357\u3002"}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u8ba1\u7b97\u65b9\u6cd5\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5206\u6790\u4e86YouTube\u4e0a\u81ea\u6740\u884c\u4e3a\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u4e0e\u81ea\u6740\u5c1d\u8bd5\u76f8\u5173\u7684\u884c\u4e3a\u6307\u6807\uff0c\u5e76\u63ed\u793a\u4e86\u5206\u4eab\u52a8\u673a\u7684\u5dee\u5f02\u3002", "motivation": "\u81ea\u6740\u662f\u897f\u65b9\u56fd\u5bb6\u7684\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e3a\u7814\u7a76\u81ea\u6740\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\u81ea\u4e0b\u800c\u4e0a\u7684\u8ba1\u7b97\u5206\u6790\u3001\u6df7\u5408\u65b9\u6cd5\u548c\u4e13\u5bb6\u9a71\u52a8\u7684\u81ea\u4e0a\u800c\u4e0b\u5206\u6790\uff0c\u5bf9181\u4e2a\u6709\u81ea\u6740\u5c1d\u8bd5\u7684YouTube\u9891\u9053\u548c134\u4e2a\u5bf9\u7167\u9891\u9053\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4e94\u4e2a\u4e0e\u81ea\u6740\u5c1d\u8bd5\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u5176\u4e2d\u4e24\u4e2a\u8868\u73b0\u51fa\u65f6\u95f4\u76f8\u5173\u6027\uff1b\u4e13\u5bb6\u672a\u53d1\u73b0\u7684\u5e73\u53f0\u7279\u5b9a\u6307\u6807\uff08YouTube\u53c2\u4e0e\u5ea6\uff09\u5177\u6709\u663e\u8457\u610f\u4e49\uff1b\u5206\u4eab\u52a8\u673a\u5728\u5c1d\u8bd5\u524d\u540e\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7ed3\u5408\u591a\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3\u81ea\u6740\u884c\u4e3a\uff0c\u5f25\u5408\u6570\u5b57\u884c\u4e3a\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.09512", "pdf": "https://arxiv.org/pdf/2506.09512", "abs": "https://arxiv.org/abs/2506.09512", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "7 pages, 1 figure", "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u548cML\u57286G-V2X\u901a\u4fe1\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u751f\u6210\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u8d44\u6e90\u5206\u914d\u3001\u6ce2\u675f\u6210\u5f62\u3001\u4ea4\u901a\u7ba1\u7406\u548c\u5b89\u5168\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u7684\u53d1\u5c55\uff0cV2X\u901a\u4fe1\u9700\u8981\u66f4\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0cAI\u548cML\u56e0\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u5353\u8d8a\u8868\u73b0\u6210\u4e3a\u4f18\u5316V2X\u901a\u4fe1\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5bf9\u8fd1\u671f\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u8fd1\u4e24\u5e74\u7684\u7814\u7a76\uff0c\u5206\u6790\u4e86\u6df1\u5ea6\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u751f\u6210\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7b49\u6280\u672f\u57286G-V2X\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "AI\u5c24\u5176\u662f\u751f\u6210\u5b66\u4e60\u5728\u63d0\u53476G-V2X\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u667a\u80fd\u5316\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5173\u4e8eAI\u9a71\u52a8\u76846G-V2X\u751f\u6001\u7cfb\u7edf\u53d1\u5c55\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.09516", "pdf": "https://arxiv.org/pdf/2506.09516", "abs": "https://arxiv.org/abs/2506.09516", "authors": ["Yingying Fan", "Jinchi Lv", "Ao Sun", "Yurou Wang"], "title": "LLM-Powered CPI Prediction Inference with Online Text Time Series", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "73 pages, 13 figures", "summary": "Forecasting the Consumer Price Index (CPI) is an important yet challenging\ntask in economics, where most existing approaches rely on low-frequency,\nsurvey-based data. With the recent advances of large language models (LLMs),\nthere is growing potential to leverage high-frequency online text data for\nimproved CPI prediction, an area still largely unexplored. This paper proposes\nLLM-CPI, an LLM-based approach for CPI prediction inference incorporating\nonline text time series. We collect a large set of high-frequency online texts\nfrom a popularly used Chinese social network site and employ LLMs such as\nChatGPT and the trained BERT models to construct continuous inflation labels\nfor posts that are related to inflation. Online text embeddings are extracted\nvia LDA and BERT. We develop a joint time series framework that combines\nmonthly CPI data with LLM-generated daily CPI surrogates. The monthly model\nemploys an ARX structure combining observed CPI data with text embeddings and\nmacroeconomic variables, while the daily model uses a VARX structure built on\nLLM-generated CPI surrogates and text embeddings. We establish the asymptotic\nproperties of the method and provide two forms of constructed prediction\nintervals. The finite-sample performance and practical advantages of LLM-CPI\nare demonstrated through both simulation and real data examples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684CPI\u9884\u6d4b\u65b9\u6cd5LLM-CPI\uff0c\u5229\u7528\u9ad8\u9891\u5728\u7ebf\u6587\u672c\u6570\u636e\u6539\u8fdbCPI\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edfCPI\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u9891\u8c03\u67e5\u6570\u636e\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3a\u5229\u7528\u9ad8\u9891\u5728\u7ebf\u6587\u672c\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u901a\u8fc7LLM\uff08\u5982ChatGPT\u548cBERT\uff09\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u6587\u672c\uff0c\u63d0\u53d6\u6587\u672c\u5d4c\u5165\uff0c\u6784\u5efa\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08ARX\u548cVARX\uff09\u7ed3\u5408CPI\u6570\u636e\u548c\u6587\u672c\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6709\u9650\u6837\u672c\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "LLM-CPI\u5c55\u793a\u4e86\u5229\u7528\u5728\u7ebf\u6587\u672c\u6570\u636e\u6539\u8fdbCPI\u9884\u6d4b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09548", "pdf": "https://arxiv.org/pdf/2506.09548", "abs": "https://arxiv.org/abs/2506.09548", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Robotics and Automation Letters", "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u8026\u5408\u7684LiDAR-IMU-\u817f\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u817f\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u9002\u5e94\u7279\u5f81\u7f3a\u5931\u73af\u5883\u548c\u53ef\u53d8\u5f62\u5730\u5f62\u3002", "motivation": "\u89e3\u51b3\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u548c\u53ef\u53d8\u5f62\u5730\u5f62\u4e2d\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u817f\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u7ed3\u5408\u89e6\u89c9\u4fe1\u606f\uff0c\u901a\u8fc7\u56e0\u5b50\u56fe\u8054\u5408\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u548c\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u3002", "result": "\u5728\u6c99\u5730\u548c\u6821\u56ed\u7b49\u590d\u6742\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09562", "pdf": "https://arxiv.org/pdf/2506.09562", "abs": "https://arxiv.org/abs/2506.09562", "authors": ["Songze Li", "Mingxuan Zhang", "Oubo Ma", "Kang Wei", "Shouling Ji"], "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.", "AI": {"tldr": "TooBadRL\u662f\u4e00\u4e2a\u7cfb\u7edf\u4f18\u5316\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u540e\u95e8\u653b\u51fb\u89e6\u53d1\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u5e45\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u89e6\u53d1\u5668\u914d\u7f6e\uff0c\u5ffd\u89c6\u4e86\u89e6\u53d1\u5668\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faTooBadRL\u6846\u67b6\uff0c\u5305\u62ec\u6027\u80fd\u611f\u77e5\u7684\u81ea\u9002\u5e94\u51bb\u7ed3\u673a\u5236\u3001\u57fa\u4e8e\u5408\u4f5c\u6e38\u620f\u7684\u7ef4\u5ea6\u9009\u62e9\u548c\u68af\u5ea6\u4f18\u5316\u7684\u5e45\u5ea6\u8c03\u6574\u3002", "result": "\u5728\u4e09\u79cd\u4e3b\u6d41DRL\u7b97\u6cd5\u548c\u4e5d\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cTooBadRL\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6b63\u5e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "TooBadRL\u8bc1\u660e\u4e86\u89e6\u53d1\u5668\u4f18\u5316\u5728DRL\u540e\u95e8\u653b\u51fb\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5206\u4e3aKG\u589e\u5f3aLLMs\u548cLLM\u589e\u5f3aKGs\u4e24\u7c7b\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u901a\u8fc7\u6574\u5408KGs\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u63d0\u5347LLMs\u7684\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528LLMs\u4f18\u5316KGs\u7684\u6784\u5efa\u548c\u67e5\u8be2\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u4e3aKG\u589e\u5f3aLLMs\uff08\u63d0\u5347\u63a8\u7406\u3001\u51cf\u5c11\u5e7b\u89c9\uff09\u548cLLM\u589e\u5f3aKGs\uff08\u4fc3\u8fdbKG\u6784\u5efa\u4e0e\u67e5\u8be2\uff09\u4e24\u7c7b\u3002", "result": "\u63ed\u793a\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u6574\u5408\u7684\u4e92\u60e0\u6027\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u795e\u7ecf\u7b26\u53f7\u6574\u5408\u3001\u52a8\u6001KG\u66f4\u65b0\u3001\u6570\u636e\u53ef\u9760\u6027\u548c\u4f26\u7406\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u77e5\u8bc6\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.09640", "pdf": "https://arxiv.org/pdf/2506.09640", "abs": "https://arxiv.org/abs/2506.09640", "authors": ["Pablo G. Arce", "Roi Naveiro", "David R\u00edos Insua"], "title": "Evasion Attacks Against Bayesian Predictive Models", "categories": ["stat.ML", "cs.LG", "68T37"], "comment": "Accepted as an oral presentation at UAI'25", "summary": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8d1d\u53f6\u65af\u9884\u6d4b\u6a21\u578b\u8bbe\u8ba1\u6700\u4f18\u89c4\u907f\u653b\u51fb\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u5bf9\u6297\u76ee\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u65b0\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7ecf\u5178\u9884\u6d4b\u6a21\u578b\u7684\u5f31\u70b9\uff0c\u800c\u8d1d\u53f6\u65af\u9884\u6d4b\u6a21\u578b\u7684\u6613\u53d7\u653b\u51fb\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9488\u5bf9\u8d1d\u53f6\u65af\u9884\u6d4b\u6a21\u578b\u7684\u6700\u4f18\u89c4\u907f\u653b\u51fb\uff0c\u5305\u62ec\u4e24\u79cd\u5bf9\u6297\u76ee\u6807\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u5728\u4e0d\u540c\u8ba1\u7b97\u8bbe\u7f6e\u4e0b\u7814\u7a76\u4e86\u653b\u51fb\u65b9\u6cd5\u7684\u5b9e\u73b0\u548c\u7279\u6027\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u8d1d\u53f6\u65af\u9884\u6d4b\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u3002"}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "RAPL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6807\u6ce8\u3001\u6a21\u578b\u65e0\u5173\u7684\u56fe\u8f6c\u6362\u548c\u57fa\u4e8e\u8def\u5f84\u7684\u63a8\u7406\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u56fe\u68c0\u7d22\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRAPL\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u6807\u6ce8\u7b56\u7565\u3001\u6a21\u578b\u65e0\u5173\u7684\u56fe\u8f6c\u6362\u65b9\u6cd5\u548c\u57fa\u4e8e\u8def\u5f84\u7684\u63a8\u7406\u7b56\u7565\u3002", "result": "RAPL\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd52.66%-20.34%\uff0c\u5e76\u663e\u8457\u7f29\u5c0f\u4e86\u4e0d\u540cLLM\u63a8\u7406\u5668\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "RAPL\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22\u548c\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u68c0\u7d22\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2506.09648", "pdf": "https://arxiv.org/pdf/2506.09648", "abs": "https://arxiv.org/abs/2506.09648", "authors": ["Mattia Rosso", "Simone Rossi", "Giulio Franzese", "Markus Heinonen", "Maurizio Filippone"], "title": "Scaling Laws for Uncertainty in Deep Learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u4ecd\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u8d8b\u52bf\u3002", "motivation": "\u53d7\u6df1\u5ea6\u5b66\u4e60\u7f29\u653e\u89c4\u5f8b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u89c4\u5f8b\uff0c\u4ee5\u89e3\u51b3\u5bf9\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u8d28\u7591\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u4f7f\u7528\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5747\u5b58\u5728\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u5373\u4f7f\u6570\u636e\u91cf\u5927\uff0c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4ecd\u4e0d\u53ef\u5ffd\u89c6\uff0c\u652f\u6301\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.09650", "pdf": "https://arxiv.org/pdf/2506.09650", "abs": "https://arxiv.org/abs/2506.09650", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.RO", "eess.IV"], "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u7684\u591a\u4eba\u7269\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u9996\u4e2a\u76f8\u5173\u6570\u636e\u96c6RHAS133\u3002\u901a\u8fc7\u63d0\u51fa\u7684HopaDIFF\u6846\u67b6\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u6761\u4ef6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4eba\u7269\u56fa\u5b9a\u52a8\u4f5c\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u591a\u4eba\u7269\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6307\u5b9a\u76ee\u6807\u4eba\u7269\u8fdb\u884c\u52a8\u4f5c\u5206\u5272\u3002", "method": "\u63d0\u51fa\u4e86HopaDIFF\u6846\u67b6\uff0c\u7ed3\u5408\u4ea4\u53c9\u8f93\u5165\u95e8\u63a7\u6ce8\u610f\u529bxLSTM\u548c\u5085\u91cc\u53f6\u6761\u4ef6\uff0c\u589e\u5f3a\u6574\u4f53-\u5c40\u90e8\u957f\u7a0b\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "HopaDIFF\u5728RHAS133\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u4eba\u7269\u52a8\u4f5c\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.09655", "pdf": "https://arxiv.org/pdf/2506.09655", "abs": "https://arxiv.org/abs/2506.09655", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "AI": {"tldr": "DipLLM\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u5fae\u8c03\uff0c\u5728\u590d\u6742\u591a\u4eba\u6e38\u620f\u300a\u5916\u4ea4\u300b\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u751f\u6210\u6e38\u620f\u6570\u636e\uff0c\u800cLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "DipLLM\u91c7\u7528\u81ea\u56de\u5f52\u5206\u89e3\u6846\u67b6\uff0c\u5c06\u591a\u5355\u4f4d\u52a8\u4f5c\u5206\u914d\u7b80\u5316\u4e3a\u5355\u4f4d\u7ea7\u51b3\u7b56\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u5b9a\u4e49\u5747\u8861\u7b56\u7565\u4f5c\u4e3a\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u4ec5\u97001.5%\u7684\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u73b0\u6709Cicero\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u5fae\u8c03LLMs\u5728\u590d\u6742\u591a\u4eba\u6e38\u620f\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659", "abs": "https://arxiv.org/abs/2506.09659", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIntent Factored Generation (IFG)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u91c7\u6837\uff08\u5148\u91c7\u6837\u610f\u56fe\uff0c\u518d\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff09\u6765\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6837\u672c\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u63d0\u9ad8\u591a\u6837\u6027\u65f6\u5f80\u5f80\u4ec5\u505c\u7559\u5728\u8bcd\u6c47\u5c42\u9762\uff0c\u5bfc\u81f4\u63a8\u7406\u95ee\u9898\u63a2\u7d22\u4e0d\u8db3\u548c\u5bf9\u8bdd\u4ee3\u7406\u91cd\u590d\u65e0\u8da3\u3002", "method": "IFG\u5c06\u91c7\u6837\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u91c7\u6837\u8bed\u4e49\u5bc6\u96c6\u7684\u610f\u56fe\uff08\u5982\u6458\u8981\u6216\u5173\u952e\u8bcd\uff09\uff1b2) \u57fa\u4e8e\u539f\u59cb\u63d0\u793a\u548c\u610f\u56fe\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u3002\u901a\u8fc7\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\uff08\u610f\u56fe\u9636\u6bb5\u9ad8\u6e29\u5ea6\uff0c\u751f\u6210\u9636\u6bb5\u4f4e\u6e29\u5ea6\uff09\u5e73\u8861\u591a\u6837\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86pass@k\u548cRLVF\u6027\u80fd\uff1b\u7ed3\u5408Direct Preference Optimisation\u63d0\u5347\u4e86\u5bf9\u8bdd\u591a\u6837\u6027\uff1b\u5728\u901a\u7528\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u3002", "conclusion": "IFG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u63d0\u793a\u548c\u6e29\u5ea6\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u591a\u6837\u6027\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.09668", "pdf": "https://arxiv.org/pdf/2506.09668", "abs": "https://arxiv.org/abs/2506.09668", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "categories": ["cs.CV", "cs.LG"], "comment": "Work currently under revision for IEEE TMI", "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "AI": {"tldr": "CINeMA\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u521b\u5efa\u9ad8\u5206\u8fa8\u7387\u3001\u591a\u6a21\u6001\u7684\u80ce\u513f\u548c\u65b0\u751f\u513f\u8111\u56fe\u8c31\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u7814\u7a76\u80ce\u513f\u548c\u65b0\u751f\u513f\u5927\u8111\u5feb\u901f\u53d1\u80b2\u9636\u6bb5\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u8111\u56fe\u8c31\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6570\u636e\uff0c\u96be\u4ee5\u5e94\u5bf9\u75c5\u7406\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u3002", "method": "CINeMA\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u5bc6\u96c6\u7684\u56fe\u50cf\u914d\u51c6\uff0c\u652f\u6301\u57fa\u4e8e\u89e3\u5256\u7279\u5f81\u7684\u7075\u6d3b\u6761\u4ef6\u751f\u6210\u3002", "result": "CINeMA\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u591a\u529f\u80fd\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u7ec4\u7ec7\u5206\u5272\u3001\u5e74\u9f84\u9884\u6d4b\u7b49\u4efb\u52a1\uff0c\u5e76\u80fd\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "conclusion": "CINeMA\u4e3a\u8111\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u56fe\u8c31\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09681", "pdf": "https://arxiv.org/pdf/2506.09681", "abs": "https://arxiv.org/abs/2506.09681", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPMs\uff09\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u63a2\u7d22DDPMs\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u6536\u655b\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u8bc4\u4f30DDPMs\u5728\u566a\u58f0\u8bc4\u5206\u51fd\u6570\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u5efa\u7acbWasserstein-2\u8ddd\u79bb\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "result": "DDPMs\u5bf9\u566a\u58f0\u8bc4\u5206\u4f30\u8ba1\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u6536\u655b\u901f\u7387\u4f18\u4e8e\u5148\u524d\u7ed3\u679c\uff0c\u63a5\u8fd1\u9ad8\u65af\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u901f\u7387\u3002", "conclusion": "DDPMs\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u901f\u7387\u5feb\u4e14\u63a5\u8fd1\u6700\u4f18\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u6dfb\u52a0\u7b80\u5355\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u56fe\u50cf\u548c\u6587\u672c\u7247\u6bb5\uff0c\u5229\u7528VLM\u5339\u914d\u5e76\u805a\u5408\u76f8\u4f3c\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u7f16\u7801\u5668VLM\u5728\u7ec4\u5408\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u7f16\u7801\u5668VLM\uff08\u5982CLIP\uff09\u5728\u7ec4\u5408\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8bad\u7ec3\u65b9\u6cd5\u5df2\u6709\u5f88\u591a\u7814\u7a76\uff0c\u63a8\u7406\u65f6\u6280\u672f\u5374\u8f83\u5c11\u88ab\u5173\u6ce8\u3002", "method": "\u5728\u63a8\u7406\u65f6\uff0c\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u5c0f\u5757\uff0c\u63d0\u53d6\u6587\u672c\u7247\u6bb5\uff08\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\uff09\uff0c\u5229\u7528VLM\u5339\u914d\u56fe\u50cf\u5757\u4e0e\u6587\u672c\u7247\u6bb5\uff0c\u5e76\u805a\u5408\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6700\u7ec8\u5f97\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86VLM\u5728\u7ec4\u5408\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63a8\u7406\u65f6\u6280\u672f\u5177\u6709\u6f5c\u529b\uff0c\u56fe\u50cf\u5757\u5904\u7406\u662f\u5173\u952e\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u65f6\u65b9\u6cd5\u3002"}}
{"id": "2506.09709", "pdf": "https://arxiv.org/pdf/2506.09709", "abs": "https://arxiv.org/abs/2506.09709", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "comment": "Interspeech 2025", "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "AI": {"tldr": "Factorized MKL-VC\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8ekNN-VC\u6d41\u7a0b\uff0c\u4ec5\u97005\u79d2\u53c2\u8003\u97f3\u9891\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3kNN-VC\u5728\u77ed\u53c2\u8003\u97f3\u9891\u4e0b\u5185\u5bb9\u4fdd\u7559\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7528\u56e0\u5b50\u5316\u7684\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u66ff\u4ee3kNN\u56de\u5f52\uff0c\u57fa\u4e8eWavLM\u5d4c\u5165\u5b50\u7a7a\u95f4\u548cMonge-Kantorovich\u7ebf\u6027\u89e3\u3002", "result": "\u5728LibriSpeech\u548cFLEURS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8ekNN-VC\uff0c\u5c24\u5176\u5728\u8de8\u8bed\u8a00\u8bed\u97f3\u8f6c\u6362\u9886\u57df\u63a5\u8fd1FACodec\u6027\u80fd\u3002", "conclusion": "MKL-VC\u5728\u77ed\u53c2\u8003\u97f3\u9891\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8de8\u8bed\u8a00\u8bed\u97f3\u8f6c\u6362\u3002"}}
{"id": "2506.09730", "pdf": "https://arxiv.org/pdf/2506.09730", "abs": "https://arxiv.org/abs/2506.09730", "authors": ["Pierre Vernimmen", "Fran\u00e7ois Glineur"], "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u4e0d\u540c\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u68af\u5ea6\u8ba1\u7b97\u5b58\u5728\u76f8\u5bf9\u4e0d\u7cbe\u786e\u6027\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u68af\u5ea6\u8ba1\u7b97\u4e0d\u7cbe\u786e\uff08\u5982\u68af\u5ea6\u538b\u7f29\uff09\u5bf9\u4f18\u5316\u65b9\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21GPU\u95ee\u9898\u4e2d\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cd\u4f18\u5316\u65b9\u6cd5\uff1a\u6052\u5b9a\u6b65\u957f\u68af\u5ea6\u4e0b\u964d\u3001\u957f\u6b65\u65b9\u6cd5\u548c\u52a0\u901f\u65b9\u6cd5\uff0c\u63d0\u51fa\u534a\u542f\u53d1\u5f0f\u7f29\u77ed\u56e0\u5b50\u6539\u8fdb\u540e\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u52a0\u901f\u65b9\u6cd5\u5728\u5b9e\u9645\u4e0d\u7cbe\u786e\u95ee\u9898\u4e2d\u8868\u73b0\u6bd4\u9884\u671f\u66f4\u9c81\u68d2\uff0c\u7f29\u77ed\u56e0\u5b50\u663e\u8457\u6539\u5584\u4e86\u957f\u6b65\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u6709\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u5728\u4e0d\u7cbe\u786e\u73af\u5883\u4e0b\u5747\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09764", "pdf": "https://arxiv.org/pdf/2506.09764", "abs": "https://arxiv.org/abs/2506.09764", "authors": ["Giulia Preti", "Gianmarco De Francisci Morales", "Matteo Riondato"], "title": "Alice and the Caterpillar: A more descriptive null model for assessing data mining results", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "We introduce novel null models for assessing the results obtained from\nobserved binary transactional and sequence datasets, using statistical\nhypothesis testing. Our null models maintain more properties of the observed\ndataset than existing ones. Specifically, they preserve the Bipartite Joint\nDegree Matrix of the bipartite (multi-)graph corresponding to the dataset,\nwhich ensures that the number of caterpillars, i.e., paths of length three, is\npreserved, in addition to other properties considered by other models. We\ndescribe Alice, a suite of Markov chain Monte Carlo algorithms for sampling\ndatasets from our null models, based on a carefully defined set of states and\nefficient operations to move between them. The results of our experimental\nevaluation show that Alice mixes fast and scales well, and that our null model\nfinds different significant results than ones previously considered in the\nliterature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e8c\u5143\u4ea4\u6613\u548c\u5e8f\u5217\u6570\u636e\u96c6\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u3002\u8be5\u6a21\u578b\u6bd4\u73b0\u6709\u6a21\u578b\u4fdd\u7559\u66f4\u591a\u6570\u636e\u7279\u6027\uff0c\u5c24\u5176\u662f\u4e8c\u90e8\u8054\u5408\u5ea6\u77e9\u9635\u3002", "motivation": "\u73b0\u6709\u96f6\u6a21\u578b\u672a\u80fd\u5145\u5206\u4fdd\u7559\u6570\u636e\u96c6\u7684\u5173\u952e\u7279\u6027\uff0c\u5982\u4e8c\u90e8\u8054\u5408\u5ea6\u77e9\u9635\u548c\u7279\u5b9a\u8def\u5f84\u6570\u91cf\u3002", "method": "\u5f00\u53d1\u4e86Alice\u7b97\u6cd5\u5957\u4ef6\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u72b6\u6001\u96c6\u548c\u9ad8\u6548\u64cd\u4f5c\u5b9e\u73b0\u6570\u636e\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAlice\u6df7\u5408\u901f\u5ea6\u5feb\u3001\u6269\u5c55\u6027\u597d\uff0c\u4e14\u80fd\u53d1\u73b0\u4e0e\u73b0\u6709\u6a21\u578b\u4e0d\u540c\u7684\u663e\u8457\u7ed3\u679c\u3002", "conclusion": "\u65b0\u96f6\u6a21\u578b\u5728\u4fdd\u7559\u66f4\u591a\u6570\u636e\u7279\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\u3002"}}
{"id": "2506.09765", "pdf": "https://arxiv.org/pdf/2506.09765", "abs": "https://arxiv.org/abs/2506.09765", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "categories": ["cs.RO", "cs.LG"], "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u8c03\u6574\u548c\u6539\u8fdb\u591a\u5438\u76d8\u672b\u7aef\u6267\u884c\u5668\u7684\u9009\u62e9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u62e3\u9009\u5931\u8d25\u7387\u3002", "motivation": "\u63d0\u5347\u4ed3\u5e93\u81ea\u52a8\u5316\u6548\u7387\uff0c\u51cf\u5c11\u62e3\u9009\u5931\u8d25\u7387\uff0c\u5f25\u8865\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aML\u6846\u67b6\uff0c\u4f18\u5316\u62e3\u9009\u6837\u672c\u7684\u5438\u76d8\u9009\u62e9\u548c\u53d8\u6362\u8c03\u6574\uff0c\u5e76\u5728\u7c7b\u4f3c\u4e9a\u9a6c\u900a\u673a\u5668\u4ebaRobin\u8f66\u961f\u7684\u6d4b\u8bd5\u73af\u5883\u4e2d\u96c6\u6210\u8bc4\u4f30\u3002", "result": "\u5728200\u4e07\u6b21\u62e3\u9009\u4e2d\uff0c\u62e3\u9009\u5931\u8d25\u7387\u964d\u4f4e\u4e8620%\uff0c\u4f18\u4e8e\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5927\u89c4\u6a21\u4ed3\u5e93\u81ea\u52a8\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.09773", "pdf": "https://arxiv.org/pdf/2506.09773", "abs": "https://arxiv.org/abs/2506.09773", "authors": ["Taulant Koka", "Manolis C. Tsakiris", "Benjam\u00edn B\u00e9jar Haro", "Michael Muma"], "title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted to ICASSP 2025. \\copyright 2025 IEEE. Personal use of this\n  material is permitted", "summary": "Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u8de8\u901a\u9053\u65e0\u6807\u8bb0\u611f\u77e5\u6846\u67b6\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u4fe1\u53f7\u7ed3\u6784\uff08\u5982\u5b50\u7a7a\u95f4\u8054\u5408\uff09\uff0c\u6539\u8fdb\u4e86\u6837\u672c\u9700\u6c42\u91cf\u7684\u8fb9\u754c\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u901a\u9053\u4fe1\u53f7\u5728\u8de8\u901a\u9053\u6d4b\u91cf\u4e2d\u56e0\u6837\u672c\u4e0e\u901a\u9053\u4e0d\u5339\u914d\u800c\u96be\u4ee5\u6062\u590d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4fe1\u53f7\u7ed3\u6784\uff08\u5982\u5b50\u7a7a\u95f4\u8054\u5408\uff09\u548c\u5b9e\u9645\u5e94\u7528\uff08\u5982\u5168\u8111\u9499\u6210\u50cf\uff09\u4e2d\u3002", "method": "\u6269\u5c55\u8de8\u901a\u9053\u65e0\u6807\u8bb0\u611f\u77e5\u6846\u67b6\uff0c\u652f\u6301\u4fe1\u53f7\u4f4d\u4e8e\u5b50\u7a7a\u95f4\u8054\u5408\u7684\u60c5\u51b5\uff0c\u63a8\u5bfc\u66f4\u7d27\u7684\u6837\u672c\u9700\u6c42\u91cf\u8fb9\u754c\uff0c\u5e76\u5728\u5168\u8111\u9499\u6210\u50cf\u4e2d\u9a8c\u8bc1\u3002", "result": "\u6539\u8fdb\u4e86\u6837\u672c\u9700\u6c42\u91cf\u7684\u7406\u8bba\u8fb9\u754c\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u4fe1\u53f7\u7c7b\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u4fe1\u53f7\u91cd\u5efa\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u4fe1\u53f7\u7ed3\u6784\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8de8\u901a\u9053\u65e0\u6807\u8bb0\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09792", "pdf": "https://arxiv.org/pdf/2506.09792", "abs": "https://arxiv.org/abs/2506.09792", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u97f3-\u8bed\u8a00\u6a21\u578b\uff08PSLMs\uff09\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u4f5c\u4e3a\u8f85\u52a9\u77e5\u8bc6\u6e90\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u97f3\u9891-\u89c6\u89c9\u76ee\u6807\u8bf4\u8bdd\u4eba\u63d0\u53d6\uff08AV-TSE\uff09\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u5229\u7528\u8bed\u8a00\u77e5\u8bc6\uff08\u5982\u8bed\u6cd5\u548c\u8bed\u4e49\uff09\u8f85\u52a9\u8bed\u97f3\u611f\u77e5\u7684\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u7d22\u4e86PSLMs\u548cPLMs\u5728AV-TSE\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5c06PSLMs\u6216PLMs\u7684\u8bed\u8a00\u7ea6\u675f\u4f5c\u4e3a\u989d\u5916\u7684\u76d1\u7763\u4fe1\u53f7\u5f15\u5165AV-TSE\u6a21\u578b\uff0c\u65e0\u9700\u5728\u63a8\u7406\u9636\u6bb5\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u7ebf\u7d22\u53d7\u635f\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u77e5\u8bc6\u6e90\u53ef\u4ee5\u6709\u6548\u589e\u5f3aAV-TSE\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09804", "pdf": "https://arxiv.org/pdf/2506.09804", "abs": "https://arxiv.org/abs/2506.09804", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schl\u00fcter", "Hermann Ney"], "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u524d\u7aef\u5728ASR\u7cfb\u7edf\u4e2d\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891\u6270\u52a8\u548cSTFT\u57df\u63a9\u7801\u6539\u8fdb\u6027\u80fd\uff0c\u7f29\u5c0f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "motivation": "\u795e\u7ecf\u524d\u7aef\u5728ASR\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u5982\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u97f3\u9891\u6270\u52a8\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u5728STFT\u57df\u8fdb\u884c\u63a9\u7801\u7684\u6539\u8fdb\u7248SpecAugment\u3002", "result": "\u7ed3\u5408\u4e24\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u540e\uff0c\u53ef\u5b66\u4e60\u7279\u5f81\u4e0e\u4f20\u7edf\u7279\u5f81\u7684\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f\u3002", "conclusion": "\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u53ef\u5b66\u4e60\u7279\u5f81\u524d\u7aef\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u63a5\u8fd1\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.09805", "pdf": "https://arxiv.org/pdf/2506.09805", "abs": "https://arxiv.org/abs/2506.09805", "authors": ["Tonghe Wang", "Yining Feng", "Xiaofeng Yang"], "title": "Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy", "categories": ["physics.med-ph", "cs.LG"], "comment": null, "summary": "Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the\npattern of needle placement solely relies on physician experience. We\ninvestigated the feasibility of using reinforcement learning (RL) to provide\nneedle positions and dwell times based on patient anatomy during pre-planning\nstage. This approach would reduce procedure time and ensure consistent plan\nquality. Materials and Methods: We train a RL agent to adjust the position of\none selected needle and all the dwell times on it to maximize a pre-defined\nreward function after observing the environment. After adjusting, the RL agent\nthen moves on to the next needle, until all needles are adjusted. Multiple\nrounds are played by the agent until the maximum number of rounds is reached.\nPlan data from 11 prostate HDR boost patients (1 for training, and 10 for\ntesting) treated in our clinic were included in this study. The dosimetric\nmetrics and the number of used needles of RL plan were compared to those of the\nclinical results (ground truth). Results: On average, RL plans and clinical\nplans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no\nstatistical significance), while RL plans have less prostate hotspot (Prostate\nV150) and Urethra D20% plans with statistical significance. Moreover, RL plans\nuse 2 less needles than clinical plan on average. Conclusion: We present the\nfirst study demonstrating the feasibility of using reinforcement learning to\nautonomously generate clinically practical HDR prostate brachytherapy plans.\nThis RL-based method achieved equal or improved plan quality compared to\nconventional clinical approaches while requiring fewer needles. With minimal\ndata requirements and strong generalizability, this approach has substantial\npotential to standardize brachytherapy planning, reduce clinical variability,\nand enhance patient outcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u9ad8\u5242\u91cf\u7387\u524d\u5217\u817a\u8fd1\u8ddd\u79bb\u653e\u5c04\u6cbb\u7597\u4e2d\u81ea\u52a8\u751f\u6210\u9488\u4f4d\u548c\u505c\u7559\u65f6\u95f4\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793aRL\u8ba1\u5212\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u7684\u9488\u3002", "motivation": "\u76ee\u524d\u9488\u4f4d\u653e\u7f6e\u4ec5\u4f9d\u8d56\u533b\u751f\u7ecf\u9a8c\uff0cRL\u65b9\u6cd5\u53ef\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u5e76\u786e\u4fdd\u8ba1\u5212\u8d28\u91cf\u7684\u4e00\u81f4\u6027\u3002", "method": "\u8bad\u7ec3RL\u4ee3\u7406\u8c03\u6574\u9488\u4f4d\u548c\u505c\u7559\u65f6\u95f4\uff0c\u57fa\u4e8e\u9884\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u4f18\u5316\uff0c\u4f7f\u752811\u4f8b\u60a3\u8005\u6570\u636e\uff081\u4f8b\u8bad\u7ec3\uff0c10\u4f8b\u6d4b\u8bd5\uff09\u3002", "result": "RL\u8ba1\u5212\u4e0e\u4f20\u7edf\u8ba1\u5212\u5728\u524d\u5217\u817a\u8986\u76d6\u548c\u76f4\u80a0\u5242\u91cf\u4e0a\u76f8\u4f3c\uff0c\u4f46\u524d\u5217\u817a\u70ed\u70b9\u548c\u5c3f\u9053\u5242\u91cf\u66f4\u4f4e\uff0c\u4e14\u5e73\u5747\u5c11\u75282\u6839\u9488\u3002", "conclusion": "RL\u65b9\u6cd5\u53ef\u884c\uff0c\u80fd\u6807\u51c6\u5316\u8ba1\u5212\u3001\u51cf\u5c11\u4e34\u5e8a\u5dee\u5f02\u5e76\u6539\u5584\u60a3\u8005\u7ed3\u679c\u3002"}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT\u6846\u67b6\u901a\u8fc7Hint-Engineering\u4f18\u5316LRM\u4e0e\u4ee3\u7801\u89e3\u91ca\u5668\u7684\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u4e2d\u6548\u7387\u4f4e\u6216\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u76f4\u63a5\u7ed3\u5408\u4ee3\u7801\u89e3\u91ca\u5668\uff08CI\uff09\u7684\u6280\u672f\u6311\u6218\u3002", "method": "\u901a\u8fc7Hint-Engineering\u5408\u6210\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u6570\u636e\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u3001\u62d2\u7edd\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u53474%\u81f38%\uff0c\u8ba1\u7b97\u5f00\u9500\u51cf\u5c1130%\u81f350%\u3002", "conclusion": "CoRT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LRM\u4e0eCI\u7684\u4ea4\u4e92\u6548\u7387\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09832", "pdf": "https://arxiv.org/pdf/2506.09832", "abs": "https://arxiv.org/abs/2506.09832", "authors": ["Dany Lauzon", "Julien Straubhaar", "Philippe Renard"], "title": "A Deep Generative Model for the Simulation of Discrete Karst Networks", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 15 figures, submitted to Earth and Space Science", "summary": "The simulation of discrete karst networks presents a significant challenge\ndue to the complexity of the physicochemical processes occurring within various\ngeological and hydrogeological contexts over extended periods. This complex\ninterplay leads to a wide variety of karst network patterns, each intricately\nlinked to specific hydrogeological conditions. We explore a novel approach that\nrepresents karst networks as graphs and applies graph generative models (deep\nlearning techniques) to capture the intricate nature of karst environments. In\nthis representation, nodes retain spatial information and properties, while\nedges signify connections between nodes. Our generative process consists of two\nmain steps. First, we utilize graph recurrent neural networks (GraphRNN) to\nlearn the topological distribution of karst networks. GraphRNN decomposes the\ngraph simulation into a sequential generation of nodes and edges, informed by\npreviously generated structures. Second, we employ denoising diffusion\nprobabilistic models on graphs (G-DDPM) to learn node features (spatial\ncoordinates and other properties). G-DDPMs enable the generation of nodes\nfeatures on the graphs produced by the GraphRNN that adhere to the learned\nstatistical properties by sampling from the derived probability distribution,\nensuring that the generated graphs are realistic and capture the essential\nfeatures of the original data. We test our approach using real-world karst\nnetworks and compare generated subgraphs with actual subgraphs from the\ndatabase, by using geometry and topology metrics. Our methodology allows\nstochastic simulation of discrete karst networks across various types of\nformations, a useful tool for studying the behavior of physical processes such\nas flow and transport.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u751f\u6210\u6a21\u578b\uff08\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u79bb\u6563\u5ca9\u6eb6\u7f51\u7edc\u7684\u590d\u6742\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u56fe\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08GraphRNN\uff09\u548c\u56fe\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08G-DDPM\uff09\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u6027\u7684\u5ca9\u6eb6\u7f51\u7edc\u3002", "motivation": "\u7531\u4e8e\u5ca9\u6eb6\u7f51\u7edc\u7684\u590d\u6742\u6027\u53ca\u5176\u4e0e\u6c34\u6587\u5730\u8d28\u6761\u4ef6\u7684\u7d27\u5bc6\u5173\u8054\uff0c\u6a21\u62df\u5176\u5f62\u6001\u548c\u7279\u5f81\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1. \u4f7f\u7528GraphRNN\u5b66\u4e60\u5ca9\u6eb6\u7f51\u7edc\u7684\u62d3\u6251\u5206\u5e03\uff1b2. \u5229\u7528G-DDPM\u5b66\u4e60\u8282\u70b9\u7279\u5f81\uff08\u5982\u7a7a\u95f4\u5750\u6807\uff09\u3002", "result": "\u751f\u6210\u7684\u5b50\u56fe\u4e0e\u5b9e\u9645\u6570\u636e\u5728\u51e0\u4f55\u548c\u62d3\u6251\u6307\u6807\u4e0a\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u968f\u673a\u6a21\u62df\u4e0d\u540c\u7c7b\u578b\u7684\u5ca9\u6eb6\u7f51\u7edc\uff0c\u4e3a\u7814\u7a76\u7269\u7406\u8fc7\u7a0b\uff08\u5982\u6d41\u52a8\u548c\u4f20\u8f93\uff09\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.09851", "pdf": "https://arxiv.org/pdf/2506.09851", "abs": "https://arxiv.org/abs/2506.09851", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "Accepted in MECON 2025", "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528LSTM\u548cGBC\u6a21\u578b\u9884\u6d4b\u7f8e\u5143/\u5b5f\u52a0\u62c9\u5854\u5361\u6c47\u7387\uff0cLSTM\u51c6\u786e\u7387\u8fbe99.449%\uff0c\u4f18\u4e8e\u4f20\u7edfARIMA\u65b9\u6cd5\uff0c\u4f46GBC\u6a21\u578b\u5728\u4ea4\u6613\u4e2d\u4e8f\u635f\u3002", "motivation": "\u5916\u6c47\u6c47\u7387\u9884\u6d4b\u5bf9\u5168\u7403\u91d1\u878d\u5e02\u573a\u81f3\u5173\u91cd\u8981\uff0c\u5f71\u54cd\u8d38\u6613\u548c\u6295\u8d44\u3002", "method": "\u4f7f\u75282018-2023\u5e74\u5386\u53f2\u6570\u636e\uff0c\u91c7\u7528LSTM\u548cGBC\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u548c\u4ea4\u6613\u6a21\u62df\u3002", "result": "LSTM\u8868\u73b0\u4f18\u5f02\uff08RMSE 0.9858\uff09\uff0cGBC\u4ea4\u6613\u6a21\u62df\u4e8f\u635f20,653.25\u7f8e\u5143\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6c47\u7387\u9884\u6d4b\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u60c5\u611f\u5206\u6790\u548c\u5b9e\u65f6\u7ecf\u6d4e\u6307\u6807\u63d0\u5347\u6a21\u578b\u3002"}}
{"id": "2506.09874", "pdf": "https://arxiv.org/pdf/2506.09874", "abs": "https://arxiv.org/abs/2506.09874", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "AI": {"tldr": "UmbraTTS\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684TTS\u6a21\u578b\uff0c\u53ef\u540c\u65f6\u751f\u6210\u8bed\u97f3\u548c\u73af\u5883\u97f3\u9891\uff0c\u89e3\u51b3\u4e86\u8bed\u97f3\u4e0e\u590d\u6742\u80cc\u666f\u73af\u5883\u878d\u5408\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524dTTS\u6280\u672f\u867d\u80fd\u751f\u6210\u9ad8\u5ea6\u81ea\u7136\u7684\u8bed\u97f3\uff0c\u4f46\u5c06\u8bed\u97f3\u4e0e\u590d\u6742\u80cc\u666f\u73af\u5883\u7ed3\u5408\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faUmbraTTS\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6846\u67b6\u4ece\u65e0\u6807\u6ce8\u5f55\u97f3\u4e2d\u63d0\u53d6\u8bed\u97f3\u3001\u80cc\u666f\u97f3\u9891\u548c\u6587\u672c\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "UmbraTTS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u751f\u6210\u81ea\u7136\u3001\u9ad8\u8d28\u91cf\u4e14\u73af\u5883\u611f\u77e5\u7684\u97f3\u9891\u3002", "conclusion": "UmbraTTS\u4e3a\u8bed\u97f3\u4e0e\u80cc\u666f\u73af\u5883\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "PersonaLens\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u52a1\u5bfc\u5411AI\u52a9\u624b\u4e2a\u6027\u5316\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u7528\u6237\u4ee3\u7406\u548c\u6cd5\u5b98\u4ee3\u7406\u63ed\u793a\u5f53\u524dLLM\u52a9\u624b\u5728\u4e2a\u6027\u5316\u80fd\u529b\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u4e2a\u6027\u5316\u4efb\u52a1\u5bfc\u5411\u52a9\u624b\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165PersonaLens\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7528\u6237\u914d\u7f6e\u548c\u4e24\u4e2aLLM\u4ee3\u7406\uff08\u7528\u6237\u4ee3\u7406\u548c\u6cd5\u5b98\u4ee3\u7406\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u5bf9\u8bdd\u548c\u8bc4\u4f30\u4e2a\u6027\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dLLM\u52a9\u624b\u5728\u4e2a\u6027\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "PersonaLens\u4e3a\u63d0\u5347\u5bf9\u8bddAI\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2506.09958", "pdf": "https://arxiv.org/pdf/2506.09958", "abs": "https://arxiv.org/abs/2506.09958", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "categories": ["cs.CV", "cs.LG", "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)", "I.2.10; I.2.6; J.3"], "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "AI": {"tldr": "Kvasir-VQA-x1\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08MedVQA\uff09\u7684\u4e34\u5e8a\u590d\u6742\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709MedVQA\u6570\u636e\u96c6\u7f3a\u4e4f\u4e34\u5e8a\u590d\u6742\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210159,549\u4e2a\u65b0\u95ee\u7b54\u5bf9\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u589e\u5f3a\u6a21\u62df\u5e38\u89c1\u6210\u50cf\u4f2a\u5f71\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u6807\u51c6VQA\u6027\u80fd\u548c\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u4e3a\u4e34\u5e8a\u573a\u666f\u63d0\u4f9b\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002", "conclusion": "Kvasir-VQA-x1\u65e8\u5728\u52a0\u901f\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\uff0c\u5e76\u9075\u5faaFAIR\u6570\u636e\u539f\u5219\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2506.09985", "pdf": "https://arxiv.org/pdf/2506.09985", "abs": "https://arxiv.org/abs/2506.09985", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "48 pages, 19 figures", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\u548c\u5c11\u91cf\u673a\u5668\u4eba\u4ea4\u4e92\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u7406\u89e3\u3001\u9884\u6d4b\u548c\u89c4\u5212\u7269\u7406\u4e16\u754c\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3AI\u7684\u4e3b\u8981\u6311\u6218\u662f\u901a\u8fc7\u89c2\u5bdf\u5b66\u4e60\u548c\u7406\u89e3\u4e16\u754c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u548c\u5c11\u91cf\u4ea4\u4e92\u6570\u636e\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u9884\u8bad\u7ec3\u65e0\u52a8\u4f5c\u7684\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784V-JEPA 2\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u4eba\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578bV-JEPA 2-AC\u3002", "result": "V-JEPA 2\u5728\u8fd0\u52a8\u7406\u89e3\u548c\u4eba\u7c7b\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bV-JEPA 2-AC\u5728\u673a\u5668\u4eba\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u548c\u5c11\u91cf\u4ea4\u4e92\u6570\u636e\u53ef\u4ee5\u6784\u5efa\u80fd\u591f\u89c4\u5212\u7269\u7406\u4e16\u754c\u7684\u4e16\u754c\u6a21\u578b\u3002"}}
{"id": "2506.09987", "pdf": "https://arxiv.org/pdf/2506.09987", "abs": "https://arxiv.org/abs/2506.09987", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MVP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u4e16\u754c\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6700\u5c0f\u53d8\u5316\u5bf9\u907f\u514d\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u56e0\u4f9d\u8d56\u8868\u9762\u89c6\u89c9\u6216\u6587\u672c\u7ebf\u7d22\u5bfc\u81f4\u8bc4\u5206\u865a\u9ad8\uff0c\u9700\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165MVP\u57fa\u51c6\uff0c\u5305\u542b55K\u9ad8\u8d28\u91cf\u591a\u9009\u9898\u89c6\u9891QA\u6837\u672c\uff0c\u6bcf\u6837\u672c\u6709\u6700\u5c0f\u53d8\u5316\u5bf9\u4ee5\u6d88\u9664\u8868\u9762\u7ebf\u7d22\u4f9d\u8d56\u3002", "result": "\u4eba\u7c7b\u8868\u73b092.9%\uff0c\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u4ec540.2%\uff08\u968f\u673a\u4e3a25%\uff09\u3002", "conclusion": "MVP\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u771f\u5b9e\u7406\u89e3\u80fd\u529b\uff0c\u907f\u514d\u8868\u9762\u7ebf\u7d22\u5e72\u6270\u3002"}}
{"id": "2506.09988", "pdf": "https://arxiv.org/pdf/2506.09988", "abs": "https://arxiv.org/abs/2506.09988", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "AI": {"tldr": "EditInspector\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u8d28\u91cf\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u5168\u9762\u8bc4\u4f30\u7f16\u8f91\u548c\u63cf\u8ff0\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u9a8c\u8bc1\u548c\u8bc4\u4f30\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u5f15\u5165EditInspector\u57fa\u51c6\uff0c\u5229\u7528\u4eba\u7c7b\u6807\u6ce8\u7684\u6a21\u677f\u9a8c\u8bc1\u7f16\u8f91\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u591a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u8bc4\u4f30\u7f16\u8f91\u65f6\u8868\u73b0\u4e0d\u5168\u9762\u4e14\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u65b0\u65b9\u6cd5\u5728\u4f2a\u5f71\u68c0\u6d4b\u548c\u5dee\u5f02\u63cf\u8ff0\u751f\u6210\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "EditInspector\u4e3a\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.09990", "pdf": "https://arxiv.org/pdf/2506.09990", "abs": "https://arxiv.org/abs/2506.09990", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "AI": {"tldr": "Chain-of-Action (CoA) \u662f\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u65b0\u578b\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8303\u5f0f\uff0c\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u751f\u6210\u5b8c\u6574\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u76ee\u6807\u7684\u5168\u5c40\u5230\u5c40\u90e8\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u524d\u5411\u9884\u6d4b\u4e0b\u4e00\u6b65\u52a8\u4f5c\uff0c\u800c CoA \u901a\u8fc7\u53cd\u5411\u63a8\u7406\u548c\u4efb\u52a1\u76ee\u6807\u9a71\u52a8\u7684\u52a8\u4f5c\u7ea7\u601d\u7ef4\u94fe\uff08CoT\uff09\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u66f4\u5168\u5c40\u5316\u7684\u52a8\u4f5c\u89c4\u5212\u3002", "method": "CoA \u91c7\u7528\u81ea\u56de\u5f52\u7ed3\u6784\uff0c\u9996\u5148\u751f\u6210\u4e00\u4e2a\u7a33\u5b9a\u7684\u5173\u952e\u5e27\u52a8\u4f5c\uff0c\u968f\u540e\u57fa\u4e8e\u5173\u952e\u5e27\u548c\u5df2\u9884\u6d4b\u52a8\u4f5c\u81ea\u56de\u5f52\u751f\u6210\u540e\u7eed\u52a8\u4f5c\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u8fde\u7eed\u52a8\u4f5c\u6807\u8bb0\u8868\u793a\u3001\u52a8\u6001\u505c\u6b62\u3001\u53cd\u5411\u65f6\u95f4\u96c6\u6210\u548c\u591a\u6807\u8bb0\u9884\u6d4b\u7b49\u6280\u672f\u3002", "result": "CoA \u5728 60 \u4e2a RLBench \u4efb\u52a1\u548c 8 \u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CoA \u5728\u4fdd\u6301\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7075\u6d3b\u6027\u548c\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09993", "pdf": "https://arxiv.org/pdf/2506.09993", "abs": "https://arxiv.org/abs/2506.09993", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "title": "Text-Aware Image Restoration with Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u4fee\u590d\u4efb\u52a1TAIR\uff0c\u4e13\u6ce8\u4e8e\u540c\u65f6\u6062\u590d\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u672c\u4fdd\u771f\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6SA-Text\u548c\u591a\u4efb\u52a1\u6269\u6563\u6846\u67b6TeReDiff\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6587\u672c\u533a\u57df\u7684\u6062\u590d\u4e2d\u5e38\u51fa\u73b0\u6587\u672c\u56fe\u50cf\u5e7b\u89c9\u73b0\u8c61\uff0c\u5373\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u6587\u672c\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86TAIR\u4efb\u52a1\u548cSA-Text\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86TeReDiff\u6846\u67b6\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\u4e0e\u6587\u672c\u68c0\u6d4b\u6a21\u5757\u7ed3\u5408\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u53d6\u4e30\u5bcc\u7684\u6587\u672c\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTeReDiff\u5728\u6587\u672c\u8bc6\u522b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TAIR\u4efb\u52a1\u548cTeReDiff\u6846\u67b6\u4e3a\u89e3\u51b3\u6587\u672c\u56fe\u50cf\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.09997", "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "DGS-LRM\u662f\u9996\u4e2a\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u52a8\u6001\u573a\u666f3D\u9ad8\u65af\u70b9\u4e91\u9884\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u9650\u4e8e\u9759\u6001\u573a\u666f\uff0c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u56e0\u6570\u636e\u7a00\u7f3a\u548c3D\u8868\u793a\u56f0\u96be\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u3001\u53ef\u53d8\u5f623D\u9ad8\u65af\u70b9\u4e91\u8868\u793a\u53ca\u5927\u578bTransformer\u7f51\u7edc\uff0c\u5b9e\u73b0\u5b9e\u65f6\u52a8\u6001\u91cd\u5efa\u3002", "result": "DGS-LRM\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u5ab2\u7f8e\u4f18\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u957f\u7a0b3D\u8ddf\u8e2a\u3002", "conclusion": "DGS-LRM\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
