<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 86]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 7]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MobileRAG: A Fast, Memory-Efficient, and Energy-Efficient Method for On-Device RAG](https://arxiv.org/abs/2507.01079)
*Taehwan Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: MobileRAG是一种完全在设备上运行的RAG解决方案，通过轻量级向量搜索算法和选择性内容减少方法，显著降低了内存占用和功耗，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法假设计算资源充足，不适用于移动设备。MobileRAG旨在解决移动设备上资源受限的问题，实现高效、隐私保护的离线操作。

Method: 结合移动友好的EcoVector算法（分区加载索引数据）和选择性内容减少（SCR）方法，减少内存和CPU使用，同时过滤无关文本。

Result: MobileRAG在延迟、内存使用和功耗上显著优于传统方法，同时保持准确性，支持离线操作。

Conclusion: MobileRAG为资源受限环境提供了一种高效、隐私保护的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective on server
infrastructures, but its application on mobile devices is still underexplored
due to limited memory and power resources. Existing vector search and RAG
solutions largely assume abundant computation resources, making them
impractical for on-device scenarios. In this paper, we propose MobileRAG, a
fully on-device pipeline that overcomes these limitations by combining a
mobile-friendly vector search algorithm, \textit{EcoVector}, with a lightweight
\textit{Selective Content Reduction} (SCR) method. By partitioning and
partially loading index data, EcoVector drastically reduces both memory
footprint and CPU usage, while the SCR method filters out irrelevant text to
diminish Language Model (LM) input size without degrading accuracy. Extensive
experiments demonstrated that MobileRAG significantly outperforms conventional
vector search and RAG methods in terms of latency, memory usage, and power
consumption, while maintaining accuracy and enabling offline operation to
safeguard privacy in resource-constrained environments.

</details>


### [2] [Handling out-of-order input arrival in CEP engines on the edge combining optimistic, pessimistic and lazy evaluation](https://arxiv.org/abs/2507.01461)
*Styliani Kyrama,Anastasios Gounaris*

Main category: cs.DB

TL;DR: LimeCEP是一种混合CEP方法，通过懒评估、缓冲和推测处理高效处理数据不一致性，支持多模式检测，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 在复杂事件处理中，处理乱序、延迟和重复事件对实时分析至关重要，尤其是在资源受限设备上处理多源异构数据时。

Method: LimeCEP结合懒评估、缓冲和推测处理，集成Kafka实现高效消息排序、保留和去重，并提供可配置策略以平衡准确性、延迟和资源消耗。

Result: 相比SASE和FlinkCEP，LimeCEP延迟降低六个数量级，内存和CPU使用分别降低10倍和6倍，同时在高乱序输入流下保持接近完美的精度和召回率。

Conclusion: LimeCEP特别适合非云部署，在资源受限环境下表现出色。

Abstract: In Complex Event Processing, handling out-of-order, late, and duplicate
events is critical for real-time analytics, especially on resource-constrained
devices that process heterogeneous data from multiple sources. We present
LimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and
speculative processing to efficiently handle data inconsistencies while
supporting multi-pattern detection under relaxed semantics. LimeCEP integrates
Kafka for efficient message ordering, retention, and duplicate elimination, and
offers configurable strategies to trade off between accuracy, latency, and
resource consumption. Compared to state-of-the-art systems like SASE and
FlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up
to 10 times lower memory usage and 6 times lower CPU utilization, while
maintaining near-perfect precision and recall under high-disorder input
streams, making it well-suited for non-cloud deployments.

</details>


### [3] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 论文提出“数据代理”概念，利用大语言模型（LLMs）优化Data+AI系统的语义理解、推理和规划能力，以解决传统系统依赖人工的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Data+AI系统依赖人工协调，缺乏语义理解和规划能力，而LLMs的成功为系统革新提供了可能。

Method: 提出“数据代理”架构，整合知识理解、推理和规划能力，解决数据任务协调问题。

Result: 展示了多种数据代理系统（如数据科学代理、数据分析代理等），并探讨了设计中的挑战。

Conclusion: 数据代理有望革新Data+AI系统，但仍面临设计挑战。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


### [4] [PathDB: A system for evaluating regular path queries](https://arxiv.org/abs/2507.01755)
*Roberto García,Renzo Angles,Vicente Rojas,Sebastián Ferrada*

Main category: cs.DB

TL;DR: PathDB是一个基于Java的图数据库，专注于内存数据加载和查询，通过正则路径查询（RPQ）和封闭路径代数优化性能。


<details>
  <summary>Details</summary>
Motivation: 设计PathDB的目的是为了高效处理动态和复杂的路径查询，同时支持模块化优化。

Method: PathDB通过解析器、逻辑计划和物理计划三个主要组件处理路径查询，并支持正则路径查询和封闭路径代数。

Result: 基准测试表明，PathDB在执行时间和灵活性上优于DFS和BFS等基线方法。

Conclusion: PathDB的模块化设计和优化策略使其在复杂路径查询中表现出色。

Abstract: PathDB is a Java-based graph database designed for in-memory data loading and
querying. By utilizing Regular Path Queries (RPQ) and a closed path algebra,
PathDB processes paths through its three main components: the parser, the
logical plan, and the physical plan. This modular design allows for targeted
optimizations and modifications without impacting overall functionality.
Benchmark experiments illustrate PathDB's execution times and flexibility in
handling dynamic and complex path queries, compared to baseline methods like
Depth-First Search (DFS) and Breadth-First Search (BFS) guided by an automaton,
highlighting its optimizations that contribute to its performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [HERCULES: Hardware accElerator foR stoChastic schedULing in hEterogeneous Systems](https://arxiv.org/abs/2507.01113)
*Vairavan Palaniappan,Adam H. Ross,Amit Ranjan Trivedi,Debjit Pal*

Main category: cs.DC

TL;DR: 提出了一种基于FPGA的随机在线调度（SOS）加速器，通过硬件并行化和预计算降低调度延迟，显著提升异构计算系统的调度效率。


<details>
  <summary>Details</summary>
Motivation: 传统软件调度器在高性能计算（HPC）系统中因高开销和动态负载适应性问题表现不佳，尤其在异构系统中资源利用率低。

Method: 修改贪心成本选择策略，利用硬件并行化和精度量化设计FPGA加速器。

Result: 实验显示调度延迟显著降低，吞吐量高，能效提升，比单线程软件实现快1060倍。

Conclusion: SOS加速器为异构计算系统提供了一种高效、可扩展的调度解决方案。

Abstract: Efficient workload scheduling is a critical challenge in modern heterogeneous
computing environments, particularly in high-performance computing (HPC)
systems. Traditional software-based schedulers struggle to efficiently balance
workload distribution due to high scheduling overhead, lack of adaptability to
dynamic workloads, and suboptimal resource utilization. These pitfalls are
compounded in heterogeneous systems, where differing computational elements can
have vastly different performance profiles. To resolve these hindrances, we
present a novel FPGA-based accelerator for stochastic online scheduling (SOS).
We modify a greedy cost selection assignment policy by adapting existing cost
equations to engage with discretized time before implementing them into a
hardware accelerator design. Our design leverages hardware parallelism,
precalculation, and precision quantization to reduce job scheduling latency. By
introducing a hardware-accelerated approach to real-time scheduling, this paper
establishes a new paradigm for adaptive scheduling mechanisms in heterogeneous
computing systems. The proposed design achieves high throughput, low latency,
and energy-efficient operation, offering a scalable alternative to traditional
software scheduling methods. Experimental results demonstrate consistent
workload distribution, fair machine utilization, and up to 1060x speedup over
single-threaded software scheduling policy implementations. This makes the SOS
accelerator a strong candidate for deployment in high-performance computing
system, deeplearning pipelines, and other performance-critical applications.

</details>


### [6] [FLARE: A Dataflow-Aware and Scalable Hardware Architecture for Neural-Hybrid Scientific Lossy Compression](https://arxiv.org/abs/2507.01224)
*Wenqi Jia,Ying Huang,Jian Xu,Zhewen Hu,Sian Jin,Jiannan Tian,Yuede Ji,Miao Yin*

Main category: cs.DC

TL;DR: FLARE是一种新型硬件架构，用于高性能科学计算中的神经混合有损压缩，显著提升了吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）系统在科学模拟中生成海量数据，但现有压缩方法因混合计算模式导致性能瓶颈。

Method: 提出FLARE架构，通过数据流感知和模块化设计减少内存访问开销和气泡时间。

Result: FLARE在运行时和能效上分别实现了3.50×至96.07×和24.51×至520.68×的提升。

Conclusion: FLARE为HPC系统中的神经混合压缩提供了高效、可扩展的解决方案。

Abstract: Scientific simulation leveraging high-performance computing (HPC) systems is
crucial for modeling complex systems and phenomena in fields such as
astrophysics, climate science, and fluid dynamics, generating massive datasets
that often reach petabyte to exabyte scales. However, managing these vast data
volumes introduces significant I/O and network bottlenecks, limiting practical
performance and scalability. While cutting-edge lossy compression frameworks
powered by deep neural networks (DNNs) have demonstrated superior compression
ratios by capturing complex data correlations, their integration into HPC
workflows poses substantial challenges due to the hybrid non-neural and neural
computation patterns, causing excessive memory access overhead, large
sequential stalls, and limited adaptability to varying data sizes and workloads
in existing hardware platforms. To overcome these challenges and push the limit
of high-performance scientific computing, we for the first time propose FLARE,
a dataflow-aware and scalable hardware architecture for neural-hybrid
scientific lossy compression. FLARE minimizes off-chip data access, reduces
bubble overhead through efficient dataflow, and adopts a modular design that
provides both scalability and flexibility, significantly enhancing throughput
and energy efficiency on modern HPC systems. Particularly, the proposed FLARE
achieves runtime speedups ranging from $3.50 \times$ to $96.07 \times$, and
energy efficiency improvements ranging from $24.51 \times$ to $520.68 \times$,
across various datasets and hardware platforms.

</details>


### [7] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: 论文提出了一种混合云和本地服务器的资源规划与任务调度方法，重点解决金融行业中任务资源使用和持续时间的不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算基础设施的普及，组织需要同时管理云和本地服务器的资源。金融行业的任务特性受市场条件影响大，资源需求和任务时长具有不确定性，因此需要一种高效的容量规划和调度方法。

Method: 采用确定性估计器和基于配对采样的约束编程方法，平衡资源使用最小化和服务质量（按时完成任务）两个冲突目标。

Result: 基于配对采样的方法在降低峰值资源使用的同时，未影响服务质量，优于人工调度。

Conclusion: 该方法在金融行业等不确定性高的环境中，能有效实现资源优化和服务质量保障。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [8] [Optimal Dispersion Under Asynchrony](https://arxiv.org/abs/2507.01298)
*Debasish Pattanayak,Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: 论文提出了一种在异步设置下实现最优时间复杂度的分散算法，解决了匿名端口标记图中的分散问题。


<details>
  <summary>Details</summary>
Motivation: 分散问题是移动代理分布式计算中的基础任务，其复杂性源于匿名性和有限内存下的本地协调挑战。

Method: 基于一种新颖的技术，构建匿名图中的端口一树，以实现最优时间和内存复杂度。

Result: 提出了首个在异步设置下运行时间为O(k)、内存为O(log(k+Δ))的分散算法。

Conclusion: 该算法填补了异步设置下的复杂度空白，且其技术可能具有独立的研究价值。

Abstract: We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

</details>


### [9] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA是一个高效系统，用于在边缘设备上多租户环境中部署LLMs，通过自适应适配器选择、异构内存管理和批量LoRA推理显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上高效部署LLMs的挑战，如适配器选择复杂性和内存开销问题。

Method: 提出EdgeLoRA系统，包含自适应适配器选择、异构内存管理和批量LoRA推理三项创新。

Result: 在Llama3.1-8B模型上评估，EdgeLoRA在延迟和吞吐量上显著优于现有方案，吞吐量提升4倍，可同时服务更多适配器。

Conclusion: EdgeLoRA为多租户边缘环境中的LLMs部署提供了可扩展且高效的解决方案。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [10] [EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data](https://arxiv.org/abs/2507.01615)
*Alper Alimoglu,Kamil Erdayandi,Mustafa A. Mustafa,Ümit Cali*

Main category: cs.DC

TL;DR: EDGChain-E是一个基于区块链和IPFS的去中心化框架，用于管理加密能源数据，支持版本控制和多方协作，确保数据完整性、可追溯性和隐私。


<details>
  <summary>Details</summary>
Motivation: 解决能源数据管理中的隐私、协作和可追溯性问题，支持智能电网监控、需求预测和点对点能源交易等应用。

Method: 结合区块链、IPFS和Git版本控制，通过加密数据集和更新补丁，利用DAO实现协作治理，嵌入哈希标识符确保透明和不可变的数据追踪。

Result: 框架支持FAIR合规的数据溯源，实现安全协作和可信的数据变更追踪，适用于去中心化能源应用。

Conclusion: EDGChain-E为能源数据管理提供了一种安全、透明且可协作的解决方案，增强了数据的可重现性和信任。

Abstract: This paper proposes a new decentralized framework, named EDGChain-E
(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,
encrypted energy data using blockchain and the InterPlanetary File System. The
framework incorporates a Decentralized Autonomous Organization (DAO) to
orchestrate collaborative data governance across the lifecycle of energy
research and operations, such as smart grid monitoring, demand forecasting, and
peer-to-peer energy trading. In EDGChain-E, initial commits capture the full
encrypted datasets-such as smart meter readings or grid telemetry-while
subsequent updates are tracked as encrypted Git patches, ensuring integrity,
traceability, and privacy. This versioning mechanism supports secure
collaboration across multiple stakeholders (e.g., utilities, researchers,
regulators) without compromising sensitive or regulated information. We
highlight the framework's capability to maintain FAIR-compliant (Findable,
Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding
hash-based content identifiers in Merkle trees, the system enables transparent,
auditable, and immutable tracking of data changes, thereby supporting
reproducibility and trust in decentralized energy applications.

</details>


### [11] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: 论文提出了一种针对深度推荐模型（DLRM）嵌入层性能瓶颈的优化方法，通过定制数据流和不对称映射策略，显著提升了嵌入查找速度。


<details>
  <summary>Details</summary>
Motivation: DLRM的嵌入层因其随机内存访问成为性能瓶颈，影响了Meta数据中心79%的AI工作负载。

Method: 提出了四种单核嵌入表查找策略和一个多核SoC不对称映射框架。

Result: 在华为Ascend AI加速器上测试，速度提升1.5x至6.5x，极端不平衡分布下可达20x以上，且对查询分布更鲁棒。

Conclusion: 该方法显著提升了DLRM嵌入层的性能，适用于实际工作负载。

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [12] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique Mendonça,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Main category: cs.DC

TL;DR: Alps研究基础设施利用GH200技术，提供10,752个GPU，为AI和ML研究人员提供显著计算优势。本文探讨如何扩展HPC服务以更好支持ML工作负载，并提出技术增强方案。


<details>
  <summary>Details</summary>
Motivation: 传统HPC服务无法满足ML社区的动态需求，需扩展服务能力以支持ML工作负载。

Method: 提出多项技术增强，包括用户环境设计、性能筛查工具、可观察性能力、节点审查工具、服务平面基础设施和存储基础设施。

Result: 这些增强旨在提升ML工作负载在HPC系统上的执行效率、系统可用性和弹性，并更好地满足ML社区需求。

Conclusion: 本文将这些提案置于HPC基础设施服务社区变化的更广泛背景下讨论。

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Faster Algorithm for Second (s,t)-mincut and Breaking Quadratic barrier for Dual Edge Sensitivity for (s,t)-mincut](https://arxiv.org/abs/2507.01366)
*Surender Baswana,Koustav Bhanja,Anupam Roy*

Main category: cs.DS

TL;DR: 研究第二最小容量的(s,t)-割，提出改进算法和理论结果，包括更高效的计算方法和空间优化结构。


<details>
  <summary>Details</summary>
Motivation: 探索第二最小容量的(s,t)-割的计算方法，以改进现有算法的效率，并研究其在双边敏感性查询中的应用。

Method: 1. 改进算法减少最大流计算次数；2. 设计空间优化的数据结构存储最小+1割；3. 研究简单图中的双边敏感性查询。

Result: 1. 算法复杂度从O(n^2)降至O(√n)；2. 空间占用从O(mn)降至O(m)；3. 在简单图中突破二次空间限制。

Conclusion: 研究为第二最小(s,t)-割提供了高效算法和紧凑数据结构，拓展了其在图论中的应用。

Abstract: We study (s,t)-cuts of second minimum capacity and present the following
algorithmic and graph-theoretic results.
  1. Vazirani and Yannakakis [ICALP 1992] designed the first algorithm for
computing an (s,t)-cut of second minimum capacity using $O(n^2)$ maximum
(s,t)-flow computations. For directed integer-weighted graphs, we significantly
improve this bound by designing an algorithm that computes an $(s,t)$-cut of
second minimum capacity using $O(\sqrt{n})$ maximum (s,t)-flow computations
w.h.p. To achieve this result, a close relationship of independent interest is
established between $(s,t)$-cuts of second minimum capacity and global mincuts
in directed weighted graphs.
  2. Minimum+1 (s,t)-cuts have been studied quite well recently [Baswana,
Bhanja, and Pandey, ICALP 2022], which is a special case of second
(s,t)-mincut.
  (a) For directed multi-graphs, we design an algorithm that, given any maximum
(s,t)-flow, computes a minimum+1 (s,t)-cut, if it exists, in $O(m)$ time.
  (b) The existing structures for storing and characterizing all minimum+1
(s,t)-cuts occupy $O(mn)$ space. For undirected multi-graphs, we design a DAG
occupying only $O(m)$ space that stores and characterizes all minimum+1
(s,t)-cuts.
  3. The study of minimum+1 (s,t)-cuts often turns out to be useful in
designing dual edge sensitivity oracles -- a compact data structure for
efficiently reporting an (s,t)-mincut after insertion/failure of any given pair
of query edges. It has been shown recently [Bhanja, ICALP 2025] that any dual
edge sensitivity oracle for (s,t)-mincut in undirected multi-graphs must occupy
${\Omega}(n^2)$ space in the worst-case, irrespective of the query time. For
simple graphs, we break this quadratic barrier while achieving a non-trivial
query time.

</details>


### [14] [Dynamic Similarity Graph Construction with Kernel Density Estimation](https://arxiv.org/abs/2507.01696)
*Steinar Laenen,Peter Macgregor,He Sun*

Main category: cs.DS

TL;DR: 本文提出了一种动态核密度估计（KDE）数据结构，用于高效维护查询点的估计值，并基于此设计了动态谱聚类算法。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境下KDE问题的高效计算需求，并扩展至动态谱聚类。

Method: 设计动态数据结构维护KDE估计，构建稀疏相似图，开发动态谱聚类算法。

Result: 在合成和真实数据集上验证了算法的有效性。

Conclusion: 提出的方法在动态环境下高效且实用。

Abstract: In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

</details>


### [15] [SPARSE-PIVOT: Dynamic correlation clustering for node insertions](https://arxiv.org/abs/2507.01830)
*Mina Dalirrooyfard,Konstantin Makarychev,Slobodan Mitrović*

Main category: cs.DS

TL;DR: 提出了一种动态相关性聚类算法，改进现有方法的近似因子，并在实践中表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决动态图中节点逐步添加时的聚类问题，提升算法效率和近似精度。

Method: 使用数据库查询访问输入图，逐步更新聚类，实现低摊销更新时间和高近似因子。

Result: 算法摊销更新时间为$O_{\epsilon}(\log^{O(1)}(n))$，近似因子为$20+\varepsilon$，优于现有方法。

Conclusion: 新算法在理论和实践中均优于现有方法，适用于动态相关性聚类问题。

Abstract: We present a new Correlation Clustering algorithm for a dynamic setting where
nodes are added one at a time. In this model, proposed by Cohen-Addad,
Lattanzi, Maggiori, and Parotsidis (ICML 2024), the algorithm uses database
queries to access the input graph and updates the clustering as each new node
is added. Our algorithm has the amortized update time of
$O_{\epsilon}(\log^{O(1)}(n))$. Its approximation factor is $20+\varepsilon$,
which is a substantial improvement over the approximation factor of the
algorithm by Cohen-Addad et al. We complement our theoretical findings by
empirically evaluating the approximation guarantee of our algorithm. The
results show that it outperforms the algorithm by Cohen-Addad et al.~in
practice.

</details>


### [16] [Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition](https://arxiv.org/abs/2507.01873)
*Anders Aamand,Justin Y. Chen,Mina Dalirrooyfard,Slobodan Mitrović,Yuriy Nevmyvaka,Sandeep Silwal,Yinzhan Xu*

Main category: cs.DS

TL;DR: 论文研究了差分隐私下的图割稀疏化算法，突破了现有高效算法在加性误差上的限制，提出了一种多项式时间算法，显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 图割稀疏化在算法、隐私和机器学习中具有基础性意义，但现有高效隐私算法的加性误差较高，需要突破这一限制。

Method: 提出了一种基于扩展分解的差分隐私多项式时间算法，用于生成私有合成图，近似所有割。

Result: 算法在多项式时间内实现了1+γ的乘性误差和n^1.25+o(1)的加性误差，显著优于现有高效算法。

Conclusion: 该研究为高效隐私图割稀疏化提供了新方法，突破了加性误差的瓶颈。

Abstract: We study differentially private algorithms for graph cut sparsification, a
fundamental problem in algorithms, privacy, and machine learning. While
significant progress has been made, the best-known private and efficient cut
sparsifiers on $n$-node graphs approximate each cut within
$\widetilde{O}(n^{1.5})$ additive error and $1+\gamma$ multiplicative error for
any $\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, "inefficient"
algorithms, i.e., those requiring exponential time, can achieve an
$\widetilde{O}(n)$ additive error and $1+\gamma$ multiplicative error
[Eli{\'a}{\v{s}}, Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the
$n^{1.5}$ additive error barrier for private and efficient cut sparsification.
We present an $(\varepsilon,\delta)$-DP polynomial time algorithm that, given a
non-negative weighted graph, outputs a private synthetic graph approximating
all cuts with multiplicative error $1+\gamma$ and additive error $n^{1.25 +
o(1)}$ (ignoring dependencies on $\varepsilon, \delta, \gamma$).
  At the heart of our approach lies a private algorithm for expander
decomposition, a popular and powerful technique in (non-private) graph
algorithms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: 研究发现，在敏捷社区实践中，线上互动的心理安全感显著低于面对面互动，且低心理安全感会降低参与者的持续贡献意愿。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在敏捷软件开发中，虚拟社区实践中心理安全感的影响及其重要性。

Method: 采用混合方法研究，通过143名参与者的调查数据进行分析。

Result: 结果显示，线上互动的心理安全感较低，且低心理安全感与参与者的持续贡献意愿和风险规避行为相关。

Conclusion: 结论建议通过明确规范、结构化引导和积极管理来提升心理安全感，为虚拟或混合工作环境提供实践指导。

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [18] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,Márcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: 提出了一种静态分析技术，用于检测Python重构过程中引入的类型错误，并在开源项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Python的动态类型系统在自动化重构中容易引入类型错误，影响软件可靠性和开发效率。

Method: 采用静态分析技术，评估了Rope重构实现，并在开源Python项目上进行了测试。

Result: 在1,152次重构尝试中发现了29个错误，部分问题也存在于主流IDE中。

Conclusion: 当前Python重构工具的健壮性需提升，以确保代码转换的正确性和软件维护的可靠性。

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [19] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: WIRL是一个基于LLM的代码接线代理，通过结合检索增强生成（RAG）技术，有效解决代码粘贴中的变量替换问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 代码粘贴修改是常见实践，但现有解决方案未能充分利用上下文信息，导致变量替换效果不佳。

Method: WIRL结合LLM、定制工具和协调模块，采用混合策略（规则步骤和状态机决策）进行智能变量替换。

Result: 在真实数据集上，WIRL的精确匹配精度达91.7%，召回率90.0%，显著优于其他方法。

Conclusion: WIRL为现代IDE提供了更智能、上下文感知的开发辅助工具，具有广泛实用价值。

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [20] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出了一种通过运行时类型追踪（type tracing）解决动态类型语言（如Python）中单元测试生成问题的技术，显著提升了代码覆盖率和测试质量。


<details>
  <summary>Details</summary>
Motivation: 动态类型语言缺乏静态类型信息，阻碍了传统测试生成器的效果。本文旨在通过运行时类型信息提取解决这一问题。

Method: 在Pynguin测试生成框架中引入类型追踪技术，通过运行时观察参数使用和返回值类型，逐步优化类型信息。

Result: 实验显示，该方法使分支覆盖率提升高达90.0%，突变分数提高，且类型信息质量与先进类型推断工具相当。

Conclusion: 类型追踪有效解决了动态类型语言测试生成的类型信息缺失问题，显著提升了测试效果。

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [21] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: DaiFu是一个轻量级的深度学习系统崩溃恢复框架，通过代码转换实现即时恢复，显著减少恢复时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统崩溃频繁且浪费资源，现有恢复方案（如检查点重试）过于笨重，无法高效应对小错误或瞬时错误。

Method: DaiFu通过轻量级代码转换，拦截崩溃并动态更新运行上下文（如代码、配置等），实现即时恢复。

Result: DaiFu将恢复时间缩短1372倍，开销低于0.40%，并在7种崩溃场景中验证了其有效性。

Conclusion: DaiFu为深度学习系统提供了高效的崩溃恢复方案，显著提升开发效率和资源利用率。

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [22] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: APRMCTS利用蒙特卡洛树搜索（MCTS）改进基于大语言模型（LLM）的自动程序修复（APR），解决了现有方法局部探索和冗余搜索的问题，显著提升了修复效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR技术采用试错策略，存在局部探索导致的修复效果有限和冗余搜索效率低的问题。

Method: APRMCTS通过迭代树搜索全局评估补丁并选择最有潜力的进行细化，结合MCTS优化搜索过程。

Result: 在Defects4J的835个错误上，APRMCTS与GPT-3.5结合修复了201个错误，优于现有方法，且在较小补丁规模下表现突出。

Conclusion: APRMCTS在修复复杂错误时表现出高效性和低成本优势，是LLM-based APR的有效改进。

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [A Full-Stack Platform Architecture for Self-Organised Social Coordination](https://arxiv.org/abs/2507.01239)
*Matthew Scott,Jeremy Pitt*

Main category: cs.NI

TL;DR: 论文提出了一种开源的全栈架构，旨在通过民主化平台来支持本地社区的自我组织社交协调，以对抗平台化的垄断趋势。


<details>
  <summary>Details</summary>
Motivation: 应对平台化带来的中心化和垄断问题，赋能本地社区通过自我组织进行社交协调。

Method: 开发一个支持分发、克隆、生成性和多种托管选项的开源全栈架构，包括元平台、基础平台、插件和开发者/用户工具链。

Result: 通过两个案例研究（体育协会平台和集体学习平台）验证了该架构的可行性。

Conclusion: 通过全栈架构和配套工具链，可以在应用层实现自我组织。

Abstract: To mitigate the restrictive centralising and monopolistic tendencies of
platformisation, we aim to empower local communities by democratising platforms
for self-organised social coordination. Our approach is to develop an
open-source, full-stack architecture for platform development that supports
ease of distribution and cloning, generativity, and a variety of hosting
options. The architecture consists of a meta-platform that is used to
instantiate a base platform with supporting libraries for generic functions,
and plugins (intended to be supplied by third parties) for customisation of
application-specification functionality for self-organised social coordination.
Associated developer- and user-oriented toolchains support the instantiation
and customisation of a platform in a two-stage process. This is demonstrated
through the proof-of-concept implementation of two case studies: a platform for
regular sporting association, and a platform for collective group study. We
conclude by arguing that self-organisation at the application layer can be
achieved by the specific supporting functionality of a full-stack architecture
with complimentary developer and user toolchains.

</details>


### [24] [Fluid Aerial Networks: UAV Rotation for Inter-Cell Interference Mitigation](https://arxiv.org/abs/2507.01289)
*Enzhi Zhou,Yue Xiao,Ziyue Liu,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.NI

TL;DR: 该论文研究了无人机辅助网络中基于位置的波束成形，提出了一种通过无人机旋转减少小区间干扰的新方法，显著提升了多小区系统容量。


<details>
  <summary>Details</summary>
Motivation: 随着无人机作为空中基站的快速发展，如何利用其灵活性和移动性提升地面网络服务质量和应急通信能力成为研究重点。

Method: 论文提出了一种流体空中网络，利用无人机旋转优化波束成形权重，并通过低复杂度算法设计最优旋转角度以减少干扰。

Result: 仿真结果表明，所提出的无人机旋转方案在干扰受限区域能显著减少干扰，多小区总速率比固定方向无人机提升约10%。

Conclusion: 无人机旋转是一种有效减少小区间干扰并提升网络效率的方法，尤其在干扰受限区域表现突出。

Abstract: With the rapid development of aerial infrastructure, unmanned aerial vehicles
(UAVs) that function as aerial base stations (ABSs) extend terrestrial network
services into the sky, enabling on-demand connectivity and enhancing emergency
communication capabilities in cellular networks by leveraging the flexibility
and mobility of UAVs. In such a UAV-assisted network, this paper investigates
position-based beamforming between ABSs and ground users (GUs). To mitigate
inter-cell interference, we propose a novel fluid aerial network that leverages
ABS rotation to increase multi-cell capacity and overall network efficiency.
Specifically, considering the line-of-sight channel model, the spatial
beamforming weights are determined by the orientation angles of the GUs. In
this direction, we examine the beamforming gain of a two-dimensional
multiple-input multiple-output (MIMO) array at various ground positions,
revealing that ABS rotation significantly affects multi-user channel
correlation and inter-cell interference. Based on these findings, we propose an
alternative low-complexity algorithm to design the optimal rotation angle for
ABSs, aiming to reduce inter-cell interference and thus maximize the sum rate
of multi-cell systems. In simulations, exhaustive search serves as a benchmark
to validate the optimization performance of the proposed sequential ABS
rotation scheme. Moreover, simulation results demonstrate that, in
interference-limited regions, the proposed ABS rotation paradigm can
significantly reduce inter-cell interference in terrestrial networks and
improve the multi-cell sum rate by approximately 10\% compared to
fixed-direction ABSs without rotation.

</details>


### [25] [Multi-User Generative Semantic Communication with Intent-Aware Semantic-Splitting Multiple Access](https://arxiv.org/abs/2507.01333)
*Jiayi Lu,Wanting Yang,Zehui Xiong,Rahim Tafazolli,Tony Q. S. Quek,Mérouane Debbah,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出了一种多用户生成语义通信框架（SS-MGSC），用于车辆网络中的内容分发，通过共享知识库和个性化语义信息传输，结合扩散模型生成高质量图像，并设计了新的语义效率评分（SES）作为优化目标。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的快速发展，语义通信（SemCom）成为可靠高效通信的新范式。本文旨在解决多用户内容分发中需求多样且重叠的问题。

Method: 提出SS-MGSC框架，构建意图感知共享知识库（SKB），广播公共语义信息并传输个性化语义信息。接收端采用ControlNet增强的扩散模型生成图像，设计SES作为优化目标，并使用强化学习算法解决联合优化问题。

Result: 仿真结果表明该方案在多用户语义提取和波束成形中具有有效性。

Conclusion: SS-MGSC框架通过结合语义通信和生成模型，显著提升了多用户内容分发的效率和个性化体验。

Abstract: With the booming development of generative artificial intelligence (GAI),
semantic communication (SemCom) has emerged as a new paradigm for reliable and
efficient communication. This paper considers a multi-user downlink SemCom
system, using vehicular networks as the representative scenario for multi-user
content dissemination. To address diverse yet overlapping user demands, we
propose a multi-user Generative SemCom-enhanced intent-aware semantic-splitting
multiple access (SS-MGSC) framework. In the framework, we construct an
intent-aware shared knowledge base (SKB) that incorporates prior knowledge of
semantic information (SI) and user-specific preferences. Then, we designate the
common SI as a one-hot semantic map that is broadcast to all users, while the
private SI is delivered as personalized text for each user. On the receiver
side, a diffusion model enhanced with ControlNet is adopted to generate
high-quality personalized images. To capture both semantic relevance and
perceptual similarity, we design a novel semantic efficiency score (SES) metric
as the optimization objective. Building on this, we formulate a joint
optimization problem for multi-user semantic extraction and beamforming, solved
using a reinforcement learning-based algorithm due to its robustness in
high-dimensional settings. Simulation results demonstrate the effectiveness of
the proposed scheme.

</details>


### [26] [MmBack: Clock-free Multi-Sensor Backscatter with Synchronous Acquisition and Multiplexing](https://arxiv.org/abs/2507.01360)
*Yijie Li,Weichong Ling,Taiting Lu,Yi-Chao Chen,Vaishnavi Ranganathan,Lili Qiu,Jingxian Wang*

Main category: cs.NI

TL;DR: mmBack是一种低功耗、无时钟的反向散射标签，通过共享参考信号实现多传感器同步数据采集和复用，无需板载时钟。


<details>
  <summary>Details</summary>
Motivation: 现有反向散射标签设计仅支持单个传感器，增加了空间开销；而多传感器复用方法依赖板载时钟或多调制链，增加了成本和体积，且易受时间漂移影响。

Method: mmBack利用环境射频激励提取共享参考信号同步传感器输入，并通过电压分配方案将多传感器输入复用到单一调制链中。接收端采用频率跟踪算法和有限状态机进行解复用。

Result: mmBack的ASIC设计功耗为25.56uW，原型支持5个5kHz带宽或3个18kHz带宽的并发传感器流，信号重构平均SNR超过15dB。

Conclusion: mmBack提供了一种高效、低功耗的多传感器同步数据采集和复用解决方案，克服了现有方法的局限性。

Abstract: Backscatter tags provide a low-power solution for sensor applications, yet
many real-world scenarios require multiple sensors-often of different types-for
complex sensing tasks. However, existing designs support only a single sensor
per tag, increasing spatial overhead. State-of-the-art approaches to
multiplexing multiple sensor streams on a single tag rely on onboard clocks or
multiple modulation chains, which add cost, enlarge form factor, and remain
prone to timing drift-disrupting synchronization across sensors.
  We present mmBack, a low-power, clock-free backscatter tag that enables
synchronous multi-sensor data acquisition and multiplexing over a single
modulation chain. mmBack synchronizes sensor inputs in parallel using a shared
reference signal extracted from ambient RF excitation, eliminating the need for
an onboard timing source. To efficiently multiplex sensor data, mmBack designs
a voltage-division scheme to multiplex multiple sensor inputs as backscatter
frequency shifts through a single oscillator and RF switch. At the receiver,
mmBack develops a frequency tracking algorithm and a finite-state machine for
accurate demultiplexing. mmBack's ASIC design consumes 25.56uW, while its
prototype supports 5 concurrent sensor streams with bandwidths of up to 5kHz
and 3 concurrent sensor streams with bandwidth of up to 18kHz. Evaluation shows
that mmBack achieves an average SNR surpassing 15dB in signal reconstruction.

</details>


### [27] [Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions](https://arxiv.org/abs/2507.01773)
*Bo Yang,Ruihuai Liang,Weixin Li,Han Wang,Xuelin Cao,Zhiwen Yu,Samson Lasaulce,Mérouane Debbah,Mohamed-Slim Alouini,H. Vincent Poor,Chau Yuen*

Main category: cs.NI

TL;DR: 该论文综述了生成式AI在网络优化中的应用，分析了其优势和局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在网络优化中的潜力与不足，填补现有文献中对生成模型关键限制的研究空白。

Method: 通过分类网络优化问题为一次性优化和马尔可夫决策过程，回顾了生成扩散模型和大预训练模型的应用，并提供了理论泛化界限。

Result: 揭示了生成式AI在约束满足、概念理解和输出概率性等方面的局限性，并提出了未来研究方向。

Conclusion: 生成式AI在网络优化中具有潜力，但需解决其局限性，并深入理解生成与优化的理论联系。

Abstract: While interest in the application of generative AI (GenAI) in network
optimization has surged in recent years, its rapid progress has often
overshadowed critical limitations intrinsic to generative models that remain
insufficiently examined in existing literature. This survey provides a
comprehensive review and critical analysis of GenAI in network optimization. We
focus on the two dominant paradigms of GenAI including generative diffusion
models (GDMs) and large pre-trained models (LPTMs), and organize our discussion
around a categorization we introduce, dividing network optimization problems
into two primary formulations: one-shot optimization and Markov decision
process (MDP). We first trace key works, including foundational contributions
from the AI community, and categorize current efforts in network optimization.
We also review frontier applications of GDMs and LPTMs in other networking
tasks, providing additional context. Furthermore, we present theoretical
generalization bounds for GDMs in both one-shot and MDP settings, offering
insights into the fundamental factors affecting model performance. Most
importantly, we reflect on the overestimated perception of GenAI's general
capabilities and caution against the all-in-one illusion it may convey. We
highlight critical limitations, including difficulties in constraint
satisfying, limited concept understanding, and the inherent probabilistic
nature of outputs. We also propose key future directions, such as bridging the
gap between generation and optimization. Although they are increasingly
integrated in implementations, they differ fundamentally in both objectives and
underlying mechanisms, necessitating a deeper understanding of their
theoretical connections. Ultimately, this survey aims to provide a structured
overview and a deeper insight into the strengths, limitations, and potential of
GenAI in network optimization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ提出了一种少样本启发的生成式零样本学习框架，通过动态调整属性评分和原型估计，减少对大规模特征合成的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统生成式零样本学习方法需要大量计算资源和合成数据，违背了零样本学习的初衷。FSIGenZ旨在通过动态属性评分和原型估计解决这一问题。

Method: 引入模型特定属性评分（MSAS）动态调整属性，估计组级原型作为未见类的代表性特征，并使用双重目的语义正则化（DPSR）训练语义感知对比分类器（SCC）。

Result: 在SUN、AwA2和CUB基准测试中，FSIGenZ使用更少的合成特征实现了竞争性性能。

Conclusion: FSIGenZ通过动态属性评分和原型估计，有效减少了生成式零样本学习对大规模特征合成的依赖，同时保持了高性能。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [29] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant是一种创新的后训练量化框架，通过双钟形分布变换减少量化误差，实现1位权重压缩和6位激活量化，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）面临计算和内存挑战，量化是解决方案，但量化误差和激活异常值限制了其效果。

Method: DBellQuant采用可学习的双钟形变换（LTDB）算法，将单钟形权重分布转换为双钟形以减少二值化误差，并通过逆变换平滑激活。

Result: 在Wikitext2数据集上，DBellQuant在LLaMA2-13B上实现14.39的困惑度，优于BiLLM的21.35。

Conclusion: DBellQuant在激进量化的同时保持模型性能，为LLMs的实际应用提供了高效压缩方案。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [30] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: 论文研究了自监督学习中非对比方法的优化和动态系统视角，证明停止梯度和指数移动平均能避免表示崩溃，并在理论上验证其稳定性。


<details>
  <summary>Details</summary>
Motivation: 探讨自监督学习中非对比方法的优化和动态系统行为，避免表示崩溃的理论基础。

Method: 从优化和动态系统理论角度分析停止梯度和指数移动平均的作用，验证其在避免崩溃中的有效性。

Result: 证明这些方法虽不优化原始目标函数，但能避免崩溃，并在线性情况下验证其稳定性。

Conclusion: 停止梯度和指数移动平均是避免表示崩溃的有效方法，且其极限点是稳定的平衡点。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [31] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: PathCoT是一种新的零样本CoT提示方法，通过整合病理学专家知识和自评估步骤，提升多模态大语言模型在病理视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在病理视觉推理任务中表现不佳，主要因为缺乏领域知识和CoT推理步骤可能引入错误。

Method: PathCoT结合病理学专家知识指导模型推理，并引入自评估步骤以减少答案偏差。

Result: 在PathMMU数据集上的实验证明了PathCoT在病理视觉理解和推理任务中的有效性。

Conclusion: PathCoT通过整合专家知识和自评估，显著提升了模型在病理视觉推理任务中的性能。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [32] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: 该研究利用四种机器学习算法（MLP、随机森林、线性回归、SVM）重建甲烷燃料燃烧模拟中的FGM库，最终选择MLP方法，并通过超参数调优达到99.81%的准确率。


<details>
  <summary>Details</summary>
Motivation: FGM在燃烧模型中精度高，但实际应用需要大量内存资源，研究旨在通过机器学习优化FGM库的构建。

Method: 使用四种机器学习算法（MLP、随机森林、线性回归、SVM）重建FGM库，并通过超参数调优优化MLP模型。

Result: MLP方法表现最佳，优化后的模型（四隐藏层，神经元数分别为10、15、20、25）准确率达99.81%。

Conclusion: MLP是重建FGM库的最佳选择，超参数调优显著提升模型性能，为甲烷燃烧模拟提供高效工具。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [33] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 论文介绍了将基于PyTorch的几何学习框架移植到Gaudi-v2 HPU的经验，提供了核心工具和教程，降低了非CUDA硬件上的研究门槛。


<details>
  <summary>Details</summary>
Motivation: 解决在非CUDA硬件（如Gaudi-v2 HPU）上运行几何学习框架的工程挑战，促进跨平台研究。

Method: 开发核心工具恢复关键操作，提供教程和实例分析，整理为公开GitHub仓库。

Result: 成功移植框架，提供实用工具和资源，支持非CUDA硬件上的几何学习研究。

Conclusion: 工作为几何学习在非CUDA硬件上的应用提供了基础，促进了进一步优化和跨平台兼容性。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [34] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的多视图动态决策框架，用于多组学数据分类，旨在降低测试成本的同时保持高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 多组学技术成本高昂且可能导致资源浪费，因此需要一种方法在减少冗余测试的同时保持诊断准确性。

Method: 在单组学层面，通过改进神经网络激活函数生成Dirichlet分布参数，量化分类结果的信度和不确定性；在多组学层面，基于Dempster-Shafer理论融合异构模态，动态决策机制逐步引入数据直至满足置信度阈值。

Result: 在四个基准数据集（ROSMAP、LGG、BRCA、KIPAN）上，50%以上病例仅需单组学数据即可准确分类，同时诊断性能与全组学模型相当。

Conclusion: 该方法有效减少了冗余测试，同时保持了诊断准确性和生物学洞察力。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [35] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 该研究提出了一种数据驱动的方法，利用历史数据预测2025年利比亚班加西的电力负荷、发电量和缺口，LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 班加西电力供应不稳定，基础设施有限，准确的电力预测对电网稳定和能源规划至关重要。

Method: 使用多种时间序列模型（如ARIMA、季节性ARIMA、LSTM等），并对数据进行缺失值填补、异常值平滑和对数转换。

Result: LSTM模型在预测非平稳和季节性模式方面表现最优，整合了温度和湿度等外生因素。

Conclusion: 研究结果为政策制定者和电网运营商提供了实用见解，有助于在数据稀缺、不稳定的地区进行主动负荷管理和资源规划。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [36] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 论文研究了如何优化基础模型以预测罕见、尖峰事件（如生产中断），并与经典随机模型进行比较，发现基础模型在特定场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 基础模型在时间序列预测中表现出色，但尚未用于罕见、尖峰事件的预测，这是极端事件的特殊情况，具有挑战性。

Method: 优化一种先进的基础模型，用于预测高性能机器学习服务的偶发性生产中断，并与经典随机模型（如移动平均和自回归）进行误差比较。

Result: 基础模型在特定数据模式上表现优于随机模型，用于估计某根因的年中断统计时，误差低于6%。

Conclusion: 基础模型在预测罕见、尖峰事件方面具有潜力，尤其在特定数据模式上优于传统方法。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [37] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出了一种基于TVM编译器的工作流，用于高效地将AI工作负载映射到RISC-V向量单元，相比GCC自动向量化和muRISCV-NN，分别提升了46%和29%的执行延迟。


<details>
  <summary>Details</summary>
Motivation: RISC-V向量扩展（RVV）在AI工作负载加速中具有潜力，但缺乏高效的自动调优框架，限制了复杂AI工作负载的部署。

Method: 将RVV扩展集成到TVM的MetaSchedule框架中，通过概率程序框架调优张量操作，并在FPGA上实现多种RISC-V SoC进行验证。

Result: 相比GCC自动向量化和muRISCV-NN，执行延迟分别平均提升46%和29%，且代码内存占用更小。在商用RISC-V SoC上，比LLVM快35%。

Conclusion: 提出的方法显著提升了RISC-V RVV扩展的AI工作负载效率，适合嵌入式设备，并开源供社区扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [38] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 本研究通过结合GNN和LLM的混合推荐系统，优化推理延迟和训练效率，采用量化、LoRA、蒸馏等技术，结合硬件加速（FPGA、DeepSpeed），显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在线服务对高效实时推荐系统的需求推动了研究，旨在解决混合GNN-LLM系统的计算瓶颈。

Method: 采用混合GNN-LLM架构，结合量化、LoRA、蒸馏等优化策略，并使用FPGA和DeepSpeed进行硬件加速。

Result: 最优配置（Hybrid + FPGA + DeepSpeed）在NDCG@10上提升13.6%（0.75），延迟40-60ms；LoRA减少训练时间66%（3.8小时）。

Conclusion: 硬件-软件协同设计和参数高效调优使混合模型优于独立GNN或LLM方法，推荐使用FPGA和LoRA进行实时部署，未来可探索联邦学习和高级融合架构。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [39] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 论文介绍了yProv4ML库，用于收集符合W3C PROV和ProvML标准的JSON格式溯源数据，以优化大规模AI模型的训练效率、执行时间、准确性和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模AI模型需求的增长，如何在计算效率、执行时间、准确性和能耗之间实现平衡成为关键挑战。溯源数据为理解资源使用模式、识别低效环节及确保AI开发的可重复性和问责性提供了支持。

Method: 提出yProv4ML库，支持灵活扩展，允许通过插件集成其他数据收集工具，并与yProv框架完全集成，适用于工作流管理系统。

Result: yProv4ML库能够高效收集和分析溯源数据，帮助优化资源利用和能源效率。

Conclusion: yProv4ML为大规模AI模型的分布式资源优化和能源高效利用提供了实用工具，支持AI开发的透明性和可扩展性。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [40] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FSTA的分解技术，结合L2Seg神经网络框架，显著加速了车辆路径问题（VRP）的迭代求解器。


<details>
  <summary>Details</summary>
Motivation: 现有迭代求解器在解决大规模VRP时存在冗余计算问题，因为解的大部分在迭代中保持稳定。

Method: 采用FSTA技术保留稳定解段，并通过L2Seg神经网络智能识别稳定与不稳定部分。

Result: 实验表明，L2Seg能将求解器速度提升高达7倍，且其非自回归与自回归变体的协同效果最佳。

Conclusion: L2Seg是一个灵活框架，适用于多种VRP求解器，显著提升了计算效率。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [41] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 本文提出yProv4ML框架，用于以PROV-JSON格式捕获机器学习过程中的溯源信息，解决现有工具在透明度和数据格式上的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的发展缺乏透明度和严谨性，特别是在超参数选择和数据溯源方面存在挑战。现有工具如MLFlow虽能自动化收集信息，但使用专有格式且忽视数据溯源。

Method: 提出yProv4ML框架，通过最小代码修改捕获机器学习过程中的溯源信息，并以PROV-JSON格式存储。

Result: yProv4ML能够有效记录机器学习过程中的关键信息，提升透明度和可追溯性。

Conclusion: yProv4ML为机器学习开发提供了更透明和标准化的溯源解决方案，弥补了现有工具的不足。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [42] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 提出了一种基于PPO的强化学习方法训练神经模糊控制器，相比之前的DQN方法，表现出更低的方差和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 改进现有的基于DQN的神经模糊控制器训练方法，利用PPO的稳定性和效率提升性能。

Method: 使用PPO代替DQN，构建一个稳定的on-policy actor-critic循环，并在CartPole-v1环境中进行测试。

Result: PPO训练的模糊控制器在CartPole-v1上实现了500 +/- 0的平均回报，方差更小且收敛更快。

Conclusion: PPO为训练可解释的神经模糊控制器提供了一种有前景的方法。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [43] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers通过引入Clifford代数优化PDE建模，在CPU上实现了比标准PyTorch快30%的性能。


<details>
  <summary>Details</summary>
Motivation: 提升PDE建模效率，优化神经网络中Clifford卷积层和多向量激活层的推理性能。

Method: 在CPU上优化2/3D Clifford卷积层和多向量激活层的实现。

Result: 在较大数据和网络规模下，性能比标准PyTorch实现快30%。

Conclusion: Clifford Neural Layers在PDE建模中具有显著性能优势，代码已开源。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [44] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 论文提出了一种基于距离的聚合方法Dist-FedAvg，用于提升图联邦推荐系统中的个性化和聚合效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习聚合方法忽视了用户嵌入的独特性和用户相似性对推荐效果的关键作用，且缺乏对动态用户交互的适应性。

Method: 提出Dist-FedAvg方法，通过为相似嵌入用户分配更高权重，并保留锚用户的影响力，优化聚合过程。

Result: 实验表明，Dist-FedAvg在多个数据集上优于基线方法，提升了推荐准确性。

Conclusion: Dist-FedAvg在保持与现有联邦学习框架兼容的同时，显著提升了推荐系统的性能。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [45] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 提出了一种基于有向无环图（DAG）的快速模型分割算法，通过最大流方法找到最优分割，显著降低训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决复杂AI模型分割的高计算复杂度问题，提升设备端计算效率。

Method: 将AI模型表示为DAG，重新建模为最小s-t割问题，提出快速DAG分割算法和块状分割算法。

Result: 算法在毫秒内找到最优分割，动态边缘网络中训练延迟降低24.62%-38.95%。

Conclusion: 提出的算法在计算效率和性能上优于现有基准，适用于动态边缘网络。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [46] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 提出了一种动态调整神经网络架构的新方法，通过蒙特卡洛树搜索实现训练中的架构优化，适用于多变量时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定架构神经网络的局限性，探索动态调整架构以提升模型性能。

Method: 使用蒙特卡洛树搜索模拟网络行为，动态调整神经网络架构（扩展或收缩）。

Result: 在多变量时间序列分类任务中表现优异，验证了方法的鲁棒性和适应性。

Conclusion: 动态架构调整方法在视觉和时间序列数据中均表现出色，尤其适合多变量时间序列分类。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [47] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的心脏感知基础模型（CSFM），通过生成式掩码预训练策略，从大规模多模态数据中学习统一表示，显著提升了心脏信号分析的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖于同质数据集和静态定制模型，难以适应多样化的临床环境和采集协议，限制了其泛化能力。

Method: 利用Transformer架构和生成式掩码预训练策略，整合多模态数据（包括MIMIC-III-WDB、MIMIC-IV-ECG和CODE等数据集），预训练模型。

Result: CSFM在诊断任务、人口统计信息识别、生命体征测量、临床结果预测和ECG问答等任务中表现优于传统单模态单任务方法，且在不同ECG导联配置和传感器模态下均表现稳健。

Conclusion: CSFM作为一种多功能、可扩展的解决方案，为全面心脏监测提供了潜力。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [48] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: 论文提出了一种变分数字孪生（VDT）框架，通过贝叶斯输出层和高效更新算法，解决了现有数字孪生在实时性、模型不确定性和信息交换方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生技术缺乏实时实现的关键特性、信息交换框架或对模型不确定性的关注，VDT旨在填补这些空白。

Method: VDT框架在标准神经网络架构中增加了一个贝叶斯输出层，并采用新型更新算法，使其能在普通GPU上快速更新并提供校准的不确定性边界。

Result: 在四个能源领域问题中验证了VDT的有效性，包括临界热通量预测、可再生能源发电预测、核反应堆瞬态冷却和锂离子电池建模，均表现出色。

Conclusion: VDT通过轻量级贝叶斯增强和高效更新方案，将传统代理模型转化为具有不确定性感知、数据高效和计算可行的数字孪生，为能源系统提供了可靠模型。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [49] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: Petrobras发布的3W数据集是用于石油井不良事件早期检测的多变量时间序列数据集，经过专家标注，支持AI和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 石油井不良事件可能导致经济损失、环境事故和人员伤亡，需要早期检测技术。缺乏公开数据集促使Petrobras开发并公开3W数据集。

Method: 3W数据集是多变量时间序列数据，由专家标注，并通过协作开发不断改进。

Result: 3W数据集已成为该领域的基础参考，支持新方法和数字产品的开发。

Conclusion: 3W数据集的详细描述鼓励社区改进现有成果并开发新方法，以提前检测不良事件。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [50] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 提出一种两阶段训练框架，用于高效去毒化社交媒体内容，同时保持语义和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中广泛传播的有毒内容对环境和公共讨论构成威胁，现有方法在去毒化性能、语义保留和数据效率方面存在不足。

Method: 两阶段训练框架：先用高质量并行数据监督微调，再用无标签数据和奖励模型通过Group Relative Policy Optimization训练LLM。

Result: 实验表明，该方法有效平衡了去毒化、语义保留和泛化能力，减少了对标注数据的依赖。

Conclusion: 该方法在去毒化任务中实现了最先进的性能，具有更高的数据效率和泛化能力。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [51] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 提出了一种新型能量函数，用于长序列记忆，基于密集Hopfield网络框架，通过高阶交互实现指数存储容量。


<details>
  <summary>Details</summary>
Motivation: 解决长序列任务中Transformer模型的局限性，如长上下文建模和时间序列数据的长期依赖处理。

Method: 引入时间核$K(m, k)$以捕捉时间依赖性，实现高效序列模式检索。

Result: 成功应用于电影帧的存储和序列检索，展示了在高维空间中的有效性。

Conclusion: 该模型为长上下文任务提供了新思路，对自然语言处理和时间序列预测有潜在影响。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [52] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出了一种基于元素组成和XRD的多模态框架，无需晶体结构输入，通过自监督预训练策略提升材料发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于晶体结构的模型在实际应用中因结构数据难以获取而不实用的问题。

Method: 采用多模态框架，结合元素组成和XRD数据，使用掩码XRD建模和对比对齐作为自监督预训练策略。

Result: 预训练显著加速收敛（最高4.2倍），提升准确性和表示质量，多模态性能随数据规模扩展更优。

Conclusion: 为材料科学提供了一种无需结构输入、基于实验数据的通用模型路径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [53] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: 研究探讨洪水对路面粗糙度的影响，利用20年数据和XAI技术分析洪水对路面性能的影响，发现洪水加速路面粗糙度增加，建议采取防洪措施。


<details>
  <summary>Details</summary>
Motivation: 洪水对路面基础设施造成严重损害，研究旨在量化洪水对路面粗糙度的影响。

Method: 整合20年路面数据和洪水事件数据，进行统计分析，并应用XAI技术（如SHAP和LIME）评估影响。

Result: 洪水路段的路面粗糙度增加速度显著快于非洪水路段。

Conclusion: 需采取防洪措施（如改进排水系统和使用抗洪材料）以提高路面在易受灾区域的韧性。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [54] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: 提出了一种基于深度卷积神经网络的智能优化系统，用于网格生成和优化，核心是Loop2Net生成器和损失函数。


<details>
  <summary>Details</summary>
Motivation: 通过深度学习技术实现高效的网格生成和优化，提升网格质量。

Method: 使用Loop2Net生成器和两个关键损失函数进行训练，并通过惩罚机制优化模型性能。

Result: 成功实现了基于给定翼坐标的网格预测和优化。

Conclusion: 该系统能够有效提升网格生成的质量和效率。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [55] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: 利用IMU数据和可解释AI方法，开发了一种早期检测和预测帕金森病冻结步态（FOG）的框架，集成模型表现优异，准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 解决帕金森病患者常见的冻结步态（FOG）早期检测和预测问题，提高诊断准确性和可解释性。

Method: 使用CatBoost、XGBoost和Extra Trees分类器，结合Stacking Ensemble模型和SHAP分析，并引入联邦学习框架。

Result: Stacking Ensemble模型表现最佳，分类准确率接近99%，时间因素对步态模式区分影响最大。

Conclusion: 提出的框架在FOG检测和预测中表现出色，结合联邦学习增强了模型的实用性和隐私保护。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [56] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: 论文提出了一种新的3D编码模块，通过旋转采样实现近似旋转不变性，并通过后对齐策略严格实现不变性，显著提升了分子属性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 传统图表示难以有效编码分子的3D空间结构，现有方法要么依赖先验知识，要么计算成本高，限制了模型的泛化性和鲁棒性。

Method: 提出了一种基于SO(3)旋转群期望计算的3D编码模块，结合后对齐策略实现严格旋转不变性。

Result: 在QM9和C10数据集上表现出更高的预测准确性、鲁棒性和泛化性能，同时保持低计算复杂度。

Conclusion: 该方法为药物发现和材料设计中的3D分子信息处理提供了高效且有效的解决方案。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [57] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: 提出了一种基于解码器的大型语言模型（LLM），用于检测电子控制单元（ECU）通信日志中的异常，解决了现有方法在专业领域中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在汽车通信系统等专业领域中效果有限，且缺乏针对ECU通信的LLM模型。

Method: 使用解码器LLM学习UDP通信日志，通过时间偏差检测异常，并引入熵正则化技术处理不一致的标注数据。

Result: 提出了一种新的解码器异常检测架构，能够处理不一致标注，并适应不同ECU通信场景，提高了检测准确性。

Conclusion: 该方法通过生成能力减少了手动标注的高成本和错误，为复杂通信环境提供了可扩展的解决方案。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [58] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: 研究比较了三种AI模型（NLP、LLM、JEPA）在急诊分诊中的表现，发现LLM模型（URGENTIAPARSE）准确性最高，优于护士分诊和其他AI模型。


<details>
  <summary>Details</summary>
Motivation: 急诊分诊中的错误（如过度或不足分诊）是持续挑战，AI的整合可能提升分诊准确性和效率。

Method: 回顾性分析7个月的患者数据，训练并验证三种AI模型，评估其与FRENCH标准的吻合度。

Result: LLM模型表现最佳，尤其在预测住院需求和结构化数据处理上。

Conclusion: AI（尤其是LLM）可提升急诊分诊的准确性和效率，但需解决模型局限性和伦理问题。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [59] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: 本文详细解释了Ziyin等人（2025）关于嵌入式深度线性网络模型（EDLN）的“完美”柏拉图表示假设（PRH）的证明，并展示了SGD训练下EDLN的柏拉图特性及其与渐进锐化的共同原因。


<details>
  <summary>Details</summary>
Motivation: 探讨SGD训练下EDLN模型的柏拉图表示现象及其与渐进锐化的联系，揭示深度学习中不可逆性导致的“熵力”对表示学习的重要性。

Method: 通过理论分析和证明，展示SGD训练下EDLN模型的柏拉图表示特性，并探讨其与渐进锐化的共同原因。

Result: 发现SGD训练下EDLN模型会学习到完美的柏拉图表示，且这一现象与渐进锐化有共同原因。

Conclusion: 研究强调了理解SGD训练中不可逆性导致的“熵力”对表示学习的关键作用，为深度学习的理论提供了新视角。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [60] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: 论文提出了一种基于动态模式分解（DMD）和深度学习的神经算子，用于高效建模时空过程，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 科学计算与人工智能结合是研究热点，但需平衡轻量化和准确性。传统PDE求解方法计算资源消耗大，需更高效的方法。

Method: 结合DMD和深度学习，自动提取关键模式和系统动态，用于预测。

Result: 在热方程、拉普拉斯方程和Burgers方程近似解中，与DeepONet和FNO相比，表现出高重构精度和低计算成本。

Conclusion: 该方法为高效建模时空过程提供了新思路，显著优于传统数值方法。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [61] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 本文研究了差分隐私训练中自适应优化器的性能问题，发现常见的无偏二阶矩估计方法效果不佳，而简单的“scale-then-privatize”方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 差分隐私训练中球形噪声对自适应优化器（如AdaGrad和Adam）的性能有负面影响，现有研究结论可能无法推广到实际模型训练中。

Method: 调查了多种改进算法，通过理论分析和实证研究比较其表现。

Result: 发现“scale-then-privatize”方法在理论和实践中均优于其他方法，尤其在小规模语言模型训练任务中。

Conclusion: “scale-then-privatize”方法更匹配实际需求，噪声添加方式更优。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [62] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种近似等变的张量分解网络（TDNs），通过低秩张量分解（如CP分解）替代计算昂贵的Clebsch-Gordan（CG）张量积，显著加速计算。


<details>
  <summary>Details</summary>
Motivation: 现有的SO(3)-等变网络在机器学习原子间势能（MLIPs）中占主导地位，但其关键操作CG张量积计算成本高，亟需优化。

Method: 采用CP分解近似CG张量积，提出路径权重共享以减少参数数量，同时保持等变性，计算复杂度从O(L^6)降至O(L^4)。

Result: 在PubChemQCR、OC20和OC22等数据集上，TDNs表现出与现有方法竞争的性能，同时显著提升计算速度。

Conclusion: TDNs作为一种即插即用的替代方案，有效平衡了计算效率和模型性能，适用于大规模分子模拟任务。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [63] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: 论文提出了一种名为Spectral Manifold Harmonization（SMH）的新方法，用于解决图结构数据中的不平衡回归问题，通过生成合成样本以关注目标分布中常被忽视的区域。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在科学领域中普遍存在，但现有研究在目标值范围不平衡的情况下缺乏关注，尤其是对科学价值高的区域。

Method: SMH方法通过生成合成图样本，保留拓扑特性，同时聚焦于目标分布中常被忽视的区域。

Result: 实验结果表明，SMH在化学和药物发现基准数据集上显著提升了目标域范围的预测性能。

Conclusion: SMH为解决图结构数据中的不平衡回归问题提供了一种有效方法，尤其适用于科学价值高的目标范围。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [64] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP是一种创新的缓存友好型DP-SGD方法，通过单次融合计算梯度，显著减少内存需求和冗余计算，同时保持隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，训练数据的隐私保护成为关键问题。现有DP-SGD方法在内存和计算效率上存在不足。

Method: FlashDP通过单次任务融合计算梯度，减少内存移动和冗余计算，优化了DP-SGD的实现。

Result: FlashDP在Llama-13B模型预训练中，内存需求降低50%，冗余计算减少20%，吞吐量达到非DP方法的90%。

Conclusion: FlashDP为高效且隐私保护的LLMs训练提供了重要进展，其代码已开源。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [65] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: 介绍了一种名为Diffusion Explorer的交互式工具，用于直观展示扩散模型的几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有解释扩散模型的资源要么需要高深理论基础，要么过于关注神经网络架构，忽略了其几何特性。

Method: 开发了Diffusion Explorer工具，用户可在浏览器中训练2D扩散模型并观察采样过程的动态变化。

Result: 通过交互式动画直观展示了扩散模型的几何特性和时间动态。

Conclusion: Diffusion Explorer为理解和教学扩散模型提供了直观且易用的工具。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [66] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文评估了大型脑波基础模型（LBMs）在脑机接口（BCI）任务中的表现，发现其性能提升有限且参数需求高，提出了通过LoRA等技术优化参数效率的方法。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在脑波建模中的潜力，评估其在BCI任务中的适用性和效率。

Method: 通过系统微调实验和LoRA技术，对比LBMs与传统深度学习架构的性能和参数效率。

Result: LBMs性能提升有限（0.9%-1.2%），参数需求高，但LoRA能显著减少参数且不降低性能。

Conclusion: 当前LBMs在脑波分析中效率不足，需领域特定的优化策略和架构改进。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [67] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为JAM的框架，通过多目标优化任务实现视觉和语言模型的表示对齐，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉和语言模型在独立训练后是否能通过优化实现表示对齐，验证柏拉图表示假设。

Method: 引入JAM框架，联合训练模态特定的自编码器，通过重建和跨模态目标促进对齐。

Result: JAM框架在多种设计轴（如对齐目标、层深度和模型规模）下均能有效诱导对齐。

Conclusion: JAM框架为将通用单模态模型转化为专业多模态模型提供了理论和实践路径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [68] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: 论文提出了一种基于蒸馏和剪枝的快速神经网络推理技术，用于在低成本平台上实时部署入侵检测系统（IDS），显著降低了硬件需求并保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 随着现代车辆的互联性增强，车载以太网成为关键技术，但其易受攻击，如流量注入攻击。现有的深度学习IDS需要昂贵硬件支持实时运行，限制了其广泛应用。

Method: 采用蒸馏和剪枝技术优化IDS模型，使其能在低成本平台（如树莓派4）上实时运行。

Result: 优化后的IDS在树莓派4上实现了727微秒的检测时间，AUCROC值达到0.9890。

Conclusion: 研究表明，蒸馏和剪枝技术能有效降低IDS的硬件需求，同时保持高性能，为低成本实时入侵检测提供了可行方案。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [69] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM是一种隐私保护且高效的移动设备端大语言模型微调方法，通过服务器辅助的侧调优和激活缓存技术，解决了通信负担和数据泄露问题。


<details>
  <summary>Details</summary>
Motivation: 移动设备资源有限，现有服务器辅助方法存在通信负担重和数据泄露风险，需要一种更高效且隐私保护的微调方法。

Method: PAE MobiLLM结合服务器端激活缓存、单令牌激活快捷方式和加法适配器侧网络设计，减少通信成本并保护隐私。

Result: PAE MobiLLM显著降低了通信负担，加速了微调收敛，同时保护了数据和模型隐私。

Conclusion: PAE MobiLLM为移动设备上的大语言模型微调提供了一种高效且隐私保护的解决方案。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [70] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习在智能交通系统中建模皮肤电导反应（SCR）事件的应用，比较了量子支持向量机（QSVM）和量子神经网络（QNN）的性能。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算在复杂机器学习任务中的潜力，特别是在智能交通系统中分析行人压力相关的SCR事件。

Method: 开发了基于Pennylane的QSVM（使用八量子位ZZ特征图）和QNN（使用树张量网络结构和八量子位ZZ特征图），并对SCR数据进行了分类。

Result: QSVM训练准确率高但测试准确率低（45%），存在过拟合问题；QNN测试准确率更高（55%），优于QSVM和经典方法。

Conclusion: QNN在分类任务中表现更优，展示了量子机器学习在复杂数据分析中的潜力。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [71] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种针对大规模语言模型（LLMs）训练的随机共轭次梯度方法，结合自适应采样，相比传统SGD方法具有更快的收敛速度和更好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度下降（SGD）在大规模应用中存在性能限制，需要更高效的优化方法。

Method: 提出随机共轭次梯度方法，结合自适应采样和AdamW-like算法调整步长，解决LLMs训练中的非凸性和非光滑性问题。

Result: 实验结果表明，该方法在速度和准确性上均优于传统SGD，且扩展性更强。

Conclusion: 该方法为LLMs训练提供了一种更高效且可扩展的优化方案。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [72] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 该论文提出了PULSE协议，用于评估大型多模态模型（LMMs）的遗忘技术，重点关注预训练知识遗忘和长期可持续性评估。


<details>
  <summary>Details</summary>
Motivation: 解决现有遗忘基准仅关注单次遗忘操作的问题，为LMMs提供更现实的遗忘场景评估框架。

Method: 引入PULSE协议，从预训练知识遗忘和长期可持续性评估两个维度分析现有遗忘方法。

Result: 现有技术能有效遗忘微调知识，但难以消除预训练信息；单次批量遗忘方法在多次操作中性能显著下降。

Conclusion: PULSE协议揭示了现有遗忘技术的局限性，为未来研究提供了方向。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [73] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的神经哈密顿算子（NHO）方法，用于解决高维随机控制问题，避免了维度灾难。


<details>
  <summary>Details</summary>
Motivation: 高维随机控制问题因维度灾难难以解决，传统动态规划方法效率低，需要替代方法。

Method: 通过定义神经哈密顿算子（NHO），将问题转化为前向-后向随机微分方程（FBSDEs），并用神经网络参数化控制与值函数的梯度。

Result: 证明了NHO在一般鞅驱动下的通用逼近能力，并分析了优化挑战。

Conclusion: NHO为高维随机控制问题提供了新的深度学习框架，具有理论支持和实际应用潜力。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [74] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出了一种针对大语言模型（LLMs）中上下文学习（ICL）后门攻击的防御机制ICLShield，通过动态调整概念偏好比例，有效减少攻击影响。


<details>
  <summary>Details</summary>
Motivation: ICL因其适应性和无需参数的特性在LLMs中表现优异，但也易受后门攻击。本文旨在解决这一安全问题。

Method: 提出双学习假设，分析ICL后门效应的上界，并设计ICLShield机制，利用置信度和相似度分数动态调整概念偏好比例。

Result: 实验表明ICLShield在多种LLMs和任务中表现优异，平均防御效果提升26.02%，且对闭源模型（如GPT-4）也有效。

Conclusion: ICLShield通过动态调整概念偏好比例，显著提升了ICL的安全性，为LLMs的防御提供了新思路。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [75] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了一种基于自适应困惑度感知强化学习（APARL）的异常事件检测框架，显著提升了模型的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决客户服务对话中异常事件检测的复杂性和动态性，以及模型在跨业务场景中的泛化能力。

Method: 采用双环动态课程学习架构，结合大语言模型的高级推理能力，逐步聚焦更具挑战性的样本。

Result: 在食品配送对话任务中，F1分数平均提升17.19%，OOD迁移测试平均提升9.59%。

Conclusion: APARL为工业部署提供了优越的异常检测解决方案，提升了运营效率和商业效益。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [76] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: 提出了一种基于小波域的生成模型（WDM），用于将降水数据从10 km分辨率降尺度到1 km，同时显著提升推理速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有全球降水数据（如IMERG）分辨率较低（10 km），无法满足水文建模和极端天气分析的需求。

Method: WDM是一种条件扩散模型，直接在小波域中学习降水数据的复杂结构，专注于高频小波系数以生成高分辨率降水场。

Result: WDM实现了10倍空间超分辨率（1 km），推理速度提升9倍，且生成结果更真实、细节更丰富，减少了伪影。

Conclusion: WDM为地球科学超分辨率问题提供了准确且高效的解决方案，有助于提升水文预报的可靠性。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [77] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出DSAC-D算法，通过扩散模型解决强化学习中值函数估计偏差问题，实现多模态策略表示，并在控制任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法使用单模态分布（如高斯分布）建模值分布，容易导致估计偏差和算法性能下降。

Method: 引入策略熵和值分布函数构建多模态分布策略迭代框架，利用扩散模型反向采样构建扩散值网络，最终提出DSAC-D算法。

Result: 在9个控制任务中实现SOTA性能，估计偏差显著抑制，总平均回报提升超10%。实车测试验证了多模态驾驶风格表征能力。

Conclusion: DSAC-D算法有效解决了值函数估计偏差问题，实现了多模态策略表示，并在实际应用中表现出色。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [78] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: 提出了一种增强的代理模型，通过引入松弛变量将两步过程统一为一步，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 受现有量子退火优化代理模型的启发，旨在通过松弛变量改进模型性能。

Method: 在因子分解机及其Ising表示中引入松弛变量，迭代更新以捕捉高阶特征交互。

Result: 实验表明，松弛变量的引入显著提升了药物组合效应预测的性能。

Conclusion: 该方法为利用量子优势构建高效代理模型提供了有前景的途径。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [79] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: 论文介绍了一种结合线性回归式连续上下文学习（ICL）和离散关联召回的新玩具问题，研究了Transformer模型在此任务中的表现，发现其需要两种机制：基于标签的关联召回和基于上下文的贝叶斯预测。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在处理结合连续和离散特征的玩具问题时的能力，特别是其在上下文学习和预测任务中的表现。

Method: 预训练Transformer模型于符号标记的随机线性确定性动态系统样本，分析其在关联召回和状态预测任务中的表现，并通过权重修剪进行机制分析。

Result: 模型需要两种机制：基于标签的关联召回和基于上下文的贝叶斯预测，且这两种机制的学习动态不同。

Conclusion: 多机制现象不仅限于玩具问题，在真实任务（如ICL翻译）中也观察到类似现象。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [80] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: 论文提出了一种名为BAR的新型RSS数据集，用于解决文化遗址中室内定位系统（IPS）开发的数据不足问题，并提供了基于邻近性和k-NN算法的分类基线。


<details>
  <summary>Details</summary>
Motivation: 文化遗址中的IPS可以提升访客体验，但缺乏针对博物馆环境的公开RSS数据集阻碍了相关算法的开发与评估。

Method: 收集了90件艺术品前的RSS数据（使用Android和iOS平台），并采用邻近性方法和k-NN算法建立分类基线。

Result: 提供了BAR数据集，并展示了基于邻近性和k-NN算法的分类结果。

Conclusion: BAR数据集填补了博物馆环境RSS数据的空白，为未来研究提供了基础，并提出了进一步研究方向。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [81] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 论文挑战了奖励频率作为任务难度衡量标准的假设，揭示了当前强化学习方法在关键子目标无直接奖励时的局限性。


<details>
  <summary>Details</summary>
Motivation: 重新审视奖励频率与任务难度的关系，解决关键子目标无直接奖励时的学习问题。

Method: 形式化零激励动态问题，分析现有子目标算法的表现。

Result: 发现现有算法无法有效利用零激励动态，学习性能受子目标与奖励时间接近度影响。

Conclusion: 指出当前方法需改进，需开发能推断潜在任务结构的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [82] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文系统分析了扩散模型中不同目标函数及其对应的损失函数，统一了它们在变分下界目标框架下的关系，并通过实证研究揭示了这些目标在性能上的差异及其原因。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为强大的生成模型，其损失函数的选择是关键问题。已有多种损失函数被提出，但缺乏统一的视角和实证比较。

Method: 通过理论分析将不同损失函数统一到变分下界目标框架下，并辅以实证研究，比较不同目标在生成质量和似然估计上的表现。

Result: 研究发现不同损失函数在特定条件下性能存在差异，并揭示了影响这些差异的因素。

Conclusion: 本研究为扩散模型的损失函数提供了统一理解，有助于未来更高效和针对性的模型设计。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [83] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax是一个基于JAX的电动车充电站仿真环境，显著提升了强化学习训练效率，性能提升100x-1000x。


<details>
  <summary>Details</summary>
Motivation: 电网系统拥堵问题急需提升运行效率，传统强化学习方法因高样本复杂度和仿真成本效率低下。

Method: 开发了Chargax，一个基于JAX的电动车充电站仿真环境，支持模块化配置。

Result: 在真实数据场景中验证，性能提升100x-1000x，并能灵活模拟多种充电站配置。

Conclusion: Chargax为可持续能源挑战提供了高效的强化学习训练解决方案。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [84] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS是一种无需训练的方法，通过将潜在嵌入空间转换为视觉表示，利用视觉语言模型的空间和细粒度推理能力，实现多模态数据的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 解决小规模专用模型缺乏灵活性和基础模型在非传统模态和长尾领域表现不佳的问题。

Method: 将潜在嵌入空间转换为视觉表示，利用视觉语言模型的空间和细粒度推理能力进行预测。

Result: 在视觉、音频、生物和表格领域表现优异，平均性能超过Gemini 16%，接近专用方法。

Conclusion: MARVIS提供了一种无需训练、保护隐私的多模态预测方法，性能接近专用模型。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [85] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO是一种自引导过程奖励优化框架，通过内在奖励和掩码步骤优势估计提升LLM的推理能力，显著提高训练效率和测试准确性。


<details>
  <summary>Details</summary>
Motivation: 解决过程强化学习中计算开销大和缺乏统一理论框架的问题。

Method: 提出SPRO框架，包括内在过程奖励和掩码步骤优势估计（MSA）。

Result: SPRO训练效率提高3.4倍，测试准确率提升17.5%，且无额外计算开销。

Conclusion: SPRO在提升LLM推理能力的同时，保持了高效和稳定，适用于工业实现。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [86] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 研究探讨了神经网络最后一层权重重采样（“zapping”）在持续学习和少样本迁移学习中的作用，发现其能加速模型在新领域的恢复，并揭示了优化器选择对学习动态的复杂影响。


<details>
  <summary>Details</summary>
Motivation: 理解zapping在持续学习和少样本迁移学习中的机制及其对模型恢复速度的影响。

Method: 在卷积神经网络中，通过手写字符和自然图像的实验，分析zapping和优化器选择对学习动态的影响。

Result: zapping能加速模型在新领域的恢复，优化器选择会影响任务间的协同/干扰模式。

Conclusion: zapping和优化器选择对持续学习和迁移学习的动态有显著影响，需进一步研究其机制。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [87] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习（FL）的动态室内定位方法，解决了传统集中式方法的隐私、带宽和服务器可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 传统室内定位技术误差大且存在隐私问题，集中式机器学习方法虽有效但仍有隐私和带宽等挑战。

Method: 采用联邦学习结合深度神经网络（DNN）模型进行动态室内定位。

Result: 实验表明FL性能接近集中式模型（CL），同时保护数据隐私、提高带宽效率和服务器可靠性。

Conclusion: FL为隐私增强的室内定位提供了可行方案，推动了安全高效定位系统的发展。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [88] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 论文分析了Muon优化器的理论性能，证明了四种变体的收敛性，并探讨了权重衰减对参数和梯度范数的影响，同时推导了最小化计算成本的临界批量大小。


<details>
  <summary>Details</summary>
Motivation: 研究Muon优化器的理论性能及其在实际应用中的表现，特别是权重衰减和学习率的关系。

Method: 通过理论分析，证明了四种Muon变体的收敛性，并推导了临界批量大小。实验验证了理论结果。

Result: 权重衰减能更严格地限制参数和梯度范数，临界批量大小可最小化计算成本。

Conclusion: Muon优化器在理论和实验中均表现出色，权重衰减和临界批量大小的分析为其实际应用提供了指导。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [89] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: 提出了一种高效的在线核稀疏表示字典学习算法，基于RLS方法递归更新字典，性能优于现有在线方法，接近批量训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决在线核稀疏表示字典学习的高效性问题，提升分类准确率并降低计算复杂度。

Method: 采用递归最小二乘法（RLS）递归更新字典，支持单样本或小批量处理。

Result: 在四个数据集上表现优于现有在线方法，分类准确率接近批量训练模型，且计算效率更高。

Conclusion: 该方法高效且性能优越，适用于在线核稀疏表示学习。

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [90] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: 本文提出了一种基于ConvLSTM的新方法DDCL，用于自动生成《Dance Dance Revolution》游戏中的舞蹈图表，改进了之前的DDC方法，显著提高了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 改进现有的Dance Dance Convolution (DDC)方法，通过更先进的模型提高舞蹈图表生成的准确性和质量。

Method: 使用ConvLSTM架构替代DDC中的CNN-LSTM，优化模型以更好地捕捉音乐节奏与舞蹈动作的关系。

Result: DDCL方法显著提高了舞蹈图表生成的准确性，优于之前的DDC方法。

Conclusion: ConvLSTM架构在自动生成舞蹈图表任务中表现更优，为游戏内容生成提供了更高效的工具。

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [91] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 论文提出了一种名为GradMetaNet的新架构，专门设计用于处理神经网络梯度，基于三个原则：等变性设计、多数据点梯度集处理和高效梯度表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理梯度时未针对梯度处理设计专门架构，限制了其适用性。本文旨在填补这一空白。

Method: 提出GradMetaNet架构，基于等变性设计、多数据点梯度集处理和秩1分解的高效表示。

Result: GradMetaNet在多种梯度任务（如优化、编辑和曲率估计）中表现优于现有方法。

Conclusion: GradMetaNet为梯度处理提供了高效且通用的解决方案，适用于多种任务。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [92] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow是一个异步流式强化学习框架，用于高效后训练，解决了传统RL框架的可扩展性和资源利用问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL框架在扩展性和资源利用上存在瓶颈，且与LLM训练或推理引擎紧密耦合，难以支持自定义引擎。

Method: 提出分布式数据存储和传输模块，支持流式数据管理和细粒度调度；采用生产者-消费者异步工作流，减少计算闲置。

Result: 实验显示吞吐量平均提升1.59倍。

Conclusion: AsyncFlow为下一代RL训练系统设计提供了模块化和可定制的解决方案。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [93] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 提出了一种名为Prefix-RFT的混合方法，结合了监督微调（SFT）和强化微调（RFT）的优势，在数学推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有SFT和RFT方法各有优缺点，SFT易导致行为克隆问题，RFT则对初始策略敏感且可能学习到意外行为。

Method: 提出Prefix-RFT，结合演示和探索学习，通过数学推理任务验证其有效性。

Result: Prefix-RFT性能优于单独SFT和RFT，且优于并行混合策略RFT方法，对演示数据质量与数量变化具有鲁棒性。

Conclusion: Prefix-RFT为LLM后训练提供了新视角，未来研究可探索统一演示与探索的范式。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [94] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 论文提出SODA算法，用于从LLM输出中精确重建输入，显著优于现有方法，但长输入序列的隐私信息提取仍有困难。


<details>
  <summary>Details</summary>
Motivation: 现有审计技术关注识别LLM的不良行为，而本文解决的是从已有输出中重建输入的取证问题，以支持事后分析和检测虚假报告。

Method: 将输入重建问题形式化为离散优化问题，提出SODA算法，基于连续松弛的输入搜索空间，结合周期性重启和参数衰减。

Result: 在33M到3B参数的LLM上实验表明，SODA能完全恢复79.5%的短分布外输入，但对长序列（15+ token）的隐私信息提取效果不佳。

Conclusion: 标准部署实践可能已足够防范恶意使用SODA方法，但长序列的隐私保护仍需改进。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [95] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: PERTINENCE是一种动态选择预训练模型的方法，通过分析输入特征复杂度，平衡准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型DNN模型虽然准确但资源消耗高，需要在不显著降低准确性的情况下减少对它们的依赖。

Method: 使用遗传算法训练输入调度器，动态选择最适合的模型处理输入。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet数据集上，PERTINENCE在减少36%操作的同时保持或提高准确性。

Conclusion: PERTINENCE通过动态模型选择，有效平衡了准确性和计算效率。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [96] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了一种变分神经网络版本的图卷积网络，用于估计模型不确定性，提升模型可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 通过估计模型不确定性，提升图卷积网络的可解释性和准确性，并支持关键应用中的结果验证。

Method: 提出空间和时空图卷积网络的变分神经网络版本，估计模型输出和层注意力的不确定性。

Result: 在社交交易分析和基于骨架的人体动作识别任务中，模型准确性和不确定性估计均有所提升。

Conclusion: 变分神经网络版本的图卷积网络能有效提升模型性能和可解释性。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [97] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: 提出了一种名为RelFCI的因果发现算法，用于处理具有潜在混杂因素的关系数据，填补了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法在处理关系数据和潜在混杂因素时存在局限性，无法满足实际需求。

Method: 基于Fast Causal Inference (FCI)和Relational Causal Discovery (RCD)算法，定义了新的图模型，支持关系领域中的因果发现。

Result: 实验证明RelFCI能有效识别具有潜在混杂因素的关系因果模型中的正确因果结构。

Conclusion: RelFCI是一种可靠且完整的算法，适用于关系数据中的因果发现，解决了现有方法的不足。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [98] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 论文提出用贝叶斯PINN替代集成方法，通过评估后验方差提升信息传播效果，性能优于集成方法。


<details>
  <summary>Details</summary>
Motivation: 解决PINN在前向问题中信息传播不足的收敛问题。

Method: 用贝叶斯PINN替代集成方法，评估后验方差确保信息传播。

Result: 在基准问题上表现优于集成方法，与Adam和LBFGS结合的PINN集成竞争。

Conclusion: 贝叶斯PINN是一种数学上更严谨的替代方案，能有效提升PINN性能。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [99] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: 论文比较了学习率控制的不同方法，发现现有方法在特定任务上表现良好但缺乏普适性，需关注算法选择和新兴方向如元学习。


<details>
  <summary>Details</summary>
Motivation: 探讨学习率控制在深度学习和AutoML中的重要性及现状。

Method: 比较多保真超参数优化、固定超参数调度和无超参数学习等方法。

Result: 现有方法在复杂模型和任务中效果下降，需算法选择和新兴方法。

Conclusion: 未来应关注更相关的测试任务和元学习等方向，以提升学习率控制的效果。

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [100] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: 提出一种基于神经后验估计的模拟推理方法，用于高效估计1型糖尿病的生理模型参数，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维参数空间中效率低下且计算成本高，需要更高效的方法来估计复杂的葡萄糖-胰岛素相互作用。

Method: 采用基于神经后验估计的模拟推理方法，捕捉饮食、胰岛素和血糖水平之间的复杂关系。

Result: 实验表明，该方法在参数估计上优于传统方法，且能更好地泛化到未见条件，提供实时后验推断和可靠的不确定性量化。

Conclusion: 该方法为1型糖尿病的数字孪生提供了更高效、可靠的参数估计解决方案。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [101] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER是一种基于黑盒优化的进化方法，用于LLM后训练，通过隐式压缩训练数据引入信息瓶颈，提供理论保证，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决梯度优化在隐私、安全和过拟合方面的局限性，同时克服黑盒方法在高维参数空间和计算成本上的挑战。

Method: 提出BBoxER方法，利用信息流可追踪性，通过隐式压缩训练数据实现信息瓶颈，提供理论保证。

Result: 实验证明BBoxER在LLM上表现优异，提升性能并泛化良好。

Conclusion: BBoxER是梯度优化的轻量级模块化补充，适用于隐私敏感环境。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [102] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 论文提出了两种新指标（Clipped Density和Clipped Coverage），用于更可靠、可解释地评估生成模型的样本质量，解决了现有指标在鲁棒性和校准方面的不足。


<details>
  <summary>Details</summary>
Motivation: 生成模型在关键应用中的使用受到样本质量评估不可靠的限制，现有指标缺乏校准或对异常值的鲁棒性。

Method: 通过剪裁单个样本贡献和最近邻球的半径，提出Clipped Density和Clipped Coverage指标，防止分布外样本影响聚合值。

Result: 实验表明，新指标在鲁棒性、敏感性和可解释性上优于现有方法，且分数随劣质样本比例线性下降。

Conclusion: Clipped Density和Clipped Coverage为生成模型的质量评估提供了更可靠和可解释的工具。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [103] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet将决策树集成转换为稀疏、部分连接的神经网络，保留符号结构并支持梯度优化，性能优于XGBoost。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的梯度优化能力与决策树的符号可解释性，构建紧凑且无需手动调参的模型。

Method: 将决策树的每条分支映射为隐藏神经元，形成稀疏神经网络，保留符号结构。

Result: 在多类分类任务中，BranchNet在准确性上显著优于XGBoost。

Conclusion: BranchNet在符号可解释性和性能上表现优异，但在二元任务中可能需要进一步校准。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [104] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 提出一种去中心化、可持续的基础模型训练方法，利用边缘AI设备的闲置算力，以减少环境影响和集中控制风险。


<details>
  <summary>Details</summary>
Motivation: 基础模型的计算需求高，导致环境问题和集中控制风险，需要更可持续和去中心化的解决方案。

Method: 利用连接边缘AI设备的闲置算力进行分布式训练。

Result: 提出了一种可持续且去中心化的训练愿景，并分析了其优势。

Conclusion: 需解决一系列挑战以实现这一愿景，但其可持续性和去中心化潜力值得探索。

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [105] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 提出了一种基于CPU的低秩适配器（LoRA）微调方法，适用于计算资源有限的用户，通过组合预训练适配器生成新适配器，性能虽不及GPU训练但优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA微调对GPU的依赖问题，为计算资源有限的用户提供可行的替代方案。

Method: 利用预训练适配器库，学习一个元操作符，将输入数据集映射到LoRA权重，直接在CPU上组合生成适配器。

Result: 生成的适配器性能优于基础Mistral模型，但不及GPU训练的适配器。

Conclusion: 该方法为资源受限用户提供了一种实用的LoRA微调替代方案。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [106] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: 提出了一种基于模型强化学习的知识转移新方法，将高容量多任务代理（317M参数）高效蒸馏为紧凑模型（1M参数），在MT30基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境中部署大型世界模型的关键挑战。

Method: 通过蒸馏技术将高容量多任务代理压缩为紧凑模型，并进一步通过FP16后训练量化优化模型大小。

Result: 蒸馏模型在MT30基准上达到28.45的归一化分数，显著优于原始1M参数模型的18.93。模型大小减少约50%。

Conclusion: 该方法解决了实际部署限制，为资源受限应用中的多任务强化学习系统提供了更高效的解决方案。

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [107] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: 提出一种新方法，利用图神经网络（GNN）解决SAT问题，通过将k-CNF公式映射为MILP问题，再编码为加权二分图输入GNN。理论证明方法的稳定性和局限性，实验结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在解决SAT问题中的应用，通过结合MILP技术，提升GNN在逻辑问题中的表现。

Method: 将k-CNF公式转化为MILP问题，编码为加权二分图，输入GNN进行训练和测试。理论分析了方法的稳定性和局限性。

Result: 理论证明方法具有稳定性和近似能力，实验显示其在实际问题中表现良好。

Conclusion: 该方法为GNN在SAT问题中的应用提供了新思路，具有理论和实践价值。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [108] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: mGRADE是一种混合内存系统，结合了1D卷积和最小门控循环单元，适用于内存受限的边缘设备多尺度时间处理。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在内存限制下同时捕捉短时和长时动态的需求，避免Transformer的高内存消耗和RNN、TCN的局限性。

Method: 提出mGRADE，结合可学习间隔的1D卷积和minGRU，卷积层捕捉快速变化，循环模块维护全局上下文。

Result: 在合成任务和像素级图像分类基准测试中，mGRADE表现优于纯卷积和纯循环模型，内存占用减少20%。

Conclusion: mGRADE是内存受限边缘设备多尺度时间处理的高效解决方案。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [109] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文重新审视了流行的OOD检测方法，指出其根本问题在于目标错位，无法有效识别OOD样本，并提出这些方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预测不确定性和特征的OOD检测方法存在目标错位问题，无法有效识别OOD样本，需重新审视其有效性。

Method: 通过分析不确定性方法和特征方法的局限性，探讨了密度估计和生成模型的OOD检测能力。

Result: 发现现有方法在OOD检测中存在不可减少的错误，且常见干预措施无法解决目标错位问题。

Conclusion: 现有OOD检测方法存在根本性缺陷，需重新设计目标和方法以有效识别OOD样本。

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [110] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: SubLoRA提出了一种基于子模函数最大化的LoRA秩确定方法，利用二阶信息（Hessian矩阵）改进传统线性化方法的不足，并通过贪心算法实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如AdaLoRA）依赖损失函数的一阶近似，在LoRA参数优化良好时表现不佳，需要更可靠且精细的二阶方法。

Method: 将秩确定问题转化为组合优化问题，引入子模函数最大化框架和贪心算法，并结合Hessian矩阵的闭式投影保证计算效率。

Result: 实验表明，SubLoRA在秩确定和联合训练性能上优于现有方法。

Conclusion: SubLoRA结合了理论严谨性、二阶准确性和计算效率，适用于物理信息神经网络（PINN）的微调任务。

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [111] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: FAE是一种基于变分自编码器和扩张卷积神经网络的时间序列异常检测基础模型，通过大规模预训练学习复杂时间模式，实现零样本异常检测。


<details>
  <summary>Details</summary>
Motivation: 受大型预训练基础模型成功的启发，研究一种通用时间序列建模方法，以解决异常检测问题。

Method: 结合变分自编码器（VAEs）和扩张卷积神经网络（DCNNs），构建通用时间序列模型，支持零样本异常检测。

Result: 在多维时间序列数据集（包括移动ISP数据和KDD 2021数据集）上展示了初步结果。

Conclusion: FAE作为一种基础模型，展现出在时间序列异常检测中的潜力，尤其是在零样本场景下。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [112] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: 论文提出了一种结合LSTM和Transformer的混合深度学习模型，利用iForest和AE进行伪标记，用于心理健康账单异常检测，解决了类别不平衡和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康账单的复杂性导致异常（如欺诈）频发，现有机器学习方法在类别不平衡、标签稀缺和复杂序列模式处理上表现不佳。

Method: 采用混合深度学习方法，结合LSTM和Transformer，利用iForest和AE生成伪标签，并在两个真实心理健康账单数据集上评估。

Result: iForest LSTM基线在声明级数据上召回率最高（0.963）；在操作级数据上，混合iForest模型召回率最高（0.744），但精度较低。

Conclusion: 研究表明，伪标记与混合深度学习结合在复杂、不平衡的异常检测场景中具有潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


### [113] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1是一个反射生成模型，通过自监督过程奖励模型（SPRM）实现高效推理，性能媲美OpenAI o3。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一接口整合策略模型和过程奖励模型，减少参数并提升推理效率。

Method: 使用共享主干网络和任务特定头，结合SPRM实现自监督训练，支持可控思考长度的推理模式。

Result: MetaStone-S1以32B参数规模达到与OpenAI-o3-mini系列相当的性能，并开源模型。

Conclusion: MetaStone-S1展示了高效推理的潜力，并通过开源推动研究社区发展。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [114] [Characterizing control between interacting subsystems with deep Jacobian estimation](https://arxiv.org/abs/2507.01946)
*Adam J. Eisen,Mitchell Ostrow,Sarthak Chandra,Leo Kozachkov,Earl K. Miller,Ila R. Fiete*

Main category: q-bio.QM

TL;DR: 论文提出了一种非线性控制理论框架JacobianODE，用于从数据中直接估计子系统交互的雅可比矩阵，并在高维混沌系统中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常为线性，无法充分描述非线性复杂系统的丰富上下文效应，因此需要一种数据驱动的非线性控制理论框架。

Method: 提出了JacobianODE，一种深度学习方法，利用雅可比矩阵的特性直接从时间序列数据中估计任意动态系统的雅可比矩阵。

Result: JacobianODE在高维混沌系统中表现优异，并在多区域递归神经网络（RNN）中展示了感官区域对认知区域的控制增强。

Conclusion: 该研究为生物子系统交互的理论和数据驱动理解奠定了基础。

Abstract: Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the "sensory" area gains greater control over the "cognitive" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [115] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Main category: astro-ph.IM

TL;DR: SpecCLIP是一个基于大语言模型（LLM）启发的恒星光谱分析框架，通过对比学习和辅助解码器实现跨光谱校准和灵活应用。


<details>
  <summary>Details</summary>
Motivation: 将LLM的成功方法扩展到恒星光谱分析，以学习鲁棒且信息丰富的嵌入，支持多样化的下游应用。

Method: 使用LAMOST低分辨率和Gaia XP光谱数据集进行预训练，结合CLIP框架进行对比对齐，并通过辅助解码器保留光谱特定信息。

Result: SpecCLIP在恒星参数估计和化学丰度测定等任务中表现出色，提高了准确性和精度，并具备异常检测潜力。

Conclusion: 对比训练的基础模型结合光谱感知解码器可以推动精密恒星光谱学的发展。

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [116] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Main category: eess.SP

TL;DR: 提出了一种基于迁移学习（TL）的可见光通信（VLC）室内定位方法，显著提高了定位精度、降低了能耗和计算时间。


<details>
  <summary>Details</summary>
Motivation: 工业环境中高精度室内定位需求迫切，但VLC技术受环境变化影响较大。

Method: 采用迁移学习框架结合深度神经网络（DNN），利用真实工厂数据优化定位性能。

Result: 定位精度提升47%，能耗降低32%，计算时间减少40%，且仅需30%数据集即可达到相似精度。

Conclusion: 该方法适应性强，成本效益高，适合工业4.0应用。

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [117] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Main category: eess.SP

TL;DR: UniToCom提出了一种统一的令牌通信范式，将令牌作为处理和无线传输的基本单位，通过生成信息瓶颈（GenIB）原则提高通信效率并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决多模态通信中令牌表示的高效性和可靠性问题，同时支持离散和连续令牌的统一处理。

Method: 提出GenIB原则优化令牌表示，开发σ-GenIB防止方差崩溃，并使用基于因果Transformer的多模态大语言模型（MLLM）处理令牌。

Result: 仿真结果表明UniToCom在动态信道条件下优于基线方法。

Conclusion: UniToCom通过结合令牌处理和MLLM，为下一代智能通信提供了可扩展和通用的解决方案。

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [118] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出了一种4D视频生成模型，通过跨视角点图对齐监督训练，实现多视角3D一致性，提升机器人动态环境预测能力。


<details>
  <summary>Details</summary>
Motivation: 增强机器人在复杂环境中的规划和交互能力，解决视频生成中时间连贯性和几何一致性的挑战。

Method: 利用RGB-D观测数据，通过跨视角点图对齐监督训练模型，学习共享3D场景表示，无需相机姿态输入。

Result: 在模拟和真实机器人数据集中生成更稳定、空间对齐的视频预测，支持机器人操作和新视角泛化。

Conclusion: 4D视频生成模型有效提升机器人动态环境预测和操作能力，具有实际应用潜力。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [119] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 该研究结合多源卫星影像和深度学习模型，提升滑坡识别和预测能力，为灾害风险管理提供支持。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，需要准确检测和预测。

Method: 利用Sentinel-2多光谱数据和ALOS PALSAR衍生的坡度及DEM层，结合深度学习模型（如U-Net、DeepLabV3+和Res-Net）进行滑坡检测。

Result: 研究为可靠的早期预警系统、灾害风险管理和可持续土地利用规划提供了支持。

Conclusion: 深度学习和多源遥感在构建稳健、可扩展和可迁移的滑坡预测模型中具有潜力。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [120] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 论文提出了一种通过将任务重新定义为镀铬球修复问题，从单张低动态范围（LDR）图像估计光照的简单有效技术。该方法利用预训练的扩散模型Stable Diffusion XL，克服了依赖有限HDR全景数据集的现有方法的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，泛化能力不足。本文旨在通过扩散模型解决这一问题。

Method: 提出DiffusionLight，通过迭代修复计算中位镀铬球作为稳定的低频光照先验，并引入Exposure LoRA生成HDR光探针。进一步提出DiffusionLight-Turbo，通过训练Turbo LoRA直接预测迭代过程结果，显著提升速度。

Result: 实验结果表明，该方法在多样化场景中生成逼真的光照估计，并在野外场景中表现出优越的泛化能力。

Conclusion: DiffusionLight及其加速版本DiffusionLight-Turbo在光照估计任务中表现出色，同时兼顾质量和效率。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [121] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种新型的少样本奖励建模方法——激活奖励模型（Activation RMs），通过激活导向构建对齐的奖励信号，无需额外微调，优于现有方法，并在新基准PreferenceHack上表现优异。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型和多模态模型的人类偏好是提升生成质量的关键，传统奖励建模难以适应新偏好。

Method: 利用激活导向技术构建奖励信号，仅需少量监督数据，无需额外模型微调。

Result: Activation RMs在标准奖励建模基准上优于现有方法，并在新基准PreferenceHack上超越GPT-4o。

Conclusion: Activation RMs是一种高效且安全的少样本奖励建模方法，适用于安全关键应用。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [122] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出了一种名为“主动测量”的人机协作AI框架，通过重要性采样和蒙特卡洛估计提升科学测量的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中的应用缺乏准确性和统计保证，需要一种更高效且可靠的方法。

Method: 结合AI预测和人工标注，采用重要性采样优化模型，并通过蒙特卡洛估计提供无偏测量。

Result: 主动测量在多个任务中显著降低了估计误差，且对AI模型的容错性较强。

Conclusion: 该框架为科学测量提供了一种高效、精确且统计可靠的新方法。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [123] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出了一种利用标准地图信息预测车道段及其拓扑的方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶汽车依赖高精地图的问题，通过车载传感器直接预测地图元素及其关系。

Method: 利用标准地图信息，结合混合车道段编码和去噪技术，提升训练稳定性和性能，并引入时间一致性。

Result: 实验表明，该方法大幅优于现有方法。

Conclusion: 提出的建模方案有效解决了高精地图在线构建的复杂性问题。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [124] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于梯度方向一致性的OOD检测方法，通过短路异常梯度提升检测效果，同时保持ID分类性能。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，OOD检测对模型安全部署至关重要。作者观察到ID样本的梯度方向一致，而OOD样本梯度混乱，从而提出改进方法。

Method: 提出一种推理阶段技术，短路异常梯度，并通过一阶近似避免重复计算。

Result: 在标准OOD基准测试中表现显著提升，方法轻量且易于实现。

Conclusion: 该方法为实际应用中的OOD检测提供了实用且高效的解决方案。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [125] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 论文提出了一种高效、低功耗的甲烷泄漏检测方法，通过改进算法（Mag1c-SAS和CEM）和结合机器学习模型，显著提升了检测速度和硬件适应性，同时提出了优化的波段选择策略。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，通过高光谱卫星图像早期检测其泄漏有助于缓解气候变化。现有任务多为手动操作，效率低且易遗漏事件，亟需一种适用于资源有限硬件的快速检测方法。

Method: 测试了快速目标检测方法（ACE、CEM），并提出Mag1c-SAS算法（Mag1c的快速变体）。结合机器学习模型（U-Net、LinkNet）评估性能，并提出了三种波段选择策略。

Result: Mag1c-SAS和CEM表现优异，分别优化了精度和速度，计算速度比原始Mag1c快100倍和230倍。其中一种波段选择策略在减少通道数的同时保持了精度。

Conclusion: 研究为资源有限的硬件上实现高效甲烷检测奠定了基础，提升了数据交付的及时性。代码、数据和模型已开源。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [126] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP是一种创新的息肉分割方法，通过边界蒸馏模块和1D-2D Mamba适配器，显著提升了弱边界分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在息肉形状、大小和颜色变化大且边界模糊的情况下表现不佳，难以满足实时临床需求。

Method: 结合边界蒸馏模块和1D-2D Mamba适配器，增强全局上下文交互和边界特征学习。

Result: 在五个数据集上表现优于现有方法，实现了更高的分割准确性和鲁棒性。

Conclusion: SAM-MaGuP为息肉分割领域设定了新标准，解决了弱边界挑战。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [127] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 提出了一种多模态课堂监控系统，结合睡意检测、手机使用追踪和人脸识别，提升学生注意力评估的精确度。


<details>
  <summary>Details</summary>
Motivation: 通过整合多种技术手段，实时监控学生行为，提高课堂参与度和自动化考勤效率。

Method: 利用YOLOv8检测手机和睡眠行为，LResNet Occ FC和MTCNN实现人脸识别，系统基于PHP和ESP32-CAM硬件实现。

Result: 睡眠检测mAP@50达97.42%，人脸识别验证准确率86.45%，手机检测mAP@50达85.89%。

Conclusion: 该系统为教育环境提供了高效、可扩展的监控解决方案，同时实现了自动化考勤。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [128] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 本文首次系统研究了深度学习人脸识别系统中的后门攻击，提出了两种新型攻击方式，并验证了其在多种配置下的有效性，同时提供了防御建议。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别系统的广泛应用引发了安全隐患，但现有研究对真实场景中的后门攻击关注不足。

Method: 通过探索深度神经网络后门攻击的可行性，提出了人脸生成和人脸关键点偏移两种攻击方式，并验证了其在20种系统配置和15种攻击案例中的有效性。

Result: 研究表明，单一后门攻击可绕过整个系统功能，同时发现大间隔损失训练的特征提取器也易受攻击。

Conclusion: 研究填补了后门攻击在真实人脸识别系统中的空白，为相关方提供了防御措施和最佳实践。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [129] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种用于大规模场景3D重建的框架，通过分块处理和优化采样策略，解决了传统NeRF方法的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法因内存限制难以处理大规模场景，需要一种高效且不损失质量的方法。

Method: 采用分块策略，将场景划分为无重叠的3D块，并优化采样策略以避免边缘误差。

Result: 实验表明，该方法能在线性时间内处理大规模卫星图像，且无需多设备支持。

Conclusion: Snake-NeRF为大规模3D重建提供了一种高效且高质量的解决方案。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [130] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT是一种新的tokenization策略，通过连续定位token来避免网格限制，显著减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 标准tokenization方法将特征限制在离散的patch网格中，阻碍了模型在稀疏场景下的潜力。

Method: 提出Subpixel Placement of Tokens (SPoT)，通过连续定位token，并结合oracle-guided搜索优化性能。

Result: SPoT显著减少了推理所需的token数量，同时提升了性能。

Conclusion: SPoT为ViT架构提供了灵活、高效且可解释的新方向，将稀疏性转化为战略优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [131] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 首届W-CODA研讨会概述，聚焦自动驾驶极端场景的多模态感知与理解技术。


<details>
  <summary>Details</summary>
Motivation: 探索下一代自动驾驶解决方案，解决极端场景下的挑战。

Method: 邀请5位学术界和工业界专家分享最新进展，并举办双轨挑战赛（极端场景理解与生成）。

Result: 研讨会汇集了前沿研究，推动自动驾驶技术在极端场景下的可靠性。

Conclusion: W-CODA将持续弥合前沿技术与完全智能、可靠的自动驾驶之间的差距。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [132] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，且语义任务优于几何任务。GPT-4o在非推理模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型在视觉理解方面的实际能力，填补现有研究的空白。

Method: 通过提示链将标准视觉任务转化为文本可提示和API兼容的任务，建立标准化评估框架。

Result: 模型在语义任务上表现优于几何任务，GPT-4o在非推理模型中表现最佳，推理模型在几何任务中有改进。

Conclusion: 多模态基础模型在视觉任务中表现尚可，但仍有改进空间，尤其是在几何任务和提示敏感性方面。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [133] [Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks](https://arxiv.org/abs/2507.01038)
*Seong-Joon Park,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim,Jong-Seon No*

Main category: cs.IT

TL;DR: 提出了一种基于Transformer架构的AI原生基础模型，用于统一且与编码无关的解码，解决了6G网络中传统解码器缺乏灵活性和可扩展性的问题。


<details>
  <summary>Details</summary>
Motivation: 6G网络的异构通信场景对传统解码器提出了灵活性和可扩展性的挑战，需要一种新型解码方法。

Method: 提出了CrossMPT（跨注意力消息传递Transformer），通过两个掩码跨注意力块迭代更新输入表示（幅度和校验向量），并进一步开发了FCrossMPT和CrossED（集成解码器）。

Result: CrossMPT在单神经解码器中实现了最先进的解码性能，FCrossMPT和CrossED展示了广泛的解码能力和强泛化性。

Conclusion: 提出的AI原生解码器具有灵活性、可扩展性和高性能，为6G网络的信道编码提供了有前景的方向。

Abstract: Channel coding for 6G networks is expected to support a wide range of
requirements arising from heterogeneous communication scenarios. These demands
challenge traditional code-specific decoders, which lack the flexibility and
scalability required for next-generation systems. To tackle this problem, we
propose an AI-native foundation model for unified and code-agnostic decoding
based on the transformer architecture. We first introduce a cross-attention
message-passing transformer (CrossMPT). CrossMPT employs two masked
cross-attention blocks that iteratively update two distinct input
representations-magnitude and syndrome vectors-allowing the model to
effectively learn the decoding problem. Notably, our CrossMPT has achieved
state-of-the-art decoding performance among single neural decoders. Building on
this, we develop foundation CrossMPT (FCrossMPT) by making the architecture
invariant to code length, rate, and class, allowing a single trained model to
decode a broad range of codes without retraining. To further enhance decoding
performance, particularly for short blocklength codes, we propose CrossMPT
ensemble decoder (CrossED), an ensemble decoder composed of multiple parallel
CrossMPT blocks employing different parity-check matrices. This architecture
can also serve as a foundation model, showing strong generalization across
diverse code types. Overall, the proposed AI-native code-agnostic decoder
offers flexibility, scalability, and high performance, presenting a promising
direction to channel coding for 6G networks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [134] [A Deterministic Partition Tree and Applications](https://arxiv.org/abs/2507.01775)
*Haitao Wang*

Main category: cs.CG

TL;DR: 提出了一种确定性版本的Chan随机划分树，用于解决高维单纯形范围计数等问题，突破了已知的下界限制。


<details>
  <summary>Details</summary>
Motivation: 改进Chan的随机划分树，提供确定性算法，适用于需要确定性结果的场景。

Method: 构建了一个空间复杂度为O(n)、预处理时间为O(n^{1+ε})的数据结构，查询时间为o(n^{1-1/d})。

Result: 在d维单纯形范围计数等问题中，突破了Ω(n^{1-1/d})的下界限制。

Conclusion: 该方法不依赖位压缩技术，适用于多种经典问题，未来可能有更多应用。

Abstract: In this paper, we present a deterministic variant of Chan's randomized
partition tree [Discret. Comput. Geom., 2012]. This result leads to numerous
applications. In particular, for $d$-dimensional simplex range counting (for
any constant $d \ge 2$), we construct a data structure using $O(n)$ space and
$O(n^{1+\epsilon})$ preprocessing time, such that each query can be answered in
$o(n^{1-1/d})$ time (specifically, $O(n^{1-1/d} / \log^{\Omega(1)} n)$ time),
thereby breaking an $\Omega(n^{1-1/d})$ lower bound known for the semigroup
setting. Notably, our approach does not rely on any bit-packing techniques. We
also obtain deterministic improvements for several other classical problems,
including simplex range stabbing counting and reporting, segment intersection
detection, counting and reporting, ray-shooting among segments, and more.
Similar to Chan's original randomized partition tree, we expect that additional
applications will emerge in the future, especially in situations where
deterministic results are preferred.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [135] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar,Parthe Pandit*

Main category: stat.ML

TL;DR: 浅层宽神经网络的输入输出映射的epigraph近似于凸函数的epigraph，解释了其良好性能。


<details>
  <summary>Details</summary>
Motivation: 研究浅层宽神经网络的输入输出映射特性，以解释其在实际应用中的良好表现。

Method: 分析浅层宽神经网络的输入输出映射的epigraph，并与凸函数的epigraph进行比较。

Result: 发现输入输出映射的epigraph在参数空间中近似于凸函数的epigraph。

Conclusion: 这种近似性为浅层宽神经网络的良好性能提供了合理解释。

Abstract: For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [136] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier,Pierre-Alexandre Mattei,Charles Bouveyron,Xavier Pennec*

Main category: stat.ML

TL;DR: 提出了一种新的高斯混合模型（GMM）家族，具有分段常数协方差特征值，平衡了全GMM和球形GMM的灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中全GMM过参数化和球形GMM灵活性不足的问题。

Method: 引入分段常数协方差特征值的GMM，并开发了EM算法学习参数，提出组件惩罚EM算法联合学习超参数。

Result: 在密度拟合、聚类和图像去噪等无监督任务中表现出优越的似然性与简洁性权衡。

Conclusion: 新模型在灵活性和参数效率之间取得了更好的平衡，适用于多种无监督学习任务。

Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [137] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu,Jingnan Zhang,Junhui Wang*

Main category: stat.ML

TL;DR: 论文挑战了序数配对比较数据比二元比较数据信息更丰富的传统观点，提出了一种无平局的序数配对比较建模框架，并证明二元化序数数据可显著提高排名恢复的准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨序数配对比较数据是否真的比二元比较数据提供更丰富的信息，并研究二元化序数数据对排名恢复的影响。

Method: 提出了一种广义加性结构的参数化框架，包含量化偏好差异的链接函数和控制序数响应分布的模式函数。

Result: 证明在计数算法下，二元比较的排名误差收敛速度比序数数据更快，并通过信号噪声比（SNR）量化了性能差距。

Conclusion: 二元化序数数据在某些情况下能显著提升排名恢复的准确性，并确定了最大化二元化效果的模式函数。

Abstract: Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [138] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Main category: stat.ML

TL;DR: 论文提出了一种结合概率框架的物理信息神经网络（PINNs），用于复杂系统中的不确定性建模，并通过随机微分方程和随机偏微分方程验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习（SciML）与不确定性量化（UQ）的结合是计算科学的前沿领域，但现有方法在复杂系统中的不确定性建模能力有限。

Method: 通过将生成建模技术与PINNs结合，提出了一种新的方法，以系统化方式控制不确定性，同时保持模型的预测准确性。

Result: 该方法在随机微分方程和随机偏微分方程的应用中表现出色，有效提升了不确定性建模能力。

Conclusion: 研究为复杂系统中的不确定性建模提供了一种有效工具，展示了SciML与UQ结合的潜力。

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [139] [Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction](https://arxiv.org/abs/2507.01913)
*Apoorv Verma,Junaid Jami,Amrita Bhattacharya*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种改进的描述符，利用材料的结构信息显著提高了对磁性排序和原子磁矩的预测准确性，适用于多种材料系统。


<details>
  <summary>Details</summary>
Motivation: 准确预测磁性行为对加速发现和设计下一代磁性材料至关重要，但现有模型局限于特定材料系统。

Method: 采用LightGBM模型，结合丰富的元素向量表示和高级特征工程，包括非线性项和减少矩阵稀疏性。

Result: 模型在磁性排序分类中达到82.4%的准确率，原子磁矩预测的相关系数为0.93，优于现有描述符。

Conclusion: 该框架为高通量筛选具有定制特性的磁性材料提供了强大工具。

Abstract: Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [140] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Main category: physics.geo-ph

TL;DR: 开发了一种基于Transformer编码器的深度学习模型，用于高效、客观地分类火山地震类型，优于传统CNN方法，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统火山地震分类依赖主观人工判断，耗时耗力，需更高效、客观的方法。

Method: 使用Transformer编码器构建深度学习模型，分析关键波形特征，并通过注意力权重可视化增强可解释性。

Result: 模型在Mount Asama的测试中表现优异（F1分数：火山构造地震0.930，低频地震0.931，噪声0.980），且数据质量和多样性对性能至关重要。

Conclusion: Transformer模型为火山地震分类提供了高效、可解释的解决方案，并可推广至其他火山区域，助力灾害评估与减灾。

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [141] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一个AI辅助评分平台，利用大语言模型（LLMs）转录和评估学生作业，显著减少评分时间并保持高评分一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模STEM课程中手写开放性答案评分的瓶颈问题。

Method: 开发Pensieve平台，整合LLMs进行转录和评分，支持从扫描作业到最终反馈的全流程。

Result: 在20多所机构的课程中部署，评分30万份答案，评分时间减少65%，高置信度预测与教师评分一致率达95.4%。

Conclusion: Pensieve有效提升评分效率，同时保持评分质量，适用于多学科STEM课程。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [142] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 利用大型语言模型（LLMs）和自主代理从专利中挖掘并生成产品概念，提出Agent Ideate框架，实验表明代理方法在创意质量、相关性和新颖性上优于独立LLMs。


<details>
  <summary>Details</summary>
Motivation: 专利蕴含丰富的技术知识，但访问和解读这些信息仍具挑战性，希望通过LLMs和代理技术挖掘其创新潜力。

Method: 设计Agent Ideate框架，结合开源LLMs和代理架构，在计算机科学、自然语言处理和材料化学领域进行实验。

Result: 代理方法在创意质量、相关性和新颖性上显著优于独立LLMs。

Conclusion: 结合LLMs与代理工作流可显著提升从专利数据生成商业创意的潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [143] [Counterfactual Explanation of Shapley Value in Data Coalitions](https://arxiv.org/abs/2507.01267)
*Michelle Si,Jian Pei*

Main category: cs.GT

TL;DR: 论文提出了一种用于解释数据联盟中Shapley值的反事实方法，并通过SV-Exp算法高效实现。


<details>
  <summary>Details</summary>
Motivation: Shapley值在数据市场中广泛应用，但如何解释数据所有者在联盟中的Shapley值仍是一个未解决的挑战。

Method: 通过反事实解释问题，提出SV-Exp算法，结合蒙特卡洛估计和启发式技术（如差分Shapley值估计和贪心子集转移）加速计算。

Result: 实验证明SV-Exp算法高效，反事实解释能有效解读Shapley值。

Conclusion: 反事实解释是可行的，SV-Exp算法为解决这一问题提供了高效工具。

Abstract: The Shapley value is widely used for data valuation in data markets. However,
explaining the Shapley value of an owner in a data coalition is an unexplored
and challenging task. To tackle this, we formulate the problem of finding the
counterfactual explanation of Shapley value in data coalitions. Essentially,
given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than
$B$, a counterfactual explanation is a smallest subset of data entries in $A$
such that transferring the subset from $A$ to $B$ makes the Shapley value of
$A$ less than that of $B$. We show that counterfactual explanations always
exist, but finding an exact counterfactual explanation is NP-hard. Using Monte
Carlo estimation to approximate counterfactual explanations directly according
to the definition is still very costly, since we have to estimate the Shapley
values of owners $A$ and $B$ after each possible subset shift. We develop a
series of heuristic techniques to speed up computation by estimating
differential Shapley values, computing the power of singular data entries, and
shifting subsets greedily, culminating in the SV-Exp algorithm. Our
experimental results on real datasets clearly demonstrate the efficiency of our
method and the effectiveness of counterfactuals in interpreting the Shapley
value of an owner.

</details>


### [144] [Rational Censorship Attack: Breaking Blockchain with a Blackboard](https://arxiv.org/abs/2507.01453)
*Michelle Yeo,Haoqian Zhang*

Main category: cs.GT

TL;DR: 论文揭示了一种基于博弈论的理性审查攻击，表明在区块链中，理性节点可能通过合谋审查其他节点以独占奖励，并证明这种攻击策略是子博弈完美均衡。


<details>
  <summary>Details</summary>
Motivation: 研究区块链的审查韧性，并从博弈论角度分析其安全性，揭示理性节点可能的行为模式。

Method: 通过博弈论框架建模，假设所有节点为理性，提出一种合谋审查攻击策略，并分析其均衡性。

Result: 证明合谋攻击策略是子博弈完美均衡，且攻击成功需知晓合谋节点的真实投票权。

Conclusion: 攻击对区块链用户和协议设计者具有重要影响，需探索潜在防御措施。

Abstract: Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

</details>


### [145] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: 研究大型语言模型（LLMs）作为市场代理时可能出现的合谋行为，分析了沟通能力、模型选择和环境压力对合谋行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会经济互动中的应用增加，识别其潜在的不良行为（如合谋）变得至关重要。

Method: 通过模拟连续双向拍卖市场，控制实验分析LLM代理作为卖家的行为，考察沟通、模型选择和环境压力对合谋的影响。

Result: 直接沟通增加合谋倾向，不同模型的合谋倾向不同，环境压力（如监管和紧迫感）影响合谋行为。

Conclusion: 研究强调了部署基于LLM的市场代理时需考虑的经济和伦理问题。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [146] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3是一个工具，通过自然语言查询简化了对MIMIC-IV临床数据库的访问，降低了技术门槛。


<details>
  <summary>Details</summary>
Motivation: 解决大型临床数据集（如MIMIC-IV）因技术复杂性和临床背景知识需求而难以有效利用的问题。

Method: M3通过单命令获取MIMIC-IV数据，利用语言模型将自然语言问题转换为SQL查询，并提供结构化结果和查询验证。

Result: 演示表明，M3能在几分钟内完成以往需要数小时手工SQL编写的复杂队列分析。

Conclusion: M3简化了临床数据的访问，加速了从原始记录到可操作见解的转化，促进了更广泛的研究社区参与。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [147] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: 论文提出了一种增强型影响感知群组推荐框架（EIGR），通过图提取采样策略、动态独立级联模型和两级哈希用户群组索引，解决了社交图规模大、影响传播动态变化及实时匹配计算开销高的挑战。


<details>
  <summary>Details</summary>
Motivation: 群组推荐在社交媒体流中应用广泛，但现有方法面临社交图规模大、影响传播动态变化和实时匹配计算开销高的挑战。

Method: 提出EIGR框架，包括图提取采样策略（GES）、动态独立级联模型（DYIC）和两级哈希用户群组索引（UG-Index）。

Result: 在真实数据集上的实验表明，EIGR在效果和效率上均优于现有基线方法。

Conclusion: EIGR框架有效解决了群组推荐中的关键挑战，提升了推荐效果和效率。

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [148] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: 该研究提出了一种结合大型语言模型（LLM）和检索增强生成（RAG）技术的框架，用于高效分析加尔各答高等法院的判决，包括文本摘要和类似案例检索。


<details>
  <summary>Details</summary>
Motivation: 司法资源需要高效利用，法律专业人士和学生在处理复杂法律文本时面临挑战。

Method: 使用Pegasus模型进行法律文本摘要，并构建RAG框架以检索类似案例。

Result: 显著提升了法律案例摘要的质量，并构建了全面的向量数据库以支持高效检索。

Conclusion: 该框架提高了法律研究效率，帮助用户快速获取关键法律信息。

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [149] [Optimizing Conversational Product Recommendation via Reinforcement Learning](https://arxiv.org/abs/2507.01060)
*Kang Liu*

Main category: cs.IR

TL;DR: 提出一种基于强化学习的方法，优化跨行业产品推荐的对话策略，通过反馈驱动的学习提升对话效果。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理在销售和服务中的广泛应用，对话的推荐方式和时机对效果至关重要。

Method: 利用强化学习从行为模式和转化结果中学习最优对话策略。

Result: 方法能够提升用户参与度和产品采纳率，同时满足上下文和法规约束。

Conclusion: 该框架为企业在个性化推荐中提供了可扩展的解决方案。

Abstract: We propose a reinforcement learning-based approach to optimize conversational
strategies for product recommendation across diverse industries. As
organizations increasingly adopt intelligent agents to support sales and
service operations, the effectiveness of a conversation hinges not only on what
is recommended but how and when recommendations are delivered. We explore a
methodology where agentic systems learn optimal dialogue policies through
feedback-driven reinforcement learning. By mining aggregate behavioral patterns
and conversion outcomes, our approach enables agents to refine talk tracks that
drive higher engagement and product uptake, while adhering to contextual and
regulatory constraints. We outline the conceptual framework, highlight key
innovations, and discuss the implications for scalable, personalized
recommendation in enterprise environments.

</details>


### [150] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: 论文提出了一种基于嵌入的检索（EBR）方法，用于补充传统分类方法在视频内容审核中的不足，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在快速响应和成本效益方面存在局限，尤其是在趋势适应和紧急升级场景中。

Method: 采用监督对比学习（SCL）框架训练单模态和多模态基础嵌入模型，并设计了一个嵌入生成与视频检索结合的EBR系统。

Result: 离线实验显示EBR将ROC-AUC从0.85提升至0.99，PR-AUC从0.35提升至0.95；在线实验表明EBR提高了10.32%的行动率并降低80%以上的运营成本。

Conclusion: EBR方法在性能、效率和灵活性上优于传统分类方法，适合视频内容审核的实际需求。

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [151] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: 本文探讨了人工智能（AI）如何通过数字技术提升交通基础设施的损害评估与监测能力，特别关注桥梁损害检测的挑战与机遇，并指出SAR数据与AI模型结合的研究缺口。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施对经济增长至关重要，但面临老化、气候变化和混合威胁（如自然灾害、网络攻击）的风险，需要新技术提升其韧性和功能性。

Method: 通过系统性文献综述，分析了现有AI模型和数据集在道路、桥梁等基础设施损害评估中的应用，特别关注SAR数据与AI模型的整合。

Result: 研究发现，目前缺乏将AI模型应用于SAR数据进行桥梁损害全面评估的研究，存在明显的研究缺口。

Conclusion: 本文旨在填补研究缺口，为AI驱动的交通基础设施损害评估与监测提供基础。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [152] [A bibliometric analysis on the current situation and hot trends of the impact of microplastics on soil based on CiteSpace](https://arxiv.org/abs/2507.01520)
*Yiran Zheng,Yue Quan,Su Yan,Xinting Lv,Yuguanmin Cao,Minjie Fu,Mingji Jin*

Main category: cs.DL

TL;DR: 本文综述了2013-2024年土壤微塑料（MPs）的研究现状与发展趋势，通过文献计量分析揭示了研究热点、高产作者及机构，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 微塑料在土壤中的积累及其对生态系统和人类健康的潜在影响，促使研究其污染控制的重要性。

Method: 利用CiteSpace和VOSviewer对Web of Science文献进行关键词共现、聚类、突现词分析及作者机构共现分析。

Result: 研究发现文献数量逐年增长，2024年达956篇；高产作者贡献显著；关键词聚类形成10大主题；研究分为探索、扩展和整合三阶段。

Conclusion: 未来需多层面评估微塑料对土壤生态的影响，以揭示危害并制定解决方案。

Abstract: This paper aims to comprehensively grasp the research status and development
trends of soil microplastics (MPs). It collects studies from the Web of Science
Core Collection covering the period from 2013 to 2024. Employing CiteSpace and
VOSviewer, the paper conducts in - depth analyses of literature regarding the
environmental impacts of microplastics. These analyses involve keyword co -
occurrence, clustering, burst term identification, as well as co - occurrence
analysis of authors and institutions. Microplastics can accumulate in soil,
transfer through food chains, and ultimately affect human health, making the
research on them essential for effective pollution control. Focusing on the
international research on the impacts of microplastics on soil and ecosystems,
the study reveals a steadily increasing trend in the number of publications
each year, reaching a peak of 956 articles in 2024. A small number of highly
productive authors contribute significantly to the overall research output. The
keyword clustering analysis results in ten major clusters, including topics
such as plastic pollution and microbial communities. The research on soil
microplastics has evolved through three distinct stages: the preliminary
exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and
the integration phase from 2021 to 2024. For future research, multi - level
assessments of the impacts of microplastics on soil ecosystems and organisms
should be emphasized, in order to fully uncover the associated hazards and
develop practical solutions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [153] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: 本研究评估了八种开源音乐生成系统（MGS）在当代音乐制作中的实用性，提出了一种结合技术和实践的评价框架，发现MGS主要作为辅助工具，强调人类创造力的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 探讨MGS在音乐制作中的实际应用潜力，填补现有系统在主题和结构连贯性上的不足，推动AI作为协作工具的发展。

Method: 采用单评估者方法，结合定性和定量分析，评估八种具有架构多样性的MGS。

Result: MGS在增强人类创造力方面表现良好，但在情感深度和复杂决策任务中仍需人类主导。

Conclusion: 研究提出了一个结构化评价框架，为未来MGS的开发和集成提供了实证指导。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [154] [Agentic AI in Product Management: A Co-Evolutionary Model](https://arxiv.org/abs/2507.01069)
*Nishant A. Parikh*

Main category: cs.CE

TL;DR: 本文提出了一种概念性协同进化框架，探讨了自主AI在产品管理中的变革作用，并指导其在整个产品生命周期中的整合。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补传统框架的空白，探索自主AI如何重新定义产品经理的角色，并推动其与AI的协同进化。

Method: 结合系统理论、协同进化理论和人机交互理论，构建了一个框架，并通过70多个来源的综述和案例研究进行分析。

Result: 研究发现产品经理需具备AI素养、治理能力和系统思维，以实现与AI的相互适应。

Conclusion: 研究为未来研究和实践提供了基础，以确保自主AI在软件组织中的负责任和有效整合。

Abstract: This study explores agentic AI's transformative role in product management,
proposing a conceptual co-evolutionary framework to guide its integration
across the product lifecycle. Agentic AI, characterized by autonomy,
goal-driven behavior, and multi-agent collaboration, redefines product managers
(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,
co-evolutionary theory, and human-AI interaction theory, the framework maps
agentic AI capabilities in discovery, scoping, business case development,
development, testing, and launch. An integrative review of 70+ sources,
including case studies from leading tech firms, highlights PMs' evolving roles
in AI orchestration, supervision, and strategic alignment. Findings emphasize
mutual adaptation between PMs and AI, requiring skills in AI literacy,
governance, and systems thinking. Addressing gaps in traditional frameworks,
this study provides a foundation for future research and practical
implementation to ensure responsible, effective agentic AI integration in
software organizations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [155] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 研究评估了标签不平衡对网络入侵警报分类的影响，发现其影响分类性能和解释性，建议通过调整SOC检测规则减少不平衡。


<details>
  <summary>Details</summary>
Motivation: 自动化在SOC中很重要，但需解决数据不平衡和可解释性问题。

Method: 使用DeepCASE评估标签不平衡对警报分类的影响。

Result: 标签不平衡影响分类性能和解释性。

Conclusion: 调整SOC检测规则可减少不平衡，提升自动化的性能和可解释性。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


### [156] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: AutoAdv是一个自动化对抗性提示生成框架，用于评估和暴露大型语言模型（LLM）的安全漏洞，通过多轮攻击方法实现高达86%的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）易受越狱攻击，现有安全机制存在漏洞，需要系统化评估和暴露这些漏洞。

Method: 利用参数化攻击者LLM生成语义伪装的恶意提示，结合角色扮演、误导和上下文操纵等技术，动态多轮攻击。

Result: 在ChatGPT、Llama和DeepSeek等先进模型上，AutoAdv实现了高达86%的越狱成功率。

Conclusion: 当前安全机制对复杂多轮攻击仍脆弱，亟需更强大的防御策略。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [157] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 本文探讨了安全混洗器的定义、实现及其在隐私保护技术中的作用，并比较了26种协议。


<details>
  <summary>Details</summary>
Motivation: 安全混洗器在隐私数据聚合和差分隐私中具有重要作用，但现有研究常将其视为黑箱，忽略了实际漏洞和性能权衡。

Method: 通过识别、分类和比较26种安全协议，统一安全定义，并提供选择指南。

Result: 提出了一个统一的框架来评估安全混洗器，并总结了其在不同隐私保护技术中的应用。

Conclusion: 安全混洗器的设计和选择需综合考虑安全性和性能，未来研究应关注实际应用中的优化。

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [158] [Symbolic identification of tensor equations in multidimensional physical fields](https://arxiv.org/abs/2507.01466)
*Tianyi Chen,Hao Yang,Wenjun Ma,Jun Zhang*

Main category: math-ph

TL;DR: 提出了一种名为SITE的数据驱动框架，用于从数据中识别张量方程，通过宿主-质粒结构和遗传信息保留策略提高鲁棒性，并引入维度一致性检查和张量线性回归技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法多限于标量方程，难以识别张量关系，因此需要一种通用的数据驱动框架来解决这一问题。

Method: SITE采用宿主-质粒结构表示张量方程，结合遗传信息保留策略、维度一致性检查和张量线性回归技术。

Result: SITE在合成数据中准确恢复目标方程，对噪声和小样本具有鲁棒性，并成功从分子模拟数据中识别本构关系。

Conclusion: SITE展示了数据驱动发现张量方程的潜力，适用于可压缩和不可压缩流动条件。

Abstract: Recently, data-driven methods have shown great promise for discovering
governing equations from simulation or experimental data. However, most
existing approaches are limited to scalar equations, with few capable of
identifying tensor relationships. In this work, we propose a general
data-driven framework for identifying tensor equations, referred to as Symbolic
Identification of Tensor Equations (SITE). The core idea of SITE--representing
tensor equations using a host-plasmid structure--is inspired by the
multidimensional gene expression programming (M-GEP) approach. To improve the
robustness of the evolutionary process, SITE adopts a genetic information
retention strategy. Moreover, SITE introduces two key innovations beyond
conventional evolutionary algorithms. First, it incorporates a dimensional
homogeneity check to restrict the search space and eliminate physically invalid
expressions. Second, it replaces traditional linear scaling with a tensor
linear regression technique, greatly enhancing the efficiency of numerical
coefficient optimization. We validate SITE using two benchmark scenarios, where
it accurately recovers target equations from synthetic data, showing robustness
to noise and small sample sizes. Furthermore, SITE is applied to identify
constitutive relations directly from molecular simulation data, which are
generated without reliance on macroscopic constitutive models. It adapts to
both compressible and incompressible flow conditions and successfully
identifies the corresponding macroscopic forms, highlighting its potential for
data-driven discovery of tensor equation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [159] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: 本文综述了机器人领域中的声源定位（SSL）技术，重点介绍了深度学习方法的最新进展，并探讨了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多关注通用音频应用，未充分考虑机器人领域的限制及深度学习的最新进展，本文旨在填补这一空白。

Method: 回顾了经典方法（如TDOA、波束成形等）和现代深度学习方法（如CNN、CRNN等），并分析了数据与训练策略。

Result: 总结了不同机器人类型和应用领域的研究，提出了当前SSL面临的挑战（如环境鲁棒性、多声源问题等）。

Conclusion: 提出了未来研究方向，旨在实现更鲁棒、高效且可解释的深度学习SSL技术。

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [160] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: JumpER是一种通过自演化先验分阶段训练强化学习策略的框架，解决了四足机器人在极端欠驱动和极端地形下的单足跳跃任务挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法在同时应对极端欠驱动和极端地形时，由于早期交互不稳定和奖励反馈不可靠，难以直接训练有效的策略。

Method: JumpER通过多阶段逐步增加复杂性的策略学习，动态生成自演化先验，无需外部专家先验或手工奖励设计。

Result: 四足机器人首次实现了在不可预测地形上的稳健单足跳跃，能应对60厘米宽间隙、不规则楼梯和15-35厘米间距的踏脚石等挑战。

Conclusion: JumpER为极端欠驱动和极端地形下的运动任务提供了一种原则性且可扩展的解决方案。

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [161] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD模型通过微调视觉语言模型（VLM）与端到端自动驾驶系统（VAD）结合，提升空间推理能力，减少碰撞率31.82%。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（如LLaVA、Qwen-VL）的通用知识，增强自动驾驶的感知、预测和规划能力。

Method: 提出VLAD模型，通过定制问答数据集微调VLM，生成高级导航指令，并由VAD处理以指导车辆操作。

Result: 在nuScenes数据集上测试，碰撞率平均降低31.82%。

Conclusion: VLAD为VLM增强的自动驾驶系统设立了新基准，提高了系统的透明度和可信度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [162] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2是一种基于SAM2的多器官医学分割模型，通过跨模态交互和语义提示解决了现有模型的细节不准确、依赖几何提示和空间信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 当前多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失的局限性，需要改进。

Method: 采用跨模态交互机制将视觉和文本输入转化为语义信息，结合语义提示策略和记忆自更新策略，增强细节感知。

Result: 在七个公开数据集上的实验表明，CRISP-SAM2优于现有模型。

Conclusion: CRISP-SAM2在解决现有问题方面表现出色，具有优越性能。

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [163] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: 提出了一种名为SWinMamba的新方法，通过蛇形窗口序列和双向状态空间模型，实现了血管结构的连续性和精确分割。


<details>
  <summary>Details</summary>
Motivation: 血管分割在医学图像中至关重要，但传统方法常因血管细长和先验建模不足导致分割不连续。

Method: SWinMamba结合蛇形窗口序列（SWToken）和双向聚合模块（BAM），通过自适应分割和特征整合实现血管连续性建模，并利用空间-频率融合单元（SFFU）增强特征表示。

Result: 在三个数据集上的实验表明，SWinMamba能够生成完整且连接的血管分割结果，性能优越。

Conclusion: SWinMamba通过创新的蛇形窗口序列和双向建模，有效解决了血管分割中的不连续性问题，具有实际应用潜力。

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [164] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE是一个开源、计算资源需求低的3D视觉基础模型，用于低剂量CT扫描分析，能快速适应多种疾病检测任务，并在14种疾病分类中达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决低剂量CT扫描分析中放射科医生短缺的问题，同时扩展肺癌筛查项目以涵盖更多呼吸系统疾病。

Method: 采用自监督学习预训练模型，基于超过98,000例胸部LDCT数据，结合掩码自编码器框架扩展到3D成像。

Result: 在14种疾病分类任务中表现优异，计算资源需求低且标签效率高。

Conclusion: TANGERINE为下一代医学影像工具提供了可扩展、开放的解决方案，有望推动肺癌筛查向全面呼吸系统疾病管理转变。

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [165] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 提出了一种名为A LoD of Gaussians的框架，用于在单个消费级GPU上训练和渲染超大规模高斯场景，无需分区。


<details>
  <summary>Details</summary>
Motivation: 解决现有高斯溅射技术在扩展到大场景时因分区策略导致的边界伪影、训练复杂性和GPU内存限制问题。

Method: 采用混合数据结构（高斯层次结构与顺序点树）和轻量级缓存与视图调度系统，动态流式传输相关高斯数据。

Result: 实现了无缝多尺度重建和复杂场景的交互式可视化，支持从高空俯瞰到地面细节的多尺度渲染。

Conclusion: 该方法在单个GPU上实现了超大规模场景的高效训练和实时渲染，解决了现有技术的局限性。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [166] [STEM Diffraction Pattern Analysis with Deep Learning Networks](https://arxiv.org/abs/2507.01889)
*Sebastian Wissel,Jonas Scheunert,Aaron Dextre,Shamail Ahmed,Andreas Bayer,Kerstin Volz,Bai-Xiang Xu*

Main category: cond-mat.dis-nn

TL;DR: 论文提出了一种基于机器学习的扫描透射电子显微镜衍射图案分析方法，用于高效预测晶体取向，并比较了三种深度学习架构的性能。


<details>
  <summary>Details</summary>
Motivation: 锂镍氧化物作为下一代锂离子电池的阴极材料，其电化学行为与微观结构特征密切相关，传统取向映射方法效率低且噪声敏感。

Method: 采用卷积神经网络（CNN）、密集卷积网络（DenseNets）和Shifted Windows（Swin）Transformer三种深度学习架构，直接从衍射图案预测欧拉角。

Result: Swin Transformer表现最佳，生成的晶体取向图清晰展示了晶界和晶内取向分布。

Conclusion: 结合先进机器学习模型与STEM数据，可实现高效、高精度的微观结构表征。

Abstract: Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [167] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 论文提出了SynPref-40M大规模偏好数据集和Skywork-Reward-V2奖励模型系列，通过人机协同的数据标注方法显著提升了奖励模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在捕捉人类偏好方面表现不佳，主要源于偏好数据集的局限性，如范围狭窄、标注质量低等。

Method: 设计了一个人机协同的两阶段数据标注流程，结合人类标注质量和AI的可扩展性，并训练了从0.6B到8B参数的奖励模型。

Result: Skywork-Reward-V2在多个基准测试中达到最优性能，验证了数据规模和高质标注的重要性。

Conclusion: 人机协同的数据标注方法能显著提升奖励模型性能，为未来研究提供了新方向。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [168] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating是一个可扩展的框架，通过将英语数据质量信号转移到17种目标语言，训练多语言评估器，显著提升了多语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的数据选择方法主要关注英语，而数据质量对多语言大模型性能至关重要。

Method: MuRating通过聚合多个英语评分器的信号，学习统一的文档质量评分，并通过翻译将这些评分投射到目标语言，训练多语言评估器。

Result: MuRating在英语和多语言基准测试中均显著提升了模型性能，尤其在知识密集型任务上表现突出。

Conclusion: MuRating为多语言数据质量评估提供了有效解决方案，并指出了未来在翻译保真度和叙事材料代表性方面的改进方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [169] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究探讨了多模态AI模型在输入信息冲突时的行为，发现模型倾向于优先处理某一模态，并揭示了内部表征结构和注意力机制的作用。


<details>
  <summary>Details</summary>
Motivation: 理解多模态模型在输入信息冲突时的行为，以提升模型在复杂环境中的表现。

Method: 通过向视觉-语言模型提供不一致的输入（如图像与标题不符），观察模型对不同模态的偏好，并分析内部表征和注意力机制。

Result: 模型通常优先处理某一模态，且不同模型偏好不同；内部表征结构和特定注意力头可调整模态偏好；发现模态无关的“路由头”可优化性能。

Conclusion: 研究为识别和控制多模态模型在冲突信号中的行为提供了关键步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [170] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文分析了MDACE数据集，评估了可解释医疗编码系统的合理性，发现真实证据与代码描述部分一致，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码可简化文档和计费流程，但现有方法因数据稀缺多限于短文本和二元设置，MDACE数据集为研究提供了新资源。

Method: 对MDACE数据集进行深入分析，评估当前可解释医疗编码系统的合理性，并提出匹配度量方法。

Result: 真实证据与代码描述部分一致，先进方法与真实证据高度重叠。

Conclusion: 研究为开发与评估可解释医疗编码系统提供了建议，加深了对自动医疗编码的理解。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [171] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 论文提出了一种系统方法，通过分析低困惑度序列来研究大语言模型（LLMs）如何利用和复制训练数据，发现部分序列无法追溯到训练数据，并对匹配序列的分布进行了量化。


<details>
  <summary>Details</summary>
Motivation: 理解训练数据如何影响LLMs的输出，以提高透明度、问责性、隐私和公平性。

Method: 通过分析低困惑度序列（高概率文本片段）并追踪其训练数据来源。

Result: 发现大量低困惑度序列无法映射到训练数据，对匹配序列的分布进行了量化。

Conclusion: 该方法有助于更好地理解LLMs训练数据对其行为的影响。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [172] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 提出了一种新型剪枝算法，专注于高层注意力头的剪枝，并通过自适应重缩放参数校准表示尺度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法不考虑注意力头在网络中的位置，导致性能下降。

Method: 提出剪枝高层注意力头的策略，并引入自适应重缩放参数以校准剪枝后的表示尺度。

Result: 在多种LLM和27个数据集上验证，生成任务表现尤其突出。

Conclusion: 新方法在压缩模型和提升推理效率方面优于现有技术。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [173] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 论文提出了一种名为GAPO的新方法，通过多目标优化解决LLM与多样化人类偏好对齐的挑战，并引入P-GAPO进一步结合用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效对齐多样化且可能冲突的人类偏好。

Method: 提出Gradient-Adaptive Policy Optimization (GAPO)，利用多梯度下降法平衡目标间的权衡，并引入P-GAPO结合用户偏好。

Result: 理论证明GAPO收敛于Pareto最优解，实验显示其在Mistral-7B上优于现有方法。

Conclusion: GAPO和P-GAPO为LLM与多样化人类偏好对齐提供了有效解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [174] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: 论文研究了一类非凸非凹的极小极大问题，其中内部最大化问题满足局部KL条件。通过证明相关极大函数的局部Hölder光滑性，提出了一种不精确近端梯度法，并给出了计算近似稳定点的复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 解决传统全局KL或PL条件过于严格的问题，适应更广泛的实践场景，同时应对局部KL条件带来的新挑战。

Method: 提出一种不精确近端梯度法，通过近端梯度法计算极大函数的不精确梯度，处理KL结构的子问题。

Result: 证明了极大函数的局部Hölder光滑性，并给出了计算近似稳定点的复杂度保证。

Conclusion: 局部KL条件更灵活，提出的方法能有效处理由此带来的复杂性和病态性。

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [175] [Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm](https://arxiv.org/abs/2507.01501)
*Eloy Peña-Asensio,Fabio Ferrari*

Main category: astro-ph.EP

TL;DR: 研究评估了HDBSCAN算法在无监督流星群识别中的表现，并与CAMS方法对比，发现HDBSCAN在统计一致性上更优。


<details>
  <summary>Details</summary>
Motivation: 准确识别流星群对理解其起源和演化至关重要，但重叠簇和背景噪声增加了分类难度，尤其是对依赖流星雨观测的任务（如ESA的LUMIO）。

Method: 使用HDBSCAN算法分析CAMS流星轨道数据库，比较三种特征向量（LUTAB、ORBIT、GEO），并通过匈牙利算法与CAMS分类对齐，评估聚类性能。

Result: HDBSCAN确认39个流星群（21个与CAMS高度一致），ORBIT向量识别30个（13个匹配度高）。eom方法表现最优。

Conclusion: HDBSCAN在统计一致性上优于CAMS方法，但需进一步验证物理有效性。

Abstract: Accurate identification of meteoroid streams is central to understanding
their origins and evolution. However, overlapping clusters and background noise
hinder classification, an issue amplified for missions such as ESA's LUMIO that
rely on meteor shower observations to infer lunar meteoroid impact parameters.
This study evaluates the performance of the Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) algorithm for unsupervised
meteoroid stream identification, comparing its outcomes with the established
Cameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze
the CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS
geocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted
geocentric parameters). HDBSCAN is applied with varying minimum cluster sizes
and two cluster selection methods (eom and leaf). To align HDBSCAN clusters
with CAMS classifications, the Hungarian algorithm determines the optimal
mapping. Clustering performance is assessed via the Silhouette score,
Normalized Mutual Information, and F1 score, with Principal Component Analysis
further supporting the analysis. With the GEO vector, HDBSCAN confirms 39
meteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies
30 streams, 13 with high matching scores. Less active showers pose
identification challenges. The eom method consistently yields superior
performance and agreement with CAMS. Although HDBSCAN requires careful
selection of the minimum cluster size, it delivers robust, internally
consistent clusters and outperforms the look-up table method in statistical
coherence. These results underscore HDBSCAN's potential as a mathematically
consistent alternative for meteoroid stream identification, although further
validation is needed to assess physical validity.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [176] [Efficient Gate Reordering for Distributed Quantum Compiling in Data Centers](https://arxiv.org/abs/2507.01090)
*Riccardo Mengoni,Walter Nadalin,Mathys Rennela,Jimmy Rotureau,Tom Darras,Julien Laurat,Eleni Diamanti,Ioannis Lavdas*

Main category: quant-ph

TL;DR: 量子编译器araQne通过电路重排序策略减少量子电路分布成本，优化量子网络中的通信需求。


<details>
  <summary>Details</summary>
Motivation: 量子计算时代需要新的基础设施和软件工具，量子网络将成为混合量子增强数据中心的核心，因此需要减少量子处理单元间的通信。

Method: 开发量子编译器araQne，通过电路重排序策略最小化分布成本（以纠缠对数量衡量）。

Result: 电路重排序策略显著降低了分布成本，优于基线方法。

Conclusion: araQne编译器在优化量子网络通信方面具有重要作用。

Abstract: Just as classical computing relies on distributed systems, the quantum
computing era requires new kinds of infrastructure and software tools. Quantum
networks will become the backbone of hybrid, quantum-augmented data centers, in
which quantum algorithms are distributed over a local network of quantum
processing units (QPUs) interconnected via shared entanglement. In this
context, it is crucial to develop methods and software that minimize the number
of inter-QPU communications. Here we describe key features of the quantum
compiler araQne, which is designed to minimize distribution cost, measured by
the number of entangled pairs required to distribute a monolithic quantum
circuit using gate teleportation protocols. We establish the crucial role
played by circuit reordering strategies, which strongly reduce the distribution
cost compared to a baseline approach.

</details>


### [177] [Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing](https://arxiv.org/abs/2507.01902)
*Grier M. Jones,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 分析了五种电子结构方法在分布式量子计算（DQC）中的适用性，发现多数方法无法通过局部操作（LO）高效并行化，需开发新方法。


<details>
  <summary>Details</summary>
Motivation: 推动量子计算进入实用阶段，探索分布式量子计算（DQC）在量子化学（电子结构理论）中的应用潜力。

Method: 分析五种常见电子结构方法（如Tequila和ffsim中的方法），结合Qiskit Circuit Cutting插件，评估其通过局部操作（LO）切割和分布的能力。

Result: 多数电子结构方法无法通过LO高效并行化，限制了其在DQC中的应用。

Conclusion: 需开发新方法以实现电子结构理论在DQC框架中的有效应用。

Abstract: To move towards the utility era of quantum computing, many corporations have
posed distributed quantum computing (DQC) as a framework for scaling the
current generation of devices for practical applications. One of these
applications is quantum chemistry, also known as electronic structure theory,
which has been poised as a "killer application" of quantum computing, To this
end, we analyze five electronic structure methods, found in common packages
such as Tequila and ffsim, which can be easily interfaced with the Qiskit
Circuit Cutting addon. Herein, we provide insights into cutting these
algorithms using local operations (LO) to determine their aptitude for
distribution. The key findings of our work are that many of these algorithms
cannot be efficiently parallelized using LO, and new methods must be developed
to apply electronic structure theory within a DQC framework.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [178] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: 提出一种基于GPU的全局搜索方法，利用区间分析和GPU计算能力，确保在非线性函数中包围全局最小值。


<details>
  <summary>Details</summary>
Motivation: 解决高维非线性函数的全局优化问题，尤其是在存在舍入误差时仍能保证结果准确性。

Method: 结合区间分析和GPU并行计算，通过排除不可能区域和变量循环技术提高效率。

Result: 成功包围了10个高维（最高10,000维）多模态基准函数的全局最小值，计算时间合理。

Conclusion: 该方法在GPU架构下表现出色，显著优于现有文献结果。

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [179] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Main category: math.NA

TL;DR: 论文证明了稀疏网格求积在高维分布数值积分中的一致性，通过运输映射和稀疏网格求积方法控制误差。


<details>
  <summary>Details</summary>
Motivation: 高维分布的数值积分是一个复杂问题，需要一种可靠的方法来保证计算的一致性和准确性。

Method: 1. 学习运输映射将分布归一化到单位立方体上的噪声分布；2. 使用Clenshaw-Curtis稀疏网格求积对生成映射与目标量的组合进行数值积分。

Result: 证明了在经验风险最小化框架下，所有误差项均可控制，数值积分能以高概率逼近理论值。

Conclusion: 该方法在高维数值积分中具有一致性和可靠性，适用于大数据集和自适应网络容量增长的情况。

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [180] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: 提出了一种神经熵稳定保守通量形式神经网络（NESCFN），直接从解轨迹学习双曲守恒律及其熵函数，无需预定义数值离散化。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络架构大多依赖预先已知的控制方程或固定离散化，而该方法通过嵌入熵稳定设计原则，实现了完全数据驱动的物理一致性动态发现。

Method: 通过联合学习数值通量函数和对应熵，确保守恒和熵耗散，适用于双曲守恒律系统的长期稳定性和保真度。

Result: 数值结果表明，该方法在长时间范围内保持稳定和守恒，并能准确捕捉激波传播速度，即使训练数据中未包含未来时间解剖面。

Conclusion: NESCFN方法在无需预定义离散化的情况下，成功实现了双曲守恒律的物理一致性和稳定性。

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>
