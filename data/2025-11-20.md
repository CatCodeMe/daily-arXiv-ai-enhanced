<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 56]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 13]
- [cs.CV](#cs.CV) [Total: 18]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Castle: Causal Cascade Updates in Relational Databases with Large Language Models](https://arxiv.org/abs/2511.14762)
*Yongye Su,Yucheng Zhang,Zeru Shi,Bruno Ribeiro,Elisa Bertino*

Main category: cs.DB

TL;DR: Castle是首个使用大语言模型进行仅模式级联更新生成的框架，能够通过自然语言指令生成多列、因果一致的SQL UPDATE语句，而无需向模型暴露表内容。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注SELECT查询，忽略了SQL更新操作及其连锁效应的挑战。传统的CASCADE UPDATE约束是静态的，不适合需要动态、上下文感知更新的现代非规范化数据库。

Method: 将UPDATE SQL生成构建为分治任务，利用LLM的推理能力确定哪些列需要直接更新，以及这些更新如何通过模式传播，通过嵌套查询和子结构确保数据机密性。

Result: 在真实世界因果更新场景中评估，证明其能够生成准确的SQL更新，展示了LLM在自动化DBMS中的推理能力。

Conclusion: Castle框架成功展示了LLM在数据库管理系统自动化中的推理能力，为仅模式的级联更新生成提供了有效解决方案。

Abstract: This work introduces Castle, the first framework for schema-only cascade update generation using large language models (LLMs). Despite recent advances in LLMs for Text2SQL code generation, existing approaches focus primarily on SELECT queries, neglecting the challenges of SQL update operations and their ripple effects. Traditional CASCADE UPDATE constraints are static and unsuitable for modern, denormalized databases, which demand dynamic, context-aware updates. Castle enables natural language instructions to trigger multi-column, causally consistent SQL UPDATE statements, without revealing table content to the model. By framing UPDATE SQL generation as a divide-and-conquer task with LLMs' reasoning capacity, Castle can determine not only which columns must be directly updated, but also how those updates propagate through the schema, causing cascading updates -- all via nested queries and substructures that ensure data confidentiality. We evaluate it on real-world causal update scenarios, demonstrating its ability to produce accurate SQL updates, and thereby highlighting the reasoning ability of LLMs in automated DBMS.

</details>


### [2] [BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer](https://arxiv.org/abs/2511.15090)
*Wenhan Yu,Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Lei Sha,Deguo Xia,Jizhou Huang*

Main category: cs.DB

TL;DR: 提出了BBox DocVQA数据集，这是一个大规模、基于边界框的文档视觉问答数据集，旨在增强视觉文档中的空间推理和证据定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有的DocVQA数据集主要停留在页面级别，缺乏细粒度的空间定位，限制了视觉语言模型的解释性和推理能力。

Method: 开发了自动构建流水线Segment Judge and Generate，结合区域分割模型、语义判断VLM和问答生成VLM，并通过人工验证确保质量。

Result: 构建的数据集包含3.6K个多样化文档和32K个问答对，涵盖单区域/多区域以及单页/多页场景。基准测试显示现有VLMs在空间定位和推理准确性方面仍面临挑战。

Conclusion: 在BBox DocVQA上进行微调显著提高了边界框定位和答案生成能力，验证了该数据集对增强VLM推理能力的有效性。

Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

</details>


### [3] [B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index](https://arxiv.org/abs/2511.15557)
*Selim Furkan Tekin,Rajesh Bordawekar*

Main category: cs.DB

TL;DR: 提出了一种基于磁盘的近似最近邻索引B+ANN，解决了HNSW算法的内存设计、随机内存访问、缓存行为差等问题，通过数据分块和B+树变体实现混合遍历，在召回率和查询性能上优于HNSW。


<details>
  <summary>Details</summary>
Motivation: 当前向量数据库主要使用基于图的HNSW算法，但存在内存设计、随机内存访问导致缓存性能下降、加速范围有限以及仅支持语义相似性查询等问题。

Method: 首先将输入数据分区到包含语义相似项的块中，然后构建B+树变体在内存和磁盘上存储块，最后启用基于边和块的混合内存遍历。

Result: B+ANN在召回率和查询性能(QPS)上优于HNSW，改善了空间和时间局部性，减少了缓存未命中(相对增益19.23%)，内存消耗和基于磁盘的构建时间比DiskANN算法减少24倍。

Conclusion: B+ANN是一种有效的基于磁盘的ANN索引，不仅提升了相似性查询性能，还支持了传统相似性ANN索引不支持的相异性查询。

Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

</details>


### [4] [A Decade of Systems for Human Data Interaction](https://arxiv.org/abs/2511.15585)
*Eugene Wu,Yiru Chen,Haneen Mohammed,Zezhou Huang*

Main category: cs.DB

TL;DR: HDI系统与传统数据管理存在根本差异，需要满足基于用户体验而非查询语义的延迟、正确性和一致性需求。界面与系统紧密耦合，必须共同设计。


<details>
  <summary>Details</summary>
Motivation: 解决人机数据交互中的独特挑战，这些挑战源于用户体验需求而非传统查询语义，系统与界面的紧密耦合为研究提供了新机会。

Method: 通过系统创新和数据库理论启发新的交互和可视化设计，采用系统与界面共同设计的方法论。

Result: 展示了十年研究成果，证明HDI系统是可靠、交互式、AI驱动应用的基础。

Conclusion: HDI系统需要系统与界面的紧密协同设计，这种耦合关系为数据管理研究开辟了新方向，是未来交互式AI应用的关键基础。

Abstract: Human-data interaction (HDI) presents fundamentally different challenges from traditional data management. HDI systems must meet latency, correctness, and consistency needs that stem from usability rather than query semantics; failing to meet these expectations breaks the user experience. Moreover, interfaces and systems are tightly coupled; neither can easily be optimized in isolation, and effective solutions demand their co-design. This dependence also presents a research opportunity: rather than adapt systems to interface demands, systems innovations and database theory can also inspire new interaction and visualization designs. We survey a decade of our lab's work that embraces this coupling and argue that HDI systems are the foundation for reliable, interactive, AI-driven applications.

</details>


### [5] [Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs](https://arxiv.org/abs/2511.15623)
*Leopoldo Bertossi,Nina Pardal*

Main category: cs.DB

TL;DR: 本文探讨了充分解释概念在关系数据库中的应用，及其与数据库修复和因果解释的联系。


<details>
  <summary>Details</summary>
Motivation: 将Halpern和 Pearl的形式化因果概念应用于关系数据库，以表征和计算查询答案的因果解释。

Method: 研究充分解释与数据库修复以及基于因果的必要解释之间的联系。

Result: 获得了一些计算方面的结果。

Conclusion: 充分解释为数据库查询答案提供了另一种解释框架，并与现有方法有密切联系。

Abstract: The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)
*Mingkun Yu,Heming Zhong,Dan Huang,Yutong Lu,Jiazhi Jiang*

Main category: cs.DC

TL;DR: PolyKAN是一个GPU加速的Kolmogorov-Arnold网络算子库，通过四种优化技术实现了比现有实现快1.2-12倍的推理和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有的KAN并行实现GPU利用率低，阻碍了KAN在实际应用中的采用，特别是在AI for Science领域。

Method: 设计了四种正交优化技术：查找表线性插值、2D分块、两阶段归约和系数布局重排，将多项式KAN层的前向和反向传播融合为优化的CUDA内核。

Result: 在高端和消费级GPU上，使用Chebyshev KAN变体在语音、音频增强和表格回归任务上实现了1.2-10倍推理加速和1.4-12倍训练加速，且精度相同。

Conclusion: PolyKAN是第一个通用的开源KAN实现，显著提升了KAN的GPU计算效率，为KAN的实际应用铺平了道路。

Abstract: Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.

</details>


### [7] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了一个名为RemoteOptiGraph的分布式优化建模抽象，扩展了Plasmo.jl中的OptiGraph模型，用于在分布式内存环境中构建和解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存系统提供统一的优化问题建模方法，避免定制化建模方法，并为开发利用分布式内存结构的通用元算法提供基础。

Method: 基于OptiGraph超图模型，通过InterWorkerEdges管理跨工作节点的链接约束，实现在分布式内存环境中的优化问题建模。

Result: 在解决美国西部混合整数容量扩展模型（超过1200万个变量和约束）时，RemoteOptiGraph结合Benders分解比无分解方法快7.5倍。

Conclusion: RemoteOptiGraph提供了一个通用且灵活的分布式优化建模抽象，能够有效处理大规模优化问题，显著提高求解效率。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [8] [GPU-Initiated Networking for NCCL](https://arxiv.org/abs/2511.15076)
*Khaled Hamidouche,John Bachan,Pak Markthub,Peter-Jan Gootzen,Elena Agostini,Sylvain Jeaugey,Aamir Shafi,Georgios Theodorakis,Manjunath Gorentla Venkata*

Main category: cs.DC

TL;DR: NCCL 2.28引入了设备API，支持设备端发起的GPU间通信，特别针对MoE架构的低延迟通信需求。GIN架构提供了三层设计，支持直接GPU到NIC通信和代理模式，显著提升了通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统GPU通信由CPU协调，存在协调开销。现代AI工作负载特别是MoE架构需要低延迟、细粒度的GPU间通信，需要设备端发起的通信来消除CPU协调开销。

Method: GIN采用三层架构：1) NCCL核心主机端API用于设备通信器设置和集体内存窗口注册；2) 设备端API可从CUDA内核调用远程内存操作；3) 网络插件架构支持GPUDirect异步内核发起和代理两种语义。

Result: 通过集成DeepEP MoE通信库进行综合基准测试，GIN在NCCL统一运行时中提供了设备发起的通信，结合了低延迟操作与NCCL的集体算法和生产基础设施。

Conclusion: GIN架构成功实现了设备端发起的GPU间通信，为现代AI工作负载特别是MoE架构提供了低延迟、高性能的通信解决方案。

Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

</details>


### [9] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层实现高性能共识，BB-Guard层提供安全保障和恢复机制，在保持强安全性和活跃性的同时实现亚秒级最终性。


<details>
  <summary>Details</summary>
Motivation: 区块链共识面临安全性、延迟和去中心化的三难问题，高性能系统往往需要在去中心化或对抗强敌能力上做出妥协，而高度去中心化和安全的系统通常性能较低。

Method: 采用双层架构：BB-Core层使用n=5f+1协议，牺牲部分容错性以获得更低的最终性延迟；BB-Guard层提供去中心化时间戳、主动错误行为检测和同步恢复路径。

Result: 实验显示BB-Core相比Mysticeti减少延迟20-25%，在温和同步假设下实现亚秒级最终性和高吞吐量，同时保持强安全性和活跃性。

Conclusion: BlueBottle的双层架构成功解决了区块链共识的三难问题，在保持去中心化和安全性的同时实现了高性能共识。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


### [10] [Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies](https://arxiv.org/abs/2511.15388)
*Lucianna Kiffer,Lioba Heimbach,Dennis Trautwein,Yann Vonlanthen,Oliver Gasser*

Main category: cs.DC

TL;DR: 对36个公共区块链网络进行首次纵向跨网络测量研究，揭示网络规模从不足10个到超过1万个活跃节点的巨大差异，分析网络弹性、去中心化和可观测性的关键差异。


<details>
  <summary>Details</summary>
Motivation: 区块链的去中心化应用、金融系统和基础设施依赖P2P网络层，但除少数顶级生态系统外，大多数区块链的P2P网络层仍不透明，需要系统性的测量研究。

Method: 部署15个主动爬虫，结合社区爬虫数据，进行9个月的纵向测量；利用以太坊发现协议推断19个辅助网络；开发基于端口扫描的通用方法，无需为每个区块链实现定制发现逻辑。

Result: 发现网络规模差异巨大（10-10,000+节点），量化IPv4/IPv6使用趋势，分析自治系统和地理集中度，表征节点流失、昼夜行为和发现协议的覆盖范围与冗余。

Conclusion: 研究结果揭示了区块链网络在弹性、去中心化和可观测性方面的关键差异，提出的方法论为大规模测量去中心化网络提供了通用框架，支持持续监控和基准测试。

Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.

</details>


### [11] [When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit](https://arxiv.org/abs/2511.15421)
*Ethan Hicks,Joseph Oglio,Mikhail Nesterenko,Gokarna Sharma*

Main category: cs.DC

TL;DR: 分析比特币交易确认最终性与交易金额和用户风险承受能力的关系，提出基于区块深度和网络延迟的确认概率模型。


<details>
  <summary>Details</summary>
Motivation: 传统比特币交易确认采用固定区块深度（如6个区块），但这种方法没有考虑交易金额大小和用户风险偏好的差异，需要更精细化的确认策略。

Method: 通过模拟不同网络延迟下的区块链分叉情况，结合实际比特币数据，建立区块深度与确认撤销概率的关系模型，并运用前景理论将确认概率与交易金额和用户风险承受能力关联。

Result: 建立了区块深度与交易确认撤销概率的定量关系，发现确认概率随区块深度增加而提高但永远不会达到100%，为不同金额和风险偏好的交易提供了个性化的确认策略。

Conclusion: 比特币交易确认应该根据交易金额和用户风险承受能力动态调整确认深度，而不是采用固定的区块数量，这能更好地平衡安全性和效率。

Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.

</details>


### [12] [Proving there is a leader without naming it](https://arxiv.org/abs/2511.15491)
*Laurent Feuilloley,Josef Erik Sedláček,Martin Slávik*

Main category: cs.DC

TL;DR: 该论文研究在特定图类中实现亚对数位数的领导者唯一性本地认证，探索在无环图的拓扑结构中是否仍需要节点标识符。


<details>
  <summary>Details</summary>
Motivation: 传统领导者唯一性认证需要O(log n)位来编码节点标识符和距离信息，但在某些图类中可能不需要标识符。研究网络结构如何影响本地认证的复杂度，特别是在不含环图的拓扑中能否实现亚对数位认证。

Method: 分析不同图类的认证复杂度，包括小直径图、弦图、网格图和稠密图，研究在这些拓扑结构中是否仍需节点标识符来实现领导者唯一性认证。

Result: 论文展示了在特定图类中可以实现亚对数位数的领导者认证，并探讨了在无环图拓扑中节点标识符的必要性问题。

Conclusion: 网络结构对本地认证复杂度有显著影响，在某些不含环图的图类中确实可以实现亚对数位认证，且可能不需要节点标识符。

Abstract: Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.
  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).
  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Exact Learning of Weighted Graphs Using Composite Queries](https://arxiv.org/abs/2511.14882)
*Michael T. Goodrich,Songyu Liu,Ioannis Panageas*

Main category: cs.DS

TL;DR: 研究加权图的精确学习问题，通过组合查询以亚二次查询次数重建图结构


<details>
  <summary>Details</summary>
Motivation: 简单的最短路径长度查询不足以重建加权图，需要更有效的查询策略

Method: 使用组合查询（结合两个或三个简单查询）来学习图的边和权重

Result: 在多种场景下能够以亚二次查询次数成功学习加权图

Conclusion: 组合查询方法能够有效解决加权图重建问题

Abstract: In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.

</details>


### [14] [Intermediate N-Gramming: Deterministic and Fast N-Grams For Large N and Large Datasets](https://arxiv.org/abs/2511.14955)
*Ryan R. Curtin,Fred Lu,Edward Raff,Priyanka Ranade*

Main category: cs.DS

TL;DR: 提出了一种名为Intergrams的多遍算法，能够准确、确定性地快速恢复前k个最频繁的n-gram，相比现有最快算法实现了33倍的加速。


<details>
  <summary>Details</summary>
Motivation: n-gram特征数量随n呈指数增长，即使对于n=3的情况，计算最频繁n-gram的计算成本也很高。基于生产机器学习系统对n-gram特征的需求，需要找到高效计算前k个最频繁n-gram的方法。

Method: 设计了基于硬件优化的多遍算法Intergrams，通过从前一个(n-1)-gram构建候选n-gram，利用n-gram的经验幂律分布特性来提高效率。

Result: 相比已知最快算法实现了超过一个数量级的加速（最高达33倍），即使对竞争算法应用类似优化后仍然保持优势。

Conclusion: Intergrams算法能够高效、准确地恢复前k个最频繁n-gram，其多遍方法在理论分析和实际应用中均表现出色，代码已开源。

Abstract: The number of n-gram features grows exponentially in n, making it computationally demanding to compute the most frequent n-grams even for n as small as 3. Motivated by our production machine learning system built on n-gram features, we ask: is it possible to accurately, deterministically, and quickly recover the top-k most frequent n-grams? We devise a multi-pass algorithm called Intergrams that constructs candidate n-grams from the preceding (n - 1)-grams. By designing this algorithm with hardware in mind, our approach yields more than an order of magnitude speedup (up to 33x!) over the next known fastest algorithm, even when similar optimizations are applied to the other algorithm. Using the empirical power-law distribution over n-grams, we also provide theory to inform the efficacy of our multi-pass approach. Our code is available at https://github.com/rcurtin/Intergrams.

</details>


### [15] [A Dichotomy for 1-Planarity with Restricted Crossing Types Parameterized by Treewidth](https://arxiv.org/abs/2511.14975)
*Sergio Cabello,Alexander Dobler,Gašper Fijavž,Thekla Hamm,Mirko H. Wagner*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A drawing of a graph is 1-planar if each edge participates in at most one crossing and adjacent edges do not cross. Up to symmetry, each crossing in a 1-planar drawing belongs to one out of six possible crossing types, where a type characterizes the subgraph induced by the four vertices of the crossing edges. Each of the 63 possible nonempty subsets $\mathcal{S}$ of crossing types gives a recognition problem: does a given graph admit an $\mathcal{S}$-restricted drawing, that is, a 1-planar drawing where the crossing type of each crossing is in $\mathcal{S}$?
  We show that there is a set $\mathcal{S}_{\rm bad}$ with three crossing types and the following properties: If $\mathcal{S}$ contains no crossing type from $\mathcal{S}_{\rm bad}$, then the recognition of graphs that admit an $\mathcal{S}$-restricted drawing is fixed-parameter tractable with respect to the treewidth of the input graph. If $\mathcal{S}$ contains any crossing type from $\mathcal{S}_{\rm bad}$, then it is NP-hard to decide whether a graph has an $\mathcal{S}$-restricted drawing, even when considering graphs of constant pathwidth.
  We also extend this characterization of crossing types to 1-planar straight-line drawings and show the same complexity behaviour parameterized by treewidth.

</details>


### [16] [Combinatorial Optimization using Comparison Oracles](https://arxiv.org/abs/2511.15142)
*Vincent Cohen-Addad,Tommaso d'Orsi,Anupam Gupta,Guru Guruganesh,Euiwoong Lee,Debmalya Panigrahi,Madhusudhan Reddy Pittu,Jon Schneider,David P. Woodruff*

Main category: cs.DS

TL;DR: 本文研究了在线性组合优化问题中使用比较查询模型（仅能比较两个可行集的权重大小）的查询复杂度和算法效率，提出了三个主要贡献：一般查询复杂度界限、离散权重下的改进算法以及经典组合问题的高效算法。


<details>
  <summary>Details</summary>
Motivation: 传统优化问题中权重向量已知或可通过值查询获得，但实际应用中往往只能进行成对比较。本文研究更弱但更鲁棒的比较查询模型，探索在这种模型下如何用较少查询找到最优解以及能否高效实现。

Method: 使用推理维度框架分析查询复杂度；提出全局子空间学习(GSL)框架处理离散整数权重；针对线性拟阵等特殊结构使用代数技术；为最小割、最小生成树、二分匹配、最短路径等经典问题设计多项式时间低查询算法。

Result: 证明了任意集合系统的查询复杂度为O(n²)；对于有界离散权重，将排序所有可行集的查询复杂度改进为O(nB log(nB))；为多个经典组合优化问题提供了首个多项式时间低查询算法。

Conclusion: 本文为比较查询模型提供了首个通用查询复杂度界限和高效算法，开辟了基于比较的优化研究新方向，揭示了信息复杂度和计算复杂度之间的分离现象。

Abstract: In a linear combinatorial optimization problem, we are given a family $\mathcal{F} \subseteq 2^U$ of feasible subsets of a ground set $U$ of $n$ elements, and aim to find $S^* = \arg\min_{S \in \mathcal{F}} \langle w, \mathbbm{1}_S \rangle$. Traditionally, the weight vector is given, or a value oracle allows evaluating $w(S) := \langle w, \mathbbm{1}_S \rangle$. Motivated by practical interest in pairwise comparisons, and by the theoretical quest to understand computational models, we study a weaker, more robust comparison oracle that for any $S, T \in \mathcal{F}$ reveals only whether $w(S) <, =, > w(T)$. We ask: when can we find $S^*$ using few comparison queries, and when can this be done efficiently?
  We present three contributions: (1) We establish that the query complexity over any set system $\mathcal{F} \subseteq 2^U$ is $\tilde O(n^2)$, using the inference dimension framework, highlighting a separation between information and computational complexity (runtime may still be exponential for NP-hard problems under ETH). (2) We introduce a Global Subspace Learning (GSL) framework for objective functions with discrete integer weights bounded by $B$, giving an algorithm to sort all feasible sets using $O(nB \log(nB))$ queries, improving the $\tilde O(n^2)$ bound when $B = o(n)$. For linear matroids, algebraic techniques yield efficient algorithms for problems including $k$-SUM, SUBSET-SUM, and $A{+}B$ sorting. (3) We give the first polynomial-time, low-query algorithms for classic combinatorial problems: minimum cuts, minimum weight spanning trees (and matroid bases), bipartite matching (and matroid intersection), and shortest $s$-$t$ paths.
  Our work provides the first general query complexity bounds and efficient algorithms for this model, opening new directions for comparison-based optimization.

</details>


### [17] [Dynamic Matroids: Base Packing and Covering](https://arxiv.org/abs/2511.15460)
*Tijn de Vos,Mara Grilnberger*

Main category: cs.DS

TL;DR: 本文研究动态拟阵，提出了维护基、基包装和基覆盖的高效动态算法，包括(1±ε)近似算法，查询复杂度与基包装数Φ或基覆盖数β相关。


<details>
  <summary>Details</summary>
Motivation: 拟阵是组合优化问题的核心结构，研究动态拟阵可以推广多个动态图问题，如树性和最大二分匹配。本文旨在为动态拟阵提供基础算法构建块。

Method: 通过探索基集合（树包装的推广）与基包装和基覆盖之间的关系，提供结构定理形式化这些连接，并设计确定性算法维护近似解。

Result: 获得了维护基包装数Φ的(1±ε)近似算法，每更新O(Φ·poly(log n, ε⁻¹))查询；维护基覆盖数β的(1±ε)近似算法，每更新O(β·poly(log n, ε⁻¹))查询；以及对抗遗忘对手的O(poly(log n, ε⁻¹))查询算法。

Conclusion: 通过结构连接实现了高效的动态拟阵算法，为未来更复杂的动态拟阵问题提供了基础构建块。

Abstract: In this paper, we consider dynamic matroids, where elements can be inserted to or deleted from the ground set over time. The independent sets change to reflect the current ground set. As matroids are central to the study of many combinatorial optimization problems, it is a natural next step to also consider them in a dynamic setting. The study of dynamic matroids has the potential to generalize several dynamic graph problems, including, but not limited to, arboricity and maximum bipartite matching. We contribute by providing efficient algorithms for some fundamental matroid questions.
  In particular, we study the most basic question of maintaining a base dynamically, providing an essential building block for future algorithms. We further utilize this result and consider the elementary problems of base packing and base covering. We provide a deterministic algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base packing number $Φ$ in $O(Φ\cdot \text{poly}(\log n, \varepsilon^{-1}))$ queries per update. Similarly, we provide a deterministic algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base covering number $β$ in $O(β\cdot \text{poly}(\log n, \varepsilon^{-1}))$ queries per update. Moreover, we give an algorithm that maintains a $(1\pm \varepsilon)$-approximation of the base covering number $β$ in $O(\text{poly}(\log n, \varepsilon^{-1}))$ queries per update against an oblivious adversary.
  These results are obtained by exploring the relationship between base collections, a generalization of tree-packings, and base packing and covering respectively. We provide structural theorems to formalize these connections, and show how they lead to simple dynamic algorithms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: PennyLane是一个用于混合量子-经典机器学习的Python框架，它连接量子电路和经典机器学习，支持构建、优化和部署变分量子算法，并提供与PyTorch、TensorFlow、JAX等经典ML框架的集成。


<details>
  <summary>Details</summary>
Motivation: 混合量子-经典机器学习结合了量子计算的优势和经典优化技术，PennyLane旨在为研究人员提供一个无缝桥接量子电路和经典机器学习的工具，促进量子增强数据科学的发展。

Method: PennyLane提供Python框架，支持量子核方法、变分量子本征求解器、投资组合优化等应用，通过scikit-learn、pandas、matplotlib等库的Python示例展示高效的量子电路构建、自动微分和混合优化工作流。

Result: 论文展示了PennyLane在量子机器学习、优化和量子化学应用中的多功能性，通过具体用例证明了其作为量子增强数据科学方法构建块的作用。

Conclusion: PennyLane成为基于Python研究的混合量子-经典工作流程的默认引用工具，为研究人员和实践者提供了连接基础量子计算概念和应用机器学习实践的简明参考。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [19] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 提出了一个用于区域供热站早期故障检测的开源框架，包含公开数据集、评估方法和基准结果，能够提前检测60%的故障，平均提前3.9天。


<details>
  <summary>Details</summary>
Motivation: 区域供热站故障的早期检测对于降低回水温度和提高效率至关重要，但该领域进展受到公开标记数据集有限的阻碍。

Method: 开发了EnergyFaultDetector开源Python框架，结合服务报告验证的公共数据集，使用准确性、可靠性和早期性三个指标进行评估，并支持使用ARCANA进行根本原因分析。

Result: 模型实现了高正常行为准确率（0.98）和事件F分数（0.83），在客户报告问题前检测到60%的故障，平均提前3.9天。

Conclusion: 整合开放数据集、指标、开源代码和基准建立了可复现的故障中心基准，为区域供热站的早期故障检测和诊断方法提供了一致的比较和开发基础。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [20] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 本文提出了irace-evo，一个将irace自动参数配置与LLM驱动的代码演化相结合的框架，能够在多语言环境中同时探索参数和代码空间，并在VSBPP问题上发现了优于现有CMSA实现的新算法变体。


<details>
  <summary>Details</summary>
Motivation: 传统自动算法配置工具如irace只能调优参数值而无法修改算法代码，限制了算法设计的探索空间。需要一种能够同时优化参数和代码的方法来推动启发式算法的发展。

Method: 扩展irace框架，集成LLM进行代码演化，支持多语言（C++、Python），采用渐进式上下文管理减少token消耗，并应用Always-From-Original原则确保代码演化的稳健性和可控性。

Result: 在VSBPP问题上，irace-evo发现了优于现有CMSA实现的新算法变体，且使用轻量级模型（如Claude Haiku 3.5）的总成本低于2欧元，展示了成本效益。

Conclusion: 将自动配置与LLM驱动的代码演化相结合，为启发式设计和元启发式优化提供了一个强大且成本效益高的途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [21] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 比较两种AI评分方法：直接评分和反向评分，评估它们在编程作业自动评分中的效果和一致性


<details>
  <summary>Details</summary>
Motivation: 手动评分编程作业耗时且不一致，单元测试只能提供二元结果，需要更客观、可扩展的自动评分方法

Method: 提出两种AI评分技术：直接应用评分标准到学生代码，以及先修复错误再根据修复情况推导分数，使用人类导师评分作为基准进行对比

Result: 直接方法更快更直接，反向方法能提供更细粒度的评估，两种方法都需要精心设计的提示词来处理部分分数和逻辑错误

Conclusion: 讨论了两种方法的优缺点，提出了未来混合人机评分系统的方向，以提高CS课程评分的一致性、效率和公平性

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [22] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 提出基于大语言模型的日志分析工具，能够在CPU上高效处理海量日志数据，实现自动化问题诊断和总结，显著节省时间和成本。


<details>
  <summary>Details</summary>
Motivation: IT环境中产生的大量日志数据使得人工检查变得不切实际，需要自动化日志分析工具来提高软件支持的效率。

Method: 开发基于大语言模型的日志分析工具，采用新颖方法在CPU上高效运行LLM处理海量日志数据，生成自动化洞察和总结。

Result: 自2024年3月部署以来，该工具已在70个软件产品中规模化应用，处理超过2000个工单进行问题诊断，节省300+人工小时，每月估计节省15,444美元人力成本。

Conclusion: 基于LLM的日志分析工具能够有效处理海量日志数据，显著提高IT软件支持的效率和成本效益。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [23] [Towards Continuous Assurance with Formal Verification and Assurance Cases](https://arxiv.org/abs/2511.14805)
*Dhaminda B. Abeywickrama,Michael Fisher,Frederic Wheeler,Louise Dennis*

Main category: cs.SE

TL;DR: 提出了一个统一的持续保证框架，整合设计时、运行时和演化时保证，通过模型驱动的工作流确保自主系统的可追溯性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统保证方法将开发时保证与运行时保证分离，导致碎片化的论证无法适应运行时变化或系统更新，这是自主系统保证面临的重要挑战。

Method: 使用RoboChart进行功能正确性验证和PRISM进行概率风险分析，构建模型驱动的转换管道（Eclipse插件），在形式化规范或验证结果变化时自动重新生成结构化保证论证。

Result: 在核检查机器人场景中演示了该方法，展示了框架与三边AI原则的一致性，反映了监管机构认可的最佳实践。

Conclusion: 该框架通过整合不同阶段的保证活动，为自主系统的持续保证提供了可追溯、适应性强的方法，是迈向有保证自主性的重要一步。

Abstract: Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.

</details>


### [24] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 探索自动流水线配置的好处及其应用方式


<details>
  <summary>Details</summary>
Motivation: 研究自动流水线配置在软件工程项目中的优势和应用潜力

Method: 专注于CI流水线的研究，并认为CD流水线也会有类似结果

Result: 未在摘要中明确说明具体研究结果

Conclusion: 自动流水线配置能够快速为软件工程项目部署流水线

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [25] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 提出了MermaidSeqBench基准，用于评估LLM从文本提示生成Mermaid序列图的能力，包含132个样本，采用混合方法扩展，并使用LLM作为评判模型进行多维度评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统评估LLM生成结构化图表能力的基准，特别是在软件工程中的序列图生成任务上。

Method: 采用混合方法构建基准：人工标注、上下文LLM提示和基于规则的变体生成，使用LLM作为评判模型评估语法正确性、激活处理、错误处理等指标。

Result: 对多个先进LLM的评估显示模型之间存在显著能力差距，基准提供了有效的评估框架。

Conclusion: 该基准为结构化图表生成研究奠定了基础，支持开发更严格、细粒度的评估方法。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [26] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 开发了FRIENDS GUI，这是一个基于Python的开源工具，用于提取、解码和可视化电子尼古丁传送系统(ENDS)的24小时吸烟行为数据，提高FRIENDS设备收集数据的可访问性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解吸烟行为特征(包括吸烟持续时间、吸烟间隔和每次会话吸烟次数)对于评估ENDS使用、有毒物质暴露和制定监管决策至关重要。

Method: 开发基于Python的开源FRIENDS GUI，能够提取、解码和可视化FRIENDS设备记录的24小时吸烟和触摸事件数据。

Result: 使用24小时实验数据进行验证，确认了时间戳转换的准确性、事件解码的可靠性以及行为可视化的有效性。

Conclusion: 该软件已在GitHub上免费提供供公众使用，为ENDS使用行为研究提供了可访问的数据分析工具。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [27] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: AdvPrompt-MIA是一种针对代码补全模型的成员推理攻击方法，通过结合代码特定的对抗性提示和深度学习来有效识别训练数据成员。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒和灰盒成员推理攻击依赖昂贵的代理模型或手动设计的启发式规则，难以捕捉过参数化代码语言模型的细微记忆模式。

Method: 设计一系列对抗性提示来诱导受害者代码模型输出变化，通过比较这些输出与真实补全结果构建特征向量，训练分类器自动区分成员和非成员样本。

Result: 在Code Llama 7B等模型上的评估显示，该方法在APPS和HumanEval基准上始终优于最先进基线，AUC提升高达102%，并展现出强大的跨模型和数据集可迁移性。

Conclusion: AdvPrompt-MIA能够捕捉更丰富的记忆模式，准确推断训练集成员身份，具有实际应用价值和通用性。

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [28] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练LLM生成高质量Selenium表单交互测试用例的新方法，创建了合成和人工标注数据集，在语法正确性、可执行性和输入字段覆盖率方面显著优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动化Web应用测试是现代软件开发的关键组成部分，但LLM在表单交互生成方面的能力尚未得到充分探索，且缺乏公开的基准数据集来系统评估LLM。

Method: 提出训练LLM生成Selenium测试用例的新方法，构建了包含合成和人工标注的数据集，涵盖多样化的真实世界表单和测试场景，定义了语法正确性、脚本可执行性和输入字段覆盖率的评估指标。

Result: 实证研究表明，该方法在所有评估指标上显著优于GPT-4o和其他流行LLM等强基线模型。

Conclusion: 该工作为基于LLM的Web测试未来研究奠定了基础，并为该领域的持续进展提供了资源支持。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [29] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 本研究系统识别了导致ML应用中资源泄漏的代码异味，在PyTorch中发现30种，TensorFlow/Keras中发现16种，并提出了50个最佳实践来减少资源泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有ML研究主要关注模型性能指标，对长期可持续性和资源效率关注有限。确保高效资源管理与高性能同等重要，对稳健部署至关重要。

Method: 对PyTorch、TensorFlow和Keras的开发者讨论和真实代码片段进行实证调查，采用三阶段验证流程（三位作者独立分析后达成共识）。

Result: 识别出PyTorch相关异味30种，TensorFlow/Keras异味16种，按根本原因和框架特性分类，为每种异味推导出至少一个最佳实践。

Conclusion: 这是首个全面研究主要ML框架中导致资源泄漏代码异味的研究，提供了可操作的最佳实践，帮助开发者构建更高效可持续的ML应用。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [30] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: M是一种基于参与者模型的建模语言和工具链，支持复杂并发系统的多目标编译，能够从单一源模型生成多种异构目标产物。


<details>
  <summary>Details</summary>
Motivation: 现有建模语言通常只针对仿真或实现等单一目标，缺乏对多目标编译的支持，导致在模型驱动工程中难以高效生成不同用途的专用模型。

Method: 设计了基于参与者模型并扩展离散事件调度语义的文本化、语法驱动的M语言，提供系统实体建模、消息交互以及时间或状态触发反应等构造。

Result: M能够从统一模型系统性地生成多种目标产物，同时保持与原始模型的语义一致性，并可作为中间语言为其他建模语言提供编译框架支持。

Conclusion: M语言和工具链为复杂并发系统的模型驱动工程提供了有效的多目标编译解决方案，解决了传统建模语言目标单一的问题。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [31] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: AutoSW是一个迭代的端到端自动化软件开发范式，通过分析-规划-实施-交付循环，将自然语言意图转化为可执行软件。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统在整个软件开发生命周期中作为人类合作伙伴的参与方式，推动真正端到端的自动化软件开发。

Method: 采用分析-规划-实施-交付的迭代循环，AI系统作为一等参与者，将自然语言意图翻译成可执行软件。

Result: 原型系统成功交付了可执行软件，表明AutoSW为真正端到端自动化软件开发提供了可行方向。

Conclusion: AutoSW范式展示了AI系统作为人类合作伙伴参与整个软件开发生命周期的潜力，是实现软件自动化开发的重要进展。

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [32] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了如何从ML文档（如ModelCards和DataSheets）中提取需求工程相关信息，并评估了三种需求表示方法将这些信息转化为结构化需求的有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统的软件工程过程中，集成和验证ML组件面临重大挑战，其中ML组件需求规范是一个关键但存在障碍的领域。ML文档作为需求工程相关信息的一个未充分探索的来源，其有效性尚不明确。

Method: 首先分析20个公开可用的ModelCards和DataSheets中RE相关信息的数量和性质，然后评估三种需求表示方法（EARS、Rupp模板和Volere）将这些知识转化为结构化需求的有效性。

Result: 研究表明ML文档包含大量潜在的RE相关信息，并且存在将ML特定知识转化为结构化需求的可行路径。

Conclusion: ML文档可以有效地整合到ML系统的软件工程过程中，为ML组件需求规范提供了新的途径。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [33] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: MutDafny是一个基于变异测试的工具，用于检测Dafny形式化规范中的弱点。通过向代码引入变异并依赖规范检测这些变异，如果包含变异的程序仍能验证通过，则表明规范可能存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 在验证感知编程语言如Dafny中，规范与实现一样容易出错。规范中的缺陷可能导致形式化验证的程序偏离预期行为，因此需要提高Dafny规范的可靠性。

Method: 采用变异测试方法，从流行工具中分析适用的变异算子，并从GitHub上的Dafny项目错误修复提交中合成新的算子，最终集成32个变异算子。在794个真实世界Dafny程序数据集上评估工具效果。

Result: 在手动分析未检测变异体子集时，识别出5个弱真实世界规范（平均每241行代码一个），这些规范需要加强。

Conclusion: MutDafny能有效识别Dafny规范中的潜在弱点，提高形式化验证的可靠性。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [34] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: EPSO是一种基于缓存的eBPF超级优化器，通过离线超级优化发现重写规则并重用，在最小运行时开销下实现高质量优化


<details>
  <summary>Details</summary>
Motivation: eBPF程序受限于内核验证器的严格安全约束，现有编译器优化支持有限，手工优化规则设计困难且效果有限，而超级优化计算成本高难以扩展

Method: 提出EPSO方法，通过离线超级优化发现重写规则并缓存重用，结合运行时最小开销实现高效优化

Result: EPSO发现795条重写规则，相比Clang输出程序大小最多减少68.87%（平均24.37%），在所有基准测试中优于K2，在92.68%测试中优于Merlin，平均运行时减少6.60%

Conclusion: EPSO通过缓存重写规则有效解决了eBPF超级优化的可扩展性问题，显著提升了程序大小和运行时性能

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


### [35] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: 提出了基于测试驱动开发(TDD)的框架，将代码规范转化为组合优化问题，使用量子退火解决测试用例最小化问题，比模拟退火快16倍，减少36.5%的token消耗并显著提升代码质量。


<details>
  <summary>Details</summary>
Motivation: 精确控制大型语言模型生成高效简洁代码是软件工程中的核心挑战，需要将代码规范转化为可优化的组合问题。

Method: 基于TDD的框架：首先生成测试套件，然后将测试用例最小化问题建模为QUBO模型，兼容经典求解器和量子退火器等新兴硬件。

Result: 量子退火解决核心TCM任务比模拟退火快16倍，端到端框架减少36.5%的token消耗并显著提高代码质量。

Conclusion: 展示了生成式AI与组合优化在软件工程中的强大协同作用，强调了精确模型制定的重要性。

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [36] [RAID: In-Network RA Signaling Storm Detection for 5G Open RAN](https://arxiv.org/abs/2511.14921)
*Mohamed Rouili,Yang Xiao,Sihang Liu,Raouf Boutaba*

Main category: cs.NI

TL;DR: RAID是一个基于P4可编程交换机的实时随机接入信令风暴检测和缓解系统，通过在数据平面嵌入轻量级随机森林分类器，实现微秒级延迟的恶意RA请求检测和过滤。


<details>
  <summary>Details</summary>
Motivation: 5G O-RAN的分解和虚拟化在控制平面引入了新的漏洞，特别是随机接入信令风暴攻击，恶意UE发送大量RRC连接请求会快速饱和CU处理管道，导致连接失败。现有基于n-RT RIC的方法无法保证及时响应。

Method: 在可编程Tofino交换机中嵌入轻量级随机森林分类器，利用P4可编程交换ASIC实现线速流分类，在数据平面直接执行基于机器学习的检测。

Result: RAID实现了94%以上的检测准确率，固定每流推理延迟约为3.4微秒，有效满足严格的O-RAN控制平面时限要求，在不同流量负载下保持稳定性能。

Conclusion: RAID是一个快速且可扩展的解决方案，能够有效检测和缓解5G O-RAN中的信令风暴攻击，通过在数据平面执行实时机器学习检测来保护控制平面。

Abstract: The disaggregation and virtualization of 5G Open RAN (O-RAN) introduces new vulnerabilities in the control plane that can greatly impact the quality of service (QoS) of latency-sensitive 5G applications and services. One critical issue is Random Access (RA) signaling storms where, a burst of illegitimate or misbehaving user equipments (UEs) send Radio Resource Control (RRC) connection requests that rapidly saturate a Central Unit's (CU) processing pipeline. Such storms trigger widespread connection failures within the short contention resolution window defined by 3GPP. Existing detection and mitigation approaches based on near-real-time RAN Intelligent Controller (n-RT RIC) applications cannot guarantee a timely reaction to such attacks as RIC control loops incur tens to hundreds of milliseconds of latency due to the non-deterministic nature of their general purpose processor (GPP) based architectures. This paper presents RAID, an in-network RA signaling storm detection and mitigation system that leverages P4-programmable switch ASICs to enable real-time protection from malicious attacks. RAID embeds a lightweight Random Forest (RF) classifier into a programmable Tofino switch, enabling line-rate flow classification with deterministic microsecond-scale inference delay. By performing ML-based detection directly in the data plane, RAID catches and filters malicious RA requests before they reach and overwhelm the RRC. RAID achieves above 94% detection accuracy with a fixed per-flow inference delay on the order of 3.4 microseconds, effectively meeting strict O-RAN control-plane deadlines. These improvements are sustained across multiple traffic loads, making RAID a fast and scalable solution for the detection and mitigation of signaling storms in 5G O-RAN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 本文研究了仅解码器Transformer中从离散提示到最后一词隐藏状态的映射的注入性，证明了在温和条件下该映射在参数空间中通常是单射的，并通过几何诊断方法在预训练模型中验证了这一性质。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer表示的单射性对于理解模型如何区分不同输入以及表示空间的结构具有重要意义，特别是在考虑参数变化和量化影响时。

Method: 定义了碰撞判别式和单射层，证明了参数空间中的二分法性质；开发了分离边际和共Lipschitz常数等几何诊断方法，并在LLaMA-3、Qwen和GPT-2模型上进行了实证研究。

Result: 理论证明在温和条件下Transformer表示在参数空间中通常是单射的；实证研究显示在完整精度和8位量化下未发现碰撞，4位量化会引入少量碰撞并显著降低共Lipschitz估计。

Conclusion: Transformer表示在连续参数理想化下通常是单射且持久的，其实际可逆性可以通过简单的几何诊断方法进行探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [38] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT是一个统一的网络层析框架，通过将不同网络指标建模为共享潜在网络状态的投影，实现多任务学习，提升跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络层析方法通常单独解决不同问题，依赖有限的任务特定信号，限制了泛化性和可解释性。

Method: 基于柏拉图表示假设，通过多模态对齐和对比学习学习共享潜在网络状态，在共享潜在空间中训练多个层析任务。

Result: 在合成和真实数据集上的实验表明，PLATONT在链路估计、拓扑推断和流量预测方面始终优于现有方法，具有更高的准确性和更强的鲁棒性。

Conclusion: PLATONT通过构建紧凑且结构化的表示，在不同网络条件下实现了更好的网络状态推断性能。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [39] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系(DR)和推导能力(DC)的概念，用于评估LLMs在数据变化时的推理能力，并开发了DEVAL评估框架和推导提示(DP)方法来提升LLMs的推导能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数据上的推理能力是一个重要但未充分研究的问题。与人类相比，LLMs缺乏基于抽象规则对输入变化进行相应输出修改的推理模式。

Method: 提出了推导关系(DR)和推导能力(DC)的形式化定义，构建了DEVAL评估框架，并开发了推导提示(DP)这种新的提示工程方法。

Result: 评估显示主流LLMs（如GPT-4o和Claude3.5）具有适中的DR识别能力，但在问题解决场景中应用DR的能力显著下降。DP方法使所有测试LLMs的DC平均提升了15.2%。

Conclusion: LLMs在推导能力方面存在不足，但通过推导提示方法可以显著改善其性能，这为提升LLMs的推理能力提供了有效途径。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [40] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出了动态嵌套层次结构，使模型能够自主调整优化层级、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在静态任务中表现出色，但在非平稳环境中由于刚性架构而表现不佳，无法持续适应和终身学习。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，允许模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，受神经可塑性启发实现无预定义约束的自我进化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界和不同机制下的次线性遗憾分析，以及在语言建模、持续学习和长上下文推理中的实证演示，证明了动态嵌套层次结构的优越性能。

Conclusion: 动态嵌套层次结构为自适应通用智能建立了基础性进展，通过动态压缩上下文流和适应分布变化，解决了现有模型的顺行性遗忘问题。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [41] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了GTPO算法，专门用于训练LLMs在多轮工具集成推理任务中，通过细粒度的轮次级奖励、基于回报的优势估计和自监督奖励塑造来解决现有RL方法训练停滞的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中存在粗粒度的轨迹级奖励问题，导致学习信号不足和训练停滞。

Method: GTPO算法包含三个关键创新：轮次级奖励分配、基于回报的优势估计和自监督奖励塑造。

Result: 在多样化推理基准测试中，GTPO平均比GRPO表现提升3.0%。

Conclusion: GTPO算法有效推进了复杂数学推理在现实世界中的应用。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [42] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务中长序列交互和多产品协调的挑战，相比传统树模型在性能和效率上都有显著提升


<details>
  <summary>Details</summary>
Motivation: 金融服务中的推荐系统面临独特挑战：长序列用户交互（数字和物理渠道）、多产品协调需求、业务目标平衡，而传统树模型虽然可解释但性能有限

Method: 提出FinTRec框架，基于Transformer架构，支持统一建模和跨产品信号共享，通过产品适配微调实现多产品推荐

Result: 通过历史模拟和在线A/B测试，FinTRec持续优于生产级树模型基线，统一架构减少训练成本和技术债务，提升所有产品的离线性能

Conclusion: FinTRec证明了Transformer架构在金融服务推荐中的可行性，是首个全面解决技术和业务考量的统一序列推荐模型研究

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [43] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出基于Transformer引导的深度强化学习方法，用于优化电动垂直起降飞机的起飞轨迹，相比传统DRL训练效率提升75%，能耗优化准确率达到97.2%。


<details>
  <summary>Details</summary>
Motivation: 解决传统最优控制方法在复杂非线性系统中的维度限制问题，以及深度强化学习方法训练困难的关键瓶颈。

Method: 使用Transformer在每个时间步探索现实状态空间，引导DRL训练过程，应用于eVTOL无人机的最优起飞轨迹设计。

Result: Transformer引导的DRL仅需457万时间步完成训练，是传统DRL所需1979万时间步的25%；在能耗优化方面达到97.2%的准确率，优于传统DRL的96.3%。

Conclusion: Transformer引导的DRL在训练效率和最优设计验证方面均优于传统DRL方法，为复杂非线性系统的最优控制提供了有效解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [44] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度和轮数复杂度之间的权衡关系，在实况和不可知两种设置下给出了最优或近似最优的算法，并提出了新的理论框架OODS来抽象化这种权衡。


<details>
  <summary>Details</summary>
Motivation: 研究多分布学习中样本复杂度和轮数复杂度之间的基本权衡关系，探索如何在有限的采样轮次内实现最优的样本效率。

Method: 提出了Optimization via On-Demand Sampling (OODS)框架来抽象化样本自适应权衡，在实况设置下分析r轮算法的样本复杂度，在不可知设置下设计了轮数复杂度为Õ(√k)的算法。

Result: 实况设置下，r轮算法的最优样本复杂度约为dk^Θ(1/r)/ε；不可知设置下，实现了样本复杂度为Õ((d+k)/ε²)且轮数复杂度为Õ(√k)的算法。

Conclusion: 建立了OODS框架下的轮数复杂度紧界，Õ(√k)轮算法对不可知MDL是最优的，要获得次多项式轮数复杂度需要绕过OODS固有难度的新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [45] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 该论文首次系统分析了在卫星星座中部署联邦学习的可行性，提出了"空间化"框架来适配轨道约束，通过768种星座配置的测试表明空间化FL算法可扩展到100颗卫星，性能接近集中式理想情况，训练周期可从数月缩短到数天。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座扩展到数千颗卫星，下行带宽限制使得分布式星上机器学习变得至关重要。联邦学习为卫星网络的协作模型训练提供了有前景的框架，但需要解决空间特定的约束条件。

Method: 提出了"空间化"框架，将地面算法（FedAvg、FedProx、FedBuff）适配到轨道约束下，创建轨道就绪的FL算法套件。通过768种星座配置的参数扫描进行评估，变化集群大小、每集群卫星数和地面站网络。

Result: 空间适配的FL算法可高效扩展到100颗卫星星座，性能接近集中式理想情况。多月的训练周期可缩短到数天，通过轨道调度和卫星集群内本地协调实现9倍加速。

Conclusion: 研究结果为未来任务设计者提供了可行的见解，使分布式星上学习能够实现更自主、弹性和数据驱动的卫星操作。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [46] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLMs使用外部可靠工具来解决实际问题，提高解决方案的可信度和可调试性


<details>
  <summary>Details</summary>
Motivation: LLMs推理过程不透明，在高风险领域难以信任，且可能选择不可靠的解决方案，即使有更好的选择可用

Method: 基于现有LLMs的工具调用能力构建框架，让LLMs选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用

Result: 创建了包含1,300个问题的新基准数据集和可定制的可靠性成本函数，LLMs使用该框架能实现更可靠和明智的问题解决，同时保持任务性能

Conclusion: LIT框架使LLMs能够通过利用外部工具实现更可靠的问题解决，同时保持解决方案的可检查性和可调试性

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [47] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出结构化对比学习(SCL)框架，将潜在空间划分为不变特征、变异特征和自由特征三个语义组，解决神经网络对语义无关变换的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关变换（如ECG相位偏移、IMU传感器旋转）表现出严重脆弱性，导致性能显著下降，根源在于"放任自由"的表示学习。

Method: 使用结构化对比学习框架，通过可控的推拉动态机制，让不同潜在维度服务于不同的可解释目的，无需架构修改即可集成到现有训练流程中。

Result: 在ECG相位不变性和IMU旋转鲁棒性实验中表现优异：ECG相似度从0.25提升到0.91，WISDM活动识别准确率达到86.65%，旋转一致性达到95.38%。

Conclusion: 这项工作代表了从反应性数据增强到主动性结构学习的范式转变，实现了神经网络中可解释的潜在表示。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [48] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: Causal-GCN是一个基于干预的图卷积框架，通过do-calculus后门调整识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，在保持与基线GNN相当性能的同时提供可解释的因果效应排序。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图学习模型大多保持相关性，混淆了人口统计学和遗传因素与疾病特异性特征，需要开发能够识别稳定因果影响的框架。

Method: 将每个受试者的MRI表示为结构连接组，节点表示皮层和皮层下区域，边编码解剖连接性。通过主成分分析总结年龄、性别和APOE4基因型等混杂因素，并包含在因果调整集中。训练后通过切断传入边和改变节点特征来模拟对单个区域的干预。

Result: 在ADNI队列的484名受试者中，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排序，突出了后部、扣带回和岛叶枢纽，与已建立的AD神经病理学一致。

Conclusion: Causal-GCN框架成功识别了对AD进展具有因果影响的大脑区域，为理解疾病机制提供了可解释的因果见解。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [49] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 比较了四种隐私保护训练方法在临床诊断编码任务中的表现，发现在中等隐私预算下，基于DP训练教师模型的知识蒸馏方法效果最佳，能恢复63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本训练中存在隐私泄露风险，但差分隐私方法往往会严重降低诊断准确性，需要找到有效的隐私保护策略。

Method: 系统比较了四种训练流程：直接DP-SGD、DP合成数据训练、知识蒸馏等，使用相同的1B参数模型和匹配的隐私预算来预测ICD-9编码。

Result: 在中等和宽松隐私预算下，基于DP训练教师模型的知识蒸馏方法表现最佳，能恢复63%的非私有性能，同时保持强大的经验隐私保护。

Conclusion: 知识蒸馏是实现隐私保护临床NLP的最实用途径，不同架构在隐私-效用权衡上存在显著差异。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [50] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，将嵌入空间总结为带有可靠性指标的原型节点，并通过编码几何和上下文关系的边连接。


<details>
  <summary>Details</summary>
Motivation: 解决传统非参数学习方法在孤立处理训练实例时缺乏全局一致性和可靠性建模的问题，旨在在局部证据和全局一致性之间建立原则性桥梁。

Method: 设计原型节点（带可靠性指标）和关系边（编码几何和上下文关系），统一实例检索、原型推理和图标签传播，支持高效推理和可信解释。

Result: 在合成和真实数据集（包括乳腺癌组织病理学IDC）上的实验表明，GM在准确率上与kNN和标签传播方法相当，但校准性更好、决策边界更平滑，且样本数量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习提供了连接局部证据和全局一致性的原则性方法，在保持竞争力的准确率的同时显著提升了校准性和效率。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [51] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast是一个基于深度学习的电离层预测模型套件，专门针对电离层动力学设计，能够预测全球总电子含量(TEC)，在风暴期和宁静条件下相比持续性预测展现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS精度、高频通信和航空运行至关重要，准确预测和建模电离层变异性变得越来越重要，但目前存在预测能力不足的问题。

Method: 采用GraphCast启发的模型，结合时空学习，整合多种物理驱动因素和观测数据集，通过可扩展的基于图的时空学习统一异构数据。

Result: 在保留的风暴期和宁静条件测试中，相比持续性预测显示出改进的技能表现。

Conclusion: IonCast展示了机器学习如何增强对电离层变异性的物理理解，并推进操作性的空间天气弹性能力。

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [52] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 开发了一个模拟课堂环境的动态时间序列系统，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计的准确性和教学干扰成本。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统需要处理每个学生的独特性，而学习过程本质上是部分可观测的，因此需要开发能够结合个体状态学习和群体信息的系统。

Method: 创建动态时间序列模拟环境，设计探测性干预机制，开发结合强化学习和基于规则的启发式方法的智能辅导系统。

Result: RL算法和启发式方法提供不同解决方案但效果相似；探测性干预能显著提升性能；RL策略在困难班级中表现不佳；测验和期中考试结构比期末考试结构获益更多。

Conclusion: 探测性干预能有效平衡信息获取和教学干扰，智能辅导系统在具有更多评估机会的课程结构中表现更好，但需要进一步改进以应对困难学生群体。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [53] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 使用电子健康记录和人工神经网络预测COVID-19患者住院时长，通过贝叶斯优化调整超参数解决类别不平衡问题


<details>
  <summary>Details</summary>
Motivation: COVID-19症状严重程度差异大，高风险患者住院时间长甚至死亡，需要预测疾病严重程度以优化医疗资源分配

Method: 使用电子健康记录数据，通过过采样处理类别不平衡问题，采用人工神经网络模型，使用贝叶斯优化进行超参数调优

Result: 选择F1分数最高的模型进行评估和讨论

Conclusion: 该方法能够有效预测COVID-19患者的住院时长，为疾病严重程度评估提供支持

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [54] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出一个用于心律失常检测和分类的框架，通过局部和全局信息提取与融合，解决心律失常长度变化的问题，在MITDB和AFDB数据库上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的临床决策支持系统在心律失常分类任务中面临挑战，因为心律失常的长度各不相同，且发作时间变化，但先前的方法未充分考虑这些条件。

Method: 提出包含局部和全局提取以及局部-全局信息融合的框架，使用注意力机制在受限输入长度内实现心律失常检测和分类。

Result: 在MITDB和AFDB数据库上，持续时间、发作和Dice分数的F1得分分别为96.45%、82.05%、96.31%和97.57%、98.31%、97.45%，性能优于基准模型。

Conclusion: 该方法能有效捕获局部和全局信息及动态变化，无显著信息损失，可更准确地检测心律失常并精确确定其发生时间，有助于制定更准确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [55] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌精准预后预测，并通过多组学分析揭示其分子机制。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统无法满足个性化医疗需求，需要开发更精准的预后分层工具。

Method: 使用TCGA队列（n=581）训练TDAM-CRC模型，在独立外部队列（n=1031）验证，并整合多组学数据提高模型可解释性和识别预后生物标志物。

Result: TDAM-CRC在两个队列中均实现稳健风险分层，预测性能显著优于传统临床分期系统和现有最优模型。多组学分析显示高风险亚型与代谢重编程和免疫抑制肿瘤微环境相关，发现MRPL37作为关键枢纽基因。

Conclusion: TDAM-CRC为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [56] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 提出Fourier-KAN-Mamba混合架构，结合傅里叶层、KAN网络和Mamba状态空间模型，用于时间序列异常检测，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mamba状态空间模型在长序列建模中表现出色，但直接应用于异常检测任务时在捕捉复杂时间模式和非线性动态方面仍面临挑战。

Method: 集成傅里叶层提取多尺度频率特征，KAN增强非线性表示能力，并引入时间门控控制机制来更好地区分正常和异常模式。

Result: 在MSL、SMAP和SWaT数据集上的大量实验表明，该方法显著优于现有的最先进方法。

Conclusion: Fourier-KAN-Mamba混合架构有效解决了时间序列异常检测中的复杂模式捕捉问题，为工业监测和故障诊断等应用提供了更优的解决方案。

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [57] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 该研究将深度学习方法与情感分析整合到传统商业模式分析中，以台积电为研究对象预测台湾半导体行业趋势。使用情感增强的时间序列数据和LSTM模型进行预测，结果准确反映了台积电晶圆技术发展和全球市场潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 半导体行业市场变化快速且晶圆技术不断发展，传统数据分析方法在处理高变化性和时间序列数据时表现不佳。需要结合文本情感分析和深度学习来提升行业趋势预测的准确性。

Method: 收集台积电季度报告中的文本数据和时间序列数据（包括财务信息）。通过情感分析考虑公司内部事件和外部全球事件的干预，使用情感增强的时间序列数据，采用LSTM模型进行行业趋势预测。

Result: 预测结果揭示了台积电晶圆技术的显著发展以及全球市场的潜在威胁，与台积电产品发布新闻和国际新闻相符。

Conclusion: 该工作通过同时考虑内部和外部事件干预，在半导体行业趋势预测方面取得了准确结果，为研究和商业领域提供了有价值的半导体行业信息。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [58] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯在线学习的不确定性感知框架，用于高效参数化建模RF无源元件，相比传统基于机器学习的流程仅需2.86%的电磁仿真时间，实现35倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的RF无源元件建模需要大量电磁仿真来覆盖几何和频率设计空间，存在计算瓶颈问题。

Method: 1) 具有可重构头部的贝叶斯神经网络，用于联合几何-频率域建模并量化不确定性；2) 基于不确定性指导的自适应采样策略，同时优化几何参数和频率域的训练数据采样。

Result: 在三个RF无源组件上验证，该框架实现了精确建模，同时仅使用传统基于机器学习流程2.86%的电磁仿真时间，达到35倍加速。

Conclusion: 该不确定性感知贝叶斯在线学习框架显著提高了RF无源元件参数化建模的效率，大幅减少了计算成本。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [59] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 开发了一种新的稀疏矩阵乘法算法，使整个Medline数据库的自组织映射成为可能，解决了现有算法内存和处理需求指数增长的问题。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理整个Medline数据库时面临内存和处理需求指数增长的挑战，限制了只能处理数据子集。

Method: 设计了一种新颖的稀疏矩阵乘法算法，应用于自组织映射方法。

Result: 成功实现了对整个Medline数据集的映射，并提高了随时间更新映射的可行性。

Conclusion: 新算法使得更完整的医学知识图谱构建成为可能，并为随时间变化的数据库更新提供了更可行的解决方案。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [60] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，通过可调超参数控制验证信号权重，提升LLMs的自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了大型语言模型的推理能力，但它们仍难以持续验证自身的推理轨迹，因此需要研究如何增强LLMs的自我验证能力以及这种能力是否能进一步改善推理性能。

Method: 提出GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，通过可调超参数控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能相当的同时，增强了自我验证能力。

Conclusion: GRPO-Verif算法能够有效提升LLMs的自我验证能力，为改善推理性能提供了新的途径。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [61] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。

Method: 使用表示对齐模块将EEG和面部特征嵌入共享潜在空间，强制执行模态间的语义一致性。将残差异常视为噪声引起的不一致性，在主动学习过程中选择性查询这些样本以获得反馈。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性。

Conclusion: 该方法是一种数据高效且噪声容忍的EEG情感解码方法，在脑机接口系统中具有潜力。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [62] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 该论文研究了复数变分自编码器中的Kähler几何结构，提出了基于复数高斯混合的Kähler势导数方法，能够高效计算Fisher信息度量并实现潜在空间的正则化。


<details>
  <summary>Details</summary>
Motivation: 探索复数VAEs中的几何结构，特别是Kähler几何，以改进潜在空间的表示质量和采样效果。

Method: 推导复数情况下的Fisher信息度量，提出复数高斯混合的Kähler势导数方法，通过该势函数高效计算度量并实现潜在空间正则化。

Result: 该方法能够产生更平滑的表示，减少语义异常值，并通过加权复数体积元素进行采样。

Conclusion: 复数VAEs确实展现出Kähler几何结构，提出的方法在保持几何忠实性的同时提高了计算效率，改善了表示质量。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [63] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器利用正常数据分布建模故障差异，并引入多样性损失防止模式崩溃，在真实性和多样性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障诊断至关重要，但故障数据稀缺（故障事件罕见且标注成本高）阻碍了数据驱动方法。现有时间序列生成模型难以在少样本场景下捕捉故障分布，生成样本缺乏真实性和多样性。

Method: 基于扩散模型的少样本故障时间序列生成框架，采用正负差异适配器利用预训练的正常数据分布建模正常与故障域之间的差异，并引入多样性损失通过样本间差异正则化鼓励生成多样化故障样本。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中实现了最先进的性能。

Conclusion: 提出的框架有效解决了少样本故障时间序列生成问题，通过利用正常数据分布和多样性正则化，显著提升了生成样本的质量和多样性。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [64] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种量子图注意力网络(Q-GAT)，在深度强化学习框架中用参数化量子电路替代传统多层感知机，用于解决车辆路径问题，减少了50%以上的可训练参数，同时提升了5%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于图神经网络的深度强化学习方法依赖参数繁多的多层感知机，存在内存限制问题。量子计算提供了一种参数效率更高的替代方案。

Method: 在DRL框架中构建量子图注意力网络，用参数化量子电路替代传统MLP在关键读出阶段，结合近端策略优化和贪心/随机解码策略。

Result: 在VRP基准测试中，Q-GAT实现了更快的收敛速度，路由成本比经典GAT基线降低约5%，同时可训练参数减少超过50%。

Conclusion: 参数化量子电路增强的图神经网络有望成为大规模路由和物流优化问题的紧凑且有效的求解器。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [65] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个蒸馏框架，将掩码自回归扩散模型压缩为单步生成，实现30倍加速并保持样本质量，同时支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归扩散模型推理速度慢的问题，特别是其分层推理机制（外部AR解掩码循环和内部扩散去噪链）不仅影响生成效率，还阻碍了在强化学习中的实际应用。

Method: 提出基于分数的变分目标，将掩码自回归扩散模型蒸馏为单步生成；开发MARVAL-RL框架，实现掩码自回归模型的高效强化学习。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在包含实体名称的ImageNet数据集上持续提升CLIP和图像奖励分数。

Conclusion: MARVAL为掩码自回归扩散模型的蒸馏和强化学习提供了首个实用路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [66] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [67] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种完全量子的图神经网络（QGNN），使用参数化量子电路实现消息传递，用于无线网络资源管理，在保持性能的同时减少参数数量并实现并行处理。


<details>
  <summary>Details</summary>
Motivation: 无线网络复杂性增加需要可扩展的资源管理方法，经典图神经网络在大规模场景下计算成本高，量子方法有望提供更高效的解决方案。

Method: 使用量子图卷积层（QGCLs）将特征编码为量子态，通过NISQ兼容的幺正变换处理图结构，通过测量获取嵌入表示。

Result: 在D2D功率控制SINR最大化任务中，QGNN与经典方法性能相当，但参数更少且具有固有并行性。

Conclusion: 这种基于PQC的端到端量子图神经网络标志着向量子加速无线优化迈出了一步。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [68] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了EntroPIC方法，通过比例-积分控制动态调整正负样本的损失系数，稳定大语言模型训练中的熵值，确保有效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以在训练过程中维持适当的熵水平，因为正负样本在不同步骤中对熵的影响方式不同，容易导致模型陷入次优行为。

Method: EntroPIC方法使用比例-积分控制机制，自适应地调整正负样本的影响，通过动态调节它们的损失系数来稳定训练过程中的熵。

Result: 实验结果表明，该方法能成功维持期望的熵水平，在大规模LLM训练中实现稳定和最优的强化学习训练。

Conclusion: EntroPIC方法通过熵稳定化机制，有效解决了LLM长期训练中的探索稳定性问题，为大规模语言模型的强化学习训练提供了可靠解决方案。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [69] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 提出基于改进PVTD3算法的电热联合系统智能调度方法，在可再生能源接入和多重不确定性下，通过引入购电波动惩罚项优化系统，显著降低综合成本和购电波动，改善储能管理。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源转型和可再生能源快速发展，新能源接入和多重不确定性下的电热联合系统调度优化挑战日益突出。

Method: 采用改进的双延迟深度确定性策略梯度(PVTD3)算法，通过引入电网购电波动惩罚项实现系统优化。

Result: 在10%、20%和30%可再生能源渗透率场景下，PVTD3算法相比传统TD3算法分别降低系统综合成本6.93%、12.68%和13.59%，平均购电波动幅度降低12.8%，低温储热罐终态值降低7.67-17.67单位，高温储热罐维持在3.59-4.25安全运行范围。

Conclusion: 所提算法在经济性、电网稳定性和储能设备管理的可持续调度能力方面均表现出优越性能。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [70] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 将GRPO方法从语言模型扩展到表示学习模型，提出GRPO-RM方法，通过预定义输出集替代token采样，设计专用奖励函数，在多个真实数据集上验证有效性


<details>
  <summary>Details</summary>
Motivation: GRPO方法在语言模型微调中效果显著，但能否推广到表示学习模型尚不明确，因此研究GRPO类策略在表示模型后训练中的性能

Method: 提出GRPO-RM方法，建立预定义输出集替代LLM中的token序列采样生成输出组，设计专门的奖励函数适应表示模型特性

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性

Conclusion: 成功将GRPO方法扩展到表示学习模型，GRPO-RM在表示模型后训练中表现出良好性能

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [71] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用量，在边缘设备上实现高效的模型适应，同时保持竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法依赖频繁适应和高计算成本，不适用于资源受限的边缘环境。

Method: 提出两个关键组件：类域代表内存(CnDRM)存储少量代表性样本，推理时批量感知内存归一化(IoBMN)动态调整归一化统计量。

Result: SNAP将延迟降低高达93.12%，精度下降低于3.3%，即使仅使用1%的数据流进行适应。

Conclusion: SNAP在边缘设备上具有强大的实际应用潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [72] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝，生成对后端和精度选择具有鲁棒性的硬件中性检查点，减少边缘加速器中低比特量化在不同编译器间的精度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 专用边缘加速器依赖低比特量化，但不同厂商编译器在缩放、裁剪和内核支持方面存在差异，导致相同浮点检查点在不同后端产生不一致的精度，迫使从业者调整标志或重构模型。

Method: 结合渐进式伪量化来对齐训练与部署的整数网格，以及反向剪枝来抑制异常值驱动的尺度膨胀，同时保持可学习性。该方法与量化方案无关，无需厂商特定的图更改。

Result: 在各种模型和任务中，Quant-Trim缩小了浮点与低比特之间的差距，减少了对编译器启发式/校准的依赖，避免了针对每个后端的重新训练。

Conclusion: Quant-Trim提供了一种有效的训练阶段解决方案，能够生成硬件中性的量化检查点，提高边缘部署的一致性和效率。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [73] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型的概念可解释性，分析了不同层编码的概念类型、线性可恢复性、表示演化以及概念组合处理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在实证上取得了成功，但其内部如何表示基本时间序列概念的机制仍然知之甚少，需要系统性的可解释性研究。

Method: 采用分层分析、线性可恢复性测试和表示相似性度量等方法，系统探究概念在模型中的编码位置、线性可恢复性、表示演化以及概念组合处理。

Result: 早期层主要捕获局部时域模式，深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复。在组合场景下，探针性能下降，显示概念间存在干扰。

Conclusion: 虽然原子概念能够可靠定位，但概念组合仍然是当前时间序列基础模型的一个关键限制，突显了其在表示交互时间现象能力上的不足。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [74] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: KrawtchoukNet是基于离散Krawtchouk多项式的GNN滤波器，解决了传统谱图神经网络在异质图和高度数多项式时的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于多项式滤波器的谱图神经网络存在两个关键限制：1）在异质图上性能崩溃；2）在高多项式度数时性能崩溃（过平滑）。这些问题源于标准滤波器的静态低通特性。

Method: 提出KrawtchoukNet，通过两个关键设计：1）将多项式域N固定为小常数，创建首个具有固有有界递归系数的GNN滤波器；2）使滤波器的形状参数p可学习，让滤波器能自适应图数据的谱响应。

Result: KrawtchoukNet在K=10时达到最先进的抗过平滑效果，并在异质图基准测试（Texas、Cornell）上显著优于GAT和APPNP等标准GNN。

Conclusion: KrawtchoukNet通过有界递归系数和可学习形状参数，为谱图神经网络的异质图适应性和高多项式度数稳定性提供了统一解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [75] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了LaguerreNet，一种基于连续拉盖尔多项式的GNN滤波器，通过可训练的alpha参数学习滤波器频谱形状，解决了异质图性能差和多项式度增加时的过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统频谱GNN在异质图上表现不佳，且在高多项式度时会出现性能崩溃（过平滑），这些问题源于标准滤波器的静态低通特性。

Method: 使用连续拉盖尔多项式构建GNN滤波器，使核心alpha参数可训练，并通过LayerNorm稳定化技术解决无界多项式的数值不稳定性问题。

Result: LaguerreNet在异质图基准测试中达到最先进性能，对过平滑具有强鲁棒性，性能在K=10时达到峰值，比ChebyNet崩溃点高一个数量级。

Conclusion: LaguerreNet通过自适应多项式滤波器和有效的稳定化技术，成功解决了频谱GNN在异质图和过平滑方面的关键限制。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [76] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: STREAM-VAE是一种用于汽车遥测时间序列异常检测的变分自编码器，通过双路径编码器分离慢漂移和快尖峰信号动态，提高异常检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据同时存在慢漂移和快尖峰，标准重建方法使用单一潜在过程会混合不同时间尺度，导致尖峰被平滑或方差膨胀，削弱异常分离能力。

Method: 使用双路径编码器分离慢漂移和快尖峰信号动态，解码器将瞬态偏差与正常操作模式分开表示，设计用于部署，在不同操作模式下产生稳定的异常分数。

Result: 在汽车遥测数据集和公开SMD基准测试中，与强基线方法相比，明确分离漂移和尖峰动态提高了鲁棒性。

Conclusion: STREAM-VAE通过分离不同时间尺度的信号动态，有效提升了汽车遥测数据异常检测的性能和稳定性。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [77] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统评估了33种时间序列预测的集成方法，发现堆叠法能持续提升精度，并提出多层堆叠框架以在不同任务中获得最优性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中集成方法应用不足，简单的线性组合仍被视为最佳方法，需要系统探索更有效的集成策略。

Method: 评估33种集成模型（包括现有和新提出的），使用50个真实数据集，并提出多层堆叠框架来结合不同堆叠模型的优势。

Result: 堆叠法能持续提高预测精度，但没有单一堆叠器在所有任务中表现最佳；多层堆叠框架在不同预测场景中始终提供更优精度。

Conclusion: 基于堆叠的方法有潜力改进时间序列预测的AutoML系统。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [78] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 本文提出了一个成本感知预测框架，结合LLM代理进行成本效益分析，以改善机器学习预测在临床决策中的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习预测模型往往没有考虑下游的价值权衡和临床可解释性，这限制了其在医疗决策中的实际应用价值。

Method: 开发了预测心力衰竭患者1年死亡率的XGBoost模型，引入临床影响投影曲线可视化成本维度，并使用四个LLM代理生成患者特定的成本效益分析描述。

Result: XGBoost模型表现最佳（AUROC 0.804），CIP曲线提供了群体层面的成本概览，LLM生成了个体层面的成本效益分析，系统获得临床医生的积极评价。

Conclusion: CAP框架利用LLM代理整合机器学习分类结果和成本效益分析，为临床决策提供更透明和可解释的支持。

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [79] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征重要性方法CID，通过生成正负反事实样本、建模分布并使用分布差异度量来评估特征重要性，在忠实性指标上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估个体特征重要性对于理解模型决策过程至关重要，但缺乏明确的真实基准进行比较，需要替代的、有理论基础的重要性度量方法。

Method: 生成正负反事实样本集，使用核密度估计建模其分布，基于分布差异度量对特征进行排序，该方法具有严格的数学基础并满足有效度量的关键属性。

Result: 与成熟的局部特征重要性解释方法相比，CID方法不仅提供了互补的视角，还在忠实性指标（全面性和充分性）上提高了性能，产生了更忠实的系统解释。

Conclusion: CID方法作为模型分析的有价值工具具有潜力，能够提供更可靠的特征重要性评估。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [80] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%的核心参数来避免灾难性遗忘，同时保持基础模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 领域特定后训练通常会导致灾难性遗忘，使基础模型失去通用推理能力，限制了它们在动态现实环境中的适应性。在获取下游领域知识的同时保持通用能力是大型语言和多模态模型的核心挑战。

Method: PIECE方法使用两种重要性估计器（基于Fisher信息的PIECE-F和基于二阶归一化的PIECE-S）来指导选择性地仅更新与新任务最相关的0.1%核心参数，无需访问先前的训练数据或增加模型参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE能够保持通用能力，并在各种下游任务中实现最先进的持续学习性能。

Conclusion: PIECE为构建可扩展、领域自适应且无灾难性遗忘的基础模型提供了一条实用路径。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [81] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: EVA-Net是一个可解释的脑年龄异常检测框架，使用稀疏注意力Transformer处理长EEG序列，通过变分信息瓶颈学习鲁棒表示，并与原型网络对齐来学习健康老化流形。


<details>
  <summary>Details</summary>
Motivation: 现有脑年龄模型难以从不完美的医疗数据中学习健康基线，且缺乏可解释性。需要开发能够处理弱监督健康数据并提供可解释异常检测的方法。

Method: 使用稀疏注意力Transformer建模长EEG序列，通过变分信息瓶颈处理噪声和变异性，将表示与连续原型网络对齐来学习健康老化流形。

Result: 在1297名健康受试者上训练，达到最先进精度。在27名MCI和AD患者验证中，该组显示出显著更高的脑年龄差距和原型对齐误差。

Conclusion: EVA-Net为使用不完美医疗数据的医疗智能提供了一个可解释的框架，能够有效检测脑健康异常。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [82] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一类用于非线性非高斯状态空间模型状态估计的算法，基于变分拉格朗日框架，将贝叶斯推断转化为带动态约束的熵信任域更新序列。


<details>
  <summary>Details</summary>
Motivation: 解决非线性非高斯状态空间模型中的状态估计问题，传统方法在处理复杂模型时存在计算复杂度和精度限制。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为熵信任域更新序列，针对高斯-马尔可夫近似推导递归方案，对一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配。

Result: 得到一族前向-后向算法，具有有利的计算复杂度，能够有效处理非线性非高斯状态估计问题。

Conclusion: 提出的变分拉格朗日框架为非线性非高斯状态空间模型的状态估计提供了一类有效的算法，在计算复杂度和估计精度之间取得了良好平衡。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [83] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 研究分析了表格ICL模型中各层对预测的贡献，发现只有部分层共享共同表示语言，存在结构冗余，为模型压缩和可解释性提供机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型与大型语言模型在架构上相似，但各层在表格预测中的具体贡献尚不清楚，需要研究层间潜在空间的演化动态。

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究各层潜在空间的演化，识别冗余层，并与LLMs的动力学进行比较。

Result: 发现只有部分层共享共同表示语言，表明存在结构冗余，这为模型压缩和可解释性改进提供了机会。

Conclusion: 表格ICL模型存在层间结构冗余，可以通过压缩冗余层来优化模型，同时提高模型的可解释性。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [84] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种基于时间序列基础模型（TSFM）的上下文学习分类方法，无需微调模型即可对训练数据之外的数据进行分类，应用于轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要针对特定任务微调模型，而本文旨在利用预训练TSFM的上下文学习能力，无需微调即可实现分类，推动从定制化AI解决方案向更广泛的AI驱动维护系统发展。

Method: 将示例以目标（类别ID）和协变量（数据矩阵）形式嵌入模型提示中，通过上下文学习在预测轴上对未知协变量数据模式进行分类。具体将频域参考信号转换为伪时间序列模式，生成对齐的协变量和目标信号，使用TSFM预测数据对应预定义标签的概率。

Result: 该方法在伺服压力机电机轴承健康状态评估中表现出有效性，能够适应不同的操作条件，展示了预训练模型的可扩展性优势。

Conclusion: 该方法标志着从定制化窄AI解决方案向更广泛AI驱动维护系统的重要进展，利用TSFM的上下文学习能力实现了无需微调的有效分类。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [85] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的联邦学习能量最小化框架，通过联合优化设备选择、带宽分配和压缩级别，在非IID数据上实现更高精度，同时能耗降低高达79%。


<details>
  <summary>Details</summary>
Motivation: 解决无线边缘系统中联邦学习面临的挑战：平衡能源效率与公平参与，同时确保高模型精度，应对异构资源、不平等客户端贡献和有限通信容量的问题。

Method: 提出FairEnergy框架，将贡献分数（考虑更新幅度和压缩比）集成到联合优化中，通过松弛二元选择变量和应用拉格朗日分解处理全局带宽耦合，然后进行每设备子问题优化。

Result: 在非IID数据上的实验表明，与基线策略相比，FairEnergy实现了更高的精度，同时能耗降低了高达79%。

Conclusion: FairEnergy框架有效解决了联邦学习在无线边缘系统中的能源效率和公平参与平衡问题，显著提升了性能表现。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [86] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出了NTK引导的隐式神经教学(NINT)方法，通过动态选择最大化全局功能更新的坐标来加速隐式神经表示的训练，利用神经正切核(NTK)评分示例，显著减少训练时间近一半。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)在拟合高分辨率信号时需要优化数百万个坐标，产生过高的计算成本，需要高效的训练加速方法。

Method: 使用神经正切核(NTK)来评分示例，基于NTK增强的损失梯度范数动态选择坐标，同时考虑拟合误差和异构杠杆效应(自影响和跨坐标耦合)。

Result: 通过广泛实验证明，NINT显著减少训练时间近一半，同时保持或提高表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的坐标选择策略，有效加速隐式神经表示的训练，为高分辨率信号建模提供了高效的解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [87] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: PCARNN-DCBF：一种结合物理编码控制仿射残差神经网络和基于预览的离散控制屏障函数的新方法，用于地面车辆的实时地理围栏，在保持可验证控制结构的同时实现高保真学习。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案难以在可验证控制的结构要求与高保真学习之间取得平衡，而运行时地理围栏对于执行操作设计域至关重要。

Method: 引入PCARNN-DCBF管道，将物理编码控制仿射残差神经网络与基于预览的离散控制屏障函数集成，明确保持车辆动力学的控制仿射结构，通过实时二次规划强制执行多边形保持约束。

Result: 在CARLA中对电动和燃烧平台的实验表明，这种结构保持方法显著优于分析和非结构化神经基线。

Conclusion: PCARNN-DCBF通过结构保持方法成功解决了高保真学习与可验证控制之间的权衡问题，为地面车辆提供了可靠的运行时地理围栏解决方案。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>


### [88] [CODE: A global approach to ODE dynamics learning](https://arxiv.org/abs/2511.15619)
*Nils Wildt,Daniel M. Tartakovsky,Sergey Oladyshkin,Wolfgang Nowak*

Main category: cs.LG

TL;DR: 提出ChaosODE（CODE）方法，使用任意多项式混沌展开来表示ODE的右侧，在稀疏采样数据下学习动力学系统，相比神经网络和核方法具有更好的外推能力。


<details>
  <summary>Details</summary>
Motivation: 传统ODE建模需要密集测量数据，但实际中通常只能获得稀疏采样数据。现有数据驱动方法（如NeuralODE、KernelODE）在稀疏数据和噪声情况下外推能力较差。

Method: 使用任意多项式混沌展开（aPCE）来表示ODE的右侧，构建全局正交多项式表示动力学系统。

Result: 在Lotka-Volterra系统上测试，CODE在噪声水平、初始条件和长期预测方面表现优异，即使在新初始条件下也展现出卓越的外推能力，优于NeuralODE和KernelODE。

Conclusion: CODE方法在稀疏数据和噪声条件下具有更好的鲁棒性和外推能力，为动力学学习问题提供了实用的优化指南。

Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.

</details>


### [89] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: 本文通过自动驾驶环境中的实验，揭示了持续强化学习面临的挑战，包括环境抽象、超参数敏感性、灾难性遗忘和神经网络容量利用等问题，并提出了重要的开放研究问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习在非平稳环境（如自动驾驶）中具有重要应用价值，但将持续学习成功应用于强化学习仍然是一个开放性问题，需要探索其中的挑战和解决方案。

Method: 在自动驾驶环境中，使用PPO算法让智能体依次学习四种不同角度的停车场景，模拟持续学习环境，通过实验分析持续强化学习的挑战。

Result: 实验揭示了持续强化学习的多个关键挑战：寻找合适的环境抽象、对超参数的过度敏感、灾难性遗忘问题以及神经网络容量的有效利用。

Conclusion: 持续强化学习需要解决神经网络适用性问题，并提出跨学科研究（特别是计算机科学和神经科学）的必要性，以创建鲁棒的持续强化学习系统。

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [90] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: DeepThinkVLA通过混合注意力解码器和两阶段训练策略解决VLA模型中思维与动作的冲突，在LIBERO基准测试中达到97.0%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型使用单一自回归解码器同时处理顺序思维链推理和高维并行机器人动作，导致运动控制性能下降且思维与动作间缺乏强因果联系。

Method: 采用混合注意力解码器：用因果注意力生成顺序思维链，然后切换到双向注意力并行解码动作向量；配合两阶段训练：先用SFT教授基础推理，再用RL通过任务成功奖励对齐推理-动作序列。

Result: 在LIBERO基准测试中达到97.0%的成功率，混合架构比标准解码器提升15.5%，RL阶段额外提供2%的关键提升。

Conclusion: DeepThinkVLA通过架构和训练策略的协同作用，有效解决了VLA模型中思维与动作的冲突，实现了最先进的性能。

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [91] [Walrus: A Cross-Domain Foundation Model for Continuum Dynamics](https://arxiv.org/abs/2511.15684)
*Michael McCabe,Payel Mukhopadhyay,Tanya Marwah,Bruno Regaldo-Saint Blancard,Francois Rozet,Cristiana Diaconu,Lucas Meyer,Kaze W. K. Wong,Hadi Sotoudeh,Alberto Bietti,Irina Espejo,Rio Fear,Siavash Golkar,Tom Hehir,Keiya Hirashima,Geraud Krawezik,Francois Lanusse,Rudy Morel,Ruben Ohana,Liam Parker,Mariel Pettee,Jeff Shen,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: cs.LG

TL;DR: Walrus是一个基于Transformer的物理模拟基础模型，专门用于流体类连续介质动力学，通过谐波分析稳定化、负载均衡分布式训练和计算自适应标记化等技术，在19个不同场景上预训练，在短期和长期预测方面优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言和视觉领域取得了巨大成功，但在物理模拟领域面临数据异构性、不稳定长期动力学、不同分辨率和维度等挑战，需要开发专门的方法来克服这些障碍。

Method: 采用谐波分析稳定化方法、负载均衡的2D和3D分布式训练策略、计算自适应标记化等技术，构建基于Transformer的Walrus模型，在19个多样化场景上进行预训练。

Result: Walrus在下游任务的短期和长期预测方面优于现有基础模型，消融研究证实了所提方法在预测稳定性、训练吞吐量和迁移性能方面的价值。

Conclusion: 通过专门设计的稳定化、训练和标记化方法，成功开发了用于物理模拟的Transformer基础模型Walrus，在多个领域表现出优越性能，代码和权重已开源。

Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.

</details>


### [92] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: 量化感知的强化学习训练对大型推理模型的推理性能有负面影响，而训练后量化和QLoRA方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究量化对大型推理模型强化学习的影响，因为目前量化主要在有监督微调背景下研究，而在强化学习环境中的影响尚不明确。

Method: 通过系统实验比较训练后量化与量化感知强化学习优化方法在数学基准测试上的推理性能差异。

Result: 发现量化感知的强化学习训练显著降低了推理性能，而训练后量化和QLoRA方法保持了更好的性能表现。

Conclusion: 量化感知的强化学习训练对大型推理模型的学习过程产生负面影响，建议采用训练后量化或QLoRA方法进行模型量化。

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [93] [Computing Power Indices in Weighted Majority Games with Formal Power Series](https://arxiv.org/abs/2511.14995)
*Naonori Kakimura,Yoshihiko Terai*

Main category: cs.GT

TL;DR: 提出了计算加权多数博弈中权力指数的快速伪多项式时间算法，包括Banzhaf指数和Shapley-Shubik指数的高效计算方法。


<details>
  <summary>Details</summary>
Motivation: 现有算法在计算加权多数博弈中的权力指数时效率较低，特别是在配额q较小时。需要开发更快的算法来应对实际应用中的计算需求。

Method: 利用形式幂级数的高效计算技术，设计了时间复杂度分别为O(n+qlog(q))和O(nqlog(q))的算法来计算Banzhaf指数和Shapley-Shubik指数。

Result: 当q=2^o(n)时，新算法比现有算法更快。Banzhaf指数可在O(n+qlog(q))时间内计算所有玩家，Shapley-Shubik指数可在O(nqlog(q))时间内计算。

Conclusion: 提出的算法显著提高了加权多数博弈中权力指数的计算效率，特别是在配额相对较小时，为实际应用提供了更实用的计算工具。

Abstract: In this paper, we propose fast pseudo-polynomial-time algorithms for computing power indices in weighted majority games. We show that we can compute the Banzhaf index for all players in $O(n+q\log (q))$ time, where $n$ is the number of players and $q$ is a given quota. Moreover, we prove that the Shapley--Shubik index for all players can be computed in $O(nq\log (q))$ time. Our algorithms are faster than existing algorithms when $q=2^{o(n)}$. Our algorithms exploit efficient computation techniques for formal power series.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 本文提出了一种结合Sharpness-Aware Minimization (SAM)的增强型Soft Actor Critic算法，在分布式多智能体强化学习框架中实现O-RAN资源管理，通过基于TD误差方差的适应性SAM机制和动态ρ调度，显著提升了资源分配效率和QoS满意度。


<details>
  <summary>Details</summary>
Motivation: 下一代网络采用O-RAN架构实现动态资源管理，但现有深度强化学习方法在动态环境中存在鲁棒性和泛化性不足的问题，需要开发更有效的资源管理方法。

Method: 将Sharpness-Aware Minimization (SAM)集成到Soft Actor Critic (SAC)算法中，构建分布式多智能体强化学习框架，引入基于TD误差方差的适应性SAM机制和动态ρ调度方案。

Result: 实验结果表明，该方法显著优于传统深度强化学习方法，资源分配效率提升高达22%，并在不同O-RAN切片中实现了优越的QoS满意度。

Conclusion: 提出的方法通过针对性正则化和动态探索-利用平衡，有效解决了O-RAN环境中深度强化学习的鲁棒性和泛化性问题，为下一代网络资源管理提供了有效解决方案。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [95] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出Macro Action Quantization (MAQ)框架，通过将人类演示蒸馏为宏动作来训练类人强化学习智能体，在D4RL Adroit基准测试中显著提高了类人性和轨迹相似度。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域达到超人类性能，但往往表现出与人类行为不符的非自然行为，这影响了可解释性和可信度。目标是设计能够表现出类人行为的强化学习智能体。

Method: 将类人性建模为轨迹优化问题，引入宏动作量化(MAQ)框架，使用Vector-Quantized VAE从人类演示中蒸馏出宏动作，并采用后退时域控制作为可扩展的高效实现方法。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了类人性，增加了轨迹相似度得分，在人类评估研究中获得了所有强化学习智能体中最高的类人性排名。

Conclusion: MAQ可以轻松集成到各种现成的强化学习算法中，为学习类人强化学习智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [96] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，通过模块化设计扩展了GeneGPT，在基因组问答任务中表现优异，同时显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、成本和隐私问题，探索开源模型在基因组问答中的潜力。

Method: 采用模块化多智能体架构，引入专门用于工具路由、查询生成和响应验证的智能体，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或超越GeneGPT，在Gene-Turing和GeneHop上分别获得0.849和0.830的平均分数，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，能够在不牺牲性能的前提下提高效率和降低成本。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [97] [Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks](https://arxiv.org/abs/2511.15003)
*Reza Mirjalili,Behrad Braghi,Shahram Shadrokh Sikari*

Main category: stat.AP

TL;DR: 提出了一种基于图神经网络的项目工期和成本预测框架，相比传统方法显著提升了预测精度，并能识别资源瓶颈和关键依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统项目管理方法（如CPM和PERT）对任务依赖关系和资源性能的假设过于简化且静态，无法准确预测资源受限和任务相互依赖的项目工期和成本。

Method: 构建异构活动-资源图表示项目，节点表示活动和资源，边编码时间和资源依赖关系，使用图神经网络（包括GraphSAGE和时序图网络）捕捉任务、资源和时间-成本动态之间的结构关系。

Result: 在合成和基准项目数据集上的实验表明，GNN框架相比传统回归和基于树的方法，平均绝对误差减少23-31%，决定系数R2从约0.78提升到0.91。

Conclusion: 该GNN框架不仅提高了项目工期和成本预测的准确性，还能通过学习到的嵌入提供对资源瓶颈和关键依赖关系的可解释洞察，支持更可解释和自适应的调度决策。

Abstract: Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [98] [Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2511.15015)
*Kexin Chu,Dawei Xiang,Zixu Shen,Yiwei Yang,Zecheng Liu,Wei Zhang*

Main category: cs.PF

TL;DR: DynaExq是一个运行时系统，通过动态管理专家精度来解决MoE模型在消费级GPU上部署的内存问题，相比静态量化方法显著提升精度。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然能高效扩展LLM容量，但在消费级GPU上部署受到非活跃专家大内存占用的限制。静态量化无法适应变化的激活模式，导致激进压缩下的精度损失。

Method: DynaExq包含三个组件：(1)基于热度的精度控制器，持续对齐专家位宽与长期激活统计；(2)全异步精度切换管道，将精度提升和降低与MoE计算重叠；(3)无碎片内存池机制，支持混合精度专家的确定性分配。

Result: 在Qwen3-30B和Qwen3-80B MoE模型及六个基准测试中，DynaExq在单张RTX 5090和A6000 GPU上部署大型LLM，相比静态低精度基线精度提升高达4.03分。

Conclusion: 结果表明，自适应、工作负载感知的量化是内存受限MoE服务的有效策略。

Abstract: Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.
  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [99] [Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications](https://arxiv.org/abs/2511.14769)
*Yifan Xu,Vipul Gupta,Rohit Aggarwal,Varsha Mahadevan,Bhaskar Krishnamachari*

Main category: cs.IR

TL;DR: CAR算法通过分析查询-文档相似度距离的聚类模式，动态确定最佳检索文档数量，解决了传统静态top-k检索方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态top-k检索方法无法适应不同查询特性：窄范围查询需要少量高度相关文档，而宽泛查询需要更多支持信息。静态方法导致要么上下文不足，要么信息冗余。

Method: CAR算法通过分析查询-文档相似度距离的聚类模式，检测从高度相关文档到不太相关候选文档的过渡点，建立自适应截断机制。

Result: 在Coinbase CDP语料库和MultiHop-RAG基准测试中，CAR始终选择最优检索深度，获得最高TES分数。在下游RAG评估中，CAR将LLM token使用减少60%，端到端延迟降低22%，幻觉减少10%，同时完全保持答案相关性。

Conclusion: CAR算法显著提升了RAG系统的效率和效果，在Coinbase虚拟助手中的集成使用户参与度提升了200%。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [100] [Compiling Set Queries into Work-Efficient Tree Traversals](https://arxiv.org/abs/2511.15000)
*Alexander J Root,Christophe Gyurgyik,Purvi Goel,Kayvon Fatahalian,Jonathan Ragan-Kelley,Andrew Adams,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: 该论文提出了一种自动优化树结构查询的方法，通过符号区间分析生成子树修剪条件，支持几何谓词和复合查询融合，能够自动推导广义单索引和双索引树连接。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要为每个查询谓词和数据结构手动实现修剪逻辑，这种方法既繁琐又缺乏通用性。

Method: 使用符号区间分析生成子树修剪条件，扩展规则处理几何谓词，编译器融合复合查询到单次树遍历。

Result: 生成的遍历代码与专家手写代码行为一致，在无手写优化情况下能渐进优于线性扫描和嵌套循环连接。

Conclusion: 该方法成功实现了树结构查询优化的通用化和自动化，显著提升了查询性能。

Abstract: Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply.

</details>


### [101] [Compiling to recurrent neurons](https://arxiv.org/abs/2511.14953)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.PL

TL;DR: 本文提出了一种将离散迭代结构编译为线性循环神经元的方法，使迭代在可微分编程中成为一等公民，从而扩展了可微分算法的表达能力。


<details>
  <summary>Details</summary>
Motivation: 当前可微分编程中离散结构（如条件语句和迭代）被视为二等公民，因为它们缺乏显式导数，限制了可微分算法的表达范围。这约束了神经网络和可微分程序的构建方式。

Method: 设计了一个名为Cajal的最小化类型化、高阶线性编程语言，包含迭代功能。证明其程序可以正确编译为循环神经元，使离散算法能够以可微分形式表达。

Result: 通过实验验证，将循环神经元与神经网络连接用于迭代图像转换任务，网络学习速度更快且数据效率更高，相比没有一等公民迭代的神经网络表现更好。

Conclusion: 循环神经元实现了学习与普通编程离散结构之间的丰富交互，突破了可微分编程中离散结构的限制。

Abstract: Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [102] [Fully Differentiable dMRI Streamline Propagation in PyTorch](https://arxiv.org/abs/2511.14807)
*Jongyeon Yoon,Elyssa M. McMaster,Michael E. Kim,Gaurav Rudravaram,Kurt G. Schilling,Bennett A. Landman,Daniel Moyer*

Main category: eess.IV

TL;DR: 提出了一种完全可微分的扩散MRI纤维束追踪方法，使用PyTorch实现的流线传播器，解决了现有方法无法集成到端到端学习框架的问题。


<details>
  <summary>Details</summary>
Motivation: 现有纤维束追踪方法通常不可微分，限制了在端到端学习框架中的集成。虽然深度学习在相关任务中取得进展，但缺乏完全可微分的传播方法。

Method: 开发了基于PyTorch的完全可微分流线传播器，确保没有组件会阻断梯度流，同时保持与领先流线算法的数值保真度。

Result: 该方法在保持与标准传播器相当性能的同时实现了完全可微分性，能够深度集成到深度学习工作流中。

Conclusion: 通过将流线传播转化为可微分的PyTorch框架，为新型宏观结构推理奠定了基础，既计算稳健又科学严谨。

Abstract: Diffusion MRI (dMRI) provides a distinctive means to probe the microstructural architecture of living tissue, facilitating applications such as brain connectivity analysis, modeling across multiple conditions, and the estimation of macrostructural features. Tractography, which emerged in the final years of the 20th century and accelerated in the early 21st century, is a technique for visualizing white matter pathways in the brain using dMRI. Most diffusion tractography methods rely on procedural streamline propagators or global energy minimization methods. Although recent advancements in deep learning have enabled tasks that were previously challenging, existing tractography approaches are often non-differentiable, limiting their integration in end-to-end learning frameworks. While progress has been made in representing streamlines in differentiable frameworks, no existing method offers fully differentiable propagation. In this work, we propose a fully differentiable solution that retains numerical fidelity with a leading streamline algorithm. The key is that our PyTorch-engineered streamline propagator has no components that block gradient flow, making it fully differentiable. We show that our method matches standard propagators while remaining differentiable. By translating streamline propagation into a differentiable PyTorch framework, we enable deeper integration of tractography into deep learning workflows, laying the foundation for a new category of macrostructural reasoning that is not only computationally robust but also scientifically rigorous.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [103] [Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report](https://arxiv.org/abs/2511.14939)
*Daniel Oliveira de Brito,Letícia Gabriella de Souza,Marcelo Matheus Gauy,Marcelo Finger,Arnaldo Candido Junior*

Main category: cs.SD

TL;DR: 该技术报告研究了预训练音频模型在COVID-19检测任务上的性能，发现模型在跨数据集评估中表现严重退化，强调了人口统计平衡对临床稳健模型评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究预训练音频模型在COVID-19检测任务中的性能，特别关注模型在人口统计平衡设置下的泛化能力，以消除人口特征与COVID-19状态之间的虚假相关性。

Method: 在Coswara和COUGHVID数据集上微调Audio-MAE和三种PANN架构（CNN6、CNN10、CNN14），实施严格的人口统计分层（按年龄和性别），评估数据集内和跨数据集的泛化性能。

Result: 数据集内评估显示中等性能（Audio-MAE在Coswara上AUC 0.82，F1-score 0.76），但在Coughvid上性能有限（AUC 0.58-0.63）。跨数据集评估显示所有模型严重泛化失败（AUC 0.43-0.68），Audio-MAE性能退化显著（F1-score 0.00-0.08）。

Conclusion: 人口统计平衡虽然降低了表面模型性能，但通过消除人口统计泄漏提供了更真实的COVID-19检测能力评估。平衡后有限的数据集大小不足以支持深度学习模型，突显了开发可泛化音频COVID-19检测系统的根本挑战。

Abstract: This technical report investigates the performance of pre-trained audio models on COVID-19 detection tasks using established benchmark datasets. We fine-tuned Audio-MAE and three PANN architectures (CNN6, CNN10, CNN14) on the Coswara and COUGHVID datasets, evaluating both intra-dataset and cross-dataset generalization. We implemented a strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). Our experiments demonstrate that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage - a confounding factor that inflate performance metrics. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These findings highlight fundamental challenges in developing generalizable audio-based COVID-19 detection systems and underscore the importance of rigorous demographic controls for clinically robust model evaluation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [104] [Tokenisation over Bounded Alphabets is Hard](https://arxiv.org/abs/2511.15709)
*Violeta Kastreva,Philip Whittington,Dennis Komm,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文证明了即使在有界字母表（包括二进制和一元字母表）下，分词问题仍然是NP完全且无法有效近似，解释了为什么实际算法如BPE和UnigramLM都是启发式的。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设分词在无界大字母表上是NP完全的，但实践中分词器在固定大小字母表（如字节或Unicode字符）上运行。本文旨在填补这一空白，分析有界n元字母表下的分词复杂性。

Method: 分析两种自然变体：自底向上分词和直接分词，分别考虑选择合并操作序列或词汇表来最优压缩数据集。证明对于n元字母表的硬度结果适用于任何更大尺寸的字母表。

Result: 即使在二进制字母表下，两种分词变体不仅是NP完全的，而且不存在多项式时间近似方案（除非P=NP）。直接分词在一元字母表下仍然是NP完全的。

Conclusion: 分词的计算难处理性不是大字母表或复杂构造的产物，而是根本性障碍。这解释了为什么BPE和UnigramLM等实用算法是启发式的，并指出近似算法是分词研究的重要方向。

Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.

</details>


### [105] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP通过分层令牌预置和均值池化解决LLM嵌入中的信息流限制问题，显著提升长文档嵌入性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的因果注意力机制限制了从后向前的信息流动，降低了表示质量，特别是在长文档场景下

Method: 分层令牌预置(HTP)：将输入分块并为后续块预置块级摘要令牌，创建多路径反向信息流；用均值池化替代最后令牌池化

Result: 在11个检索数据集和30个通用嵌入基准上取得一致性能提升，尤其在长上下文设置中表现突出

Conclusion: HTP作为一种简单、架构无关的方法，能够增强零样本和微调模型，为优质长文档嵌入提供了可扩展路径

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [106] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一个学生感知的数学辅导框架，通过整合学生画像、记忆和遗忘动态，实现个性化数学学习。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统未能捕捉学生知识随熟练度、概念差距和遗忘模式的动态演变，特别是在需要精细支架的数学辅导中。

Method: 维护结构化学生画像和事件记忆，结合连续遗忘曲线与知识追踪，动态更新学生掌握状态并生成难度校准的问题和解释。

Result: 实证结果显示TASA相比基线方法获得更优的学习成果和更自适应的辅导行为。

Conclusion: 建模时间遗忘和学习者画像在基于LLM的辅导系统中至关重要。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [107] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 提出了一个评估多语言视觉语言模型在印度语言中表现的可扩展框架，并创建了HinTel-AlignBench基准测试，发现模型在印度语言上的表现比英语平均下降8.3分（印地语）和5.5分（泰卢固语）。


<details>
  <summary>Details</summary>
Motivation: 当前多语言VLM评估存在四个主要局限：依赖未经验证的自动翻译、任务/领域覆盖范围窄、样本量有限、缺乏文化和本地来源的问答数据。印度作为拥有12亿人口和120多种主要语言的多样化地区，需要更公平的AI评估方法。

Method: 开发半自动化数据集创建框架，结合回译、过滤和人工验证；构建最全面的印地语和泰卢固语视觉语言基准测试，包括改编的英文数据集和本地新颖的印度数据集，每个语言约4000个问答对。

Result: 在所有模型中，5个任务中有4个在印度语言上的表现比英语有所下降，印地语平均下降8.3分，泰卢固语平均下降5.5分。对常见失败模式进行了分类，突出了多语言多模态理解的具体改进领域。

Conclusion: 该研究揭示了当前多语言VLMs在印度语言上的性能差距，提出了改进多语言多模态理解的具体方向，为开发更公平的AI系统提供了重要基准。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [108] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次系统研究了内在维度(ID)与可解释文本属性之间的关系，发现ID与基于熵的指标互补，能够捕捉与预测质量正交的几何复杂性；ID在不同文体间呈现稳定分层：科学文本ID最低(~8)，百科全书内容中等(~9)，创意/意见写作最高(~10.5)；通过稀疏自编码器识别出科学信号降低ID，而人性化信号增加ID。


<details>
  <summary>Details</summary>
Motivation: 内在维度是现代LLM分析的重要工具，但对其文本决定因素的研究仍不足。本文旨在通过可解释的文本属性来理解ID，填补这一研究空白。

Method: 采用交叉编码器分析、语言特征和稀疏自编码器(SAEs)进行多角度研究，包括控制长度后的相关性分析、不同文体的ID分层研究，以及通过SAEs识别因果特征并进行转向实验验证。

Result: 发现ID与熵指标在控制长度后不相关；不同文体呈现稳定的ID分层模式；识别出科学信号(正式语调、报告模板、统计)降低ID，人性化信号(个性化、情感、叙事)增加ID，且这些效应具有因果性。

Conclusion: 对于当代模型，科学写作相对"简单"，而小说、意见和情感表达增加了表征自由度。本研究为ID的正确使用和基于ID结果的合理解释提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [109] [MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging](https://arxiv.org/abs/2511.14806)
*Siyuan Li,Kai Yu,Anna Wang,Zicheng Liu,Chang Yu,Jingbo Zhou,Qirong Yang,Yucheng Guo,Xiaoming Zhang,Stan Z. Li*

Main category: q-bio.GN

TL;DR: MergeDNA提出了一种分层架构，通过联合优化动态基因组标记器和潜在Transformer，结合上下文感知的预训练任务，解决了基因组序列建模中信息密度不均和最小词汇单元定义不清的挑战。


<details>
  <summary>Details</summary>
Motivation: 基因组序列建模面临两个未解决的挑战：不同区域的信息密度差异很大，同时没有明确定义的最小词汇单元。现有的基于四个原始碱基或独立设计的DNA标记器的方法，通过简单的掩码语言建模预训练，往往无法适应基因组序列的不同复杂性。

Method: 采用Token Merging技术，引入分层架构：标记化模块通过堆叠多层具有局部窗口约束的可微分标记合并块，将相邻碱基自动分块成单词；潜在编码器通过全注意力块捕获这些合并单词的全局上下文；对称使用潜在解码器和局部解码器，通过两个预训练任务进行学习：合并标记重建和自适应掩码标记建模。

Result: 在三个流行的DNA基准测试和多个多组学任务上，MergeDNA通过微调或零样本评估实现了优越性能，超越了典型的标记化方法和大规模DNA基础模型。

Conclusion: MergeDNA通过联合优化动态标记器和上下文感知预训练任务，有效解决了基因组序列建模的关键挑战，在多个任务上表现出色。

Abstract: Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.

</details>


### [110] [CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery](https://arxiv.org/abs/2511.15139)
*Amit Kumar,Maninder Kaur,Raghvendra Mall,Sukrit Gupta*

Main category: q-bio.GN

TL;DR: CASPER是一个基于交叉注意力的框架，通过利用单细胞RNA测序的质心级表示来预测空间转录组学中未测量的基因表达，在多个数据集和指标上显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学平台由于实验限制和成本过高，只能测量有限数量的基因。需要计算方法来整合单细胞RNA测序数据以预测未测量的基因。

Method: 提出CASPER框架，使用交叉注意力机制，利用单细胞RNA测序的质心级表示来预测空间转录组学中未测量的基因表达。

Result: 在四个最先进的空间转录组学/单细胞RNA测序数据集对上进行了严格测试，CASPER在12个指标中的9个上显示出显著改进。

Conclusion: 这项工作为空间转录组学到单细胞RNA测序模态转换的进一步研究铺平了道路。

Abstract: Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [111] [Multimodal Wireless Foundation Models](https://arxiv.org/abs/2511.15162)
*Ahmed Aboulfotouh,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 提出了首个多模态无线基础模型，能够同时处理原始IQ流和图像式无线模态，在多个任务上实现竞争性甚至超越单模态模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前无线基础模型仅处理单一模态，但不同任务和环境下最有效的模态会变化，需要多模态支持来适应更广泛的任务和场景。

Method: 引入掩码无线建模作为自监督目标，从IQ流和图像式无线模态中学习联合表示，支持跨模态的多任务处理。

Result: 在5个任务上评估：基于图像的人类活动感知、RF信号分类、5G NR定位，以及基于IQ的RF设备指纹识别、干扰检测/分类。多模态模型与单模态模型竞争，并在多个案例中超越其性能。

Conclusion: 多模态无线基础模型具有强大潜力，支持跨不同模态的多样化无线任务，为实现AI原生6G和联合感知、通信与定位愿景迈出具体一步。

Abstract: Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.

</details>


### [112] [CODE-II: A large-scale dataset for artificial intelligence in ECG analysis](https://arxiv.org/abs/2511.15632)
*Petrus E. O. G. B. Abreu,Gabriela M. M. Paixão,Jiawei Li,Paulo R. Gomes,Peter W. Macfarlane,Ana C. S. Oliveira,Vinicius T. Carvalho,Thomas B. Schön,Antonio Luiz P. Ribeiro,Antônio H. Ribeiro*

Main category: eess.SP

TL;DR: CODE-II是一个大规模真实世界心电图数据集，包含273万份12导联心电图，来自巴西209万成年患者，提供66个临床诊断类别，并发布了公开子集用于评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI心电图分析面临注释质量、数据集规模和范围有限等挑战，需要大规模高质量标注数据集来推动技术进步。

Method: 收集巴西远程医疗网络的273万份心电图，采用标准化诊断标准并由心脏病专家审核，开发66个临床诊断类别，并提供公开子集用于评估。

Result: 在CODE-II上预训练的神经网络在外部基准测试（PTB-XL和CPSC 2018）中表现出优越的迁移性能，优于在更大数据集上训练的替代方案。

Conclusion: CODE-II数据集为心电图AI分析提供了高质量的大规模资源，能够显著提升模型在外部数据集上的性能表现。

Abstract: Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [113] [QADR: A Scalable, Quantum-Resistant Protocol for Anonymous Data Reporting](https://arxiv.org/abs/2511.15272)
*Nilesh Vyas,Konstantin Baier*

Main category: quant-ph

TL;DR: 提出QADR协议，一种混合量子-经典框架，为大规模物联网提供量子抗性匿名数据报告，在性能与安全性间取得平衡，通信成本从O(n^4)降至O(n^2)。


<details>
  <summary>Details</summary>
Motivation: 现有方案无法满足大规模物联网的量子安全需求：后量子经典协议依赖可能失效的计算假设，纯量子协议资源消耗过大。需要既量子抗性又高度可扩展的解决方案。

Method: 混合安全模型：使用QKD生成信息论安全密钥，结合量子安全伪随机函数(QS-PRF)，在匿名时隙预留阶段接受可量化信息泄露以换取高性能，但保持数据提交的强不可链接性。

Result: 协议通信成本从量子原生方案的O(n^4)显著降低至O(n^2)，建立了未来量子安全匿名系统的高性能基准，同时通过安全分析正式量化了匿名性降低。

Conclusion: QADR协议为大规模物联网的量子安全匿名数据报告提供了理论基准和高效架构，在性能与安全性间达成实用平衡，指明了未来量子网络的发展方向。

Abstract: The security of future large-scale IoT networks is critically threatened by the ``Harvest Now, Decrypt Later'' (HNDL) attack paradigm. Securing the massive, long-lived data streams from these systems requires protocols that are both quantum-resistant and highly scalable. Existing solutions are insufficient: post-quantum classical protocols rely on computational assumptions that may not hold for decades, while purely quantum protocols are too resource-intensive for the sheer scale of IoT. This paper introduces the Quantum Anonymous Data Reporting (QADR) protocol, a hybrid framework that provides a theoretical benchmark and high-performance architecture for this challenge, designed for future fully-connected quantum networks. The protocol achieves scalable, quantum-resistant anonymity through a hybrid security model; it leverages information-theoretically secure keys from Quantum Key Distribution (QKD) to seed a quantum-secure pseudorandom function (QS-PRF), grounding its long-term data protection in well-established computational hardness assumptions. We also propose and analyze an automated slot reservation mechanism by making a deliberate trade-off: achieving high performance by accepting a quantifiable information leak during the anonymous slot reservation phase while maintaining strong unlinkability for the final data submission. Our security analysis formally quantifies the anonymity reduction caused by the leak and discusses pathways to fully mitigate it at a significant performance cost. We prove the protocol's critical advantage as a performance benchmark: its primary communication cost scales as $O(n^2)$, a dramatic improvement over quantum-native alternatives ($O(n^4)$), establishing a high-performance goal for future quantum-secured anonymity systems.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [114] [Reservoir Computing via Multi-Scale Random Fourier Features for Forecasting Fast-Slow Dynamical Systems](https://arxiv.org/abs/2511.14775)
*S. K. Laha*

Main category: cs.NE

TL;DR: 提出了一种结合延迟嵌入和随机傅里叶特征映射的储层计算框架，用于预测具有多尺度时间结构的非线性时间序列。多尺度版本通过集成多个核带宽来同时捕捉快慢时间依赖性，在多种复杂系统中表现优于单尺度版本。


<details>
  <summary>Details</summary>
Motivation: 预测具有多尺度时间结构的非线性时间序列是复杂系统建模中的核心挑战。传统方法难以有效捕捉由快慢相互作用产生的复杂动力学行为。

Method: 开发了两种储层计算框架：单尺度RFF储层（固定核带宽）和多尺度RFF储层（集成多个核带宽）。应用于神经元模型（Rulkov映射、Izhikevich模型等）和生态模型（捕食者-猎物动力学等），这些系统表现出由快慢相互作用产生的尖峰、爆发和混沌行为。

Result: 在所有测试案例中，多尺度RFF储层始终优于单尺度版本，实现了更低的归一化均方根误差和更稳健的长期预测。

Conclusion: 将多尺度特征映射明确纳入储层计算架构，对于建模具有内在快慢相互作用的复杂动力系统非常有效。

Abstract: Forecasting nonlinear time series with multi-scale temporal structures remains a central challenge in complex systems modeling. We present a novel reservoir computing framework that combines delay embedding with random Fourier feature (RFF) mappings to capture such dynamics. Two formulations are investigated: a single-scale RFF reservoir, which employs a fixed kernel bandwidth, and a multi-scale RFF reservoir, which integrates multiple bandwidths to represent both fast and slow temporal dependencies. The framework is applied to a diverse set of canonical systems: neuronal models such as the Rulkov map, Izhikevich model, Hindmarsh-Rose model, and Morris-Lecar model, which exhibit spiking, bursting, and chaotic behaviors arising from fast-slow interactions; and ecological models including the predator-prey dynamics and Ricker map with seasonal forcing, which display multi-scale oscillations and intermittency. Across all cases, the multi-scale RFF reservoir consistently outperforms its single-scale counterpart, achieving lower normalized root mean square error (NRMSE) and more robust long-horizon predictions. These results highlight the effectiveness of explicitly incorporating multi-scale feature mappings into reservoir computing architectures for modeling complex dynamical systems with intrinsic fast-slow interactions.

</details>


### [115] [Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking](https://arxiv.org/abs/2511.15199)
*Jiajun Zhan,Zeyuan Ma,Yue-Jiao Gong,Kay Chen Tan*

Main category: cs.NE

TL;DR: 本文提出了一种基于强化学习的通用知识转移策略，通过多角色RL系统解决进化多任务优化中的知识转移问题，包括任务路由、知识控制和策略适应三个模块。


<details>
  <summary>Details</summary>
Motivation: 传统的进化多任务算法需要针对特定问题设计知识转移机制，缺乏通用性和系统性。本文旨在设计一个可泛化的知识转移策略。

Method: 构建多角色强化学习系统：任务路由代理使用注意力机制识别源-目标转移对；知识控制代理确定精英解转移比例；策略适应代理动态控制底层EMT框架的超参数。通过端到端预训练获得元策略。

Result: 综合验证实验表明该方法在代表性基准测试中达到最先进性能，深入分析揭示了系统学习到的知识转移原理。

Conclusion: 提出的基于强化学习的知识转移策略能够有效解决进化多任务优化中的知识转移问题，具有通用性和优越性能。

Abstract: Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [116] [Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution](https://arxiv.org/abs/2511.15262)
*Tomas Espana,Yadh Hafsi,Fabrizio Lillo,Edoardo Vittori*

Main category: q-fin.TR

TL;DR: 使用强化学习进行大宗交易最优执行，通过模型无关的数据驱动方法在限价订单簿模拟中训练智能体，相比传统方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统参数化方法在价格动态和影响建模方面存在局限，需要模型无关的数据驱动框架来解决大宗交易执行中的市场影响最小化问题。

Method: 采用Double Deep Q-Network强化学习算法，在基于Queue-Reactive Model生成的限价订单簿模拟环境中训练智能体，状态空间包含时间、库存、价格和深度变量。

Result: 智能体学会了既战略性又战术性的策略，能有效适应订单簿条件，在多种训练配置下均优于标准方法。

Conclusion: 模型无关的强化学习能够为最优执行问题提供自适应且稳健的解决方案。

Abstract: We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [117] [Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework](https://arxiv.org/abs/2511.14980)
*Ahmet Umur Özsoy*

Main category: q-fin.MF

TL;DR: 提出了一种用于期权定价模型校准的选择性遗忘框架，解决现有系统无法从已校准模型中移除特定数据而无需完全重新训练的问题。


<details>
  <summary>Details</summary>
Motivation: 现有期权定价模型校准系统缺乏选择性遗忘操作符，当报价数据过时、损坏或需要删除时，必须重新构建整个非线性最小二乘问题，即使只需排除少量数据。

Method: 引入参数化期权校准中的选择性遗忘（机器遗忘）原则性框架，提供稳定性保证和扰动边界。

Result: 所提出的操作符在标准正则性假设下满足局部精确性。

Conclusion: 该框架为期权定价模型校准提供了高效的数据移除机制，避免了完全重新训练的开销。

Abstract: Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [118] [Modelling and Model-Checking a ROS2 Multi-Robot System using Timed Rebeca](https://arxiv.org/abs/2511.15227)
*Hiep Hong Trinh,Marjan Sirjani,Federico Ciccozzi,Abu Naser Masud,Mikael Sjödin*

Main category: cs.RO

TL;DR: 本文展示了如何使用Timed Rebeca建模语言来设计和验证多机器人系统，通过离散化策略和优化技术解决连续系统建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统具有复杂的异步交互和并发特性，需要形式化验证来确保设计意图的正确性。模型驱动开发能够加速原型设计，而模型检查提供了自动化验证机制。

Method: 使用基于actor的Timed Rebeca建模语言，开发不同的离散化策略来处理复杂信息，识别抽象阈值，并应用优化技术提升计算效率。

Result: 成功展示了如何离散化建模连续系统以进行高效的模型检查，并建立了模型与实现之间的双向工程流程。

Conclusion: Timed Rebeca能够有效建模和验证多机器人系统，发布的代码可作为多自主机器人系统建模的基础。

Abstract: Model-based development enables quicker prototyping, earlier experimentation and validation of design intents. For a multi-agent system with complex asynchronous interactions and concurrency, formal verification, model-checking in particular, offers an automated mechanism for verifying desired properties. Timed Rebeca is an actor-based modelling language supporting reactive, concurrent and time semantics, accompanied with a model-checking compiler. These capabilities allow using Timed Rebeca to correctly model ROS2 node topographies, recurring physical signals, motion primitives and other timed and time-convertible behaviors. The biggest challenges in modelling and verifying a multi-robot system lie in abstracting complex information, bridging the gap between a discrete model and a continuous system and compacting the state space, while maintaining the model's accuracy. We develop different discretization strategies for different kinds of information, identifying the 'enough' thresholds of abstraction, and applying efficient optimization techniques to boost computations. With this work we demonstrate how to use models to design and verify a multi-robot system, how to discretely model a continuous system to do model-checking efficiently, and the round-trip engineering flow between the model and the implementation. The released Rebeca and ROS2 codes can serve as a foundation for modelling multiple autonomous robots systems.

</details>


### [119] [Decentralized Gaussian Process Classification and an Application in Subsea Robotics](https://arxiv.org/abs/2511.15529)
*Yifei Gao,Hans J. He,Daniel J. Stilwell,James McMahon*

Main category: cs.RO

TL;DR: 本文提出了一种分散式数据共享策略，用于水下机器人团队实时构建通信成功概率地图，以应对水声通信的不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决水下机器人团队在水声通信受限环境中的协调问题，包括通信范围有限、多径效应和低带宽等挑战。

Method: 采用分散式分类方法，机器人共享部分通信测量数据，通过严格推导的数据共享策略选择要共享的测量值。

Result: 使用弗吉尼亚理工大学690型AUV团队收集的真实水声通信数据进行实验验证，证明该策略在水下环境中的有效性。

Conclusion: 提出的数据共享策略能够有效帮助水下机器人团队实时构建通信成功概率地图，提升团队协调能力。

Abstract: Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [120] [Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation](https://arxiv.org/abs/2511.15445)
*Victorita Dolean,Daria Hrebenshchykova,Stéphane Lanteri,Victor Michel-Dansac*

Main category: math.NA

TL;DR: 该论文研究了有限基物理信息神经网络(FBPINNs)及其多级扩展在解决二维复杂域中高频波传播问题中的应用，作为传统有限差分和有限元方法的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在高频波传播问题的复杂二维域中面临显著计算挑战，需要更有效的替代方案。

Method: 采用有限基物理信息神经网络(FBPINNs)及其多级扩展，利用域分解技术将计算域划分为重叠子域，每个子域由局部神经网络控制。

Result: 在求解齐次亥姆霍兹方程时展示了该方法的准确性和计算效率。

Conclusion: FBPINNs方法有潜力缓解传统方法的局限性，为高频波传播问题提供有前景的替代方案。

Abstract: Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.

</details>


### [121] [Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss](https://arxiv.org/abs/2511.15530)
*Max Hirsch,Federico Pichi*

Main category: math.NA

TL;DR: 本文证明了基于神经正切核(NTK)的自适应权重在多目标优化中的收敛性，并提出了一种高效的随机化算法来估计NTK，显著降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 在多目标优化中，自适应权重选择对于平衡竞争损失至关重要，但基于NTK的权重更新方法存在收敛性不明确和计算效率低的问题。

Method: 1) 证明了梯度下降结合自适应NTK权重的收敛性；2) 开发了基于预测-校正方法和矩阵素描的随机化算法，生成NTK的无偏估计。

Result: 理论分析证实了自适应权重算法的收敛性，数值实验验证了随机化算法在保持精度的同时显著提高了计算效率。

Conclusion: 本文为基于NTK的自适应权重方法提供了理论保证，并提出了一种高效的计算方案，使其在实际应用中更加可行。

Abstract: In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [122] [Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks](https://arxiv.org/abs/2511.14962)
*Haizhou Wen,He Li,Zhen Li*

Main category: physics.comp-ph

TL;DR: 提出多保真度神经网络方法，从部分显微镜图像重建红细胞3D形态，结合高保真横截面和低保真参考形状，实现95%以上坐标精度的3D表面重建。


<details>
  <summary>Details</summary>
Motivation: 从部分观测（如显微镜图像）重建红细胞3D形态对于理解红细胞衰老生理学和各种红细胞疾病病理学至关重要。

Method: 多保真度神经网络结合卷积神经网络和前馈神经网络，利用拓扑同胚理论，通过耗散粒子动力学模拟生成训练数据，并加入表面积和体积约束进行正则化。

Result: 在正常和衰老红细胞群体中，当提供至少两个正交横截面时，可以重建复杂红细胞形态，坐标精度超过95%。斜向横截面能改善局部和全局特征重建。

Conclusion: 该方法能够从常规显微镜图像的部分横截面重建正常和衰老红细胞的3D形状，有助于正常和疾病相关红细胞样本的形态参数定量分析。

Abstract: Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [123] [Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits](https://arxiv.org/abs/2511.15173)
*Jun Hyeun Kang,Jung Eek Son,Tae In Ahn*

Main category: q-bio.QM

TL;DR: 本研究使用人工智能方法分析光谱转换膜对作物产量的影响，通过数据增强和多种机器学习模型，成功预测了光谱转换膜应用对作物产量的显著影响，准确率达到91.4%。


<details>
  <summary>Details</summary>
Motivation: 光谱转换膜在温室中的应用对作物生长产生不同响应，但仅考虑单一作物属性难以理解光质调整与作物生长性能的关系，因此需要综合考虑多种植物表型性状和生理响应。

Method: 在2021-2024年间，在覆盖PEF或SF的温室中种植多种作物，测量叶片反射率、叶质量面积比、叶绿素含量、日累积光照和光饱和点等参数，使用变分自编码器进行数据增强，并训练逻辑回归、决策树、随机森林、XGBoost和前馈神经网络等模型进行二元分类。

Result: 收集了210个数据点，SF下大多数作物产量平均增加22.5%，前馈神经网络在测试集上达到91.4%的分类准确率。

Conclusion: 本研究通过改进预测光谱转换效果的能力，深入理解了叶片表型和光合特性、环境条件与太阳光谱组分之间的复杂相互作用。

Abstract: The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [124] [How to pick the best anomaly detector?](https://arxiv.org/abs/2511.14832)
*Marie Hein,Gregor Kasieczka,Michael Krämer,Louis Moureaux,Alexander Mück,David Shih*

Main category: hep-ph

TL;DR: 本文提出了ARGOS指标，用于在模型无关的方式下选择最佳异常检测器，该指标在理论和实证上都证明能稳健地选择对给定数据最敏感的异常检测模型。


<details>
  <summary>Details</summary>
Motivation: 异常检测有潜力在数据未探索区域发现新物理，但以模型无关的方式为给定数据集选择最佳异常检测器是一个重要挑战，此前很大程度上被忽视。

Method: 引入数据驱动的ARGOS指标，该指标具有坚实的理论基础，重点关注弱监督、基于分类器的异常检测方法。

Result: ARGOS指标优于文献中使用的其他模型选择指标，特别是二元交叉熵损失。在多个实际应用中，包括超参数调优、架构和特征选择，ARGOS都对异常检测的噪声条件表现出稳健性。

Conclusion: ARGOS是一个有效的异常检测模型选择指标，能够在各种实际应用场景中稳健地识别最敏感的异常检测模型。

Abstract: Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [125] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，解决了主机处理器与PIM核心数据布局不匹配的问题，在多种PIM后端上显著提升了ML内核和LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 主机处理器和PIM核心需要不同的数据布局：主机需要跨DRAM存储体的连续元素分布，而PIM核心需要本地存储体内的元素。这种数据重排需求在ML内核执行中带来了显著的性能和可编程性挑战，且现有编译方法缺乏对多样化PIM后端的系统优化。

Method: 设计了DCC编译器，通过多层PIM抽象支持不同PIM后端的数据分布和处理策略，将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并利用快速准确的性能预测模型选择最优配置。

Result: 在各种ML内核中，DCC在HBM-PIM上实现最高7.68倍加速（平均2.7倍），在AttAcc PIM后端上实现最高13.17倍加速（平均5.75倍）。在端到端LLM推理中，DCC在AttAcc上使GPT-3和LLaMA-2加速最高达7.71倍（平均4.88倍）。

Conclusion: 数据重排和计算代码优化是相互依赖的，需要在调优过程中联合优化。DCC通过统一调优过程成功解决了这一挑战，为PIM系统上的ML工作负载提供了显著的性能提升。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [126] [Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion](https://arxiv.org/abs/2511.14969)
*Zanxu Wang,Homayoon Beigi*

Main category: eess.AS

TL;DR: 通过系统质量控制和多阶段迁移学习解决多模态对话情感识别的数据质量问题，在MELD和IEMOCAP数据集上分别达到64.8%和74.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态对话情感识别中的数据质量问题，利用身份识别嵌入来捕捉个体特定的情感表达模式。

Method: 实施质量控制流程验证说话人身份、音频-文本对齐和面部检测；使用RecoMadeEasy引擎提取512维说话人和面部嵌入；微调MPNet-v2用于情感感知文本表示；通过情感特定MLP在单模态数据集上适应特征；采用MAMBA-based三模态融合。

Result: 在MELD数据集上达到64.8%准确率，在IEMOCAP数据集上达到74.3%准确率。

Conclusion: 结合基于身份的音频和视觉嵌入与情感调优的文本表示，在质量控制的数据子集上为多模态对话情感识别提供了持续竞争性性能，并为改进低频情感类别提供了基础。

Abstract: This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [127] [Privacy-Preserving IoT in Connected Aircraft Cabin](https://arxiv.org/abs/2511.15278)
*Nilesh Vyas,Benjamin Zhao,Aygün Baltaci,Gustavo de Carvalho Bertoli,Hassan Asghar,Markus Klügel,Gerrit Schramm,Martin Kubisch,Dali Kaafar*

Main category: cs.CR

TL;DR: 提出并评估了一个在CSMIM架构上集成可配置隐私增强技术(PETs)的框架，通过差分隐私和加法秘密共享方案解决机舱物联网环境中的数据治理挑战。


<details>
  <summary>Details</summary>
Motivation: 解决机舱多供应商物联网环境中数据协作与乘客隐私、供应商知识产权及合规风险之间的冲突，填补CSMIM协议在应用层的数据治理空白。

Method: 在CSMIM架构上集成可配置的隐私增强技术层，实证分析差分隐私和加法秘密共享方案，使用资源受限硬件的高保真测试平台量化隐私、效用和性能之间的权衡。

Result: PETs的计算开销相比网络和协议延迟通常可忽略不计，架构选择(如设备端vs虚拟化处理)对端到端延迟和计算性能的影响远大于PETs本身。

Conclusion: 为系统架构师提供了实用的PETs选择和配置路线图，支持在航空电子等关键领域设计可信的协作物联网生态系统。

Abstract: The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.

</details>


### [128] [GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems](https://arxiv.org/abs/2511.15031)
*Yifan Cai,Linh Thi Xuan Phan*

Main category: cs.CR

TL;DR: GeoShield是一个针对地理分布式信息物理系统的资源高效拜占庭容错解决方案，通过检测故障并在有限时间内恢复系统来保证安全，相比传统方法显著减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有容错方法要么消耗过多资源，要么只提供最终一致性保证，不适合实时资源受限的信息物理系统。这些系统需要能够在有限时间内从故障中恢复以保证安全。

Method: GeoShield采用拜占庭容错的网络测量和跨区域遗漏故障检测协议，主动检测恶意消息延迟，并配备恢复机制在保证及时恢复的同时最大化操作鲁棒性。

Result: 评估显示GeoShield在有效性和资源效率方面显著优于现有方法，是在不可靠网络上无需可信硬件的首个有界时间恢复解决方案。

Conclusion: GeoShield通过检测而非屏蔽故障的方式，在保证系统安全的同时大幅减少了资源需求，为大规模地理分布式信息物理系统提供了实用的容错解决方案。

Abstract: Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.
  We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.

</details>


### [129] [Towards a Formal Verification of Secure Vehicle Software Updates](https://arxiv.org/abs/2511.15479)
*Martin Slind Hagen,Emil Lundqvist,Alex Phu,Yenan Wang,Kim Strandberg,Elad Michael Schiller*

Main category: cs.CR

TL;DR: 对UniSUF软件更新框架进行形式化安全分析，验证其满足机密性、完整性、真实性、新鲜性、顺序性和活跃性等安全要求。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆(SDVs)的兴起，软件漏洞对安全、经济和社会的影响日益严重，需要确保软件更新的安全性。虽然UniSUF框架已被提出，但之前的评估未使用形式化验证方法。

Method: 建立UniSUF架构和假设的模型以反映真实汽车系统，开发基于ProVerif的形式化验证框架，通过符号执行验证安全要求的满足性。

Result: 分析结果表明UniSUF符合指定的安全保证，确保其安全框架的正确性和可靠性。

Conclusion: 通过形式化安全分析验证了UniSUF框架能够满足关键安全要求，为软件定义车辆的软件更新提供了可靠的安全保障。

Abstract: With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.
  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.

</details>


### [130] [Beluga: Block Synchronization for BFT Consensus Protocols](https://arxiv.org/abs/2511.15517)
*Tasos Kichidis,Lefteris Kokoris-Kogias,Arun Koshy,Ilya Sergey,Alberto Sonnino,Mingwei Tian,Jianting Zhang*

Main category: cs.CR

TL;DR: 本文提出了Beluga，一种模块化、资源感知的区块同步器，用于现代BFT共识协议，在保持最优常见情况延迟的同时，能够抵御攻击并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有高性能BFT共识协议缺乏原则性和高效的区块交换机制，容易受到针对性攻击，在网络异步情况下性能会崩溃。

Method: 引入区块同步器的概念，作为驱动增量区块检索和执行资源感知交换的简单抽象，并开发了Beluga作为其实例化实现。

Result: 在分布式AWS部署中，Beluga在乐观路径下保持最优性能，在攻击下比先前设计提供高达3倍的吞吐量和25倍的延迟降低。Sui区块链已在生产环境中采用Beluga。

Conclusion: Beluga区块同步器为现代BFT共识协议提供了有效的区块交换机制，能够在保持高性能的同时抵御攻击，已在生产环境中得到验证。

Abstract: Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.

</details>


### [131] [Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard](https://arxiv.org/abs/2511.14876)
*Henry Wong,Clement Fung,Weiran Lin,Karen Li,Stanley Chen,Lujo Bauer*

Main category: cs.CR

TL;DR: 评估对抗性示例对自动驾驶的风险，通过在CARLA模拟器中创建对抗性补丁攻击多种驾驶代理，发现虽然某些攻击能误导ML模型，但驾驶代理的其他模块（如PID控制或GPS规则）可以覆盖被攻击的预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗性示例对单个ML模型的误导，但自动驾驶系统包含多个组件，需要评估对抗性攻击对整个驾驶代理的实际影响。

Method: 使用CARLA模拟器创建对抗性补丁，实时流式传输到模拟器中，针对CARLA Leaderboard中的高性能驾驶代理进行攻击测试，不修改任何代理代码。

Result: 某些攻击能成功误导ML模型产生错误的停止或转向命令，但部分驾驶代理的PID控制或GPS规则模块能够覆盖被攻击的预测，从而减轻攻击效果。

Conclusion: 对抗性攻击对自动驾驶系统的实际风险取决于驾驶代理的整体架构，仅针对ML模型的攻击评估可能高估了实际危害。

Abstract: To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.

</details>


### [132] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 开源模型在安全事件分类中与专有模型对比评估，结果显示专有模型准确率更高，但开源模型在隐私、成本和数据主权方面具有优势


<details>
  <summary>Details</summary>
Motivation: 评估开源模型在安全事件分类中的表现，并与专有模型进行比较，探讨开源模型在隐私保护、成本效益和数据主权方面的潜在优势

Method: 使用基于NIST SP 800-61r3分类标准的匿名真实事件数据集，应用五种提示工程技术（PHP、SHP、HTP、PRP和ZSL）进行处理

Result: 专有模型准确率更高，但本地部署的开源模型在隐私保护、成本效益和数据主权方面表现更好

Conclusion: 虽然专有模型在准确率上仍占优势，但开源模型在隐私、成本和数据主权方面的优势使其成为某些应用场景的可行替代方案

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [133] [Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets](https://arxiv.org/abs/2511.15222)
*Pol Benítez,Cibrán López,Edgardo Saucedo,Teruyasu Mizoguchi,Claudio Cazorla*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究评估了两种不同数据集训练图神经网络(GNN)模型的效果：随机生成原子配置和基于晶格振动的物理信息采样。研究发现物理信息采样模型在预测光电材料电子和机械性能方面表现更优，且所需数据点更少。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型性能严重依赖训练数据的质量、规模和多样性。在材料科学中，低对称性原子配置（包含热激发、结构缺陷和化学无序）在大多数数据集中代表性不足，这限制了ML模型在能源转换和光子学等关键技术领域的预测能力。

Method: 使用图神经网络(GNN)模型，比较两种数据集训练效果：随机生成原子配置和基于晶格振动的物理信息采样。以典型光电材料家族在有限温度条件下的电子和机械性能预测为案例研究。

Result: 基于晶格振动的物理信息采样模型在预测性能上始终优于随机训练模型，尽管依赖更少的数据点。可解释性分析显示高性能模型更重视控制性能变化的化学意义键。

Conclusion: 更大的数据集不一定产生更好的GNN预测模型，物理指导的数据生成策略能够高效构建高质量训练数据，为材料信息学提供了简单通用的方法。

Abstract: Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [134] [RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue](https://arxiv.org/abs/2511.15698)
*Naveen Raman,Jingwu Tang,Zhiyu Chen,Zheyuan Ryan Shi,Sean Hudson,Ameesh Kapoor,Fei Fang*

Main category: cs.CY

TL;DR: 开发了RescueLens工具，利用大语言模型自动分析食品救援志愿者的反馈，帮助组织者识别问题、建议跟进对象并更新志愿者指南，显著提高了反馈处理效率。


<details>
  <summary>Details</summary>
Motivation: 食品救援组织需要手动监控志愿者反馈，过程繁琐且劳动密集，难以优先处理最重要的问题。

Method: 与412 Food Rescue合作设计RescueLens工具，使用LLM自动分类志愿者反馈，建议需要跟进的捐赠者和接收者，并根据反馈更新志愿者指南。

Result: RescueLens在标注数据集上能恢复96%的志愿者问题，准确率达71%。通过按问题率排名，可识别出仅占0.5%但负责超过30%问题的捐赠者。

Conclusion: RescueLens已在412 Food Rescue部署，通过半结构化访谈发现该工具简化了反馈流程，帮助组织者更好地分配时间。

Abstract: Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [135] [Convex Clustering Redefined: Robust Learning with the Median of Means Estimator](https://arxiv.org/abs/2511.14784)
*Sourav De,Koustav Chowdhury,Bibhabasu Mandal,Sagar Ghosh,Swagatam Das,Debolina Paul,Saptarshi Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种结合凸聚类和中位数均值估计器的鲁棒聚类方法，能够抵抗异常值且无需预先指定聚类数量


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法需要预先指定聚类数量且对初始化敏感，凸聚类虽然稳定但处理高维数据和噪声时存在挑战，强融合正则化也会阻碍有效聚类形成

Method: 将凸聚类与中位数均值估计器集成，利用MoM的鲁棒性和凸聚类的稳定性，构建抗异常值的聚类框架

Result: 理论分析显示在特定条件下具有弱一致性，在合成和真实数据集上的实验验证了该方法优于现有方法

Conclusion: 该方法通过结合凸聚类和MoM，实现了对异常值鲁棒且高效的聚类，特别适用于大规模数据集

Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.

</details>


### [136] [Implicit Bias of the JKO Scheme](https://arxiv.org/abs/2511.14827)
*Peter Halmos,Boris Hanin*

Main category: stat.ML

TL;DR: 本文分析了JKO方案的二阶隐式偏差，发现它等价于在修改的能量函数J^η上进行Wasserstein梯度流，其中J^η通过减去J的度量曲率平方项得到。


<details>
  <summary>Details</summary>
Motivation: 理解JKO方案相比其他一阶积分器的独特性质，如能量耗散保持和无条件稳定性，并揭示其高阶行为。

Method: 通过理论分析表征JKO方案在η二阶项的隐式偏差，证明其等价于在修改能量J^η上的Wasserstein梯度流。

Result: 发现JKO方案在二阶近似下等价于最小化J^η = J - (η/4)∫∥∇_g(δJ/δρ)∥²ρ(dx)，这对应于对常见泛函的典型隐式偏差。

Conclusion: JKO方案在J的度量曲率快速变化的方向上引入了二阶减速效应，这解释了其在各种优化问题中的优越性能。

Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.

</details>


### [137] [Latent space analysis and generalization to out-of-distribution data](https://arxiv.org/abs/2511.15010)
*Katie Rainey,Erin Hausmann,Donald Waagen,David Gray,Donald Hulsey*

Main category: stat.ML

TL;DR: 研究表明，潜在空间中的离群分布检测不能作为深度学习模型性能的代理指标。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习系统在潜在决策空间中数据点之间的关系对于评估和解释系统在真实世界数据上的性能至关重要。检测深度学习系统的离群分布数据仍然是一个活跃的研究课题。

Method: 使用开源模拟和实测合成孔径雷达数据集，实证研究潜在空间离群分布检测与模型分类准确性之间的联系。

Result: 实证证明离群分布检测不能用作模型性能的代理衡量指标。

Conclusion: 希望激发对潜在空间几何特性的进一步研究，这可能为深度学习鲁棒性和泛化性提供未来见解。

Abstract: Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.

</details>


### [138] [Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit](https://arxiv.org/abs/2511.15120)
*Bohan Zhang,Zihao Wang,Hengyu Fu,Jason D. Lee*

Main category: stat.ML

TL;DR: 该论文证明了两层神经网络通过分层梯度下降可以最优地学习高斯多索引模型，样本和时间复杂度都达到了信息论极限，并且发现第一层需要训练超过常数步才能获得最优结果。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何高效学习高维特征，特别是探索在表示学习标准设置下，神经网络通过梯度下降学习高斯多索引模型的能力。

Method: 使用标准两层神经网络，通过分层梯度下降训练，证明内层权重可以执行幂迭代过程，隐式模拟隐藏子空间的谱启动，最终消除有限样本噪声并恢复该子空间。

Result: 在通用非退化链接函数假设下，神经网络可以用O(d)样本和O(d²)时间以o_d(1)测试误差不可知地学习目标，样本和时间复杂度均达到信息论极限。

Conclusion: 该工作证明了神经网络在样本和时间效率方面有效学习层次函数的能力，发现第一层需要训练超过常数步才能获得最优结果。

Abstract: In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.

</details>


### [139] [Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings](https://arxiv.org/abs/2511.15146)
*Eugene Ndiaye*

Main category: stat.ML

TL;DR: 本文提出了基于最优传输(OT)的向量值共形预测方法，将共形预测扩展到多维度，并首次构建了具有有限样本校准保证的多变量共形预测分布(CPD)。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测(CP)仅限于标量值，无法处理向量值分数。虽然最优传输(OT)提供了向量排序的方法，但只有渐近覆盖保证。需要将有限样本、分布无关的覆盖保证扩展到多维度。

Method: 通过共形化向量值OT分位数区域来定义候选输出的秩，使用校准分数和候选分数计算的传输映射。证明最优分配在分数空间的固定多面体分区上是分段常数。

Result: 构建了首个具有有限样本校准的多变量共形预测分布(CPD)，包括保守版本和精确随机化版本，后者是多变量经典Dempster-Hill程序的推广。

Conclusion: 该方法将共形预测扩展到多维度，不仅指示哪些结果是合理的，还能提供它们的相对似然性，解决了预测集只能指示合理性而不能提供相对似然性的深层限制。

Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.

</details>


### [140] [Particle Monte Carlo methods for Lattice Field Theory](https://arxiv.org/abs/2511.15196)
*David Yallup*

Main category: stat.ML

TL;DR: GPU加速的粒子方法（SMC和嵌套采样）在标量场理论基准测试中，在样本质量和运行时间上匹配或优于最先进的神经采样器，同时还能估计配分函数。


<details>
  <summary>Details</summary>
Motivation: 高维多模态采样问题已成为机器学习辅助采样方法的重要基准，需要评估经典方法与神经采样器的性能对比。

Method: 使用GPU加速的粒子方法，包括顺序蒙特卡洛（SMC）和嵌套采样，仅采用单一数据驱动的协方差矩阵进行调优，无需问题特定结构。

Result: 这些方法在样本质量和运行时间上达到竞争性性能，匹配或优于最先进的神经采样器，同时还能估计配分函数。

Conclusion: 这些经典方法为学习型提案设定了更高的性能标准，只有当其训练成本得到充分证明时才值得使用。

Abstract: High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.

</details>


### [141] [Robust Bayesian Optimisation with Unbounded Corruptions](https://arxiv.org/abs/2511.15315)
*Abdelhamid Ezzerg,Ilija Bogunovic,Jeremias Knoblauch*

Main category: stat.ML

TL;DR: 提出了RCGP-UCB算法，通过结合鲁棒共轭高斯过程和UCB方法，解决了贝叶斯优化对极端异常值的脆弱性问题，能够在异常值幅度无限的情况下实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒方法假设累积腐败预算有界，这使得它们即使面对单个足够大的腐败也无法防御。需要一种新方法，其预算仅在腐败频率上有界，而不在幅度上有界。

Method: 引入RCGP-UCB算法，将上置信界方法与鲁棒共轭高斯过程相结合，提出了稳定和自适应两个版本。

Result: 算法在存在O(T^{1/2})和O(T^{1/3})腐败的情况下仍能实现次线性遗憾，且腐败幅度可能无限。在没有异常值时，其遗憾界与标准GP-UCB算法相匹配。

Conclusion: RCGP-UCB在几乎零成本的情况下提供了强大的鲁棒性，能够抵御幅度无限的异常值，同时在没有异常值时保持标准算法的性能。

Abstract: Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.

</details>


### [142] [Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss](https://arxiv.org/abs/2511.15332)
*The Tien Mai*

Main category: stat.ML

TL;DR: 提出了指数Lasso方法，通过指数型损失函数在Lasso框架中实现稳健的变量选择和参数估计，在保持高斯噪声下统计效率的同时，对异常值和重尾噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统Lasso方法依赖平方损失函数，对异常值和重尾噪声非常敏感，导致模型选择不可靠和估计偏差。需要开发一种既能保持统计效率又具有鲁棒性的方法。

Method: 在Lasso框架中引入指数型损失函数，通过平滑递减的方式削弱极端异常值的影响，同时保持对小误差的近二次行为。使用Majorization-Minimization算法高效优化，迭代求解加权Lasso子问题。

Result: 理论保证显示指数Lasso在理想条件下达到经典Lasso的收敛速率，在重尾污染下保持鲁棒性。数值实验表明该方法在污染设置下优于经典Lasso，在高斯噪声下仍保持强性能。

Conclusion: 指数Lasso是一种有效的稳健变量选择方法，在多种噪声条件下均表现出色，填补了传统Lasso在鲁棒性方面的不足。

Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso

</details>


### [143] [Gini Score under Ties and Case Weights](https://arxiv.org/abs/2511.15446)
*Alexej Brauer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: 本文讨论了基尼分数在统计建模中的应用，特别是在处理排名并列和案例权重时的扩展方法。


<details>
  <summary>Details</summary>
Motivation: 基尼分数在二元响应模型中应用广泛，但在处理连续分布函数时假设风险排名由连续分布函数生成。本文旨在解决风险排名中出现并列情况时如何使用基尼分数，并适应精算中常见的案例权重情况。

Method: 通过洛伦兹曲线和集中曲线将基尼分数从二元响应扩展到一般实值随机变量，讨论在风险排名出现并列时基尼分数的使用方法，并调整基尼分数以适应案例权重的精算场景。

Result: 提出了处理风险排名并列情况的基尼分数计算方法，并开发了适用于案例权重的基尼分数扩展版本。

Conclusion: 基尼分数可以有效地扩展到处理风险排名并列的情况，并且能够适应精算中常见的案例权重场景，为统计建模和机器学习中的模型验证和选择提供了更灵活的工具。

Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.

</details>


### [144] [A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation](https://arxiv.org/abs/2511.15543)
*Georgios Venianakis,Constantinos Theodoropoulos,Michail Kavousanakis*

Main category: stat.ML

TL;DR: 提出了一个基于PINNs的框架，同时解决最优传感器布局和参数估计问题，通过将参数作为额外输入训练PINN模型，利用自动微分计算灵敏度函数，并采用D最优性准则确定最优传感器位置。


<details>
  <summary>Details</summary>
Motivation: 在分布式参数系统中，传感器布局对参数估计精度至关重要，但现有PINNs方法很少关注传感器位置对性能的影响。

Method: 训练包含待估参数作为额外输入的PINN模型，通过自动微分计算灵敏度函数，应用D最优性准则优化传感器布局。

Result: 在两个反应-扩散-对流问题上的验证表明，该方法比直觉或随机传感器布局获得更高的参数估计精度。

Conclusion: 所提出的PINNs框架能有效同时优化传感器布局和参数估计，显著提高参数估计精度。

Abstract: Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.

</details>


### [145] [Near-optimal delta-convex estimation of Lipschitz functions](https://arxiv.org/abs/2511.15615)
*Gábor Balázs*

Main category: stat.ML

TL;DR: 提出了一种可处理的算法，用于从噪声观测中估计未知Lipschitz函数，并建立了其收敛率的上界。该方法将max-affine方法从凸形状限制回归扩展到更一般的Lipschitz设置。


<details>
  <summary>Details</summary>
Motivation: 将max-affine方法扩展到Lipschitz函数估计，利用delta-凸函数作为Lipschitz函数的通用逼近器，同时保持其Lipschitz常数。

Method: 使用非线性特征扩展将max-affine函数映射到delta-凸函数子类，结合自适应分区捕获内在维度、基于惩罚的正则化机制，以及两阶段优化过程（凸初始化和局部细化）。

Result: 估计器在平方损失和次高斯分布的随机设计设置下，相对于数据内在维度达到了极小极大收敛率（最多对数因子）。

Conclusion: 该框架在实验中表现出与其他理论证明方法（包括最近邻和基于核的回归器）相竞争的性能，并且可以轻松适应凸形状限制回归。

Abstract: This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.

</details>


### [146] [Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities](https://arxiv.org/abs/2511.15634)
*Benjamin Dupuis,Mert Gürbüzbalaban,Umut Şimşekli,Jian Wang,Sinan Yildirim,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文提出了首个针对重尾随机微分方程及其离散化版本的Rényi差分隐私保证，解决了现有方法在维度依赖性和隐私概念扩展方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私研究主要关注轻尾噪声，而重尾SGD的DP保证虽然已有初步结果，但存在对参数数量的强依赖且无法扩展到RDP等其他隐私概念。

Method: 基于新的Rényi流计算和已建立的分数Poincaré不等式，在满足这些不等式的前提下推导重尾SDE的RDP保证。

Result: 获得了比现有方法维度依赖更弱的DP保证，能够扩展到Rényi差分隐私概念。

Conclusion: 提出的框架为重尾算法提供了更优的隐私保护保证，显著降低了维度依赖性，并成功扩展到RDP隐私概念。

Abstract: Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.

</details>


### [147] [Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion](https://arxiv.org/abs/2511.15679)
*Jianqiao Mao,Max A. Little*

Main category: stat.ML

TL;DR: 本文提出了前门可约性(FDR)概念，将经典前门准则扩展到更复杂的因果图中，通过聚合变量为超节点来简化干预分布估计。


<details>
  <summary>Details</summary>
Motivation: 经典前门准则适用性有限，而ID算法虽然通用但得到的表达式往往复杂难用。作者认为前门准则的适用性实际上更广泛，许多复杂因果图可以简化为前门设置。

Method: 引入前门可约性(FDR)作为有向无环混合图(ADMG)的图形条件，通过将变量聚合为超节点(FDR三元组)来简化因果图。提出了FDR-TID算法来检测可接受的FDR三元组。

Result: 证明了FDR准则满足性与FDR调整适用性之间的图级等价性。FDR-TID算法被证明具有正确性、完备性和有限终止性。

Conclusion: FDR通过优先考虑可解释性和计算简单性，在不牺牲混合图通用性的情况下补充了现有的识别方法，为复杂因果图提供了简单可估计的调整公式。

Abstract: Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\left(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*}\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [148] [HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation](https://arxiv.org/abs/2511.14897)
*Pranav Indrakanti,Ivor Simpson*

Main category: cs.CV

TL;DR: 提出了一种无监督的单图像双向MRI合成器，能够在高场和超低场MRI图像之间进行双向转换，基于物理原理模拟对比度变化。


<details>
  <summary>Details</summary>
Motivation: 现有MRI合成模型缺乏对高场和超低场MRI之间对比度变化的物理原理考虑，需要开发基于物理驱动的双向合成方法。

Method: 前向模型通过估计组织类型信噪比来模拟高场到超低场的转换；超分辨率任务使用隐式神经表示网络同时预测组织类型分割和图像强度。

Result: 合成超低场图像的WM-GM对比度提高了52%，64mT图像提高了37%；敏感性实验显示模型对目标对比度、噪声和初始种子的变化具有鲁棒性。

Conclusion: 该方法成功实现了基于物理原理的双向MRI合成，在对比度改善和模型鲁棒性方面表现出色。

Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.

</details>


### [149] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 提出基于深度学习的自适应控制方法，通过材料分类和烟雾检测动态调节激光切割机抽气泵功率，实现20-50%的能耗降低


<details>
  <summary>Details</summary>
Motivation: 解决激光切割中能耗高和环境影响的挑战，针对CO2激光抽气泵缺乏自适应控制和开环运行的问题

Method: 采用闭环配置，结合基于无透镜散斑传感的定制CNN和基于USB摄像头与VGG16迁移学习的材料分类方法，以及烟雾检测DL模型

Result: 实验证明抽气泵能耗降低20-50%，系统能在空闲时自动停止，运行时动态调节功率

Conclusion: 该方法显著提升了制造业的可持续发展能力，为激光切割能耗优化提供了有效解决方案

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [150] [Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation](https://arxiv.org/abs/2511.14981)
*Nicholas Cooper,Lijun Chen,Sailesh Dwivedy,Danna Gurari*

Main category: cs.CV

TL;DR: 提出了一种仅使用特征知识蒸馏的框架，无需logit损失函数，通过知识质量指标选择最有效的教师层进行蒸馏，在多个数据集和模型上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征知识蒸馏方法通常结合logit损失和中间层特征损失，本文旨在探索仅使用特征损失进行知识蒸馏的可能性。

Method: 利用潜在表示几何特性的新发现，引入知识质量指标来识别哪些教师层能提供最有效的蒸馏知识，构建纯特征知识蒸馏框架。

Result: 在三个图像分类数据集和四种不同的师生模型对上，该方法实现了最先进的性能，相比标准方法top-1准确率提升高达15%。

Conclusion: 纯特征知识蒸馏框架是有效的，通过精心选择教师层可以显著提升学生模型的性能，无需依赖logit损失函数。

Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.

</details>


### [151] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0是一个用于高分辨率图像和10秒视频生成的最先进基础模型系列，包含三个核心模型：6B参数的图像生成模型、2B参数的轻量视频生成模型和19B参数的高质量视频生成模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个大规模、公开可用的生成框架，推动高质量生成模型的发展和可访问性。

Method: 采用多阶段训练流程，包括数据收集、处理、过滤和聚类的完整数据管理生命周期，以及预训练、自监督微调(SFT)和基于强化学习(RL)的后训练等质量增强技术。

Result: Kandinsky 5.0在人类评估中展现出最先进的性能，实现了高生成速度，并支持广泛的生成应用。

Conclusion: 通过开源代码和训练检查点的发布，Kandinsky 5.0将显著推进研究社区中高质量生成模型的开发和可访问性。

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [152] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出基于结构化复值2D高斯基元的新全息图表示方法，将参数搜索空间减少10:1，实现更低VRAM使用和更快优化速度，同时生成更高保真度重建


<details>
  <summary>Details</summary>
Motivation: 传统全息图表示需要逐像素信息存储，参数搜索空间大，计算效率低，需要更高效的表示方法来提升下一代计算机生成全息系统的可扩展性

Method: 开发基于结构化复值2D高斯基元的表示方法，配合可微分光栅化器和GPU优化的自由空间光传播核，实现端到端训练，并引入转换程序适配实际全息图格式

Result: 相比现有方法，VRAM使用降低2.5倍，优化速度提升50%，重建保真度更高，能有效抑制先前方法中观察到的噪声伪影

Conclusion: 通过减少全息图参数搜索空间，该方法为下一代计算机生成全息系统提供了更可扩展的全息图估计方案

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [153] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: WaveFuse-AL是一个多策略主动学习框架，通过周期性时间先验和性能驱动自适应融合BALD、BADGE、Entropy和CoreSet四种采集策略，在医学影像任务中显著提升性能并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 单个主动学习策略在不同学习阶段表现不一致，需要开发能够自适应融合多种策略的方法来最大化有限标注预算的效用。

Method: 提出WaveFuse-AL框架，将周期性（正弦）时间先验与性能驱动自适应相结合，动态调整策略重要性，融合BALD、BADGE、Entropy和CoreSet四种采集策略。

Result: 在APTOS-2019、RSNA肺炎检测和ISIC-2018三个医学影像基准测试中，WaveFuse-AL在十二个指标中的十个上取得统计显著性能提升，优于单策略和交替策略基线。

Conclusion: WaveFuse-AL通过自适应多策略融合有效解决了主动学习中策略不一致性问题，在医学影像任务中实现了更好的性能和标注效率。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [154] [DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging](https://arxiv.org/abs/2511.15151)
*Meihua Zhou,Xinyu Tong,Jiarui Zhao,Min Cheng,Li Yang,Lei Tian,Nan Wan*

Main category: cs.CV

TL;DR: DCL-SE是一个端到端框架，通过数据驱动的时空编码和动态课程学习策略，将3D脑数据高效编码为2D动态表示，在多个神经影像任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维神经影像分析中时空保真度不足和大规模通用模型适应性有限的问题。

Method: 使用近似秩池化(ARP)将3D脑体积数据编码为信息丰富的2D动态表示，并通过动态课程学习策略逐步训练解码器，从全局解剖结构到精细病理细节逐步提取特征。

Result: 在六个公开数据集（包括阿尔茨海默病和脑肿瘤分类、脑动脉分割和脑年龄预测）上评估，DCL-SE在准确性、鲁棒性和可解释性方面始终优于现有方法。

Conclusion: 研究结果强调了在大规模预训练网络时代，紧凑、任务特定架构的关键重要性。

Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

</details>


### [155] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 提出了一种结构感知的手术反馈生成管道，通过从真实训练师-学员对话中学习手术动作本体，使用仪器-动作-目标三元组来指导GPT-4o生成临床基础的手术反馈。


<details>
  <summary>Details</summary>
Motivation: 高质量的手术中反馈对学员技能提升至关重要，但自动化生成自然、训练师风格的反馈需要理解临床相关表征的模型。

Method: 1) 从真实反馈文本中挖掘IAT三元组并聚类归一化；2) 微调视频到IAT模型，利用手术程序和任务上下文以及细粒度时间仪器运动；3) 使用IAT三元组表示指导GPT-4o生成临床基础反馈。

Result: 视频到IAT识别任务中AUC持续提升（仪器：0.67到0.74；动作：0.60到0.63；组织：0.74到0.79）。反馈生成任务中，IAT条件化将GPT-4o的保真度从2.17提升到2.44，可接受生成比例从21%翻倍到42%。

Conclusion: 基于明确IAT结构的生成提高了保真度并产生临床可验证的理由，支持在手术训练中的可审计使用。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [156] [BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI](https://arxiv.org/abs/2511.15188)
*Wasif Jalal,Md Nafiu Rahman,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 提出BrainRotViT混合架构，结合ViT的全局上下文建模和残差CNN的局部细化，在11个MRI数据集上实现3.34年MAE，优于现有方法，并展示了与神经退行性疾病的关联。


<details>
  <summary>Details</summary>
Motivation: 传统回归和CNN方法存在手动特征工程、感受野有限和异质数据过拟合问题，纯transformer模型需要大数据集和高计算成本，需要一种高效且可泛化的脑龄预测方法。

Method: 提出BrainRotViT混合架构：首先在辅助年龄和性别分类任务上训练ViT编码器学习切片级特征，然后将冻结的编码器应用于所有矢状切片生成嵌入向量2D矩阵，输入到残差CNN回归器中，在最终全连接层加入受试者性别信息来估计连续脑龄。

Result: 在11个MRI数据集（超过130个采集点）验证集上达到3.34年MAE（Pearson r=0.98，Spearman ρ=0.97，R²=0.95），优于基线和最先进模型。在4个独立队列中MAE在3.77-5.04年之间，展示了良好的泛化能力。

Conclusion: 该方法提供了一个高效、可解释且可泛化的脑龄预测框架，弥合了CNN和transformer方法之间的差距，为衰老和神经退行性疾病研究开辟了新途径。

Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.

</details>


### [157] [Graph Query Networks for Object Detection with Automotive Radar](https://arxiv.org/abs/2511.15271)
*Loveneet Saini,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出Graph Query Networks (GQN)框架，通过图查询在BEV空间中动态构建对象特定图，解决3D雷达检测中稀疏不规则反射的挑战，在NuScenes数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 3D雷达检测面临长波长导致的稀疏不规则反射问题，传统基于网格和序列的卷积和变换器检测器难以有效处理。

Method: 使用图查询动态关注BEV空间构建对象特定图，包含EdgeFocus模块进行关系推理和DeepContext Pooling模块进行上下文聚合。

Result: 在NuScenes数据集上相对mAP提升高达+53%，比最强雷达方法提升+8.2%，同时将峰值图构建开销降低80%，FLOPs成本适中。

Conclusion: GQN框架通过建模雷达感知对象为图，有效提取个体化关系和上下文特征，显著提升3D雷达检测性能。

Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.

</details>


### [158] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种轻量级、模型无关的后处理框架，用于无人机导航中的开放集检测，能够同时处理已知目标、未知物体和背景的三分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放集检测方法通常依赖单一不确定性评分和阈值处理，灵活性有限且容易将未知物体与背景混淆，无法满足无人机导航对安全性的要求。

Method: 采用融合方案聚合多个置信度估计和每检测特征，使用紧凑的多层感知机进行三分类，整合不同logit变体提升性能而不影响吞吐量。

Result: 在二分类中平均AUROC提升2.7%，保持或改进开放集mAP，在三分类中实现稳健性能，封闭集mAP最高提升9个点（相对增益18%）。

Conclusion: 该方法超越了基于阈值的基线方法，为无人机安全导航提供了关键的三分类能力，能够主动避免未知物体并安全忽略背景区域。

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [159] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 提出了一种用于医学图像分割的后处理框架，通过保形预测构建置信掩码，在图像级别控制假阳性预测，提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 深度分割模型很少提供明确的统计误差保证，这在临床决策中至关重要。需要一种方法能够控制假阳性预测，特别是在过度分割可能带来临床后果的场景下。

Method: 使用预训练分割模型，通过增加得分阈值或形态学腐蚀构建嵌套的收缩掩码族。利用标定集通过保形预测选择收缩参数，确保新图像中假阳性比例低于用户指定容忍度。

Result: 在息肉分割基准测试中实现了目标级别的经验有效性，方法具有模型无关性，无需重新训练，并提供有限样本保证。

Conclusion: 该框架在过度分割可能产生临床后果的场景下，实现了实用的、风险感知的分割，为临床决策提供了统计可靠性保证。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [160] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: D4C是首个针对CLIP模型的数据自由量化框架，通过语义注入、结构对比生成和扰动增强三个组件，解决了现有DFQ方法在CLIP上性能大幅下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由量化方法在扩展到视觉语言模型（如CLIP）时存在性能显著下降的问题，主要原因是合成样本语义内容不足和图像内多样性低。

Method: 提出D4C框架，包含三个关键组件：提示引导语义注入、结构对比生成和扰动感知增强，用于生成语义丰富且结构多样的伪图像。

Result: 在W4A8设置下，D4C在多个数据集上显著提升性能，如CLIP ResNet-50在CIFAR-10上Top-1准确率提升12.4%，ViT-B/32提升18.9%。

Conclusion: D4C有效解决了CLIP模型数据自由量化的挑战，通过生成高质量的合成图像显著提升了量化性能，为隐私敏感场景下的模型压缩提供了实用解决方案。

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [161] [SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome](https://arxiv.org/abs/2511.15464)
*Dabin Jeong,Amirhossein Vahidi,Ciro Ramírez-Suástegui,Marie Moullet,Kevin Ly,Mohammad Vali Sanian,Sebastian Birk,Yinshui Chang,Adam Boxall,Daniyal Jafree,Lloyd Steele,Vijaya Baskar MS,Muzlifah Haniffa,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: Sigmma是一个多模态对比对齐框架，用于学习HE图像和空间转录组谱的多尺度层次表示，通过多尺度对比对齐和图结构建模细胞相互作用，显著提升了基因表达预测和跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在单一尺度上对齐HE图像和ST谱，忽略了细粒度细胞结构及其空间组织，需要开发能够捕捉多层次组织结构的框架。

Method: 提出Sigmma框架，采用多尺度对比对齐确保不同尺度的表示在模态间保持一致，并通过图结构建模细胞间相互作用，整合图间和图内关系来捕捉组织微环境中的细胞相互作用。

Result: Sigmma在基因表达预测任务中平均提升9.78%，在跨模态检索任务中平均提升26.93%，并在下游分析中学习到有意义的多组织组织结构。

Conclusion: Sigmma通过多尺度对比对齐和图结构建模，能够有效学习HE图像和空间转录组谱的层次表示，显著提升了计算病理学中的多模态表示学习性能。

Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.

</details>


### [162] [RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection](https://arxiv.org/abs/2511.15476)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出了一种混合深度学习框架RS-CA-HSICT，结合CNN和Transformer的优势来增强MPox检测，在Kaggle基准数据集上达到98.30%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用CNN和Transformer各自的优势，CNN擅长提取局部特征，Transformer擅长捕捉长距离依赖关系，结合两者可以更好地检测MPox病变的细微模式和对比度变化。

Method: 构建了包含HSICT块、残差CNN模块、空间CNN块和通道增强的混合架构。HSICT模块集成CNN主干和定制ICT块，用于多头注意力和结构化CNN层。通过逆残差学习解决梯度消失，阶段式分辨率降低确保尺度不变性。通道融合注意力块筛选有区分度的通道。

Result: 在Kaggle基准和多样化MPox数据集上，分类准确率达到98.30%，F1分数达到98.13%，优于现有的CNN和ViT模型。

Conclusion: 提出的RS-CA-HSICT框架通过有效结合CNN和Transformer的优势，在MPox检测任务中表现出色，能够捕捉全局和局部结构线索、细微纹理和对比度变化。

Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.

</details>


### [163] [US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery](https://arxiv.org/abs/2511.15600)
*Miruna-Alexandra Gafencu,Yordanka Velikova,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态深度学习方法，利用单张X射线图像信息来补全3D超声中被遮挡的脊柱解剖结构，解决超声成像中骨骼遮挡导致的结构不完整问题。


<details>
  <summary>Details</summary>
Motivation: 超声成像在脊柱手术中具有无辐射、成本低、实时可视化等优势，但受限于骨骼声影效应，无法完整显示椎体等解剖结构。需要解决超声成像中结构不完整的问题。

Method: 生成配对的训练数据：模拟X射线扫描的2D侧位椎体视图和模拟超声成像中有限可见性的3D部分椎体表示。通过多模态深度学习整合两种成像模式的形态信息。

Result: 在椎体重建方面相比现有3D超声椎体补全方法有显著改进(p < 0.001)，实现了更准确、完整的腰椎体积可视化，无需与术前CT等模式配准。

Conclusion: 整合单张X射线投影能够缓解超声成像的关键限制，同时保留其作为主要成像模式的优势，为未来临床转化奠定了基础。

Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete

</details>


### [164] [Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.15633)
*Tao Hu,Lan Li,Zhen-Hao Xie,Da-Wei Zhou*

Main category: cs.CV

TL;DR: HASTEN方法通过将层次语义信息嵌入到双曲空间中，有效缓解了类增量学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP的类增量学习方法未能显式捕捉视觉和语言概念的内在层次结构，导致细粒度类别特征在增量更新时发生漂移，最终造成灾难性遗忘。

Method: 1. 使用外部知识图谱作为监督，在双曲空间中嵌入视觉和文本特征以保持层次结构；2. 将梯度投影到共享双曲映射器的零空间，防止对先前任务的干扰。

Result: 大量实验表明HASTEN持续优于现有方法，同时提供统一的结构化表示。

Conclusion: HASTEN通过层次语义树锚定机制，成功缓解了类增量学习中的灾难性遗忘问题，并保持了概念的层次关系。

Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.

</details>


### [165] [VisPlay: Self-Evolving Vision-Language Models from Images](https://arxiv.org/abs/2511.15661)
*Yicheng He,Chengsong Huang,Zongxia Li,Jiaxin Huang,Yonghui Yang*

Main category: cs.CV

TL;DR: VisPlay是一个自进化的强化学习框架，让视觉语言模型能够使用大量未标注图像数据自主提升推理能力，通过角色分工和GRPO优化实现视觉推理、组合泛化和幻觉减少的持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖人工标注标签或任务特定启发式来定义可验证奖励，成本高且难以扩展，需要一种能够自主改进视觉语言模型推理能力的可扩展方法。

Method: 将基础VLM分配到两个交互角色：图像条件提问者生成具有挑战性但可回答的视觉问题，多模态推理者生成银标回答。使用GRPO联合训练，结合多样性和难度奖励来平衡问题复杂度和回答质量。

Result: 在Qwen2.5-VL和MiMo-VL两个模型系列上训练，VisPlay在8个基准测试（包括MM-Vet和MMMU）中实现了视觉推理、组合泛化和幻觉减少的持续改进。

Conclusion: VisPlay为自进化多模态智能提供了一条可扩展的路径，能够有效利用未标注数据自主提升模型性能。

Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/

</details>
