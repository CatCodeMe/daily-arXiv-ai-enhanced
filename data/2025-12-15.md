<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 60]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.IR](#cs.IR) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems](https://arxiv.org/abs/2512.11001)
*Zoi Kaoudi,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 本文提出针对多智能体工作流的新一代查询优化框架愿景，旨在解决当前智能体架构缺乏通用性、可扩展性和系统优化的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及加速了基于智能体的工作流应用，但现有构建方法缺乏通用性、可扩展性和系统优化。当前系统通常依赖固定模型和单一执行引擎，无法有效优化跨异构数据源和查询引擎的多智能体操作。

Method: 基于真实案例和多智能体工作流分析，提出新一代查询优化框架架构愿景，重新设计查询优化原则以应对新挑战：多样化智能体编排、昂贵LLM调用下的成本效率、跨异构引擎优化以及任务冗余处理。

Result: 提出了一个多智能体查询优化框架的愿景架构，旨在实现自动化模型选择、工作流组合和跨异构引擎执行，为新兴多智能体架构中的查询优化奠定基础。

Conclusion: 该愿景为多智能体架构中的查询优化建立了基础，并开启了一系列未来研究方向，包括智能体编排、成本优化和跨引擎执行等关键挑战。

Abstract: The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.

</details>


### [2] [KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration](https://arxiv.org/abs/2512.11067)
*Guorui Xiao,Enhao Zhang,Nicole Sullivan,Will Hansen,Magdalena Balazinska*

Main category: cs.DB

TL;DR: KathDB是一个结合关系型语义与基础模型多模态推理能力的新系统，通过人机交互通道实现可解释的多模态查询


<details>
  <summary>Details</summary>
Motivation: 传统DBMS只能处理结构化数据且复杂SQL难写，而现代多模态系统要么需要用户手动使用机器学习UDF，要么完全依赖黑盒LLM，牺牲了可用性或可解释性

Method: 提出KathDB系统，将关系型语义与基础模型在多模态数据上的推理能力相结合，并在查询解析、执行和结果解释阶段加入人机交互通道

Result: 用户能够迭代地获得跨数据模态的可解释答案，系统结合了关系型数据库的语义保证和基础模型的多模态推理能力

Conclusion: KathDB通过融合关系型语义与基础模型推理，并引入人机交互，解决了传统DBMS和多模态系统在可用性和可解释性方面的局限性

Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.

</details>


### [3] [Acyclic Conjunctive Regular Path Queries are no Harder than Corresponding Conjunctive Queries](https://arxiv.org/abs/2512.11129)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu*

Main category: cs.DB

TL;DR: 提出一种输出敏感算法用于评估无环CRPQ，其复杂度与相应CQ的最佳已知输出敏感复杂度匹配，表明无环CRPQ的递归特性在输出敏感分析中不会增加额外复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有无环CRPQ的输出敏感算法仍有改进空间，且递归Datalog程序在复杂度方面通常难以理解，需要探索无环CRPQ是否比相应非递归CQ更复杂。

Method: 提出输出敏感算法，复杂度基于输入大小、输出大小和查询参数"自由连接分数超树宽度"，算法复杂度与相应CQ的最佳已知输出敏感复杂度匹配。

Result: 算法改进了现有无环CRPQ输出敏感算法的复杂度，且复杂度与相应CQ的最佳已知输出敏感复杂度相同，表明无环CRPQ的递归特性不会增加额外复杂度。

Conclusion: 无环CRPQ的递归特性在输出敏感分析中不会增加超过相应非递归CQ的复杂度，这一结果令人惊讶，因为递归Datalog程序通常复杂度较高。

Abstract: We present an output-sensitive algorithm for evaluating an acyclic Conjunctive Regular Path Query (CRPQ). Its complexity is written in terms of the input size, the output size, and a well-known parameter of the query that is called the "free-connex fractional hypertree width". Our algorithm improves upon the complexity of the recently introduced output-sensitive algorithm for acyclic CRPQs. More notably, the complexity of our algorithm for a given acyclic CRPQ Q matches the best known output-sensitive complexity for the "corresponding" conjunctive query (CQ), that is the CQ that has the same structure as the CRPQ Q except that each RPQ is replaced with a binary atom (or a join of two binary atoms). This implies that it is not possible to improve upon our complexity for acyclic CRPQs without improving the state-of-the-art on output-sensitive evaluation for acyclic CQs. Our result is surprising because RPQs, and by extension CRPQs, are equivalent to recursive Datalog programs, which are generally poorly understood from a complexity standpoint. Yet, our result implies that the recursion aspect of acyclic CRPQs does not add any extra complexity on top of the corresponding (non-recursive) CQs, at least as far as output-sensitive analysis is concerned.

</details>


### [4] [Benchmarking RL-Enhanced Spatial Indices Against Traditional, Advanced, and Learned Counterparts](https://arxiv.org/abs/2512.11161)
*Guanli Liu,Renata Borovica-Gajic,Hai Lan,Zhifeng Bao*

Main category: cs.DB

TL;DR: 论文提出了首个模块化、可扩展的RLESI基准测试框架，通过统一评估发现RLESI虽然能通过调优降低查询延迟，但在查询效率和构建成本上均不如学习型空间索引和高级变体，实际应用受限。


<details>
  <summary>Details</summary>
Motivation: 强化学习增强的空间索引（RLESI）旨在通过强化学习改进索引构建过程中的查询效率，但由于缺乏统一的实现和全面的评估（特别是在基于磁盘的环境中），其实际效益尚不明确。

Method: 基于现有空间索引库构建模块化、可扩展的基准测试框架，将索引训练与构建解耦，支持参数调优，并能与传统、高级和学习型空间索引进行一致比较。评估了12个代表性空间索引在6个数据集和多样化工作负载上的表现。

Result: 评估发现：RLESI通过调优可以降低查询延迟，但在查询效率和索引构建成本方面始终不如学习型空间索引和高级变体。使用延迟、I/O和索引统计作为指标进行测量。

Conclusion: 尽管RLESI提供了有前景的架构兼容性，但其高昂的调优成本和有限的泛化能力阻碍了实际应用。研究强调了RLESI在实际部署中的局限性。

Abstract: Reinforcement learning has recently been used to enhance index structures, giving rise to reinforcement learning-enhanced spatial indices (RLESIs) that aim to improve query efficiency during index construction. However, their practical benefits remain unclear due to the lack of unified implementations and comprehensive evaluations, especially in disk-based settings.
  We present the first modular and extensible benchmark for RLESIs. Built on top of an existing spatial index library, our framework decouples index training from building, supports parameter tuning, and enables consistent comparison with traditional, advanced, and learned spatial indices.
  We evaluate 12 representative spatial indices across six datasets and diverse workloads, including point, range, kNN, spatial join, and mixed read/write queries. Using latency, I/O, and index statistics as metrics, we find that while RLESIs can reduce query latency with tuning, they consistently underperform learned spatial indices and advanced variants in both query efficiency and index build cost. These findings highlight that although RLESIs offer promising architectural compatibility, their high tuning costs and limited generalization hinder practical adoption.

</details>


### [5] [A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications](https://arxiv.org/abs/2512.11363)
*Junyi Fan,Li Sun*

Main category: cs.DB

TL;DR: 该论文构建了首个全面的Aave V3事件级数据集，覆盖6条主要EVM链，包含8种核心事件类型，超过5000万条结构化记录，为去中心化借贷市场研究提供基础数据资源。


<details>
  <summary>Details</summary>
Motivation: 尽管去中心化借贷协议（如Aave V3）管理着超过100亿美元的总锁定价值，但实证研究因缺乏标准化、跨链的事件级数据集而受到严重限制。

Method: 开发开源Python数据管道，收集并完全解码Aave V3在6条主要EVM兼容链（以太坊、Arbitrum、Optimism、Polygon、Avalanche和Base）上的8种核心事件类型，使用动态批处理和自动分片技术确保数据完整性和可重复性。

Result: 创建了包含超过5000万条结构化记录的数据集，每条记录都包含区块元数据和美元估值，数据按严格时间顺序排列，每个文件不超过100万行，确保可管理性和可访问性。

Conclusion: 该公开数据集为资本流动、利率动态、清算级联和跨链用户行为的细粒度分析提供了基础资源，将支持未来对去中心化借贷市场和系统性风险的研究。

Abstract: Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.
  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.
  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.

</details>


### [6] [Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach](https://arxiv.org/abs/2512.11403)
*Jacques Chabin,Mirian Halfeld Ferrari,Nicolas Hiot*

Main category: cs.DB

TL;DR: 提出一种自动化方法，将文本数据转换为模型无关的模式，能够与任何数据库模型对齐，并同时生成模式和实例。


<details>
  <summary>Details</summary>
Motivation: 需要一种自动化方法来将非结构化的文本数据转换为结构化的数据库模式，特别是对于临床医疗病例等复杂领域数据。

Method: 首先将文本数据表示为语义增强的语法树，然后通过迭代的树重写和语法提取进行精炼，整个过程由属性语法元模型 \metaG 指导。

Result: 该方法能够生成模型无关的模式及其对应的实例，并以临床医疗病例作为概念验证展示了方法的适用性。

Conclusion: 提出的自动化方法能够有效地将文本数据转换为结构化的数据库模式，为文本数据到结构化数据的转换提供了一种通用解决方案。

Abstract: We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 提出一种基于CPU、磁盘和I/O利用率的任务调度算法，通过提高资源利用率来降低云服务的能耗


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，而现有能源有限且对环境有温室效应，需要开发节能算法。现有研究大多只关注最大化平均资源利用率或最小化完工时间，未考虑物理机中不同类型的资源。

Method: 提出一种任务调度算法，通过综合考虑CPU、磁盘、I/O利用率和任务处理时间的适应度函数，显式提高各类资源的利用率，从而增加活动资源的利用率。

Result: 通过合成数据集对提出的算法和现有MaxUtil算法进行广泛仿真，结果显示提出的算法是更好的节能算法，比MaxUtil算法消耗更少的能量。

Conclusion: 提出的任务调度算法通过显式考虑多种资源类型（CPU、磁盘、I/O）的利用率，有效提高了资源利用率，从而实现了更好的节能效果，为解决云服务能耗问题提供了有效方案。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [8] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，用于新兴加速器平台，重点关注覆盖率和正确性而非性能优化。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端，传统方法只关注少数高性能内核，而TritorX旨在实现整个算子集的广泛覆盖和正确性。

Method: 集成开源大语言模型、自定义代码检查器、JIT编译和基于PyTorch OpInfo的测试框架，兼容真实MTIA芯片和硬件模拟环境。

Result: 成功为481个独特的ATen算子生成内核和包装器，通过超过20,000个PyTorch OpInfo测试，实现了高覆盖率和正确性。

Conclusion: TritorX能够在一夜之间为新加速器平台生成完整的PyTorch ATen后端，大大加速了硬件适配过程。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [9] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文对实时迁移技术进行了全面综述，重点关注容器和虚拟机迁移，分析了技术现状、实际挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述往往忽略了实时迁移技术在实际应用中的关键技术方面和实践挑战。本文旨在填补这一空白，通过整合现有综述内容，从多个维度对实时迁移技术进行全面分析。

Method: 本文通过整合现有综述内容，从迁移技术、迁移单元和基础设施特性等多个维度对实时迁移技术进行综合分析。重点关注容器和虚拟机两种迁移方法，并探讨迁移目标和操作约束对现有技术可用性和有效性的影响。

Result: 研究发现，尽管实时迁移技术已得到广泛研究，但其对多种系统因素的依赖带来了挑战。在某些情况下，复杂性和资源需求超过了其带来的好处，使得实施难以合理化。容器和虚拟机迁移在采用程度上存在明显差异。

Conclusion: 本文通过概述当前技术挑战并为未来研究和开发方向提供指导，既为爱好者提供了关于实时迁移的宝贵资源，又有助于推动实时迁移技术在各种计算环境中的实际应用和发展。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [10] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 论文系统评估GPU集群利用率低的问题，提出三种动态调度器(HPS、PBS、SBS)，在64-GPU集群模拟中显著提升利用率至78.2%，减少任务饥饿现象


<details>
  <summary>Details</summary>
Motivation: GPU集群实际部署中平均利用率仅约50%，主要由于碎片化、异构工作负载和静态调度策略限制，需要更高效的调度方案提升资源利用率和公平性

Method: 提出三种专用动态调度器：混合优先级(HPS)、预测性回填(PBS)和智能批处理(SBS)，在包含1000个AI任务的64-GPU、8节点集群上进行受控模拟评估

Result: 动态调度器显著优于静态基线：HPS达到最高利用率78.2%和最高吞吐量25.8任务/小时，将饥饿任务从156个减少到12个；PBS和SBS分别达到76.1%和74.6%利用率

Conclusion: 动态多目标调度器在吞吐量、等待时间、公平性和饥饿减少等关键指标上一致优于单目标启发式方法，为未来生产调度框架提供实用基础

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [11] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 本文比较了集中式分层联邦学习(HFL)与两种去中心化架构(聚合联邦学习AFL和持续联邦学习CFL)，发现去中心化方法在多个指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 集中式分层联邦学习(HFL)存在通信瓶颈和隐私问题，而去中心化的AFL和CFL架构通过分布式计算和聚合提供了有前景的替代方案，需要系统比较这些方法的性能差异。

Method: 使用Fashion MNIST和MNIST数据集，对集中式HFL与去中心化AFL、CFL架构进行综合比较，评估精度、召回率、F1分数和平衡准确率等指标。

Result: 去中心化的AFL和CFL在精度、召回率、F1分数和平衡准确率等多个指标上均优于集中式HFL，展示了去中心化聚合机制在分布式设备协同模型训练中的优势。

Conclusion: 去中心化联邦学习方法(AFL和CFL)相比集中式HFL具有更好的性能表现，为研究者和实践者提供了有价值的指导，推动联邦学习向去中心化方向发展以获得更好的协同模型训练效果。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [12] [Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI](https://arxiv.org/abs/2512.10990)
*Jianli Jin,Ziyang Lin,Qianli Dong,Yi Chen,Jayanth Srinivasa,Myungjin Lee,Zhaowei Tan,Fan Lai*

Main category: cs.DC

TL;DR: Dora是一个面向边缘AI训练和推理的QoE感知混合并行框架，通过异构感知模型分区、竞争感知网络调度和运行时适配器，在满足用户体验质量的同时提升执行效率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI应用的普及，满足用户QoE要求（如推理延迟）成为首要目标。然而，现代AI模型通常超过单个设备资源容量，需要在异构设备上进行分布式执行，而现有并行规划器主要优化吞吐量或设备利用率，忽视了QoE，导致资源效率低下或QoE违规。

Method: Dora通过三个关键机制实现QoE感知混合并行：1) 异构感知模型分区器，确定并分配模型分区，形成紧凑的QoE合规计划集；2) 竞争感知网络调度器，通过最大化计算通信重叠来优化候选计划；3) 运行时适配器，自适应组合多个计划以最大化全局效率同时尊重整体QoE。

Result: 在代表性边缘部署场景（智能家居、交通分析、小型边缘集群）中，Dora实现了1.1-6.3倍的执行加速，同时能耗降低21-82%，并在运行时动态变化下保持QoE。

Conclusion: Dora框架成功解决了边缘AI分布式执行中的QoE优化问题，通过联合优化异构计算、竞争网络和多维QoE目标，在保证用户体验的同时显著提升了系统效率和能源效率。

Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.

</details>


### [13] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 该论文提出三种GPU原生编译方法消除CPU-GPU数据传输瓶颈，实现10-100倍代码迭代加速


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，限制了代码迭代效率

Method: 提出三种互补的GPU原生编译方法：1) 并行传统编译适配GPU执行；2) 使用学习序列到序列翻译和概率验证的神经编译；3) 结合两种策略的混合架构

Result: 传统GPU编译通过消除传输实现2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有正确性保证的实用部署路径

Conclusion: GPU原生编译能显著加速代码迭代周期，概率验证框架允许在编译准确性和并行探索之间权衡，对自改进AI系统和未来模拟计算基板有重要影响

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [14] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个用于强化学习后训练中rollout-training解耦架构的集群调度框架，通过跨集群编排消除同步依赖带来的空闲时间，将成本效率提升1.84倍。


<details>
  <summary>Details</summary>
Motivation: 在强化学习后训练中，rollout和training解耦到专用集群能最大化硬件效率，但on-policy算法的严格同步要求导致严重依赖气泡，使得一个集群在另一个集群运行时空闲，造成资源浪费。

Method: 提出RollMux框架，基于"一个作业的结构性空闲可以被另一个作业的活动阶段利用"的洞察。引入co-execution group抽象将集群划分为隔离的局部性域，实现双层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮询调度。group抽象还施加驻留约束，确保大模型状态保留在主机内存中，实现"热启动"上下文切换。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上评估，RollMux相比标准解耦架构将成本效率提升1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的SLO达成率。

Conclusion: RollMux通过跨集群编排有效回收了rollout-training解耦架构中的依赖气泡，显著提升了资源利用率和成本效率，为大规模强化学习后训练提供了高效的调度解决方案。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [15] [Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512)
*Patrick D. Manya,Eugene M. Mbuyi,Gothy T. Ngoie,Jordan F. Masakuna*

Main category: cs.DC

TL;DR: 提出一种基于多包消息传递的分布式修剪方法增强技术，显著减少大型网络中接近中心性计算的通信开销


<details>
  <summary>Details</summary>
Motivation: 现有分布式近似技术（如修剪方法）在大型网络设置中无法有效降低大量数据包交换的通信成本，需要解决这一通信瓶颈问题

Method: 引入多包消息传递技术，允许节点批量处理和传输更大的整合数据块，减少消息交换数量并最小化数据丢失，同时保持中心性估计的准确性

Result: 多包方法在消息效率（更少的总体消息）和计算时间方面显著优于原始修剪技术，保持了基线方法的近似特性，在通信效率方面获得显著提升

Conclusion: 该方法为去中心化接近中心性计算提供了更可扩展和高效的解决方案，尽管存在节点内存使用和本地开销增加的可管理权衡，但通信效率的提升更为显著

Abstract: Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.

</details>


### [16] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax是一个移动端DNN推理框架，通过计算图分区、分支感知内存管理和自适应调度，在不修改模型的情况下加速动态控制流模型的推理，减少延迟46%，控制内存开销26.5%，节省能耗30%。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上实时DNN应用需求增长，但动态控制流算子和不支持的内核往往回退到CPU执行，现有框架处理不佳，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1) 分区计算DAG以暴露并行性；2) 采用分支感知内存管理，使用专用内存池和缓冲区重用减少运行时内存占用；3) 自适应调度器根据设备内存约束执行分支；4) 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN模型，Parallax相比最先进框架：延迟降低最高46%，平均内存开销控制在26.5%，能耗节省最高30%。

Conclusion: Parallax框架无需模型重构或自定义算子实现，就能显著加速移动端DNN推理，满足实时移动推理的响应性需求，在延迟、内存和能耗方面均有显著改进。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [17] [FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634)
*Elia Palme,Juan Pablo Dorsch,Ali Khosravi,Giovanni Pizzi,Francesco Pagnamenta,Andrea Ceriani,Eirini Koutsaniti,Rafael Sarmiento,Ivano Bonesana,Alejandro Dabin*

Main category: cs.DC

TL;DR: FirecREST v2是下一代开源RESTful API，为HPC资源提供程序化访问，相比前代性能提升100倍，重点关注安全性和高吞吐量集成。


<details>
  <summary>Details</summary>
Motivation: 重新设计FirecEST以解决代理式API在密集I/O操作中的性能瓶颈，集成增强的安全性和高吞吐量作为核心需求。

Method: 采用系统化的性能测试方法，识别常见瓶颈，并进行关键的设计和架构更改，包括从零开始重新设计。

Result: 实现了100倍的性能提升，获得了独立的同行验证支持，显著改善了HPC资源的程序化访问效率。

Conclusion: FirecREST v2的成功重新设计展示了解决代理式API性能瓶颈的有效方法，并为进一步改进提供了机会。

Abstract: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.
  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.

</details>


### [18] [Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643)
*Manideep Reddy Chinthareddy*

Main category: cs.DC

TL;DR: 提出一种无需显式worker ID的分布式ID生成协议，利用容器私有IPv4地址作为节点唯一性来源，消除中心化协调需求


<details>
  <summary>Details</summary>
Motivation: 传统Snowflake分布式ID生成器需要手动分配或中心化协调的worker ID，这在容器编排环境（如Kubernetes）中带来显著摩擦，因为工作负载是短暂且自动伸缩的。维护稳定的worker身份需要复杂的有状态集或外部协调服务，违背了无状态微服务的操作优势。

Method: 提出云无关、容器原生的ID生成协议，通过从容器的私有IPv4地址确定性推导节点唯一性来消除对显式worker ID的依赖。引入修改的位分配方案（1-41-16-6），容纳16位网络衍生熵同时保持严格单调性。在AWS、GCP和Azure环境中验证该方法。

Result: 评估结果显示，虽然设计有约64,000 TPS的理论单节点上限，但在实际微服务部署中，网络I/O主导延迟，导致端到端性能（3节点集群上约31,000 TPS）与经典有状态生成器相当，同时提供有效无界的水平可扩展性。

Conclusion: 该协议成功消除了分布式ID生成中对显式worker ID的依赖，通过利用容器网络属性实现无中心化协调，在保持性能的同时提供更好的容器环境适应性。

Abstract: Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.
  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.

</details>


### [19] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低计算和通信成本，实现资源高效的持续学习。


<details>
  <summary>Details</summary>
Motivation: 当前视频分析中为每个摄像头单独重新训练专用轻量DNN模型的做法存在高计算和通信成本问题，无法扩展到大规模摄像头部署场景。

Method: ECCO包含三个核心组件：1) 轻量级分组算法动态形成和更新摄像头组；2) GPU分配器动态分配GPU资源以提高重新训练准确性和确保公平性；3) 每个摄像头的传输控制器配置帧采样并协调带宽共享。

Result: 在三个不同数据集上对两个视觉任务进行评估，相比领先基线方法，ECCO在使用相同计算和通信资源的情况下将重新训练准确性提高了6.7%-18.1%，或在相同准确性下支持3.3倍更多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，采用共享模型训练策略，有效解决了视频分析中持续学习的高成本问题，实现了资源高效的视频分析框架。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


### [20] [Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.DC

TL;DR: H-MPCs：基于超图的多方支付通道，通过集体资金超边替代双边通道，实现无领导者、高并发的链下支付，解决流动性碎片化和通道耗尽问题


<details>
  <summary>Details</summary>
Motivation: 现有支付通道网络存在流动性碎片化（资金锁定在单一通道无法复用）和通道耗尽问题，限制了路由效率和交易成功率。现有的多方通道方案依赖领导者或协调者，存在单点故障且跨通道支付灵活性有限。

Method: 提出基于超图的多方支付通道（H-MPCs），用集体资金的超边替代传统双边通道。通过可验证、提议者排序的有向无环图更新，实现完全并发、无领导者的超边内和超边间支付。

Result: 在150个节点的网络上实现，交易成功率约94%，且没有HTLC过期或路由失败，证明了H-MPCs的鲁棒性。

Conclusion: H-MPCs提供了比现有设计显著更高的灵活性和并发性，有效解决了支付通道网络的流动性碎片化和通道耗尽问题，同时避免了单点故障。

Abstract: Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.
  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.
  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [21] [Scalable Similarity Search over Large Attributed Bipartite Graphs](https://arxiv.org/abs/2512.11606)
*Xi Ou,Longlong Lin,Zeli Wang,Pingpeng Yuan,Rong-Hua Li*

Main category: cs.DS

TL;DR: 提出AHPP模型，结合二分图高阶结构邻近性和节点属性相似性，用于属性二分图的相似性搜索，并设计高效近似算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确捕捉二分图的独特结构特性，未能有效结合节点属性信息，且计算复杂度高，限制了在大规模图上的应用。

Method: 1. 提出属性增强的隐藏个性化PageRank（AHPP）随机游走模型；2. 将相似性搜索问题转化为近似AHPP问题；3. 设计两种具有可证明近似保证的push-style局部算法。

Result: 在真实世界和合成数据集上的实验表明，AHPP在效果上优于15个竞争方法，且提出的算法在效率上表现优异。

Conclusion: AHPP模型能有效结合二分图结构邻近性和属性相似性，提出的高效算法可处理大规模属性二分图的相似性搜索问题。

Abstract: Bipartite graphs are widely used to model relationships between entities of different types, where nodes are divided into two disjoint sets. Similarity search, a fundamental operation that retrieves nodes similar to a given query node, plays a crucial role in various real-world applications, including machine learning and graph clustering. However, existing state-of-the-art methods often struggle to accurately capture the unique structural properties of bipartite graphs or fail to incorporate the informative node attributes, leading to suboptimal performance. Besides, their high computational complexity limits scalability, making them impractical for large graphs with millions of nodes and tens of thousands of attributes. To overcome these challenges, we first introduce Attribute-augmented Hidden Personalized PageRank (AHPP), a novel random walk model designed to blend seamlessly both the higher-order bipartite structure proximity and attribute similarity. We then formulate the similarity search over attributed bipartite graphs as an approximate AHPP problem and propose two efficient push-style local algorithms with provable approximation guarantees. Finally, extensive experiments on real-world and synthetic datasets validate the effectiveness of AHPP and the efficiency of our proposed algorithms when compared with fifteen competitors.

</details>


### [22] [New Entropy Measures for Tries with Applications to the XBWT](https://arxiv.org/abs/2512.11618)
*Lorenzo Carfagna,Carlo Tosoni*

Main category: cs.DS

TL;DR: 本文提出了两种新的trie熵度量方法，克服了现有方法的局限性，并展示了如何将XBWT压缩到k阶经验熵加o(n)比特的空间内。


<details>
  <summary>Details</summary>
Motivation: 现有的trie熵度量方法存在两个主要问题：标准最坏情况熵不考虑符号频率分布，无法捕捉偏斜分布的压缩性；标签熵则忽略了树拓扑结构。需要新的熵度量来克服这些限制。

Method: 提出了两种新的trie熵度量：最坏情况熵和经验熵。这些熵满足与字符串熵类似的性质，是字符串情况的自然推广。经验熵与最坏情况熵密切相关，可通过算术编码的自然扩展从字符串推广到trie。

Result: 证明了trie的XBWT可以在k阶经验熵加o(n)比特的空间内被压缩和高效索引，其中n是节点数。这种编码总是严格小于原始XBWT编码，在某些情况下是渐近更小的。

Conclusion: 提出的新熵度量是字符串熵的自然推广，克服了现有方法的局限性。基于这些熵的XBWT编码实现了更好的压缩效果，为trie的高效存储和索引提供了理论基础。

Abstract: Entropy quantifies the number of bits required to store objects under certain given assumptions. While this is a well established concept for strings, in the context of tries the state-of-the-art regarding entropies is less developed. The standard trie worst-case entropy considers the set of tries with a fixed number of nodes and alphabet size. However, this approach does not consider the frequencies of the symbols in the trie, thus failing to capture the compressibility of tries with skewed character distributions. On the other hand, the label entropy [FOCS '05], proposed for node-labeled trees, does not take into account the tree topology, which has to be stored separately. In this paper, we introduce two new entropy measures for tries - worst-case and empirical - which overcome the two aforementioned limitations. Notably, our entropies satisfy similar properties of their string counterparts, thereby becoming very natural generalizations of the (simpler) string case. Indeed, our empirical entropy is closely related to the worst-case entropy and is reachable through a natural extension of arithmetic coding from strings to tries. Moreover we show that, similarly to the FM-index for strings [JACM '05], the XBWT of a trie can be compressed and efficiently indexed within our k-th order empirical entropy plus o(n) bits, with n being the number of nodes. Interestingly, the space usage of this encoding includes the trie topology and the upper-bound holds for every k sufficiently small, simultaneously. This XBWT encoding is always strictly smaller than the original one [JACM '09] and we show that in certain cases it is asymptotically smaller.

</details>


### [23] [The parameterized complexity of Strong Conflict-Free Vertex-Connection Colorability](https://arxiv.org/abs/2512.11725)
*Carl Feghali,Hoang-Oanh Le,Van Bang Le*

Main category: cs.DS

TL;DR: 该论文研究了具有连通性约束的图着色新变体，证明了在顶点覆盖数参数化下，强无冲突k-着色判定问题是固定参数可处理的，但对于二分图，强无冲突3-着色判定问题不存在多项式核。


<details>
  <summary>Details</summary>
Motivation: 研究Hsieh等人最近提出的具有连通性约束的图着色新变体，探索其计算复杂性，特别是与经典k-着色问题在参数化复杂性方面的差异。

Method: 采用参数化复杂性理论方法，以顶点覆盖数作为参数，分析强无冲突顶点连接k-着色判定问题的计算复杂性。

Result: 1. 强无冲突k-着色判定问题在顶点覆盖数参数化下是固定参数可处理的；2. 在NP⊈coNP/poly假设下，强无冲突3-着色判定问题（即使是二分图）不存在多项式核。

Conclusion: 该图着色变体在参数化复杂性方面与经典k-着色问题存在显著差异，为理解具有连通性约束的图着色问题提供了新的复杂性见解。

Abstract: This paper continues the study of a new variant of graph coloring with a connectivity constraint recently introduced by Hsieh et al. [COCOON 2024]. A path in a vertex-colored graph is called conflict-free if there is a color that appears exactly once on its vertices. A connected graph is said to be strongly conflict-free vertex-connection $k$-colorable if it admits a (proper) vertex $k$-coloring such that any two distinct vertices are connected by a conflict-free shortest path. Among others, we show that deciding, for a given graph $G$ and an integer $k$, whether $G$ is strongly conflict-free $k$-colorable is fixed-parameter tractable when parameterized by the vertex cover number. But under the standard complexity-theoretic assumption NP $\not\subseteq$ coNP/poly, deciding, for a given graph $G$, whether $G$ is strongly conflict-free $3$-colorable does not admit a polynomial kernel, even for bipartite graphs. This kernel lower bound is in stark contrast to the ordinal $k$-Coloring problem which is known to admit a polynomial kernel when parameterized by the vertex cover number.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 自动生成的测试比手动测试有更高的分支覆盖率，但在基于频谱的故障定位（SBFL）得分上较低，特别是在深度嵌套代码结构中。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注测试覆盖率指标，很少评估测试对故障定位的支持效果，特别是使用变异测试引入的人工故障。需要比较自动生成测试和手动创建测试在SBFL得分和代码覆盖率上的差异。

Method: 使用SBFL得分作为评估指标，比较自动生成测试和手动创建测试的SBFL得分和代码覆盖率。特别关注深度嵌套代码结构的情况。

Result: 自动生成的测试比手动创建的测试获得更高的分支覆盖率，但SBFL得分较低，特别是在深度嵌套结构的代码中。

Conclusion: 自动生成测试和手动创建测试各有优势：自动测试在覆盖率方面表现更好，而手动测试在故障定位支持方面更优。这为有效结合两种测试方法提供了指导。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [25] [AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing](https://arxiv.org/abs/2512.11398)
*Qiuming Luo,Yanming Lei,Kunzhong Wu,Yixuan Cao,Chengjian Liu*

Main category: cs.SE

TL;DR: AutoFSM是一个用于有限状态机Verilog代码生成的多智能体协作框架，通过结构化中间表示降低语法错误率，结合SystemC建模和自动测试平台生成提高调试效率，在SKT-FSM基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成有限状态机控制逻辑的Verilog代码时面临语法错误频繁、调试效率低、过度依赖测试基准等问题，需要更可靠的硬件设计代码生成方案。

Method: 提出AutoFSM多智能体协作框架，引入结构清晰的中间表示来降低语法错误，提供从IR到Verilog的自动转换工具链，首次集成SystemC建模与自动测试平台生成。

Result: 在SKT-FSM基准测试（包含67个不同复杂度FSM样本）上，AutoFSM相比开源框架MAGE实现了最高11.94%的通过率提升和最高17.62%的语法错误率降低。

Conclusion: 结合大语言模型、结构化中间表示和自动化测试，能够显著提高寄存器传输级代码生成的可靠性和可扩展性，为硬件设计自动化提供了有效解决方案。

Abstract: With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation.

</details>


### [26] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 19个小量化LLM在C到Java代码转换任务中表现差异巨大，仅3个模型通过超50%测试，大多数模型完全失败，显示量化模型在复杂C概念推理上存在硬性天花板。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动代码转换面临巨大挑战，包括编程范式（过程式vs面向对象）、内存模型（手动指针vs垃圾回收）和数据类型不兼容等根本性差异。本研究旨在评估小规模量化LLM在此困难任务上的实际效果。

Method: 采用新颖的混合流水线方法：利用抽象语法树（AST）进行语义分解，并采用高度约束的基于规则的提示策略。评估了19个参数小于200亿的小型量化LLM。

Result: 结果呈现明显的三级性能分化：Tier 3模型（如llama3.1、gemma3、starcoder2）100%测试失败，连基本可运行的Java模板代码都无法生成；Tier 2模型（如mistral-nemo、mistral）能生成可运行代码但存在危险的语义错误和错误翻译；仅3个Tier 1模型（phi4、deepseek-coder-v2、codeqwen）表现可行，通过超50%测试套件。即使顶级模型也在函数指针、sizeof和枚举逻辑等复杂C概念上失败。

Conclusion: 当前量化LLM在C到Java代码转换任务中的推理能力存在硬性天花板，大多数模型完全无法处理此任务，仅少数模型表现可行但仍无法处理复杂C概念。这表明需要更先进的模型架构或方法来解决此类复杂的代码转换问题。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [27] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 该研究首次系统评估了差分隐私在代码大语言模型中的应用，发现DP能有效降低模型记忆训练数据片段的风险，同时保持代码生成能力，且不影响训练效率。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型可能无意中记忆并复现训练数据中的代码片段，导致隐私泄露和知识产权侵权风险，限制了其在敏感领域的部署。需要在不损害模型性能的前提下降低记忆风险。

Method: 首先分析代码大语言模型在微调过程中的记忆行为原因，然后实证评估差分隐私在缓解记忆风险同时保持代码生成能力的效果，包括对训练效率和能耗的影响分析。

Result: DP显著降低了所有测试代码片段类型的记忆风险，最容易记忆的片段类型也最容易被DP缓解。DP略微增加了困惑度，但保持甚至提升了代码生成能力，且不影响训练时间和能耗。

Conclusion: 差分隐私是保护代码大语言模型隐私的实用方法，能有效降低记忆风险而不显著损害模型效用，为在敏感领域部署CodeLLMs提供了可行方案。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


### [28] [Mini-SFC: A Comprehensive Simulation Framework for Orchestration and Management of Service Function Chains](https://arxiv.org/abs/2512.11527)
*Xi Wang,Shuo Shi,Chenyu Wu*

Main category: cs.SE

TL;DR: 本文介绍了Mini-SFC，一个支持数值和容器虚拟仿真的模块化SFC仿真框架，支持在线动态拓扑调整，旨在简化算法验证和服务部署验证。


<details>
  <summary>Details</summary>
Motivation: 现有SFC仿真工具存在局限性，无法满足云环境和网络环境中复杂服务的灵活部署需求，需要更用户友好、灵活可扩展的仿真框架来支持算法验证和实际部署验证。

Method: 提出Mini-SFC模块化仿真框架，支持数值仿真和基于容器的虚拟仿真，提供在线动态拓扑调整功能，通过简化模块设计和标准化求解器接口来降低学习曲线。

Result: Mini-SFC作为一个开源平台，显著缩短了研究人员的学习曲线，增强了SFC管理和优化所需的灵活性和可扩展性，支持快速算法验证和现实服务部署验证。

Conclusion: Mini-SFC解决了现有SFC仿真工具的局限性，提供了一个用户友好、灵活可扩展的仿真框架，有助于推动SFC管理和优化的研究与应用。

Abstract: In the continuously evolving cloud computing and network environment, service function chain (SFC) plays a crucial role in implementing complex services in the network with its flexible deployment capabilities. To address the limitations of existing SFC simulation tools, this paper introduces Mini-SFC, a modular simulation framework that supports both numerical and container-based virtual simulations, while also supporting online dynamic topology adjustments. As an open-source platform emphasizing user-friendliness, Mini-SFC facilitates rapid algorithm verification and realistic service deployment validation. By simplifying module design and providing standardized solver interfaces, Mini-SFC significantly shortens the learning curve for researchers and enhances the flexibility and scalability required for advanced SFC management and optimization. For readers interested in exploring or utilizing Mini-SFC, more information is available on the official project page.

</details>


### [29] [A Study of Library Usage in Agent-Authored Pull Requests](https://arxiv.org/abs/2512.11589)
*Lukas Twist*

Main category: cs.SE

TL;DR: AI编码代理在29.5%的PR中导入库，但仅1.3%添加新依赖；添加依赖时75.0%指定版本，优于直接LLM使用；代理使用的库集比非代理LLM研究中的"库偏好"更加多样化。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码代理能够完成端到端软件工程工作流，但我们对其在生成代码时如何使用库（真实软件开发的核心部分）了解甚少。本研究旨在填补这一空白，通过分析代理编写的PR来探究其库使用模式。

Method: 使用AIDev数据集中的26,760个代理编写的pull requests（PRs），研究三个问题：1）代理导入库的频率；2）引入新依赖的频率及版本控制实践；3）代理选择的具体库类型。

Result: 1）代理经常导入库（29.5%的PRs）；2）很少添加新依赖（仅1.3%的PRs）；3）添加依赖时遵循强版本控制实践（75.0%指定版本），优于直接LLM使用；4）代理使用的外部库集非常多样化，与先前非代理LLM研究中观察到的有限"库偏好"形成对比。

Conclusion: 研究提供了AI编码代理与当前软件生态系统交互的早期实证视角，显示代理在库使用方面表现出谨慎（很少添加新依赖）但规范（版本控制良好）的特点，且库选择更加多样化，这有助于理解AI代理在实际软件开发中的行为模式。

Abstract: Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [SHIFT: An RDMA Failure-Resilient Layer for Distributed Training](https://arxiv.org/abs/2512.11094)
*Shengkai Lin,Kairui Zhou,Yibo Wu,Hongtao Zhang,Qinwei Yang,Wei Zhang,Arvind Krishnamurthy,Shizhen Zhao*

Main category: cs.NI

TL;DR: SHIFT是一个在RDMA层实现容错的系统，通过跨主机内不同NIC重定向RDMA流量，使应用在网络异常时能继续执行直到下一个检查点，最小化训练进度损失。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式LLM训练中，单个网络异常会通过gang scheduling传播导致整个任务失败。现有容错机制（如检查点和运行时恢复）主要在应用层操作，不可避免地中断训练进度。

Method: 在RDMA层引入容错机制，设计SHIFT作为RDMA上的容错层，通过精心设计的故障状态机和控制流，实现跨不同主机内NIC的RDMA流量无缝重定向。

Result: SHIFT引入最小的数据路径开销，同时确保在网络故障下应用的连续性。未修改的应用如PyTorch with NCCL可以在RDMA级容错下运行。

Conclusion: SHIFT通过RDMA层容错与应用层技术结合，有效最小化训练进度损失，是应用无关、对应用透明且低开销的解决方案。

Abstract: With gang scheduling in large-scale distributed Large Language Model training, a single network anomaly can propagate and cause complete task failure. The frequency of such anomalies increases with network scale. However, existing fault-tolerance mechanisms, such as checkpointing and runtime resilience methods, primarily operate at the application layer and inevitably cause disruptions in training progress.
  We propose to address this challenge by introducing fault tolerance at the Remote Direct Memory Access (RDMA) layer and integrating it with existing application-layer techniques. We present SHIFT, a fault-resilient layer over RDMA that enables seamless redirection of RDMA traffic across different intra-host NICs. By allowing applications to continue execution in the presence of network anomalies until the next checkpoint, SHIFT effectively minimizes training progress loss. SHIFT is designed to be application-agnostic, transparent to applications, and low-overhead.
  Through a carefully designed failure state machine and control flow, unmodified applications such as PyTorch with NCCL can run with RDMA-level fault tolerance. Experimental results demonstrate that SHIFT introduces minimal data path overhead while ensuring application continuity under network failures.

</details>


### [31] [BIER-Star: Stateless Geographic Multicast for Scalable Satellite-Terrestrial Integration](https://arxiv.org/abs/2512.11156)
*Mostafa Abdollahi,Wenjun Yang,Jianping Pan*

Main category: cs.NI

TL;DR: BIER-Star是一种用于天地一体化网络的无状态组播协议，采用两层地理空间网格编码目的地，减少头部开销并简化转发


<details>
  <summary>Details</summary>
Motivation: 随着LEO卫星星座的快速发展，天地一体化网络需要可扩展且高效的组播协议来支持紧急警报、大规模软件更新和实时广播等关键应用。传统IP组播和软件定义组播方法在动态移动的卫星拓扑中产生大量控制开销且适应性差。

Method: BIER-Star采用两层地理空间网格方案（H3），将目的地编码为地球和空间单元标识符而非每个终端地址。这种基于单元的抽象缩短了头部比特串，简化了转发，并消除了每流状态和复杂信令。

Result: 模拟结果显示，BIER-Star相比BIER减少了头部大小，并避免了贪婪方法中的地理路径查找失败问题。

Conclusion: BIER-Star为天地一体化网络提供了一种高效、可扩展的无状态组播解决方案，通过地理空间网格编码有效解决了传统协议在动态卫星环境中的局限性。

Abstract: The rapid expansion of LEO satellite constellations has enabled an integrated terrestrial network and non-terrestrial network (TN-NTN), connecting diverse users such as aircraft, ships, and remote communities. These networks increasingly need a scalable and efficient multicast protocol for critical applications like emergency alerts, large-scale software updates, and real-time broadcasting. However, traditional multicast protocols, such as IP-based multicast and software-defined multicast approaches, introduce significant control overhead and struggle to adapt to the dynamic and mobile nature of satellite topologies. This paper presents BIER-Star, a stateless multicast protocol designed for the integrated TN-NTN. BIER-Star uses a two-layer geospatial gridding scheme (i.e., H3) to encode destinations as Earth- and space-cell identifiers rather than per-terminal addresses. This cell-based abstraction shortens the header bitstring, simplifies forwarding, and eliminates per-flow state and complex signaling. Our simulations indicate that BIER-Star reduces header size versus BIER and avoids geographic path-finding failures seen in greedy methods.

</details>


### [32] [Distributed Resource Allocation and Application Deployment in Mesh Edge Networks](https://arxiv.org/abs/2512.11230)
*Antoine Bernard,Antoine Legrain,Maroua Ben Attia,Abdo Shabah*

Main category: cs.NI

TL;DR: 将虚拟网络嵌入扩展到移动边缘设备的受限网状网络，解决快速拓扑变化和资源限制的挑战，提出三种分配策略并展示优于传统方法的性能


<details>
  <summary>Details</summary>
Motivation: 传统虚拟网络嵌入方法假设静态或缓慢变化的网络拓扑，但新兴应用需要在移动环境中部署，传统方法变得不足。需要解决移动边缘设备网络中快速拓扑变化和资源限制的独特挑战。

Method: 开发包含设备能力、连接性、移动性和能量约束的模型，提出三种分配策略：用于最优分配的整数线性规划、用于即时部署的贪心启发式算法、用于平衡优化的多目标遗传算法。

Result: 初步评估分析了资源限制下的应用接受率、资源利用率和延迟性能。结果显示相比传统方法有改进，为高度移动环境中的VNE部署提供了基础。

Conclusion: 该工作成功将虚拟网络嵌入扩展到移动边缘环境，通过提出的三种策略有效解决了动态拓扑和资源限制的挑战，为移动环境中的网络部署提供了可行方案。

Abstract: Virtual Network Embedding (VNE) approaches typically assume static or slowly-changing network topologies, but emerging applications require deployment in mobile environments where traditional methods become insufficient. This work extends VNE to constrained mesh networks of mobile edge devices, addressing the unique challenges of rapid topology changes and limited resources. We develop models incorporating device capabilities, connectivity, mobility and energy constraints to evaluate optimal deployment strategies for mobile edge environments. Our approach handles the dynamic nature of mobile networks through three allocation strategies: an integer linear program for optimal allocation, a greedy heuristic for immediate deployment, and a multi-objective genetic algorithm for balanced optimization. Our initial evaluation analyzes application acceptance rates, resource utilization, and latency performance under resource limitations. Results demonstrate improvements over traditional approaches, providing a foundation for VNE deployment in highly mobile environments.

</details>


### [33] [Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks](https://arxiv.org/abs/2512.11667)
*Gabriel Almeida,João Paulo Esper,Cleverson Nahum,Audebaro Klautau,Kleber Vieira Cardoso*

Main category: cs.NI

TL;DR: 提出用于6G网络中VR云游戏的可扩展QoE感知多阶段优化框架，通过三阶段分解资源分配问题，实现QoE提升50%、通信资源减少75%、成本节约35%


<details>
  <summary>Details</summary>
Motivation: VR云游戏对带宽、延迟和资源管理要求极高，需要智能解决方案来确保最佳用户体验，特别是在6G网络环境下

Method: 将联合资源分配问题分解为三个相互依赖的阶段：用户关联与通信资源分配、VR游戏引擎放置与自适应多路径路由、基于运动到光子延迟的注意力感知调度与无线资源分配。为每个阶段设计专门的启发式算法，并引入基于虚拟对象视觉注意力的用户中心QoE模型

Result: 相比现有方法，QoE提升达50%，通信资源使用减少75%，成本节约达35%，平均最优性差距为5%。启发式算法在0.1秒内解决大规模场景

Conclusion: 提出的多阶段优化框架在6G网络中为VR云游戏提供了高效、可扩展的解决方案，具有实时部署潜力，显著改善了用户体验和资源效率

Abstract: Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\%, reduces communication resource usage by 75\%, and achieves up to 35\% cost savings, while maintaining an average optimality gap of 5\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering](https://arxiv.org/abs/2512.10962)
*Yifei He,Pranit Chawla,Yaser Souri,Subhojit Som,Xia Song*

Main category: cs.LG

TL;DR: 提出WebSTAR数据集和StepRM奖励模型，通过步骤级过滤从嘈杂的计算机使用代理轨迹中合成高质量训练数据，显著提升计算机使用代理性能


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理训练面临GUI交互成本高和高质量轨迹数据稀缺的问题，现有数据集依赖人工演示难以扩展，需要从嘈杂的强代理轨迹中提取有效监督信号

Method: 提出可扩展的数据合成流程：1) 步骤级过滤，评估单个动作保留正确步骤；2) 推理增强改进规划；3) 从OpenAI计算机使用预览模型合成WebSTAR数据集；4) 基于步骤级评分创建WebSCORE数据集；5) 训练轻量级奖励模型StepRM

Result: 构建了13.3K轨迹、100K评分步骤的WebSTAR数据集，训练的7B模型在WebVoyager上超越现有最佳开源模型15%以上；StepRM奖励模型与o4-mini评分质量相当但部署效率更高

Conclusion: 步骤级过滤是扩展计算机使用代理训练的关键原则，WebSTAR、WebSCORE数据集和StepRM奖励模型为构建稳健高效的计算机使用代理提供了实用工具

Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.

</details>


### [35] [Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2512.10966)
*Farica Zhuang,Dinara Aliyeva,Shu Yang,Zixuan Wen,Duy Duong-Tran,Christos Davatzikos,Tianlong Chen,Song Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出MREF-AD模型，通过多模态区域专家融合框架，自适应平衡脑区特异性生物标志物贡献，提升阿尔茨海默病诊断性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态融合方法通常简单拼接特征，无法自适应平衡不同脑区中淀粉样蛋白PET和MRI等生物标志物的贡献，需要更精细的融合策略来模拟临床实践。

Method: 提出MREF-AD模型，采用混合专家框架，将每个模态中的中尺度脑区建模为独立专家，使用两级门控网络学习受试者特定的融合权重。

Result: 在ADNI数据上，MREF-AD实现了最先进的诊断性能，同时提供了增强的脑区特异性生物标志物相关性可解释性。

Conclusion: MREF-AD作为一个通用框架，展示了自适应和可解释的多模态融合在神经影像中的实用性，能够揭示结构和分子成像如何共同贡献于疾病诊断。

Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.

</details>


### [36] [MoB: Mixture of Bidders](https://arxiv.org/abs/2512.10969)
*Dev Vyas*

Main category: cs.LG

TL;DR: MoB用VCG拍卖机制替代MoE中的学习门控网络，通过专家真实成本竞标实现无状态路由，解决了持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构在持续学习中存在根本限制：学习到的门控网络自身会遭受灾难性遗忘，这阻碍了MoE在持续学习场景中的应用。

Method: 提出Mixture of Bidders框架，用VCG拍卖机制替代学习门控网络。专家通过竞标真实成本（执行成本+遗忘成本）来竞争处理数据批次，实现无状态路由和真实竞标。

Result: 在Split-MNIST基准测试中，MoB达到88.77%平均准确率，相比Gated MoE的19.54%和Monolithic EWC的27.96%，提升了4.5倍。还扩展了具有自主监控能力的专家系统。

Conclusion: MoB通过将专家路由重新概念化为去中心化经济机制，成功解决了MoE在持续学习中的灾难性遗忘问题，为持续学习提供了新的游戏理论解决方案。

Abstract: Mixture of Experts (MoE) architectures have demonstrated remarkable success in scaling neural networks, yet their application to continual learning remains fundamentally limited by a critical vulnerability: the learned gating network itself suffers from catastrophic forgetting. We introduce Mixture of Bidders (MoB), a novel framework that reconceptualizes expert routing as a decentralized economic mechanism. MoB replaces learned gating networks with Vickrey-Clarke-Groves (VCG) auctions, where experts compete for each data batch by bidding their true cost -- a principled combination of execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This game-theoretic approach provides three key advantages: (1) {stateless routing that is immune to catastrophic forgetting, (2) \textbf{truthful bidding} guaranteed by dominant-strategy incentive compatibility, and (3) emergent specialization without explicit task boundaries. On Split-MNIST benchmarks, MoB achieves 88.77% average accuracy compared to 19.54% for Gated MoE and 27.96% for Monolithic EWC, representing a 4.5 times improvement over the strongest baseline. We further extend MoB with autonomous self-monitoring experts that detect their own knowledge consolidation boundaries, eliminating the need for explicit task demarcation.

</details>


### [37] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可在每轮常数时间内实现，作者提出更实用的算法并分析遗憾界与时间复杂度的权衡


<details>
  <summary>Details</summary>
Motivation: EXP3算法是经典的对抗性多臂老虎机算法，但其实现可能不够高效。作者旨在提供更实用的实现方案，并探索算法性能与计算效率之间的平衡关系。

Method: 作者指出EXP3算法可以在每轮以常数时间实现，并提出了一系列更实用的算法变体。通过分析这些算法的时间复杂度与遗憾界的权衡关系，为实际应用提供指导。

Result: 展示了EXP3算法可以实现常数时间每轮操作，提出了多个实用算法变体，并系统分析了不同算法在遗憾界和时间复杂度之间的具体权衡关系。

Conclusion: EXP3算法的高效实现是可行的，通过算法设计可以在保持理论性能的同时显著提升计算效率，为实际对抗性老虎机问题提供了实用的解决方案。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [38] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 提出基于强化学习的个性化肝素治疗优化框架，使用连续cxSOFA评分和TECM评估矩阵，在手术脓毒症患者中降低死亡率并缩短住院时间


<details>
  <summary>Details</summary>
Motivation: 脓毒症是危及生命的严重感染状态，需要优化个性化肝素治疗。传统离散SOFA评分不够精细，需要更连续的状态表示和评估方法来优化治疗策略

Method: 使用MIMIC-IV和eICU数据库数据，提出强化学习框架：1) 将离散SOFA转换为连续cxSOFA评分；2) 基于cxSOFA定义"好/坏"治疗策略；3) 提出类似混淆矩阵的TECM评估方法；应用Q-Learning、DQN、DDQN、BCQ、CQL等算法优化治疗

Result: cxSOFA-CQL模型表现最佳，将死亡率从1.83%降至0.74%，平均住院时间从11.11天缩短至9.42天。TECM在不同模型中显示一致结果，证明框架稳健性

Conclusion: 提出的强化学习框架能够可解释且稳健地优化手术脓毒症患者的肝素治疗，连续cxSOFA评分和TECM评估为治疗提供了更精细的评估，有望改善临床结果和决策支持可靠性

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [39] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出一种多智能体框架用于训练多模态情感识别系统，通过模块化架构实现新模态的灵活集成和组件替换，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前多模态深度学习模型在情感识别方面虽然准确率高，但训练和维护计算密集，且对模态变化不够灵活，需要更高效、可扩展的解决方案。

Method: 采用多智能体框架，每个模态编码器和融合分类器作为自主智能体，由中央监督器协调。支持视觉、音频和文本模态，分类器作为共享决策智能体。

Result: 通过概念验证实现证明了方法的可行性，框架不仅提高了训练效率，还支持新模态（如emotion2vec音频特征）的模块化集成和过时组件的无缝替换。

Conclusion: 该框架为HAI场景中的具身和虚拟智能体提供了更灵活、可扩展和可维护的感知模块设计，改善了多模态情感识别系统的训练效率和适应性。

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [40] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt是一个从化学语法"雕刻"3D分子几何结构的新框架，通过冻结的1D分子基础模型和3D分子扩散模型，将1D化学知识注入3D生成过程，在3D分子生成任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。

Method: 基于冻结的1D分子基础模型和3D分子扩散模型，引入可学习的查询从基础模型中提取化学知识，通过可训练投影器将跨模态信息注入扩散模型的条件空间，通过端到端优化将1D潜在化学知识深度集成到3D生成过程。

Result: 在GEOM-DRUGS和QM9数据集上，MolSculpt在从头3D分子生成和条件3D分子生成任务中达到最先进性能，展现出优越的3D保真度和稳定性。

Conclusion: MolSculpt通过将1D化学知识深度集成到3D生成过程，成功弥合了1D语法生成与3D几何实现之间的鸿沟，为药物发现和材料科学提供了精确的3D分子几何生成方法。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [41] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略迭代算法族，用于求解部分可观测马尔可夫决策过程（POMDPs），通过交替进行单阶段输出策略改进和周期性策略评估，实现了单调改进和计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 无记忆和有限记忆策略为求解POMDPs提供了实用替代方案，因为它们直接在输出空间而非高维信念空间中操作。然而，将经典方法（如策略迭代）扩展到这一设置仍然困难，因为输出过程是非马尔可夫的，导致策略改进步骤在不同阶段相互依赖。

Method: 引入新的单调改进策略迭代算法族，交替进行单阶段输出策略改进和按照预定周期模式进行策略评估。该族算法允许优化模式以最大化计算效率指标，并识别具有最小周期的最简单模式。基于此结构，进一步开发了无模型变体，可从数据中估计价值并直接学习无记忆策略。

Result: 在多个POMDPs示例中，该方法在基于模型和无模型设置下均实现了相对于策略梯度基线和近期专用算法的显著计算加速。

Conclusion: 提出的算法族为POMDPs提供了一种高效且实用的求解方法，通过优化周期性评估模式实现了计算效率的最大化，并在实际应用中表现出优越性能。

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [42] [Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification](https://arxiv.org/abs/2512.11087)
*Duo Zhou,Jorge Chavez,Hesun Chen,Grani A. Hanasusanto,Huan Zhang*

Main category: cs.LG

TL;DR: 提出线性约束驱动的剪裁框架，通过利用线性约束减少分支定界中的子问题数量并改进中间边界，显著提升神经网络验证器的效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络验证器在处理复杂验证属性时，分支定界(BaB)过程与快速边界技术结合是关键，但仍有提升空间。需要更高效的方法来增强验证器的效能。

Method: 提出线性约束驱动的剪裁框架，开发两种新算法：1)利用线性约束减少已验证或无关的输入空间；2)直接改进网络中的中间边界。采用专门的GPU程序高效处理线性约束，无需昂贵的外部求解器。

Result: Clip-and-Verify在多个基准测试中持续收紧边界，在分支定界中最多减少96%的子问题数量，并在多个基准测试中达到最先进的验证准确率。

Conclusion: 线性约束驱动的剪裁框架是高效可扩展的神经网络验证增强方法，已集成到α,β-CROWN验证器中，成为VNN-COMP 2025获胜者，显著提升了验证效率。

Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $α,β$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $α,β$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.

</details>


### [43] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 首次系统研究部分标签学习在ECG诊断中的应用，评估九种PLL算法对多种临床模糊标签的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现实世界ECG诊断中存在固有的标签模糊问题（如重叠病症和诊断分歧），但现有ECG模型假设标签干净无歧义，限制了模型在真实条件下的发展和评估

Method: 将九种部分标签学习算法适配到多标签ECG诊断中，使用多种临床驱动的模糊生成策略（包括非结构化随机模糊和结构化模糊如心脏病专家相似性、治疗关系、诊断分类学）进行评估

Result: 在PTB-XL和Chapman数据集上的实验表明，不同PLL方法对不同类型和程度的模糊性表现出显著差异的鲁棒性

Conclusion: 识别了当前PLL方法在临床环境中的关键局限性，为开发鲁棒且临床对齐的模糊感知学习框架指明了未来方向

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [44] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文系统研究了测试时缩放（TTS）在视觉语言模型（VLM）中的应用，发现闭源模型能从结构化推理和迭代自优化中获益，而开源VLM表现不一致，且TTS效果高度依赖于数据集和任务类型。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）在提升大语言模型推理能力方面表现出色，但在多模态系统如视觉语言模型（VLM）中的应用仍未被充分探索。本研究旨在填补这一空白，系统评估TTS在不同VLM上的效果。

Method: 对开源和闭源视觉语言模型进行系统性实证研究，应用不同的推理时方法（包括结构化推理、迭代自优化、外部验证等），并在多个基准测试上进行评估。

Result: 闭源模型能稳定受益于结构化推理和迭代自优化；开源VLM表现不一致，外部验证提供最可靠的提升，而迭代优化常导致性能下降；TTS效果高度数据集依赖，在多步推理任务上改善明显，但在感知导向任务上增益有限。

Conclusion: TTS并非通用解决方案，必须根据模型能力和任务特性进行定制。未来工作需要开发自适应TTS策略和多模态奖励模型，以实现更有效的推理时优化。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [45] [In-Context Multi-Objective Optimization](https://arxiv.org/abs/2512.11114)
*Xinyu Zhang,Conor Hassan,Julien Martinelli,Daolang Huang,Samuel Kaski*

Main category: cs.LG

TL;DR: TAMO：基于Transformer的完全摊销式多目标黑盒优化策略，通过预训练实现跨问题迁移，无需每任务重新训练，大幅提升优化效率


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化存在三个主要问题：1）需要针对每个问题定制代理模型和采集函数，难以迁移；2）通常是近视的，缺乏多步规划；3）在并行或时间敏感场景中重新拟合开销大。需要一种通用、高效的优化方法

Method: 提出TAMO方法，使用Transformer架构处理可变输入和目标维度，通过强化学习预训练策略以最大化累积超体积改进。策略基于完整查询历史条件生成新设计，测试时只需单次前向传播即可生成提案

Result: 在合成基准和真实任务中，TAMO生成提案速度比替代方法快50-1000倍，同时在有限评估预算下匹配或改进帕累托前沿质量。实现了完全上下文内的多目标优化

Conclusion: Transformer能够在完全上下文内执行多目标优化，消除了每任务的代理模型拟合和采集函数工程，为科学发现工作流开辟了基础模型式即插即用优化器的新路径

Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.

</details>


### [46] [Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee](https://arxiv.org/abs/2512.11127)
*Kshitiz Khanal*

Main category: cs.LG

TL;DR: 提出结合物理信息图神经网络和连续流匹配的两阶段学习框架，用于快速求解直流最优潮流问题，在保持100%可行性的同时达到接近最优解。


<details>
  <summary>Details</summary>
Motivation: 传统优化求解器计算成本高，不适合大规模系统频繁重计算；机器学习方法虽快但难以保证约束满足和成本最优性。需要一种既快速又能保证可行性和最优性的解决方案。

Method: 两阶段学习框架：第一阶段用物理信息图神经网络生成可行初始解，损失函数编码经济调度最优性条件、基尔霍夫定律和KKT互补条件；第二阶段用连续流匹配技术通过向量场回归优化解。

Result: 在IEEE 30节点系统的5个负荷场景（70%-130%额定负荷）测试中，方法达到接近最优解：额定负荷下成本差距低于0.1%，极端条件下低于3%，同时保持100%可行性。

Conclusion: 该框架在快速神经网络预测和慢速数值求解器之间架起桥梁，为高可再生能源渗透率需要频繁调度更新的现代电力系统提供实用解决方案。

Abstract: The DC Optimal Power Flow (DC-OPF) problem is fundamental to power system operations, requiring rapid solutions for real-time grid management. While traditional optimization solvers provide optimal solutions, their computational cost becomes prohibitive for large-scale systems requiring frequent recalculations. Machine learning approaches offer promise for acceleration but often struggle with constraint satisfaction and cost optimality. We present a novel two-stage learning framework that combines physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM) for solving DC-OPF problems. Our approach embeds fundamental physical principles--including economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker (KKT) complementarity conditions--directly into the training objectives. The first stage trains a GNN to produce feasible initial solutions by learning from physics-informed losses that encode power system constraints. The second stage employs CFM, a simulation-free continuous normalizing flow technique, to refine these solutions toward optimality through learned vector field regression. Evaluated on the IEEE 30-bus system across five load scenarios ranging from 70\% to 130\% nominal load, our method achieves near-optimal solutions with cost gaps below 0.1\% for nominal loads and below 3\% for extreme conditions, while maintaining 100\% feasibility. Our framework bridges the gap between fast but approximate neural network predictions and optimal but slow numerical solvers, offering a practical solution for modern power systems with high renewable penetration requiring frequent dispatch updates.

</details>


### [47] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 论文提出FairOBD算法，解决公平性正则化平滑在线凸优化问题，通过引入辅助变量将长期公平成本分解为在线成本，在动态计算资源分配场景中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在线优化问题中公平性和动作平滑性是两个关键考虑因素，但现有研究尚未同时解决这两个问题。论文研究公平性正则化平滑在线凸优化这一新挑战性场景，旨在同时最小化命中成本、切换成本和公平成本。

Method: 提出FairOBD算法：1）将长期公平成本通过引入辅助变量分解为序列在线成本；2）利用辅助变量正则化在线动作以实现公平结果；3）采用新方法处理切换成本。算法针对离线最优算法的参数化约束基准提供竞争比分析。

Result: 理论证明：1）即使无切换成本，任何在线算法都无法获得次线性遗憾或有限竞争比；2）FairOBD在最坏情况下提供渐近竞争比。实验验证：在动态计算资源分配场景中，FairOBD能有效减少总公平正则化成本，相比基线方案更好地促进公平结果。

Conclusion: FairOBD算法成功调和了命中成本、切换成本和公平成本之间的张力，为解决公平性正则化平滑在线凸优化问题提供了有效方案，在负责任AI推理等应用中具有实用价值。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [48] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Vekua Layer (VL)作为隐式神经表示(INRs)的替代方案，通过将假设空间限制在微分算子核内，将学习任务转化为严格凸最小二乘问题，实现机器精度重建和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)在参数化物理场方面表现出色，但存在谱偏差和非凸优化计算昂贵的问题，需要更高效、稳定的方法。

Method: 基于广义解析函数理论，提出Vekua Layer (VL)微分谱方法。通过将假设空间限制在控制微分算子的核内（使用调和基和傅里叶-贝塞尔基），将学习任务从迭代梯度下降转化为通过线性投影解决的严格凸最小二乘问题。

Result: 在齐次椭圆偏微分方程上评估VL，相比SIRENs：1) 在精确重建任务上达到机器精度(MSE≈10^{-33})；2) 在非相干传感器噪声下表现出优越稳定性(MSE≈0.03)，起到物理信息谱滤波器作用；3) 支持从部分边界数据通过解析延拓进行"全息"外推。

Conclusion: Vekua Layer提供了一种基于微分谱方法的INRs替代方案，通过将学习转化为凸优化问题，实现了机器精度重建、噪声鲁棒性和解析延拓能力，克服了传统INRs的谱偏差和计算效率问题。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [49] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 提出一种结合聚类损失和对比损失的增强自编码器框架，用于高维科学集合数据的可视化与特征提取。


<details>
  <summary>Details</summary>
Motivation: 高维复杂的科学集合数据在分析和可视化方面面临挑战，传统降维方法和自编码器难以有效处理此类数据。

Method: 使用EfficientNetV2为无标签数据生成伪标签，构建结合重构损失、聚类损失（基于软轮廓系数）和对比损失的自编码器框架，最后用UMAP生成2D投影。

Result: 在土壤通道结构和液滴冲击薄膜两个科学集合数据集上，结合聚类或对比损失的模型略优于基线方法。

Conclusion: 提出的增强自编码器框架能有效改善科学集合数据的可视化效果和可解释性，为高维复杂数据的分析提供了新方法。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [50] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异构数据管道，用于融合时空变化的多模态城市数据，通过图学习方法整合空间同质性，实现跨领域、跨城市的可迁移预测框架。


<details>
  <summary>Details</summary>
Motivation: 现代城市依赖数据驱动决策，但城市级数据存在异构格式、多机构独立收集、标准不一的问题。国家级数据集虽然丰富但具有显著异质性和多模态性，需要解决跨领域、跨地域的复杂城市问题。

Method: 提出异构数据管道，执行跨领域数据融合，处理时空变化和时空序列数据集。数据学习模块将空间变化数据集中的同质性整合到图学习中，将不同地域的信息嵌入模型。使用50多个数据源，通过五个真实世界案例验证。

Result: 框架在多种公开数据集（如拼车、交通事故、犯罪报告）上表现出强大的预测性能，迁移到新地域或领域时只需最小化重新配置，展示了良好的泛化性和灵活性。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，为跨城市、跨领域的智能决策提供了有效解决方案。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [51] [Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning](https://arxiv.org/abs/2512.11179)
*Wei Duan,Jie Lu,En Yu,Junyu Xuan*

Main category: cs.LG

TL;DR: 提出BVME方法解决带宽限制下多智能体强化学习的通信压缩问题，通过变分消息编码实现高效信息传输


<details>
  <summary>Details</summary>
Motivation: 现有图基多智能体强化学习方法虽然能学习稀疏协调图（确定谁与谁通信），但未解决带宽限制下应该传输什么信息的问题。简单的维度压缩会降低协调性能，需要更智能的消息编码方法。

Method: 提出带宽约束变分消息编码（BVME），将消息视为从学习到的高斯后验中采样的样本，通过KL散度正则化到无信息先验。该变分框架通过可解释的超参数提供原则性的压缩强度控制。

Result: 在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的情况下，实现了相当或更优的性能。在稀疏图上表现尤为突出，消息质量对协调影响关键。

Conclusion: BVME为带宽受限的多智能体协调提供了有效的消息编码解决方案，在极端压缩比下表现优异，同时增加的计算开销最小。

Abstract: Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.

</details>


### [52] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 该论文提出"进展导向基准测试"新范式，将基准测试从静态问题排行榜转变为开放但可测量的科学问题研究，并以NanoGPT速度跑环境为例，实现了新的训练时间记录。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要关注静态、已解决的问题（如数学应用题），这种方法限制了可测量和激励的进步类型。需要一种新的基准测试范式，使基准测试本身成为科学进步的核心目标。

Method: 提出"进展导向基准测试"概念，并实例化基于NanoGPT速度跑的环境。该环境标准化了数据集切片、参考模型和训练工具、丰富的遥测数据，包含运行时验证和反作弊检查。评估重点放在实现的科学增量：最佳达到的损失和效率前沿。

Result: 使用该环境实现了新的最先进训练时间，比之前记录提高了3秒。定性观察到新颖算法思想的出现。模型和代理之间的比较仍然可能，但只是手段而非目的。

Conclusion: 基准测试应成为科学进步的载体，而非仅仅是模型比较的工具。该工作旨在推动社区从静态问题排行榜转向开放但可测量的科学问题研究，使基准测试进展等同于科学进展。

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [53] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: 本文诊断了ReLU激活函数在物理信息机器学习中表现不佳的原因，发现自动微分无法正确处理不连续场的导数，导致梯度计算错误。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习使用控制微分方程训练神经网络，激活函数的选择影响解的质量和性能。已有研究表明ReLU在基准微分方程上表现不如sigmoid、tanh和swish等激活函数，本文旨在诊断ReLU表现不佳的根本原因。

Method: 通过分析ReLU在物理信息机器学习中的失败案例，特别是针对仅涉及一阶导数的变分问题，研究自动微分在PyTorch中处理不连续场导数时的局限性，揭示梯度计算错误的原因。

Result: 发现ReLU失败的根本原因在于其二阶导数问题，这些导数不是在损失函数公式中直接使用，而是在训练过程中通过自动微分计算。PyTorch的自动微分无法正确表征不连续场的导数，导致物理信息损失的梯度被错误指定。

Conclusion: ReLU在物理信息机器学习中表现不佳的原因是自动微分无法正确处理其不连续特性导致的梯度计算错误，这解释了为什么ReLU在物理信息训练中不如其他平滑激活函数。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [54] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，在扩散模型训练中通过正交投影消除敏感概念特征的影响，实现概念级别的选择性遗忘，解决记忆化带来的安全和知识产权风险。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的记忆化带来严重安全和知识产权风险，传统去记忆化方法（如正则化和数据过滤）只能限制对特定训练样本的过拟合，无法系统性地防止模型内化禁止的概念级特征。直接丢弃包含敏感特征的所有图像会浪费宝贵的训练数据，需要概念级别的选择性遗忘方法。

Method: 提出梯度投影框架，在反向传播过程中识别并消除与禁止属性嵌入对齐的训练信号。具体方法是将每个梯度更新投影到敏感特征嵌入空间的正交补空间上，从而消除该特征对模型权重的影响。该方法可无缝集成到标准扩散模型训练流程中，并与现有防御方法互补。

Result: 在大量实验中证明，该框架能显著减少记忆化，同时严格保持生成质量和语义保真度。通过将记忆化控制重新定义为选择性学习，为IP安全和隐私保护的生成式AI建立了新范式。

Conclusion: 梯度投影框架通过概念级别的选择性遗忘，有效解决了扩散模型记忆化带来的安全和知识产权风险，为隐私保护和IP安全的生成式AI提供了新方法。

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [55] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种利用秩约束来处理线性高斯模型中潜在变量因果发现的样本选择偏差问题的新方法


<details>
  <summary>Details</summary>
Motivation: 潜在变量因果发现中的选择偏差问题重要但研究不足，主要原因是缺乏合适的统计工具。现有的处理潜在变量的工具都没有针对选择偏差进行适配。

Method: 研究秩约束作为条件独立性约束的推广，利用线性高斯模型中协方差子矩阵的秩。提供了这种秩约束的图论特征化，并展示了单因子模型在选择偏差下可被识别。

Result: 尽管选择偏差会显著复杂化联合分布，但偏差协方差矩阵中的秩仍然保留了关于因果结构和选择机制的有意义信息。模拟和真实世界实验证实了秩约束的有效性。

Conclusion: 秩约束为处理潜在变量因果发现中的选择偏差问题提供了一个有效的统计工具，填补了该领域的研究空白。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [56] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态管理KV缓存，减少55-67%的活跃KV缓存大小，同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 解决长上下文LLM部署中的内存限制问题，传统方法要么永久丢弃上下文（影响质量），要么需要大量内存。需要一种无需训练、架构无关的推理时优化方案

Method: 1) 可逆软冻结机制：在滑动注意力窗口中识别低重要性token，临时暂停其KV更新；2) 所有token保存在GPU外存储，按需恢复；3) 次线性冻结调度：冻结时长随重复检测次数次线性增长，避免过度压缩

Result: 在LLaMA-3 8B上的初步实验显示：活跃KV缓存大小减少55-67%，保持生成质量，通过needle-in-haystack检索测试。方法无需微调，架构无关

Conclusion: ASR-KF-EGR为内存受限的长上下文LLM部署提供了实用的推理时解决方案，平衡了内存效率和生成质量，无需额外训练

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [57] [Task-Aware Multi-Expert Architecture For Lifelong Deep Learning](https://arxiv.org/abs/2512.11243)
*Jianyu Wang,Jacob Nean-Hua Sheikh,Cat P. Le,Hoda Bidkhori*

Main category: cs.LG

TL;DR: TAME是一种终身深度学习算法，通过任务感知的多专家系统、重放缓冲区和注意力机制来平衡新任务适应和旧任务知识保留。


<details>
  <summary>Details</summary>
Motivation: 终身深度学习需要在连续学习多个任务时，既能适应新任务，又能保留先前任务的知识，避免灾难性遗忘。

Method: TAME算法包含三个核心组件：1) 任务感知的多专家系统，根据任务相似性选择最相关的预训练专家；2) 共享密集层整合专家特征进行预测；3) 重放缓冲区存储先前任务的代表性样本和嵌入；4) 注意力机制优先处理最相关的存储信息。

Result: 在基于CIFAR-100的二分类任务实验中，TAME在提升新任务准确率的同时，保持了先前任务的性能，有效平衡了适应性和知识保留。

Conclusion: TAME通过任务相似性引导的专家选择、知识转移和注意力机制，在终身学习场景中实现了灵活适应和知识保留的良好平衡。

Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.

</details>


### [58] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: Insight Miner：一个用于生成高质量时间序列描述的多模态大模型，通过TS-Insights数据集进行指令微调，在时间序列分析任务上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域至关重要，但从中挖掘洞察需要深厚的领域专业知识，这个过程耗时耗力。需要开发能够自动生成高质量时间序列描述和洞察的模型。

Method: 提出Insight Miner多模态大模型，并构建TS-Insights数据集（包含100k时间序列窗口）。使用代理工作流：先用统计工具从原始时间序列提取特征，再用GPT-4合成连贯的趋势描述。在TS-Insights上进行指令微调。

Result: Insight Miner在生成时间序列描述和洞察方面超越了最先进的多模态模型（如LLaVA和GPT-4）。

Conclusion: 研究展示了利用多模态大模型进行时间序列分析的有前景方向，是使LLM能够将时间序列作为原生输入模态的基础性步骤。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [59] [A Simple Generalisation of the Implicit Dynamics of In-Context Learning](https://arxiv.org/abs/2512.11255)
*Francesco Innocenti,El Mehdi Achour*

Main category: cs.LG

TL;DR: 该论文扩展了Dherin等人(2025)的理论，将Transformer中的上下文学习视为隐式权重更新，并推广到所有序列位置、任意Transformer块以及包含层归一化的更实际残差块。


<details>
  <summary>Details</summary>
Motivation: 现有关于上下文学习的理论多基于简化模型和数据设置，而Dherin等人最近发现Transformer块可被视为对其前馈网络权重进行隐式更新。本文旨在将此理论推广到更实际、更一般的Transformer架构中。

Method: 提出对Dherin等人理论的简单推广：(1)扩展到所有序列位置（不仅是最后一个位置）；(2)扩展到任意Transformer块（不仅是第一个块）；(3)扩展到包含层归一化的更实际残差块。在简单上下文线性回归任务上进行实证验证。

Result: 成功将理论推广到更一般的Transformer设置，并通过实验验证了理论。研究了不同token之间以及不同块之间的隐式更新关系。

Conclusion: 该工作使Dherin等人的理论更接近实际应用，为在大规模模型上进行验证奠定了基础，有助于更好地理解Transformer中的上下文学习机制。

Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.

</details>


### [60] [Features Emerge as Discrete States: The First Application of SAEs to 3D Representations](https://arxiv.org/abs/2512.11263)
*Albert Miao,Chenliang Zhou,Jiawei Zhou,Cengiz Oztireli*

Main category: cs.LG

TL;DR: 首次将稀疏自编码器应用于3D领域，分析3D重建VAE的特征，发现模型编码离散特征而非连续特征，近似离散状态空间，通过相变框架解释多个反直觉行为。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在文本领域表现强大，但很少应用于3D领域，限制了特征分解的理论探索。本文旨在将SAE应用于3D重建模型，分析其特征表示。

Method: 将SAE应用于最先进的3D重建VAE模型，分析来自Objaverse数据集的53k个3D模型的特征表示，通过状态转换框架分析特征激活。

Result: 发现模型编码离散而非连续特征，近似离散状态空间，特征激活驱动相变。解释了三个反直觉行为：模型倾向于位置编码表示、特征消融的重建损失呈现S形行为、相变点分布的双峰性。

Conclusion: 不仅解释特征分解中的意外现象，还提供了理解模型特征学习动态的框架。双峰分布表明模型重新分配叠加干扰以优先考虑不同特征的显著性。

Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

</details>


### [61] [SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks](https://arxiv.org/abs/2512.11298)
*Hao Zhou,Suman Sourav,Binbin Chen,Ke Yu*

Main category: cs.LG

TL;DR: SRLR：基于符号回归的PLC逻辑恢复方案，用于检测控制器逻辑攻击，通过利用ICS特定属性提升恢复精度，在复杂环境中比现有方法提升高达39%


<details>
  <summary>Details</summary>
Motivation: PLC作为工业控制系统关键组件易受网络攻击，现有检测方法存在局限：基于规范的方法需要专家手动工作或访问源代码，而机器学习方法缺乏决策解释性

Method: 设计SRLR（基于符号回归的逻辑恢复）方案，仅基于PLC输入输出识别其逻辑，利用ICS特定属性增强最新深度符号回归方法：1）部分重要控制逻辑在频域表示更佳；2）控制器多模式运行；3）过滤异常输入；4）降低公式复杂度

Result: SRLR在各种ICS设置中始终优于现有方法，恢复精度在某些挑战性环境中可提高39%，在包含数百个电压调节器的配电网上验证了其处理大规模复杂系统的稳定性

Conclusion: SRLR通过利用ICS特定属性显著提升了PLC逻辑恢复的准确性和可解释性，为控制器逻辑攻击检测提供了有效的解决方案，并能处理大规模复杂工业系统

Abstract: Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.

</details>


### [62] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出使用Golay码的量子纠错方法QGEC，通过Transformer解码器评估性能，在特定噪声模型下Golay码比toric码表现更好


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上计算负载远小于经典计算机，但量子比特易受外部噪声影响。量子纠错(QEC)对于处理量子比特至关重要，其中通过稳定子生成元的测量结果预测实际错误。Golay码在经典信息论中是高效编码方法，研究其在量子纠错中的应用潜力。

Method: 提出Quantum Golay code Error Correction (QGEC)方法，使用Golay码进行量子纠错。采用Transformer作为解码器，在由生成多项式定义的码空间中评估解码器准确性。使用三种不同权重集合和三种具有不同比特翻转错误与相位翻转错误相关性的噪声模型进行评估。在离散均匀分布的噪声模型下，比较相同架构的Transformer解码器在Golay码和toric码上的性能。

Result: 1. 噪声模型相关性越小，解码准确性越好；2. 生成多项式的权重对解码器准确性影响很小；3. Golay码（需要23个数据量子比特，码距为7）比toric码（需要50个数据量子比特，码距为5）获得更高的解码准确性。

Conclusion: 使用Transformer实现量子纠错可能使Golay码能够更高效地实现容错量子计算。Golay码在量子纠错中表现出优于toric码的性能，尽管所需量子比特数更少，但具有更好的纠错能力。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [63] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估分散在不同基准中，难以判断基础模型是否真正超越了训练分布实现泛化。需要统一基准来测量模型在跨领域任务中的通用性。

Method: 提出MultiNet v1.0基准，涵盖六个核心能力领域：视觉定位、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。评估了GPT-5、Pi0和Magma等模型。

Result: 所有模型都未展示出一致的泛化能力。尽管在训练分布内表现良好，但在未见领域、不熟悉模态或跨领域任务转移时都出现显著性能下降。失败表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型的实际能力与通用智能的期望之间存在持续差距。MultiNet v1.0为诊断这些差距和指导未来通用智能体的开发提供了标准化评估框架。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [64] [Condensation-Concatenation Framework for Dynamic Graph Continual Learning](https://arxiv.org/abs/2512.11317)
*Tingxu Yan,Ye Yuan*

Main category: cs.LG

TL;DR: 提出CCC框架解决动态图中因结构变化导致的灾难性遗忘问题，通过压缩历史图快照并选择性拼接嵌入来保护现有节点


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图结构持续变化会导致图神经网络出现灾难性遗忘，现有方法忽略了拓扑变化对现有节点的影响

Method: CCC框架：1) 将历史图快照压缩为紧凑的语义表示，保持原始标签分布和拓扑特性；2) 选择性拼接历史嵌入与当前图表示；3) 改进遗忘度量以量化结构更新对现有节点预测性能的影响

Result: 在四个真实世界数据集上的广泛实验中，CCC表现出优于最先进基线的性能

Conclusion: 提出的CCC框架有效解决了动态图持续学习中的灾难性遗忘问题，通过压缩-拼接策略保护了现有节点免受拓扑变化的影响

Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.

</details>


### [65] [Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation](https://arxiv.org/abs/2512.11332)
*Sara Sameer,Wei Zhang,Kannan Dhivya Dharshini,Xin Lou,Yulin Gao,Terence Goh,Qingyu Yan*

Main category: cs.LG

TL;DR: Pace：一种用于电池健康估计的物理感知注意力时序卷积网络，通过整合原始传感器数据和电池物理特征，在公共数据集上比现有模型性能提升6.5倍，并在树莓派上实现实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 电池是现代能源系统（如电动汽车和电网储能）的关键组件，有效的电池健康管理对系统安全、成本效益和可持续性至关重要。需要开发能够准确预测电池健康状况的模型。

Method: 提出Pace模型，整合原始传感器测量数据和等效电路模型导出的电池物理特征。开发三个电池专用模块：用于高效时序编码的扩张时序块、用于上下文建模的分块注意力块，以及融合短期和长期电池退化模式的双头输出块。

Result: 在大型公共数据集上，Pace性能显著优于现有模型，相比两个最佳基线模型平均性能提升6.5倍和2.0倍。进一步在树莓派上实现实时边缘部署，证明了实际可行性。

Conclusion: Pace为电池健康分析提供了一个实用且高性能的解决方案，能够准确高效地预测各种电池使用条件下的健康状况，具有实际部署价值。

Abstract: Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.

</details>


### [66] [Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss](https://arxiv.org/abs/2512.11334)
*Cong Yao,Chunye Gong,Jin Zhang*

Main category: cs.LG

TL;DR: 本文提出SEPI-TFPNet混合模型，结合经验模型与深度学习，通过物理先验子模块和数据驱动子模块提升磁芯损耗建模精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统磁芯损耗建模方法预测精度有限，纯数据驱动模型虽然拟合性能强，但可解释性和跨分布泛化能力不足。为解决这些问题，需要开发结合物理先验和数据驱动优势的混合模型。

Method: 提出SEPI-TFPNet混合模型：1）物理先验子模块使用谱熵判别机制选择最适合不同激励波形的经验模型；2）数据驱动子模块结合CNN、多头注意力机制和双向LSTM提取磁通密度时间序列特征；3）引入自适应特征融合模块改善多模态特征交互与整合。

Result: 使用MagNet数据集评估，与2023年挑战赛的21个代表性模型以及2024-2025年的三种先进方法比较，所提方法在建模精度和鲁棒性方面均取得改进。

Conclusion: SEPI-TFPNet混合模型成功结合了经验模型与深度学习的优势，在磁芯损耗建模中实现了更高的精度和更好的泛化能力，为高功率电子系统设计提供了更可靠的工具。

Abstract: Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.

</details>


### [67] [DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning](https://arxiv.org/abs/2512.11342)
*Jinming Ge,Linfeng Du,Likith Anaparty,Shangkun Li,Tingyuan Liang,Afzal Ahmad,Vivek Chaturvedi,Sharad Sinha,Zhiyao Xie,Jiang Xu,Wei Zhang*

Main category: cs.LG

TL;DR: DAPO框架通过设计结构感知的优化策略排序，利用对比学习生成程序语义嵌入，结合硬件指标估计模型，指导强化学习代理发现特定设计的优化策略，相比Vitis HLS平均获得2.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具依赖从软件编译继承的固定优化策略，缺乏针对特定设计的定制化能力，需要深度语义理解、准确硬件指标估计和高级搜索算法等当前方法不具备的能力。

Method: 提出DAPO框架：1) 从控制流和数据流图提取程序语义；2) 使用对比学习生成丰富嵌入；3) 利用分析模型进行准确硬件指标估计；4) 这些组件共同指导强化学习代理发现设计特定的优化策略。

Result: 在经典HLS设计上的评估显示，端到端流程相比Vitis HLS平均获得2.36倍加速。

Conclusion: DAPO通过设计结构感知的优化策略排序，结合程序语义理解、硬件指标估计和强化学习，能够有效发现特定设计的优化策略，显著提升HLS性能。

Abstract: High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.
  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.

</details>


### [68] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 本文提出了一种对称感知的强化学习框架，用于引导等变扩散策略，通过利用几何对称性提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但使用标准（非等变）强化学习进行微调会忽略这些对称性，导致样本效率低下和不稳定。需要开发能够利用对称性的引导框架。

Method: 首先从理论上证明等变扩散过程的等变性，这诱导出适合等变扩散引导的群不变潜在噪声MDP。基于此理论，提出原则性的对称感知引导框架，并比较标准、等变和近似等变强化学习策略。

Result: 实验表明，在对称性引导过程中利用对称性能带来显著好处：提高样本效率、防止价值发散，即使在演示数据极其有限的情况下也能实现强大的策略改进。同时识别了严格等变性在对称性破坏下的实际边界。

Conclusion: 对称感知的强化学习框架能够有效引导等变扩散策略，利用几何对称性提高样本效率和稳定性，为从有限演示数据中学习提供了有前景的方法。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [69] [CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?](https://arxiv.org/abs/2512.11352)
*Jie Wang,Zheng Yan,Jiahe Lan,Xuyan Li,Elisa Bertino*

Main category: cs.LG

TL;DR: CAT是首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，通过连续时间表示、双重注意力机制和元路径上下文特征提取，在三个真实数据集上优于五组基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的信任预测模型存在三个主要局限：1) 无法捕捉信任动态性；2) 忽略真实网络的异质性；3) 不支持上下文感知这一信任基本属性。这些限制导致预测结果不可靠且粒度粗糙。

Method: CAT模型包含图构建层、嵌入层、异质注意力层和预测层。采用连续时间表示处理动态图，通过时间编码函数捕捉时序信息。使用双重注意力机制建模图异质性（节点类型重要性和类型内节点重要性）。引入元路径概念提取上下文特征，构建上下文嵌入并集成上下文感知聚合器。

Result: 在三个真实世界数据集上的大量实验表明，CAT在信任预测方面优于五组基线方法，同时展现出对大规模图的强可扩展性，以及对信任导向和GNN导向攻击的鲁棒性。

Conclusion: CAT是首个支持信任动态性、准确表示真实世界异质性并具备上下文感知能力的GNN信任预测模型，通过创新的架构设计解决了现有方法的局限性，在多个维度上表现出优越性能。

Abstract: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.
  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.

</details>


### [70] [Attacking and Securing Community Detection: A Game-Theoretic Framework](https://arxiv.org/abs/2512.11359)
*Yifan Niu,Aochuan Chen,Tingyang Xu,Jia Li*

Main category: cs.LG

TL;DR: 该论文将对抗图概念扩展到社区检测问题，提出攻击和防御技术，并建立博弈论框架CD-GAME模拟攻防交互行为


<details>
  <summary>Details</summary>
Motivation: 现有对抗图研究主要关注分类任务，但社区检测问题更具挑战性。需要研究如何在社区检测中保护个人隐私（如社交网络）和理解伪装模式（如交易网络），这需要开发有效的攻击和防御技术

Method: 提出针对社区检测的新型攻击和防御技术，并建立博弈论框架CD-GAME。攻击方试图隐藏目标个体，防御方使用Rayleigh Quotient方法增强模型鲁棒性。双方动态更新策略直至达到纳什均衡

Result: 实验表明提出的攻击和防御方法均显著优于现有基线。CD-GAME揭示了攻防交互的动态演化过程：传统单步攻击中攻击者采用最有效但易被检测的策略，而在纳什均衡时攻击者采用更隐蔽且仍能保持攻击效果的策略

Conclusion: 成功将对抗图概念扩展到社区检测领域，提出的CD-GAME框架为理解社区检测中的交互攻防场景提供了宝贵见解，展示了在纳什均衡下攻击者会采用更隐蔽策略的博弈动态

Abstract: It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.

</details>


### [71] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新的强化学习框架，通过将安全策略梯度投影到通用任务零空间来减少对齐税，在保持LLM核心能力的同时实现安全对齐


<details>
  <summary>Details</summary>
Motivation: LLM在现实应用中需要确保行为符合人类价值观和社会规范，但传统的RL安全对齐会导致模型遗忘已学习的通用能力（对齐税问题）

Method: 提出零空间约束策略优化（NSPO），将安全策略梯度几何投影到通用任务的零空间中，从而减少对齐税。该方法理论上保证保持模型原始核心能力，同时确保安全对齐的有效下降方向

Result: NSPO大幅超越现有方法，在不牺牲数学、代码和指令跟随等通用任务准确性的情况下，实现了最先进的安全性能。数据效率高，仅需PKU-SafeRLHF中40%的安全数据

Conclusion: NSPO有效解决了LLM安全对齐中的对齐税问题，在保持核心能力的同时实现高效安全对齐，为LLM的安全部署提供了有前景的解决方案

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [72] [Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings](https://arxiv.org/abs/2512.11392)
*S Sairam,Prateek P Kulkarni*

Main category: cs.LG

TL;DR: 提出一种结合数论中Bhargava立方体代数约束的神经表示学习方法，在3维潜在空间中学习满足二次关系的结构化嵌入，提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在非结构化潜在空间中学习表示，缺乏可解释性和数学一致性。需要将数学结构融入神经网络表示学习。

Method: 将输入数据映射到受约束的3维潜在空间，嵌入通过可微辅助损失函数进行正则化，使其满足从Bhargava组合结构推导出的二次关系，独立于分类目标。

Result: 在MNIST上达到99.46%准确率，同时产生可解释的3D嵌入，自然地按数字类别聚类并满足学习的二次约束。

Conclusion: 这是数论构造在神经表示学习中的首次应用，为在神经网络中融入结构化数学先验奠定了基础，相比需要显式几何监督的流形学习方法，该方法通过可微约束施加弱代数先验。

Abstract: We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.

</details>


### [73] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过排序实现O(n log n)复杂度，适合长上下文处理，保持理论表达能力


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如softmax和ReLU-based）在计算复杂度或表达能力方面存在限制，需要一种既能处理长上下文又保持理论表达能力的注意力机制

Method: 提出切片ReLU注意力机制：对键-查询差异的一维投影进行操作，利用排序获得准线性复杂度，构建可微分的非对称核函数

Result: 实现O(n log n)计算复杂度，保持理论表达能力（能执行非平凡的序列到序列解缠任务，满足上下文通用逼近性质），小规模实验显示实用潜力

Conclusion: 切片ReLU注意力机制在计算效率和理论表达能力之间取得了良好平衡，为处理长上下文序列提供了有前景的替代方案

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [74] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS扩展高斯模糊均值漂移到双曲空间，用于处理具有层次结构的数据集，在非欧几里得设置中显著优于传统均值漂移方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移(GBMS)在欧几里得空间中有效，但无法处理具有层次或树状结构的数据集，需要扩展到双曲空间来捕捉潜在层次结构。

Method: 将欧几里得计算替换为双曲距离，使用Möbius加权均值确保所有更新与空间几何保持一致，保持密度追踪行为的同时适应双曲几何。

Result: 在11个真实世界数据集上的实验表明，HypeGBMS在非欧几里得设置中显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。

Conclusion: HypeGBMS桥接了经典均值漂移聚类和双曲表示学习，为弯曲空间中的密度基聚类提供了原则性方法，能有效捕捉数据中的层次结构。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [75] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 提出Plasticity-Ceiling框架，理论分析专家轨迹利用机制，确立SFT-then-RL为最佳流程，并提供具体扩展指南


<details>
  <summary>Details</summary>
Motivation: 当前后训练中SFT和RL结合时，如何最优利用专家轨迹的问题尚未解决，需要理论框架指导实践

Method: 提出Plasticity-Ceiling理论框架，将性能分解为基础SFT性能和后续RL可塑性，通过大量基准测试验证

Result: 确立SFT-then-RL顺序流程为最佳标准，提供三个具体扩展指南：1)在SFT稳定或轻度过拟合阶段转向RL；2)数据规模决定后训练潜力，轨迹难度作为性能乘数；3)SFT最小验证损失可作为选择专家轨迹的指标

Conclusion: 为最大化专家轨迹价值提供可操作指南，解决了后训练中SFT和RL结合的最优机制问题

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [76] [NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics](https://arxiv.org/abs/2512.11525)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Guangliang Liu,Yuxuan Liang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: NeuralOGCM：融合可微分编程与深度学习的海洋建模框架，通过可学习的物理求解器和神经网络协同工作，在保持物理一致性的同时提升计算效率


<details>
  <summary>Details</summary>
Motivation: 解决科学模拟中长期存在的计算效率与物理保真度之间的权衡问题，传统数值模型计算成本高，纯AI方法缺乏物理一致性

Method: 1) 完全可微分的动力学求解器作为核心物理归纳偏置；2) 将关键物理参数（如扩散系数）转化为可学习参数；3) 深度神经网络校正亚网格尺度过程和离散化误差；4) 统一ODE求解器集成两个组件的输出

Result: NeuralOGCM保持长期稳定性和物理一致性，在速度上显著优于传统数值模型，在精度上优于纯AI基线方法

Conclusion: 该工作为构建快速、稳定且物理合理的科学计算模型开辟了新路径，实现了物理知识与数据驱动方法的有效融合

Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.

</details>


### [77] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常和持久性分布漂移来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测中，有些异常事件具有持久影响需要响应，而有些短期异常应该被忽略。传统预测模型无法区分这两类异常，要么对噪声过度反应，要么错过真正的分布漂移。

Method: 提出Co-TSFA对比学习正则化框架：1) 生成输入增强和输入-输出增强分别模拟预测无关和预测相关异常；2) 引入潜在输出对齐损失，将表示变化与预测变化关联；3) 鼓励对无关扰动保持不变性，同时对有意义的分布漂移保持敏感性。

Result: 在Traffic和Electricity基准测试以及真实现金需求数据集上的实验表明，Co-TSFA在异常条件下提升预测性能，同时在正常数据上保持准确性。

Conclusion: Co-TSFA通过对比学习有效区分短期异常和持久性分布漂移，为时间序列预测提供了一种鲁棒的异常处理方法，在现实应用中具有实用价值。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [78] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: xGR是一个面向生成式推荐系统的服务系统，通过统一处理prefill和decode阶段、早期排序终止和基于掩码的项目过滤、重构流水线实现多级重叠和多流并行，在严格延迟约束下实现至少3.49倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统虽然整合了LLM来增强对长用户-项目序列的理解，但其工作负载与LLM服务有显著差异。GR通常处理长提示但产生短固定长度输出，且由于大波束宽度导致每个解码阶段计算成本特别高。此外，波束搜索涉及庞大的项目空间，排序开销特别耗时。

Method: 1. 通过分阶段计算和分离的KV缓存统一处理prefill和decode阶段；2. 实现早期排序终止和基于掩码的项目过滤，重用数据结构；3. 重构整体流水线以利用多级重叠和多流并行。

Result: 在真实世界推荐服务数据集上的实验表明，xGR在严格延迟约束下相比最先进的基线实现了至少3.49倍的吞吐量提升。

Conclusion: xGR是一个面向生成式推荐系统的专用服务系统，能够满足高并发场景下的严格低延迟要求，通过优化计算模式、排序机制和流水线设计，显著提升了系统性能。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [79] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 提出一种结合导数信息的微分机器学习方法，用于求解参数积分问题，在多个问题类别中优于传统机器学习方法


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在求解参数积分问题时存在局限性，需要一种能够有效利用导数信息、提高精度和效率的新方法

Method: 采用微分学习框架，在训练过程中融入导数信息，应用于三类代表性参数积分问题：统计泛函、切比雪夫展开函数逼近、微分方程积分

Result: 微分机器学习方法在所有测试案例中均优于标准架构，实现了更低的均方误差、更好的可扩展性和更高的样本效率

Conclusion: 微分机器学习是求解参数积分问题的有效方法，能够显著提升性能，适用于从光滑闭式基准到复杂数值积分的广泛问题

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [80] [A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts](https://arxiv.org/abs/2512.11541)
*Emmanuel K. Katalay,David O. Dimandja,Jordan F. Masakuna*

Main category: cs.LG

TL;DR: 提出一个自动化MLOps管道，用于检测数据分布漂移并仅在必要时触发神经网络分类器重训练，提高计算效率和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型性能会因数据分布随时间变化而下降（数据分布漂移），而当前的MLOps流程通常是手动的，需要人工触发模型重训练和重新部署，效率低下且资源消耗大。

Method: 设计了一个自动化MLOps管道，采用多标准统计技术检测数据分布变化，仅在检测到显著漂移时触发模型更新，确保计算效率和资源优化。

Result: 在多个基准异常检测数据集上的实验表明，相比传统重训练策略，该框架能显著提高模型准确性和鲁棒性。

Conclusion: 该工作为在动态现实环境中部署更可靠、自适应的机器学习系统奠定了基础，这些环境中数据分布变化很常见。

Abstract: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.

</details>


### [81] [Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting](https://arxiv.org/abs/2512.11546)
*Federico Pennino,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 提出一个数据选择框架，通过优化训练数据的组成而非模型超参数，发现从大规模未标记时间序列数据中提取最优"训练食谱"，在某些情况下实现"少即是多"的效果。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习训练范式假设数据越多越好，但原始传感器数据通常存在不平衡和冗余问题，并非所有数据点对模型泛化都有同等贡献。需要探索数据选择对模型性能的影响。

Method: 1) 使用大规模编码器和k-means聚类将数据集划分为行为一致的簇，作为训练"原料"；2) 采用Optuna优化框架搜索可能的数据混合空间；3) 对每个试验，Optuna提出各簇的采样比例，构建新训练集；4) 训练并评估较小的目标模型。

Result: 在PMSM数据集上，该方法将基线MSE从1.70提升到1.37，性能提高了19.41%。数据中心的搜索持续发现比在整个数据集上训练的基线性能显著更高的数据混合方案。

Conclusion: 通过优化训练数据的组成而非模型超参数，可以实现"少即是多"的效果。数据选择对模型性能有重要影响，数据中心的优化方法能够发现比传统全数据训练更好的数据混合方案。

Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.

</details>


### [82] [Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction](https://arxiv.org/abs/2512.11547)
*Janaina Mourão-Miranda,Zakria Hussain,Konstantinos Tsirlis,Christophe Phillips,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 提出一种新的弹性网络正则化多核学习方法，通过解析更新核权重，在神经影像应用中实现更稀疏、更可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有弹性网络正则化多核学习方法采用两阶段优化过程，计算复杂。需要一种更简单高效的算法来在神经影像等需要模型可解释性的领域应用弹性网络正则化。

Method: 提出新的弹性网络正则化多核学习公式，推导出核权重的解析更新规则。为支持向量机和核岭回归分别开发了具体算法，并在PRoNTo工具箱中实现。

Result: 在三个神经影像应用中，弹性网络正则化多核学习在所有任务中都匹配或优于l1正则化多核学习，仅在一个场景中略逊于标准支持向量机。更重要的是，它能产生更稀疏、更可解释的模型。

Conclusion: 提出的弹性网络正则化多核学习方法提供了简单高效的核权重更新方案，在保持性能的同时提高了模型的可解释性，特别适用于神经影像等需要解释相关核信息的领域。

Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

</details>


### [83] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 提出Graph View Transformation (GVT)和Recurrent GVT，通过视图空间实现完全归纳的图节点表示学习，在27个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 预训练模型在未见数据集上的泛化是基础模型的关键，但图结构数据中特征空间的维度和语义差异很大，限制了模型的归纳能力

Method: 引入视图空间作为统一表示轴，提出节点和特征置换等变的Graph View Transformation (GVT)，并构建Recurrent GVT作为完全归纳的节点表示学习模型

Result: 在OGBN-Arxiv上预训练，在27个节点分类基准测试中，Recurrent GVT比现有完全归纳模型GraphAny提升8.93%，比12个单独调优的GNN至少提升3.30%

Conclusion: 视图空间为完全归纳的节点表示学习提供了原则性和有效的理论基础，GVT方法在跨数据集泛化方面表现出色

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [84] [Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model](https://arxiv.org/abs/2512.11582)
*Sam Gijsen,Marc-Andre Schulz,Kerstin Ritter*

Main category: cs.LG

TL;DR: Brain-Semantoks是一个自监督框架，通过语义标记器和自蒸馏目标学习大脑动态的抽象表征，在fMRI时间序列上表现出色，无需大量微调即可用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI基础模型通常在小脑区域上使用掩码重建目标进行训练，这种对低层次信息的关注导致表征对噪声和时间波动敏感，需要大量微调才能用于下游任务。

Method: 1. 语义标记器：将噪声区域信号聚合成代表功能网络的鲁棒标记；2. 自蒸馏目标：强制表征在时间上的稳定性；3. 新颖的训练课程：确保模型从低信噪比时间序列中稳健学习有意义的特征。

Result: 学习到的表征即使仅使用线性探针也能在各种下游任务上表现出强大的性能。全面的缩放分析表明，更多的无标签数据可靠地带来分布外性能提升，无需领域适应。

Conclusion: Brain-Semantoks框架能够学习大脑动态的抽象表征，解决了当前fMRI基础模型对噪声敏感和需要大量微调的问题，为预测疾病和认知相关表型提供了有前景的解决方案。

Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.

</details>


### [85] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 提出原子动作切片(AAS)方法，将长时程演示分解为短类型化原子动作，提升VLA模型泛化能力，在LIBERO数据集上验证了效果


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作(VLA)模型泛化能力差，特别是在需要新技能或对象组合的任务中表现不佳，需要更好的动作分解方法

Method: 提出原子动作切片(AAS)方法，将长时程演示分解为短类型化原子动作，使用更强的分割器(Gemini 2.5 Pro)匹配规划器定义的计划，并在LIBERO数据集上构建验证数据集

Result: 在LIBERO-Goal上任务成功率从94.2%提升到95.3%，在LIBERO-Long上从83.8%提升到88.8%，并公开了GATE-VLAP数据集

Conclusion: 原子动作切片方法能有效提升VLA模型的泛化能力，特别是在需要新技能组合的任务中，为机器人学习提供了更好的动作表示方法

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [86] [Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration](https://arxiv.org/abs/2512.11587)
*Alexander Tyurin*

Main category: cs.LG

TL;DR: 该论文通过将神经网络梯度下降简化为广义感知器算法，揭示了非线性模型相比线性模型能实现更快的迭代复杂度，解释了神经网络中的隐式加速现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络训练的优化动态（包括收敛速度、迭代轨迹、函数值振荡以及隐式加速现象）是一个具有挑战性的问题。作者旨在为梯度下降在神经网络训练中的动态行为提供新的理论视角。

Method: 将使用逻辑损失的神经网络梯度下降步骤简化为广义感知器算法（Rosenblatt, 1958），利用经典线性代数工具分析简化后的算法步骤，并在一个最小化示例上进行理论分析。

Result: 理论证明在两层模型中，非线性相比线性模型能实现更快的迭代复杂度：非线性模型为$\tilde{O}(\sqrt{d})$，而线性模型为$Ω(d)$，其中$d$是特征数量。数值实验支持了理论结果。

Conclusion: 通过将梯度下降简化为广义感知器算法，为理解神经网络的优化动态和隐式加速现象提供了新的理论视角，这一替代观点有望进一步推动神经网络优化研究。

Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.

</details>


### [87] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 提出一种将经典树分裂算法从清晰规则扩展到模糊树的适应方法，结合贪心算法的计算效率和模糊逻辑的可解释性优势，在保持竞争性预测性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 模糊规则系统因其可解释的语义规则而被广泛用于可解释决策，但现有模糊规则挖掘算法不能同时保证合理的语义分区和小规则库规模。进化方法计算成本过高，而基于神经的方法（如ANFIS）难以保持语义解释。

Method: 将经典树分裂算法从清晰规则扩展到模糊树，结合贪心算法的计算效率和模糊逻辑的可解释性优势，实现可解释的语义分区。

Result: 在表格分类基准测试中，该方法达到了与最先进模糊分类器相当的准确率，同时显著降低了计算成本，并产生具有约束复杂度的更可解释规则库。

Conclusion: 该方法成功地将树分裂算法扩展到模糊树，在保持预测性能的同时实现了计算效率和可解释性的平衡，为可解释模糊决策系统提供了一种高效解决方案。

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [88] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 论文提出将大型上下文表格模型作为流式持续学习的桥梁，通过将无限数据流实时压缩为紧凑摘要，同时满足流学习的数据压缩需求和持续学习的经验回放需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究社区将流学习和持续学习分开处理：流学习关注高频数据流的快速适应但忽略遗忘问题，持续学习关注长期知识保留但缺乏实时约束。需要一种统一框架来同时处理流式数据的高效适应和长期知识保留。

Method: 提出使用大型上下文表格模型作为流式持续学习的桥梁，将无限数据流实时压缩为紧凑摘要供模型使用。基于数据选择的两个核心原则：1) 分布匹配（平衡可塑性和稳定性），2) 分布压缩（通过多样化和检索机制控制内存大小）。

Result: 论文提出了一个理论框架，将流学习和持续学习统一在流式持续学习范式下，通过大型上下文表格模型实现数据流的实时压缩和知识保留的平衡。

Conclusion: 大型上下文表格模型为流式持续学习提供了自然桥梁，通过分布匹配和分布压缩两个核心原则，能够同时满足流学习的数据压缩需求和持续学习的经验回放需求，为两个研究领域的融合提供了清晰的理论框架。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [89] [High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control](https://arxiv.org/abs/2512.11705)
*Sebastian Hirt,Valentinus Suwanto,Hendrik Alsmeier,Maik Pfefferkorn,Rolf Findeisen*

Main category: cs.LG

TL;DR: 贝叶斯神经网络作为代理模型在优化高维控制器参数方面优于传统高斯过程，能处理数百甚至上千维的参数空间


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理密集高维控制器参数（如模型预测控制器的参数）时效果不佳，因为标准代理模型难以捕捉高维空间的结构

Method: 使用贝叶斯神经网络作为代理模型，并与使用Matern核的高斯过程、有限宽度贝叶斯神经网络和无限宽度贝叶斯神经网络在倒立摆任务上进行比较

Result: 贝叶斯神经网络代理模型能更快更可靠地收敛闭环成本，成功优化数百维参数化；无限宽度贝叶斯神经网络在超过一千个参数的设置中仍保持性能，而Matern核高斯过程效果迅速下降

Conclusion: 贝叶斯神经网络代理模型适合学习密集高维控制器参数化，为基于学习的控制器设计中代理模型选择提供了实用指导

Abstract: Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.

</details>


### [90] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum：一种融合谱子空间估计与几何邻居选择的联邦学习防御方法，通过历史聚合学习良性更新轨迹的低维流形，在压缩坐标中应用Krum选择，过滤正交残差能量超标的候选更新。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临拜占庭客户端注入恶意更新的威胁，现有鲁棒聚合方法在数据分布异构且攻击者能观察防御机制时效果显著下降，需要更有效的防御方案。

Method: SpectralKrum结合谱子空间估计与几何邻居选择：1）从历史聚合中学习良性更新轨迹的低维流形；2）将传入更新投影到该子空间；3）在压缩坐标中应用Krum选择；4）过滤正交残差能量超过数据驱动阈值的候选更新。

Result: 在CIFAR-10非IID数据上评估，SpectralKrum对方向性和子空间感知攻击（adaptive-steer, buffer-drift）具有竞争力，但在标签翻转和最小最大攻击下优势有限，因为恶意更新在谱特征上与良性更新难以区分。

Conclusion: SpectralKrum为联邦学习提供了一种无需辅助数据、保护隐私的防御方法，特别适用于方向性和子空间感知攻击，但在某些攻击场景下仍需改进。

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [91] [The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation](https://arxiv.org/abs/2512.11776)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: AVC提出了一种混合架构，通过解耦流形学习和函数逼近，使用深度网络学习物理域的微分同胚扭曲，将复杂时空动力学投影到潜在流形上，并用广义解析函数基表示解，同时用可微分线性求解器替代梯度下降输出层，实现谱系数的闭式最优求解。


<details>
  <summary>Details</summary>
Motivation: 解决基于坐标的神经网络在表示连续物理场时面临的两个基本问题：频谱偏差（阻碍高频动力学学习）和维度灾难（导致离散特征网格参数爆炸）。

Method: AVC架构通过深度网络学习物理域的微分同胚扭曲，将复杂动力学投影到潜在流形，用广义解析函数基表示解，并用可微分线性求解器替代标准梯度下降输出层，实现谱系数的闭式最优求解。

Result: 在五个严格的物理基准测试中，AVC实现了最先进的精度，同时将参数数量减少了数个数量级（例如，840个参数 vs. 3D网格的420万个参数），收敛速度比隐式神经表示快2-3倍。

Conclusion: 这项工作为内存高效、频谱精确的科学机器学习建立了新范式，通过结合深度学习和经典逼近理论，有效解决了频谱偏差和维度灾难问题。

Abstract: Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.

</details>


### [92] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 提出基于测度的统一框架分析softmax注意力，证明在长提示下softmax注意力收敛到线性算子，继承了线性注意力的分析结构


<details>
  <summary>Details</summary>
Motivation: softmax注意力是Transformer的核心组件，但其非线性结构给理论分析带来挑战。需要建立统一框架来研究softmax注意力在有限和无限提示下的行为

Method: 开发基于测度的统一框架，利用softmax算子在无限提示极限下收敛到线性算子的性质，建立输出和梯度的非渐近集中界，分析训练动态

Result: 证明softmax注意力在长提示下快速收敛到其无限提示对应物，训练轨迹保持稳定，在线性回归等任务中可基于无限提示动态分析有限提示训练

Conclusion: 长提示下的softmax注意力继承了线性注意力的分析结构，为研究softmax注意力层的训练动态和统计行为提供了原则性工具包

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [93] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于几何模式的L形分析方法，通过随机顺序添加元素并绘制贡献图来发现特征间的交互、冗余和独立关系。


<details>
  <summary>Details</summary>
Motivation: 许多系统存在复杂的组件交互：有些特征相互增强，有些提供冗余信息，有些独立贡献。需要一种统一的方法来量化和可视化这些交互结构。

Method: 通过随机顺序添加元素，在多轮试验中绘制贡献图，观察L形模式。提出L-score连续度量（-1到+1），量化交互程度。可视化二维点云，冗余对形成L形（仅先添加元素贡献），协同对形成反L形（仅共同贡献），独立元素显示顺序不变分布。

Result: 该方法能够区分交互、独立和冗余模式，通过L-score量化关系，相对尺度揭示特征主导性。虽然仅基于成对测量，但通过一致的跨对关系自然涌现高阶交互（如AB、AC、BC）。

Conclusion: 该方法与度量无关，适用于任何可以按非重复元素序列增量评估性能的领域，为揭示交互结构提供了统一的几何方法。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [94] [Generalization of Long-Range Machine Learning Potentials in Complex Chemical Spaces](https://arxiv.org/abs/2512.10989)
*Michal Sanocki,Julija Zavadlav*

Main category: physics.chem-ph

TL;DR: 长程修正对机器学习原子间势的泛化能力至关重要，特别是在处理化学空间分布外样本时能显著提升性能


<details>
  <summary>Details</summary>
Motivation: 化学空间的广阔性使得机器学习原子间势的泛化成为核心挑战，现有模型在分布外样本上的迁移能力有限

Method: 系统评估不同长程修正的MLIP架构，引入有偏训练-测试分割策略来测试模型在化学空间不同区域的性能

Result: 长程修正方案不仅提升分布内性能，更重要的是显著增强了模型对未见化学空间区域的迁移能力

Conclusion: 长程建模对实现可泛化的MLIP至关重要，提出的框架可用于诊断化学空间中的系统性失败，方法适用于多种材料

Abstract: The vastness of chemical space makes generalization a central challenge in the development of machine learning interatomic potentials (MLIPs). While MLIPs could enable large-scale atomistic simulations with near-quantum accuracy, their usefulness is often limited by poor transferability to out-of-distribution samples. Here, we systematically evaluate different MLIP architectures with long-range corrections across diverse chemical spaces and show that such schemes are essential, not only for improving in-distribution performance but, more importantly, for enabling significant gains in transferability to unseen regions of chemical space. To enable a more rigorous benchmarking, we introduce biased train-test splitting strategies, which explicitly test the model performance in significantly different regions of chemical space. Together, our findings highlight the importance of long-range modeling for achieving generalizable MLIPs and provide a framework for diagnosing systematic failures across chemical space. Although we demonstrate our methodology on metal-organic frameworks, it is broadly applicable to other materials, offering insights into the design of more robust and transferable MLIPs.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [95] [LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems](https://arxiv.org/abs/2512.11750)
*Ernesto Casablanca,Oliver Schön,Paolo Zuliani,Sadegh Soudjani*

Main category: eess.SY

TL;DR: LUCID是首个能够为黑盒随机动力系统提供量化安全保证的验证工具，通过数据驱动的控制屏障证书方法，利用核希尔伯特空间嵌入和傅里叶核展开，将半无限非凸优化问题转化为可处理的线性规划问题。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和医疗等高风险领域，AI系统的安全性至关重要。传统形式化验证工具难以处理包含不透明黑盒AI组件和复杂随机动态的系统，需要新的验证方法。

Method: 采用数据驱动的控制屏障证书方法，使用条件均值嵌入将数据映射到再生核希尔伯特空间，构建RKHS模糊集以增强对分布外行为的鲁棒性。关键创新是使用有限傅里叶核展开将半无限非凸优化问题转化为可处理的线性规划问题。

Result: LUCID是首个能够为黑盒随机动力系统建立量化安全保证的工具，具有模块化架构和良好文档，易于扩展。通过快速傅里叶变换高效生成松弛问题，提供可扩展且分布鲁棒的安全验证框架。

Conclusion: LUCID提供了一个鲁棒高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供形式化的安全保证，在具有挑战性的基准测试中展示了其独特能力。

Abstract: Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [96] [From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines](https://arxiv.org/abs/2512.11724)
*Titaya Mairittha,Tanakon Sawanglok,Panuwit Raden,Jirapast Buntub,Thanapat Warunee,Napat Asawachaisuvikrom,Thanaphum Saiwongin*

Main category: cs.HC

TL;DR: 论文分析语音AI系统中模块化S2S-RAG流水线导致的对话摩擦，识别三种对话崩溃模式，指出这是模块化设计优先控制而非流畅性的结构后果，建议将自然语音AI视为基础设施设计挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管语音AI系统在生成能力上取得显著进展，但其交互常感觉对话断裂。本文旨在研究模块化语音到语音检索增强生成流水线中出现的交互摩擦，超越简单的延迟指标，深入理解对话崩溃的根本原因。

Method: 通过分析代表性生产系统，识别三种反复出现的对话崩溃模式：时间错位、表达扁平化和修复僵化。进行系统级分析，展示这些摩擦点不应被视为缺陷，而是模块化设计优先控制而非流畅性的结构后果。

Result: 识别出三种核心对话摩擦模式：1)时间错位-系统延迟违反用户对对话节奏的期望；2)表达扁平化-副语言线索丢失导致字面化、不适当的回应；3)修复僵化-架构门控阻止用户实时纠正错误。这些是模块化设计的结构性后果。

Conclusion: 构建自然语音AI是基础设施设计挑战，需要从优化孤立组件转向精心编排组件间的连接。摩擦点不应被视为缺陷，而是模块化设计优先控制而非流畅性的必然结果，需要系统级设计思维。

Abstract: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [97] [Boosted Random Forests for Predicting Treatment Failure of Chemotherapy Regimens](https://arxiv.org/abs/2512.10995)
*Muhammad Usamah Shahid,Muddassar Farooq*

Main category: q-bio.QM

TL;DR: 基于真实世界证据构建癌症治疗失败预测模型，使用EMR数据，通过特征工程从临床记录、诊断和药物中提取特征，采用提升随机森林模型达到80%准确率和75%F1分数。


<details>
  <summary>Details</summary>
Motivation: 癌症患者化疗治疗失败或提前终止会造成严重的身体、经济和情感负担，需要预测治疗失败以改善治疗决策和患者管理。

Method: 从肿瘤EMR系统中提取患者真实世界证据，构建特征工程管道从临床记录、诊断和药物中提取特征，采用三轴框架（性能、复杂性和可解释性）进行模型选择。

Result: 提升随机森林模型在五种主要癌症类型上达到80%基线准确率和75%F1分数，模型复杂度较低，对肿瘤医生更具可解释性和实用性。

Conclusion: 基于EMR数据的治疗失败预测模型能够有效识别高风险患者，提升随机森林在性能、复杂性和可解释性之间取得良好平衡，有助于临床决策支持。

Abstract: Cancer patients may undergo lengthy and painful chemotherapy treatments, comprising several successive regimens or plans. Treatment inefficacy and other adverse events can lead to discontinuation (or failure) of these plans, or prematurely changing them, which results in a significant amount of physical, financial, and emotional toxicity to the patients and their families. In this work, we build treatment failure models based on the Real World Evidence (RWE) gathered from patients' profiles available in our oncology EMR/EHR system. We also describe our feature engineering pipeline, experimental methods, and valuable insights obtained about treatment failures from trained models. We report our findings on five primary cancer types with the most frequent treatment failures (or discontinuations) to build unique and novel feature vectors from the clinical notes, diagnoses, and medications that are available in our oncology EMR. After following a novel three axes - performance, complexity and explainability - design exploration framework, boosted random forests are selected because they provide a baseline accuracy of 80% and an F1 score of 75%, with reduced model complexity, thus making them more interpretable to and usable by oncologists.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [98] [An LLVM-Based Optimization Pipeline for SPDZ](https://arxiv.org/abs/2512.11112)
*Tianye Dai,Hammurabi Mendes,Heuichan Lim*

Main category: cs.CR

TL;DR: 基于LLVM的SPDZ协议优化框架，通过自动批处理、非阻塞调度和GPU加速，显著提升安全多方计算的性能和可扩展性


<details>
  <summary>Details</summary>
Motivation: 当前安全多方计算框架存在性能瓶颈：需要特定编译栈、程序员需显式表达并行性、通信开销高，限制了实际应用

Method: 设计LLVM优化流水线：前端接受带隐私标注的C子集，后端进行数据流和控制流分析，实现非阻塞运行时调度，支持GPU内核映射

Result: CPU后端在中等和重度代数工作负载下实现最高5.56倍加速，GPU后端随输入规模扩展性更好，在线阶段性能显著提升

Conclusion: 结合LLVM和协议感知调度是提取并行性而不牺牲可用性的有效架构方向，为安全多方计算提供了实用优化方案

Abstract: Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.

</details>


### [99] [Visualisation for the CIS benchmark scanning results](https://arxiv.org/abs/2512.11316)
*Zhenshuo Zhao,Maria Spichkova,Duttkumari Champavat,Juilee N. Kulkarni,Sahil Singla,Muhammad A. Zulkefli,Pradhuman Khandelwal*

Main category: cs.CR

TL;DR: GraphSecure是一个用于分析和可视化AWS安全扫描结果的Web应用，支持CIS基准验证并提供统计图表和账户状态警告


<details>
  <summary>Details</summary>
Motivation: 当前云安全扫描结果分析工具缺乏直观的可视化和自动化验证能力，用户需要更高效的方式来评估AWS账户安全状况并符合CIS基准要求

Method: 开发Web应用程序，集成AWS扫描功能，实现CIS基准自动验证，构建统计图表可视化系统，并设计账户状态警告机制

Result: 成功创建了GraphSecure系统，能够自动扫描AWS账户、验证CIS合规性、生成可视化统计图表，并提供实时的账户安全状态警告

Conclusion: GraphSecure为AWS安全扫描结果提供了有效的分析和可视化解决方案，帮助用户更好地理解和改善云安全状况，符合行业最佳实践

Abstract: In this paper, we introduce GraphSecure, a web application that provides advanced analysis and visualisation of security scanning results. GraphSecure enables users to initiate scans for their AWS account, validate them against specific Center for Internet Security (CIS) Benchmarks and return results, showcase those returned results in the form of statistical charts and warn the users about their account status.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [100] [RMSup: Physics-Informed Radio Map Super-Resolution for Compute-Enhanced Integrated Sensing and Communications](https://arxiv.org/abs/2512.10965)
*Qiming Zhang,Xiucheng Wang,Nan Cheng,Zhisheng Yin,Xiang Li*

Main category: eess.SP

TL;DR: RMSup：基于物理信息的超分辨率框架，通过稀疏采样和不完美环境先验构建高保真无线电地图，同时恢复环境轮廓


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建方法存在局限性：基于物理的求解器耗时且需要精确场景模型，而学习方法在稀疏测量和不完整先验下性能下降，会平滑掉关键的传播不连续性

Method: 提出RMSup框架，从测量中提取亥姆霍兹方程信息的边界和奇点提示，与基站侧信息和粗糙场景描述符融合作为条件输入，采用边界感知的双头网络联合重建高保真无线电地图和环境轮廓

Result: 实验结果表明RMSup在无线电地图构建和ISAC相关环境感知方面均达到最先进的性能

Conclusion: RMSup能够通过均匀稀疏采样和不完美环境先验有效构建高保真无线电地图，为集成感知与通信提供了统一解决方案

Abstract: Radio maps (RMs) provide a spatially continuous description of wireless propagation, enabling cross-layer optimization and unifying communication and sensing for integrated sensing and communications (ISAC). However, constructing high-fidelity RMs at operational scales is difficult, since physics-based solvers are time-consuming and require precise scene models, while learning methods degrade under incomplete priors and sparse measurements, often smoothing away critical discontinuities. We present RMSup, a physics-informed super-resolution framework that functions with uniform sparse sampling and imperfect environment priors. RMSup extracts Helmholtz equation-informed boundary and singularity prompts from the measurements, fuses them with base-station side information and coarse scene descriptors as conditional inputs, and employs a boundary-aware dual-head network to reconstruct a high-fidelity RM and recover environmental contours jointly. Experimental results show the proposed RMsup achieves state-of-the-art performance both in RM construction and ISAC-related environment sensing.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [101] [Safe Bayesian optimization across noise models via scenario programming](https://arxiv.org/abs/2512.11580)
*Abdullah Tokmak,Thomas B. Schön,Dominik Baumann*

Main category: math.OC

TL;DR: 提出一种适用于多种噪声模型的安全贝叶斯优化方法，通过场景方法提供噪声的高概率边界，并证明算法的安全性和最优性。


<details>
  <summary>Details</summary>
Motivation: 大多数安全贝叶斯优化算法假设同方差次高斯测量噪声，但这一假设在许多实际应用中不成立。需要开发能够处理异方差重尾分布等多种噪声模型的安全BO方法。

Method: 使用场景方法为测量噪声提供高概率边界，将这些边界集成到高概率置信区间中，提出适用于多种噪声模型的安全贝叶斯优化算法。

Result: 在合成示例和Franka Emika机械臂控制器调优仿真中部署算法，证明了算法的有效性。

Conclusion: 提出了一种简单而严谨的方法，扩展了安全贝叶斯优化到多种噪声模型，包括同方差次高斯和异方差重尾分布，为安全关键系统的控制策略调优提供了更通用的工具。

Abstract: Safe Bayesian optimization (BO) with Gaussian processes is an effective tool for tuning control policies in safety-critical real-world systems, specifically due to its sample efficiency and safety guarantees. However, most safe BO algorithms assume homoscedastic sub-Gaussian measurement noise, an assumption that does not hold in many relevant applications. In this article, we propose a straightforward yet rigorous approach for safe BO across noise models, including homoscedastic sub-Gaussian and heteroscedastic heavy-tailed distributions. We provide a high-probability bound on the measurement noise via the scenario approach, integrate these bounds into high probability confidence intervals, and prove safety and optimality for our proposed safe BO algorithm. We deploy our algorithm in synthetic examples and in tuning a controller for the Franka Emika manipulator in simulation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [102] [Stable spectral neural operator for learning stiff PDE systems from limited data](https://arxiv.org/abs/2512.11686)
*Rui Zhang,Han Wan,Yang Liu,Hao Sun*

Main category: physics.comp-ph

TL;DR: 提出SSNO框架，通过谱神经网络架构学习刚性PDE系统的时空动力学，仅需少量数据即可实现高精度预测


<details>
  <summary>Details</summary>
Motivation: 当控制方程未知且观测数据稀疏时，准确建模时空动力学面临挑战。系统刚性（多时间尺度耦合）进一步加剧了这一问题，阻碍长期预测。现有方法要么需要大量数据，要么依赖于已知方程和精细时间步长。

Method: 提出稳定谱神经算子（SSNO）框架，在架构中嵌入谱启发结构，自动学习频域中的局部和全局空间相互作用，并使用鲁棒的积分因子时间步进方案处理系统刚性。

Result: 在2D和3D基准测试（笛卡尔和球面几何）中，SSNO的预测误差比领先模型低1-2个数量级。仅需2-5个训练轨迹即可实现鲁棒泛化，表现出卓越的数据效率。

Conclusion: SSNO为从有限数据中学习刚性时空动力学提供了一种鲁棒且可泛化的方法，无需显式的PDE项先验知识。

Abstract: Accurate modeling of spatiotemporal dynamics is crucial to understanding complex phenomena across science and engineering. However, this task faces a fundamental challenge when the governing equations are unknown and observational data are sparse. System stiffness, the coupling of multiple time-scales, further exacerbates this problem and hinders long-term prediction. Existing methods fall short: purely data-driven methods demand massive datasets, whereas physics-aware approaches are constrained by their reliance on known equations and fine-grained time steps. To overcome these limitations, we introduce an equation-free learning framework, namely, the Stable Spectral Neural Operator (SSNO), for modeling stiff partial differential equation (PDE) systems based on limited data. Instead of encoding specific equation terms, SSNO embeds spectrally inspired structures in its architecture, yielding strong inductive biases for learning the underlying physics. It automatically learns local and global spatial interactions in the frequency domain, while handling system stiffness with a robust integrating factor time-stepping scheme. Demonstrated across multiple 2D and 3D benchmarks in Cartesian and spherical geometries, SSNO achieves prediction errors one to two orders of magnitude lower than leading models. Crucially, it shows remarkable data efficiency, requiring only very few (2--5) training trajectories for robust generalization to out-of-distribution conditions. This work offers a robust and generalizable approach to learning stiff spatiotemporal dynamics from limited data without explicit \textit{a priori} knowledge of PDE terms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [103] [Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control](https://arxiv.org/abs/2512.11247)
*Iftekharul Islam,Weizi Li*

Main category: cs.MA

TL;DR: 提出分层框架结合多目标强化学习与战略路由，通过冲突威胁向量和队列均衡惩罚，在混合交通控制中同时优化效率、公平性和安全性


<details>
  <summary>Details</summary>
Motivation: 现有混合交通控制方法虽然能优化效率和安全性，但缺乏公平性机制，导致低需求方向车辆长期等待。需要平衡效率、公平和安全三个目标，实现公平的混合自动驾驶部署。

Method: 分层框架：1) 多目标强化学习用于局部交叉口控制，引入冲突威胁向量提供显式风险信号进行主动冲突避免，以及队列均衡惩罚确保各交通流公平服务；2) 战略路由用于网络级协调，根据自动驾驶车辆渗透率进行优化。

Result: 在真实世界网络中测试不同自动驾驶车辆渗透率：平均等待时间最多减少53%，最大饥饿等待最多减少86%，冲突率最多减少86%，同时保持燃油效率。战略路由效果随自动驾驶车辆渗透率增加而增强。

Conclusion: 精心设计的奖励函数结合多目标优化与战略自动驾驶车辆路由，能在混合自动驾驶部署中显著提升公平性和安全性等关键指标，实现效率、公平和安全的三重平衡。

Abstract: Effective mixed traffic control requires balancing efficiency, fairness, and safety. Existing approaches excel at optimizing efficiency and enforcing safety constraints but lack mechanisms to ensure equitable service, resulting in systematic starvation of vehicles on low-demand approaches. We propose a hierarchical framework combining multi-objective reinforcement learning for local intersection control with strategic routing for network-level coordination. Our approach introduces a Conflict Threat Vector that provides agents with explicit risk signals for proactive conflict avoidance, and a queue parity penalty that ensures equitable service across all traffic streams. Extensive experiments on a real-world network across different robot vehicle (RV) penetration rates demonstrate substantial improvements: up to 53% reductions in average wait time, up to 86% reductions in maximum starvation, and up to 86\% reduction in conflict rate compared to baselines, while maintaining fuel efficiency. Our analysis reveals that strategic routing effectiveness scales with RV penetration, becoming increasingly valuable at higher autonomy levels. The results demonstrate that multi-objective optimization through well-curated reward functions paired with strategic RV routing yields significant benefits in fairness and safety metrics critical for equitable mixed-autonomy deployment.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [104] [Developmental Symmetry-Loss: A Free-Energy Perspective on Brain-Inspired Invariance Learning](https://arxiv.org/abs/2512.10984)
*Arif Dönmez*

Main category: q-bio.NC

TL;DR: Symmetry-Loss是一种受大脑启发的算法框架，通过环境对称性的可微分约束来强制不变性和等变性，将学习建模为有效对称群的迭代优化过程。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是建立大脑发育学习与人工系统表示学习之间的桥梁。大脑皮层表示在发育过程中会与世界的结构对齐，而当前的人工系统缺乏这种基于对称性的自组织机制。研究者希望将预测编码和群论视角结合起来，为表示学习提供一个统一的计算框架。

Method: 提出Symmetry-Loss框架，通过从环境对称性推导出的可微分约束来强制不变性和等变性。该方法将学习建模为有效对称群的迭代优化过程，通过最小化结构惊奇（即偏离对称一致性的程度）来实现。该框架操作化了一个类似自由能的目标函数，用于表示学习。

Result: 该框架展示了如何从基于对称性的自组织中涌现出高效、稳定和组合式的表示。它提供了一种通用的计算机制，将大脑的发育学习与人工系统的原则性表示学习联系起来。

Conclusion: Symmetry-Loss成功地将预测编码和群论视角统一起来，为表示学习提供了一个受大脑启发的算法框架。该工作表明，通过对称性约束的自组织可以产生高效且稳定的表示，为理解大脑发育学习和设计更好的人工学习系统提供了新的理论框架。

Abstract: We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.

</details>


### [105] [Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness](https://arxiv.org/abs/2512.10985)
*Igor Pivovarov,Sergey Shumsky*

Main category: q-bio.NC

TL;DR: 提出一个基于大脑"what"和"where"通路的生物启发数学模型，通过分离自我与环境来构建自我模型，实现更好的预测和行为选择，并在Atari游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大脑中存在"what"（是什么）和"where"（在哪里）信息处理通路的概念提出近30年，但缺乏清晰的数学模型来解释这些通路如何协同工作。研究者希望建立一个能够分离自我与环境、构建自我模型的数学框架。

Method: 提出一个受生物学启发的数学模型，将新皮层柱分为"what"柱和"where"柱，由基底神经节控制进行预测和行动选择。基于此模型构建强化学习智能体，在虚拟环境中学习目的性行为。

Result: 在Atari游戏Pong和Breakout中成功训练智能体学会玩游戏。实验表明，分离自我与环境的能力为智能体带来了优势。

Conclusion: 提出自我意识原则1：将自我与世界分离的能力是自我意识的必要但不充分条件。这种模型可能在生物进化过程中出现，因为它提供了适应性优势。

Abstract: The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [106] [amc: The Automated Mission Classifier for Telescope Bibliographies](https://arxiv.org/abs/2512.11202)
*John F. Wu,Joshua E. G. Peek,Sophie J. Miller,Jenny Novacescu,Achu J. Usha,Christopher A. Wilkinson*

Main category: astro-ph.IM

TL;DR: 开发了基于大语言模型的自动望远镜文献分类工具AMC，用于高效识别和分类望远镜相关论文，在TRACS挑战中取得良好性能


<details>
  <summary>Details</summary>
Motivation: 随着天文学出版物快速增长，手动标注望远镜文献已无法满足需求，需要自动化工具来测量望远镜设施的科学影响力

Method: 使用大语言模型处理大量论文文本，开发自动任务分类器AMC，识别和分类望远镜引用，包括改进版本用于TRACS挑战

Result: AMC在TRACS Kaggle挑战的测试集上获得0.84的宏观F1分数，成功识别NASA任务的科学成果论文，并能检测历史数据集中的潜在标签错误

Conclusion: 基于大语言模型的应用为图书馆科学提供了强大且可扩展的辅助工具，能够有效处理天文文献分类任务

Abstract: Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [107] [Shortest Paths on Convex Polyhedral Surfaces](https://arxiv.org/abs/2512.11299)
*Haitao Wang*

Main category: cs.CG

TL;DR: 本文改进了凸多面体上两点最短路径查询问题的数据结构和算法，将预处理时间和空间从O(n^{8+ε})降至O(n^{6+ε})，同时保持O(log n)查询时间，并在特殊情况下进一步优化。


<details>
  <summary>Details</summary>
Motivation: 凸多面体上的最短路径查询是计算几何中的重要问题，现有最佳结果需要O(n^{8+ε})的预处理时间和空间才能实现O(log n)查询时间，这在实际应用中代价过高。本文旨在显著降低预处理成本，同时保持高效的查询性能。

Method: 提出新的数据结构，通过改进算法设计和分析，减少预处理复杂度。对于特殊查询情况（一个查询点位于多面体边上），采用更优化的预处理策略。同时开发新算法计算最短路径边序列的精确集合。

Result: 1. 通用查询：预处理时间和空间从O(n^{8+ε})降至O(n^{6+ε})，查询时间保持O(log n)
2. 特殊查询：预处理时间和空间从O(n^{6+ε})降至O(n^{5+ε})，查询时间保持O(log n)
3. 最短路径边序列计算：时间复杂度从O(n^6 log n log* n)降至O(n^{5+ε})

Conclusion: 本文在凸多面体最短路径查询问题上取得了显著进展，大幅降低了预处理成本，同时保持了高效的查询性能。这些改进使算法更具实用性，并为相关几何计算问题提供了新的技术思路。

Abstract: Let $\mathcal{P}$ be the surface of a convex polyhedron with $n$ vertices. We consider the two-point shortest path query problem for $\mathcal{P}$: Constructing a data structure so that given any two query points $s$ and $t$ on $\mathcal{P}$, a shortest path from $s$ to $t$ on $\mathcal{P}$ can be computed efficiently. To achieve $O(\log n)$ query time (for computing the shortest path length), the previously best result uses $O(n^{8+ε})$ preprocessing time and space [Aggarwal, Aronov, O'Rourke, and Schevon, SICOMP 1997], where $ε$ is an arbitrarily small positive constant. In this paper, we present a new data structure of $O(n^{6+ε})$ preprocessing time and space, with $O(\log n)$ query time. For a special case where one query point is required to lie on one of the edges of $\mathcal{P}$, the previously best work uses $O(n^{6+ε})$ preprocessing time and space to achieve $O(\log n)$ query time. We improve the preprocessing time and space to $O(n^{5+ε})$, with $O(\log n)$ query time. Furthermore, we present a new algorithm to compute the exact set of shortest path edge sequences of $\mathcal{P}$, which are known to be $Θ(n^4)$ in number and have a total complexity of $Θ(n^5)$ in the worst case. The previously best algorithm for the problem takes roughly $O(n^6\log n\log^*n)$ time, while our new algorithm runs in $O(n^{5+ε})$ time.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification](https://arxiv.org/abs/2512.11015)
*Anoop Krishnan*

Main category: cs.CV

TL;DR: 提出两种文本引导方法（ITM指导和图像文本融合）来增强面部图像性别分类算法的公平性，无需人口统计标签，通过语义信息减少偏见


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中的公平性问题，特别是在面部图像性别分类算法中存在的偏见问题。当前方法存在人口统计偏差，需要更公平、可解释的解决方案

Method: 1. 图像文本匹配（ITM）指导：训练模型识别图像和文本之间的细粒度对齐，获得增强的多模态表示；2. 图像文本融合：将两种模态结合成综合表示以改善公平性

Result: 在基准数据集上的广泛实验表明，这些方法有效减轻了偏见，提高了跨性别和种族群体的准确性，相比现有方法表现更好

Conclusion: 通过文本引导的语义信息整合，为计算机视觉系统提供了可解释且直观的训练范式，为解决面部分析算法中的人口统计偏见提供了有价值的见解

Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.

</details>


### [109] [CADKnitter: Compositional CAD Generation from Text and Geometry Guidance](https://arxiv.org/abs/2512.11199)
*Tri Le,Khang Nguyen,Baoru Huang,Tung D. Ta,Anh Nguyen*

Main category: cs.CV

TL;DR: CADKnitter：一个基于几何引导扩散采样的组合式CAD生成框架，能够生成符合几何约束和语义约束的互补CAD零件


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模耗时且需要专业技能，现有3D生成方法主要关注单零件生成，而实际应用中需要多个零件在语义和几何约束下组装

Method: 提出CADKnitter框架，采用几何引导的扩散采样策略，能够根据给定CAD模型的几何约束和设计文本提示的语义约束生成互补CAD零件

Result: 创建了包含31万多个样本的KnitCAD数据集，实验表明该方法明显优于现有最先进基线

Conclusion: CADKnitter为组合式CAD生成提供了有效解决方案，能够处理实际应用中的多零件组装需求

Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.

</details>


### [110] [VFMF: World Modeling by Forecasting Vision Foundation Model Features](https://arxiv.org/abs/2512.11225)
*Gabrijel Boduljak,Yushi Lan,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 提出一种在视觉基础模型特征空间中进行自回归流匹配的生成式预测方法，通过紧凑的潜在空间编码实现多模态预测，比确定性回归方法更准确


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个极端：基于像素的视频生成方法计算量大且难以直接用于决策；基于VFM特征的确定性回归方法虽然高效但无法捕捉不确定性，平均了多个可能的未来状态

Method: 在VFM特征空间中进行自回归流匹配的生成式预测，关键创新是将VFM特征编码到适合扩散的紧凑潜在空间，该空间比之前使用的PCA方法能更有效地保留信息

Result: 在相同架构和计算条件下，该方法在所有模态（语义分割、深度、表面法线、RGB）上都比回归方法产生更清晰、更准确的预测，潜在空间在预测和图像生成等应用中信息保留效果更好

Conclusion: VFM特征的随机条件生成为未来世界模型提供了一个有前景且可扩展的基础，能够高效地产生多模态、可解释的输出

Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.

</details>


### [111] [Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty](https://arxiv.org/abs/2512.11373)
*Arnold Brosch,Abdelrahman Eldesokey,Michael Felsberg,Kira Maag*

Main category: cs.CV

TL;DR: 提出基于Wasserstein损失的证据分割框架，用于开放世界场景中的未知物体识别与分割，相比基于不确定性的方法有更好表现。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在语义分割中表现出色，但仅限于预定义类别集，在开放世界场景中遇到未知物体时会失败。识别和分割这些分布外（OOD）物体对于自动驾驶等安全关键应用至关重要。

Method: 提出证据分割框架，使用Wasserstein损失捕捉分布距离同时尊重概率单纯形几何结构，结合Kullback-Leibler正则化和Dice结构一致性项。

Result: 相比基于不确定性的方法，该方法在OOD分割性能上有所改进。

Conclusion: 提出的证据分割框架能有效处理开放世界场景中的未知物体分割问题，为安全关键应用提供了更好的解决方案。

Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.

</details>


### [112] [Exploring MLLM-Diffusion Information Transfer with MetaCanvas](https://arxiv.org/abs/2512.11464)
*Han Lin,Xichen Pan,Ziqi Huang,Ji Hou,Jialiang Wang,Weifeng Chen,Zecheng He,Felix Juefei-Xu,Junzhe Sun,Zhipeng Fan,Ali Thabet,Mohit Bansal,Chu Wang*

Main category: cs.CV

TL;DR: MetaCanvas是一个轻量级框架，让多模态大语言模型直接在空间和时空潜在空间中进行推理和规划，并与扩散生成器紧密接口，显著提升图像/视频生成的精确控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉理解方面表现出色，能够解析复杂布局、属性和知识密集型场景，但在视觉生成中，这些强大的核心模型通常被降级为扩散模型的全局文本编码器，其大部分推理和规划能力未被充分利用，导致理解与生成之间存在差距。

Method: 提出MetaCanvas框架，让MLLMs直接在空间和时空潜在空间中进行推理和规划，并与扩散生成器紧密接口。该框架在三种不同的扩散模型骨干上实现，并在六个任务上进行评估。

Result: MetaCanvas在六个任务（包括文本到图像生成、文本/图像到视频生成、图像/视频编辑和上下文视频生成）上持续优于全局条件化基线方法，这些任务都需要精确布局、鲁棒的属性绑定和推理密集型控制。

Conclusion: 将MLLMs视为潜在空间规划器是缩小多模态理解与生成之间差距的有前景方向，MetaCanvas框架为这一方向提供了有效实现。

Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.

</details>


### [113] [DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation](https://arxiv.org/abs/2512.11465)
*Mohamed Abdelsamad,Michael Ulrich,Bin Yang,Miao Zhang,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: DOS是一个自监督学习框架，通过仅在可观察点蒸馏语义相关性软映射来学习3D点云表示，使用Zipfian原型和Zipf-Sinkhorn算法解决语义不平衡问题，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D点云的自监督学习面临几何不规则、重建易走捷径和语义分布不平衡等挑战，需要更有效的表示学习方法。

Method: 提出DOS框架：1) 仅在可观察点蒸馏语义相关性软映射，避免掩码区域信息泄露；2) 引入Zipfian原型和Zipf-Sinkhorn算法，强制原型使用遵循幂律分布，调节目标软映射的锐度。

Result: 在nuScenes、Waymo、SemanticKITTI、ScanNet和ScanNet200等多个基准测试中，DOS在语义分割和3D目标检测任务上超越了当前最先进方法，无需额外数据或标注。

Conclusion: 可观察点软映射蒸馏为学习鲁棒的3D表示提供了一个可扩展且有效的范式，解决了自监督学习中的关键挑战。

Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.

</details>


### [114] [Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France](https://arxiv.org/abs/2512.11524)
*Ekaterina Kalinicheva,Florian Helen,Stéphane Mermoz,Florian Mouret,Milena Planells*

Main category: cs.CV

TL;DR: THREASURE-Net是一个用于树木高度回归和超分辨率的端到端深度学习框架，仅使用Sentinel-2时序数据和LiDAR高度信息，无需预训练模型或高分辨率光学影像，在2.5-10米分辨率上实现高精度森林监测。


<details>
  <summary>Details</summary>
Motivation: 精细尺度的森林监测对于理解冠层结构及其动态至关重要，这些是碳储量、生物多样性和森林健康的关键指标。深度学习能有效整合光谱、时间和空间信号来反映冠层结构。

Method: 提出THREASURE-Net端到端框架，使用Sentinel-2时序数据训练，参考LiDAR HD数据在多个空间分辨率下的高度指标，生成年度高度图。模型有三个变体，分别输出2.5米、5米和10米分辨率的树高预测。不依赖预训练模型或高分辨率光学影像，仅从LiDAR高度信息学习。

Result: 模型在2.5米、5米和10米分辨率上的平均绝对误差分别为2.62米、2.72米和2.88米。性能优于基于Sentinel数据的现有方法，与基于高分辨率影像的方法相当。

Conclusion: THREASURE-Net展示了仅使用免费卫星数据进行可扩展、经济高效的温带森林结构监测的潜力，能够生成高精度的年度冠层高度图。

Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.

</details>


### [115] [In-Context Learning for Seismic Data Processing](https://arxiv.org/abs/2512.11575)
*Fabian Fuchs,Mario Ruben Fernandez,Norman Ettrich,Janis Keuper*

Main category: cs.CV

TL;DR: 提出ContextSeisNet，一种基于上下文学习的模型，用于地震多次波压制处理，通过支持集实现用户控制和空间一致性


<details>
  <summary>Details</summary>
Motivation: 传统地震处理方法面临噪声数据和手动参数调优等挑战，现有深度学习方法存在空间不一致性和缺乏用户控制的问题

Method: 引入上下文学习模型ContextSeisNet，通过支持集（空间相关的示例对）在推理时学习任务特定处理行为，无需重新训练

Result: 在合成数据上优于U-Net基线，在野外数据上比传统Radon方法和U-Net具有更好的空间一致性，仅用10%训练数据达到相当性能

Conclusion: ContextSeisNet是一种实用的空间一致性地震多次波压制方法，具有扩展到其他地震处理任务的潜力

Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [116] [Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis](https://arxiv.org/abs/2512.10963)
*Zheqi Hu,Xuanjing Chen,Jinlin Hu*

Main category: cs.IR

TL;DR: 本文提出了一种基于多模态情感意图识别的AIGC推荐系统，通过融合视觉、听觉和文本模态来理解用户情感状态，实现更个性化的内容推荐。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC在音乐、视频、文学等领域的快速发展，传统推荐系统主要依赖用户行为数据（点击、观看、评分），忽视了用户在内容交互过程中的实时情感和意图状态，因此需要情感感知的推荐系统。

Method: 提出了基于BERT跨模态Transformer的多模态情感意图识别模型（MMEI），通过预训练的ViT、Wav2Vec2和BERT编码器处理视觉（面部表情）、听觉（语音语调）和文本（评论或话语）模态，然后使用注意力融合模块学习情感意图表示，最后通过上下文匹配层驱动个性化内容推荐。

Result: 在基准情感数据集（AIGC-INT、MELD、CMU-MOSEI）和AIGC交互数据集上的实验显示，MMEI模型相比最佳融合Transformer基线，F1分数提高了4.3%，交叉熵损失降低了12.3%。用户级在线评估表明，情感驱动推荐使参与时间增加了15.2%，满意度评分提高了11.8%。

Conclusion: 这项工作展示了跨模态情感智能在下一代AIGC生态系统中的潜力，能够实现自适应、共情和情境感知的推荐体验，有效将AI生成内容与用户情感和意图状态对齐。

Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [117] [Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis](https://arxiv.org/abs/2512.11593)
*Hyungrok Do,Yuyan Wang,Mengling Liu,Myeonggyun Lee*

Main category: stat.AP

TL;DR: 提出NeuralPLSI框架，结合半参数回归可解释性与深度学习表达能力，用于评估复杂环境混合物健康效应


<details>
  <summary>Details</summary>
Motivation: 现有评估环境混合物健康效应的方法在灵活性、可解释性、可扩展性和支持多种结局类型方面存在局限，限制了实际应用

Method: 提出基于神经网络的偏线性单指数模型框架，通过可学习投影构建可解释的暴露指数，用灵活神经网络建模其与结局的关系，支持连续、二元和时间到事件结局，通过自助法进行推断

Result: 通过多种情景的模拟研究评估了NeuralPLSI，并在NHANES数据中展示了实际应用价值，证明其作为可扩展、可解释、多功能的混合物分析工具

Conclusion: NeuralPLSI是评估环境混合物健康效应的有效工具，作者发布了开源软件包促进方法采用和可重复性

Abstract: Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [118] [MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data](https://arxiv.org/abs/2512.11074)
*Christopher Driggers-Ellis,Detravious Brinkley,Ray Chen,Aashish Dhawan,Daisy Zhe Wang,Christan Grant*

Main category: cs.CL

TL;DR: 提出MultiScript30k数据集，将Multi30k扩展到阿拉伯语、西班牙语、乌克兰语和中文（简繁），支持更多语言和文字系统。


<details>
  <summary>Details</summary>
Motivation: 现有Multi30k数据集仅支持捷克语、英语、法语和德语，限制了多模态机器翻译研究在多样化语言上的发展。需要扩展数据集以涵盖更多语言家族和文字系统。

Method: 使用NLLB200-3.3B模型将Multi30k英文版翻译成阿拉伯语、西班牙语、乌克兰语和中文（简繁），创建包含30000+句子的新数据集。

Result: 相似性分析显示除繁体中文外，所有语言都达到大于0.8的余弦相似度和小于0.000251的对称KL散度。COMETKiwi评分显示与相关工作相比结果不一。

Conclusion: MultiScript30k成功扩展了Multi30k数据集，支持更多全球语言和文字系统，为多模态机器翻译研究提供了更广泛的语言资源。

Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.

</details>


### [119] [When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents](https://arxiv.org/abs/2512.11277)
*Mrinal Rawat,Arkajyoti Chakraborty,Neha Gupta,Roberto Pieraccini*

Main category: cs.CL

TL;DR: 使用强化学习训练LLM推理策略，通过任务结果直接学习，提升工具调用准确性和答案正确性，相比SFT模型获得1.5%相对提升


<details>
  <summary>Details</summary>
Motivation: 监督微调在数据分布变化时泛化能力有限，而高质量推理标注成本高、主观性强且难以扩展。现有推理模型（如o1和R1）显示推理对泛化和可靠性很重要，但缺乏有效的推理训练方法

Method: 提出基于强化学习的管道：LLM生成推理步骤指导工具调用和最终答案生成，使用组相对策略优化（GRPO），奖励设计围绕工具准确性和答案正确性，迭代优化推理和行动

Result: 方法提升了推理质量和工具调用精度：相比无显式思考的SFT模型获得1.5%相对提升，相比基础Qwen3-1.7B模型获得40%增益

Conclusion: 通过强化学习统一推理和行动学习，可以构建更强大、更可泛化的对话智能体，展示了RL在训练LLM推理策略方面的潜力

Abstract: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.

</details>


### [120] [Visualizing token importance for black-box language models](https://arxiv.org/abs/2512.11573)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出DBSA方法，用于审计黑盒大语言模型，分析每个输入token对输出的敏感度，无需模型内部信息或分布假设。


<details>
  <summary>Details</summary>
Motivation: 在医疗、法律等高风险领域部署LLM时，需要确保模型行为可靠。现有审计方法通常只关注特定方面（如偏见检测），缺乏对输入token依赖性的全面理解。由于LLM是黑盒API且具有随机性，计算梯度不可行，因此需要轻量级、模型无关的敏感度分析工具。

Method: 提出Distribution-Based Sensitivity Analysis (DBSA)，一种轻量级、模型无关的方法，无需对LLM做分布假设。通过分析输入token变化时输出的分布变化，评估每个token对输出的敏感度，支持快速可视化探索。

Result: DBSA能够帮助用户检查LLM输入，发现现有可解释性方法可能忽略的敏感度问题。通过示例展示了该方法在实际应用中的有效性。

Conclusion: DBSA为实践者提供了一个实用的插件式工具，能够快速分析黑盒LLM对特定输入token的依赖关系，有助于在高风险应用中确保模型行为的可靠性。

Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

</details>


### [121] [Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols](https://arxiv.org/abs/2512.11614)
*Björn Deiseroth,Max Henning Höth,Kristian Kersting,Letitia Parcalabescu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于Merlin-Arthur协议的RAG训练框架，将检索增强生成系统视为交互式证明系统，通过对抗性训练提升模型对检索证据的验证能力，减少幻觉并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统将检索视为弱启发式而非可验证证据，导致LLM在缺乏支持时仍回答、在上下文不完整或误导时产生幻觉、依赖虚假证据。需要更可靠的RAG系统将检索文档作为可验证证据而非建议。

Method: 采用Merlin-Arthur协议框架：Arthur（生成器LLM）处理来源未知的问题，Merlin提供有用证据，Morgana注入对抗性误导上下文。两者使用线性时间XAI方法识别和修改对Arthur最有影响的证据。Arthur学习：1）在上下文支持时回答；2）证据不足时拒绝；3）依赖真正支撑答案的具体上下文片段。同时引入严格评估框架分离解释保真度和基线预测误差。

Result: 在三个RAG数据集和两种不同规模模型家族上，M/A训练的LLM在groundedness、完整性、正确性和拒绝行为方面均有改善，幻觉减少，且无需手动标注不可回答问题。检索器通过自动生成的M/A硬正负样本也提高了召回率和MRR。

Conclusion: 自主交互式证明风格监督为可靠RAG系统提供了原则性和实用路径，使检索文档成为可验证证据而非建议。该方法通过对抗性训练和可解释性技术实现了更好的证据验证能力。

Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [122] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: CORL框架使用强化学习端到端微调MILP方案，将分支定界求解的MILP转化为可微随机策略，以最大化实际操作性能而非建模准确性。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设能获取最优决策真值，并使用MILP梯度的替代方法。

Method: 提出CORL框架，将分支定界算法求解的MILP转化为可微随机策略，使其与强化学习兼容，在真实数据上端到端微调MILP方案。

Result: 在简单的组合序贯决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过强化学习直接优化MILP的实际操作性能，避免了传统建模准确性与实际性能之间的差距问题。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [123] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器偏见，发现对少数群体文本检测性能存在一致差异


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但缺乏对更广泛社会语言因素的系统性偏见评估，特别是对英语学习者等群体的偏见问题

Method: 提出BAID评估框架，包含7大类（人口统计、年龄、教育等级、方言、正式程度、政治倾向、主题）超过20万样本，并为每个样本生成保留原始内容但反映特定群体写作风格的合成版本

Result: 评估4个开源SOTA AI文本检测器，发现检测性能存在一致差异，特别是对少数群体文本的召回率较低

Conclusion: BAID提供了可扩展、透明的AI检测器审计方法，强调在工具部署前需要进行偏见感知评估

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [124] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI代理系统，通过迭代检索增强生成(RAG)整合多种生物医学工具，在CURE-Bench挑战赛中获得卓越奖


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策是高风险领域，需要AI系统进行多步骤推理并基于可靠的生物医学知识。医疗应用有严格的安全约束，要求推理过程和工具调用都准确可靠

Method: 使用微调的Llama-3.1-8B模型，动态生成和执行函数调用到统一的生物医学工具套件(ToolUniverse)，整合FDA Drug API、OpenTargets和Monarch资源

Result: 在CURE-Bench NeurIPS 2025挑战赛中表现出色，获得卓越奖。分析显示工具检索质量显著影响整体性能，改进工具检索策略能带来性能提升

Conclusion: 医疗AI系统需要将token级推理和工具使用行为作为显式监督信号进行评估，工具检索质量对治疗推理系统的性能至关重要

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [125] [Maritime object classification with SAR imagery using quantum kernel methods](https://arxiv.org/abs/2512.11367)
*John Tanner,Nicholas Davies,Pascal Elahi,Casey R. Myers,Du Huynh,Wei Liu,Mark Reynolds,Jingbo Wang*

Main category: quant-ph

TL;DR: 量子核方法首次应用于SAR图像海上目标分类，在最佳情况下性能与经典核方法相当或更好，但对复数SAR数据未显示明显优势。


<details>
  <summary>Details</summary>
Motivation: 非法、未报告和未受管制(IUU)捕鱼每年造成100-250亿美元经济损失，破坏海洋可持续性和治理。合成孔径雷达(SAR)能在全天候条件下提供可靠的海上监视，但在SAR图像中分类小型海上目标仍然具有挑战性。

Method: 研究量子机器学习用于SAR图像分类，重点应用量子核方法(QKMs)处理从SARFish数据集中提取的实数和复数SAR芯片。处理两个二元分类问题：1)区分船只与非船只；2)区分渔船与其他类型船只。将QKMs应用于实数和复数SAR芯片，与应用于实数SAR芯片的经典拉普拉斯核、RBF核和线性核进行比较。

Result: 使用量子核的无噪声数值模拟显示，在最佳情况下，QKMs能够获得与经典核相当或更好的性能，但对于复数SAR数据未表现出明显优势。

Conclusion: 这是量子核方法首次应用于SAR图像海上分类，为量子增强学习在海事监视中的潜力和当前局限性提供了见解。

Abstract: Illegal, unreported, and unregulated (IUU) fishing causes global economic losses of \$10-25 billion annually and undermines marine sustainability and governance. Synthetic Aperture Radar (SAR) provides reliable maritime surveillance under all weather and lighting conditions, but classifying small maritime objects in SAR imagery remains challenging. We investigate quantum machine learning for this task, focusing on Quantum Kernel Methods (QKMs) applied to real and complex SAR chips extracted from the SARFish dataset. We tackle two binary classification problems, the first for distinguishing vessels from non-vessels, and the second for distinguishing fishing vessels from other types of vessels. We compare QKMs applied to real and complex SAR chips against classical Laplacian, RBF, and linear kernels applied to real SAR chips. Using noiseless numerical simulations of the quantum kernels, we find that QKMs are capable of obtaining equal or better performance than the classical kernel on these tasks in the best case, but do not demonstrate a clear advantage for the complex SAR data. This work presents the first application of QKMs to maritime classification in SAR imagery and offers insight into the potential and current limitations of quantum-enhanced learning for maritime surveillance.

</details>


### [126] [FRQI Pairs method for image classification using Quantum Recurrent Neural Network](https://arxiv.org/abs/2512.11499)
*Rafał Potempa,Michał Kordasz,Sundas Naqeeb Khan,Krzysztof Werner,Kamil Wereszczyński,Krzysztof Simiński,Krzysztof A. Cyran*

Main category: quant-ph

TL;DR: FRQI Pairs方法是一种使用量子循环神经网络和灵活量子图像表示进行图像分类的新方法，有望降低量子算法复杂度并推动量子机器学习发展。


<details>
  <summary>Details</summary>
Motivation: 将量子编码数据应用于图像分类任务，探索量子计算方法在机器学习中的潜力，通过量子原理与神经网络架构的整合来降低量子算法的复杂度。

Method: 使用FRQI Pairs方法，结合量子循环神经网络和灵活量子图像表示技术，将图像编码为量子态进行处理和分类。

Result: 与当代技术相比，FRQI Pairs方法显示出显著优势，表明量子计算方法在图像分类任务中具有巨大潜力，能够有效降低算法复杂度。

Conclusion: 量子计算原理与神经网络架构的整合为量子机器学习的发展提供了有前景的方向，FRQI Pairs方法展示了量子方法在图像分类中的实际应用价值。

Abstract: This study aims to introduce the FRQI Pairs method to a wider audience, a novel approach to image classification using Quantum Recurrent Neural Networks (QRNN) with Flexible Representation for Quantum Images (FRQI).
  The study highlights an innovative approach to use quantum encoded data for an image classification task, suggesting that such quantum-based approaches could significantly reduce the complexity of quantum algorithms. Comparison of the FRQI Pairs method with contemporary techniques underscores the promise of integrating quantum computing principles with neural network architectures for the development of quantum machine learning.

</details>


### [127] [Learning Minimal Representations of Fermionic Ground States](https://arxiv.org/abs/2512.11767)
*Felix Frohnert,Emiel Koridon,Stefano Polla*

Main category: quant-ph

TL;DR: 提出无监督机器学习框架，通过自编码器发现量子多体基态的最优压缩表示，在费米-哈伯德模型中识别出与系统内在自由度匹配的最小潜在空间维度。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统的基态表示通常维度很高，需要寻找最优压缩表示来降低计算复杂度，同时避免N-可表示性问题（即确保优化过程保持在物理有效的量子态空间内）。

Method: 使用自编码器神经网络架构处理L-位点费米-哈伯德模型数据，识别最小潜在空间；利用训练好的解码器作为可微分变分拟设，直接在潜在空间中最小化能量。

Result: 发现重建质量在L-1个潜在维度处存在尖锐阈值，这与系统的内在自由度相匹配；该方法成功规避了N-可表示性问题，因为学习到的流形隐式地将优化限制在物理有效的量子态上。

Conclusion: 该无监督机器学习框架能够自动发现量子多体基态的最优压缩表示，为量子态的高效表示和优化提供了新方法，特别适用于处理高维量子系统。

Abstract: We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system's intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [128] [Integrated Prediction and Multi-period Portfolio Optimization](https://arxiv.org/abs/2512.11273)
*Qi Deng,Yuxuan Linghu,Zhiyuan Liu*

Main category: cs.CE

TL;DR: IPMO模型通过端到端学习将多期收益预测与投资组合优化整合，解决了传统两阶段方法中预测与决策不匹配的问题，在考虑交易成本的情况下显著提升了风险调整后收益。


<details>
  <summary>Details</summary>
Motivation: 传统多期投资组合优化采用两阶段方法：先用机器学习预测收益，再用预测值进行优化。这种方法存在预测与决策目标不匹配的问题，且忽略了交易成本的影响，导致次优的投资决策。

Method: 提出IPMO（集成预测与多期投资组合优化）模型，将预测器和优化层整合为端到端学习框架。预测器生成多期收益预测，参数化可微凸优化层，通过投资组合表现驱动学习。为提升可扩展性，引入镜像下降固定点（MDFP）微分方案，避免分解KKT系统，获得稳定的隐式梯度和接近规模不敏感的运行时间。

Result: 在真实市场数据和两种代表性时间序列预测模型上的实验表明，IPMO方法在扣除交易成本后的风险调整表现上持续优于两阶段基准方法，并实现了更一致的投资配置路径。

Conclusion: 在多期设置中将机器学习预测与优化集成能够改善财务结果，同时保持计算可行性。端到端学习方法解决了预测与决策目标的对齐问题，为实际投资组合管理提供了更有效的框架。

Abstract: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.

</details>


### [129] [Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models](https://arxiv.org/abs/2512.11412)
*Kwun Sy Lee,Jiawei Chen,Fuk Sheng Ford Chung,Tianyu Zhao,Zhenyuan Chen,Debby D. Wang*

Main category: cs.CE

TL;DR: 提出一种多任务学习框架，通过稀疏注意力机制同时提升分子毒性预测的准确性和可解释性，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的分子毒性预测模型多为黑盒模型，虽然预测性能好但缺乏可解释性，而药物发现中的安全决策需要可验证的结构洞察。因此需要开发既能保持高精度又能提供化学直觉解释的方法。

Method: 提出新颖的多任务学习框架，整合共享的化学语言模型和任务特定的注意力模块，通过对注意力模块施加L1稀疏惩罚，使模型聚焦于每个毒性终点的最小关键分子片段集合。

Result: 在ClinTox、SIDER和Tox21基准数据集上评估，该方法一致优于单任务和标准多任务学习基线模型。稀疏注意力权重提供了化学直觉的可视化，揭示了影响预测的具体分子片段。

Conclusion: 该框架成功平衡了预测性能与可解释性，通过稀疏注意力机制提供了化学相关的解释，有助于理解模型的决策过程，在药物发现的安全评估中具有重要应用价值。

Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [130] [Universal entrywise eigenvector fluctuations in delocalized spiked matrix models and asymptotics of rounded spectral algorithms](https://arxiv.org/abs/2512.11785)
*Shujing Chen,Dmitriy Kunisky*

Main category: math.PR

TL;DR: 该论文研究了尖峰矩阵模型在超临界区域中顶部特征向量的分布，证明了当真实向量充分去局域化时，特征向量分量的分布具有普遍性，并应用于稠密随机块模型和同步问题的谱算法分析。


<details>
  <summary>Details</summary>
Motivation: 研究尖峰矩阵模型中顶部特征向量的分布特性，特别是其分量的普遍性性质。现有工作观察到当真实向量充分局域化时分布不具有普遍性，本文旨在探索在去局域化条件下的普遍性结果。

Method: 采用尖峰矩阵模型 H = θvv* + W，其中W为广义Wigner矩阵。在超临界区域（H具有与||W||相当大小的异常特征值）下，分析顶部特征向量分量的分布。通过比较高斯正交/酉系综情况，证明当W的方差接近常数时，特征向量分量函数的平均值表现为围绕v的倍数具有高斯波动。

Result: 证明当真实向量v充分去局域化时，顶部特征向量分量的分布（不仅仅是内积）在具有独立条目的广义Wigner矩阵类中具有普遍性，仅依赖于W条目分布的前两阶矩。当W的方差接近常数时，特征向量分量函数的平均值表现为高斯波动行为。

Conclusion: 该研究为尖峰矩阵模型中特征向量分量的分布提供了新的普遍性结果，特别在去局域化条件下。这些结果被应用于稠密随机块模型和循环群/圆群上的同步问题，首次精确刻画了谱算法结合舍入程序的渐近误差率。

Abstract: We consider the distribution of the top eigenvector $\widehat{v}$ of a spiked matrix model of the form $H = θvv^* + W$, in the supercritical regime where $H$ has an outlier eigenvalue of comparable magnitude to $\|W\|$. We show that, if $v$ is sufficiently delocalized, then the distribution of the individual entries of $\widehat{v}$ (not, we emphasize, merely the inner product $\langle \widehat{v}, v\rangle$) is universal over a large class of generalized Wigner matrices $W$ having independent entries, depending only on the first two moments of the distributions of the entries of $W$. This complements the observation of Capitaine and Donati-Martin (2018) that these distributions are not universal when $v$ is instead sufficiently localized. Further, for $W$ having entrywise variances close to constant and thus resembling a Wigner matrix, we show by comparing to the case of $W$ drawn from the Gaussian orthogonal or unitary ensembles that averages of entrywise functions of $\widehat{v}$ behave as they would if $\widehat{v}$ had Gaussian fluctuations around a suitable multiple of $v$. We apply these results to study spectral algorithms followed by rounding procedures in dense stochastic block models and synchronization problems over the cyclic and circle groups, obtaining the first precise asymptotic characterizations of the error rates of such algorithms.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [STARK denoises spatial transcriptomics images via adaptive regularization](https://arxiv.org/abs/2512.10994)
*Sharvaj Kubal,Naomi Graham,Matthieu Heitz,Andrew Warren,Michael P. Friedlander,Yaniv Plan,Geoffrey Schiebinger*

Main category: stat.ML

TL;DR: STARK是一种用于空间转录组学图像去噪的新方法，通过自适应图拉普拉斯正则化和核岭回归，特别适用于超低测序深度下的细胞身份识别和基因表达插值。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学在超低测序深度下存在严重噪声问题，传统方法难以准确识别细胞身份和进行基因表达插值。需要开发能够在低测序深度下稳健去噪并提高数据质量的方法。

Method: STARK方法结合核岭回归和自适应图拉普拉斯正则化，采用交替最小化方案：1) 固定图结构时用核岭回归更新图像；2) 基于新图像更新图结构。通过改进的表示定理将无限维问题降为有限维，从纯空间图开始逐步自适应优化。

Result: 理论证明该方法收敛到非凸目标的稳定点，统计上以O(R^{-1/2})速率收敛到真实值（R为reads数）。在实际空间转录组数据实验中，STARK在标签转移准确度方面相比竞争方法有持续改进。

Conclusion: STARK为空间转录组学图像去噪提供了一种有效方法，特别适用于低测序深度场景，通过自适应图正则化提高了去噪性能，为细胞身份识别和基因表达分析提供了更可靠的工具。

Abstract: We present an approach to denoising spatial transcriptomics images that is particularly effective for uncovering cell identities in the regime of ultra-low sequencing depths, and also allows for interpolation of gene expression. The method -- Spatial Transcriptomics via Adaptive Regularization and Kernels (STARK) -- augments kernel ridge regression with an incrementally adaptive graph Laplacian regularizer. In each iteration, we (1) perform kernel ridge regression with a fixed graph to update the image, and (2) update the graph based on the new image. The kernel ridge regression step involves reducing the infinite dimensional problem on a space of images to finite dimensions via a modified representer theorem. Starting with a purely spatial graph, and updating it as we improve our image makes the graph more robust to noise in low sequencing depth regimes. We show that the aforementioned approach optimizes a block-convex objective through an alternating minimization scheme wherein the sub-problems have closed form expressions that are easily computed. This perspective allows us to prove convergence of the iterates to a stationary point of this non-convex objective. Statistically, such stationary points converge to the ground truth with rate $\mathcal{O}(R^{-1/2})$ where $R$ is the number of reads. In numerical experiments on real spatial transcriptomics data, the denoising performance of STARK, evaluated in terms of label transfer accuracy, shows consistent improvement over the competing methods tested.

</details>


### [132] [An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees](https://arxiv.org/abs/2512.11052)
*Joe Suk,Samory Kpotufe*

Main category: stat.ML

TL;DR: SONAR：针对非平稳流数据的单程异常检测方法，通过SGD优化的OCSVM解决传统方法计算量大和假阴性错误高的问题，提供理论保证和自适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统离线异常检测方法（如核OCSVM）在非平稳流数据场景下存在两个主要问题：1）计算量大，不适合流式处理；2）在非平稳数据下假阴性（Type II）错误率高。需要一种高效的单程流式异常检测方法。

Method: 提出SONAR方法：1）基于SGD的OCSVM求解器，使用强凸正则化；2）在对抗性非平稳数据中，将SONAR集成到集成方法中，结合变点检测实现自适应保证。

Result: 理论证明SONAR的Type I/II错误保证优于传统OCSVM，在良性分布漂移下具有有利的终身学习保证，在对抗性非平稳数据中能确保每个数据阶段的小错误率。在合成和真实数据集上验证了理论结果。

Conclusion: SONAR是一种高效的单程流式异常检测方法，通过SGD优化和强凸正则化解决了传统方法的计算和性能问题，在非平稳数据下提供理论保证，适用于实际流数据场景。

Abstract: We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets.

</details>


### [133] [Provable Recovery of Locally Important Signed Features and Interactions from Random Forest](https://arxiv.org/abs/2512.11081)
*Kata Vuk,Nicolas Alexander Ihlo,Merle Behr*

Main category: stat.ML

TL;DR: 提出了一种新的局部特征与交互重要性方法，用于随机森林的个体预测解释，该方法结合全局模式和特定测试点的路径特征，在理论保证下识别局部信号特征及其交互作用。


<details>
  <summary>Details</summary>
Motivation: 在许多领域如个性化医疗中，需要针对个体预测的局部解释而非全局特征重要性。随机森林在这些场景中广泛应用，但现有的局部特征与交互重要性方法理论理解有限，难以解释个体预测中的高重要性分数。

Method: 提出了一种新的局部、模型特定的FII方法，通过识别决策路径中特征的频繁共现，将全局模式与特定测试点路径上观察到的模式相结合。该方法在局部稀疏尖峰模型下具有理论保证。

Result: 证明了该方法在局部稀疏尖峰模型下能够一致地恢复真实的局部信号特征及其交互作用，并能识别是大值还是小值驱动预测。通过模拟研究和真实世界数据示例验证了方法的有效性。

Conclusion: 该方法为随机森林的局部特征与交互重要性提供了理论保证的解释框架，能够可靠地识别个体预测中的关键特征和交互作用，有助于个性化医疗等领域的应用。

Abstract: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.

</details>


### [134] [TPV: Parameter Perturbations Through the Lens of Test Prediction Variance](https://arxiv.org/abs/2512.11089)
*Devansh Arpit*

Main category: stat.ML

TL;DR: 本文提出测试预测方差(TPV)作为连接深度网络泛化多个经典观察的统一量度，TPV是标签无关的量，其迹形式分离了训练模型几何与特定扰动机制，可用于分析SGD噪声、标签噪声等多种参数扰动。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络泛化能力时，存在多个经典观察但缺乏统一的理论框架。本文旨在找到一个能够连接这些观察的数学量，从而更好地理解深度网络的泛化行为。

Method: 提出测试预测方差(TPV)概念，定义为模型输出对参数扰动的一阶敏感性。TPV的迹形式将训练模型几何与扰动机制分离，允许在统一框架下分析多种参数扰动。理论证明在过参数化极限下，训练集估计的TPV收敛到测试集值。

Result: TPV在不同数据集和架构（包括极窄网络）中表现出惊人的稳定性，且与干净测试损失有良好相关性。将剪枝建模为TPV扰动，得到简单的标签无关重要性度量，性能与最先进剪枝方法相当。

Conclusion: TPV作为统一框架连接了深度网络泛化的多个经典观察，理论证明其可仅从训练输入推断，实证显示其稳定性和实用性，特别是在剪枝任务中表现出色。

Abstract: We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV.

</details>


### [135] [Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics](https://arxiv.org/abs/2512.11090)
*Biraj Dahal,Jiahui Cheng,Hao Liu,Rongjie Lai,Wenjing Liao*

Main category: stat.ML

TL;DR: WeldNet：一种基于窗口化编码器的非线性模型降阶框架，通过分窗口训练自编码器和传播器网络来学习复杂演化系统的低维代理模型


<details>
  <summary>Details</summary>
Motivation: 科学和工程中的许多问题涉及复杂物理过程产生的高维时变数据集，这些模拟成本高昂。需要开发数据驱动的非线性模型降阶方法来构建低维代理模型

Method: 将时域划分为多个重叠窗口，在每个窗口内使用自编码器进行非线性降维以捕获潜在编码。训练传播器网络学习每个窗口内潜在编码的演化，训练转码器连接相邻窗口的潜在编码

Result: 数值实验表明WeldNet能够捕获非线性潜在结构及其底层动力学，在多种微分方程上表现优于传统投影方法和近期发展的非线性模型降阶方法

Conclusion: WeldNet通过窗口化分解将长时程动力学分解为多个短而可管理的片段，简化了传播器训练，同时转码器确保了窗口间的一致性，为复杂演化系统提供了有效的非线性模型降阶框架

Abstract: Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods.

</details>


### [136] [Conditional Coverage Diagnostics for Conformal Prediction](https://arxiv.org/abs/2512.11779)
*Sacha Braun,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: stat.ML

TL;DR: 提出ERT指标，通过分类问题评估条件覆盖，比现有方法更高效，能区分过覆盖和欠覆盖，并提供开源工具包。


<details>
  <summary>Details</summary>
Motivation: 评估条件覆盖是预测系统可靠性评估中最持久的挑战之一。现有方法无法保证正确的条件覆盖，且现有指标存在样本效率低和过拟合问题，缺乏解释局部偏差的清晰方法。

Method: 将条件覆盖估计转化为分类问题：条件覆盖被违反当且仅当任何分类器能获得低于目标覆盖的风险。通过选择适当的损失函数，得到的风险差提供对自然误覆盖度量的保守估计，称为目标覆盖的过度风险(ERT)。

Result: 实验表明，使用现代分类器比基于简单分类器的现有指标(如CovGap)具有更高的统计功效。使用该指标对不同共形预测方法进行基准测试，并发布了ERT及先前条件覆盖指标的开源包。

Conclusion: ERT指标为理解、诊断和改进预测系统的条件可靠性提供了新视角，解决了条件覆盖评估的长期挑战。

Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [137] [Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling](https://arxiv.org/abs/2512.11415)
*Marco Baiesi,Alberto Rosso*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了一种非平衡动力学驱动的无监督机器学习模型，通过打破隐变量空间中的细致平衡，诱导概率流的自发循环，从而提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统隐变量模型（如受限玻尔兹曼机）通常基于平衡统计力学，但作者认为非平衡动力学可以在无监督学习中发挥建设性作用。他们探索非平衡统计物理与现代机器学习的接口，旨在通过引入不可逆性来增强生成模型的性能。

Method: 引入一个可见变量和隐变量通过两个独立参数化的转移矩阵相互作用的模型，定义了一个稳态本质上处于非平衡状态的马尔可夫链。似然最大化驱动系统趋向具有有限熵产、降低自转移概率和隐空间中持续概率流的非平衡稳态。该模型打破了前向和后向条件转移之间的细致平衡，并依赖于明确依赖于马尔可夫链最后两步的对数似然梯度。

Result: 训练过程中自发出现了隐状态循环，这些循环不是由架构强加的，而是从训练中产生的。开发这些循环的模型避免了与近乎可逆动力学相关的低对数似然状态，并且更忠实地再现了数据类别的经验分布。与非平衡方法相比，该模型在生成性能上有所提升。

Conclusion: 将不可逆性引入隐变量模型可以增强生成性能，非平衡动力学在无监督机器学习中具有建设性作用。这项工作探索了非平衡统计物理与现代机器学习的接口，为开发更强大的生成模型提供了新思路。

Abstract: We show that nonequilibrium dynamics can play a constructive role in unsupervised machine learning by inducing the spontaneous emergence of latent-state cycles. We introduce a model in which visible and hidden variables interact through two independently parametrized transition matrices, defining a Markov chain whose steady state is intrinsically out of equilibrium. Likelihood maximization drives this system toward nonequilibrium steady states with finite entropy production, reduced self-transition probabilities, and persistent probability currents in the latent space. These cycles are not imposed by the architecture but arise from training, and models that develop them avoid the low-log-likelihood regime associated with nearly reversible dynamics while more faithfully reproducing the empirical distribution of data classes. Compared with equilibrium approaches such as restricted Boltzmann machines, our model breaks the detailed balance between the forward and backward conditional transitions and relies on a log-likelihood gradient that depends explicitly on the last two steps of the Markov chain. Hence, this exploration of the interface between nonequilibrium statistical physics and modern machine learning suggests that introducing irreversibility into latent-variable models can enhance generative performance.

</details>
