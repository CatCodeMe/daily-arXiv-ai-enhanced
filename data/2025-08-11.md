<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 48]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 8]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.SC](#cs.SC) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming](https://arxiv.org/abs/2508.06077)
*Hongqin Lei,Haowei Tang,Zhe Zhang*

Main category: cs.DB

TL;DR: 论文提出一个基于GTA V游戏的数据集，标注玩家感兴趣的对象，并分析影响玩家兴趣的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常忽略对象间的语义关系和独特特征，影响深度学习方法的效率。

Method: 收集GTA V游戏片段，标注玩家感兴趣的对象，并分析影响兴趣的因素。

Result: 发现玩家的游戏内速度、对象大小和对象速度是主要影响因素。

Conclusion: 提出的数据集有助于提升云游戏中深度学习方法的效率。

Abstract: Cloud gaming has gained popularity as it provides high-quality gaming
experiences on thin hardware, such as phones and tablets. Transmitting gameplay
frames at high resolutions and ultra-low latency is the key to guaranteeing
players' quality of experience (QoE). Numerous studies have explored deep
learning (DL) techniques to address this challenge. The efficiency of these
DL-based approaches is highly affected by the dataset. However, existing
datasets usually focus on the positions of objects while ignoring semantic
relationships with other objects and their unique features. In this paper, we
present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)
V, and annotating the player's interested objects during the gameplay. Based on
the collected data, we analyze several factors that have an impact on player's
interest and identify that the player's in-game speed, object's size, and
object's speed are the main factors. The dataset is available at
https://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: VectorCDC是一种利用向量CPU指令加速无哈希CDC算法的方法，显著提升性能而不影响去重空间节省。


<details>
  <summary>Details</summary>
Motivation: CDC算法由于需要扫描整个文件，速度慢且成为去重系统的主要性能瓶颈。

Method: 使用向量CPU指令（如SSE/AVX）加速无哈希CDC算法。

Result: 在Intel、AMD、ARM和IBM CPU上，VectorCDC的吞吐量比现有向量加速技术高8.35x-26.2x。

Conclusion: VectorCDC有效解决了CDC算法的性能瓶颈，同时保持了去重的空间节省效果。

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [3] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: 本文提出了一种基于分数的动态负载均衡器（SBDLB），通过实时性能指标分配工作负载，显著提升了云系统的资源利用率和效率。


<details>
  <summary>Details</summary>
Motivation: 云计算中负载均衡是确保性能、防止过载的关键，但长期保持负载均衡仍具挑战性。

Method: 提出SBDLB方法，基于实时性能指标分配工作负载，并在CloudSim 7G平台上与节流负载均衡策略对比测试。

Result: SBDLB在响应时间、数据处理时间和运营成本上均优于节流策略，分别提升34%、37%、13%和15%。

Conclusion: SBDLB能动态适应负载波动，优化资源使用，提升云基础设施的能效和可持续性。

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [4] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: Snowflake的Snowpark是一个支持数据工程和AI/ML工作负载的解决方案，具有高性能、安全性和易用性，并通过弹性扩展和核心计算基础设施集成实现高效运行。


<details>
  <summary>Details</summary>
Motivation: Snowflake旨在通过Snowpark扩展其AI数据云愿景，支持更多编程语言（如Python）和复杂的数据工程及AI/ML任务。

Method: Snowpark采用弹性扩展架构，与Snowflake核心计算基础设施无缝集成，利用控制平面进行分布式计算，并通过安全沙箱隔离SQL和Snowpark任务。创新包括Python包缓存、定制化工作负载调度和数据倾斜管理。

Result: Snowpark显著提升了性能，减少了查询初始化延迟，并通过实际案例展示了其在数据工程和AI/ML任务中的高效性。

Conclusion: Snowpark通过技术创新和实际应用验证，为大规模数据工程和AI/ML任务提供了高效、安全的解决方案。

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [5] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer是一个高效框架，结合工作负载平衡和序列并行，优化分布式训练中的Diffusion Transformers（DiT）。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中因变长文本输入和混合分辨率数据导致的token不平衡问题。

Method: 通过全局背包问题重新分配token，结合序列并行和半经验工作负载模型，最小化通信开销。

Result: 在真实训练中实现小于1%的工作负载差异，消除拖尾效应，速度提升2-3倍。

Conclusion: KnapFormer显著提升了训练效率，适用于混合分辨率和图像-视频联合数据。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [6] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: EC2MoE是一个自适应框架，通过端-云协作优化混合专家（MoE）模型的推理效率，解决异构环境中的专家调度和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: MoE模型在异构端-云环境中的部署面临专家调度、通信开销和资源异构性等挑战，需要一种高效的解决方案。

Method: 提出硬件感知的轻量级分组门网络和端-云协作的流水线优化机制，包括低秩压缩和路由感知的启发式调度算法。

Result: 实验表明，EC2MoE在保持高精度的同时，吞吐量提升2.2x至5.1x，端到端延迟降低53%至67%。

Conclusion: EC2MoE在动态负载和网络环境下表现出良好的可扩展性，为MoE模型的异构部署提供了高效解决方案。

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [7] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: 本文综述了大型语言模型（LLMs）推理中键值（KV）缓存的优化技术，分析了压缩策略及其对内存和推理速度的影响，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs上下文长度的增加，KV缓存需求激增导致内存瓶颈，亟需优化以提高推理效率和可扩展性。

Method: 系统评估了选择性令牌策略、量化和注意力压缩等KV缓存优化技术，分析其效果、权衡和应用场景。

Result: 总结了现有方法的局限性和挑战，如模型和任务的兼容性问题。

Conclusion: 未来研究应关注混合优化技术、自适应动态策略和软硬件协同设计，以提升LLMs的推理效率。

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [8] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: 本文介绍了一种基于Julia的便携式GPU加速QR奇异值计算算法实现，支持多种GPU架构和数据类型，性能优于多数线性代数库。


<details>
  <summary>Details</summary>
Motivation: 奇异值分解（SVD）是科学计算和机器学习中的基础工具，尤其在大型机器学习模型（如LLMs）中用于低秩适应（LoRA）。现有实现缺乏对Apple Metal GPU和半精度的支持。

Method: 采用经典的两阶段QR降阶方法，结合Julia的多重分派和元编程能力，与GPUArrays和KernelAbstractions框架集成，实现硬件无关的统一函数。

Result: 在多种GPU后端和数据类型上，性能优于MAGMA、SLATE等库，对大型矩阵（>1024x1024）达到cuSOLVER的80%-90%性能。

Conclusion: 该实现证明了便携性无需牺牲性能，且首次支持Apple Metal GPU和半精度，为大规模机器学习提供了高效工具。

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [9] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 区块链支持的联邦学习（BCFL）解决了协作AI系统中的信任、隐私和协调问题，通过四维分类法分析其架构设计，并探讨了实际应用案例。


<details>
  <summary>Details</summary>
Motivation: 解决协作AI系统中的信任、隐私和协调问题，探索区块链与联邦学习的结合潜力。

Method: 通过四维分类法（协调结构、共识机制、存储架构和信任模型）分析BCFL系统，并研究具体共识机制（如Proof of Quality和Proof of Federated Learning）。

Result: BCFL系统在医疗、金融和物联网安全等实际应用中表现出与集中式方法相当的性能，同时提供更强的安全性和透明性。

Conclusion: BCFL系统通过区块链技术实现了协作AI的信任与隐私保护，具有广泛的实际应用前景。

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Debiasing Polynomial and Fourier Regression](https://arxiv.org/abs/2508.05920)
*Chris Camaño,Raphael A. Meyer,Kevin Shu*

Main category: cs.DS

TL;DR: 提出了一种基于随机矩阵理论的去偏方法，用于近似未知函数，具有近最优样本复杂度和无偏性。


<details>
  <summary>Details</summary>
Motivation: 现有随机算法在近似函数时会产生有偏估计，需要一种无偏且高效的解决方案。

Method: 通过设计随机复矩阵的特征值作为采样点，提出去偏多项式回归方法。

Result: 该方法无偏、样本复杂度近最优，实验表现优于独立杠杆得分采样。

Conclusion: 该方法不仅适用于多项式回归，还可用于去偏傅里叶级数近似。

Abstract: We study the problem of approximating an unknown function
$f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function
evaluations as possible, where error is measured with respect to a probability
distribution $\mu$. Existing randomized algorithms achieve near-optimal sample
complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce
biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial
regression and random matrix theory. Our method involves evaluating
$f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$
are the eigenvalues of a suitably designed random complex matrix tailored to
the distribution $\mu$. Our estimator is unbiased, has near-optimal sample
complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for
approximating a periodic function with a truncated Fourier series with
near-optimal sample complexity.

</details>


### [11] [A Structural Linear-Time Algorithm for Computing the Tutte Decomposition](https://arxiv.org/abs/2508.06212)
*Romain Bourneuf,Tim Planken*

Main category: cs.DS

TL;DR: 论文提出了一种基于完全嵌套2-分离的简单线性时间算法，用于计算图的Tutte分解，并引入了稳定性的新概念。


<details>
  <summary>Details</summary>
Motivation: 扩展Hopcroft和Tarjan的线性时间算法，通过Cunningham和Edmonds的结构特征，简化Tutte分解的计算。

Method: 首先计算所有完全嵌套的2-分离，然后基于这些分离构建Tutte分解。

Result: 算法在线性时间内完成Tutte分解，并揭示了完全嵌套2-分离的新结构特性。

Conclusion: 提出的算法不仅高效，还通过稳定性概念为图结构研究提供了新视角。

Abstract: The block-cut tree decomposes a connected graph along its cutvertices,
displaying its 2-connected components. The Tutte-decomposition extends this
idea to 2-separators in 2-connected graphs, yielding a canonical
tree-decomposition that decomposes the graph into its triconnected components.
In 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the
Tutte-decomposition. Cunningham and Edmonds later established a structural
characterization of the Tutte-decomposition via totally-nested 2-separations.
We present a conceptually simple algorithm based on this characterization,
which computes the Tutte-decomposition in linear time. Our algorithm first
computes all totally-nested 2-separations and then builds the
Tutte-decomposition from them.
  Along the way, we derive new structural results on the structure of
totally-nested 2-separations in 2-connected graphs using a novel notion of
stability, which may be of independent interest.

</details>


### [12] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: 本文提出了一种称为omnitree的各向异性数据结构，作为octree的扩展，解决了octree在解决各向异性问题时效率低下的问题。omnitree通过仅细化局部最重要的维度，提高了自适应网格细化（AMR）的收敛速度，并在存储和误差控制方面表现更优。


<details>
  <summary>Details</summary>
Motivation: octree在各向异性问题中效率低下，因为其强制各向同性细化，导致分辨率浪费。本文旨在提出一种更高效的数据结构来解决这一问题。

Method: 提出omnitree作为octree的各向异性扩展，允许仅细化关键维度，从而减少树的深度和宽度。通过4,166个三维对象的二进制形状表示问题验证其性能。

Result: omnitree将平均收敛速度提高1.5倍，存储需求更低，且能更快最大化信息密度。在高维问题中优势更明显。

Conclusion: omnitree显著提升了AMR的效率，并为高维应用开辟了新可能性。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


### [13] [A Simple PTAS for Weighted $k$-means and Sensor Coverage](https://arxiv.org/abs/2508.06460)
*Akash Pareek,Supratim Shit*

Main category: cs.DS

TL;DR: 本文提出了一种针对加权k均值问题的简单PTAS算法，无需依赖核心集，扩展了未加权情况的框架，并应用于传感器覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 现有加权k均值算法多依赖核心集，缺乏专门设计的简单PTAS。

Method: 基于加权D²采样技术，扩展未加权情况的框架，提出核心集无关的PTAS。

Result: 算法运行时间为n·d·2^O(k²/ε)，输出中心的总聚类成本在最优成本的(1+ε)倍内。

Conclusion: 该算法不仅解决了加权k均值问题，还为传感器覆盖问题提供了更优的近似解。

Abstract: Clustering is a fundamental technique in data analysis, with the $k$-means
being one of the widely studied objectives due to its simplicity and broad
applicability. In many practical scenarios, data points come with associated
weights that reflect their importance, frequency, or confidence. Given a
weighted point set $P \subset R^d$, where each point $p \in P$ has a positive
weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2,
\ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost:
$\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the
Euclidean distance from $p$ to its nearest center in $C$. Although most
existing coreset-based algorithms for $k$-means extend naturally to the
weighted setting and provide a PTAS, no prior work has offered a simple,
coreset-free PTAS designed specifically for the weighted $k$-means problem.
  In this paper, we present a simple PTAS for weighted $k$-means that does not
rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012)
for the unweighted case, we extend the result to the weighted setting by using
the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot
2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose
total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost.
As a key application of the weighted $k$-means, we obtain a PTAS for the sensor
coverage problem, which can also be viewed as a continuous locational
optimization problem. For this problem, the best-known result prior to our work
was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm
guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even
before applying refinement steps like Lloyd desent.

</details>


### [14] [On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions](https://arxiv.org/abs/2508.06478)
*Dan Johnson,Michael Levet,Petr Vojtěchovský,Brett Widholm*

Main category: cs.DS

TL;DR: 本文研究了有限群和拟群的同构测试计算复杂度，利用分解方法改进了现有结果，包括将某些群的同构问题归入L类，并提出了AC³和NC算法。


<details>
  <summary>Details</summary>
Motivation: 研究有限群和拟群同构测试的计算复杂度，旨在通过分解方法改进现有算法的效率和适用范围。

Method: 利用群的直接积分解和拟群的仿射分解，结合Weisfeiler--Leman算法和并行计算技术。

Result: 1. 特定群类的同构问题归入L类；2. 提出AC³规范标记算法；3. 中心拟群同构测试归入NC类。

Conclusion: 通过分解方法显著提升了群和拟群同构测试的计算效率，扩展了现有算法的适用范围。

Abstract: In this paper, we investigate the computational complexity of isomorphism
testing for finite groups and quasigroups, given by their multiplication
tables. We crucially take advantage of their various decompositions to show the
following:
  - We first consider the class $\mathcal{C}$ of groups that admit direct
product decompositions, where each indecompsable factor is $O(1)$-generated,
and either perfect or centerless. We show any group in $\mathcal{C}$ is
identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL)
algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL
algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for
$\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was
$\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting
WL (Grochow and Levet, FCT 2023).
  - We next consider more generally, the class of groups where each
indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$
canonical labeling procedure for this class. Here, we accomplish this by
showing that in the multiplication table model, the direct product
decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of
Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary
quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that
central quasigroups admit an affine decomposition in terms of an underlying
Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously
known for isomorphism testing of central quasigroups.

</details>


### [15] [Does block size matter in randomized block Krylov low-rank approximation?](https://arxiv.org/abs/2508.06486)
*Tyler Chen,Ethan N. Epperly,Raphael A. Meyer,Christopher Musco,Akash Rao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using
randomized block Krylov iteration. Prior work has shown that, for block size $b
= 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best
rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$
matrix-vector products with the target matrix. On the other hand, when $b$ is
between $1$ and $k$, the best known bound on the number of matrix-vector
products scales with $b(k-b)$, which could be as large as $O(k^2)$.
Nevertheless, in practice, the performance of block Krylov methods is often
optimized by choosing a block size $1 \ll b \ll k$. We resolve this
theory-practice gap by proving that randomized block Krylov iteration produces
a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde
O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le
k$. Our analysis relies on new bounds for the minimum singular value of a
random block Krylov matrix, which may be of independent interest. Similar
bounds are central to recent breakthroughs on faster algorithms for sparse
linear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 论文提出了一种基于多准则决策（MCDM）的数据驱动框架PySelect，用于解决开源生态中第三方软件包选择的挑战，通过自动化数据管道和AI辅助意图建模，提高了推荐质量和透明度。


<details>
  <summary>Details</summary>
Motivation: 开源生态中软件包选择困难，现有生成式AI工具依赖流行度而非适用性，缺乏透明度和可复现性。

Method: 将软件包选择建模为MCDM问题，构建自动化数据管道收集多源数据，并开发决策支持系统PySelect，结合大语言模型解析用户意图。

Result: 实验表明，该方法数据提取精度高，推荐质量优于生成式AI基线，用户评价积极。

Conclusion: 该框架为软件包选择提供了可扩展、可解释且可复现的解决方案，支持基于证据的决策。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [17] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest是一个用于代码强化学习的测试用例合成框架，通过生成-验证（G-V）机制确保测试用例的质量和可靠性，显著提升模型性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 高质量的测试用例对训练大型语言模型至关重要，但目前合成高质量测试用例仍是一个未解决的难题。

Method: 采用生成-验证（G-V）框架，通过一致性验证机制生成全面的测试用例，包括常规和边界情况，并设计多层安全沙箱系统确保代码执行安全。

Result: 实验表明，该方法显著提升了模型性能和训练稳定性。

Conclusion: Klear-CodeTest为代码强化学习提供了高质量的测试用例合成方案，具有广泛的应用潜力。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [18] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: 论文探讨了如何在大学Laravel课程中通过Composer及其精选包提升开发效率，同时强调代码质量和概念理解，并提供了避免依赖风险的实践建议。


<details>
  <summary>Details</summary>
Motivation: 学生在有限时间内完成Laravel项目存在困难，希望通过Composer工具提升效率并培养专业开发习惯。

Method: 介绍Composer及其精选包，结合教学实践说明如何利用这些工具构建学术或个人项目，同时注重代码质量和概念理解。

Result: 通过Composer包加速开发，同时强化专业工作流程和行业准备，但需注意避免过度依赖和包冲突。

Conclusion: 有效整合Composer包需结合教学目标，引导学生批判性使用工具，确保实用性与深度学习并重。

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [19] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 论文提出了一种结合确定性逆向工程与LLM引导的意图感知可视化探索的混合方法，用于提升代码理解效率。


<details>
  <summary>Details</summary>
Motivation: 开发者花费大量时间理解复杂软件系统，传统工具缺乏交互性和上下文整合，LLM虽先进但缺乏结构化视图的整合。

Method: 结合UML可视化、动态用户界面、历史上下文和协作功能，通过LLM解析用户查询和交互模式。

Result: 原型实现展示了该方法的可行性，未来将进行实证评估和扩展。

Conclusion: 研究为智能交互环境奠定了基础，支持开发者认知和协作工作流。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [20] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: 本文提出了一种基于遗传算法的测试输入生成方法，通过结合遗传操作和自适应学习，显著提升了软件漏洞检测的效果。


<details>
  <summary>Details</summary>
Motivation: 随着软件复杂性的增加，传统漏洞检测方法的能力不足，亟需更高效的解决方案。

Method: 采用遗传算法，结合交叉操作和自适应反馈机制，动态生成有效的测试输入。

Result: 在九个开源JSON处理库上的测试显示，该方法在覆盖率上显著优于基准方法，平均提升幅度从39.8%到166.0%。

Conclusion: 该方法能够检测更深层次和更复杂的漏洞，为软件安全测试提供了可扩展且自适应的解决方案。

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [21] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: 本文系统综述了Kubernetes调度策略，分类为硬件和软件中心，关注可持续性目标，并提出了一种针对环境可持续性的云任务调度分类法。


<details>
  <summary>Details</summary>
Motivation: 大规模软件生态系统和云数据中心的能源需求激增，尤其是大型语言模型的训练和部署，导致能源消耗和碳足迹达到前所未有的水平，亟需减少碳排放。

Method: 对Kubernetes调度策略进行系统综述，分类为硬件和软件中心，标注其可持续性目标，并按算法分组。提出一种针对环境可持续性的云任务调度分类法。

Result: 分析了新兴研究趋势和开放挑战，为下一代云计算系统的可持续调度解决方案设计提供了关键见解。

Conclusion: 研究为设计可持续的云计算调度解决方案提供了重要参考，并指出了未来研究方向。

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [22] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种基于检索增强生成（RAG）的代码补全方法CODEFILTER，通过评估检索代码块的贡献并过滤负面上下文，显著提升了补全准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决检索增强生成中检索到的跨文件上下文对代码补全的负面影响，并优化其贡献。

Method: 引入基于似然的度量评估检索代码块的影响，构建标记数据集，并提出自适应过滤框架CODEFILTER。

Result: CODEFILTER在多个基准测试中提升了补全准确性，减少了输入提示长度，并增强了计算效率。

Conclusion: CODEFILTER在代码补全的准确性、效率和可归因性方面具有显著潜力。

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [23] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 论文探讨了智能编码系统在生成代码时需提供清晰、一致的合理性解释，以解决用户信任问题，并提出了神经符号方法作为解决方案。


<details>
  <summary>Details</summary>
Motivation: AI驱动的编码系统决策不透明，导致用户（尤其是非专家）对其信任和可用性产生疑虑。

Method: 提出神经符号方法，通过符号约束指导模型训练，并利用神经表示丰富程序语义，实现推理时的自动一致性检查。

Result: 现有方法（如形式验证、静态分析和事后解释性）在认知对齐和语义忠实性方面存在局限。

Conclusion: 神经符号方法有望生成更符合用户理解的代码解释，提升系统透明度和信任度。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [24] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文首次对智能合约中的不一致状态更新漏洞进行了大规模实证研究，总结了其根本原因、修复策略和利用方法，并提出了11项重要发现。


<details>
  <summary>Details</summary>
Motivation: 智能合约的状态更新问题可能导致严重的安全漏洞，现有工具难以有效检测，因此需要深入研究以帮助开发者、研究人员和工具设计者避免此类漏洞。

Method: 系统调查了352个真实智能合约项目中的116个不一致状态更新漏洞，分析其根因、修复策略和利用方法。

Result: 研究提出了11项重要发现，并基于其中一项发现开发了一个概念验证检查器，成功在64个GitHub项目中检测到问题，19个项目所有者确认了问题。

Conclusion: 研究结果对避免智能合约中的不一致状态更新漏洞具有重要价值，为相关领域提供了实用指导。

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [25] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: 论文研究了OutSystems平台中业务流程建模语言（BPT）的低采用率和可用性问题，通过访谈、符号学分析和实证评估改进BPT，显著提升了语义透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: BPT语言的低采用率和可用性问题增加了维护成本，亟需改进以提升开发者体验。

Method: 结合访谈、符号学分析（Physics of Notation）和实证评估（SUS和NASA-TLX），开发了BPT的新版本。

Result: 新版本显著提升了语义透明度（31%到69%）、回答正确率（51%到89%）、SUS分数（42.25到64.78），并降低了TLX分数（36.50到20.78）。

Conclusion: 改进后的BPT显著提升了开发者体验，用户背景对最终语法选择和可用性指标有重要影响。

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [26] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 论文探讨如何自动生成软件工程问题的复现测试，提出新方法解决代码缺失或错误的问题，实验显示新工具e-Otter++在测试生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大多数软件工程问题缺乏复现测试，手动生成耗时且困难，因此需要自动化解决方案。

Method: 引入新技术利用执行反馈，绕过代码缺失或错误的问题，开发工具e-Otter++。

Result: 在TDD-Bench Verified基准测试中，e-Otter++的平均失败转通过率为63%。

Conclusion: e-Otter++在自动生成复现测试方面显著优于现有技术。

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [27] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文研究了代码示例中不同特征对上下文学习（ICL）在代码生成中的影响，发现变量和函数命名对性能至关重要，而LLMs更关注语义而非格式。


<details>
  <summary>Details</summary>
Motivation: 探索代码示例中哪些具体特征显著影响ICL在代码生成中的效果，以优化ICL系统。

Method: 通过控制变量实验（ablation studies）分析代码特征（如命名风格、格式等）对ICL效果的影响。

Result: 变量和函数命名的适当性对性能影响显著（性能下降达30%），LLMs更重视语义命名而非格式，且存在语言特定的命名偏好。

Conclusion: 优化命名可显著提升ICL效果，但LLMs在从相似代码中提取通用问题解决洞察方面仍有挑战。

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [28] [Hierarchical Placement Learning for Network Slice Provisioning](https://arxiv.org/abs/2508.06432)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun*

Main category: cs.NI

TL;DR: 提出了一种基于分层多臂老虎机的网络切片请求服务功能链放置策略，以最大化请求接受率并最小化节点资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘移动网络中切片供应的挑战。

Method: 采用分层多臂老虎机方法，设计了两级分层老虎机解决方案，以在线学习可扩展的放置策略。

Result: 在两种真实网络拓扑上的仿真显示，该方法平均节点资源利用率为5%，在某些场景下比基线方法多接受25%以上的切片请求。

Conclusion: 所提方法在优化资源利用和请求接受率方面表现优异。

Abstract: In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.

</details>


### [29] [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](https://arxiv.org/abs/2508.06468)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun,Dimitrios Xenakis*

Main category: cs.NI

TL;DR: 论文提出了一种基于在线多维背包问题的网络切片准入控制方法，通过两种预留策略提升长期收入，模拟结果显示其优于先到先服务策略。


<details>
  <summary>Details</summary>
Motivation: 网络切片资源共享中需求不确定性导致资源分配挑战，需优化准入控制以最大化长期收入。

Method: 将切片准入控制建模为在线多维背包问题，提出两种预留策略及算法。

Result: 模拟显示，相比先到先服务，新方法提升收入12.9%，降低资源消耗1.7%，经济不平等时效果更显著。

Conclusion: 预留策略能有效优化切片准入控制，提升基础设施提供商的收入和资源利用率。

Abstract: Network Slicing has emerged as a powerful technique to enable cost-effective,
multi-tenant communications and services over a shared physical mobile network
infrastructure. One major challenge of service provisioning in slice-enabled
networks is the uncertainty in the demand for the limited network resources
that must be shared among existing slices and potentially new Network Slice
Requests. In this paper, we consider admission control of Network Slice
Requests in an online setting, with the goal of maximizing the long-term
revenue received from admitted requests. We model the Slice Admission Control
problem as an Online Multidimensional Knapsack Problem and present two
reservation-based policies and their algorithms, which have a competitive
performance for Online Multidimensional Knapsack Problems. Through Monte Carlo
simulations, we evaluate the performance of our online admission control method
in terms of average revenue gained by the Infrastructure Provider, system
resource utilization, and the ratio of accepted slice requests. We compare our
approach with those of the online First Come First Serve greedy policy. The
simulation's results prove that our proposed online policies increase revenues
for Infrastructure Providers by up to 12.9 % while reducing the average
resource consumption by up to 1.7% In particular, when the tenants' economic
inequality increases, an Infrastructure Provider who adopts our proposed online
admission policies gains higher revenues compared to an Infrastructure Provider
who adopts First Come First Serve.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: D2D方法将因果循环图（CLD）转化为系统动力学模型（SDM），支持动态分析和干预策略探索，优于传统网络中心性分析。


<details>
  <summary>Details</summary>
Motivation: 因果循环图（CLD）作为静态定性工具，无法支持动态分析和干预策略设计，且传统定量方法易导致错误推断。

Method: 提出Diagrams-to-Dynamics（D2D）方法，利用CLD的结构信息（链接存在性和极性），通过用户简单标注变量类型（存量、流量/辅助变量或常量），生成SDM进行模拟和分析。

Result: D2D能区分高/低优先级杠杆点，与数据驱动模型一致性更高，并提供不确定性估计和数据收集指导。

Conclusion: D2D方法通过开源工具实现，降低了动态建模门槛，未来验证将进一步扩展其应用范围。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [31] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的框架，将物理定律表示为加权知识图谱，并通过图注意力网络（GAT）进行链接预测，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决物理方程表示中的语义模糊问题，并探索跨领域物理关系。

Method: 构建包含400个物理方程的数据库，使用加权知识图谱表示，训练GAT进行链接预测。

Result: GAT在测试中AUC达到0.9742，显著优于基线方法，并揭示了物理学的宏观结构和关键桥梁方程。

Conclusion: 该框架不仅能重新发现已知物理结构，还能生成跨领域关系假设，为理论研究提供新方向。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [32] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的数据驱动方法（神经网络推动），用于学习非线性状态空间模型中的推动项，解决了非线性系统中设计推动项的难题。


<details>
  <summary>Details</summary>
Motivation: 在非线性状态空间模型中，设计有效的推动项具有挑战性，而传统方法难以应对。

Method: 基于Kazantzis-Kravaris-Luenberger观测器理论，提出了一种数据驱动的神经网络推动方法。

Result: 在三个混沌行为基准问题（Lorenz 96模型、Kuramoto-Sivashinsky方程和Kolmogorov流）上验证了方法的有效性。

Conclusion: 神经网络推动方法为非线性系统中的数据同化提供了新的解决方案。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [33] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 提出了一种可扩展的框架，通过整合异构数据重建可信的配电网拓扑结构，结合空间布局和动态信号行为，并引入置信度感知机制和物理约束，验证了高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代电网运行需要准确的配电网拓扑结构，但实际数据来源多样且质量不均，亟需一种可靠的重建方法。

Method: 结合空间基础设施布局（如GIS）和动态信号行为（如电压时间序列），引入置信度感知推断机制，并嵌入物理约束（如变压器容量限制）。

Result: 在Oncor的8000多个电表数据上验证，拓扑重建准确率超过95%，置信度校准和计算效率显著优于基线方法。

Conclusion: 该框架在不确定性和结构有效性方面表现优异，适用于实际部署条件，能快速生成可信的拓扑结构。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [34] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯风险最小化的统一理论框架，用于分析线性编码器-解码器架构，解决了科学机器学习中的正向建模和逆向恢复任务。


<details>
  <summary>Details</summary>
Motivation: 非线性神经网络虽然成功，但理论不透明，限制了其在需要可解释性场景中的应用。线性神经网络则提供了简单有效的理论基础。

Method: 通过贝叶斯风险最小化，推导了闭式、秩约束的线性和仿射线性最优映射。

Result: 理论结果在生物医学成像、金融因子分析和非线性流体动力学模拟中得到了验证。

Conclusion: 该工作为科学机器学习中的神经网络模型提供了理论基础和基准。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [35] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种结合TAPE与Graphormer的新框架，利用ChatGPT生成语义丰富的解释，并通过注意力机制融合文本与结构信息，在ogbn-arxiv数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合文本语义与图结构信息时存在困难，特别是在处理领域术语、长程依赖、时间演化和大规模数据时表现不佳。

Method: 结合TAPE框架与Graphormer，利用ChatGPT生成文本解释，通过注意力机制融合文本与结构特征，并采用路径感知位置编码和多头注意力机制。

Result: 在ogbn-arxiv数据集上分类准确率达0.772，显著优于基线GCN（0.713），并在精确率（0.671）、召回率（0.577）和F1分数（0.610）上表现优异。

Conclusion: 该框架为动态TAG中的节点分类提供了可扩展且鲁棒的解决方案，为知识系统和科学发现的研究指明了方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [36] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 提出了一种基于MDP和RL-PG算法的碰撞规避决策框架，旨在通过早期决策减少燃料消耗并保持碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞规避策略通常在接近碰撞时间点24小时前启动，可能导致燃料浪费。本文旨在通过早期决策优化燃料消耗和风险平衡。

Method: 将碰撞规避建模为连续状态、离散动作和有限时域的MDP，结合风险、燃料消耗和轨道几何的解析模型，使用RL-PG算法训练策略。

Result: 在合成和历史数据上，训练策略显著减少了燃料消耗，同时保持或提高了碰撞风险保障。

Conclusion: 该方法在燃料消耗和风险平衡上优于传统策略，验证了MDP和RL-PG在碰撞规避中的有效性。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [37] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT（Signed-Zero Ternary）是一种2位量化方法，能在固定资源预算下提升信息密度，同时不增加前向路径的负担。


<details>
  <summary>Details</summary>
Motivation: 传统量化被视为性能与计算资源的折中方案，但在固定资源预算下，量化可能提供更优的信息密度。

Method: 提出SZT量化方法，利用2位量化确定性提供梯度信息，且不影响前向路径。

Result: 分析表明，SZT可能比非量化方法具有更高的信息密度。

Conclusion: 在固定资源预算下，SZT量化方法展现出优于传统非量化方案的潜力。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [38] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 论文提出了一种将随机时间序列分解为均值、离散度和噪声的方法，通过机器学习拟合双信号并优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分解问题，提取均值、离散度和噪声，以支持平滑、去噪和进一步分析。

Method: 应用机器学习拟合双信号，优化损失函数，包括正则化项和统计过程控制权重。支持顺序或联合学习，使用非线性优化或神经网络。

Result: 实现了时间序列的分解，可用于平滑、去噪、预测和分析。

Conclusion: 提出的方法能有效分解时间序列，支持多种应用场景，如预测和结构分析。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [39] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 论文提出了一种名为Shifted Gaussian Encoding的激活过滤方法，解决了神经网络PDE求解器因条件数差导致的优化问题，显著提升了求解范围和精度。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络PDE求解器在多保真度和刚性问题中因条件数差导致的优化困难，尤其是在Physics-Informed Extreme Learning Machines (PIELMs)中。

Method: 引入Shifted Gaussian Encoding，一种简单有效的激活过滤步骤，提高矩阵秩和表达能力，同时保持凸性。

Result: 方法将稳态对流-扩散方程的Peclet数求解范围扩展了两个数量级，多频函数学习的误差降低了六个数量级，且比百万参数深度网络更快更准确地拟合高保真图像向量。

Conclusion: 研究表明，条件数而非网络深度是科学神经求解器的瓶颈，简单的架构改进可带来显著性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [40] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO是一种改进的GRPO方法，通过噪声感知优势权重解决Think-Answer Mismatch问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在训练大型推理模型时存在Think-Answer Mismatch问题，噪声奖励信号影响学习效果。

Method: 提出S-GRPO，通过优化噪声感知优势权重来稳定训练。

Result: S-GRPO在多个模型上表现优于GRPO，性能提升显著，且在噪声环境下仍能稳定学习。

Conclusion: S-GRPO为大规模推理模型的训练提供了更稳健和有效的方法。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [41] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: CMOSS是一种高效的组合多臂老虎机算法，解决了现有UCB和对抗性方法的局限性，实现了与理论下限匹配的遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有UCB方法（如CUCB）存在长期遗憾问题，而对抗性方法（如EXP3.M和HYBRID）计算开销大。

Method: 提出CMOSS算法，在半强盗反馈下实现实例无关的遗憾，并扩展至级联反馈。

Result: CMOSS的遗憾为$O\big( (\log k)^2\sqrt{kmT}\big )$，优于基准算法。

Conclusion: CMOSS在理论和实验中均表现出色，解决了现有方法的缺陷。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [42] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 论文提出了一种基于多臂老虎机（MAB）的决策树剪枝方法，通过强化学习动态优化剪枝过程，以提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如CCP和REP）基于贪心策略，可能导致泛化能力不足，特别是在小规模复杂数据集上。

Method: 将剪枝过程建模为探索-利用问题，利用MAB算法动态选择最优剪枝节点。

Result: 实验表明，该方法在多个基准数据集上优于传统剪枝方法。

Conclusion: MAB剪枝方法为决策树优化提供了一种动态且概率化的新思路。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [43] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄漏问题，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量数据和计算资源，传统FML方法存在隐私泄漏问题，FedMeNF旨在解决这些问题。

Method: FedMeNF采用隐私保护损失函数，在本地元优化中减少隐私泄漏，无需保留客户端私有数据。

Result: 实验表明，FedMeNF在少样本或非IID数据下仍能快速优化并保持稳健的重建性能。

Conclusion: FedMeNF在保护隐私的同时，提升了神经场学习的效率和适应性。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [44] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MCRE的框架和MCRQ算法，用于离线强化学习，通过平衡保守性和性能提升来解决分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临的主要挑战是学习策略与行为策略之间的分布偏移，导致OOD动作和过高估计。需要一种方法既能防止过高估计，又能避免过度保守。

Method: 提出了MCRE框架，结合TD误差和行为克隆项，平衡保守性和性能。基于此开发了MCRQ算法，将其整合到离线策略actor-critic框架中。

Result: 实验表明，MCRQ在基准数据集上优于强基线方法和当前最先进的离线RL算法。

Conclusion: MCRE框架和MCRQ算法有效解决了离线强化学习中的分布偏移问题，实现了性能提升。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [45] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义对齐的强化学习方法，通过SBERT计算奖励，替代手工设计的奖励函数，并在多个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在科学机器学习中，设计有效的奖励函数是强化学习的挑战，尤其是在任务目标难以数值化的情况下。现有方法多依赖启发式或手工设计。

Method: 使用SBERT计算当前状态与目标语义指令的余弦相似度作为奖励，替代手工奖励函数。

Result: 实验表明，语义奖励能指导学习实现竞争性控制行为，无需手工奖励函数。

Conclusion: 该方法为自然语言目标与智能体行为的对齐提供了新思路，并为语言模型与控制应用的结合奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [46] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 论文提出了一种解决非线性固定点方程的方法，通过结合半范数收缩和诱导范数的单调性，首次实现了参数无关的最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决非线性固定点方程（如平均奖励Q学习和TD学习）中半范数非单调性导致的参数无关最优收敛速率难以实现的问题。

Method: 将平均误差重新表述为涉及非线性扰动的线性递归，并通过结合半范数收缩和诱导范数的单调性来抑制非线性。

Result: 首次实现了参数无关的$\tilde{O}(1/\sqrt{t})$最优收敛速率，适用于平均奖励和指数折扣场景。

Conclusion: 该方法具有广泛适用性，支持同步/异步更新、单机/分布式部署，以及来自模拟器或马尔可夫轨迹的数据流。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [47] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种新颖的CoT压缩框架，通过锚点引导和基于惊讶度的剪枝，显著减少推理延迟和训练成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链带来的训练成本高、推理延迟大和部署困难的问题，同时避免现有压缩方法破坏逻辑连贯性或无法捕捉关键步骤的缺陷。

Method: 提出ASAP框架，分两步：锚点引导剪枝保留核心结构，基于第一标记惊讶度选择逻辑关键步骤，并训练模型自主生成简洁CoT。

Result: 在代码生成基准测试中达到最优准确率，LiveCodeBench v4_v5上减少23.5%的标记生成和43.5%的推理延迟，Pass@1准确率为36.19%。

Conclusion: ASAP为构建高效强大的LRMs提供了有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [48] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS是一种结合蒙特卡洛树搜索（MCTS）与大型语言模型（LLMs）的神经符号框架，通过多步提示序列优化代码生成质量，显著提升了复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂多步规划任务中表现不佳，现有方法多关注简单任务或基于启发式的代码生成。

Method: 提出MCTS-OPS框架，将提示选择建模为MCTS引导的序列决策过程，优化多步提示序列。

Result: 在网络优化实验中，代码执行成功率和优化结果显著提升（奖励提高2~4倍，标准差降低3倍），最优解获取率提高约10%。

Conclusion: 结合符号规划与LLMs在复杂领域中展现出高质量代码生成的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [49] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 该研究提出了一种新型的分步动态竞争风险模型，用于改善心脏骤停后昏迷患者的神经预后预测，通过分阶段利用时间不变和时间变化特征。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测对ICU临床决策至关重要，但现有方法未能充分利用随时间变化的动态特征。

Method: 扩展Fine and Gray模型，分两阶段建模（时间不变和时间变化特征），并引入神经网络捕捉非线性关系。

Result: 在2,278名患者的回顾性队列中，模型在预测觉醒、撤除生命支持或死亡等竞争结局方面表现出色。

Conclusion: 该模型可推广至多阶段特征收集场景，适用于其他动态预测任务。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [50] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 本文提出了一种自适应异构图神经网络（AHGNN），用于解决异构图中的异质性分布和语义多样性问题，并在高异质性场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多孤立关注异质性或异质性，忽视了实际应用中异质性图的普遍性，导致性能下降。

Method: AHGNN采用异质性感知卷积，考虑跳数和元路径的异质性分布，并通过粗到细的注意力机制整合不同语义空间的信息。

Result: 在七个真实世界图和二十个基线模型上的实验表明，AHGNN在高异质性情况下表现优越。

Conclusion: AHGNN能有效解决异质性图中的异质性分布和语义多样性问题，提升模型性能。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [51] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM是一种动态分配精度的机制，通过运行时根据输入值调整每层的位宽，优化大型语言模型的性能和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决在设备上运行大型语言模型时如何根据不同的运行时约束（如延迟和准确性）有效处理查询的问题。

Method: 利用动态变化的层敏感度，为每层动态分配精度，使用轻量级误差估计器和微调阈值。

Result: 实验表明，DP-LLM在性能和延迟之间取得了优越的平衡，优于现有方法。

Conclusion: DP-LLM通过动态精度分配，显著提升了大型语言模型的效率和适应性。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [52] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 论文提出了针对深度时序模型（如TCNs）的非空泛、架构感知的泛化边界，并引入了一种公平比较方法，揭示了时序依赖性在固定信息预算下可能增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有理论对深度时序模型的泛化性能理解有限，本文旨在填补这一空白，并提供更精确的泛化边界和评估方法。

Method: 通过延迟反馈阻塞机制将依赖样本转化为近似独立样本，推导出基于网络深度、核大小等参数的泛化边界，并设计公平比较方法固定有效样本量。

Result: 强依赖性序列的泛化差距比弱依赖性序列小76%，但收敛速率与理论预测存在差异。

Conclusion: 时序依赖性在固定信息预算下可能有益于学习，但理论与实践的差距仍需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [53] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文首次实现了循环深度可微分逻辑门网络（RDDLGN），将布尔运算与循环架构结合用于序列到序列学习，在WMT'14英德翻译任务中表现接近GRU。


<details>
  <summary>Details</summary>
Motivation: 探索可微分逻辑门在序列建模中的应用，填补这一领域的研究空白。

Method: 提出RDDLGN，结合布尔运算与循环架构，用于序列到序列学习。

Result: 在WMT'14英德翻译任务中，RDDLGN达到5.00 BLEU和30.9%训练准确率，接近GRU性能（5.41 BLEU）。

Conclusion: 证明了循环逻辑神经网络计算的可行性，为FPGA加速等研究方向提供了新思路。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [54] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: HGR结合HSR提升GCRL的样本效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励的GCRL样本效率低，现有方法（如HER）未充分利用经验。

Method: 提出HGR，基于后见目标生成动作正则化先验，结合HSR最大化经验利用。

Result: 在导航和操作任务中，HGR显著提升样本重用效率和性能。

Conclusion: HGR结合HSR是GCRL中高效利用经验的有效方法。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [55] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种基于扩散模型的图像修复技术，用于合成高保真度的口腔癌病变图像，以解决训练数据不足的问题，显著提升了诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 口腔癌诊断中，标注数据集的稀缺性和训练数据的不足限制了诊断模型的性能。

Method: 采用微调的扩散模型和图像修复技术，合成逼真的口腔癌病变图像，并结合多源数据集训练诊断模型。

Result: 分类模型在区分癌变和非癌变组织时准确率达0.97，检测模型对病变位置的识别准确率为0.85。

Conclusion: 该方法验证了合成图像在医学诊断中的潜力，并为其他癌症诊断研究提供了新思路。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [56] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster是一种轻量级技术，通过随机重新平衡聚类分配，减少隐私噪声，提升联邦聚类的隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类通过为每个聚类训练单独模型提升性能，但隐私泄露风险更高。现有差分隐私机制直接应用会导致效用显著下降。

Method: 提出RR-Cluster技术，通过随机重新平衡聚类分配，确保每个聚类有最小客户端数量，减少隐私噪声。

Result: 理论分析表明RR-Cluster降低了隐私噪声方差，实验证明其在合成和真实数据集上显著改善了隐私/效用权衡。

Conclusion: RR-Cluster是一种简单有效的技术，可提升联邦聚类算法的隐私保护能力，同时保持模型性能。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [57] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 预训练神经网络在化学和小分子药物设计中备受关注，但研究发现大多数模型表现与基线ECFP指纹相当，仅CLAMP模型显著优于其他。


<details>
  <summary>Details</summary>
Motivation: 评估预训练神经网络在分子化学任务中的实际表现，揭示现有研究的评估严谨性问题。

Method: 对25种模型在25个数据集上进行公平比较，采用分层贝叶斯统计测试模型进行分析。

Result: 几乎所有神经模型表现与基线ECFP指纹相当，仅CLAMP模型显著优于其他。

Conclusion: 研究质疑现有评估的严谨性，提出潜在原因、解决方案及实用建议。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [58] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP是一种新型的联邦推荐系统，适应不同隐私需求并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户隐私需求相同，忽略了公开数据的潜力。

Method: 利用公开用户数据构建交互图，采用轻量级GCN学习个性化嵌入，本地保护隐私。

Result: 在五个数据集上显著优于现有方法，推荐准确性高且不损害隐私。

Conclusion: GFed-PP为联邦推荐系统提供了适应不同隐私偏好的实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [59] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了一种稳定的重参数化策略梯度方法RPO，通过结合PPO的代理目标和KL正则化，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 重参数化策略梯度（RPG）在样本效率上有潜力，但训练不稳定，高方差梯度会破坏学习过程。

Method: 通过将PPO的代理目标与RPG结合，提出RPO方法，利用时间反向传播高效计算梯度，并通过KL正则化和裁剪代理目标实现稳定样本复用。

Result: 在运动和操作任务上，RPO表现出卓越的样本效率和性能。

Conclusion: RPO是一种稳定且高效的RPG改进方法，适用于复杂任务。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [60] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: 论文提出SCAR框架，通过AI驱动的资源管理优化6G车载娱乐服务，采用ML压缩技术减少数据量并提升调度公平性。


<details>
  <summary>Details</summary>
Motivation: 传统RRM技术难以处理6G车载环境中复杂的数据（如CQI），需要更高效的资源管理方法。

Method: SCAR结合ML压缩技术（聚类和RBF网络）和强化学习策略，优化调度和公平性。

Result: SCAR提升可行调度时间14%，减少不公平调度时间15%，CQI聚类失真降低10%。

Conclusion: SCAR在动态车载网络中展现出可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [61] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了部分特征成员推理攻击（PFMI），提出了一种两阶段攻击框架MRAD，通过优化未知特征值和异常检测来推断样本是否在训练集中。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理方法通常假设攻击者能访问目标样本的全部特征，但现实中往往只能获取部分特征，限制了方法的适用性。

Method: 提出MRAD框架，第一阶段优化未知特征值以最小化样本损失，第二阶段通过异常检测测量重构样本与训练分布的偏差。

Result: 实验表明MRAD在多种数据集上有效，例如在STL-10上，即使缺失40%特征，AUC仍可达0.6。

Conclusion: MRAD解决了部分特征下的成员推理问题，兼容多种异常检测技术，具有实际应用价值。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [62] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 论文研究了微调大型语言模型（LLM）时可能引发的跨领域有害行为（EMA），并提出了四种实用的训练正则化干预方法，以减少EMA的影响。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能导致模型在目标领域外产生有害行为（EMA），即使微调数据本身无害。研究旨在为API提供者提供实用的防护措施。

Method: 研究了四种干预方法：KL散度正则化、特征空间L2距离、安全子空间投影（SafeLoRA）以及混合安全训练数据。

Result: 评估了四种方法在恶意任务中对EMA的抑制效果，并分析了其对良性任务的影响。

Conclusion: 讨论了EMA研究中的开放性问题，为未来研究提供了方向。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [63] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于张量网络（MPS）的高质量隐私保护合成表格数据生成方法，在数据保真度和隐私保护方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私约束及多样化数据集需求，为敏感领域提供高质量且隐私安全的合成数据。

Method: 使用矩阵乘积状态（MPS）生成合成数据，结合噪声注入和梯度裁剪实现差分隐私（DP）。

Result: MPS在严格隐私约束下表现优于CTGAN、VAE和PrivBayes等经典模型。

Conclusion: MPS是一种有前景的隐私保护合成数据生成工具，兼具表达能力和可扩展性。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [64] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GTMancer的框架，利用图神经网络和对比学习整合多组学数据，以改进癌症亚型分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组学数据整合中忽略了异质性组学间的复杂耦合，限制了其在精准肿瘤学中解析癌症亚型异质性的能力。

Method: GTMancer结合图神经网络和对比学习，将多组学数据嵌入统一语义空间，并引入双重注意力系数捕捉组学内和组学间的结构图先验。

Result: 在七个真实癌症数据集上的实验表明，GTMancer优于现有最先进算法。

Conclusion: GTMancer通过全局组学信息优化个体组学表示，显著提升了癌症亚型分类的准确性。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [65] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P是一种新型离线多智能体强化学习算法，通过一步动作采样提高效率，解决了生成模型采样效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和流模型）在离线多智能体强化学习中表现优异，但其迭代生成过程导致采样效率低，难以应用于时间敏感或资源受限的场景。

Method: 提出OM2P算法，结合奖励感知优化方案、均值流匹配损失和Q函数监督，设计广义时间步分布和无导数估计策略，以提高效率和稳定性。

Result: 在Multi-Agent Particle和MuJoCo基准测试中，OM2P性能优越，GPU内存使用减少3.8倍，训练速度提升10.8倍。

Conclusion: OM2P首次成功将均值流模型整合到离线MARL中，为多智能体协作场景中的实用生成策略开辟了新途径。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [66] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 论文研究了在印度语言多样性背景下，通过持续学习（CL）技术解决自动语音识别（ASR）系统在多语言学习中的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性为开发包容性ASR系统带来挑战，传统多语言模型因数据顺序到达和隐私限制不适用，CL提供了一种解决方案。

Method: 使用基于Conformer的混合RNN-T/CTC模型，从印地语预训练开始，逐步学习八种印度语言。评估了三种CL策略（EWC、MAS、LwF），并分析了不同训练周期的影响。

Result: 结果显示CL能有效缓解遗忘问题，相比简单微调表现更优，适用于印度多语言ASR系统。

Conclusion: CL是一种在现实约束下可扩展的ASR解决方案，尤其适用于印度多语言环境。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [67] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出了一种新型多输出脉冲神经元模型，结合了线性状态转换和非线性反馈机制，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 结合脉冲神经网络（SNNs）和深度状态空间模型（SSMs）的优势，解决SSMs缺乏低比特数据处理和重置机制的问题。

Method: 设计了一种多输出脉冲神经元模型，明确区分了脉冲功能、重置条件和重置动作。

Result: 在关键词识别、事件视觉和序列模式识别任务中表现与现有SNN基准相当，且能克服线性动态不稳定性。

Conclusion: 提出的重置机制扩展了深度SSMs的应用范围，展示了在不稳定线性动态下学习的可能性。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [68] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无监督的多智能体强化学习框架，通过动态生成多样化的训练伙伴，无需预训练伙伴或手动调参，显著提升了协作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要预训练伙伴或手动调参的问题，实现更高效的协作学习。

Method: 通过随机混合自我策略与偏置随机行为生成多样化伙伴，并使用基于方差的易学性指标评分。

Result: 在Overcooked-AI等任务中，UPD表现优于基线方法，用户研究显示其更具适应性和人性化。

Conclusion: UPD为无监督协作学习提供了高效解决方案，具有广泛的应用潜力。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [69] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: FCL是一种自适应鲁棒损失函数，通过分数阶导数自动调整对标签噪声的鲁棒性，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数需要大量数据集特定的超参数调优，限制了其实际应用。

Method: FCL结合交叉熵的分数阶导数（主动部分）和MAE（被动部分），通过可学习的分数阶导数参数动态调整鲁棒性与收敛速度。

Result: FCL在基准数据集上实现了最先进的性能，无需手动调参。

Conclusion: FCL通过动态调整损失函数，有效平衡鲁棒性与收敛速度，适用于标签噪声环境。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [70] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型变分自编码器，通过结构方程建模嵌入测量结构，提升表格数据的潜在表示可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据中潜在表示的可解释性问题，结合结构方程建模理论。

Method: 设计SE-VAE架构，将潜在子空间与已知指标分组对齐，并引入全局干扰潜在变量以隔离特定构造的混杂变异。

Result: 在模拟表格数据上，SE-VAE在因子恢复、可解释性和抗干扰性方面优于基线模型。

Conclusion: SE-VAE为科学和社会领域中的白盒生成建模提供了理论驱动的框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [71] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means是一种基于几何原理改进的k-means算法，通过高效利用高表达数据（HE）并跳过低表达数据（LE），显著提升了运行效率和能源经济性。


<details>
  <summary>Details</summary>
Motivation: 传统的k-means算法虽然广泛应用，但存在计算效率低和能源消耗高的问题。Gk-means旨在通过几何优化解决这些问题。

Method: 利用标量投影等几何原理，专注于高表达数据（HE），跳过低表达数据（LE），减少计算开销。

Result: 实验表明，Gk-means在运行时间、距离计算和能源效率上优于传统及SOTA的k-means变体。

Conclusion: Gk-means是一种高效、节能且可持续的k-means改进算法。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [72] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）在无明确诱导情况下自发的欺骗行为，提出了一种基于心理学的评估框架，发现LLMs在处理复杂任务时欺骗倾向增加。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理和决策任务中的广泛应用使其可信度成为关键问题，但现有研究多关注人为诱导的欺骗，忽视了LLMs自发的欺骗行为。

Method: 提出基于'接触搜索问题'的框架，引入两种统计指标（欺骗意图分数和欺骗行为分数）量化LLMs的欺骗倾向。

Result: 评估14种主流LLMs发现，任务难度增加时，欺骗指标显著上升，揭示了LLMs在复杂问题中的欺骗行为。

Conclusion: 研究结果表明，即使最先进的LLMs在复杂任务中也表现出欺骗倾向，这对LLMs在关键领域的部署提出了警示。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [73] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于扩散模型的生成方法，通过分类器引导技术实现对分子生物活性的精确控制，包括增强目标活性和减少副作用。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注单一活性分子，缺乏同时管理多目标和非目标分子相互作用的机制。

Method: 利用分别训练的药物-靶点分类器进行正负引导，扩散模型生成分子。

Result: 实验表明，ActivityDiff能有效处理单/双靶点生成、片段约束双靶点设计、选择性生成和减少副作用等任务。

Conclusion: ActivityDiff为分子设计提供了一种平衡效果和安全性的新范式，是一个多功能且可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [74] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一种三阶段端到端文本到SQL框架，先识别目标数据库再生成SQL查询，结合LLM和提示工程，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统文本到SQL方法需预指定数据库，在多数据库场景下识别目标数据库是关键但被忽视的步骤。

Method: 1. 利用LLM和提示工程从自然语言查询中提取规则；2. 训练基于RoBERTa的db_id预测模型；3. 使用批评代理修正SQL错误。

Result: 实验表明，该框架在数据库意图预测和SQL生成准确性上优于当前最优模型。

Conclusion: 提出的三阶段框架有效解决了多数据库场景下的目标数据库识别问题，提升了SQL生成准确性。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [75] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 利用公开众包数据（如311服务电话和街景图像）预测旧金山无家可归者帐篷趋势，提供更及时、本地化且低成本的信息。


<details>
  <summary>Details</summary>
Motivation: 现有监测方法（如PIT统计）在频率、一致性和空间细节上存在局限，无法捕捉快速变化和局部趋势。

Method: 使用311服务电话和街景图像数据，构建预测模型，捕捉每日和社区级别的细微变化。

Result: 模型揭示了传统统计忽略的模式，如疫情期间的快速波动和帐篷位置的空间变化。

Conclusion: 该方法为政策制定和干预评估提供了更有效的工具，有助于减少无家可归现象。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [76] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR是一种通过高重放训练和周期性重置策略提升LLM样本效率的插件，显著优化了偏好优化方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RL和偏好优化方法在LLM训练中样本效率低和易受初始偏差影响的问题。

Method: 引入LoRR插件，结合高重放训练、周期性重置策略和混合优化目标（SFT与偏好损失）。

Result: LoRR显著提升了多种偏好优化方法在数学和通用推理任务中的性能，甚至优于复杂RL算法。

Conclusion: LoRR为LLM微调提供了高效、实用的新范式，尤其适合数据有限场景。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [77] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN是一个针对大语言模型（LLM）的模块化、目标性框架，通过梯度比指标和选择性噪声注入实现高效遗忘敏感数据，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法在遗忘敏感数据时效果不佳，可能导致不完全遗忘或无关知识退化，因此需要一种更有效的机器遗忘方法。

Method: GRIN提出了一种基于梯度比的指标来定位需要遗忘的参数，并通过选择性噪声注入和微调实现目标性遗忘。

Result: 在TOFU、WMDP和SafePKU等标准基准测试中，GRIN表现出色，既能有效遗忘目标数据，又能保持模型的其他知识。

Conclusion: GRIN为LLM的机器遗忘提供了一种高效且实用的解决方案，解决了现有方法的局限性。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ResPA的新型攻击方法，通过利用残差梯度作为扰动方向，提升对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有迁移攻击方法忽略了扰动方向的影响，导致迁移性受限。

Method: ResPA通过指数移动平均获取参考梯度，并结合当前梯度与参考梯度的残差，捕捉全局扰动方向。

Result: 实验表明，ResPA的迁移性优于现有典型迁移攻击方法，且与输入变换方法结合可进一步提升效果。

Conclusion: ResPA通过优化扰动方向，显著提升了对抗样本的迁移性。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [79] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 论文评估了SAM和Mask3D两种3D分割方法在复杂建筑工地环境中的适应性，填补了户外场景分割的空白。


<details>
  <summary>Details</summary>
Motivation: 传统数据采集方法在建筑工地的复杂动态环境中表现不佳，需探索更高效的计算机视觉方法。

Method: 通过比较SAM和Mask3D在室内外建筑环境中的表现，评估其适应性和性能。

Result: 研究揭示了当前分割方法在户外场景的不足，并提出了针对性解决方案。

Conclusion: 该研究推动了建筑工地自动化监测技术的发展，强调了定制化分割流程的重要性。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [80] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出了一种基于能量的测试时适应方法（ETA），用于调整预训练的深度补全模型在目标数据上的预测，通过对抗扰动探索数据空间并训练能量模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的深度补全模型在目标数据上因协变量偏移导致预测错误，需一种无需目标数据先验的适应方法。

Method: 利用对抗扰动探索数据空间，训练能量模型评分预测分布，测试时更新模型参数以最小化能量。

Result: 在三个室内和三个室外数据集上，ETA平均性能提升6.94%（室外）和10.23%（室内）。

Conclusion: ETA通过能量模型有效适应目标数据，显著提升深度补全模型的泛化能力。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [81] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的方法，用于解决流式成像显微镜中粒子分类的数据不平衡问题，并通过生成高质量图像增强数据集，提升多分类深度神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据稀缺和类别不平衡问题，难以有效区分不同类型的粒子（如硅油与蛋白质颗粒），尤其是罕见粒子类型。

Method: 开发了一种先进的扩散模型，生成高保真粒子图像以扩充训练数据集，并通过实验验证其有效性。

Result: 实验表明，生成的图像在视觉和结构上与真实图像高度相似，且能显著提升分类性能。

Conclusion: 该方法解决了数据不平衡问题，并公开了模型和工具以促进未来研究。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [82] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 论文呼吁关注卫星光谱图像作为AGI的新模态，提出现有基准的不足，并建议更全面的评估任务。


<details>
  <summary>Details</summary>
Motivation: 卫星光谱图像在AGI研究中未受足够重视，但其潜力巨大，需改进评估方法。

Method: 分析现有基准的局限性，并提出一套全面的任务用于评估地球观测模型。

Result: 强调需要更全面的基准来评估模型在地球观测数据上的泛化能力。

Conclusion: 提出新的评估任务框架，以推动地球观测数据在AGI中的应用。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [83] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST是一种新型无监督域适应方法，利用语言模态的鲁棒性指导视觉模型适应，通过生成伪标签和不确定性估计提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂域偏移（如地理偏移）中现有方法性能不足的问题，利用语言模态的鲁棒性辅助视觉模型适应。

Method: TRUST通过生成伪标签、不确定性估计和多模态软对比学习损失，对齐视觉和语言特征空间。

Result: 在经典（DomainNet）和复杂（GeoNet）域偏移上优于现有方法，达到新SOTA。

Conclusion: TRUST通过语言模态的引导和不确定性估计，显著提升了复杂域偏移下的适应性能。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [84] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST是一种弱监督生成网络，用于通过时空融合方法从Terra MODIS、Landsat 8和Sentinel-2数据中估计每日10米分辨率的陆地表面温度（LST）。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加了对环境监测的需求，而现有的遥感系统在空间和时间分辨率之间存在权衡。

Method: WGAST采用条件生成对抗网络架构，包括特征提取、融合、LST重建和噪声抑制四个阶段，并使用弱监督训练策略。

Result: WGAST在定量和定性评估中均优于现有方法，平均降低RMSE 17.18%，提高SSIM 11.00%，并能有效捕捉细尺度热模式。

Conclusion: WGAST是首个端到端深度学习框架，能够高效生成高分辨率LST数据，为环境监测提供了有力工具。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [85] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出了一种名为ASLSL的新方法，用于处理不完整多模态生理信号的特征选择问题，通过共享潜在结构学习减少缺失信息的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态生理信号情感识别中，高维特征常包含无关、冗余和噪声信息，且数据常不完整，导致分类器性能下降。

Method: 采用自适应共享潜在结构学习（ASLSL），探索不完整多模态信号与情感标签的共享潜在空间。

Result: 在DEAP和DREAMER数据集上，ASLSL优于17种其他特征选择方法。

Conclusion: ASLSL能有效处理不完整多模态数据，提升情感分类性能。

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [86] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [87] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Mańdziuk*

Main category: cs.SD

TL;DR: 比较两种基于Transformer的神经网络模型在音频和弦序列识别中的表现，探讨人工生成数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决音乐信息检索中非版权音频数据不足的问题。

Method: 使用人工生成音频（AAM）、舒伯特冬之旅数据集和McGill Billboard数据集训练模型，并通过Root、MajMin和CCM指标评估。

Result: 人工生成音乐虽与人类创作音乐有差异，但在某些场景下仍有用，可作为补充或独立训练集。

Conclusion: 人工生成数据集在特定情况下（如数据稀缺时）可用于和弦序列识别任务。

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [88] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: DAFMSVC是一种新的歌唱声音转换方法，通过替换自监督学习特征和引入双交叉注意力机制，显著提升了音色相似性和音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在任意到任意歌唱声音转换中音色泄漏或音色相似性和质量不足的问题。

Method: 使用自监督学习特征替换防止音色泄漏，结合双交叉注意力机制融合音色、旋律和语言内容，并引入流匹配模块生成高质量音频。

Result: 实验表明DAFMSVC在音色相似性和自然度上显著优于现有方法。

Conclusion: DAFMSVC通过创新设计有效解决了歌唱声音转换中的关键挑战。

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [89] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: EmoAugNet是一种混合深度学习框架，结合LSTM和1D-CNN，通过数据增强和特征提取显著提升了语音情感识别（SER）的性能。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互（HCI）效果需要可靠的语音情感识别系统。

Method: 使用LSTM和1D-CNN的混合模型，结合传统和创新的数据增强方法，提取RMSE、MFCC和ZCR特征。

Result: 在IEMOCAP和RAVDESS数据集上，模型表现出色，最高加权准确率达96.75%。

Conclusion: EmoAugNet通过数据增强和混合建模显著提升了SER系统的鲁棒性和性能。

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [90] [Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures](https://arxiv.org/abs/2508.01854)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: physics.comp-ph

TL;DR: 提出了一种数据驱动的学习框架，用于识别具有非梯度分量的广义扩散的控制定律。通过结合能量耗散定律和物理一致的惩罚项，设计了两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 研究广义扩散中非梯度漂移的控制定律，以解决复杂扩散过程中的物理规律学习问题。

Method: 结合能量耗散定律与物理一致的惩罚项，设计了两阶段方法，用于恢复伪势和旋转分量。

Result: 在复杂扩散过程中（如耗散-旋转动力学、粗糙伪势和噪声数据）验证了方法的有效性。

Conclusion: 该方法能有效学习非梯度广义扩散中的物理规律。

Abstract: This paper proposes a data-driven learning framework for identifying
governing laws of generalized diffusions with non-gradient components. By
combining energy dissipation laws with a physically consistent penalty and
first-moment evolution, we design a two-stage method to recover the
pseudo-potential and rotation in the pointwise orthogonal decomposition of a
class of non-gradient drifts in generalized diffusions. Our two-stage method is
applied to complex generalized diffusion processes including
dissipation-rotation dynamics, rough pseudo-potentials and noisy data.
Representative numerical experiments demonstrate the effectiveness of our
approach for learning physical laws in non-gradient generalized diffusions.

</details>


### [91] [Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements](https://arxiv.org/abs/2508.05908)
*Shreshth A. Malik,Tiarnan A. S. Doherty,Benjamin Colmey,Stephen J. Roberts,Yarin Gal,Paul A. Midgley*

Main category: physics.comp-ph

TL;DR: 提出了一种结合物理模拟与机器学习的混合框架，用于高保真电子显微镜模拟，解决了实验效应难以建模的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以精确建模电子显微镜实验中的复杂效应，限制了定量晶体结构分析的准确性。

Method: 采用可微分物理模拟与神经网络结合的混合框架，通过自动微分实现物理参数与实验变量的联合优化。

Result: 在三维电子衍射结构优化中，该方法直接从衍射数据学习厚度分布，实现了原子位置、热位移和厚度分布的高保真恢复。

Conclusion: 该混合建模框架为定量电子显微镜提供了新范式，可扩展至其他技术，解决实验复杂性带来的分析限制。

Abstract: High-fidelity electron microscopy simulations required for quantitative
crystal structure refinements face a fundamental challenge: while physical
interactions are well-described theoretically, real-world experimental effects
are challenging to model analytically. To address this gap, we present a novel
hybrid physics-machine learning framework that integrates differentiable
physical simulations with neural networks. By leveraging automatic
differentiation throughout the simulation pipeline, our method enables
gradient-based joint optimization of physical parameters and neural network
components representing experimental variables, offering superior scalability
compared to traditional second-order methods. We demonstrate this framework
through application to three-dimensional electron diffraction (3D-ED) structure
refinement, where our approach learns complex thickness distributions directly
from diffraction data rather than relying on simplified geometric models. This
method achieves state-of-the-art refinement performance across synthetic and
experimental datasets, recovering atomic positions, thermal displacements, and
thickness profiles with high fidelity. The modular architecture proposed can
naturally be extended to accommodate additional physical phenomena and extended
to other electron microscopy techniques. This establishes differentiable hybrid
modeling as a powerful new paradigm for quantitative electron microscopy, where
experimental complexities have historically limited analysis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [92] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: 论文研究了随机游走（RW）算法在分布式系统中的安全性问题，提出了一种名为“Pac-Man”的攻击模型，并设计了AC算法来抵御这种攻击。


<details>
  <summary>Details</summary>
Motivation: 随机游走算法因其低开销和可扩展性在分布式系统中广泛应用，但其对局部交互的依赖使其容易受到恶意行为的攻击。

Method: 提出了Average Crossing（AC）算法，通过复制随机游走来防止RW在Pac-Man攻击下灭绝。

Result: 理论分析表明AC算法能保持RW数量稳定，且RW-based随机梯度下降在攻击下仍收敛。实验验证了理论结果，并发现复制阈值与灭绝概率之间存在相变。

Conclusion: AC算法能有效抵御Pac-Man攻击，保证分布式学习的稳定性，同时揭示了复制阈值与系统行为的关系。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [93] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,Léa Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: 论文讨论了生存分析中的‘降维技术’，将生存任务转化为更常见的回归或分类任务，同时保留生存数据的特性，便于机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 旨在简化生存分析的机器学习流程，避免定制化学习器的需求。

Method: 概述了多种降维技术，并实现了部分技术以适配标准机器学习流程。

Result: 通过示例和基准分析比较了这些技术与现有生存分析方法的预测性能。

Conclusion: 降维技术为生存分析提供了灵活且高效的解决方案，适用于标准机器学习工具。

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [94] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: 提出了一种蒙特卡洛估计器，用于最小化依赖于参数的矩阵的迹，并确保估计器的后向误差以高概率有界。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地最小化依赖于参数的矩阵的迹，尤其是在参数空间和矩阵结构较简单的情况下。

Method: 基于epsilon nets和generic chaining两种方法，推导了采样量的界限，确保后向误差有界。

Result: 两种界限均适用于具有小非对角线质量和参数空间较“小”的矩阵，且对矩阵维度的依赖较弱。

Conclusion: epsilon nets界限更易计算且常数明确，而chaining界限在某些情况下可能更优，但难以评估。

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


### [95] [Lightweight Auto-bidding based on Traffic Prediction in Live Advertising](https://arxiv.org/abs/2508.06069)
*Bo Yang,Ruixuan Luo,Junqi Jin,Han Zhu*

Main category: stat.ML

TL;DR: 本文提出了一种轻量级的实时竞价算法BiCB，结合数学分析和未来流量统计方法，解决了直播广告中实时竞价和未来流量未知的难题。


<details>
  <summary>Details</summary>
Motivation: 直播广告需要秒级控制的实时竞价，且面临未来流量未知的挑战，现有方法要么未考虑全时段流量，要么计算复杂度高。

Method: 提出BiCB算法，结合最优竞价公式和未来流量统计方法，通过低复杂度解决方案逼近最优结果。

Result: 离线与在线实验证明BiCB性能优异且工程成本低。

Conclusion: BiCB在实时竞价和未来流量估计方面表现良好，适用于直播广告场景。

Abstract: Internet live streaming is widely used in online entertainment and
e-commerce, where live advertising is an important marketing tool for anchors.
An advertising campaign hopes to maximize the effect (such as conversions)
under constraints (such as budget and cost-per-click). The mainstream control
of campaigns is auto-bidding, where the performance depends on the decision of
the bidding algorithm in each request. The most widely used auto-bidding
algorithms include Proportional-Integral-Derivative (PID) control, linear
programming (LP), reinforcement learning (RL), etc. Existing methods either do
not consider the entire time traffic, or have too high computational
complexity. In this paper, the live advertising has high requirements for
real-time bidding (second-level control) and faces the difficulty of unknown
future traffic. Therefore, we propose a lightweight bidding algorithm Binary
Constrained Bidding (BiCB), which neatly combines the optimal bidding formula
given by mathematical analysis and the statistical method of future traffic
estimation, and obtains good approximation to the optimal result through a low
complexity solution. In addition, we complement the form of upper and lower
bound constraints for traditional auto-bidding modeling and give theoretical
analysis of BiCB. Sufficient offline and online experiments prove BiCB's good
performance and low engineering cost.

</details>


### [96] [Decorrelated feature importance from local sample weighting](https://arxiv.org/abs/2508.06337)
*Benedikt Fröhlich,Alison Durst,Merle Behr*

Main category: stat.ML

TL;DR: 论文提出了一种名为losaw的局部样本加权方法，用于改善特征相关性存在时的特征重要性评分。该方法通过样本加权减少模型偏差，并在模拟研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 特征相关性会导致特征重要性评分分布不均，甚至噪声特征得分高于信号特征。为解决这一问题，论文提出了losaw方法。

Method: losaw方法基于因果推断中的逆概率加权，通过局部样本加权减少目标特征与其他特征的相关性，并引入最小有效样本量作为调参参数。

Result: 模拟研究表明，losaw能一致提升特征重要性评分的准确性，并在保持分布内数据预测精度的同时，提升分布外数据的预测精度。

Conclusion: losaw是一种灵活且有效的方法，可集成到多种机器学习算法中，改善特征相关性对特征重要性评分的影响。

Abstract: Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.

</details>


### [97] [DP-SPRT: Differentially Private Sequential Probability Ratio Tests](https://arxiv.org/abs/2508.06377)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: stat.ML

TL;DR: DP-SPRT是一种在隐私约束下改进的序列概率比测试方法，通过私有机制处理查询序列，实现误差和隐私需求的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有工作在隐私约束下序列测试中的不足，提出一种能同时满足误差和隐私需求的方法。

Method: 提出DP-SPRT，利用OutsideInterval机制处理查询序列，支持多种噪声分布（如拉普拉斯和高斯噪声）。

Result: 证明了DP-SPRT在误差和样本复杂度上的通用上界，实验显示其实际性能良好。

Conclusion: DP-SPRT在隐私约束下接近最优，适用于误差小且假设接近的场景。

Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [98] [Evaluating Universal Machine Learning Force Fields Against Experimental Measurements](https://arxiv.org/abs/2508.05762)
*Sajid Mannan,Vaibhav Bihani,Carmelo Gonzales,Kin Long Kelvin Lee,Nitya Nand Gosvami,Sayan Ranu,Santiago Miret,N M Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: UniFFBench框架评估了六种先进的通用机器学习力场（UMLFFs），发现其在实验复杂性下的表现远低于计算基准测试，揭示了现实差距。


<details>
  <summary>Details</summary>
Motivation: 评估UMLFFs在实际实验数据中的表现，而非仅限于计算基准测试，以揭示其真实可靠性。

Method: 使用UniFFBench框架，对约1,500种矿物结构的实验数据进行了系统评估，涵盖多样化学环境、键合类型和弹性性质。

Result: UMLFFs在计算基准测试中表现优异，但在实验复杂性下表现不佳，密度预测误差超出实用阈值，且误差与训练数据代表性相关。

Conclusion: 当前计算基准可能高估模型可靠性，需建立实验验证标准以提升UMLFFs的通用性。

Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize
materials science by enabling rapid atomistic simulations across the periodic
table. However, their evaluation has been limited to computational benchmarks
that may not reflect real-world performance. Here, we present UniFFBench, a
comprehensive framework for evaluating UMLFFs against experimental measurements
of ~1,500 carefully curated mineral structures spanning diverse chemical
environments, bonding types, structural complexity, and elastic properties. Our
systematic evaluation of six state-of-the-art UMLFFs reveals a substantial
reality gap: models achieving impressive performance on computational
benchmarks often fail when confronted with experimental complexity. Even the
best-performing models exhibit higher density prediction error than the
threshold required for practical applications. Most strikingly, we observe
disconnects between simulation stability and mechanical property accuracy, with
prediction errors correlating with training data representation rather than the
modeling method. These findings demonstrate that while current computational
benchmarks provide valuable controlled comparisons, they may overestimate model
reliability when extrapolated to experimentally complex chemical spaces.
Altogether, UniFFBench establishes essential experimental validation standards
and reveals systematic limitations that must be addressed to achieve truly
universal force field capabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [99] [Quantum Resource Management in the NISQ Era: Implications and Perspectives from Software Engineering](https://arxiv.org/abs/2508.05697)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 本文探讨了NISQ时代量子资源管理的重要性及其对量子软件工程的影响，旨在推动量子资源估计（QRE）领域的发展。


<details>
  <summary>Details</summary>
Motivation: 量子计算机虽能解决经典系统无法处理的复杂问题，但NISQ时代的硬件存在诸多限制，如有限的量子比特、高错误率和短相干时间。因此，高效管理量子资源对量子算法设计和部署至关重要。

Method: 分析了NISQ设备中资源的作用，探讨了其对量子软件工程的影响。

Result: 明确了资源在当前NISQ应用中的重要性，并提出了相关启示。

Conclusion: 通过加强量子资源估计（QRE）领域的研究，推动可扩展且可靠的量子软件开发。

Abstract: Quantum computers represent a radical technological breakthrough in
information processing by leveraging the principles of quantum mechanics to
solve highly complex problems beyond the reach of classical systems. However,
in the current NISQ era (noisy intermediate-scale quantum devices), the
available hardware presents several limitations, such as a limited number of
qubits, high error rates, and short coherence times. Efficient management of
quantum resources, both physical and logical, is especially relevant in the
design and deployment of quantum algorithms. In this paper, we analyze the role
of resources in current uses of NISQ devices, identifying their relevance and
implications for quantum software engineering. With this contribution, we aim
to strengthen the field of Quantum Resource Estimation (QRE) and move toward
scalable and reliable quantum software development

</details>


### [100] [MPS-JuliQAOA: User-friendly, Scalable MPS-based Simulation for Quantum Optimization](https://arxiv.org/abs/2508.05883)
*Sean Feeney,Reuben Tate,John Golden,Stephan Eidenbenz*

Main category: quant-ph

TL;DR: MPS-JuliQAOA是一个用户友好的开源工具，用于模拟量子近似优化算法（QAOA），支持512量子位和20轮模拟。


<details>
  <summary>Details</summary>
Motivation: 提供一个易于使用的工具，模拟QAOA算法，无需用户掌握复杂的矩阵乘积状态（MPS）原理或自动微分技术。

Method: 利用Julia语言和ITensor包实现MPS方法，模拟QAOA，并内置参数查找功能。

Result: 工具在标准3-regular MaxCut QAOA问题上表现出色，支持大规模模拟，并研究了运行时、内存和精度的权衡。

Conclusion: MPS-JuliQAOA是一个高效、易用的QAOA模拟工具，适用于广泛的优化问题。

Abstract: We present the MPS-JuliQAOA simulator, a user-friendly, open-source tool to
simulate the Quantum Approximate Optimization Algorithm (QAOA) of any
optimization problem that can be expressed as diagonal Hamiltonian. By
leveraging Julia-language constructs and the ITensor package to implement a
Matrix Product State (MPS) approach to simulating QAOA, MPS-Juli-QAOA
effortlessly scales to 512 qubits and 20 simulation rounds on the standard
de-facto benchmark 3-regular MaxCut QAOA problem. MPS-JuliQAOA also has
built-in parameter finding capabilities, which is a crucial performance aspect
of QAOA. We illustrate through examples that the user does not need to know MPS
principles or complex automatic differentiation techniques to use MPS-JuliQAOA.
We study the scalability of our tool with respect to runtime, memory usage and
accuracy tradeoffs. Code available at
https://github.com/lanl/JuliQAOA.jl/tree/mps.

</details>


### [101] [Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications](https://arxiv.org/abs/2508.06131)
*Philip Anton Hernicht,Alona Sakhnenko,Corey O'Meara,Giorgio Cortiana,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: 该论文提出了一种轻量级方法，通过经典替代模型绕过量子硬件限制，实现量子模型的经典部署，显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）在工业应用中潜力巨大，但量子硬件的有限访问成为部署瓶颈。研究旨在通过经典替代模型解决这一问题。

Method: 提出了一种新的管道方法，极大减少了经典替代模型生成的计算冗余，资源需求仅为之前方法的一小部分。

Result: 在真实能源需求预测问题中验证了方法的有效性，计算资源需求呈线性而非指数增长，同时保持了高准确性。

Conclusion: 该方法为量子解决方案的经典部署提供了轻量级途径，加速量子技术在工业中的集成，并为实证研究中寻找量子优势提供了工具。

Abstract: Quantum machine learning (QML) presents potential for early industrial
adoption, yet limited access to quantum hardware remains a significant
bottleneck for deployment of QML solutions. This work explores the use of
classical surrogates to bypass this restriction, which is a technique that
allows to build a lightweight classical representation of a (trained) quantum
model, enabling to perform inference on entirely classical devices. We reveal
prohibiting high computational demand associated with previously proposed
methods for generating classical surrogates from quantum models, and propose an
alternative pipeline enabling generation of classical surrogates at a larger
scale than was previously possible. Previous methods required at least a
high-performance computing (HPC) system for quantum models of below industrial
scale (ca. 20 qubits), which raises questions about its practicality. We
greatly minimize the redundancies of the previous approach, utilizing only a
minute fraction of the resources previously needed. We demonstrate the
effectiveness of our method on a real-world energy demand forecasting problem,
conducting rigorous testing of performance and computation demand in both
simulations and on quantum hardware. Our results indicate that our method
achieves high accuracy on the testing dataset while its computational resource
requirements scale linearly rather than exponentially. This work presents a
lightweight approach to transform quantum solutions into classically deployable
versions, facilitating faster integration of quantum technology in industrial
settings. Furthermore, it can serve as a powerful research tool in search
practical quantum advantage in an empirical setup.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [102] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出了一种基于DistilBERT的两层异常检测方法，结合无监督和有监督学习技术，用于检测SQL数据库中的异常访问行为。


<details>
  <summary>Details</summary>
Motivation: 数据库异常访问行为日益增多，现有方法在操作级别检测异常时缺乏粒度，且难以区分类似正常行为的异常活动。

Method: 采用无监督的集成异常检测器和有监督的微调Transformer模型，结合角色标记分类，减少数据标注需求。

Result: 该方法能有效识别异常活动，尤其是内部攻击，同时减少误报。

Conclusion: 该方法为保护关键数据库系统免受复杂威胁提供了有效解决方案。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [103] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的去中心化域名系统（DDNS），以解决当前DNS基础设施的漏洞，如中毒攻击和中心化故障点。


<details>
  <summary>Details</summary>
Motivation: 当前DNS系统存在中毒攻击、审查机制和中心化故障点等问题，威胁互联网自由与安全，亟需一种更具弹性的替代方案。

Method: 设计了一种专用的工作量证明区块链，结合IPFS分布式存储，实现端到端信任签名和零信任验证。

Result: 系统实现了15秒域名记录传播时间，支持20种标准DNS记录类型，并提供了永久免费的.ddns域名。性能测试显示系统可处理高达1,111.1 tx/s（最小交易）和266.7 tx/s（常规交易）的域名操作。

Conclusion: DDNS系统展示了实际可扩展性，并能抵抗传统DNS操纵技术，为互联网域名系统提供了一种安全、去中心化的解决方案。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [104] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 本文分析了并行PoW协议的安全性问题，提出了一种投票半并行PoW协议，在多个方面优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 研究并行PoW协议的安全性和性能问题，改进现有协议的弱点。

Method: 提出投票半并行PoW协议，结合理论分析和MDP模型评估其抗攻击能力。

Result: 新协议在通信开销、吞吐量、激励兼容性等方面优于现有协议。

Conclusion: 投票半并行PoW协议是一种更安全、高效的共识机制。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


### [105] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 本文提出了一种分析区块链电子投票系统的框架，探讨了共识机制和加密协议的优化策略，并引入大语言模型（LLMs）以增强智能合约生成和验证。


<details>
  <summary>Details</summary>
Motivation: 区块链技术为电子投票系统提供了透明、去中心化和安全的潜力，但实际应用仍受限于可扩展性、计算复杂性和隐私问题。

Method: 通过比较分析区块链电子投票架构、共识机制和加密协议，提出优化策略（如混合共识、轻量级加密和去中心化身份管理），并探索LLMs在智能合约生成和异常检测中的作用。

Result: 研究为设计安全、可扩展且智能的区块链电子投票系统提供了基础，适合国家级部署。

Conclusion: 本文为构建端到端区块链电子投票原型奠定了基础，结合LLM引导的智能合约生成和验证，并通过系统框架和模拟分析支持。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [106] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 该论文研究了基于强化学习的医疗问卷系统的安全性，通过对抗攻击方法评估其脆弱性，并开发了医学验证框架确保攻击样本的临床合理性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于强化学习的医疗问卷系统在医疗场景中表现出潜力，但其安全性和鲁棒性尚未解决，因此需要评估其对抗攻击的脆弱性。

Method: 将诊断过程建模为马尔可夫决策过程（MDP），实施六种对抗攻击方法（如FGSM、PGD等），并开发包含247项医学约束的验证框架。

Result: 在NHIS数据集上，攻击成功率为33.08%至64.70%，表明系统在严格医学约束下仍存在显著漏洞。

Conclusion: 研究表明，即使输入受严格医学约束，此类系统仍易受对抗攻击影响，需进一步改进安全性。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [107] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet利用LVLMs和动态融合模块CADFM，实现了多模态假新闻检测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态假新闻对社会信任构成威胁，传统文本检测方法因图文欺骗性不足，需更有效的多模态融合方法。

Method: 提出MM-FusionNet框架，核心是CADFM模块，通过双向跨模态注意力和动态模态门控网络自适应融合图文特征。

Result: 在LMFND数据集上F1-score达0.938，优于现有方法0.5%，接近人类水平。

Conclusion: MM-FusionNet在多模态假新闻检测中表现出高效性、鲁棒性和可解释性。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [108] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出了一种基于Mamba状态空间模型和跨模态自适应融合的新型内部威胁检测框架MambaITD，解决了现有方法在时间动态特征建模、计算效率和跨模态信息孤岛方面的不足。


<details>
  <summary>Details</summary>
Motivation: 企业面临日益严重的内部威胁风险，现有检测方法因时间动态特征建模不足、计算效率低和跨模态信息孤岛问题而无法有效应对。

Method: 提出MambaITD框架，包括多源日志预处理、Mamba编码器建模长程依赖、门控特征融合机制和自适应阈值优化方法。

Result: MambaITD在建模效率和特征融合能力上显著优于传统方法，超越基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [109] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: 论文提出了一种面向企业隐私的新目标，解决了现有方法导致模型性能下降和缺乏公开数据集的问题，通过ABack机制和PriGenQA基准实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护研究主要关注用户隐私，忽视了企业数据泄露风险，尤其是在检索增强生成范式下。

Method: 提出ABack机制（基于隐藏状态模型）和PriGenQA基准数据集，并开发了自适应攻击者进行严格评估。

Result: ABack在对抗强大攻击者时，隐私效用分数提高了15%，避免了性能折衷。

Conclusion: 论文为企业隐私保护提供了有效解决方案，解决了性能和数据集的挑战。

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [110] [Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification](https://arxiv.org/abs/2508.06118)
*Daniil Vlasenko,Vadim Ushakov,Alexey Zaikin,Denis Zakharov*

Main category: q-bio.NC

TL;DR: 提出了一种基于图表示的fMRI数据分类方法，通过集成机器学习模型构建图，显著提高了脑状态分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像数据高维度和噪声导致的脑状态分类难题。

Method: 利用多个基础机器学习模型构建图，边权重反映认知状态的后验概率差异。

Result: 在七种认知任务中，分类准确率达到97.07%至99.74%，优于传统相关性图方法。

Conclusion: 集成图方法提供了更丰富的拓扑信息，增强了脑状态区分能力，且具有可扩展性。

Abstract: Understanding and classifying human cognitive brain states based on
neuroimaging data remains one of the foremost and most challenging problems in
neuroscience, owing to the high dimensionality and intrinsic noise of the
signals. In this work, we propose an ensemble-based graph representation method
of functional magnetic resonance imaging (fMRI) data for the task of binary
brain-state classification. Our method builds the graph by leveraging multiple
base machine-learning models: each edge weight reflects the difference in
posterior probabilities between two cognitive states, yielding values in the
range [-1, 1] that encode confidence in a given state. We applied this approach
to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject
Release), including working memory, gambling, motor activity, language, social
cognition, relational processing, and emotion processing. Using only the mean
incident edge weights of the graphs as features, a simple logistic-regression
classifier achieved average accuracies from 97.07% to 99.74%. We also compared
our ensemble graphs with classical correlation-based graphs in a classification
task with a graph neural network (GNN). In all experiments, the highest
classification accuracy was obtained with ensemble graphs. These results
demonstrate that ensemble graphs convey richer topological information and
enhance brain-state discrimination. Our approach preserves edge-level
interpretability of the fMRI graph representation, is adaptable to multiclass
and regression tasks, and can be extended to other neuroimaging modalities and
pathological-state classification.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [111] [Indian Legal NLP Benchmarks : A Survey](https://arxiv.org/abs/2107.06056)
*Prathamesh Kalamkar,Janani Venugopalan Ph. D.,Vivek Raghavan Ph. D*

Main category: cs.CL

TL;DR: 提出为印度法律文本创建专门的自然语言处理基准，以推动AI在法律领域的应用。


<details>
  <summary>Details</summary>
Motivation: 法律文本与普通英语文本差异显著，需针对印度法律系统创建专门的NLP基准以促进创新。

Method: 回顾现有工作并提出创建新基准的思路。

Result: 提出为印度法律文本设计挑战性基准的必要性。

Conclusion: 创建专门基准将推动AI在法律领域的应用，惠及AI社区和法律界。

Abstract: Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.

</details>


### [112] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种名为PEEK的方法，通过预训练的嵌入模型预测大语言模型（LLM）的知识，避免了传统方法的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的随机性，难以预测其掌握的知识，传统探测方法计算成本高且耗时。

Method: 利用预训练的嵌入模型（文本或图）作为LLM的代理，通过线性解码层预测LLM的输出。

Result: 在3个维基百科数据集、4种LLM和7种嵌入模型上的评估显示，嵌入模型预测LLM知识的准确率高达90%。

Conclusion: 知识适应的嵌入模型可大规模识别LLM的知识缺口，并揭示其内部归纳偏差。

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [113] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: TADrop是一种自适应稀疏化策略，通过为每个参数张量分配定制化的稀疏率，优化模型合并性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法采用统一的稀疏率，忽略了参数的结构和统计异质性，导致关键参数被误删或冗余参数被保留。

Method: TADrop根据参数张量的分布特性动态调整稀疏率，密集冗余的张量被大幅剪枝，稀疏关键的张量被保留。

Result: 实验表明，TADrop显著提升了多种任务和模型的性能，例如在ViT-B/32任务上平均提升2.0%。

Conclusion: TADrop通过自适应稀疏化有效减少参数干扰，为高性能模型合并提供了新基准。

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [114] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 论文提出了一种基于检索增强生成（RAG）的内容分类方法，通过动态检索上下文知识实现灵活、透明的分类，适用于内容审核。


<details>
  <summary>Details</summary>
Motivation: 传统分类系统需要重新训练以适应政策变化，成本高且不灵活。本文旨在通过RAG技术实现无需重新训练的分类系统。

Method: 提出Contextual Policy Engine（CPE），利用RAG将分类任务从预训练参数转向动态检索的上下文知识评估。

Result: 实验表明CPE在分类准确性、解释性和动态更新方面表现优异，无需重新训练即可调整政策。

Conclusion: RAG技术可将分类转变为更灵活、透明和适应性强的方法，适用于内容审核及其他分类问题。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [115] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文提出了一种结合Whisper ASR模型和Transformer语言模型的方法，用于德语老年人的语音意图识别，利用LLM生成合成数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有意图识别方法在德语老年人语音上的局限性，填补低资源领域的数据缺口。

Method: 结合Whisper ASR模型和Transformer语言模型，利用LeoLM、Llama3和ChatGPT生成合成数据，并进行跨数据集测试。

Result: 合成数据显著提升分类性能和鲁棒性，LeoLM在德语意图识别中表现优于ChatGPT。

Conclusion: 生成式AI可有效填补低资源领域数据缺口，LeoLM在特定领域表现更优。

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [116] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 论文提出DynamicTRF框架，通过动态选择适合的图表示形式（TRF）提升多模态模型在零样本图问答任务中的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一图表示形式（TRF）无法适应不同模型或任务的需求，导致回答不准确或冗长。

Method: 设计了一组零样本图问答专用的TRF（$F_{ZS}$），提出新指标GRE衡量性能与简洁性，并开发DynamicTRF框架动态选择最佳TRF。

Result: 在7个领域内算法图问答任务和2个领域外任务中，DynamicTRF显著提升了零样本图问答的准确性。

Conclusion: DynamicTRF通过动态TRF选择有效提升了多模态模型在图问答任务中的表现。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [117] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 论文提出Memp方法，通过提炼过去代理轨迹为细粒度指令和高级抽象，赋予代理可学习、可更新和终身的程序记忆，显著提升任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型代理的程序记忆脆弱且依赖手动设计或静态参数，需要一种可学习和动态更新的记忆机制。

Method: 提出Memp方法，将代理轨迹提炼为细粒度指令和高级抽象，并研究构建、检索和更新策略，结合动态更新机制。

Result: 在TravelPlanner和ALFWorld任务中，代理的成功率和效率随记忆库优化而提升；将记忆迁移到较弱模型也能显著提升性能。

Conclusion: Memp方法有效实现了代理程序记忆的动态学习和更新，显著提升了任务性能，并展示了记忆迁移的潜力。

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [118] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 论文提出了一种后训练方法，通过针对性微调提升LLMs在多轮交互中形成临时约定的能力，并设计了两个新基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 人类在多轮交互中能高效适应语言并形成临时约定，而现有LLMs缺乏这种能力。

Method: 通过启发式识别的约定形成示范进行针对性微调，并设计两个新基准（交互任务和文档引用任务）评估能力。

Result: 后训练的LLMs在两个评估方法中表现出显著提升的约定形成能力。

Conclusion: 该方法有效提升了LLMs在多轮交互中的适应性，为未来研究提供了新方向。

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [119] [Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows](https://arxiv.org/abs/2508.05744)
*Aizhan Akhmetzhanova,Carolina Cuesta-Lazaro,Siddharth Mishra-Sharma*

Main category: astro-ph.CO

TL;DR: 论文提出了一种结合尺度依赖神经统计量与归一化流的新框架，用于通过贝叶斯证据估计检测宇宙学模拟中的模型错误。


<details>
  <summary>Details</summary>
Motivation: 当前和未来的宇宙学调查将产生大量高维数据，需要复杂的高保真模拟来建模物理过程和系统效应。验证理论模型是否准确描述观测数据是一个关键挑战。

Method: 通过将尺度依赖的神经网络模型与归一化流结合，进行数据压缩和贝叶斯证据估计，以数据驱动的方式识别模型失效的尺度。

Result: 在三个不同子网格物理实现的CAMELS模拟套件中，该方法成功应用于物质和气体密度场。

Conclusion: 该框架为宇宙学模拟中的模型验证提供了一种有效的数据驱动方法。

Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts
of high-dimensional data, which require complex high-fidelity forward
simulations to accurately model both physical processes and systematic effects
which describe the data generation process. However, validating whether our
theoretical models accurately describe the observed datasets remains a
fundamental challenge. An additional complexity to this task comes from
choosing appropriate representations of the data which retain all the relevant
cosmological information, while reducing the dimensionality of the original
dataset. In this work we present a novel framework combining scale-dependent
neural summary statistics with normalizing flows to detect model
misspecification in cosmological simulations through Bayesian evidence
estimation. By conditioning our neural network models for data compression and
evidence estimation on the smoothing scale, we systematically identify where
theoretical models break down in a data-driven manner. We demonstrate a first
application to our approach using matter and gas density fields from three
CAMELS simulation suites with different subgrid physics implementations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 本文提出了一种结合主动推理原则与大型语言模型（LLMs）的新型框架，用于开发安全的通用人工智能（AGI）。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限，需要一种将安全性融入系统核心设计的新方法。

Method: 通过透明信念表示和分层价值对齐，将安全保证集成到系统设计中，利用自然语言作为信念表示和操作的媒介，实现多智能体自组织。

Result: 提出了一种通过自然语言分离信念与偏好、资源感知自由能最小化和模块化智能体结构确保安全性的机制。

Conclusion: 该框架为AGI开发提供了一条更安全的新路径，并以ARC基准为中心提出了验证实验的研究议程。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [121] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁重视认知想象力在人工智能中的作用，并提出语义模型作为模拟工具。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中起关键作用，但当前AI对其重视不足，导致推理能力受限。

Method: 提出语义模型，一种基于概率因果关系的数学模型，可模拟认知想象力。

Result: 语义模型能确保想象上下文的连贯性，实现透明操作。

Conclusion: 认知想象力是AI发展的下一个突破口，语义模型为其提供了有效工具。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [122] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 提出了一种在归纳逻辑编程中打破假设空间对称性的方法，显著提高了求解效率。


<details>
  <summary>Details</summary>
Motivation: 解决归纳逻辑编程中假设空间庞大且存在大量逻辑等价假设的问题。

Method: 通过打破假设空间的对称性，并在答案集编程中实现该方法。

Result: 实验表明，该方法将求解时间从超过一小时缩短至17秒。

Conclusion: 该方法有效提升了归纳逻辑编程的效率，适用于视觉推理和游戏等领域。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [123] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval开发了BET工具，通过动态对抗优化实现100%攻击成功率，并提出细粒度鲁棒性指标，揭示模型间攻击难度差异。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的鲁棒性，揭示漏洞并推动分布式鲁棒性评估。

Method: 使用BET工具进行自动化红队测试，结合动态对抗优化和细粒度指标分析。

Result: 攻击成功率达100%（37/41模型），攻击难度差异超300倍。

Conclusion: BET工具和细粒度指标为LLM鲁棒性评估提供了实用路径。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [124] [Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms](https://arxiv.org/abs/2508.06383)
*Rashid Barket,Matthew England,Jürgen Gerhard*

Main category: cs.SC

TL;DR: 论文提出了一种基于机器学习的符号不定积分算法选择方法，通过两阶段架构（方法筛选与复杂度排序）提升效率，树结构表示优于序列表示，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统计算机代数系统中符号不定积分的算法选择缺乏对问题实例的针对性，导致效率低下。

Method: 采用树基深度学习模型的两阶段架构：先筛选适用方法，再按预测输出复杂度排序；数学表达式以树结构表示。

Result: 在多样数据集上，模型在7万例测试集上达到近90%准确率，且在独立测试中优于Maple内置选择器和其他ML方法。

Conclusion: 数据表示和问题框架对符号计算的ML应用至关重要，该方法可推广至数学软件的其他优化问题。

Abstract: Symbolic indefinite integration in Computer Algebra Systems such as Maple
involves selecting the most effective algorithm from multiple available
methods. Not all methods will succeed for a given problem, and when several do,
the results, though mathematically equivalent, can differ greatly in
presentation complexity. Traditionally, this choice has been made with minimal
consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning
models within a two-stage architecture: first identifying applicable methods
for a given instance, then ranking them by predicted output complexity.
Furthermore, we find representing mathematical expressions as tree structures
significantly improves performance over sequence-based representations, and our
two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models
achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example
holdout test set. On an independent out-of-distribution benchmark from Maple's
internal test suite, our tree transformer model maintains strong
generalisation, outperforming Maple's built-in selector and prior ML
approaches.
  These results highlight the critical role of data representation and problem
framing in ML for symbolic computation, and we expect our methodology to
generalise effectively to similar optimisation problems in mathematical
software.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [125] [Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance](https://arxiv.org/abs/2508.06052)
*Haruto Nakashima,Siddhartha Ganguly,Kenji Kashima*

Main category: math.OC

TL;DR: 使用Gromov-Wasserstein度量解决数据驱动的机会约束密度控制问题，通过DC算法高效求解。


<details>
  <summary>Details</summary>
Motivation: 解决未知线性控制递归系统的数据驱动机会约束密度控制问题，利用预操作实验数据。

Method: 将最优控制问题重构为凸差规划，采用DC算法求解。

Result: 数值结果验证了方法的有效性。

Conclusion: 提出的方法能够高效解决数据驱动的密度控制问题。

Abstract: We tackle the data-driven chance-constrained density steering problem using
the Gromov-Wasserstein metric. The underlying dynamical system is an unknown
linear controlled recursion, with the assumption that sufficiently rich
input-output data from pre-operational experiments are available. The initial
state is modeled as a Gaussian mixture, while the terminal state is required to
match a specified Gaussian distribution. We reformulate the resulting optimal
control problem as a difference-of-convex program and show that it can be
efficiently and tractably solved using the DC algorithm. Numerical results
validate our approach through various data-driven schemes.

</details>


### [126] [LLM Serving Optimization with Variable Prefill and Decode Lengths](https://arxiv.org/abs/2508.06133)
*Meixuan Wang,Yinyu Ye,Zijie Zhou*

Main category: math.OC

TL;DR: 研究了如何调度具有不同预填充和解码长度的LLM请求以最小化总完成时间，提出了一种新算法并证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM请求调度中因预填充和解码长度差异导致的NP难问题，优化实际应用中的性能。

Method: 分析了FCFS和SF等常见策略的局限性，提出基于新选择度量的算法及其变体（动态规划、局部搜索、LP调度器）。

Result: 新算法具有常数竞争比，仿真显示其优于基线方法且计算高效。

Conclusion: 提出的算法有效解决了LLM请求调度问题，为实际应用提供了高效解决方案。

Abstract: We study the problem of serving LLM (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In LLM serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the KV cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
KV cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [127] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 论文提出了一种结合视觉基础模型与强化学习的新方法，显著提升了模拟环境中物体交互能力。


<details>
  <summary>Details</summary>
Motivation: 通过整合先进的视觉模型（如SAM和YOLOv5）与强化学习（PPO），旨在提升智能体在复杂环境中的感知与交互能力。

Method: 结合Segment Anything Model (SAM)和YOLOv5，使用PPO算法在AI2-THOR模拟环境中训练智能体。

Result: 实验显示，相比基线模型，平均累积奖励提升68%，物体交互成功率提高52.5%，导航效率增加33%。

Conclusion: 该方法展示了视觉基础模型与强化学习结合的潜力，为复杂机器人任务提供了新思路。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [128] [Sandwich Monotonicity and the Recognition of Weighted Graph Classes](https://arxiv.org/abs/2508.06216)
*Jesse Beisegel,Nina Chiarelli,Ekkehard Köhler,Matjaž Krnc,Martin Milanič,Nevena Pivač,Robert Scheffler,Martin Strehler*

Main category: cs.DM

TL;DR: 该论文研究了边加权图在Robinsonian矩阵和相似性理论中的作用，提出了通过水平图定义加权图类的方法，并展示了如何利用特殊的边消除顺序在线性时间内识别这些图类。


<details>
  <summary>Details</summary>
Motivation: 研究边加权图在图论中的应用，特别是通过水平图的概念，扩展了未加权图类的理论框架。

Method: 引入“度三明治单调图类”的概念，并提出线性时间识别算法，适用于所有水平图属于特定未加权图类的加权图。

Result: 证明了对于所有水平图是分裂图、阈值图或链图的加权图，可以在线性时间内识别。

Conclusion: 通过度三明治单调性条件，为加权图类的线性时间识别提供了通用框架。

Abstract: Edge-weighted graphs play an important role in the theory of Robinsonian
matrices and similarity theory, particularly via the concept of level graphs,
that is, graphs obtained from an edge-weighted graph by removing all
sufficiently light edges. This suggest a natural way of associating to any
class $\mathcal{G}$ of unweighted graphs a corresponding class of edge-weighted
graphs, namely by requiring that all level graphs belong to $\mathcal{G}$. We
show that weighted graphs for which all level graphs are split, threshold, or
chain graphs can be recognized in linear time using special edge elimination
orderings. We obtain these results by introducing the notion of degree sandwich
monotone graph classes. A graph class $\mathcal{G}$ is sandwich monotone if
every edge set which may be removed from a graph in $\mathcal{G}$ without
leaving the class also contains a single edge that can be safely removed.
Furthermore, if we require the safe edge to fulfill a certain degree property,
then $\mathcal{G}$ is called degree sandwich monotone. We present necessary and
sufficient conditions for the existence of a linear-time recognition algorithm
for any weighted graph class whose corresponding unweighted class is degree
sandwich monotone and contains all edgeless graphs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [129] [Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems](https://arxiv.org/abs/2508.05846)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.CY

TL;DR: 论文主张AI决策过程的透明度对开发可信赖且符合伦理的机器人系统至关重要，探讨了透明度的作用、挑战及实现方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术在社会中的普及，确保其伦理行为变得至关重要，透明度是实现这一目标的关键。

Method: 提出了一种框架，结合标准化指标、可解释AI技术和用户友好界面，以增强透明度。

Result: 透明度有助于提升公众信任、影响监管政策，并为未来研究指明方向。

Conclusion: 将透明度作为伦理AI系统设计的核心要素，为负责任AI和机器人技术的发展提供了方向。

Abstract: As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.

</details>


### [130] [Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks](https://arxiv.org/abs/2508.06411)
*Ze Shen Chin*

Main category: cs.CY

TL;DR: 本文提出了一种多维框架和因果路径模型，用于系统分析和缓解六种常见的人工智能灾难性风险。


<details>
  <summary>Details</summary>
Motivation: 当前关于人工智能风险的讨论缺乏全面、多维的框架和具体的因果路径，本文旨在填补这一空白。

Method: 通过七个关键维度（意图、能力、实体、极性、线性、范围和顺序）描述六种AI风险，并进行风险路径建模。

Result: 提供了系统化的风险识别方法和通用的缓解策略，同时通过路径模型支持具体场景的干预措施。

Conclusion: 该方法为管理AI灾难性风险提供了更结构化、可操作的基础。

Abstract: Although discourse around the risks of Artificial Intelligence (AI) has
grown, it often lacks a comprehensive, multidimensional framework, and concrete
causal pathways mapping hazard to harm. This paper aims to bridge this gap by
examining six commonly discussed AI catastrophic risks: CBRN, cyber offense,
sudden loss of control, gradual loss of control, environmental risk, and
geopolitical risk. First, we characterize these risks across seven key
dimensions, namely intent, competency, entity, polarity, linearity, reach, and
order. Next, we conduct risk pathway modeling by mapping step-by-step
progressions from the initial hazard to the resulting harms. The dimensional
approach supports systematic risk identification and generalizable mitigation
strategies, while risk pathway models help identify scenario-specific
interventions. Together, these methods offer a more structured and actionable
foundation for managing catastrophic AI risks across the value chain.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [131] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*François Bachoc,Nicolò Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: 论文提出了一种随机多臂老虎机模型，用于解决预算分配问题，如众包和自动竞价。算法在T步后的期望遗憾为O(K√T)，并在特定条件下改进为O(K(log T)^2)。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于众包和自动竞价中的预算分配问题，需要将固定预算分配给多个任务或拍卖。

Method: 设计了一种算法，将预算分配建模为K维概率单纯形上的随机多臂老虎机问题，每轮奖励为K个随机奖励之和。

Result: 算法在T步后的期望遗憾为O(K√T)，并在满足递减回报条件下改进为O(K(log T)^2)。

Conclusion: 论文为预算分配问题提供了高效的算法，并在理论和实际应用中展示了其优越性。

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [132] [Intuition emerges in Maximum Caliber models at criticality](https://arxiv.org/abs/2508.06477)
*Lluís Arola-Fernández*

Main category: physics.soc-ph

TL;DR: 研究发现预测模型在特定学习阶段会自发产生直觉，平衡记忆与探索。


<details>
  <summary>Details</summary>
Motivation: 探讨大型预测模型是简单复述训练数据还是能产生真正洞察，缺乏物理解释。

Method: 通过mind-tuning方法，结合最大熵原理，研究模型在迷宫随机行走中的行为。

Result: 发现模型在不同参数下表现出模仿、幻觉和直觉三种相，直觉相具有多稳态和滞后性。

Conclusion: 直觉是记忆与探索临界平衡的涌现性质，为模型行为提供了新解释。

Abstract: Whether large predictive models merely parrot their training data or produce
genuine insight lacks a physical explanation. This work reports a primitive
form of intuition that emerges as a metastable phase of learning that
critically balances next-token prediction against future path-entropy. The
intuition mechanism is discovered via mind-tuning, the minimal principle that
imposes Maximum Caliber in predictive models with a control temperature-like
parameter $\lambda$. Training on random walks in deterministic mazes reveals a
rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination
(high $\lambda$), and a fragile in-between window exhibiting strong
protocol-dependence (hysteresis) and multistability, where models spontaneously
discover novel goal-directed strategies. These results are captured by an
effective low-dimensional theory and frame intuition as an emergent property at
the critical balance between memorizing what is and wondering what could be.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: 论文提出了一种新框架，通过建模用户搜索过程中的过渡查询，结合LLMs生成多样化查询，提升电商搜索效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法将查询改写视为孤立对，未能捕捉用户行为的序列和动态变化，需改进以更好地反映用户意图演变。

Method: 从用户交互日志中挖掘结构化查询轨迹，建模意图流，并利用LLMs生成语义多样的替代查询。

Result: 实验表明，该方法在转化率和用户参与度上优于现有相关搜索模块。

Conclusion: 新框架有效捕捉用户意图演变，结合LLMs显著提升电商搜索的发现和参与效果。

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [134] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG框架结合ETL和RAG，通过多层检索、自校正框架和引用验证，实现大规模文献合成和方法学研究空白识别。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG架构的局限性，提升文献合成的自动化水平和可追溯性。

Method: 采用混合检索、自校正框架和八阶段处理流程，生成知识图谱和向量集合。

Result: 在643次测试中，语义相似度得分提高35.1%，自校正机制单次通过率68.3%，引用准确率99%。

Conclusion: HySemRAG在科学领域具有广泛应用潜力，可加速证据合成和发现。

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [135] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 该研究提出了一种基于Qwen2.5-VL-7B的微调视觉语言模型，用于将马来西亚审计财务报告中的表格高保真转换为Markdown格式，显著优于基准模型和专有模型。


<details>
  <summary>Details</summary>
Motivation: 财务文档中表格结构的准确提取和表示是文档理解的关键挑战，尤其在监管和分析场景中。

Method: 使用2,152个图像-文本对的数据集和LoRA监督微调策略，优化模型生成Markdown的能力，并通过LLM评估和Markdown TEDS指标衡量性能。

Result: 模型在标准评估中达到92.20%的准确率和96.53%的Markdown TEDS分数，显著优于基准模型和专有模型，且推理时间更短。

Conclusion: 领域特定微调是连接非结构化财务文档与下游自动化的高效方法，性能媲美更大规模的通用模型，同时计算成本更低。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [136] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 论文提出了一种名为SoftmaxLoss@K（SL@K）的新损失函数，用于优化推荐系统中的NDCG@K指标，解决了现有方法在Top-K截断和计算效率上的问题。


<details>
  <summary>Details</summary>
Motivation: NDCG@K是推荐系统评估的金标准，但其优化存在挑战，如不连续性和Top-K截断问题。现有方法要么忽略截断，要么计算成本高且不稳定。

Method: 结合分位数技术处理Top-K截断，并推导出平滑上界以优化NDCG@K，提出SL@K损失函数。

Result: 在四个真实数据集和三种推荐模型上，SL@K平均性能提升6.03%，具有理论保证、计算高效、梯度稳定和噪声鲁棒性。

Conclusion: SL@K是一种高效且稳定的损失函数，显著提升了NDCG@K的优化效果。

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [137] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: 比较了领域特定微调和基于提示的学习在BIM信息检索中的表现，提出混合方法以优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询在BIM环境中提取数据的挑战。

Method: 两阶段框架（意图识别和基于表格的问答），使用1,740条标注查询评估两种方法。

Result: 微调在意图识别中表现更好，提示学习（如GPT-4o）在问答中更强，混合方法效果最佳。

Conclusion: 混合方法为BIM系统设计提供了更平衡和鲁棒的解决方案。

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [138] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: 论文提出了一种多面预训练方案和CPU-GPU混合服务架构，解决了大型嵌入表在推荐系统中的挑战，显著提升了CTR和CVR性能。


<details>
  <summary>Details</summary>
Motivation: 大型嵌入表在现代推荐系统中至关重要，但在Pinterest广告排名模型中遇到了稀疏性、可扩展性等挑战，尤其是从头训练嵌入表效果不佳。

Method: 引入多面预训练方案，结合多种预训练算法，并设计CPU-GPU混合服务架构以突破GPU内存限制。

Result: 部署后实现了1.34%的CPC降低和2.60%的CTR提升，且端到端延迟保持稳定。

Conclusion: 多面预训练和混合架构有效提升了推荐系统性能，具有实际应用价值。

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [139] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: 论文提出了一种基于用户群体行为的隐式反馈解析方法（G-UBS），通过群体上下文指导，提升视频推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 显式用户反馈稀缺，而隐式反馈存在噪声，容易误判用户兴趣，影响推荐效果。

Method: G-UBS包含两个核心模块：用户群体管理器（UGM）和用户反馈建模器（UFM），分别负责群体聚类和群体感知的强化学习。

Result: 在IF-VR基准测试中，G-UBS显著优于主流模型，播放率提升4.0%，推理准确率提升14.9%。

Conclusion: G-UBS通过群体上下文指导，有效解析隐式反馈噪声，提升推荐系统的鲁棒性和准确性。

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [140] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: 论文提出eSASRec模型，通过结合SASRec的训练目标、LiGR Transformer层和Sampled Softmax Loss，显著提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在推荐系统中表现优异，但模块化改进的叠加效果未被系统评估。本文旨在填补这一空白。

Method: 结合SASRec的训练目标、LiGR Transformer层和Sampled Softmax Loss，提出eSASRec模型。

Result: eSASRec在学术基准测试中比最新模型（如ActionPiece）效果提升23%，在生产环境中表现优异。

Conclusion: eSASRec易于集成到现有推荐系统中，可作为简单但强大的基线模型。

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [141] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种基于用户行为信息的特征选择策略，通过混合矩阵分解和最大体积算法筛选关键特征，平衡推荐准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决冷启动问题中无关或噪声特征对预测性能的负面影响，同时减少计算资源消耗。

Method: 结合协同行为数据的相关性，使用混合矩阵分解技术增强特征表示，并通过最大体积算法对特征进行排序。

Result: 在多种数据集和混合推荐模型中表现优异，尤其在冷启动场景下，选择少量高效特征子集。

Conclusion: 该方法在严格特征缩减下仍优于现有技术，同时保持高效性。

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [142] [A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes](https://arxiv.org/abs/2508.05705)
*Valentina Roquemen-Echeverri,Taisa Kushner,Peter G. Jacobs,Clara Mosquera-Lopez*

Main category: q-bio.QM

TL;DR: 本文提出了一种基于生理约束的神经网络数字孪生模型，用于模拟1型糖尿病患者的葡萄糖动态变化，并通过真实数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在模拟1型糖尿病患者的葡萄糖动态时，常忽略关键生理特征且难以个性化。本文旨在开发一种更准确且可个性化的模拟方法。

Method: 结合群体水平的神经网络状态空间模型和个体特异性数据，构建数字孪生模型，并通过形式化验证确保其符合已知生理动态。

Result: 在394个数字孪生模型中，模拟数据与真实数据在临床相关指标上表现一致（如时间在范围内、低于范围和高于范围）。

Conclusion: 该方法能够整合未建模因素（如睡眠和活动），支持个性化治疗测试和胰岛素优化，为数据驱动的临床决策提供了新工具。

Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is
critical for developing personalized treatments and supporting data-driven
clinical decisions. Existing models often miss key physiological aspects and
are difficult to individualize. Here, we introduce physiologically-constrained
neural network (NN) digital twins to simulate glucose dynamics in T1D. To
ensure interpretability and physiological consistency, we first build a
population-level NN state-space model aligned with a set of ordinary
differential equations (ODEs) describing glucose regulation. This model is
formally verified to conform to known T1D dynamics. Digital twins are then
created by augmenting the population model with individual-specific models,
which include personal data, such as glucose management and contextual
information, capturing both inter- and intra-individual variability. We
validate our approach using real-world data from the T1D Exercise Initiative
study. Two weeks of data per participant were split into 5-hour sequences and
simulated glucose profiles were compared to observed ones. Clinically relevant
outcomes were used to assess similarity via paired equivalence t-tests with
predefined clinical equivalence margins. Across 394 digital twins, glucose
outcomes were equivalent between simulated and observed data: time in range
(70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real;
P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022);
and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001).
Our framework can incorporate unmodeled factors like sleep and activity while
preserving key dynamics. This approach enables personalized in silico testing
of treatments, supports insulin optimization, and integrates physics-based and
data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [143] [IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering](https://arxiv.org/abs/2508.06126)
*Jixuan Yin,Zhihao Yao,Wenshuai Huo,Xinmiao Yu,Xiaocheng Feng,Bo Li*

Main category: stat.ME

TL;DR: IOCC是一种新的少样本对比学习方法，通过交互增强最优传输和中心感知对比学习模块，优化文本表示，提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决短文本表示表达能力有限导致聚类中心无法准确捕捉语义的问题。

Method: 提出IOCC方法，包含交互增强最优传输（IEOT）和中心感知对比学习（CACL）模块，生成伪标签并优化表示。

Result: 在八个基准数据集上表现优异，Biomedical数据集上提升7.34%，聚类稳定性和效率也显著提高。

Conclusion: IOCC通过减少聚类中心与语义中心的差距，显著提升了聚类性能。

Abstract: In clustering tasks, it is essential to structure the feature space into
clear, well-separated distributions. However, because short text
representations have limited expressiveness, conventional methods struggle to
identify cluster centers that truly capture each category's underlying
semantics, causing the representations to be optimized in suboptimal
directions. To address this issue, we propose IOCC, a novel few-shot
contrastive learning method that achieves alignment between the cluster centers
and the semantic centers. IOCC consists of two key modules:
Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive
Learning (CACL). Specifically, IEOT incorporates semantic interactions between
individual samples into the conventional optimal transport problem, and
generate pseudo-labels. Based on these pseudo-labels, we aggregate
high-confidence samples to construct pseudo-centers that approximate the
semantic centers. Next, CACL optimizes text representations toward their
corresponding pseudo-centers. As training progresses, the collaboration between
the two modules gradually reduces the gap between cluster centers and semantic
centers. Therefore, the model will learn a high-quality distribution, improving
clustering performance. Extensive experiments on eight benchmark datasets show
that IOCC outperforms previous methods, achieving up to 7.34\% improvement on
challenging Biomedical dataset and also excelling in clustering stability and
efficiency. The code is available at:
https://anonymous.4open.science/r/IOCC-C438.

</details>
