<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 52]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 8]
- [math.NA](#math.NA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [physics.data-an](#physics.data-an) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: 论文提出了一种基于分层强化学习（HRL）的框架CogniQ-H，用于自动化数据准备任务，通过结合大语言模型（LLM）和强化学习，显著提升了管道质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 数据准备是机器学习生命周期中基础但复杂的环节，现有强化学习方法效率低下，未能捕捉问题的层次结构。

Method: CogniQ-H采用软分层范式，将动作选择建模为贝叶斯推断问题，结合LLM的战略先验、学习排序模型的细粒度评分和强化学习的长期价值估计。

Result: 在18个数据集上的实验表明，CogniQ-H在管道质量上提升13.9%，收敛速度加快2.8倍。

Conclusion: CogniQ-H通过软分层范式有效解决了数据准备中的层次结构问题，为自动化数据准备提供了新思路。

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [2] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: LLaPipe是一个结合大型语言模型（LLMs）的智能策略顾问框架，用于优化数据预处理管道的探索效率，显著提升管道质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的数据预处理方法在庞大的预处理管道空间中探索效率低下，LLaPipe通过LLMs的语义理解能力解决这一瓶颈。

Method: LLaPipe引入三个创新：LLM策略顾问、经验蒸馏机制和自适应顾问触发策略，结合LLMs的语义分析和历史经验指导探索。

Result: 在18个数据集上的实验表明，LLaPipe管道质量提升22.4%，收敛速度快2.3倍，且计算效率高（LLM仅用于19.0%的探索步骤）。

Conclusion: LLaPipe通过智能LLM干预显著提升了数据预处理管道的探索效率和质量，同时保持计算效率。

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


### [3] [Efficient and Scalable Self-Healing Databases Using Meta-Learning and Dependency-Driven Recovery](https://arxiv.org/abs/2507.13757)
*Joydeep Chandra,Prabal Manhas*

Main category: cs.DB

TL;DR: 该研究提出了一种基于元学习和强化学习的自愈数据库框架，旨在解决动态工作负载环境中的实时适应性和最小化再训练问题。


<details>
  <summary>Details</summary>
Motivation: 动态工作负载环境中的数据库需要实时适应性和高效的自愈能力，传统方法难以满足这些需求。

Method: 结合模型无关元学习（MAML）和强化学习，利用多目标优化、图神经网络（GNNs）、合成任务增强和自监督学习，并整合可解释AI技术和联邦元学习。

Result: 框架显著提升了适应性、效率和可靠性。

Conclusion: 该研究为数据库管理和自愈系统提供了创新解决方案。

Abstract: This study explored the development of a novel self-healing framework for
databases using meta-learning and reinforcement learning techniques. The
primary objective was to address the challenges of real-time adaptability and
minimal retraining in dynamic workload environments. The proposed approach
integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning to
enable anomaly detection and corrective actions that adapted swiftly to
evolving database conditions. Multi-objective optimization was employed to
balance performance, resource utilization, and cost efficiency during the
healing process. Graph Neural Networks (GNNs) were incorporated to model
interdependencies within database components, ensuring holistic recovery
strategies. Data efficiency was enhanced through synthetic task augmentation
and self-supervised learning, enabling effective training in sparse data
regimes. To promote trust and transparency, explainable AI techniques were
integrated to provide interpretable insights into anomaly detection and healing
actions. Federated meta-learning further enabled privacy-preserving
adaptability in distributed database environments. The framework demonstrated
significant improvements in adaptability, efficiency, and reliability,
contributing to advancements in database management and self-healing systems.

</details>


### [4] [Towards Next Generation Data Engineering Pipelines](https://arxiv.org/abs/2507.13892)
*Kevin M. Kramer,Valerie Restat,Sebastian Strasser,Uta Störl,Meike Klettke*

Main category: cs.DB

TL;DR: 本文提出了下一代数据工程管道的三个层次：优化、自感知和自适应，以解决数据质量和反应性问题。


<details>
  <summary>Details</summary>
Motivation: 当前数据工程管道在数据质量和变化反应性方面存在不足，可能导致管道崩溃或输出不良结果。

Method: 提出三个层次的解决方案：优化管道组成和参数化、实现自感知监控、开发自适应反应机制。

Result: 通过优化、自感知和自适应方法，可以提升数据质量和管道的反应能力。

Conclusion: 下一代数据工程管道应具备优化、自感知和自适应能力，以应对数据质量和变化挑战。

Abstract: Data engineering pipelines are a widespread way to provide high-quality data
for all kinds of data science applications. However, numerous challenges still
remain in the composition and operation of such pipelines. Data engineering
pipelines do not always deliver high-quality data. By default, they are also
not reactive to changes. When new data is coming in which deviates from prior
data, the pipeline could crash or output undesired results. We therefore
envision three levels of next generation data engineering pipelines: optimized
data pipelines, self-aware data pipelines, and self-adapting data pipelines.
Pipeline optimization addresses the composition of operators and their
parametrization in order to achieve the highest possible data quality.
Self-aware data engineering pipelines enable a continuous monitoring of its
current state, notifying data engineers on significant changes. Self-adapting
data engineering pipelines are then even able to automatically react to those
changes. We propose approaches to achieve each of these levels.

</details>


### [5] [Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries](https://arxiv.org/abs/2507.14101)
*Diego Figueira,Cibele Freire*

Main category: cs.DB

TL;DR: 论文提出了一种名为'project-connex'树宽的新度量，用于衡量带有'group-by'投影的半环聚合查询的可处理性，统一了多种查询类型的复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对不同类型的查询（如半环聚合查询、枚举算法和计数查询）提出了各自的结构条件，缺乏统一的度量标准。本文旨在填补这一空白。

Method: 通过扩展'free-connex'分解的概念，定义了'project-connex'树分解，并提供了一种统一的算法框架。

Result: 证明了'project-connex'树宽能够解释现有的可处理性结果，并恢复了与计数查询和枚举查询相关的已知结论。

Conclusion: 'project-connex'树分解为多种查询类型提供了统一的复杂性分析工具，并通过经典树分解算法实现计算。

Abstract: We introduce 'project-connex' tree-width as a measure of tractability for
counting and aggregate conjunctive queries over semirings with 'group-by'
projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure
allows to obtain comparable complexity bounds to the ones obtained by previous
structural conditions tailored for efficient evaluation of semiring aggregate
queries, enumeration algorithms of conjunctive queries, and tractability of
counting answers to conjunctive queries.
  Project-connex tree decompositions are defined as the natural extension of
the known notion of 'free-connex' decompositions. They allow for a unified,
simple and intuitive algorithmic manipulation for evaluation of aggregate
queries and explain some existing tractability results on conjunctive query
enumeration, counting conjunctive query evaluation, and evaluation of semiring
aggregate queries. Using this measure we also recover results relating
tractable classes of counting conjunctive queries and bounded free-connex
tree-width, or the constant-time delay enumeration of semiring aggregate
queries over bounded project-connex classes. We further show that
project-connex tree decompositions can be obtained via algorithms for computing
classical tree decompositions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate系统通过多播抽象实现DNN训练中的每迭代检查点，避免传统检查点方法的训练暂停，显著提升检查点频率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法需要在训练暂停时复制模型状态，存在检查点频率与故障成本之间的权衡。Checkmate旨在消除这一权衡。

Method: 利用数据并行训练中的梯度信息，通过多播抽象将梯度同时传递到基于CPU的影子集群，影子集群通过应用梯度维护检查点。

Result: Checkmate实现每迭代检查点，训练吞吐量与无检查点基线相当，检查点频率提升5至34.5倍，重复工作减少80%至97.1%。

Conclusion: Checkmate显著优于现有检查点系统，提供更高的检查点频率和吞吐量。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [7] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 本文提出了一种名为FAR的三阶段算法，用于在NVIDIA MIG技术下优化多任务调度的完成时间。通过动态重新配置GPU资源，FAR显著提升了任务调度的效率。


<details>
  <summary>Details</summary>
Motivation: NVIDIA MIG技术允许将物理GPU划分为多个逻辑实例，但现有调度方法未充分利用其动态重新配置的潜力。本文旨在通过优化任务调度，最小化多任务执行的完成时间。

Method: FAR算法分为三个阶段：1）基于经典任务可塑性方法；2）结合最长处理时间优先和列表调度，并引入针对MIG约束的新启发式方法；3）通过任务移动和交换进行局部搜索。

Result: 实验结果显示，FAR在不考虑重新配置成本时，近似因子为7/4（A30模型）和2（A100/H100模型）；考虑重新配置成本后，完成时间与最优解的比值不超过1.22x（基准测试）和1.10x（合成输入）。

Conclusion: FAR算法显著优于现有技术，展示了MIG技术的研究潜力，并为未来工作提供了有用的指标和评估方法。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [8] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow是一种新型分布式强化学习框架，通过多控制器范式消除中心节点，实现近线性扩展和高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模强化学习中负载不平衡导致的扩展瓶颈问题。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，实现完全分布式。

Result: 实验显示DistFlow具有优异的线性扩展性，吞吐量比现有技术提升7倍。

Conclusion: DistFlow突破了强化学习的扩展限制，为高效算法实验提供了灵活性。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [9] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 该论文综述了基于脉冲神经网络（SNNs）的边缘智能（EdgeSNNs），探讨其在资源受限设备上实现低功耗、事件驱动计算的潜力，并系统分类了其基础、应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在边缘计算中面临延迟、带宽和隐私问题，而SNNs通过模拟生物神经元动态提供了一种低功耗的替代方案。

Method: 论文提出了EdgeSNNs的系统分类，包括神经元模型、学习算法和硬件平台，并深入讨论了轻量级推理、资源感知训练和隐私保护等实际问题。

Result: 论文总结了EdgeSNNs的当前进展和局限性，并提出了双轨基准测试策略以支持公平比较和硬件优化。

Conclusion: 该研究旨在填补脑启发学习与边缘部署之间的空白，为研究人员提供参考，并推动未来研究方向。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [10] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright是一个验证框架，用于证明分布式系统的正确性和活性，特别是针对恶意参与者的情况。它通过模块化分解和密码学签名支持，成功验证了PBFT的部分实现。


<details>
  <summary>Details</summary>
Motivation: 在去中心化系统中（如PBFT），确保活性至关重要，但传统方法难以验证活性，尤其是面对恶意参与者时。

Method: Shipwright引入三种技术：支持恶意参与者的形式化推理、模块化分解系统和证明、以及密码学签名的合理推理。

Result: 成功实现并验证了PBFT中单个日志条目协议的初始原型，并将其转化为可执行的Go代码。实验展示了其常见情况和故障场景下的活性和运行。

Conclusion: Shipwright为分布式系统的活性和正确性验证提供了可行框架，尤其在恶意参与者存在的情况下。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication](https://arxiv.org/abs/2507.13470)
*Michael Elkin,Chhaya Trehan*

Main category: cs.DS

TL;DR: 该论文提出了一种改进的集中式和并行算法，用于解决有向图中从多个源点到所有顶点的可达性问题，并在特定参数范围内优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决有向图中多源点可达性问题的现有算法在时间和空间复杂度上仍有优化空间，尤其是在特定参数范围内。

Method: 利用Kogan和Parter的快捷构造技术，开发了一种集中式算法，并改进了Cohen的并行算法，适用于具有小递归分割器或树宽度的图。

Result: 新算法在特定参数范围内（如σ在~σ到0.53之间）优于现有方法，且并行算法在更广泛的参数范围内降低了工作复杂度。

Conclusion: 论文提出的算法在多源点可达性问题中显著提升了性能，并扩展了适用范围，包括近似距离计算。

Abstract: Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \subseteq V$
of $|S| = n^{\sigma}$ (for some $0 \le \sigma \le 1$) designated sources, the
$S \times V$ reachability problem is to compute the sets $\mathcal V_s$ of
vertices reachable from $s$, for every $s \in S$. Naive centralized algorithms
run BFS/DFS from each source in $O(m \cdot n^{\sigma})$ time or compute $G$'s
transitive closure in $\hat O(n^{\omega})$ time, where $\omega \le
2.371552\ldots$ is the matrix multiplication exponent. Thus, the best known
bound is $\hat O(n^{\min \{ 2 + \sigma, \omega\}})$. Leveraging shortcut
constructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a
centralized algorithm with running time $\hat O(n^{1 + \frac{2}{3}
\omega(\sigma)})$, where $\omega(\sigma)$ is the rectangular matrix
multiplication exponent. Using current estimates on $\omega(\sigma)$, our
exponent improves upon $\min \{2 + \sigma, \omega \}$ for $\tilde \sigma \leq
\sigma \leq 0.53$, where $1/3 < \tilde \sigma < 0.3336$ is a universal
constant.
  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel
algorithms for $S \times V$ reachability on graphs admitting balanced recursive
separators of size $n^{\rho}$ for $\rho < 1$, requiring polylogarithmic time
and work $n^{\max \{\omega \rho, 2\rho + \sigma \} + o(1)}$. We significantly
improve, extend, and generalize Cohen's result. First, our parallel algorithm
for graphs with small recursive separators has lower work complexity than
Cohen's in boraod paramater ranges. Second, we generalize our algorithm to
graphs of treewidth at most $n^{\rho}$ ($\rho < 1$) and provide a centralized
algorithm that outperforms existing bounds for $S \times V$ reachability on
such graphs. We also do this for some other graph familes with small
separators. Finally, we extend these results to $(1 + \epsilon)$-approximate
distance computation.

</details>


### [12] [Strassen $2\times2$ Matrix Multiplication from a 3-dimensional Volume Form](https://arxiv.org/abs/2507.13510)
*Benoit Jacob*

Main category: cs.DS

TL;DR: Strassen的2×2矩阵乘法算法源于2×2矩阵除以单位矩阵倍数的3维商空间上的体积形式。


<details>
  <summary>Details</summary>
Motivation: 探索矩阵乘法的更高效算法，通过几何视角理解其数学基础。

Method: 利用3维商空间上的体积形式推导出Strassen的2×2矩阵乘法算法。

Result: 提出了Strassen算法，展示了其几何背景。

Conclusion: Strassen算法不仅是一种计算工具，还具有深刻的几何意义。

Abstract: The Strassen $2\times2$ matrix multiplication algorithm arises from the
volume form on the 3-dimensional quotient space of the $2\times 2$ matrices by
the multiples of identity.

</details>


### [13] [Combinatorics of Palindromes](https://arxiv.org/abs/2507.13671)
*Michael Itzhaki*

Main category: cs.DS

TL;DR: 研究了Manacher数组的结构和重建复杂性，提出了组合下界、图论框架，并分析了重建算法的性能。


<details>
  <summary>Details</summary>
Motivation: 深入理解Manacher数组的组合性质，并探索其在字符串重建中的应用。

Method: 1. 建立组合下界；2. 引入图论框架；3. 分析重建算法。

Result: 证明了组合下界，提出了图论框架，并验证了算法的全局最优性和适应性。

Conclusion: 研究推进了对Manacher数组的理解，并为结构约束下的字符串重建提供了新方向。

Abstract: We investigate the structure and reconstruction complexity of Manacher
arrays. First, we establish a combinatorial lower bound, proving that the
number of rooted tandem repeat trees with $n+1$ genes exceeds the number of
distinct Manacher arrays of length $n$. Second, we introduce a graph-theoretic
framework that associates a graph to each Manacher array, where every proper
vertex coloring yields a string consistent with the array. Finally, we analyze
a reconstruction algorithm by I et al. (SPIRE 2010), showing that it
simultaneously achieves a globally minimal alphabet size, uses at most
$\log_2(n{-}1) + 2$ distinct symbols, and can be adapted to produce
reconstructions over arbitrary alphabets when possible. Our results also
resolve an open problem posed by the original authors. Together, these findings
advance the combinatorial understanding of Manacher arrays and open new
directions for string reconstruction under structural constraints.

</details>


### [14] [Tight Bounds for Answering Adaptively Chosen Concentrated Queries](https://arxiv.org/abs/2507.13700)
*Emma Rapoport,Edith Cohen,Uri Stemmer*

Main category: cs.DS

TL;DR: 本文研究了自适应数据分析中数据集样本相关性的问题，指出在相关样本下，即使是非自适应设置也可能难以处理。Bassily和Freund提出的集中查询框架在非自适应设置中简化了问题，但在自适应设置中仍具挑战性。本文证明了在当前框架下，查询数量的差距是固有的，并提出了一种简化算法。


<details>
  <summary>Details</summary>
Motivation: 研究在数据集样本相关的情况下，自适应数据分析的可行性和效率，特别是集中查询框架的局限性。

Method: 通过理论证明和算法设计，分析了集中查询框架在自适应设置中的固有局限性，并提出了一种简化算法。

Result: 证明了在当前框架下，查询数量的差距是固有的，且提出的简化算法与不可能性结果相匹配。

Conclusion: 集中查询框架在自适应设置中存在固有局限性，未来研究需探索更有效的框架或方法。

Abstract: Most work on adaptive data analysis assumes that samples in the dataset are
independent. When correlations are allowed, even the non-adaptive setting can
become intractable, unless some structural constraints are imposed. To address
this, Bassily and Freund [2016] introduced the elegant framework of
concentrated queries, which requires the analyst to restrict itself to queries
that are concentrated around their expected value. While this assumption makes
the problem trivial in the non-adaptive setting, in the adaptive setting it
remains quite challenging. In fact, all known algorithms in this framework
support significantly fewer queries than in the independent case: At most
$O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the
independent setting.
  In this work, we prove that this utility gap is inherent under the current
formulation of the concentrated queries framework, assuming some natural
conditions on the algorithm. Additionally, we present a simplified version of
the best-known algorithms that match our impossibility result.

</details>


### [15] [Improved girth approximation in weighted undirected graphs](https://arxiv.org/abs/2507.13869)
*Avi Kadria,Liam Roditty,Aaron Sidford,Virginia Vassilevska Williams,Uri Zwick*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $G = (V,E,\ell)$ be a $n$-node $m$-edge weighted undirected graph, where
$\ell: E \rightarrow (0,\infty)$ is a real \emph{length} function defined on
its edges, and let $g$ denote the girth of $G$, i.e., the length of its
shortest cycle. We present an algorithm that, for any input, integer $k \geq
1$, in $O(kn^{1+1/k}\log{n} + m(k+\log{n}))$ expected time finds a cycle of
length at most $\frac{4k}{3}g$. This algorithm nearly matches a
$O(n^{1+1/k}\log{n})$-time algorithm of \cite{KadriaRSWZ22} which applied to
unweighted graphs of girth $3$. For weighted graphs, this result also improves
upon the previous state-of-the-art algorithm that in $O((n^{1+1/k}\log n+m)\log
(nM))$ time, where $\ell: E \rightarrow [1, M]$ is an integral length function,
finds a cycle of length at most $2kg$~\cite{KadriaRSWZ22}. For $k=1$ this
result improves upon the result of Roditty and Tov~\cite{RodittyT13}.

</details>


### [16] [Quantum Pattern Matching with Wildcards](https://arxiv.org/abs/2507.13885)
*Masoud Seddighin,Saeed Seddighin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pattern matching is one of the fundamental problems in Computer Science. Both
the classic version of the problem as well as the more sophisticated version
where wildcards can also appear in the input can be solved in almost linear
time $\tilde O(n)$ using the KMP algorithm and Fast Fourier Transform,
respectively. In 2000, Ramesh and Vinay~\cite{ramesh2003string} give a quantum
algorithm that solves classic pattern matching in sublinear time and asked
whether the wildcard problem can also be solved in sublinear time? In this
work, we give a quantum algorithm for pattern matching with wildcards that runs
in time $\tilde O(\sqrt{n}\sqrt{k})$ when the number of wildcards is bounded by
$k$ for $k \geq \sqrt{n}$. This leads to an algorithm that runs in sublinear
time as long as the number of wildcards is sublinear.

</details>


### [17] [Optimal antimatroid sorting](https://arxiv.org/abs/2507.13994)
*Benjamin Aram Berendsohn*

Main category: cs.DS

TL;DR: 论文研究了受限排序问题，提出了一种适用于更广泛类别的拓扑堆排序算法，解决了多种受限排序问题。


<details>
  <summary>Details</summary>
Motivation: 研究受限排序问题，探索在给定提示集T的情况下如何高效找到总顺序。

Method: 推广拓扑堆排序算法，适用于由反拟阵定义的受限排序问题。

Result: 提出了适用于多种受限排序问题的最优算法，包括单调优先公式、弦图的完美消除顺序和连通有根图的顶点搜索顺序。

Conclusion: 推广的拓扑堆排序算法在多种受限排序问题中表现最优，扩展了其应用范围。

Abstract: The classical comparison-based sorting problem asks us to find the underlying
total order of a given set of elements, where we can only access the elements
via comparisons. In this paper, we study a restricted version, where, as a
hint, a set $T$ of possible total orders is given, usually in some compressed
form.
  Recently, an algorithm called topological heapsort with optimal running time
was found for the case where $T$ is the set of topological orderings of a given
directed acyclic graph, or, equivalently, $T$ is the set of linear extensions
of a given partial order [Haeupler et al. 2024]. We show that a simple
generalization of topological heapsort is applicable to a much broader class of
restricted sorting problems, where $T$ corresponds to a given antimatroid.
  As a consequence, we obtain optimal algorithms for the following restricted
sorting problems, where the allowed total orders are restricted by: a given set
of monotone precedence formulas; the perfect elimination orders of a given
chordal graph; or the possible vertex search orders of a given connected rooted
graph.

</details>


### [18] [Sparse Navigable Graphs for Nearest Neighbor Search: Algorithms and Hardness](https://arxiv.org/abs/2507.14060)
*Sanjeev Khanna,Ashwin Padaki,Erik Waingarten*

Main category: cs.DS

TL;DR: 论文研究了稀疏α-导航图的近似算法和计算障碍，提出了与经典集合覆盖问题的等价性，并开发了快速近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏α-导航图的构建问题，以支持基于图的最近邻搜索的进展。

Method: 通过等价性分析，提出近似算法，包括基于快速矩阵乘法的双标准算法。

Result: 证明了最优解的稀疏性下限，并提出了高效的近似算法。

Conclusion: 在稀疏性优化方面取得了理论上的突破，为实际应用提供了高效的算法支持。

Abstract: We initiate the study of approximation algorithms and computational barriers
for constructing sparse $\alpha$-navigable graphs [IX23, DGM+24], a core
primitive underlying recent advances in graph-based nearest neighbor search.
Given an $n$-point dataset $P$ with an associated metric $\mathsf{d}$ and a
parameter $\alpha \geq 1$, the goal is to efficiently build the sparsest graph
$G=(P, E)$ that is $\alpha$-navigable: for every distinct $s, t \in P$, there
exists an edge $(s, u) \in E$ with $\mathsf{d}(u, t) < \mathsf{d}(s,
t)/\alpha$. We consider two natural sparsity objectives: minimizing the maximum
out-degree and minimizing the total size.
  We first show a strong negative result: the slow-preprocessing version of
DiskANN (analyzed in [IX23] for low-doubling metrics) can yield solutions whose
sparsity is $\widetilde{\Omega}(n)$ times larger than optimal, even on
Euclidean instances. We then show a tight approximation-preserving equivalence
between the Sparsest Navigable Graph problem and the classic Set Cover problem,
obtaining an $O(n^3)$-time $(\ln n + 1)$-approximation algorithm, as well as
establishing NP-hardness of achieving an $o(\ln n)$-approximation. Building on
this equivalence, we develop faster $O(\ln n)$-approximation algorithms. The
first runs in $\widetilde{O}(n \cdot \mathrm{OPT})$ time and is thus much
faster when the optimal solution is sparse. The second, based on fast matrix
multiplication, is a bicriteria algorithm that computes an $O(\ln
n)$-approximation to the sparsest $2\alpha$-navigable graph, running in
$\widetilde{O}(n^{\omega})$ time.
  Finally, we complement our upper bounds with a query complexity lower bound,
showing that any $o(n)$-approximation requires examining $\Omega(n^2)$
distances. This result shows that in the regime where $\mathrm{OPT} =
\widetilde{O}(n)$, our $\widetilde{O}(n \cdot \mathrm{OPT})$-time algorithm is
essentially best possible.

</details>


### [19] [An Efficient Massively Parallel Constant-Factor Approximation Algorithm for the $k$-Means Problem](https://arxiv.org/abs/2507.14089)
*Vincent Cohen-Addad,Fabian Kuhn,Zahra Parsaeian*

Main category: cs.DS

TL;DR: 本文提出了一种高效的并行近似算法，用于解决$k$-means问题，在MPC模型中实现了$o(\log n)$轮次的常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决$k$-means问题的高效并行计算需求，尤其是在MPC模型中的快速近似算法。

Method: 基于Jain和Vazirani的框架，通过设施位置问题的常数因子近似（满足LMP性质）转化为$k$-means近似。

Result: 算法在$O(\log\log n \cdot \log\log\log n)$轮次内完成，每台机器使用$O(n^\sigma)$内存，全局内存略超线性。

Conclusion: 首次在MPC模型中实现了$k$-means问题的常数因子近似，且轮次少于$O(\log n)$。

Abstract: In this paper, we present an efficient massively parallel approximation
algorithm for the $k$-means problem. Specifically, we provide an MPC algorithm
that computes a constant-factor approximation to an arbitrary $k$-means
instance in $O(\log\log n \cdot \log\log\log n)$ rounds. The algorithm uses
$O(n^\sigma)$ bits of memory per machine, where $\sigma > 0$ is a constant that
can be made arbitrarily small. The global memory usage is
$O(n^{1+\varepsilon})$ bits for an arbitrarily small constant $\varepsilon >
0$, and is thus only slightly superlinear. Recently, Czumaj, Gao, Jiang,
Krauthgamer, and Vesel\'{y} showed that a constant-factor bicriteria
approximation can be computed in $O(1)$ rounds in the MPC model. However, our
algorithm is the first constant-factor approximation for the general $k$-means
problem that runs in $o(\log n)$ rounds in the MPC model.
  Our approach builds upon the foundational framework of Jain and Vazirani. The
core component of our algorithm is a constant-factor approximation for the
related facility location problem. While such an approximation was already
achieved in constant time in the work of Czumaj et al.\ mentioned above, our
version additionally satisfies the so-called Lagrangian Multiplier Preserving
(LMP) property. This property enables the transformation of a facility location
approximation into a comparably good $k$-means approximation.

</details>


### [20] [Weighted Matching in a Poly-Streaming Model](https://arxiv.org/abs/2507.14114)
*Ahammed Ullah,S. M. Ferdous,Alex Pothen*

Main category: cs.DS

TL;DR: 论文提出了一种多流并行计算模型（poly-streaming），并设计了一种单遍算法来近似解决最大权重匹配问题（MWM），在共享内存并行环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的流式计算模型在处理大规模数据时效率有限，因此需要一种更高效的并行计算模型来提升处理能力。

Method: 提出多流并行模型（poly-streaming），设计单遍算法近似MWM，支持共享内存并行和分层架构。

Result: 算法在共享内存系统中表现出色，处理万亿级边图时显著提升速度和内存效率，匹配权重远超理论保证。

Conclusion: 多流并行模型和算法在大规模数据处理中具有高效性和可扩展性，适用于复杂架构。

Abstract: We introduce the poly-streaming model, a generalization of streaming models
of computation in which $k$ processors process $k$ data streams containing a
total of $N$ items. The algorithm is allowed $O\left(f(k)\cdot M_1\right)$
space, where $M_1$ is either $o\left(N\right)$ or the space bound for a
sequential streaming algorithm. Processors may communicate as needed.
Algorithms are assessed by the number of passes, per-item processing time,
total runtime, space usage, communication cost, and solution quality.
  We design a single-pass algorithm in this model for approximating the maximum
weight matching (MWM) problem. Given $k$ edge streams and a parameter
$\varepsilon > 0$, the algorithm computes a
$\left(2+\epsilon\right)$-approximate MWM. We analyze its performance in a
shared-memory parallel setting: for any constant $\varepsilon > 0$, it runs in
time $\widetilde{O}\left(L_{\max}+n\right)$, where $n$ is the number of
vertices and $L_{\max}$ is the maximum stream length. It supports
$O\left(1\right)$ per-edge processing time using $\widetilde{O}\left(k\cdot
n\right)$ space. We further generalize the design to hierarchical
architectures, in which $k$ processors are partitioned into $r$ groups, each
with its own shared local memory. The total intergroup communication is
$\widetilde{O}\left(r \cdot n\right)$ bits, while all other performance
guarantees are preserved.
  We evaluate the algorithm on a shared-memory system using graphs with
trillions of edges. It achieves substantial speedups as $k$ increases and
produces matchings with weights significantly exceeding the theoretical
guarantee. On our largest test graph, it reduces runtime by nearly two orders
of magnitude and memory usage by five orders of magnitude compared to an
offline algorithm.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence](https://arxiv.org/abs/2507.13481)
*Arthur Bueno,Bruno Cafeo,Maria Cagnin,Awdren Fontão*

Main category: cs.SE

TL;DR: 研究探讨了开源生态系统中代码样本的技术异味（如大类、模块化差）与社区异味（如单一贡献者、沟通碎片化）的共现与演化关系，发现社区问题常预示或加剧技术退化。


<details>
  <summary>Details</summary>
Motivation: 代码样本在开源生态中至关重要，但其管理松散，易受技术和社会问题影响，而两者关系尚未充分研究。

Method: 采用多声文献综述方法，分析30篇学术论文和17篇实践导向资料（2013-2024），通过主题合成识别异味动态的社会技术模式。

Result: 识别出9种模式，显示社区异味常先于或强化技术退化，如“无线电静默”和集中所有权与持续结构问题相关。

Conclusion: 开源生态中，社区问题不仅与技术退化相关，还可能是其信号，需针对共享教学工件设计轻量级治理机制。

Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving
as lightweight artifacts that support knowledge transfer, onboarding, and
framework adoption. Despite their instructional relevance, these samples are
often governed informally, with minimal review and unclear ownership, which
increases their exposure to socio-technical degradation. In this context, the
co-occurrence and longitudinal interplay of code smells (e.g., large classes,
poor modularity) and community smells (e.g., lone contributors, fragmented
communication) become particularly critical. While each type of smell has been
studied in isolation, little is known about how community-level dysfunctions
anticipate or exacerbate technical anomalies in code samples over time. This
study investigates how code and community smells emerge, co-occur, and evolve
within code samples maintained in OSSECOs. A Multivocal Literature Review
protocol was applied, encompassing 30 peer-reviewed papers and 17
practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to
identify recurring socio-technical patterns related to smell dynamics. Nine
patterns were identified, showing that community smells often precede or
reinforce technical degradation in code samples. Symptoms such as "radio
silence" and centralized ownership were frequently associated with persistent
structural anomalies. Additionally, limited onboarding, the absence of
continuous refactoring, and informal collaboration emerged as recurring
conditions for smell accumulation. Conclusion: In OSSECOs, particularly within
code samples, community-level dysfunctions not only correlate with but often
signal maintainability decay. These findings underscore the need for
socio-technical quality indicators and lightweight governance mechanisms
tailored to shared instructional artifacts.

</details>


### [22] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta开发了MetaMateCR，利用AI辅助修复代码审查评论，通过微调Llama模型并在生产环境中测试，结果显示其性能优于GPT-4o，且通过安全试验优化了用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决Meta每周数万条代码审查评论的处理问题，提高效率和准确性。

Method: 使用64k数据点微调Llama模型，进行离线测试、安全试验和生产实验。

Result: LargeLSFT模型离线准确率68%，优于GPT-4o 9个百分点；生产环境中ActionableToApplied率提升9.2个百分点。

Conclusion: MetaMateCR成功展示了AI辅助修复的潜力，并通过安全试验确保不降低工程师效率。

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>


### [23] [Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software](https://arxiv.org/abs/2507.13553)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 研究探讨了开源软件中功能请求的模糊性和不完整性，以及开发者澄清问题的动态过程，发现开发者更关注项目目标而非文本澄清。


<details>
  <summary>Details</summary>
Motivation: 功能请求常因自然语言表达模糊或不完整导致误解和实施错误，但开发者澄清问题的实际行为尚不明确。

Method: 研究分析了开源软件平台上的功能请求及其澄清过程，关注开发者如何处理模糊或不完整的请求。

Result: 功能请求普遍存在模糊性和不完整性，但开发者较少进行明确澄清，更注重与项目目标的一致性。

Conclusion: 研究揭示了澄清问题的动态模式，为改善用户与开发者协作提供了实践建议。

Abstract: As user demands evolve, effectively incorporating feature requests is crucial
for maintaining software relevance and user satisfaction. Feature requests,
typically expressed in natural language, often suffer from ambiguity or
incomplete information due to communication gaps or the requester's limited
technical expertise. These issues can lead to misinterpretation, faulty
implementation, and reduced software quality. While seeking clarification from
requesters is a common strategy to mitigate these risks, little is known about
how developers engage in this clarification process in practice-how they
formulate clarifying questions, seek technical or contextual details, align on
goals and use cases, or decide to close requests without attempting
clarification. This study investigates how feature requests are prone to NL
defects (i.e. ambiguous or incomplete) and the conversational dynamics of
clarification in open-source software (OSS) development, aiming to understand
how developers handle ambiguous or incomplete feature requests. Our findings
suggest that feature requests published on the OSS platforms do possess
ambiguity and incompleteness, and in some cases, both. We also find that
explicit clarification for the resolution of these defects is uncommon;
developers usually focus on aligning with project goals rather than resolving
unclear text. When clarification occurs, it emphasizes understanding user
intent/goal and feasibility, rather than technical details. By characterizing
the dynamics of clarification in open-source issue trackers, this work
identifies patterns that can improve user-developer collaboration and inform
best practices for handling feature requests effectively.

</details>


### [24] [Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software](https://arxiv.org/abs/2507.13555)
*Pragyan K C,Rambod Ghandiparsi,Thomas Herron,John Heaps,Mitra Bokaei Hosseini*

Main category: cs.SE

TL;DR: 论文提出了一种利用大型语言模型（LLMs）自动检测和优化自然语言（NL）特征请求中的缺陷的方法，以提高其对开发者的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着软件应用的广泛使用，快速变化的市场需求导致软件需求不断演变，但这些需求常因自然语言描述的模糊性和不完整性而难以解读。传统验证方法在去中心化环境（如开源软件）中不切实际。

Method: 利用LLMs自动识别模糊和不完整的请求，并生成澄清问题（CQs）。方法在真实开源项目中进行评估，并与人工标注对比。

Result: 方法在检测和优化NL缺陷方面表现出色，并通过开发者访谈验证了其实际应用价值。

Conclusion: LLMs能有效提升特征请求的质量，减少下游软件工程任务的负担。

Abstract: The growing popularity and widespread use of software applications (apps)
across various domains have driven rapid industry growth. Along with this
growth, fast-paced market changes have led to constantly evolving software
requirements. Such requirements are often grounded in feature requests and
enhancement suggestions, typically provided by users in natural language (NL).
However, these requests often suffer from defects such as ambiguity and
incompleteness, making them challenging to interpret. Traditional validation
methods (e.g., interviews and workshops) help clarify such defects but are
impractical in decentralized environments like open-source software (OSS),
where change requests originate from diverse users on platforms like GitHub.
This paper proposes a novel approach leveraging Large Language Models (LLMs) to
detect and refine NL defects in feature requests. Our approach automates the
identification of ambiguous and incomplete requests and generates clarification
questions (CQs) to enhance their usefulness for developers. To evaluate its
effectiveness, we apply our method to real-world OSS feature requests and
compare its performance against human annotations. In addition, we conduct
interviews with GitHub developers to gain deeper insights into their
perceptions of NL defects, the strategies they use to address these defects,
and the impact of defects on downstream software engineering (SE) tasks.

</details>


### [25] [Testing Autonomous Driving Systems -- What Really Matters and What Doesn't](https://arxiv.org/abs/2507.13661)
*Changwen Li,Joseph Sifakis,Rongjie Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文探讨了自动驾驶系统（ADS）测试方法的有效性和有效性，指出当前方法存在不足，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统的测试方法缺乏统一标准，无法准确评估其重要性及贡献，亟需改进。

Method: 提出一个框架，用于比较现有测试方法的内在有效性和有效性，并分析其依赖的设计原则。

Result: 研究发现多数测试方法未能满足有效性和有效性要求，且测试结果受自动驾驶系统设计影响显著。

Conclusion: 当前技术下无法为自动驾驶系统提供足够强的保障，建议开发时注重合理性和确定性。

Abstract: Despite extensive research, the testing of autonomous driving systems (ADS)
landscape remains fragmented, and there is currently no basis for an informed
technical assessment of the importance and contribution of the current state of
the art. This paper attempts to address this problem by exploring two
complementary aspects.
  First, it proposes a framework for comparing existing test methods in terms
of their intrinsic effectiveness and validity. It shows that many methods do
not meet both of these requirements. Either because they are based on criteria
that do not allow for rapid, inexpensive, and comprehensive detection of
failures, or because the degree of validity of the properties tested cannot be
accurately estimated. In particular, it is shown that most critical test
methods do not take into account the nominal operational capabilities of
autopilots and generate scenarios that are impossible for the tested vehicles
to handle, resulting in unjustified rejections.
  Secondly, the paper shows that test effectiveness and validity are highly
dependent on how autopilots are designed: how they choose between different
control policies to perform maneuvers, as well as on the reproducibility of the
results. In fact, most test methods take for granted two principles underlying
traditional methods, but do not generally apply to ADS. We maintain that the
absence of rationality and determinacy significantly impairs the effectiveness
and validity of test methods, and provide test results on eight open
autopilots, in which most do not satisfy these properties, thereby illustrating
this fact.
  We conclude that under the current state of the art, it is impossible to
obtain strong enough guarantees for essential autopilot properties and
recommend that autopilots be developed with a view to both rationality and
determinacy.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [26] [Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica](https://arxiv.org/abs/2507.13476)
*Jaber Daneshamooz,Jessica Nguyen,William Chen,Sanjay Chandrasekaran,Satyandra Guthula,Ankit Gupta,Arpit Gupta,Walter Willinger*

Main category: cs.NI

TL;DR: NetReplica通过生成具有协议动态真实性和网络条件可控性的训练数据集，解决了机器学习模型在网络中的领域适应问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在不同生产环境中的领域适应问题，提升模型在多样化网络条件下的泛化能力。

Method: NetReplica将网络建模为具有特定属性的瓶颈链路集合，利用生产网络痕迹实现真实性，并通过精细控制链路属性实现可控性。

Result: 评估显示，NetReplica生成的数据不仅匹配现有数据特征，还能补充Puffer数据中缺失或不足的样本。使用NetReplica增强数据集训练的模型在挑战性网络条件下，传输时间预测误差降低高达47%。

Conclusion: NetReplica为解决领域适应问题提供了重要进展，显著提升了基于机器学习的网络系统的有效性。

Abstract: Machine learning models in networking suffer from the domain adaptation
problem; models trained in one domain often fail when deployed in different
production environments. This paper presents the design and implementation of
NetReplica, a system that addresses this challenge by generating training
datasets with two critical properties: realism in protocol dynamics and
controllability of network conditions. NetReplica models networks as
collections of bottleneck links with specific attributes, achieves realism by
leveraging production network traces, and enables controllability through fine
grained control knobs for each link attribute. Our evaluation using Puffer
demonstrates that NetReplica not only matches existing data characteristics but
generates realistic samples that are underrepresented in or absent from Puffer
data. Models trained on NetReplica augmented datasets show substantially
improved generalizability, reducing transmission time prediction error by up to
47% for challenging network conditions compared to models trained solely on
Puffer data. This work represents a significant step toward solving the domain
adaptation problem that has limited the effectiveness of ML based networking
systems.

</details>


### [27] [CARTS: Cooperative and Adaptive Resource Triggering and Stitching for 5G ISAC](https://arxiv.org/abs/2507.13676)
*Cheng Jiang,Yihe Yan,Yanxiang Wang,Jiawei Hu,Chun Tung Chou,Wen Hu*

Main category: cs.NI

TL;DR: CARTS是一种自适应5G上行链路感知方案，通过融合DMRS和SRS的CSI流，提升CSI更新频率和感知机会。


<details>
  <summary>Details</summary>
Motivation: 现代5G网络中，上行链路CSI来自DMRS和SRS，但基站将其视为独立信息流，限制了CSI的准确性和感知机会。

Method: CARTS提出了一种新颖的信道拼接与补偿方法，整合异步CSI估计，并设计了实时SRS触发算法。

Result: CARTS显著提升了可扩展性，支持用户数量翻倍，同时保持低信道估计误差和高跟踪精度。

Conclusion: CARTS通过结合DMRS和SRS，无需额外资源即可提升ISAC的CSI可用性。

Abstract: This paper presents CARTS, an adaptive 5G uplink sensing scheme designed to
provide Integrated Sensing and Communication (ISAC) services. The performance
of both communication and sensing fundamentally depends on the availability of
accurate and up-to-date channel state information (CSI). In modern 5G networks,
uplink CSI is derived from two reference signals: the demodulation reference
signal (DMRS) and the sounding reference signal (SRS). However, current base
station implementations treat these CSI measurements as separate information
streams. The key innovation of CARTS is to fuse these two CSI streams, thereby
increasing the frequency of CSI updates and extending sensing opportunities to
more users. CARTS addresses two key challenges: (i) a novel channel stitching
and compensation method that integrates asynchronous CSI estimates from DMRS
and SRS, despite their different time and frequency allocations, and (ii) a
real-time SRS triggering algorithm that complements the inherently
uncontrollable DMRS schedule, ensuring sufficient and non-redundant sensing
opportunities for all users. Our trace-driven evaluation shows that CARTS
significantly improves scalability, achieving a channel estimation error (NMSE)
of 0.167 and UE tracking accuracy of 85 cm while supporting twice the number of
users as a periodic SRS-only baseline with similar performance. By
opportunistically combining DMRS and SRS, CARTS therefore provides a practical,
standard-compliant solution to improve CSI availability for ISAC without
requiring additional radio resources.

</details>


### [28] [ATRO: A Fast Solver-Free Algorithm for Topology and Routing Optimization of Reconfigurable Datacenter Networks](https://arxiv.org/abs/2507.13717)
*Yingming Mao,Qiaozhu Zhai,Zhen Yao,Xia Zhu,Ximeng Liu,Xinchi Han*

Main category: cs.NI

TL;DR: 提出了一种名为ATRO的无求解器框架，用于可重构数据中心网络的拓扑和路由优化，通过交替优化步骤显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 可重构数据中心网络的规模和复杂性增加，需要更高效和可扩展的算法来优化拓扑和路由。现有方法难以平衡解决方案质量和运行效率。

Method: ATRO框架通过交替进行拓扑优化（TO）和路由优化（RO），利用单调性保证性能提升，并使用加速二分搜索方法（ABSM）高效解决TO子问题。

Result: ATRO在一跳场景中达到全局最优，在多跳场景中显著优于基线方法，具有较高的可扩展性和鲁棒性。

Conclusion: ATRO为可重构数据中心网络提供了一种高效、可扩展的优化方法，显著提升了性能和运行效率。

Abstract: The growing scale and complexity of reconfigurable data center networks
(DCNs) demand more scalable and efficient algorithms for computing logical
topologies and routing. Reconfigurable DCNs typically operate in two modes:
one-hop configurations that require frequent topology optimization (TO), and
multi-hop scenarios that involve joint topology and routing optimization (TRO).
In both cases, the combinatorial nature of topology decisions makes it
difficult for existing methods to balance solution quality and runtime
efficiency. To address this, we introduce Alternating Topology and Routing
Optimization (ATRO), a solver-free framework that alternates between TO and
routing optimization (RO). This decomposition exploits two key insights: first,
each alternating update step monotonically reduces maximum link utilization
(MLU), ensuring consistent performance improvement across iterations; second,
the TO subproblem, equivalent to one-hop optimization, exhibits a monotonic
structure that enables optimal solutions via an efficient Accelerated Binary
Search Method (ABSM). To preserve the solver-free design, RO is solved using
existing Traffic Engineering accelerators. ATRO attains the global optimum in
one-hop scenarios and significantly outperforms baselines in multi-hop settings
in terms of both runtime and solution quality. Evaluations confirm its
scalability and robustness across diverse DCNs.

</details>


### [29] [On the Trade-Off Between Sum-Rate and Energy Efficiency through the Convergence of HAPS and Active RIS Technologies](https://arxiv.org/abs/2507.13889)
*Bilal Karaman,Ilhan Basturk,Ferdi Kara,Metin Ozturk,Sezai Taskin,Halil Yanikomeroglu*

Main category: cs.NI

TL;DR: 研究将主动可重构智能表面（RIS）中继与高空平台站（HAPS）结合，提升下一代无线系统中非地面网络（NTN）性能。主动RIS因其信号放大能力优于被动RIS，尤其在长距离HAPS链路中。通过联合优化功率分配和RIS单元分配，实现地面用户设备（UEs）的速率最大化。仿真显示主动RIS显著提升服务质量（QoS），且子连接架构在能效上表现更优。


<details>
  <summary>Details</summary>
Motivation: 长距离HAPS链路存在严重路径损耗和双衰落问题，被动RIS架构表现不佳，需探索主动RIS以提升性能。

Method: 提出联合优化功率分配和RIS单元分配的速率最大化问题，并研究多种子连接主动RIS架构以降低功耗和硬件复杂度。

Result: 主动RIS配置显著优于被动RIS，尤其在QoS方面；完全连接架构吞吐量最高，但子连接架构在能效上更优。

Conclusion: 主动RIS支持的HAPS系统有望满足未来超蜂窝覆盖和绿色网络的需求。

Abstract: This paper investigates the integration of active reconfigurable intelligent
surfaces (RIS) relay with high-altitude platform stations (HAPS) to enhance
non-terrestrial network (NTN) performance in next-generation wireless systems.
While prior studies focused on passive RIS architectures, the severe path loss
and double fading in long-distance HAPS links make active RIS a more suitable
alternative due to its inherent signal amplification capabilities. We formulate
a sum-rate maximization problem to jointly optimize power allocation and RIS
element assignment for ground user equipments (UEs) supported by a HAPS-based
active RIS-assisted communication system. To reduce power consumption and
hardware complexity, several sub-connected active RIS architectures are also
explored. Simulation results reveal that active RIS configurations
significantly outperform passive RIS in terms of quality of service (QoS).
Moreover, although fully-connected architectures achieve the highest
throughput, sub-connected schemes demonstrate superior energy efficiency under
practical power constraints. These findings highlight the potential of active
RIS-enabled HAPS systems to meet the growing demands of beyond-cellular
coverage and green networking.

</details>


### [30] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 论文提出了一种高可靠性、可扩展的管道方法，用于检测由大型语言模型（LLM）主导的网站内容，解决了现有检测器在复杂网页内容上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成内容（LLM-dominant）的普及，其不可靠性和伦理问题日益突出，但现有检测器无法有效识别复杂网页内容，因此需要开发更可靠的检测方法。

Method: 通过分类多个散文式页面的LLM文本检测结果，而非单独提取文本，构建了一个高准确率的网站分类管道，并使用两个总计120个网站的真实数据集进行训练和评估。

Result: 在真实数据集中实现了100%的准确率，并在搜索引擎和Common Crawl存档中检测到大量LLM主导的网站，发现这些网站在搜索结果中排名较高且数量增长迅速。

Conclusion: LLM主导的网站对用户和网络生态系统可能产生负面影响，需要进一步关注和研究。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: 论文从物理角度分析Transformer架构，提出基于开放量子系统的物理模型。


<details>
  <summary>Details</summary>
Motivation: 现有理论对Transformer的工作原理缺乏物理层面的理解。

Method: 从现代芯片的物理视角，在Fock空间构建Hilbert空间中token的物理模型。

Result: 提出了支持Transformer架构的开放量子系统物理模型。

Conclusion: 物理模型为Transformer架构提供了理论基础。

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [32] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 该论文提出了一种多元对齐方法，通过引入DIVE数据集和实证研究，改进文本到图像（T2I）模型以更好地反映多样的人类价值观。


<details>
  <summary>Details</summary>
Motivation: 解决当前T2I模型未能涵盖多样人类经验的问题，推动AI系统向多元价值观对齐。

Method: 1. 创建DIVE数据集，用于多元对齐评估；2. 通过实证研究验证人口统计学作为多元观点的代理；3. 探讨T2I模型对齐的实践意义。

Result: DIVE数据集成功捕捉了多样化的安全感知，实证研究揭示了与传统评估不同的危害感知差异。

Conclusion: 研究为构建更公平和对齐的T2I系统提供了基础工具。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [33] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: 论文探讨了在机器学习中使用CDF归一化的优势，通过KANs展示了其优于传统归一化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习中常用的归一化方法（如均值标准差或固定范围缩放）在金融领域的copula理论中并不常见，而CDF归一化能减少过拟合并提供更简单的表示。

Method: 通过将数据转换为CDF(x)实现归一化，应用于Kolmogorov-Arnold Networks (KANs)，并与传统方法（如Legendre-KAN）进行对比。

Result: CDF归一化显著提升了KANs的预测性能，同时神经元权重可作为混合矩，支持局部联合分布建模和概率分布传播。

Conclusion: CDF归一化在机器学习中具有潜力，能改善模型性能并提供更灵活的分布建模能力。

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [34] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出了一种名为选择性嵌入的新数据加载策略，通过交替加载多源数据的短片段来提升深度学习模型的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习对输入数据敏感，性能在非平稳条件下和多域数据中下降，传统数据加载策略泛化能力有限或计算成本高。

Method: 选择性嵌入策略，模仿人类信息处理方式，交替加载多源数据的短片段。

Result: 在六个时域数据集上验证，该方法显著提高分类准确性并减少训练时间。

Conclusion: 该方法适用于多源数据的复杂系统，为医疗、重工业等领域提供高效解决方案。

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [35] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab是一个结合LLM代码生成和多个AutoML工具的多代理系统，用于表格数据任务，提高了灵活性和鲁棒性，并在Kaggle任务中表现优于现有开源方案。


<details>
  <summary>Details</summary>
Motivation: 当前AutoML在处理复杂任务时依赖特定工具，效率受限。

Method: 结合LLM代码生成和多个AutoML工具，设计多代理系统LightAutoDS-Tab。

Result: 在多个Kaggle数据科学任务中表现优于现有开源方案。

Conclusion: LightAutoDS-Tab通过多代理系统提升了AutoML的灵活性和鲁棒性，代码已开源。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [36] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models是一种新型生成流模型，通过引入可学习的Gauge Field在流ODE中，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统流模型在某些任务中表现有限，需要更高效的生成模型。

Method: 在流ODE中引入可学习的Gauge Field，构建数学框架并进行实验验证。

Result: 在Gaussian Mixture Models上表现优于传统流模型，且可能适用于更广泛的生成任务。

Conclusion: Gauge Flow Models具有潜力成为更高效的生成模型。

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [37] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin通过服务器端数字孪生预测客户端梯度更新，动态跳过通信轮次，减少带宽消耗，同时提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信开销是主要瓶颈，尤其是移动和物联网设备带宽受限的场景。

Method: 使用LSTM实现的数字孪生预测客户端梯度更新的幅度和不确定性，动态决定是否跳过通信轮次。

Result: 在非IID数据分布下，FedSkipTwin减少12-15.5%通信量，模型精度提升0.5个百分点。

Conclusion: 预测引导的跳过策略在带宽受限的边缘环境中是实用且有效的。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [38] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 论文提出了一种层次化的多保真度数据驱动学习方法，能够量化认知不确定性并区分数据噪声（随机不确定性），适用于从单保真度确定性神经网络到多保真度贝叶斯循环神经网络的多种学习场景。


<details>
  <summary>Details</summary>
Motivation: 解决多保真度数据驱动学习中的认知不确定性和数据噪声问题，提升模型在科学和工程领域中的适用性。

Method: 提出了一种层次化的多保真度学习方法，结合贝叶斯循环神经网络，量化模型误差并发现噪声分布。

Result: 方法能够准确预测响应、量化模型误差，并在存在噪声时发现噪声分布。

Conclusion: 该方法为涉及不确定性的设计和分析提供了广泛的应用潜力。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [39] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: 本文提出了一种新的聚类算法Soft-ECM，用于处理复杂数据（如混合数据和时间序列），通过半度量方法解决了传统算法在非欧几里得空间中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于置信函数的聚类算法无法处理复杂数据（如混合数据或时间序列），因为这些数据通常不在欧几里得空间中表示。

Method: 提出Soft-ECM算法，仅需半度量即可定位不精确簇的质心，适用于非欧几里得空间。

Result: 实验表明，Soft-ECM在数值数据上与模糊聚类方法效果相当，并能有效处理混合数据和时间序列。

Conclusion: Soft-ECM扩展了基于置信函数的聚类算法的适用范围，为复杂数据提供了一种有效的解决方案。

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [40] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出了一种多层DNN调度框架，扩展了OctopuScheduler，支持从PyTorch模型到SpiNNaker2芯片的端到端推理流程。


<details>
  <summary>Details</summary>
Motivation: 为在神经形态平台SpiNNaker2上实现大型复杂DNN（如Transformer）的边缘执行提供解决方案。

Method: 结合量化和降阶步骤的前端，扩展OctopuScheduler为多层DNN调度框架。

Result: 实现了在SpiNNaker2芯片上高效运行复杂DNN的能力。

Conclusion: 该框架为神经形态硬件上的DNN推理提供了高效且可扩展的解决方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [41] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的可解释框架，用于实时评估空中交通管制员（ATCO）的任务需求，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性指标无法捕捉操作中的细微驱动因素，需要更精准的任务需求评估方法。

Method: 采用注意力机制的GNN模型，通过静态交通场景中的交互预测ATCO即将发布的指令数量，并通过系统消融飞机来获得任务需求评分。

Result: 模型显著优于启发式方法，是更可靠的复杂性评估工具，并能将任务需求归因于特定飞机。

Conclusion: 该框架为ATCO培训和空域重新设计提供了新的复杂性分析工具。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [42] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出了一种跨模态自监督预训练方法，用于从大规模未标记的IMU-视频数据中学习表示，提高了在分布外IMU数据集上的HAR任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于IMU的HAR方法依赖特定应用标签、泛化能力不足的问题。

Method: 采用跨模态自监督预训练方法，利用IMU-视频数据学习通用表示。

Result: 在零样本和少样本评估中，该方法优于当前最先进的IMU-视频预训练和仅IMU预训练方法。

Conclusion: 跨模态预训练是学习动态数据模态（如IMU信号）通用表示的有效工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [43] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: 论文提出基于模型的智能体作为模型无关强化学习的替代方案，以提高样本效率、安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 模型无关强化学习（RL）在复杂系统中表现优异，但存在样本效率低、学习不安全及可解释性差的问题。

Method: 引入基于模型的智能体，利用系统动力学、成本和约束的可适应模型进行安全策略学习，并结合模型无关RL弥补模型不匹配的缺陷。

Result: 基于模型的智能体（如模型预测控制）在样本效率、安全性和可解释性方面表现更优。

Conclusion: 结合基于模型和模型无关RL的方法，有望实现样本高效、安全且可解释的决策智能体。

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [44] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: Kaggle竞赛“Fake or Real”旨在解决AI安全威胁，要求参与者区分正常与恶意修改的LLM输出。


<details>
  <summary>Details</summary>
Motivation: 基于欧洲航天局资助的项目，识别了数据毒化和对大型语言模型的过度依赖两大威胁，需研究新方法应对。

Method: 参与者需开发新技术或调整现有方法，以区分正常与恶意修改的LLM输出。

Result: 竞赛旨在填补该问题研究空白，推动相关技术进步。

Conclusion: 该竞赛为AI安全领域提供了新的研究方向和实践平台。

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [45] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 本文通过引入双收敛的统一框架，首次严格解释了大型语言模型（LLMs）如何通过上下文学习（ICL）从输入序列中学习新行为，而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何通过ICL从输入序列中学习新行为，并解释其背后的机制。

Method: 提出双收敛框架，分析隐藏表示在上下文和层间的收敛过程，证明其对平滑（低频）表示的隐式偏好。

Result: 理论解释了ICL的多个经验观察，如表示几何的全局结构与局部扭曲，以及总能量衰减但不消失的现象。

Conclusion: 研究结果为ICL的机制提供了新见解，并为其理论研究奠定了基础，有望扩展到更一般的数据分布和设置。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [46] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: 提出了一种名为Acoustic Index的新型AI衍生超声心动图参数，用于量化心脏功能障碍，结合了EDMD和混合神经网络，在独立测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统超声心动图参数（如EF和GLS）在早期检测心脏功能障碍时存在局限性，需要一种可重复、可解释且操作者独立的参数。

Method: 结合EDMD和混合神经网络，提取超声心动图序列的时空动态特征，并通过注意力机制和流形学习融合临床数据，生成0到1的连续评分。

Result: 在736名患者的队列中，Acoustic Index的AUC为0.89，交叉验证显示敏感性和特异性均超过0.8。

Conclusion: Acoustic Index是一种物理驱动的可解释AI生物标志物，有望成为早期检测和监测心脏功能的工具，未来需进一步验证和扩展。

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [47] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 提出两种指标（谱可预测性评分和最大Lyapunov指数）来量化时间序列的预测性，避免传统模型评估的事后性。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估指标在模型开发后才评估预测性能，而这两种指标能在模型开发前评估数据的固有预测性，帮助优化资源分配。

Method: 使用谱可预测性评分评估时间序列频率成分的强度和规律性，Lyapunov指数量化系统混沌性。在合成和M5竞赛数据集上验证。

Result: 两种指标能准确反映时间序列的固有预测性，并与实际模型预测性能强相关。

Conclusion: 提前了解时间序列的预测性有助于优化资源分配，为预测性差的产品制定替代策略。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [48] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: SELF-Transformer通过内部迭代更新注意力权重，提升编码器Transformer的表达能力，无需依赖自回归。


<details>
  <summary>Details</summary>
Motivation: 提升编码器Transformer的表达能力，避免依赖外部化的中间状态（如自回归）。

Method: 引入SELF-Transformer，通过迭代更新注意力权重至固定点，动态调整计算复杂度。

Result: 在编码器基准测试中准确率提升高达20%，且不增加参数量。

Conclusion: SELF-Transformer通过内部迭代实现了接近自回归的表达能力，同时保持了编码器的简洁性。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [49] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: 苹果推出了两种多语言、多模态的基础语言模型，分别用于设备端和服务器端，支持多种语言和图像理解，性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为苹果设备和服务提供智能功能，同时兼顾性能、成本、隐私和负责任的人工智能开发。

Method: 设备端模型采用KV缓存共享和2位量化感知训练；服务器模型采用并行轨道混合专家（PT-MoE）架构，结合稀疏计算和全局-局部注意力。

Result: 模型在公开基准和人工评估中表现优异，支持多语言和工具调用。

Conclusion: 苹果通过技术创新和负责任的人工智能实践，提供了高性能且隐私保护的智能模型。

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [50] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS框架通过生成用户偏好摘要，实现个性化LLM响应，优于传统RLHF和上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF无法区分用户差异，需开发个性化响应方法。

Method: 提出PLUS框架，通过文本摘要学习用户偏好，并动态更新奖励模型。

Result: PLUS能有效捕捉用户偏好，适应新用户和多样话题，并可迁移至GPT-4。

Conclusion: PLUS提供透明、可控制的个性化LLM对齐方案。

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [51] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出两种新的离线策略评估（OPE）方法DiPS和DPR，针对匹配市场的双向性和数据稀疏性问题，结合DM、IPS和DR方法，通过引入中间信号优化偏差-方差平衡，并在理论和实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 匹配市场（如求职和约会平台）的推荐系统需要频繁更新策略，但A/B测试成本高且不实用。传统OPE方法因数据稀疏性和双向交互问题效果不佳，因此需要专门设计的评估方法。

Method: 提出DiPS和DPR两种新OPE方法，结合DM、IPS和DR估计器，并利用初始参与信号等中间标签优化偏差-方差控制。

Result: 理论分析表明新方法在偏差和方差上优于传统方法，实验验证其在合成数据和真实求职平台数据上的优越性。

Conclusion: DiPS和DPR为匹配市场的离线策略评估和学习提供了高效解决方案，显著优于现有方法。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [52] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 论文提出了一种结合GCN、AE和Graph Transformer的Tri-GFN框架，通过三重学习机制和特征融合策略提升图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决GCN在大规模复杂图数据中过平滑和过压缩的问题，以及Graph Transformer在异构图数据中的性能限制。

Method: 提出Tri-GFN框架，整合GCN、AE和Graph Transformer，通过三重学习机制和特征融合策略增强全局与局部信息。

Result: 在ACM、Reuters和USPS数据集上分别提升0.87%、14.14%和7.58%的准确率。

Conclusion: Tri-GFN在异构图数据上表现优异，适用于新闻分类和主题检索等领域。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [53] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: 本文综述了基于Transformer的语言模型在蛋白质序列分析与设计中的应用，总结了相关研究的优缺点，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer模型在生物信息学中的应用，特别是蛋白质序列分析与设计的最新进展。

Method: 综述并分析了大量关于Transformer模型在基因本体、蛋白质功能与结构识别、蛋白质生成及结合等方面的研究。

Result: 总结了现有研究的优缺点，为读者提供了全面的视角。

Conclusion: 指出了现有研究的不足，并提出了未来发展的潜在方向，为相关领域的研究者提供了参考。

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [54] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 论文提出GRU-KAN和LSTM-KAN两种新架构，用于提前预测贷款违约事件，显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有贷款违约预测模型在早期预测精度不足和依赖特定时间框架的问题，以帮助金融机构提前采取预防措施。

Method: 结合Kolmogorov-Arnold Networks (KAN)与GRU和LSTM网络，提出GRU-KAN和LSTM-KAN模型。

Result: 新模型在提前三个月和八个月的预测中分别达到92%和88%的准确率，显著优于基线模型。

Conclusion: GRU-KAN和LSTM-KAN在贷款违约早期预测中表现出色，具有实际应用潜力。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [55] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs在组合优化问题中表现良好，但随着问题图密度增加，性能下降。研究发现训练动态中存在相变，并提出基于模糊逻辑和二值化神经网络的改进方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究PI-GNNs在组合优化问题中的性能随图密度变化的规律，并解决其在高密度问题中的性能下降问题。

Method: 分析了PI-GNNs的训练动态，发现相变现象，提出基于模糊逻辑和二值化神经网络的改进策略。

Result: 改进方法显著提升了PI-GNNs在高密度组合优化问题中的性能。

Conclusion: 通过模糊逻辑和二值化神经网络的改进，PI-GNNs在高密度问题中的性能得到有效提升。

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [56] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: 多目标贝叶斯优化（MOBO）在分子设计中优于标量化方法，尤其在低数据量下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索MOBO在分子设计中的实际优势，尤其是与标量化方法相比。

Method: 使用基于Pareto的MOBO策略（EHVI）与固定权重的标量化方法（EI）进行对比实验。

Result: EHVI在Pareto前沿覆盖、收敛速度和化学多样性上均优于EI。

Conclusion: Pareto感知的获取策略在分子优化中具有实际优势，特别是在低数据量和复杂权衡情况下。

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [57] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: 提出了一种自适应空间标记化（AST）方法，通过网格划分和注意力机制，高效模拟可变形体交互，解决了现有方法的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的方法在模拟可变形体交互时存在扩展性问题，动态创建全局边计算量大。

Method: 将模拟空间划分为网格单元，将非结构化网格映射到结构化网格上，利用交叉注意力模块生成紧凑的固定长度嵌入作为标记，并通过自注意力模块预测下一状态。

Result: 实验表明，该方法在大规模网格（超过10万个节点）上显著优于现有方法，且计算高效。

Conclusion: AST方法结合标记化效率和注意力机制的表达能力，为可变形体交互模拟提供了准确且可扩展的解决方案。

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [58] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究系统比较了传统机器学习和深度学习模型在帕金森病分类中的表现，发现CNN-LSTM模型表现最佳，同时XGBoost等传统方法也有竞争力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的早期诊断对临床干预至关重要，但开发可靠的自动化诊断模型仍具挑战性。EEG作为一种非侵入性方法，为PD相关神经变化检测提供了可能。

Method: 研究采用统一的七步预处理流程，并应用一致的主体交叉验证和评估标准，比较了传统ML和DL模型在公开数据集上的表现。

Result: 结果显示，CNN-LSTM模型在性能上优于其他深度学习架构，而XGBoost等传统分类器也表现出色。

Conclusion: 研究为未来开发更复杂或专用架构提供了基准框架，强调了科学严谨性和可重复性在EEG神经诊断领域的重要性。

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [59] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的EEG信号分类方法，用于检测欺骗行为，测试准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 欺骗检测在安全、心理学和法医学等领域具有重要意义，但传统方法存在局限性。

Method: 使用双向门控循环单元（Bi-GRU）神经网络对Bag-of-Lies数据集中的EEG信号进行二分类。

Result: 模型测试准确率为97%，且在精确率、召回率和F1分数上表现优异。

Conclusion: 双向时序建模在EEG欺骗检测中效果显著，具有实时应用潜力，未来可探索更先进的神经架构。

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [60] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: 本文提出了一种混合特征融合框架，用于构建自主货船（ACS）故障模式的图结构数据集，显著提高了文献检索效率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决自主货船中组件故障引发的级联反应和应急决策中的不确定性。

Method: 采用改进的布谷鸟搜索算法（HN-CSA）提高检索效率，构建分层特征融合框架，结合Word2Vec、BERT-KPCA和Sentence-BERT处理特征和语义关联。

Result: 数据集覆盖12个系统、1262种故障模式和6150条传播路径，GATE-GNN模型分类准确率为0.735，特征区分度高（轮廓系数0.641）。

Conclusion: 为自主货船的故障分析、风险评估和智能决策系统提供了可靠支持。

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [61] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 研究探讨了对抗训练在音频分类中对泛化性能和对抗鲁棒性的影响，发现输出空间攻击策略显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 对抗训练在音频分类中对数据分布变化的泛化能力研究不足，需填补这一空白。

Method: 采用两种对抗训练策略（输出空间攻击和嵌入空间攻击），评估其在两种模型架构（ConvNeXt和AudioProtoPNet）上的效果。

Result: 输出空间攻击策略平均提升干净测试数据性能10.5%，同时增强模型对抗鲁棒性。

Conclusion: 对抗训练在音频分类中能有效应对数据分布变化和对抗攻击，具有广泛应用潜力。

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [62] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG框架通过采样机制解决图神经网络中的类别和大小不平衡问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中的类别和大小不平衡会偏置学习过程，现有方法通常只解决一种不平衡或计算成本高。

Method: SamGoG通过重要性采样构建多个图之图（GoG），并顺序训练，结合可学习相似性和自适应节点度提升边同质性。

Result: 在基准数据集上，SamGoG实现了15.66%的准确率提升和6.7倍的训练加速。

Conclusion: SamGoG能有效解决图分类任务中的双重不平衡问题，并与多种GNN无缝集成。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [63] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督学习的变压器模型优化方法，通过动态量化和执行提供者优化，显著提升了推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型AI模型在资源受限环境中的部署挑战，如能耗、内存使用和延迟问题。

Method: 使用Microsoft Olive搜索优化目标，结合动态量化和Intel工具链（Neural Compressor和IPEX）进行模型优化。

Result: 在DEFT 2020任务中达到新SOTA，推理速度提升20倍，内存使用减少约70%。

Conclusion: 该方法有效优化了模型性能，适用于资源受限场景。

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [64] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: 论文提出了一种新的参数插值流模型（PIF），用于分子生成，解决了贝叶斯流网络在灵活性和适应性上的不足，并在药物设计中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯流网络（BFNs）在分子生成任务中表现出色，但其基于贝叶斯推断的策略限制了分布变换路径的灵活性，难以适应多样化数据分布和任务需求。此外，参数空间模型的潜力尚未充分探索。

Method: 提出了一种名为PIF的参数插值流模型，提供了详细的理论基础、训练和推断流程，并开发了MolPIF用于基于结构的药物设计。

Result: MolPIF在多种指标上优于基线方法，验证了参数空间生成模型在分子设计中的有效性。

Conclusion: PIF模型为分子生成提供了新的设计视角，展示了参数空间模型的潜力。

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [65] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种基于邻居分布特性的双中心图聚类方法（DCGC），通过邻居分布作为监督信号提升对比学习效果，并引入双中心优化。


<details>
  <summary>Details</summary>
Motivation: 传统目标导向聚类方法仅利用特征构建单目标分布，导致监督信号不可靠且优化不完整。

Method: 利用邻居分布作为监督信号挖掘硬负样本，并引入邻居分布中心与特征中心共同构建双目标分布进行优化。

Result: 实验证明该方法在性能和效果上优于现有方法。

Conclusion: DCGC通过可靠的邻居分布监督和双中心优化，显著提升了图聚类的效果。

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [66] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯神经网络的方法，用于微调预训练的基础模型，并通过实时学习工作流程自动检测和采样罕见事件。


<details>
  <summary>Details</summary>
Motivation: 由于从头计算原子间力的计算复杂性，机器学习力场的研究变得活跃，但生成足够多样化的训练数据仍然具有挑战性，尤其是对于罕见事件或大构型空间的系统。

Method: 采用贝叶斯神经网络方法微调预训练的基础模型，结合实时学习工作流程，利用模型不确定性自动更新模型并检测罕见事件。

Result: 该方法能够在保持预设精度的同时自动微调模型，并显著提高对罕见事件（如过渡态）的采样率。

Conclusion: 提出的方法有效解决了基础模型微调中的不确定性量化问题，为处理罕见事件和大构型空间系统提供了实用解决方案。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [67] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: 本文研究了强化学习中奖励函数为次模函数的问题，提出了一种基于剪枝次模图的方法，以在可行时间内提供近似最优解。实验表明该方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中奖励函数通常为加法形式，但现实中许多问题（如路径规划、覆盖控制）的奖励函数表现为收益递减，可用次模函数建模。本文旨在解决次模奖励函数的强化学习问题。

Method: 提出了一种基于剪枝次模图的方法，该方法在计算时间和空间上可行，并提供性能保证。

Result: 实验结果表明，所提方法获得的策略比基线方法产生更高的奖励。

Conclusion: 本文方法在次模奖励函数的强化学习问题中表现优越，为类似问题提供了可行的解决方案。

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [68] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: 研究探讨了自监督学习方法在基因表达数据表型预测中的应用，展示了其优于传统监督模型的潜力，并减少了标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 基因表达数据的表型预测对疾病机制、药物反应和个性化医疗至关重要，但传统方法依赖大量标注数据，成本高。自监督学习能直接从无标注数据中提取信息，克服这一限制。

Method: 研究选择了三种自监督学习方法，评估其在基因表达数据中提取复杂信息的能力，并用于下游预测任务。

Result: 结果表明，自监督学习方法在表型预测中优于传统监督模型，且减少了对标注数据的依赖。

Conclusion: 研究首次将自监督学习应用于批量RNA-Seq数据，提供了方法性能分析和未来研究方向。

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [69] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Causal Process的新理论框架，用于表示动态因果结构假设，并实现了Causal Process Model。该方法将Transformer的注意力机制与强化学习结合，从视觉观察中推断可解释的因果过程。


<details>
  <summary>Details</summary>
Motivation: 现有神经因果模型多假设静态因果图，忽略了因果交互的动态性，因此需要一种新理论框架来填补这一空白。

Method: 提出Causal Process框架，并实现Causal Process Model，将Transformer的注意力机制融入强化学习，通过嵌套RL任务推断动态因果图。

Result: 在强化学习环境中，该方法在因果表示学习和智能体性能上优于现有方法，并能恢复动态因果过程图。

Conclusion: Causal Process框架为动态因果结构建模提供了新思路，结合Transformer注意力机制，显著提升了因果推断能力。

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [70] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: 提出了一种名为MoDyGAN的新方法，结合分子动力学模拟和生成对抗网络，用于高效探索蛋白质构象空间。


<details>
  <summary>Details</summary>
Motivation: 由于基于物理的动态模拟计算成本高，探索蛋白质构象空间仍具挑战性。

Method: 利用分子动力学模拟和生成对抗网络，通过生成器将高斯分布映射到蛋白质轨迹，并结合双判别器提升构象合理性。创新性地将3D蛋白质结构转换为2D矩阵，以便使用图像GAN架构。

Result: 在三种刚性蛋白质上验证了MoDyGAN生成新构象的能力，并在十丙氨酸案例中展示了与引导分子动力学模拟轨迹的一致性。

Conclusion: 将蛋白质表示为图像数据为深度学习在生物分子模拟中的应用开辟了新途径，框架可扩展至其他复杂3D结构。

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [71] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出两种新方法，通过平均可控性改进图机器学习模型的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 复杂领域中的异常检测因标记数据稀缺和样本不平衡而具有挑战性，图机器学习模型结合属性和关系数据可解决此问题。

Method: 提出两种方法：1) 将平均可控性作为边权重；2) 将其编码为独热边属性向量。

Result: 在真实和合成网络中验证，新方法在异常检测性能上优于六种基线模型。

Conclusion: 平均可控性作为额外指标可有效提升稀疏和不平衡数据集中的异常检测性能。

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [72] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: 本文研究了机器学习在楔形文字分类中的应用，探讨了数据差异对模型性能的影响，并提出了未来数据采集标准的建议。


<details>
  <summary>Details</summary>
Motivation: 楔形文字因来源、用途、书写者和数字化方式的不同而存在很大变异性，导致模型在不同数据集上表现不佳。本文旨在研究这种差异对性能的影响。

Method: 使用ResNet50模型，基于来自三个美索不达米亚城市的手写古巴比伦文本数据进行训练和测试。

Result: 模型在至少20个实例的符号上取得了87.1%的top-1准确率和96.5%的top-5准确率。

Conclusion: 本文为楔形文字分类任务提供了基础，并建议未来改进数据采集标准。

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [73] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于图结构的深度学习方法，用于解决多站点神经影像研究中结构连接组（SC）的标准化问题，无需依赖详细元数据或旅行受试者。


<details>
  <summary>Details</summary>
Motivation: 小样本量和多站点采集偏差限制了神经影像生物标志物的可靠性和泛化性，现有方法依赖元数据或忽略图拓扑结构。

Method: 提出了一种站点条件化的深度标准化框架，测试了三种深度架构（全连接自编码器、卷积自编码器和图卷积自编码器）与线性回归基线的性能。

Result: 非图模型在边权重预测和边存在检测上表现优异，而图自编码器在保留拓扑结构和个体特征上更优。线性回归基线性能最高但依赖元数据，实用性受限。

Conclusion: 图结构方法在多站点SC研究中更适合结构感知和领域泛化的标准化任务。

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [74] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: 论文提出了一种动态加权机制ParallelTime Weighter，结合局部窗口注意力和Mamba，优化了长短期依赖的权重分配，显著提升了时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对长短期依赖赋予相等权重，但这对时间序列预测并非最优。

Method: 提出ParallelTime Weighter动态计算权重，并设计ParallelTime架构，结合局部窗口注意力和Mamba。

Result: 在多个基准测试中表现优异，计算效率高，参数少，且适应更长的预测范围。

Conclusion: ParallelTime为并行Attention-Mamba在时间序列预测中的发展提供了新方向。

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [75] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: 论文揭示了动态规划方法在MDP中寻找静态CVaR最优策略时失败的原因，并提出风险分配一致性约束的概念。


<details>
  <summary>Details</summary>
Motivation: 解决动态规划方法在CVaR优化中的失败问题，并探究其根本原因。

Method: 通过政策评估任务，将静态CVaR评估问题转化为两个最小化问题，并引入风险分配一致性约束。

Result: 发现约束条件不满足是导致评估错误的根源，并证明双CVaR分解方法存在固有局限性。

Conclusion: 双CVaR分解方法无法在所有初始风险水平下找到单一最优策略。

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [76] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出了一种拜占庭容错的联邦高斯过程回归算法，用于在部分代理存在拜占庭故障时提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 研究拜占庭容错的联邦在线学习，解决代理可能存在的任意或对抗行为对高斯过程回归的影响。

Method: 通过拜占庭容错的产品专家聚合规则，云端与代理协作学习潜在函数，代理融合全局模型优化本地预测。

Result: 实验验证了算法在玩具示例和真实数据集上的性能，量化了融合GPR对本地GPR的精度提升。

Conclusion: 提出的算法有效提升了拜占庭故障代理环境下的学习性能。

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [77] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT是一种基于物理感知的神经网络，用于快速自动化分析纳米束衍射数据，解决了实时分析的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 实时分析纳米束衍射数据的挑战，包括计算需求高和伪影问题，需要一种无需标记数据或预训练的解决方案。

Method: DONUT结合了可微分的几何衍射模型，通过无监督训练预测晶格应变和取向。

Result: 实验证明，DONUT比传统拟合方法效率高200倍以上，且能准确提取数据特征。

Conclusion: DONUT为X射线科学中的实时分析提供了高效且无需监督的解决方案。

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [78] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: 论文研究了持续学习中的稳定性间隙问题，提出了一种基于不确定性调制的增益动态机制来平衡知识整合与干扰，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在稳定性间隙，即在掌握新任务时对已掌握任务的性能下降，这与持续学习的目标相矛盾。研究旨在解决这一问题，特别是在理想联合训练背景下。

Method: 受生物大脑多时间尺度动态启发，提出不确定性调制的增益动态机制，模拟两时间尺度优化器，动态平衡知识整合与干扰。

Result: 在MNIST和CIFAR基准测试中，该机制有效减少了稳定性间隙。

Conclusion: 不确定性调制的增益动态机制不仅解决了稳定性间隙问题，还为持续学习提供了新的优化思路，并揭示了其与生物神经机制的相似性。

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [79] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的多目标强化学习方法（Pb-MORL），通过偏好指导策略优化，避免了复杂的奖励函数设计，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）通常依赖预定义的奖励函数，难以平衡冲突目标且易简化问题。偏好能提供更灵活直观的决策指导。

Method: 引入Pb-MORL，将偏好整合到MORL框架中，构建与偏好一致的多目标奖励模型，并证明优化该模型等同于训练帕累托最优策略。

Result: 在多个基准任务、能源管理任务和自动驾驶任务中，Pb-MORL表现优于使用真实奖励函数的基准方法。

Conclusion: Pb-MORL在复杂现实系统中具有实际应用潜力，避免了奖励函数设计的复杂性。

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [80] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了一种基于双过程多尺度心智理论（DPMT）的框架，用于提升人类与AI在动态场景中的协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）代理难以准确建模复杂的人类心理特征（如领域意图），尤其是在缺乏直接沟通的情况下。

Method: 结合认知科学中的双过程理论，提出DPMT框架，包含多尺度心智理论（ToM）模块，用于通过心理特征推理建模人类伙伴。

Result: 实验表明DPMT显著提升了人类与AI的协作效果，消融研究验证了多尺度ToM在慢系统中的贡献。

Conclusion: DPMT框架为解决人类与AI协作中的心理建模问题提供了有效方案。

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [81] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) 在类别不平衡分类中表现优于标准神经网络（MLPs），但传统不平衡策略与KANs冲突，且计算成本高。MLPs结合不平衡技术可达到与KANs相当的效果，但资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 研究KANs在类别不平衡分类中的表现，探索其与传统不平衡策略的兼容性及计算效率。

Method: 在十个基准数据集上对KANs和MLPs进行实证评估，比较其在原始不平衡数据和使用不平衡策略后的表现。

Result: KANs在原始不平衡数据上表现优于MLPs，但传统不平衡策略显著降低其性能。MLPs结合不平衡技术可达到与KANs相当的效果，且资源消耗更低。

Conclusion: KANs适用于资源充足的原始不平衡数据场景，但其性能与资源消耗的权衡及与传统策略的冲突限制了实际应用。未来需优化KANs的架构和计算效率。

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [82] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: 论文提出了一种结合时间因果表示学习与非规则张量分解的框架CaRTeD，用于处理高维不规则数据，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据常为高维且不规则，传统方法难以有效处理，需要一种能同时提取因果表示和分解不规则张量的方法。

Method: 提出CaRTeD框架，结合时间因果表示学习与非规则张量分解，提供灵活的正则化设计。

Result: 理论证明算法收敛，实验在合成和真实数据（如MIMIC-III）上表现优于现有技术。

Conclusion: CaRTeD填补了理论空白，提升了因果表示的可解释性，适用于下游任务。

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [83] [State Space Models Naturally Produce Traveling Waves, Time Cells, and Scale to Abstract Cognitive Functions](https://arxiv.org/abs/2507.13638)
*Sen Lu,Xiaoyu Zhang,Mingtao Hu,Eric Yeu-Jer Lee,Soohyeon Kim,Wei D. Lu*

Main category: q-bio.NC

TL;DR: 论文提出基于状态空间模型（SSM）的框架，用于连接微观神经回路与认知功能，并通过实验验证其能自发产生类似生物时间细胞的神经表征。


<details>
  <summary>Details</summary>
Motivation: 现代神经科学面临的核心挑战是如何将微观神经回路的详细映射与认知功能的机制理解联系起来。

Method: 采用状态空间模型（SSM），特别是S5变体，结合强化学习训练时间辨别任务，分析其动态特性。

Result: 模型自发产生类似生物时间细胞的神经表征，并揭示这些细胞源于隐藏状态向量在复平面中的旋转动态。

Conclusion: SSM框架为连接单神经元动态与认知现象提供了统一且计算可行的理论基础。

Abstract: A grand challenge in modern neuroscience is to bridge the gap between the
detailed mapping of microscale neural circuits and a mechanistic understanding
of cognitive functions. While extensive knowledge exists about neuronal
connectivity and biophysics, a significant gap remains in how these elements
combine to produce flexible, learned behaviors. Here, we propose that a
framework based on State-Space Models (SSMs), an emerging class of deep
learning architectures, can bridge this gap. We argue that the differential
equations governing elements in an SSM are conceptually consistent with the
biophysical dynamics of neurons, while the combined dynamics in the model lead
to emergent behaviors observed in experimental neuroscience. We test this
framework by training an S5 model--a specific SSM variant employing a diagonal
state transition matrix--on temporal discrimination tasks with reinforcement
learning (RL). We demonstrate that the model spontaneously develops neural
representations that strikingly mimic biological 'time cells'. We reveal that
these cells emerge from a simple generative principle: learned rotational
dynamics of hidden state vectors in the complex plane. This single mechanism
unifies the emergence of time cells, ramping activity, and
oscillations/traveling waves observed in numerous experiments. Furthermore, we
show that this rotational dynamics generalizes beyond interval discriminative
tasks to abstract event-counting tasks that were considered foundational for
performing complex cognitive tasks. Our findings position SSMs as a compelling
framework that connects single-neuron dynamics to cognitive phenomena, offering
a unifying and computationally tractable theoretical ground for temporal
learning in the brain.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: 论文探讨了在卫星图像机器学习（SatML）中结合其他地理数据层对模型性能的影响，发现多模态输入能显著提升性能，尤其在数据有限和跨区域测试时。


<details>
  <summary>Details</summary>
Motivation: 研究其他地理数据层（如高程模型、传感器数据等）与光学卫星图像结合在监督学习中的价值，以提升SatML模型的性能。

Method: 通过生成增强版的SatML基准任务数据集，将额外地理数据层与光学图像结合，用于分类、回归和分割任务。

Result: 多模态输入显著提升了模型性能，尤其在数据有限和跨区域测试时；硬编码融合策略优于学习型策略。

Conclusion: 多模态输入对SatML的数据效率和跨区域性能有重要价值，硬编码融合策略值得未来研究关注。

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [85] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 提出了一种基于生成输出分布距离的最小化概念擦除方法，通过端到端优化和神经元掩码技术，在不损害模型性能的情况下有效擦除概念。


<details>
  <summary>Details</summary>
Motivation: 生成模型依赖大规模无标签数据引发安全和版权问题，现有擦除方法过度修改模型，损害其整体效用。

Method: 基于生成输出分布距离设计目标函数，利用反向传播进行端到端优化，并引入神经元掩码增强鲁棒性。

Result: 在先进流匹配模型上的实验表明，该方法能有效擦除概念且不降低模型性能。

Conclusion: 该方法为更安全、负责任的生成模型提供了可行路径。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [86] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种名为MADI的框架，通过Masking-Augmented gaussian Diffusion (MAgD)和推理时容量扩展机制，显著提升了扩散模型的可编辑性、组合性和可控性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像生成方面取得了显著成功，但在基于视觉的编辑和组合控制方面仍面临挑战。

Method: 提出了MADI框架，包括MAgD训练策略和基于Pause Tokens的推理时容量扩展机制。

Result: MADI显著提升了扩散模型的可编辑性和组合性，特别是在局部和结构化编辑方面表现优异。

Conclusion: MADI为扩散模型在更通用的上下文生成架构中的集成铺平了道路。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [87] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 该研究提供了一个全面的多模态驾驶员 drowsiness 检测数据集，包含面部、行为和生物特征信号，数据来自19名受试者，总时长1400分钟。


<details>
  <summary>Details</summary>
Motivation: 创建更全面的驾驶员 drowsiness 数据集，捕捉生理、行为和驾驶相关信号的连续变化。

Method: 整合3D面部视频、红外摄像、后视视频、生物特征信号（心率、皮肤电活动等）及驾驶模拟器数据，受试者每4分钟自评 drowsiness 水平。

Result: 生成了一个包含连续40分钟数据采集会话的多模态数据集，记录了驾驶员状态的逐渐变化。

Conclusion: 该数据集为驾驶员 drowsiness 研究提供了更全面的多模态数据支持，未来可进一步用于算法开发和验证。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [88] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: COREVQA是一个新的视觉语言模型基准，专注于拥挤场景中的视觉蕴含任务，揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准很少测试模型在视觉蕴含任务（如基于图像接受或反驳假设）上的能力，尤其是在拥挤场景中。

Method: 提出了COREVQA基准，包含5608张图像和合成的真假陈述对，图像来自CrowdHuman数据集。

Result: 即使表现最好的模型准确率也低于80%，其他模型表现更差（39.98%-69.95%）。

Conclusion: 结果表明视觉语言模型在拥挤场景的视觉蕴含推理能力存在显著不足。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [89] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 通过结合古老的CORONA卫星影像升级深度学习模型，显著提升了考古遗址自动识别的精度，并在伊拉克阿布格莱布地区发现了四个新遗址。


<details>
  <summary>Details</summary>
Motivation: 利用CORONA影像弥补现代环境变化导致的考古遗址消失问题，验证AI技术在考古领域的潜力。

Method: 基于Bing卷积网络模型，使用CORONA影像对模型进行重新训练，应用于美索不达米亚平原的阿布格莱布地区。

Result: 检测精度显著提升（IoU超过85%，整体准确率达90%），并发现四个新遗址。

Conclusion: AI技术与历史影像结合是发现已消失考古遗址的有效方法，对考古研究具有重要意义。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [90] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: 提出了一种基于3D卷积自编码器的无监督方法，用于从Sentinel-2卫星图像序列中检测甜菜田的压力状态。


<details>
  <summary>Details</summary>
Motivation: 卫星图像时间序列（SITS）数据因其丰富的频谱和时间特性，在农业任务中表现优异。本研究旨在通过无监督方法解决甜菜田压力检测问题。

Method: 使用3D卷积自编码器从Sentinel-2图像序列中提取特征，并结合特定采集日期的时间编码以捕捉甜菜生长动态。

Result: 学习到的特征用于下游聚类任务，成功区分压力田和健康田，且系统可直接应用于不同年份的数据。

Conclusion: 该方法为甜菜田压力检测提供了一种实用且易用的工具。

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [91] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一种新的大规模RGB-事件数据集EvReID，并提出了TriPro-ReID框架，用于提升行人重识别的特征学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有事件相机行人重识别方法因数据稀缺而难以评估真实性能的问题。

Method: 构建EvReID数据集并设计TriPro-ReID框架，结合RGB和事件流数据，利用行人属性进行对比学习。

Result: 在EvReID和MARS数据集上的实验验证了框架的有效性。

Conclusion: EvReID数据集和TriPro-ReID框架为未来研究提供了数据和基准支持。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [92] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse是一个统一的V2X协同感知框架，通过分层融合机制和自适应空间分辨率调整，解决了异构传感器配置下的特征融合问题，并在TUMTraf-V2X数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的V2X协同感知系统常因异构传感器配置导致特征融合和感知可靠性问题，需要一种统一的解决方案。

Method: 提出HeCoFuse框架，采用分层融合机制（通道和空间注意力）和自适应空间分辨率调整模块，并结合动态调整融合类型的协同学习策略。

Result: 在TUMTraf-V2X数据集上，HeCoFuse在LC+LC配置下达到43.22% 3D mAP，L+LC配置下达到43.38%，并在CVPR 2025 DriveX挑战赛中排名第一。

Conclusion: HeCoFuse在异构传感器配置下表现出色，成为当前TUM-Traf V2X数据集上的最先进方法。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [93] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: 本文分析了StyleGAN生成器的内部机制，探讨了其关键架构和技术，如均衡学习率，并通过修剪权重减少计算需求。同时研究了潜在向量对生成人脸的影响，揭示了其精确操控能力及潜在的伦理风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像技术的普及，理解StyleGAN的工作原理及其潜在风险变得尤为重要。

Method: 使用PyTorch框架训练StyleGAN模型，通过修剪权重和详细分析潜在向量来研究其行为。

Result: 发现大量权重可被修剪而不显著影响输出，且潜在向量可精确操控人脸特征。

Conclusion: StyleGAN的精确操控能力虽具学术价值，但也可能被恶意利用，引发严重的伦理问题。

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [94] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: 该论文提出了一种基于排列熵（PE）的新型图像分类方法，结合HOG和LBP特征，训练SVM分类器，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在图像分类中，特征工程在可解释性和计算效率方面仍具优势，尤其是与参数庞大的深度学习模型相比。

Method: 扩展PE至二维图像，结合HOG和LBP提取多尺度、多方向熵特征，训练SVM分类器。

Result: 在Fashion-MNIST等数据集上表现优异，展示了PE与HOG、LBP融合的潜力。

Conclusion: 该方法为图像分类提供了一种轻量、可解释的替代方案，展示了熵描述符的潜力。

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [95] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: 研究发现视觉模型的感知能力与其短期预测性能强相关，提出了一种通用预测框架，利用潜在扩散模型在冻结表示空间预测未来特征。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模型的感知能力如何影响其预测性能，以提升视频理解的时空基础。

Method: 提出通用预测框架，训练潜在扩散模型预测冻结视觉骨干的未来特征，并通过轻量级任务特定解码器解码。

Result: 在九种模型和四项任务中验证了框架的有效性，显示感知能力与预测性能强相关。

Conclusion: 结合表示学习和生成模型对时空视频理解具有重要价值。

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [96] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: 提出了一种基于量子辅助推理的超轻量级EIT图像重建框架QuantEIT，显著减少了模型复杂性和参数数量，并在无监督、无需训练数据的情况下实现了高精度重建。


<details>
  <summary>Details</summary>
Motivation: EIT作为一种低成本、高时间分辨率的床边成像技术，其逆问题的病态性限制了图像重建的准确性。现有深度学习方法依赖复杂网络结构，效率低且难以扩展。

Method: QuantEIT采用量子辅助网络（QA-Net），结合并行2量子比特电路生成隐式非线性先验的潜在表示，并通过单层线性网络重建电导率。

Result: 在模拟和真实2D/3D肺部EIT数据上，QuantEIT仅用0.2%的参数即达到或优于传统方法的精度，且对噪声更具鲁棒性。

Conclusion: QuantEIT首次将量子电路引入EIT图像重建，为高效、轻量化的床边监测提供了新思路。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [97] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: 该研究评估了一种基于深度学习的自动化软件（Carebot AI Bones）用于测量脊柱侧弯的Cobb角，结果显示其与放射科医生的测量结果具有高度一致性，可用于临床工作流。


<details>
  <summary>Details</summary>
Motivation: 脊柱侧弯影响2-4%的青少年，传统手动测量Cobb角耗时且存在观察者间差异，因此需要一种自动化解决方案。

Method: 研究回顾性评估了Carebot AI Bones软件在103张站立位全脊柱X光片上的表现，以两位放射科医生的独立测量为参考。

Result: AI与放射科医生的测量结果高度一致（MAE约3.9度，Pearson相关系数0.88-0.91），且能准确分类严重程度（Cohen kappa 0.51-0.64）。

Conclusion: 该软件能复现专家水平的Cobb角测量和分类，可用于优化脊柱侧弯的临床报告和分诊流程。

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [98] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: 提出了一种自动化、模块化的流程，用于挖掘高质量的三元组数据（原始图像、指令、编辑图像），以支持生成模型的训练，并发布了开源数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 生成模型需要大量高质量的三元组数据，但手动标注成本高且难以满足要求，因此需要自动化解决方案。

Method: 使用公共生成模型和Gemini验证器自动评分，通过反转和组合自举扩展数据集。

Result: 发布了NHR-Edit数据集（358k三元组）和Bagel-NHR-Edit模型，在跨数据集评估中表现最佳。

Conclusion: 该方法实现了大规模高质量数据集的自动化生成，推动了生成模型研究的发展。

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 扩展了低成本GELLO遥操作系统，加入力反馈和力信息用于模仿学习，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升遥操作系统的用户体验和任务成功率，通过力反馈和力信息增强模仿学习模型的性能。

Method: 在GELLO系统中实现力反馈，并将力信息用于数据收集和模仿学习训练，通过用户研究和任务性能对比验证。

Result: 有机器人经验的用户偏好新控制器，力信息显著提高了多数任务的完成率。

Conclusion: 力反馈和力信息的加入提升了遥操作系统的性能和用户满意度。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [100] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出了一种半监督框架，利用世界模型的潜在空间中的控制屏障证书（CBCs）合成安全的视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 从视觉数据合成安全控制器通常需要大量标记安全关键数据，这在现实场景中不切实际。

Method: 结合学习神经屏障函数和安全控制器，利用现代视觉变换器的预测能力进行潜在动力学建模。

Result: 通过有限的标记数据实现了安全的视觉运动策略合成。

Conclusion: 该方法为可扩展且数据高效的安全控制提供了新途径。

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [101] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT是一种结构感知的微调方法，通过注入图拓扑信息到预训练的大型语言模型（LLMs）中，显著提升了AMR到文本生成的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理抽象意义表示（AMRs）时，常忽略结构信息或使用与标准LLMs不兼容的架构，限制了性能。

Method: SAFT利用磁拉普拉斯变换计算方向敏感的位置编码，并将其投影到LLM的嵌入空间中，无需改变模型架构。

Result: 在AMR 3.0上，SAFT比基线方法提升了3.5 BLEU分数，且性能提升随图复杂度增加而显著。

Conclusion: SAFT为结构化数据与语言模型的结合提供了一种通用且高效的途径。

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [102] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: 提出了一种基于图的上下文方法，结合NLP和MDL-GBAD算法，用于检测假新闻。


<details>
  <summary>Details</summary>
Motivation: 解决数字世界中假新闻快速传播的问题。

Method: 使用NLP将新闻文章转换为图结构，并应用MDL-GBAD算法进行图挖掘和异常检测。

Result: 方法能够识别数据集中的规范模式并发现异常模式。

Conclusion: 基于图的方法在检测假新闻方面表现出色，能够发现复杂模式。

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [103] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: PARAM-1是一个专注于印度语言多样性的2.9B参数语言模型，通过公平的数据分配、适应印度形态的标记化和文化对齐的评估基准，提供了一种公平的基础模型设计方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的开发以英语为中心，导致印度等多语言地区的语言多样性被忽视。PARAM-1旨在通过专注于印度语言多样性来解决这一问题。

Method: PARAM-1是一个仅解码器、仅文本的模型，训练于印地语和英语的双语数据集，采用公平的语料分配、适应印度形态的标记化方法，并设计了文化对齐的评估基准。

Result: PARAM-1既是一个通用的语言模型，也是印度中心应用的强大基线。

Conclusion: PARAM-1通过在预训练阶段嵌入多样性，为公平的基础模型设计提供了蓝图。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [104] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: 研究使用机器学习预测法国上诉法院的儿童抚养权判决，发现法官个体决策模式显著影响结果，支持法律现实主义观点。


<details>
  <summary>Details</summary>
Motivation: 挑战法官作为中立变量统一适用法律的假设，探讨法官个体决策对案件结果的影响。

Method: 采用混合方法，结合大型语言模型（LLMs）提取结构化特征和机器学习模型（RF、XGB、SVC）预测结果，比较个体法官模型与通用模型。

Result: 个体法官模型预测准确率显著高于通用模型（F1分数92.85% vs 82.63%），支持法官个体模式对判决的影响。

Conclusion: 研究支持法律现实主义，法官身份对法律结果有可测量影响，数据与代码将公开。

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [105] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: 论文提出两种生成问答对的方法：一种基于文章内容，另一种基于知识图谱，用于快速提取科学文章的核心思想。


<details>
  <summary>Details</summary>
Motivation: 学者需要快速理解文章核心思想，但现有方法缺乏对新颖性的评估。

Method: 方法一：利用大语言模型从文章段落生成问答对；方法二：基于知识图谱，通过实体关系提取和三元组筛选生成问答对。

Result: 知识图谱方法能有效捕捉文章核心思想，且实体关系模型的微调对高质量三元组提取至关重要。

Conclusion: 知识图谱方法优于纯内容方法，适用于科学文章的核心思想提取。

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [106] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: RHYTHM是一个利用大型语言模型（LLMs）进行时空预测和轨迹推理的框架，通过分层时间标记化和冻结LLM实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹预测中序列长度过长和计算效率低的问题，同时提升预测准确性。

Method: 将轨迹分割为每日段并编码为离散标记，利用分层注意力捕获依赖关系，并通过预计算提示嵌入增强模型能力。

Result: 在三个真实数据集上，准确率提升2.4%，周末提升5.0%，训练时间减少24.6%。

Conclusion: RHYTHM在计算效率和预测性能上均优于现有方法。

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [107] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: 提出CPC-CMS框架用于文档级情感分析，通过专家知识计算权重，选择最佳分类模型。


<details>
  <summary>Details</summary>
Motivation: 解决文档级情感分析中模型选择问题，结合多评价标准优化选择。

Method: 基于专家知识计算权重，构建加权决策矩阵，比较多种基线模型。

Result: ALBERT在排除时间因素时表现最佳，但考虑时间消耗时无单一模型始终最优。

Conclusion: CPC-CMS可扩展至其他分类应用领域。

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [108] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: DENSE系统通过模拟医生参考历史记录的方式，利用大型语言模型生成临床连贯且时间敏感的进展笔记，填补了电子健康记录中进展笔记的缺失。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中进展笔记的严重缺失（如MIMIC-III数据集中仅8.56%的医院访问包含进展笔记）导致患者纵向叙事的断裂，影响了临床决策支持等下游任务。

Method: DENSE系统通过细粒度笔记分类和时间对齐机制，从当前和既往访问中检索相关证据，并利用大型语言模型生成连贯的进展笔记。

Result: 生成的笔记在时间对齐比上达到1.089，优于原始笔记的连续性，支持了总结、预测建模和临床决策支持等任务。

Conclusion: DENSE系统为现实医疗环境中基于LLM的笔记合成提供了可扩展的解决方案，恢复了碎片化文档的叙事连贯性。

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [109] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: 提出了一种基于小波系数衰减行为的（近）线性时间算法，用于检测非均匀采样多元信号的局部规律性。


<details>
  <summary>Details</summary>
Motivation: 受小波系数衰减行为的启发，研究旨在解决高维和非均匀数据中的局部规律性检测问题。

Method: 利用快速样本变换（一种针对分散数据的小波变换）分析样本系数的衰减与信号点规律性的关系。

Result: 建立了样本系数衰减与多元信号点规律性之间的联系，并在数值研究中验证了其在一维、二维和三维信号中的有效性。

Conclusion: 样本变换在高维和分散数据中表现出色，优于传统小波方法。

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升了CUDA内核的性能，并在多种GPU架构上表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型对GPU计算资源的需求激增，亟需自动化CUDA优化策略，而现有模型优化成功率低。

Method: 提出CUDA-L1框架，通过强化学习优化CUDA内核性能，无需人工干预。

Result: 在A100上平均加速17.7倍，峰值达449倍，且在其他GPU架构上表现优异。

Conclusion: CUDA-L1展示了强化学习在自动化CUDA优化中的潜力，有望提升GPU效率并缓解资源压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


### [111] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应建模实体而非感知或描述，指出关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，应直接建模这些实体。

Method: 分析关系学习（如统计关系AI）的现状及其受限原因。

Result: 关系学习仅在少数受限关系中成功，未广泛普及。

Conclusion: 需改进方法以使关系学习发挥其潜力。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [112] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在加速透明、可重复和稳健的AI解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 解决高质量糖尿病数据集获取困难的问题，推动AI在糖尿病管理中的应用。

Method: 收集并整合10个公开数据集，提供比较分析和血糖预测案例研究。

Result: 同一算法在不同数据集上的预测结果差异显著，为开发稳健AI提供建议。

Conclusion: Glucose-ML为糖尿病AI研究提供丰富数据和基准，促进健康领域AI发展。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [113] [Treedepth Inapproximability and Exponential ETH Lower Bound](https://arxiv.org/abs/2507.13818)
*Édouard Bonnet,Daniel Neuen,Marek Sokołowski*

Main category: cs.CC

TL;DR: 论文研究了树深度的计算和近似问题，证明了1.0003-近似是NP难的，且精确计算需要指数时间，除非ETH不成立。


<details>
  <summary>Details</summary>
Motivation: 树深度是算法图论中的核心参数，当前最优算法和近似算法的性能仍有改进空间，研究其计算复杂性和近似难度具有重要意义。

Method: 通过从Satisfiability到Treedepth的直接简化，类似于Treewidth的简化方法，证明了近似和精确计算的复杂性。

Result: 证明了1.0003-近似是NP难的，精确计算需要2^Ω(n)时间，且存在常数δ,c使得(1+δ)-近似需要2^Ω(n/log^c n)时间。

Conclusion: 树深度的近似和精确计算具有极高的复杂性，除非ETH不成立，否则难以显著改进当前算法性能。

Abstract: Treedepth is a central parameter to algorithmic graph theory. The current
state-of-the-art in computing and approximating treedepth consists of a
$2^{O(k^2)} n$-time exact algorithm and a polynomial-time $O(\text{OPT}
\log^{3/2} \text{OPT})$-approximation algorithm, where the former algorithm
returns an elimination forest of height $k$ (witnessing that treedepth is at
most $k$) for the $n$-vertex input graph $G$, or correctly reports that $G$ has
treedepth larger than $k$, and $\text{OPT}$ is the actual value of the
treedepth. On the complexity side, exactly computing treedepth is NP-complete,
but the known reductions do not rule out a polynomial-time approximation scheme
(PTAS), and under the Exponential Time Hypothesis (ETH) only exclude a running
time of $2^{o(\sqrt n)}$ for exact algorithms.
  We show that 1.0003-approximating treedepth is NP-hard, and that exactly
computing the treedepth of an $n$-vertex graph requires time $2^{\Omega(n)}$,
unless the ETH fails. We further derive that there exist absolute constants
$\delta, c > 0$ such that any $(1+\delta)$-approximation algorithm requires
time $2^{\Omega(n / \log^c n)}$. We do so via a simple direct reduction from
Satisfiability to Treedepth, inspired by a reduction recently designed for
Treewidth [STOC '25].

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [114] [A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design](https://arxiv.org/abs/2507.13580)
*Hao Tuo,Yan Li,Xuanning Hu,Haishi Zhao,Xueyan Liu,Bo Yang*

Main category: q-bio.BM

TL;DR: AutoLeadDesign是一个基于大语言模型和化学片段的先导化合物设计框架，优于基线方法，并在针对PRMT5和SARS-CoV-2 PLpro的实验中表现出专家级设计效果。


<details>
  <summary>Details</summary>
Motivation: 当前方法在整合领域知识方面存在挑战，限制了其设计新颖且有效结合模式的先导化合物的能力。

Method: AutoLeadDesign利用大语言模型编码的领域知识和化学片段，逐步探索化学空间。

Result: 实验表明AutoLeadDesign优于基线方法，并在针对PRMT5和SARS-CoV-2 PLpro的实验中生成专家级先导化合物。

Conclusion: AutoLeadDesign为药物设计提供了一种高效方法，具有潜在应用价值。

Abstract: Combinatorial optimization algorithm is essential in computer-aided drug
design by progressively exploring chemical space to design lead compounds with
high affinity to target protein. However current methods face inherent
challenges in integrating domain knowledge, limiting their performance in
identifying lead compounds with novel and valid binding mode. Here, we propose
AutoLeadDesign, a lead compounds design framework that inspires extensive
domain knowledge encoded in large language models with chemical fragments to
progressively implement efficient exploration of vast chemical space. The
comprehensive experiments indicate that AutoLeadDesign outperforms baseline
methods. Significantly, empirical lead design campaigns targeting two
clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate
AutoLeadDesign's competence in de novo generation of lead compounds achieving
expert-competitive design efficacy. Structural analysis further confirms their
mechanism-validated inhibitory patterns. By tracing the process of design, we
find that AutoLeadDesign shares analogous mechanisms with fragment-based drug
design which traditionally rely on the expert decision-making, further
revealing why it works. Overall, AutoLeadDesign offers an efficient approach
for lead compounds design, suggesting its potential utility in drug design.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [115] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: PHASE是一个机器学习框架，通过分析Zeek连接日志，以超过90%的准确率区分人类与非人类活动，提升网络安全模拟环境中合成用户行为的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏定量方法来评估合成用户行为的真实性，而网络安全模拟环境需要逼真的人类行为以提高有效性。

Method: PHASE框架被动分析Zeek连接日志，利用本地DNS记录分类流量，并结合SHAP分析揭示人类用户的行为特征。

Result: PHASE能准确区分人类与非人类活动，并通过改进合成用户配置显著提升行为真实性。

Conclusion: PHASE为网络安全模拟环境提供了一种高效、被动的行为真实性评估方法，显著提升了合成用户行为的逼真度。

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [116] [Quantum Blockchain Survey: Foundations, Trends, and Gaps](https://arxiv.org/abs/2507.13720)
*Saurav Ghosh*

Main category: cs.CR

TL;DR: 本文综述了量子计算对区块链的威胁及应对方向，包括后量子区块链和量子区块链，比较了其技术方案、安全性和可扩展性，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统区块链的加密基础，需要研究量子安全解决方案。

Method: 综述后量子区块链（量子抗性算法）和量子区块链（量子特性）的技术发展、架构设计和实现挑战。

Result: 提供了技术方案的比较，分析了安全性、可扩展性和部署的权衡，并指出了硬件、共识和网络设计中的开放问题。

Conclusion: 为量子时代推进安全区块链系统提供了结构化、全面的参考。

Abstract: Quantum computing poses fundamental risks to classical blockchain systems by
undermining widely used cryptographic primitives. In response, two major
research directions have emerged: post-quantum blockchains, which integrate
quantum-resistant algorithms, and quantum blockchains, which leverage quantum
properties such as entanglement and quantum key distribution. This survey
reviews key developments in both areas, analyzing their cryptographic
foundations, architectural designs, and implementation challenges. This work
provides a comparative overview of technical proposals, highlight trade-offs in
security, scalability, and deployment, and identify open research problems
across hardware, consensus, and network design. The goal is to offer a
structured and comprehensive reference for advancing secure blockchain systems
in the quantum era.

</details>


### [117] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: FuSeFL是一种完全安全且可扩展的联邦学习方案，专为跨机构设置设计，通过轻量级安全多方计算（MPC）实现高效训练，同时保护数据和模型的隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在隐私保护方面存在高计算、通信或内存开销的问题，且忽略了全局模型的保密性，限制了其在实际应用中的可行性。

Method: FuSeFL采用轻量级MPC在客户端对之间分散训练，服务器仅负责安全聚合，避免了数据卸载和服务器瓶颈。

Result: FuSeFL显著降低了通信延迟（高达95%）和服务器内存使用（50%），同时提高了准确性，并有效防御推理攻击。

Conclusion: FuSeFL在安全性和效率方面表现出色，适用于大规模跨机构联邦学习场景。

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [118] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT是一种梯度感知免疫技术，通过双层优化问题防御扩散模型免受恶意微调，同时保持安全内容生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制（如安全检查器）容易被绕过，概念擦除方法在对抗性微调下失效，需要更鲁棒的防御方法。

Method: GIFT将免疫问题建模为双层优化：上层目标通过表示噪声和最大化降低有害概念表达能力，下层目标保持安全数据性能。

Result: 实验表明，GIFT显著削弱模型重新学习有害概念的能力，同时保持安全内容生成质量。

Conclusion: GIFT为创建抗对抗性微调攻击的安全生成模型提供了有前景的方向。

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [119] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: 该论文综述了大型语言模型（LLMs）在网络安全中的应用，包括威胁检测、漏洞评估和事件响应，并探讨了LLMs自身的漏洞及缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在网络安全领域的潜力巨大，但其自身也存在漏洞，需要全面研究和策略应对。

Method: 通过综合近期研究，分析LLMs在关键网络安全领域的应用及其自身漏洞。

Result: LLMs在网络安全中表现优于传统方法，但需关注其漏洞和风险。

Conclusion: 论文提供了实用见解和战略建议，以利用LLMs构建安全、可扩展的未来网络防御系统。

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [120] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao,Bimal Adhikari,Shangqing Zhao,Jingxian Wu,Yanjun Pan*

Main category: cs.CR

TL;DR: 论文探讨了基于深度学习的射频指纹识别系统的安全漏洞，发现域偏移会导致设备被误分类为特定设备，可能被用作后门攻击。此外，训练模型时使用原始信号会导致指纹与环境特征纠缠，增加攻击风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注系统在无线环境中的鲁棒性，而忽略了深度学习模型的安全漏洞。本文旨在系统分析这些漏洞及其潜在威胁。

Method: 通过对抗性实验分析，研究了域偏移对深度学习模型的影响，并基于真实世界数据验证了攻击的可行性。

Result: 发现域偏移会导致设备被误分类为特定设备，形成后门攻击机会；同时，使用原始信号训练的模型会纠缠指纹与环境特征，增加攻击面。

Conclusion: 需重新设计深度学习模型以分离指纹与环境特征，并开发更全面的安全措施，而非仅依赖后处理技术。

Abstract: Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [121] [Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection](https://arxiv.org/abs/2507.13459)
*Vijay K. Dubey,Collin E. Haese,Osman Gültekin,David Dalton,Manuel K. Rausch,Jan N. Fuhg*

Main category: cs.CE

TL;DR: 提出了一种基于图神经网络的代理模型，用于快速推断非线性边界值问题中的软体接触，解决了现有方法局限于刚性接触的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理软体接触时存在局限，尤其是在几何变化的情况下，无法满足充分条件。

Method: 采用图神经网络架构，结合连续碰撞检测和充分接触条件，优化损失函数中的接触项。

Result: 在生物假体主动脉瓣的软组织力学问题上表现优异，实现了千倍的推理速度提升。

Conclusion: 该方法在软体接触问题上具有优势，但训练成本较高，需权衡计算开销与性能提升。

Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems
in mechanics are helpful in a broad range of engineering applications. However,
effective surrogate modeling of applications involving the contact of
deformable bodies, especially in the context of varying geometries, is still an
open issue. In particular, existing methods are confined to rigid body contact
or, at best, contact between rigid and soft objects with well-defined contact
planes. Furthermore, they employ contact or collision detection filters that
serve as a rapid test but use only the necessary and not sufficient conditions
for detection. In this work, we present a graph neural network architecture
that utilizes continuous collision detection and, for the first time,
incorporates sufficient conditions designed for contact between soft deformable
bodies. We test its performance on two benchmarks, including a problem in soft
tissue mechanics of predicting the closed state of a bioprosthetic aortic
valve. We find a regularizing effect on adding additional contact terms to the
loss function, leading to better generalization of the network. These benefits
hold for simple contact at similar planes and element normal angles, and
complex contact at differing planes and element normal angles. We also
demonstrate that the framework can handle varying reference geometries.
However, such benefits come with high computational costs during training,
resulting in a trade-off that may not always be favorable. We quantify the
training cost and the resulting inference speedups on various hardware
architectures. Importantly, our graph neural network implementation results in
up to a thousand-fold speedup for our benchmark problems at inference.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [122] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: 该论文提出了一个框架，用于对偶化Kolmogorov结构函数，并引入统计力学中的概念，证明了Legendre-Fenchel对偶性，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 研究信息论构造与统计力学之间的数学类比，探索模型复杂度与泛化能力之间的关系。

Method: 开发了对偶化Kolmogorov结构函数的框架，引入分区函数和自由能泛函，证明Legendre-Fenchel对偶性，并通过线性回归和树模型进行实验验证。

Result: 理论预测得到实验验证，展示了模型复杂度、泛化能力和过拟合阈值之间的相互作用。

Conclusion: 该框架成功地将信息论与统计力学联系起来，为理解模型复杂度和泛化能力提供了新视角。

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [123] [Neural Architecture Search with Mixed Bio-inspired Learning Rules](https://arxiv.org/abs/2507.13485)
*Imane Hamzaoui,Riyadh Baghdadi*

Main category: cs.NE

TL;DR: 通过神经架构搜索（NAS）自动发现不同层的生物启发学习规则，混合使用这些规则可以提升生物启发神经网络的准确性和可扩展性，甚至在某些情况下超越基于反向传播的模型。


<details>
  <summary>Details</summary>
Motivation: 生物启发神经网络在对抗鲁棒性、能效和生理学对齐方面具有优势，但在准确性和可扩展性上落后于基于反向传播的模型。

Method: 扩展NAS搜索空间以包含生物启发学习规则，自动发现每层的最佳架构和学习规则。

Result: 混合使用不同生物启发学习规则的神经网络在多个数据集上创下新记录，甚至在某些情况下超越基于反向传播的模型。

Conclusion: 层间学习规则的多样性有助于提升生物启发神经网络的性能和可扩展性，值得进一步研究。

Abstract: Bio-inspired neural networks are attractive for their adversarial robustness,
energy frugality, and closer alignment with cortical physiology, yet they often
lag behind back-propagation (BP) based models in accuracy and ability to scale.
We show that allowing the use of different bio-inspired learning rules in
different layers, discovered automatically by a tailored
neural-architecture-search (NAS) procedure, bridges this gap. Starting from
standard NAS baselines, we enlarge the search space to include bio-inspired
learning rules and use NAS to find the best architecture and learning rule to
use in each layer. We show that neural networks that use different bio-inspired
learning rules for different layers have better accuracy than those that use a
single rule across all the layers. The resulting NN that uses a mix of
bio-inspired learning rules sets new records for bio-inspired models: 95.16% on
CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on
ImageNet. In some regimes, they even surpass comparable BP-based networks while
retaining their robustness advantages. Our results suggest that layer-wise
diversity in learning rules allows better scalability and accuracy, and
motivates further research on mixing multiple bio-inspired learning rules in
the same network.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [124] [Application Placement with Constraint Relaxation](https://arxiv.org/abs/2507.13895)
*Damiano Azzolini,Marco Duca,Stefano Forti,Francesco Gallo,Antonio Ielo*

Main category: cs.LO

TL;DR: 本文提出了一种基于答案集编程（ASP）的方法，用于解决云边网络中服务部署的组合优化问题，特别是处理不可满足问题实例和偏好需求。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案无法处理不可满足问题实例和偏好需求，而云边网络中的服务部署需要更灵活的方法。

Method: 利用答案集编程（ASP）的优化能力，结合功能和非功能约束，解决服务部署问题。

Result: 在模拟环境中，该方法在真实网络和应用中表现有效。

Conclusion: ASP方法为解决云边网络中的服务部署问题提供了有效且灵活的解决方案。

Abstract: Novel utility computing paradigms rely upon the deployment of multi-service
applications to pervasive and highly distributed cloud-edge infrastructure
resources. Deciding onto which computational nodes to place services in
cloud-edge networks, as per their functional and non-functional constraints,
can be formulated as a combinatorial optimisation problem. Most existing
solutions in this space are not able to deal with \emph{unsatisfiable} problem
instances, nor preferences, i.e. requirements that DevOps may agree to relax to
obtain a solution. In this article, we exploit Answer Set Programming
optimisation capabilities to tackle this problem. Experimental results in
simulated settings show that our approach is effective on lifelike networks and
applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [125] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: 提出了一种无监督的DRC违规预测方法，解决了传统方法需要大量平衡数据集和训练时间的问题，验证了高准确性和高效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习和神经网络的DRC检测方法需要大量平衡数据集和训练时间，限制了其应用。

Method: 提出无监督学习方法，仅需单类不平衡数据集，通过设定阈值对新数据进行分类。

Result: 在28纳米CMOS技术下验证，预测准确率达99.95%，训练时间显著低于SVM和NN模型。

Conclusion: 该方法显著提升了DRC检测的效率和准确性，为下一代EDA工具提供了新思路。

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [126] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 论文探讨了利用大语言模型（LLM）生成硬件设计RTL代码的现状，构建了一个高质量Verilog数据集，并评估了其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计自动化中的应用需求增长，但缺乏高质量的训练数据集。

Method: 通过自动化三步骤流程（数据库创建、数据收集、预处理）构建Verilog数据集，并实现高效数据库基础设施。

Result: 生成了20,392个Verilog样本（751 MB），是目前最大的高质量Verilog数据集。

Conclusion: 该数据集为LLM在硬件生成中的未来研究和开发提供了重要资源。

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [127] [DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation](https://arxiv.org/abs/2507.13957)
*Yitong Li,Raoul Grasman*

Main category: cs.IR

TL;DR: 论文提出DUALRec模型，结合LSTM的时间动态建模能力和LLM的语义推理能力，以解决推荐系统中动态用户偏好的建模问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以捕捉动态和上下文丰富的用户偏好，传统方法如协同过滤和基于内容的方法在时间模式和用户意图演化方面表现不足。

Method: 提出DUALRec模型，结合LSTM的时间建模能力和微调LLM的语义推理能力，通过用户历史行为生成推荐。

Result: 在MovieLens-1M数据集上，DUALRec在HR@k、NDCG@k和类型相似性指标上优于基线模型。

Conclusion: DUALRec填补了时间序列建模与语义推理之间的空白，为开发更智能的推荐系统提供了新方向。

Abstract: The modern recommender systems are facing an increasing challenge of
modelling and predicting the dynamic and context-rich user preferences.
Traditional collaborative filtering and content-based methods often struggle to
capture the temporal patternings and evolving user intentions. While Large
Language Models (LLMs) have gained gradual attention in recent years, by their
strong semantic understanding and reasoning abilities, they are not inherently
designed to model chronologically evolving user preference and intentions. On
the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which
is good at capturing the temporal dynamics of user behaviour and evolving user
preference over time, but still lacks a rich semantic understanding for
comprehensive recommendation generation. In this study, we propose DUALRec
(Dynamic User-Aware Language-based Recommender), a novel recommender that
leverages the complementary strength of both models, which combines the
temporal modelling abilities of LSTM networks with semantic reasoning power of
the fine-tuned Large Language Models. The LSTM component will capture users
evolving preference through their viewing history, while the fine-tuned LLM
variants will leverage these temporal user insights to generate next movies
that users might enjoy. Experimental results on MovieLens-1M dataset shows that
the DUALRec model outperforms a wide range of baseline models, with
comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted
Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes
a novel architecture that bridges the gap between temporal sequence modeling
and semantic reasoning, and offers a promising direction for developing more
intelligent and context-aware recommenders.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [128] [Asymptotic behavior of eigenvalues of large rank perturbations of large random matrices](https://arxiv.org/abs/2507.12182)
*Ievgenii Afanasiev,Leonid Berlyand,Mariia Kiyashko*

Main category: math-ph

TL;DR: 论文研究了变形Wigner随机矩阵及其与深度神经网络（DNNs）的联系，提出了针对秩增长情况的渐近分析方法。


<details>
  <summary>Details</summary>
Motivation: DNNs的权重矩阵可表示为随机部分与高度相关部分的组合，其谱特性对基于随机矩阵理论的剪枝技术至关重要，但此前研究仅限于有限秩情况。

Method: 开发了针对秩增长情况的渐近分析方法。

Result: 为秩增长情况提供了理论支持。

Conclusion: 该方法为DNNs剪枝技术的理论基础提供了扩展。

Abstract: The paper is concerned with deformed Wigner random matrices. These matrices
are closely connected with Deep Neural Networks (DNNs): weight matrices of
trained DNNs could be represented in the form $R + S$, where $R$ is random and
$S$ is highly correlated. The spectrum of such matrices plays a key role in
rigorous underpinning of the novel pruning technique based on Random Matrix
Theory. Mathematics has been done only for finite-rank matrix $S$. However, in
practice rank may grow. In this paper we develop asymptotic analysis for the
case of growing rank.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [129] [The Proportional Fair Scheduler in Wavelength-Multiplexed Quantum Networks](https://arxiv.org/abs/2507.13999)
*Sanidhay Bhambay,Siddarth Koduru Joshi,Thirupathaiah Vasantam,Neil Walton*

Main category: quant-ph

TL;DR: 本文提出了一种比例公平泵浦策略（PF-PS），用于优化量子网络中的资源分配，平衡公平性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 量子网络中的资源竞争、优化和公平性问题对量子密钥分发（QKD）网络的性能至关重要，需要解决时间变化信道和多用户同时泵浦的挑战。

Method: 设计了自适应泵浦策略，特别是比例公平泵浦策略（PF-PS），动态优先处理平均密钥率较低的用户，并优化公平性与吞吐量的平衡。

Result: 理论和数值模拟表明，PF-PS在纠缠态分发中表现最优，是量子网络中高效资源分配的强有力候选方案。

Conclusion: 比例公平泵浦策略（PF-PS）是量子网络中资源分配的有效方法，能够兼顾公平性和网络性能。

Abstract: We address the problem of optimal pumping strategies in quantum networks.
These networks enable secure communication by distributing entangled photon
pairs to user (or node) pairs. Quantum Key Distribution (QKD) protocols, like
BBM92, generate secret keys from entangled photons. While secure communication
and error correction are essential for any quantum communication channel,
resource contention, optimization, and fairness issues are critical for
networks. In this article, we analyze the performance of quantum networks,
proposing simple distributed algorithms for QKD networks generating secret
keys.
  There are significant advantages of pumping entangled photons in QKD
networks, but challenges arise in practical implementations. The underlying
channels are inherently time-varying, and thus data rates fluctuate between
nodes. Moreover, multiple edges (node pairs) can be pumped simultaneously,
albeit at the cost of a reduced secret key rate (SKR). These temporal and
spatial constraints yield a complex decision-making problem whose solutions may
favor a small set of user pairs to the detriment of overall, long-run network
performance.
  We design adaptive pumping strategies that address these challenges in QKD
networks. In particular, we find that a proportional fairness pumping strategy
(PF-PS) stands out by dynamically prioritizing users with lower average secret
key rates and optimally balancing fairness with throughput. The proposed
algorithm is a natural extension to quantum networks of the Proportional Fair
Scheduler deployed in 4G LTE and 5G mobile networks. Both theoretical analysis
and numerical simulations confirm that PF-PS is optimal for entangled state
distribution, and thus, when adapted appropriately, proportional fair pumping
is a strong candidate for efficient resource allocation in quantum networks.

</details>


### [130] [Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification](https://arxiv.org/abs/2507.14116)
*Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner*

Main category: quant-ph

TL;DR: 本文提出了一种改进的并行量子退火方法，用于在监督学习设置中训练量子玻尔兹曼机（QBMs），显著减少了训练时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 量子退火器生成的样本遵循玻尔兹曼分布，但训练QBMs需要大量量子处理单元（QPU）时间，限制了其在NISQ时代的应用。本文旨在通过改进的并行量子退火技术降低成本。

Method: 采用改进的并行量子退火方法，在监督学习设置中训练QBMs，节省了编码输入的量子比特，并在MedMNIST数据集上测试。

Result: 实验表明，改进的QBMs在较少训练周期内取得了与类似规模卷积神经网络（CNNs）相当的结果，并行退火技术比常规方法提速近70%。

Conclusion: 改进的并行量子退火技术显著提升了QBMs的训练效率，使其更接近实际应用。

Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently
follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann
Machines (QBMs) have gained increasing popularity in the quantum research
community. While they harbor great promises for quantum speed-up, their usage
currently stays a costly endeavor, as large amounts of QPU time are required to
train them. This limits their applicability in the NISQ era. Following the idea
of No\`e et al. (2024), who tried to alleviate this cost by incorporating
parallel quantum annealing into their unsupervised training of QBMs, this paper
presents an improved version of parallel quantum annealing that we employ to
train QBMs in a supervised setting. Saving qubits to encode the inputs, the
latter setting allows us to test our approach on medical images from the
MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world
applicability of the technology. Our experiments show that QBMs using our
approach already achieve reasonable results, comparable to those of
similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller
numbers of epochs than these classical models. Our parallel annealing technique
leads to a speed-up of almost 70 % compared to regular annealing-based BM
executions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [Differential Privacy in Kernelized Contextual Bandits via Random Projections](https://arxiv.org/abs/2507.13639)
*Nikola Pavlovic,Sudeep Salgia,Qing Zhao*

Main category: stat.ML

TL;DR: 论文提出了一种在差分隐私约束下的上下文核赌博算法，实现了最优的累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究在差分隐私约束下，上下文核赌博问题的解决方案，确保查询序列对上下文和奖励序列的隐私保护。

Method: 提出了一种基于私有核岭回归估计器的新算法，结合私有协方差估计和私有随机投影。

Result: 算法在联合和局部差分隐私模型下分别实现了最优的累积遗憾。

Conclusion: 新算法通过降低敏感性并保持高预测精度，实现了最优性能。

Abstract: We consider the problem of contextual kernel bandits with stochastic
contexts, where the underlying reward function belongs to a known Reproducing
Kernel Hilbert Space. We study this problem under an additional constraint of
Differential Privacy, where the agent needs to ensure that the sequence of
query points is differentially private with respect to both the sequence of
contexts and rewards. We propose a novel algorithm that achieves the
state-of-the-art cumulative regret of
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$
and
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$
over a time horizon of $T$ in the joint and local models of differential
privacy, respectively, where $\gamma_T$ is the effective dimension of the
kernel and $\varepsilon_{\mathrm{DP}} > 0$ is the privacy parameter. The key
ingredient of the proposed algorithm is a novel private kernel-ridge regression
estimator which is based on a combination of private covariance estimation and
private random projections. It offers a significantly reduced sensitivity
compared to its classical counterpart while maintaining a high prediction
accuracy, allowing our algorithm to achieve the state-of-the-art performance
guarantees.

</details>


### [132] [Conformal Data Contamination Tests for Trading or Sharing of Data](https://arxiv.org/abs/2507.13835)
*Martin V. Vejling,Shashi Raj Pandey,Christophe A. N. Biscio,Petar Popovski*

Main category: stat.ML

TL;DR: 提出了一种分布无关、污染感知的数据共享框架，通过新颖的两样本测试方法识别对模型个性化最有价值的外部数据代理。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习任务中本地数据有限的问题，同时确保外部数据的质量，避免污染或不相关数据的影响。

Method: 引入基于严格理论基础的共形异常检测的两样本测试方法，称为共形数据污染测试，用于判断数据是否超过污染阈值。

Result: 在多种协作学习场景中验证了方法的鲁棒性和有效性，能够控制错误发现率。

Conclusion: 共形数据污染测试是一种通用的数据聚合方法，提供统计上严格的质量保证。

Abstract: The amount of quality data in many machine learning tasks is limited to what
is available locally to data owners. The set of quality data can be expanded
through trading or sharing with external data agents. However, data buyers need
quality guarantees before purchasing, as external data may be contaminated or
irrelevant to their specific learning task. Previous works primarily rely on
distributional assumptions about data from different agents, relegating quality
checks to post-hoc steps involving costly data valuation procedures. We propose
a distribution-free, contamination-aware data-sharing framework that identifies
external data agents whose data is most valuable for model personalization. To
achieve this, we introduce novel two-sample testing procedures, grounded in
rigorous theoretical foundations for conformal outlier detection, to determine
whether an agent's data exceeds a contamination threshold. The proposed tests,
termed conformal data contamination tests, remain valid under arbitrary
contamination levels while enabling false discovery rate control via the
Benjamini-Hochberg procedure. Empirical evaluations across diverse
collaborative learning scenarios demonstrate the robustness and effectiveness
of our approach. Overall, the conformal data contamination test distinguishes
itself as a generic procedure for aggregating data with statistically rigorous
quality guarantees.

</details>


### [133] [A Survey of Dimension Estimation Methods](https://arxiv.org/abs/2507.13887)
*James A. D. Binnie,Paweł Dłotko,John Harvey,Jakub Malinowski,Ka Man Yim*

Main category: stat.ML

TL;DR: 本文综述了多种高维数据集内在维度估计方法，分类并评估了其性能，指出了过拟合和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据集通常具有低维内在结构，但缺乏可靠的维度估计方法指导。

Method: 将维度估计方法分为三类：基于局部仿射结构的切线估计、依赖维度概率分布的参数估计，以及利用拓扑或度量不变性的估计。

Result: 评估了方法在曲率、噪声、超参数选择、样本量等方面的表现，发现许多方法存在过拟合问题。

Conclusion: 许多维度估计方法在基准数据集上表现良好，但泛化能力有限，需进一步改进。

Abstract: It is a standard assumption that datasets in high dimension have an internal
structure which means that they in fact lie on, or near, subsets of a lower
dimension. In many instances it is important to understand the real dimension
of the data, hence the complexity of the dataset at hand. A great variety of
dimension estimators have been developed to find the intrinsic dimension of the
data but there is little guidance on how to reliably use these estimators.
  This survey reviews a wide range of dimension estimation methods,
categorising them by the geometric information they exploit: tangential
estimators which detect a local affine structure; parametric estimators which
rely on dimension-dependent probability distributions; and estimators which use
topological or metric invariants.
  The paper evaluates the performance of these methods, as well as
investigating varying responses to curvature and noise. Key issues addressed
include robustness to hyperparameter selection, sample size requirements,
accuracy in high dimensions, precision, and performance on non-linear
geometries. In identifying the best hyperparameters for benchmark datasets,
overfitting is frequent, indicating that many estimators may not generalise
well beyond the datasets on which they have been tested.

</details>


### [134] [Conformalized Regression for Continuous Bounded Outcomes](https://arxiv.org/abs/2507.14023)
*Zhanli Wu,Fabrizio Leisen,F. Javier Rubio*

Main category: stat.ML

TL;DR: 论文提出了一种基于转换模型和Beta回归的保形预测区间方法，用于处理有界连续结果的回归问题，解决了现有方法在点预测或渐近区间预测上的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决有界连续结果回归问题中的预测挑战，特别是在点预测和渐近区间预测之外的更有效方法的需求。

Method: 基于转换模型和Beta回归，设计非一致性度量，考虑异方差性，并验证保形预测的渐近有效性。

Result: 理论和模拟研究表明，该方法在有限样本和模型误设情况下均能提供有效的预测覆盖。

Conclusion: 提出的保形预测区间在实际数据和模拟中表现优异，优于基于Bootstrap的替代方法。

Abstract: Regression problems with bounded continuous outcomes frequently arise in
real-world statistical and machine learning applications, such as the analysis
of rates and proportions. A central challenge in this setting is predicting a
response associated with a new covariate value. Most of the existing
statistical and machine learning literature has focused either on point
prediction of bounded outcomes or on interval prediction based on asymptotic
approximations. We develop conformal prediction intervals for bounded outcomes
based on transformation models and beta regression. We introduce tailored
non-conformity measures based on residuals that are aligned with the underlying
models, and account for the inherent heteroscedasticity in regression settings
with bounded outcomes. We present a theoretical result on asymptotic marginal
and conditional validity in the context of full conformal prediction, which
remains valid under model misspecification. For split conformal prediction, we
provide an empirical coverage analysis based on a comprehensive simulation
study. The simulation study demonstrates that both methods provide valid
finite-sample predictive coverage, including settings with model
misspecification. Finally, we demonstrate the practical performance of the
proposed conformal prediction intervals on real data and compare them with
bootstrap-based alternatives.

</details>


### [135] [Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design](https://arxiv.org/abs/2507.14057)
*Marcel Hedman,Desi R. Ivanova,Cong Guan,Tom Rainforth*

Main category: stat.ML

TL;DR: Step-DAD是一种半摊销、基于策略的贝叶斯实验设计方法，通过动态更新策略提升灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有完全摊销策略在实验过程中固定不变的问题，提升实验设计的适应性和鲁棒性。

Method: Step-DAD在实验前训练设计策略，并在实验过程中动态更新策略以适应具体实验实例。

Result: Step-DAD在决策能力和鲁棒性上优于现有贝叶斯实验设计方法。

Conclusion: Step-DAD通过动态策略更新显著提升了实验设计的灵活性和鲁棒性。

Abstract: We develop a semi-amortized, policy-based, approach to Bayesian experimental
design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,
fully amortized, policy-based BED approaches, Step-DAD trains a design policy
upfront before the experiment. However, rather than keeping this policy fixed,
Step-DAD periodically updates it as data is gathered, refining it to the
particular experimental instance. This test-time adaptation improves both the
flexibility and the robustness of the design strategy compared with existing
approaches. Empirically, Step-DAD consistently demonstrates superior
decision-making and robustness compared with current state-of-the-art BED
methods.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [136] [Physics-guided impact localisation and force estimation in composite plates with uncertainty quantification](https://arxiv.org/abs/2507.13376)
*Dong Xiao,Zahra Sharif-Khodaei,M. H. Aliabadi*

Main category: physics.data-an

TL;DR: 提出了一种结合物理模型和机器学习的混合框架，用于复合材料板的冲击定位和力估计，减少了实验数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决复合材料结构中冲击识别的问题，尤其是在实验数据稀缺时，提高准确性和泛化能力。

Method: 结合一阶剪切变形理论（FSDT）、机器学习和不确定性量化，通过物理模型进行数据增强和自适应正则化。

Result: 在复合材料板实验中验证了框架的准确性、鲁棒性，并减少了对大规模训练数据的依赖。

Conclusion: 该方法为复合材料航空结构的冲击监测和健康管理提供了可扩展和可转移的解决方案。

Abstract: Physics-guided approaches offer a promising path toward accurate and
generalisable impact identification in composite structures, especially when
experimental data are sparse. This paper presents a hybrid framework for impact
localisation and force estimation in composite plates, combining a data-driven
implementation of First-Order Shear Deformation Theory (FSDT) with machine
learning and uncertainty quantification. The structural configuration and
material properties are inferred from dispersion relations, while boundary
conditions are identified via modal characteristics to construct a low-fidelity
but physically consistent FSDT model. This model enables physics-informed data
augmentation for extrapolative localisation using supervised learning.
Simultaneously, an adaptive regularisation scheme derived from the same model
improves the robustness of impact force reconstruction. The framework also
accounts for uncertainty by propagating localisation uncertainty through the
force estimation process, producing probabilistic outputs. Validation on
composite plate experiments confirms the framework's accuracy, robustness, and
efficiency in reducing dependence on large training datasets. The proposed
method offers a scalable and transferable solution for impact monitoring and
structural health management in composite aerostructures.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [137] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: 本文研究了Vision Mamba模型中图像扫描顺序对MRI分割性能的影响，提出了一种无参数模块MS2D，并通过大规模实验证明扫描顺序是显著影响性能的因素。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba模型在医学图像处理中依赖1D序列化2D图像，但扫描顺序的设计选择未被充分研究，尤其在具有强解剖先验的MRI中。

Method: 引入MS2D模块，支持多种扫描路径的探索，并在三个公共数据集上对21种扫描策略进行了大规模实验。

Result: 扫描顺序对性能有显著影响（Friedman检验：χ²=43.9, p=0.0016），性能差异可达27 Dice点，连续扫描路径表现最佳。

Conclusion: 扫描顺序是无需额外成本的高效超参数，研究提供了优化路径的实证建议。

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [138] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: 深度学习通过合成图像训练提升了神经影像分析的泛化能力，但计算需求增加。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在神经影像分析中因训练数据范围狭窄导致的泛化能力不足问题。

Method: 采用域随机化策略，通过合成随机强度和内容的图像训练深度神经网络。

Result: 模型能够处理未见过的图像类型，无需重新训练或微调，适用于多种影像模态。

Conclusion: 合成驱动训练范式有望提升深度学习工具的通用性，但需权衡计算成本。

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [139] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: 提出了一种名为D2IP的新框架，用于3D时间序列成像，通过三种策略（UPWS、TPP和轻量级重建骨干网络）显著提升了计算效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督学习方法（如DIP）在复杂3D或时间序列成像任务中计算成本高的问题。

Method: 引入UPWS、TPP和3D-FastResUNet三种策略，加速收敛、增强时间一致性并提升计算效率。

Result: 在模拟和临床数据集上，D2IP实现了快速且准确的3D时间序列EIT重建，图像质量提升（MSSIM增加24.8%，ERR降低8.1%），计算时间减少7.1倍。

Conclusion: D2IP在临床动态肺部成像中具有显著潜力，能够高效且高质量地完成任务。

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [140] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: UGPL是一种不确定性引导的渐进学习框架，通过全局到局部分析提升CT图像分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理病理特征的细微和空间多样性，UGPL旨在解决这一问题。

Method: 采用证据深度学习量化预测不确定性，通过非极大值抑制机制提取信息丰富的区域，结合自适应融合机制。

Result: 在三个CT数据集上，UGPL在肾脏异常、肺癌和COVID-19检测中的准确率分别提高了3.29%、2.46%和8.08%。

Conclusion: UGPL通过不确定性引导和渐进学习显著提升了CT图像分类性能。

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>
