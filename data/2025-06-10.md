<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.LG](#cs.LG) [Total: 196]
- [quant-ph](#quant-ph) [Total: 4]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.IR](#cs.IR) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 35]
- [eess.SP](#eess.SP) [Total: 19]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.SI](#cs.SI) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: KRAMABENCH是一个包含104个真实数据科学管道的基准测试，用于评估AI系统在数据处理、发现、清理和编排方面的能力。尽管现有模型在明确任务上表现良好，但在复杂管道构建中仍需改进。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在设计和执行复杂数据科学管道中的能力，填补现有模型在真实场景中应用的不足。

Method: 通过KRAMABENCH基准测试，评估5种通用模型和3种代码生成模型在数据处理、发现、清理和编排任务中的表现。

Result: 现有模型在明确任务上表现良好，但在需要广泛数据处理和领域知识的复杂管道构建中表现不足。

Conclusion: KRAMABENCH是开发自主数据科学代理的重要一步，未来需进一步改进模型能力。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


### [2] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: 论文提出了一种基于LLM的SQL查询重写方法QUITE，解决了规则方法的局限性，通过多智能体框架和实时反馈显著提升了查询性能。


<details>
  <summary>Details</summary>
Motivation: 规则基础的SQL查询重写方法存在局限性，难以处理新查询模式，而人类专家和LLM的语义推理能力为解决这一问题提供了可能。

Method: 设计了基于有限状态机的多智能体框架，结合外部工具和实时数据库反馈，并采用提示注入技术优化查询执行计划。

Result: QUITE将查询执行时间减少35.8%，生成的重写查询比现有方法多24.1%，覆盖了更多查询模式。

Conclusion: QUITE通过LLM和多智能体框架，显著提升了SQL查询重写的性能和覆盖范围，超越了传统规则方法。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Generating representative macrobenchmark microservice systems from distributed traces with Palette](https://arxiv.org/abs/2506.06448)
*Vaastav Anand,Matheus Stolet,Jonathan Mace,Antoine Kaufmann*

Main category: cs.DC

TL;DR: 论文提出了一种利用分布式追踪数据生成代表性微服务系统的方法，以解决研究中缺乏真实系统的问题。


<details>
  <summary>Details</summary>
Motivation: 由于研究人员和实践者通常无法访问具有代表性的微服务系统，只能使用不具代表性的简化替代方案，这限制了研究的有效性。

Method: 引入了一种基于图形因果模型（GCMs）的系统拓扑抽象方法，结合分支概率、执行顺序和执行时间，并通过Palette系统从分布式追踪数据生成代表性微服务基准。

Result: 该方法能够生成具有代表性且灵活的微服务系统基准，解决了现有替代方案的不足。

Conclusion: 利用分布式追踪数据和GCMs可以高效生成代表性微服务系统，为研究和实践提供了更可靠的基准。

Abstract: Microservices are the dominant design for developing cloud systems
  today. Advancements for microservice need to be evaluated in representative
systems, e.g. with matching scale, topology, and execution patterns.
  Unfortunately in practice, researchers and practitioners alike often do not
have access to representative systems. Thus they have to resort to sub-optimal
non-representative alternatives, e.g. small and oversimplified synthetic
benchmark systems or simulated system models instead.
  To solve this issue, we propose the use of distributed trace datasets,
available from large internet companies,
  to generate representative microservice systems.
  To do so, we introduce a novel abstraction of a system topology which uses
Graphical Causal Models (GCMs)
  to model the underlying system by incorporating the branching probabilities,
execution order of outgoing
  calls to every dependency, and execution times.
  We then incorporate this topology in Palette, a system that generates
  representative flexible macrobenchmarks microservice systems from distributed
traces.

</details>


### [4] [Performance Impact of Containerized METADOCK 2 on Heterogeneous Platforms](https://arxiv.org/abs/2506.06450)
*Antonio Jesús Banegas-Luna,Baldomero Imbernón Tudela,Carlos Martínez-Cortés,José María Cecilia,Horacio Pérez-Sánchez*

Main category: cs.DC

TL;DR: 研究评估了容器化技术对METADOCK 2在高性能计算平台上的性能影响，结果显示性能开销可忽略不计（<1%），且容器化部署具有便携性和可扩展性优势。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选（VS）是药物发现中计算密集的过程，需要大量资源分析化学库和预测配体-蛋白质相互作用。研究旨在评估容器化技术在高性能计算平台上的性能影响。

Method: 通过测试Docker、Singularity和Apptainer三种容器化技术在不同CPU和GPU配置下的表现，评估METADOCK 2的性能。

Result: 容器化引入的性能开销低于1%，METADOCK 2能高效处理大分子复合物，优于AutoDock Vina等商业工具。

Conclusion: 容器化的METADOCK 2是异构高性能计算平台上虚拟筛选任务的稳健高效解决方案。

Abstract: Virtual screening (VS) is a computationally intensive process crucial for
drug discovery, often requiring significant resources to analyze large chemical
libraries and predict ligand-protein interactions. This study evaluates the
performance impact of containerization on METADOCK 2, a high-throughput docking
software when deployed on heterogeneous high-performance computing (HPC)
platforms. By testing three containerization technologies - Docker,
Singularity, and Apptainer - across varying CPU and GPU configurations, the
experiments reveal that containerization introduces negligible performance
overhead, with deviations below 1%. Moreover, METADOCK 2 demonstrated the
capability to efficiently process large molecular complexes, surpassing the
limitations of commercial tools such as AutoDock Vina. The results underscore
the advantages of container-based deployment for ensuring portability,
reproducibility, and scalability in scientific computing. This study concludes
that containerized METADOCK 2 is a robust and efficient solution for VS tasks
on heterogeneous HPC platforms.

</details>


### [5] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: TERAIO是一个基于SSD的GPU内存扩展框架，通过优化张量卸载/预取计划提升大型语言模型训练性能。


<details>
  <summary>Details</summary>
Motivation: 观察到训练中活跃张量仅占GPU内存的1.7%，非活跃张量占用大量内存且长时间未使用，为卸载/预取提供了机会。

Method: 利用前几次迭代分析张量生命周期，生成优化卸载/预取计划，并通过PyTorch集成到编译程序中，使用GPUDirect存储直接迁移张量。

Result: 相比现有技术（如ZeRO-Offload和ZeRO-Infinity），TERAIO平均提升训练性能1.47倍，达到理想性能的80.7%。

Conclusion: TERAIO通过高效利用SSD扩展GPU内存，显著提升了LLM训练效率。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [6] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: pFedSOP利用二阶优化加速个性化联邦学习，减少通信轮次并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在数据异构性下模型泛化能力不足的问题，同时减少现有PFL方法因一阶优化导致的通信轮次增加和本地计算负担。

Method: 使用基于Gompertz函数的归一化角度计算个性化梯度更新，并通过正则化Fisher信息矩阵近似Hessian矩阵，实现二阶优化。

Result: 在异构图像分类数据集上，pFedSOP优于现有FL和PFL算法。

Conclusion: pFedSOP通过二阶优化有效加速训练并减少通信轮次，同时避免额外数据输入，提升了性能。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


### [7] [Addressing tokens dynamic generation, propagation, storage and renewal to secure the GlideinWMS pilot based jobs and system](https://arxiv.org/abs/2506.07379)
*Bruno Moreira Coimbra,Marco Mambelli*

Main category: cs.DC

TL;DR: GlideinWMS从X.509过渡到支持令牌，面临挑战并改进凭证模块以支持动态生成、存储和失效机制。


<details>
  <summary>Details</summary>
Motivation: 适应更广泛的令牌采用，满足实验和资源的新需求，提升基础设施安全性。

Method: 设计新凭证模块，支持动态生成、类型化、用途化和不同流程，并加入存储、续订和失效机制。

Result: 实现了按需生成定制化凭证，支持最小权限原则，并提升了实验需求的服务能力。

Conclusion: GlideinWMS通过改进凭证管理，更好地支持了实验需求并提升了安全性。

Abstract: GlideinWMS has been one of the first middleware in the WLCG community to
transition from X.509 to support also tokens. The first step was to get from
the prototype in 2019 to using tokens in production in 2022. This paper will
present the challenges introduced by the wider adoption of tokens and the
evolution plans for securing the pilot infrastructure of GlideinWMS and
supporting the new requirements. In the last couple of years, the GlideinWMS
team supported the migration of experiments and resources to tokens. Inadequate
support in the current infrastructure, more stringent requirements, and the
higher spatial and temporal granularity forced GlideinWMS to revisit once more
how credentials are generated, used, and propagated. The new credential modules
have been designed to be used in multiple systems (GlideinWMS, HEPCloud) and
use a model where credentials have type, purpose, and different flows.
Credentials are dynamically generated in order to customize the duration and
limit the scope to the targeted resource. This allows to enforce the least
privilege principle. Finally, we also considered adding credential storage,
renewal, and invalidation mechanisms within the GlideinWMS infrastructure to
better serve the experiments' needs.

</details>


### [8] [New Limits on Distributed Quantum Advantage: Dequantizing Linear Programs](https://arxiv.org/abs/2506.07574)
*Alkida Balliu,Corinna Coupette,Antonio Cruciani,Francesco d'Amore,Massimo Equi,Henrik Lievonen,Augusto Modanese,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 论文提出了两个结果，限制了分布式量子计算在LOCAL模型中的优势。首先，证明了线性规划问题中不存在量子优势；其次，展示了量子-LOCAL在LCL问题中弱于经典SLOCAL模型。


<details>
  <summary>Details</summary>
Motivation: 研究分布式量子计算在LOCAL模型中的局限性，探索量子与经典算法的性能边界。

Method: 通过构造性证明，展示量子算法在特定问题中无法超越经典算法，并利用此结果进一步比较量子-LOCAL与SLOCAL模型。

Result: 1. 线性规划问题中量子-LOCAL无优势；2. 量子-LOCAL在LCL问题中弱于SLOCAL。

Conclusion: 量子-LOCAL在特定问题中无法超越经典算法，且与SLOCAL模型存在性能差异，扩展了对分布式量子计算局限性的理解。

Abstract: In this work, we give two results that put new limits on distributed quantum
advantage in the context of the LOCAL model of distributed computing. First, we
show that there is no distributed quantum advantage for any linear program. Put
otherwise, if there is a quantum-LOCAL algorithm $\mathcal{A}$ that finds an
$\alpha$-approximation of some linear optimization problem $\Pi$ in $T$
communication rounds, we can construct a classical, deterministic LOCAL
algorithm $\mathcal{A}'$ that finds an $\alpha$-approximation of $\Pi$ in $T$
rounds. As a corollary, all classical lower bounds for linear programs,
including the KMW bound, hold verbatim in quantum-LOCAL. Second, using the
above result, we show that there exists a locally checkable labeling problem
(LCL) for which quantum-LOCAL is strictly weaker than the classical
deterministic SLOCAL model. Our results extend from quantum-LOCAL also to
finitely dependent and non-signaling distributions, and one of the corollaries
of our work is that the non-signaling model and the SLOCAL model are
incomparable in the context of LCL problems: By prior work, there exists an LCL
problem for which SLOCAL is strictly weaker than the non-signaling model, and
our work provides a separation in the opposite direction.

</details>


### [9] [A Terminology for Scientific Workflow Systems](https://arxiv.org/abs/2506.07838)
*Frédéric Sutera,Tainã Coleman,İlkay Altintaş,Rosa M. Badia,Bartosz Balis,Kyle Chard,Iacopo Colonnelli,Ewa Deelman,Paolo Di Tommaso,Thomas Fahringer,Carole Goble,Shantenu Jha,Daniel S. Katz,Johannes Köster,Ulf Leser,Kshitij Mehta,Hilary Oliver,J. -Luc Peterson,Giovanni Pizzi,Loïc Pottier,Raül Sirvent,Eric Suchyta,Douglas Thain,Sean R. Wilkinson,Justin M. Wozniak,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文总结了科学工作流管理系统（WMS）的多样性问题，并提出了一种基于社区的新术语来分类和描述WMS，以帮助研究人员选择合适的系统。


<details>
  <summary>Details</summary>
Motivation: 科学工作流管理系统的多样性导致研究人员在选择合适的系统时面临困难，缺乏统一的标准。

Method: 通过社区合作开发了一套术语体系，包括五个轴：工作流特性、组合、编排、数据管理和元数据捕获，并对23个现有WMS进行了分类。

Result: 提出了一个标准化的术语框架，用于描述和分类WMS，帮助研究人员更好地理解和选择系统。

Conclusion: 新术语框架为科学工作流管理系统的选择和比较提供了实用工具，促进了社区的标准化和协作。

Abstract: The term scientific workflow has evolved over the last two decades to
encompass a broad range of compositions of interdependent compute tasks and
data movements. It has also become an umbrella term for processing in modern
scientific applications. Today, many scientific applications can be considered
as workflows made of multiple dependent steps, and hundreds of workflow
management systems (WMSs) have been developed to manage and run these
workflows. However, no turnkey solution has emerged to address the diversity of
scientific processes and the infrastructure on which they are implemented.
Instead, new research problems requiring the execution of scientific workflows
with some novel feature often lead to the development of an entirely new WMS. A
direct consequence is that many existing WMSs share some salient features,
offer similar functionalities, and can manage the same categories of workflows
but also have some distinct capabilities. This situation makes researchers who
develop workflows face the complex question of selecting a WMS. This selection
can be driven by technical considerations, to find the system that is the most
appropriate for their application and for the resources available to them, or
other factors such as reputation, adoption, strong community support, or
long-term sustainability. To address this problem, a group of WMS developers
and practitioners joined their efforts to produce a community-based terminology
of WMSs. This paper summarizes their findings and introduces this new
terminology to characterize WMSs. This terminology is composed of fives axes:
workflow characteristics, composition, orchestration, data management, and
metadata capture. Each axis comprises several concepts that capture the
prominent features of WMSs. Based on this terminology, this paper also presents
a classification of 23 existing WMSs according to the proposed axes and terms.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Efficient Computation of Closed Substrings](https://arxiv.org/abs/2506.06452)
*Samkith K Jain,Neerja Mhaskar*

Main category: cs.DS

TL;DR: 论文提出了一种快速且实用的算法，用于计算字符串中的所有闭合子串，并提供了紧凑表示方法。同时，给出了计算所有最大闭合子串的简单高效方案，并在斐波那契词中精确计算了其数量。


<details>
  <summary>Details</summary>
Motivation: 研究闭合子串的计算问题，旨在提供高效的算法和紧凑的表示方法，以解决现有方法在时间和空间上的不足。

Method: 引入了一种O(n log n)时间的算法，利用紧凑表示法计算所有闭合子串；同时使用后缀数组（SA）和最长公共前缀（LCP）数组计算最大闭合子串（MCSs）。

Result: 算法在O(n log n)时间和O(n log n)空间内完成计算；在斐波那契词中，MCSs的数量约为1.382倍的斐波那契数。

Conclusion: 提出的算法高效且实用，适用于闭合子串的计算问题，并在特定字符串（如斐波那契词）中验证了其有效性。

Abstract: A closed string $u$ is either of length one or contains a border that occurs
only as a prefix and as a suffix in $u$ and nowhere else within $u$. In this
paper, we present a fast and practical $O(n\log n)$ time algorithm to compute
all $\Theta(n^2)$ closed substrings by introducing a compact representation for
all closed substrings of a string $ w[1..n]$, using only $O(n \log n)$ space.
We also present a simple and space-efficient solution to compute all maximal
closed substrings (MCSs) using the suffix array ($\mathsf{SA}$) and the longest
common prefix ($\mathsf{LCP}$) array of $w[1..n]$. Finally, we show that the
exact number of MCSs ($M(f_n)$) in a Fibonacci word $ f_n $, for $n \geq 5$, is
$\approx \left(1 + \frac{1}{\phi^2}\right) F_n \approx 1.382 F_n$, where $ \phi
$ is the golden ratio.

</details>


### [11] [Sample and Expand: Discovering Low-rank Submatrices With Quality Guarantees](https://arxiv.org/abs/2506.06456)
*Martino Ciaperoni,Aristides Gionis,Heikki Mannila*

Main category: cs.DS

TL;DR: 论文提出了一种两阶段方法，用于发现矩阵中接近低秩的子矩阵，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵通常并非整体低秩，而是局部存在低秩子矩阵，因此需要一种方法来发现这些子矩阵。

Method: 采用两阶段方法：首先通过采样发现接近低秩的小子矩阵，然后扩展这些子矩阵并保持其低秩近似性。

Result: 实验表明，该方法在发现接近低秩子矩阵方面优于现有方法。

Conclusion: 该方法有效解决了矩阵中局部低秩子矩阵的发现问题，具有实际应用价值。

Abstract: The problem of approximating a matrix by a low-rank one has been extensively
studied. This problem assumes, however, that the whole matrix has a low-rank
structure. This assumption is often false for real-world matrices. We consider
the problem of discovering submatrices from the given matrix with bounded
deviations from their low-rank approximations. We introduce an effective
two-phase method for this task: first, we use sampling to discover small nearly
low-rank submatrices, and then they are expanded while preserving proximity to
a low-rank approximation. An extensive experimental evaluation confirms that
the method we introduce compares favorably to existing approaches.

</details>


### [12] [Modern Minimal Perfect Hashing: A Survey](https://arxiv.org/abs/2506.06536)
*Hans-Peter Lehmann,Thomas Mueller,Rasmus Pagh,Giulio Ermanno Pibiri,Peter Sanders,Sebastiano Vigna,Stefan Walzer*

Main category: cs.DS

TL;DR: 本文综述了完美哈希函数的最新进展，包括空间效率、构建时间和查询时间等关键参数，并提供了实验评估以指导应用选择。


<details>
  <summary>Details</summary>
Motivation: 自1997年以来的研究进展需要总结，以帮助研究者了解完美哈希函数的最新发展及其应用。

Method: 综述了不同方法在空间、构建时间和查询时间上的权衡，并进行了实验评估。

Result: 现代完美哈希函数在查询速度、空间效率和扩展性方面表现优异，部分方法接近理论空间下限。

Conclusion: 本文为研究者提供了完美哈希函数的全面概述，并可作为应用选择的参考。

Abstract: Given a set $S$ of $n$ keys, a perfect hash function for $S$ maps the keys in
$S$ to the first $m \geq n$ integers without collisions. It may return an
arbitrary result for any key not in $S$ and is called minimal if $m = n$. The
most important parameters are its space consumption, construction time, and
query time. Years of research now enable modern perfect hash functions to be
extremely fast to query, very space-efficient, and scale to billions of keys.
Different approaches give different trade-offs between these aspects. For
example, the smallest constructions get within 0.1% of the space lower bound of
$\log_2(e)$ bits per key. Others are particularly fast to query, requiring only
one memory access. Perfect hashing has many applications, for example to avoid
collision resolution in static hash tables, and is used in databases,
bioinformatics, and stringology.
  Since the last comprehensive survey in 1997, significant progress has been
made. This survey covers the latest developments and provides a starting point
for getting familiar with the topic. Additionally, our extensive experimental
evaluation can serve as a guide to select a perfect hash function for use in
applications.

</details>


### [13] [Online Job Assignment](https://arxiv.org/abs/2506.06893)
*Farbod Ekbatani,Yiding Feng,Ian Kash,Rad Niazadeh*

Main category: cs.DS

TL;DR: 研究了一种在线分配问题，设计了一种新算法FLB，通过惩罚函数和调整奖励实现最优竞争比。


<details>
  <summary>Details</summary>
Motivation: 主要应用于云计算中的资源分配问题，目标是最大化奖励并保持与全知基准的竞争力。

Method: 设计了FLB算法，结合惩罚函数和未来时间奖励调整，使用对偶拟合技术和归纳论证。

Result: FLB算法实现了ln(RD)+3lnln(max(R,D))+O(1)的竞争比，且参数依赖最优。

Conclusion: FLB算法在异构环境下具有最优竞争力，分析技术可能具有独立价值。

Abstract: Motivated primarily by applications in cloud computing, we study a simple,
yet powerful, online allocation problem in which jobs of varying durations
arrive over continuous time and must be assigned immediately and irrevocably to
one of the available offline servers. Each server has a fixed initial capacity,
with assigned jobs occupying one unit for their duration and releasing it upon
completion. The algorithm earns a reward for each assignment upon completion.
We consider a general heterogeneous setting where both the reward and duration
of a job depend on the job-server pair. The objective of the online algorithm
is to maximize the total collected reward, and remain competitive against an
omniscient benchmark that knows all job arrivals in advance. Our main
contribution is the design of a new online algorithm, termed Forward-Looking
BALANCE (FLB), and using primal-dual framework to establish that it is
(asymptotically) optimal-competitive.
  This meta-algorithm has two main primitives: (i) keeping track of the
capacity used for each server at each time and applying a penalty function to
this quantity, and (ii) adjusting the reward of assigning a job to a server by
subtracting the total penalty of a particularly chosen subset of future times,
in contrast to just looking at the current time. The FLB algorithm then assigns
the arriving job to the server with the maximum adjusted reward. If R and D are
the ratios of maximum over minimum rewards and durations, we show that the FLB
algorithm obtains an asymptotic competitive ratio of
ln(RD)+3lnln(max(R,D))+O(1). We further show this bound has optimal
dependencies on all the parameters. Our main analysis combines a novel
dual-fitting technique, which leverages the configuration LP benchmark for this
problem, and a novel inductive argument to establish the capacity feasibility
of the algorithm, which might be of independent interest.

</details>


### [14] [On Sketching Trimmed Statistics](https://arxiv.org/abs/2506.07342)
*Honghao Lin,Hoai-An Nguyen,David P. Woodruff*

Main category: cs.DS

TL;DR: 论文提出了空间高效的线性草图方法，用于估计频率向量的修剪统计量，如最大k个频率的F_p矩或k-修剪向量。该方法适用于流式和分布式场景，并在不同条件下提供近似保证。


<details>
  <summary>Details</summary>
Motivation: 修剪统计量在鲁棒估计中有广泛应用（如R语言中的trim.var函数），但现有方法在时间和空间效率上不足。线性草图能提升效率并适应流式数据。

Method: 提出线性草图方法，针对不同k值（如k≥n/poly log n或一般k）设计空间复杂度条件，并提供(1±ε)近似保证。扩展至p>2及相关问题。

Result: 在k≥n/poly log n时，使用poly(1/ε, log n)空间实现近似；对一般k，提出必要条件并证明其最优性。实验显示比Count-Sketch更省空间。

Conclusion: 论文为修剪统计量的草图方法提供了理论框架和实际优化，扩展了应用场景并提升了效率。

Abstract: We present space-efficient linear sketches for estimating trimmed statistics
of an $n$-dimensional frequency vector $x$, e.g., the sum of $p$-th powers of
the largest $k$ frequencies (i.e., entries) in absolute value, or the
$k$-trimmed vector, which excludes the top and bottom $k$ frequencies. This is
called the $F_p$ moment of the trimmed vector. Trimmed measures are used in
robust estimation, as seen in the R programming language's `trim.var' function
and the `trim' parameter in the mean function. Linear sketches improve time and
memory efficiency and are applicable to streaming and distributed settings. We
initiate the study of sketching these statistics and give a new condition for
capturing their space complexity. When $k \ge n/poly\log n$, we give a linear
sketch using $poly(1/\varepsilon, \log n)$ space which provides a $(1 \pm
\varepsilon)$ approximation to the top-$k$ $F_p$ moment for $p \in [0,2]$. For
general $k$, we give a sketch with the same guarantees under a condition
relating the $k$-th largest frequency to the tail mass, and show this condition
is necessary. For the $k$-trimmed version, our sketch achieves optimal error
guarantees under the same condition. We extend our methods to $p > 2$ and also
address related problems such as computing the $F_p$ moment of frequencies
above a threshold, finding the largest $k$ such that the $F_p$ moment of the
top $k$ exceeds $k^{p+1}$, and the $F_p$ moment of the top $k$ frequencies such
that each entry is at least $k$. Notably, our algorithm for this third
application improves upon the space bounds of the algorithm of Govindan,
Monemizadeh, and Muthukrishnan (PODS '17) for computing the $h$-index. We show
empirically that our top $k$ algorithm uses much less space compared to
Count-Sketch while achieving the same error.

</details>


### [15] [On Deterministically Finding an Element of High Order Modulo a Composite](https://arxiv.org/abs/2506.07668)
*Ziv Oznovich,Ben Lee Volk*

Main category: cs.DS

TL;DR: 本文提出了一种确定性算法，用于在给定合数$N$和目标阶数$D \ge N^{1/6}$时，在时间$D^{1/2+o(1)}$内找到$\mathbb{Z}_N^*$中阶数至少为$D$的元素或$N$的非平凡因子。


<details>
  <summary>Details</summary>
Motivation: 改进Hittmeir的算法，后者在更强的假设$D \ge N^{2/5}$下运行，并在确定性整数分解算法中发挥了关键作用。

Method: 提出了一种确定性算法，适用于更宽松的条件$D \ge N^{1/6}$，并在$N$具有$r$-幂因子时进一步放宽条件。

Result: 算法在时间$D^{1/2+o(1)}$内成功找到目标元素或因子，优于Hittmeir的算法。

Conclusion: 该算法在更宽松的条件下提供了高效的计算保证，扩展了确定性整数分解的应用范围。

Abstract: We give a deterministic algorithm that, given a composite number $N$ and a
target order $D \ge N^{1/6}$, runs in time $D^{1/2+o(1)}$ and finds either an
element $a \in \mathbb{Z}_N^*$ of multiplicative order at least $D$, or a
nontrivial factor of $N$. Our algorithm improves upon an algorithm of Hittmeir
(arXiv:1608.08766), who designed a similar algorithm under the stronger
assumption $D \ge N^{2/5}$. Hittmeir's algorithm played a crucial role in the
recent breakthrough deterministic integer factorization algorithms of Hittmeir
and Harvey (arXiv:2006.16729, arXiv:2010.05450, arXiv:2105.11105). When $N$ is
assumed to have an $r$-power divisor with $r\ge 2$, our algorithm provides the
same guarantees assuming $D \ge N^{1/6r}$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Enhancing Software Supply Chain Security Through STRIDE-Based Threat Modelling of CI/CD Pipelines](https://arxiv.org/abs/2506.06478)
*Sowmiya Dhandapani*

Main category: cs.SE

TL;DR: 该研究通过结构化威胁建模方法，识别并缓解CI/CD生命周期中的风险，结合STRIDE框架分析漏洞，提出基于安全标准和SLSA框架的控制措施，并评估其成熟度。


<details>
  <summary>Details</summary>
Motivation: 随着CI/CD管道的广泛应用，保障软件供应链安全成为DevOps团队的关键挑战。

Method: 采用STRIDE框架对代表性CI/CD管道（如GitHub、Jenkins、Docker、Kubernetes）进行威胁建模，分析各阶段漏洞，并基于NIST、OWASP和SLSA标准提出安全控制措施。

Result: 研究提出了一套基于安全代码和Shift Left-Shield Right原则的实用安全工具链集成策略，提升管道安全。

Conclusion: 该方法为增强CI/CD管道安全性提供了实用路线图，以应对不断演变的软件供应链威胁。

Abstract: With the increasing adoption of Continuous Integration and Continuous
Deployment pipelines, securing software supply chains has become a critical
challenge for modern DevOps teams. This study addresses these challenges by
applying a structured threat modeling approach to identify and mitigate risks
throughout the CI/CD lifecycle. By modeling a representative pipeline
architecture incorporating tools such as GitHub, Jenkins, Docker, and
Kubernetes and applying the STRIDE framework, we systematically analyze
vulnerabilities at each stage, from source code management to deployment.
Threats are documented and mapped to comprehensive security controls drawn from
standards like NIST SP 800-218, OWASP Top 10 CI/CD risks, and the SLSA
framework. Controls are further evaluated against SLSA maturity levels to
assess improvements in trust and provenance. To operationalize these findings,
the study outlines a practical security toolchain integration strategy grounded
in Security as Code and Shift Left-Shield Right principles, enabling automated,
enforceable security across the pipeline. This approach provides a pragmatic
roadmap for enhancing CI/CD pipeline security against evolving software supply
chain threats.

</details>


### [17] [Information-Theoretic Detection of Unusual Source Code Changes](https://arxiv.org/abs/2506.06508)
*Adriano Torres,Sebastian Baltes,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 该论文从信息论角度测量开源项目源代码的信息内容，定义文本和结构熵，并通过实证评估验证其与经典代码复杂度指标的关系，同时展示熵在异常检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究源代码信息内容的测量方法，探索熵作为衡量代码复杂度的新维度。

Method: 定义文本熵和结构熵，分析95个开源项目的熵演化模式，并与经典复杂度指标进行统计关系评估。

Result: 熵能捕捉与经典指标不同的复杂度维度，异常检测精度超过60%。

Conclusion: 熵为静态评估程序复杂度提供了新方法，为代码演化研究奠定基础。

Abstract: The code base of software projects evolves essentially through inserting and
removing information to and from the source code. We can measure this evolution
via the elements of information - tokens, words, nodes - of the respective
representation of the code. In this work, we approach the measurement of the
information content of the source code of open-source projects from an
information-theoretic standpoint. Our focus is on the entropy of two
fundamental representations of code: tokens and abstract syntax tree nodes,
from which we derive definitions of textual and structural entropy. We proceed
with an empirical assessment where we evaluate the evolution patterns of the
entropy of 95 actively maintained open source projects. We calculate the
statistical relationships between our derived entropy metrics and classic
methods of measuring code complexity and learn that entropy may capture
different dimensions of complexity than classic metrics. Finally, we conduct
entropy-based anomaly detection of unusual changes to demonstrate that our
approach may effectively recognise unusual source code change events with over
60% precision, and lay the groundwork for improvements to information-theoretic
measurement of source code evolution, thus paving the way for a new approach to
statically gauging program complexity throughout its development.

</details>


### [18] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: 论文研究了私有GPT自动生成可执行测试代码的能力，通过需求（如验收标准）作为输入，探索了直接生成代码和通过Gherkin语法中间步骤生成代码的效果。结果表明两步法在代码可读性和最佳实践方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用私有GPT基于需求自动生成高质量的测试代码，为产品负责人或业务智能提供直接生成可测试标准的方法。

Method: 采用两种方式生成测试代码：i) 直接从需求生成代码，ii) 通过Gherkin语法中间步骤生成。评估了两种方法在简单程序和复杂模型中的效果。

Result: 两步法（通过Gherkin语法）在代码可读性和最佳实践方面优于直接生成代码。结构化提示能产生更高质量的测试输出。

Conclusion: 通过中间步骤（如Gherkin语法）生成测试代码能显著提升质量，结构化提示是关键因素。

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [19] [Mind the Gap: A Readability-Aware Metric for Test Code Complexity](https://arxiv.org/abs/2506.06764)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 论文提出了一种针对单元测试的认知复杂度度量CCTR，结合了结构和语义特征，优于传统度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有复杂度度量（如Cognitive Complexity）对测试代码适用性不足，尤其是LLM和搜索工具生成的测试代码。

Method: 引入CCTR度量，整合断言密度、注解角色和测试组合模式等特征。

Result: CCTR能有效区分结构化与碎片化测试套件，评分更符合开发者感知的复杂度。

Conclusion: CCTR为测试代码评估和改进提供了更可靠的基础，并公开了数据和脚本以支持复现。

Abstract: Automatically generated unit tests-from search-based tools like EvoSuite or
LLMs-vary significantly in structure and readability. Yet most evaluations rely
on metrics like Cyclomatic Complexity and Cognitive Complexity, designed for
functional code rather than test code. Recent studies have shown that
SonarSource's Cognitive Complexity metric assigns near-zero scores to
LLM-generated tests, yet its behavior on EvoSuite-generated tests and its
applicability to test-specific code structures remain unexplored. We introduce
CCTR, a Test-Aware Cognitive Complexity metric tailored for unit tests. CCTR
integrates structural and semantic features like assertion density, annotation
roles, and test composition patterns-dimensions ignored by traditional
complexity models but critical for understanding test code. We evaluate 15,750
test suites generated by EvoSuite, GPT-4o, and Mistral Large-1024 across 350
classes from Defects4J and SF110. Results show CCTR effectively discriminates
between structured and fragmented test suites, producing interpretable scores
that better reflect developer-perceived effort. By bridging structural analysis
and test readability, CCTR provides a foundation for more reliable evaluation
and improvement of generated tests. We publicly release all data, prompts, and
evaluation scripts to support replication.

</details>


### [20] [Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings with Structural and Semantic Awareness](https://arxiv.org/abs/2506.06767)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CTSES是一种新的复合指标，用于评估LLM自动重构单元测试的效果，结合了CodeBLEU、METEOR和ROUGE-L，能更准确地平衡行为保留、词汇质量和结构对齐。


<details>
  <summary>Details</summary>
Motivation: 传统指标如CodeBLEU对重命名和结构编辑过于敏感，而基于嵌入的相似性忽略了可读性和模块化，因此需要一种更全面的评估方法。

Method: 引入CTSES指标，并在两个Java基准数据集（Defects4J和SF110）上，使用GPT-4o和Mistral-Large-2407自动重构5000多个测试套件进行评估。

Result: CTSES比现有指标更能忠实、可解释地评估重构效果，更符合开发者预期和人类直觉。

Conclusion: CTSES为LLM自动重构单元测试提供了一种更可靠的评估标准。

Abstract: Large Language Models (LLMs) are increasingly employed to automatically
refactor unit tests, aiming to enhance readability, naming, and structural
clarity while preserving functional behavior. However, evaluating such
refactorings remains challenging: traditional metrics like CodeBLEU are overly
sensitive to renaming and structural edits, whereas embedding-based
similarities capture semantics but ignore readability and modularity. We
introduce CTSES, a composite metric that integrates CodeBLEU, METEOR, and
ROUGE-L to balance behavior preservation, lexical quality, and structural
alignment. CTSES is evaluated on over 5,000 test suites automatically
refactored by GPT-4o and Mistral-Large-2407, using Chain-of-Thought prompting,
across two established Java benchmarks: Defects4J and SF110. Our results show
that CTSES yields more faithful and interpretable assessments, better aligned
with developer expectations and human intuition than existing metrics.

</details>


### [21] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 论文探讨了将机器学习训练管道部署到生产环境中的挑战，并通过SPIRA项目的三个架构版本演变，展示了如何提升软件质量属性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过改进架构设计，提升机器学习训练管道在生产环境中的可维护性、鲁棒性和可扩展性。

Method: 比较了SPIRA项目的三个架构版本：从混乱的大泥球到模块化单体，再到微服务，并采用不同的设计原则和模式。

Result: 通过架构演变，显著提升了训练管道的软件质量属性，为ML工程师和数据科学家提供了实践参考。

Conclusion: 论文为生产化ML训练管道和采用MLOps实践提供了有价值的见解。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [22] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 该研究通过开发量子电路重构问题的分类法，利用大语言模型（LLMs）分析Qiskit版本迁移中的重构需求，并整合专家和LLM的分类结果，为量子软件工程提供结构化框架。


<details>
  <summary>Details</summary>
Motivation: 量子计算软件的异构性和快速更新导致代码重构问题复杂化，传统软件工程方法难以应对，需要新的解决方案。

Method: 研究通过分析Qiskit文档和发布说明，创建重构需求的初始分类法，并比较专家和LLM生成的分类法，最终整合为统一分类法。

Result: 生成了两种分类法（专家和LLM），并整合为统一分类法，为AI辅助迁移和自动化重构技术评估提供基础。

Conclusion: 统一分类法为量子软件工程的研究和实践提供了结构化框架，有助于优化开发流程和提升兼容性。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [23] [GUIPilot: A Consistency-based Mobile GUI Testing Approach for Detecting Application-specific Bugs](https://arxiv.org/abs/2506.07385)
*Ruofan Liu,Xiwen Teoh,Yun Lin,Guanjie Chen,Ruofei Ren,Denys Poshyvanyk,Jin Song Dong*

Main category: cs.SE

TL;DR: GUIPilot是一种检测移动设计与其实现之间不一致性的方法，通过抽象屏幕为小部件容器并优化对齐问题，以及验证GUI转换，实现了高精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 移动设计通常包含设计模型和预期行为，但实现中可能存在不一致性，需要自动化工具检测。

Method: GUIPilot将屏幕抽象为小部件容器，定义小部件顺序和操作成本，优化对齐问题；同时通过视觉语言模型推断小部件操作，验证GUI转换。

Result: 在80个移动应用和160个设计模型上，GUIPilot检测屏幕不一致的精度为94.5%，召回率为99.6%，过程不一致检测零错误。

Conclusion: GUIPilot高效检测设计实现不一致性，工业案例中成功发现并确认了多个应用错误。

Abstract: In this work, we propose GUIPilot, an approach for detecting inconsistencies
between the mobile design and their implementations. The mobile design usually
consists of design mock-ups that specify (1) the expected screen appearances
(e.g., widget layouts, colors, and shapes) and (2) the expected screen
behaviors, regarding how one screen can transition into another (e.g., labeled
widgets with textual description). Given a design mock-up and the
implementation of its application, GUIPilot reports both their screen
inconsistencies as well as process inconsistencies. On the one hand, GUIPilot
detects the screen inconsistencies by abstracting every screen into a widget
container where each widget is represented by its position, width, height, and
type. By defining the partial order of widgets and the costs of replacing,
inserting, and deleting widgets in a screen, we convert the screen-matching
problem into an optimizable widget alignment problem. On the other hand, we
translate the specified GUI transition into stepwise actions on the mobile
screen (e.g., click, long-press, input text on some widgets). To this end, we
propose a visual prompt for the vision-language model to infer widget-specific
actions on the screen. By this means, we can validate the presence or absence
of expected transitions in the implementation. Our extensive experiments on 80
mobile applications and 160 design mock-ups show that (1) GUIPilot can achieve
94.5% precision and 99.6% recall in detecting screen inconsistencies,
outperforming the state-of-the-art approach, such as GVT, by 66.2% and 56.6%
respectively, and (2) GUIPilot reports zero errors in detecting process
inconsistencies. Furthermore, our industrial case study on applying GUIPilot on
a trading mobile application shows that GUIPilot has detected nine application
bugs, and all the bugs were confirmed by the original application experts.

</details>


### [24] [Generate Realistic Test Scenes for V2X Communication Systems](https://arxiv.org/abs/2506.07419)
*An Guo,Xinyu Gao,Chunrong Fang,Haoxiang Tian,Weisong Sun,Yanzhou Mu,Shuncheng Tang,Lei Ma,Zhenyu Chen*

Main category: cs.SE

TL;DR: V2XGen是一个自动化测试生成工具，用于V2X协同感知系统，通过高保真方法生成真实场景并提升测试效率。


<details>
  <summary>Details</summary>
Motivation: 解决V2X协同感知系统测试中手动数据收集和标注的高成本与耗时问题。

Method: V2XGen采用高保真方法生成真实对象实例，并结合适应性场景生成策略优化测试效率。

Result: 实验表明V2XGen能生成真实测试场景，有效检测错误行为，并提升系统检测精度。

Conclusion: V2XGen为V2X协同感知系统的测试提供了高效且可靠的自动化解决方案。

Abstract: Accurately perceiving complex driving environments is essential for ensuring
the safe operation of autonomous vehicles. With the tremendous progress in deep
learning and communication technologies, cooperative perception with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
the limitations of single-agent perception systems in perceiving distant
objects and occlusions. Despite the considerable advancements, V2X cooperative
perception systems require thorough testing and continuous enhancement of
system performance. Given that V2X driving scenes entail intricate
communications with multiple vehicles across various geographic locations,
creating V2X test scenes for these systems poses a significant challenge.
Moreover, current testing methodologies rely on manual data collection and
labeling, which are both time-consuming and costly.
  In this paper, we design and implement V2XGen, an automated testing
generation tool for V2X cooperative perception systems. V2XGen utilizes a
high-fidelity approach to generate realistic cooperative object instances and
strategically place them within the background data in crucial positions.
Furthermore, V2XGen adopts a fitness-guided V2X scene generation strategy for
the transformed scene generation process and improves testing efficiency. We
conduct experiments on V2XGen using multiple cooperative perception systems
with different fusion schemes to assess its performance on various tasks. The
experimental results demonstrate that V2XGen is capable of generating realistic
test scenes and effectively detecting erroneous behaviors in different
V2X-oriented driving conditions. Furthermore, the results validate that
retraining systems under test with the generated scenes can enhance average
detection precision while reducing occlusion and long-range perception errors.

</details>


### [25] [A Framework for Creating Non-Regressive Test Cases via Branch Consistency Analysis Driven by Descriptions](https://arxiv.org/abs/2506.07486)
*Yuxiang Zhang,Pengyu Xue,Zhen Yang,Xiaoxue Ren,Xiang Li,Linhao Wu,Jiancheng Zhao,Xingda Yu*

Main category: cs.SE

TL;DR: 论文提出DISTINCT框架，通过自然语言描述引导LLM生成测试用例，显著提升缺陷检测率和代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成工具假设目标方法正确，但实践中目标方法可能存在缺陷，需改进缺陷检测能力。

Method: 构建新基准Defects4J-Desc和QuixBugs-Desc，提出DISTINCT框架，包含生成器、验证器和分析器三部分。

Result: DISTINCT在编译成功率、通过率和缺陷检测率上显著提升，代码覆盖率平均提高3.77%-5.36%。

Conclusion: DISTINCT为回归测试生成设定了新基准，展示了自然语言描述引导LLM在缺陷检测中的潜力。

Abstract: Automated test-generation research overwhelmingly assumes the correctness of
focal methods, yet practitioners routinely face non-regression scenarios where
the focal method may be defective. A baseline evaluation of EvoSuite and two
leading Large Language Model (LLM)-based generators, namely ChatTester and
ChatUniTest, on defective focal methods reveals that despite achieving up to
83% of branch coverage, none of the generated tests expose defects.
  To resolve this problem, we first construct two new benchmarks, namely
Defects4J-Desc and QuixBugs-Desc, for experiments. In particular, each focal
method is equipped with an extra Natural Language Description (NLD) for code
functionality understanding.
  Subsequently, we propose DISTINCT, a Description-guided, branch-consistency
analysis framework that transforms LLMs into fault-aware test generators.
DISTINCT carries three iterative components: (1) a Generator that derives
initial tests based on the NLDs and the focal method, (2) a Validator that
iteratively fixes uncompilable tests using compiler diagnostics, and (3) an
Analyzer that iteratively aligns test behavior with NLD semantics via
branch-level analysis.
  Extensive experiments confirm the effectiveness of our approach. Compared to
state-of-the-art methods, DISTINCT achieves an average improvement of 14.64% in
Compilation Success Rate (CSR) and 6.66% in Passing Rate (PR) across both
benchmarks. It notably enhances Defect Detection Rate (DDR) on both benchmarks,
with a particularly significant gain of 149.26% observed on Defects4J-Desc. In
terms of code coverage, DISTINCT improves Statement Coverage (SC) by an average
of 3.77% and Branch Coverage (BC) by 5.36%. These results set a new baseline
for non-regressive test generation and highlight how description-driven
reasoning enables LLMs to move beyond coverage chasing toward effective defect
detection.

</details>


### [26] [Large Language Models for Multilingual Vulnerability Detection: How Far Are We?](https://arxiv.org/abs/2506.07503)
*Honglin Shu,Michael Fu,Junji Yu,Dong Wang,Chakkrit Tantithamthavorn,Junjie Chen,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 该论文通过评估预训练语言模型（PLMs）和大语言模型（LLMs）在多语言和多粒度漏洞检测中的表现，发现GPT-4o在指令微调和少样本提示的增强下显著优于其他模型，尤其在检测高危漏洞方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定编程语言（如C/C++）和函数级漏洞检测，缺乏对PLMs和LLMs在多语言和多粒度场景下的全面评估。本文旨在填补这一空白。

Method: 使用超过30,000个真实漏洞修复补丁，覆盖七种编程语言，系统评估模型在函数级和行级的表现。

Result: GPT-4o在指令微调和少样本提示的增强下显著优于其他模型，LLM方法在多语言漏洞检测中表现优异，尤其是高危漏洞。

Conclusion: LLMs在多语言漏洞检测中展现出巨大潜力，尤其在函数级和行级检测中优于PLMs，为解决实际软件安全问题提供了新思路。

Abstract: Various deep learning-based approaches utilizing pre-trained language models
(PLMs) have been proposed for automated vulnerability detection. With recent
advancements in large language models (LLMs), several studies have begun
exploring their application to vulnerability detection tasks. However, existing
studies primarily focus on specific programming languages (e.g., C/C++) and
function-level detection, leaving the strengths and weaknesses of PLMs and LLMs
in multilingual and multi-granularity scenarios largely unexplored. To bridge
this gap, we conduct a comprehensive fine-grained empirical study evaluating
the effectiveness of state-of-the-art PLMs and LLMs for multilingual
vulnerability detection. Using over 30,000 real-world vulnerability-fixing
patches across seven programming languages, we systematically assess model
performance at both the function-level and line-level. Our key findings
indicate that GPT-4o, enhanced through instruction tuning and few-shot
prompting, significantly outperforms all other evaluated models, including
CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability
in detecting unique multilingual vulnerabilities, particularly excelling in
identifying the most dangerous and high-severity vulnerabilities. These results
underscore the promising potential of adopting LLMs for multilingual
vulnerability detection at function-level and line-level, revealing their
complementary strengths and substantial improvements over PLM approaches. This
first empirical evaluation of PLMs and LLMs for multilingual vulnerability
detection highlights LLMs' value in addressing real-world software security
challenges.

</details>


### [27] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest是一个API中心化的压力测试框架，用于系统性地发现LLM代理中的意图完整性违规问题。


<details>
  <summary>Details</summary>
Motivation: LLM代理通过自然语言指令调用API时，常因用户意图误解导致行为偏离目标，传统测试方法无法处理自然语言的模糊性。

Method: IntenTest基于工具包文档生成真实任务，并通过语义分区和轻量级预测器指导测试，同时利用数据类型感知的策略记忆提高效率。

Result: 在80个工具包API上的实验表明，IntenTest能有效发现意图违规，显著优于基线方法。

Conclusion: IntenTest不仅对小规模LLM生成测试具有良好泛化能力，还能适应跨领域API的演变。

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [28] [Evaluating LLMs Effectiveness in Detecting and Correcting Test Smells: An Empirical Study](https://arxiv.org/abs/2506.07594)
*E. G. Santana Jr,Jander Pereira Santos Junior,Erlon P. Almeida,Iftekhar Ahmed,Paulo Anselmo da Mota Silveira Neto,Eduardo Santana de Almeida*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLMs）在识别和重构测试代码中的异味（test smells）方面的表现，发现Gemini表现最佳，但也存在一些挑战。


<details>
  <summary>Details</summary>
Motivation: 测试异味会降低代码的可维护性和可靠性，但现有工具主要关注检测而非自动化重构，LLMs的潜力尚未充分探索。

Method: 使用GPT-4-Turbo、LLaMA 3 70B和Gemini-1.5 Pro对Python和Java测试套件进行评估，结合PyNose和TsDetect进行初始检测，随后进行LLM驱动的重构。

Result: Gemini在检测准确率（Python 74.35%，Java 80.32%）和重构效果上表现最佳，但所有模型在重构时可能引入新异味。Gemini还提高了测试覆盖率，而其他模型则降低了覆盖率。

Conclusion: LLMs在自动化测试异味重构方面具有潜力，Gemini表现最强，但仍存在语言和异味类型的挑战。

Abstract: Test smells indicate poor development practices in test code, reducing
maintainability and reliability. While developers often struggle to prevent or
refactor these issues, existing tools focus primarily on detection rather than
automated refactoring. Large Language Models (LLMs) have shown strong potential
in code understanding and transformation, but their ability to both identify
and refactor test smells remains underexplored. We evaluated GPT-4-Turbo, LLaMA
3 70B, and Gemini-1.5 Pro on Python and Java test suites, using PyNose and
TsDetect for initial smell detection, followed by LLM-driven refactoring.
Gemini achieved the highest detection accuracy (74.35\% Python, 80.32\% Java),
while LLaMA was lowest. All models could refactor smells, but effectiveness
varied, sometimes introducing new smells. Gemini also improved test coverage,
unlike GPT-4 and LLaMA, which often reduced it. These results highlight LLMs'
potential for automated test smell refactoring, with Gemini as the strongest
performer, though challenges remain across languages and smell types.

</details>


### [29] [Leveraging Network Methods for Hub-like Microservice Detection](https://arxiv.org/abs/2506.07683)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文提出了一种检测微服务架构中Hub-like反模式的鲁棒方法，通过多种网络中心检测技术，发现Kirkley基于Erdos-Renyi编码的方法最准确。


<details>
  <summary>Details</summary>
Motivation: Hub-like反模式缺乏明确的定义和检测方法，本文旨在找到一种高精度的检测方法。

Method: 利用25个微服务网络数据集，结合无标度属性、中心性指标、聚类系数、最小描述长度原则及Arcan工具的方法进行检测。

Result: 研究发现微服务网络无标度性不显著，多数检测方法结果不一致，Kirkley的Erdos-Renyi编码方法在检测数量和精度上最优。

Conclusion: Kirkley的方法最具潜力，建议更新Arcan工具或采用归一化度中心性方法，为未来研究提供方向。

Abstract: Context: Microservice Architecture is a popular architectural paradigm that
facilitates flexibility by decomposing applications into small, independently
deployable services. Catalogs of architectural anti-patterns have been proposed
to highlight the negative aspects of flawed microservice design. In particular,
the Hub-like anti-pattern lacks an unambiguous definition and detection method.
Aim: In this work, we aim to find a robust detection approach for the Hub-like
microservice anti-pattern that outputs a reasonable number of Hub-like
candidates with high precision. Method: We leveraged a dataset of 25
microservice networks and several network hub detection techniques to identify
the Hub-like anti-pattern, namely scale-free property, centrality metrics and
clustering coefficient, minimum description length principle, and the approach
behind the Arcan tool. Results and Conclusion: Our findings revealed that the
studied architectural networks are not scale-free, that most considered hub
detection approaches do not agree on the detected hubs, and that the method by
Kirkley leveraging the Erdos-Renyi encoding is the most accurate one in terms
of the number of detected hubs and the detection precision. Investigating
further the applicability of these methods to detecting Hub-like components in
microservice-based and other systems opens up new research directions.
Moreover, our results provide an evaluation of the approach utilized by the
widely used Arcan tool and highlight the potential to update the tool to use
the normalized degree centrality of a component in the network, or for the
approach based on ER encoding to be adopted instead.

</details>


### [30] [Centrality Change Proneness: an Early Indicator of Microservice Architectural Degradation](https://arxiv.org/abs/2506.07690)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究探讨了时间中心性指标是否能通过关联或影响软件指标，为微服务架构退化的早期检测提供见解。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的广泛采用需要识别模式和反模式以防止架构退化，时间网络的研究为分析动态网络提供了新方法。

Method: 重构了一个包含42个服务的开源微服务项目的7个版本架构，计算每个服务在每个版本中的软件和中心性指标，并探索了指标间的相关性。

Result: 确定了7个大小和5个复杂性指标与中心性有稳定相关性，而中心性变化倾向性不影响软件指标，可作为架构退化的早期指标。

Conclusion: 时间中心性指标为微服务架构退化提供了新的视角和早期检测方法。

Abstract: Over the past decade, the wide adoption of Microservice Architecture has
required the identification of various patterns and anti-patterns to prevent
Microservice Architectural Degradation. Frequently, the systems are modelled as
a network of connected services. Recently, the study of temporal networks has
emerged as a way to describe and analyze evolving networks. Previous research
has explored how software metrics such as size, complexity, and quality are
related to microservice centrality in the architectural network. This study
investigates whether temporal centrality metrics can provide insight into the
early detection of architectural degradation by correlating or affecting
software metrics. We reconstructed the architecture of 7 releases of an OSS
microservice project with 42 services. For every service in every release, we
computed the software and centrality metrics. From one of the latter, we
derived a new metric, Centrality Change Proneness. We then explored the
correlation between the metrics. We identified 7 size and 5 complexity metrics
that have a consistent correlation with centrality, while Centrality Change
Proneness did not affect the software metrics, thus providing yet another
perspective and an early indicator of microservice architectural degradation.

</details>


### [31] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 该研究提出了一个综合的小型语言模型（SLM）生命周期框架，通过整合学术和实践资源，填补了现有研究的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 随着对高效可部署语言模型需求的增长，小型语言模型（SLMs）受到关注，但现有研究缺乏统一的生命周期视角。

Method: 通过对36项研究进行综合分析，分类并提取了生命周期相关技术。

Result: 提出了一个模块化的生命周期模型，包含主要、可选和跨领域组件，支持方法重用和生命周期意识。

Conclusion: 该框架为SLM的开发与维护提供了统一基础，连接理论与实践，并指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


### [32] [Adversarial Attack Classification and Robustness Testing for Large Language Models for Code](https://arxiv.org/abs/2506.07942)
*Yang Liu,Armstrong Foundjem,Foutse Khomh,Heng Li*

Main category: cs.SE

TL;DR: 该研究探讨了自然语言输入中的对抗性扰动对代码生成大语言模型（LLM4Code）的影响，发现词级扰动最具威胁，而句级扰动影响较小。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，确保其对对抗性输入的鲁棒性变得至关重要，尤其是自然语言输入中的扰动可能导致不安全的代码生成。

Method: 研究采用混合方法，结合定量性能指标和定性漏洞分析，对字符、词和句级扰动进行分类评估。

Result: 词级扰动对模型影响最大，暴露了语义漏洞；句级扰动影响最小；字符级扰动效果因模型而异。

Conclusion: 研究提出了一个测试LLM4Code鲁棒性的框架，并强调提升模型对语义级扰动的抵抗力对安全代码生成的重要性。

Abstract: Large Language Models (LLMs) have become vital tools in software development
tasks such as code generation, completion, and analysis. As their integration
into workflows deepens, ensuring robustness against vulnerabilities especially
those triggered by diverse or adversarial inputs becomes increasingly
important. Such vulnerabilities may lead to incorrect or insecure code
generation when models encounter perturbed task descriptions, code, or
comments. Prior research often overlooks the role of natural language in
guiding code tasks. This study investigates how adversarial perturbations in
natural language inputs including prompts, comments, and descriptions affect
LLMs for Code (LLM4Code). It examines the effects of perturbations at the
character, word, and sentence levels to identify the most impactful
vulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and
datasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial
attacks. The first dimension classifies the input type code, prompts, or
comments while the second dimension focuses on granularity: character, word, or
sentence-level changes. We adopted a mixed-methods approach, combining
quantitative performance metrics with qualitative vulnerability analysis.
LLM4Code models show varying robustness across perturbation types.
Sentence-level attacks were least effective, suggesting models are resilient to
broader contextual changes. In contrast, word-level perturbations posed serious
challenges, exposing semantic vulnerabilities. Character-level effects varied,
showing model sensitivity to subtle syntactic deviations.Our study offers a
structured framework for testing LLM4Code robustness and emphasizes the
critical role of natural language in adversarial evaluation. Improving model
resilience to semantic-level disruptions is essential for secure and reliable
code-generation systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP是一个用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了性能并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 高内涵筛选数据（如Cell Painting）的语义与自然图像差异大，且不同扰动类型难以在同一潜在空间中表示，因此需要专门的方法。

Method: 结合预训练图像编码器、新型通道编码方案和自然语言编码器，学习统一的潜在空间以对齐扰动及其形态效应。

Result: CellCLIP在跨模态检索和生物学相关任务中表现最佳，同时显著减少计算时间。

Conclusion: CellCLIP为高内涵筛选数据提供了一种高效的跨模态对比学习解决方案。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [34] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出了一种基于学习和图神经网络的方法，用于加速混合整数线性规划（MILP）求解器的初始解生成，减少计算时间和方差。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划（MILP）在规划和调度问题中应用广泛，但计算时间长限制了其在大规模实时场景中的使用。

Method: 结合行为克隆（BC）和强化学习（RL）训练图神经网络（GNN），为MILP求解器生成高质量初始解。

Result: 实验表明，该方法减少了优化时间和方差，同时保持解的质量和可行性。

Conclusion: 该框架为MILP求解器提供了高效的初始解生成方法，适用于多智能体任务分配和调度问题。

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [35] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为Mutual-Taught的自训练方法，通过迭代优化策略模型（PM）和奖励模型（RM）来解决分布偏移问题，无需额外人工标注。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）的偏好优化过程中，新生成的模型样本与奖励模型训练数据之间的分布偏移会降低RM的效果，进而影响PM的性能。

Method: Mutual-Taught方法模仿期望最大化（EM）算法：E步用当前RM反馈更新PM，M步用PM输出更新RM。

Result: 实验表明，该方法持续改进模型性能，8B策略模型在AlpacaEval-2上达到54.1%的胜率，8B奖励模型在RewardBench上与GPT-4o-2024-08-06表现相当。

Conclusion: Mutual-Taught通过迭代优化PM和RM，有效解决了分布偏移问题，提升了模型性能。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [36] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 研究利用持久同调构建银行关系网络，结合传统借贷网络形成异构网络，提升信用评级预测准确性。


<details>
  <summary>Details</summary>
Motivation: 银行信用评级对经济稳定和决策至关重要，但完整的银行间连接图因隐私问题难以获取，限制了图神经网络的应用。

Method: 使用持久同调构建银行关系网络，结合传统借贷网络形成异构网络，提出HTGNN模型。

Result: 在真实全球数据集上验证了HTGNN的有效性，提升了预测准确性。

Conclusion: 研究为投资者和监管机构提供了改进风险缓解和市场干预的工具。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [37] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: GLProtein是一种蛋白质预训练框架，结合全局结构相似性和局部氨基酸细节，提升预测精度和功能理解。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质序列分析已取得进展，但整合蛋白质结构信息仍有潜力，尤其是从局部氨基酸到全局蛋白质结构相似性的信息。

Method: GLProtein创新地结合了蛋白质掩码建模、三重结构相似性评分、蛋白质3D距离编码和基于子结构的氨基酸分子编码。

Result: 实验表明，GLProtein在蛋白质-蛋白质相互作用预测、接触预测等任务中优于现有方法。

Conclusion: GLProtein为蛋白质预训练提供了新思路，整合全局和局部信息，显著提升了预测性能。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [38] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: dLLM-Cache是一种无需训练的缓存框架，通过结合长间隔提示缓存和部分响应更新，显著降低了扩散式大语言模型（dLLMs）的推理延迟，速度提升高达9.1倍。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）虽然表现出潜力，但推理延迟高，且传统加速技术（如Key-Value缓存）不适用。

Method: 基于dLLM推理中静态提示和部分动态响应的特点，提出dLLM-Cache框架，通过长间隔提示缓存和特征相似性引导的部分响应更新实现高效计算复用。

Result: 在LLaDA 8B和Dream 7B等代表性dLLMs上，dLLM-Cache实现了高达9.1倍的加速，且不牺牲输出质量。

Conclusion: dLLM-Cache显著降低了dLLMs的推理延迟，使其接近自回归模型（ARMs）的水平，代码将公开。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [39] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: Jacobi-KAN-DGCNN结合了动态图卷积神经网络和Jacobi多项式KAN，用于三维点云分类，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合KAN和DGCNN，提升点云分类的准确性和收敛速度，同时保持参数效率。

Method: 用可调单变量多项式扩展替代MLP层，简化DGCNN架构，避免深度网络结构。

Result: 在ModelNet40数据集上，Jacobi多项式KAN层在准确性和收敛速度上优于传统线性层DGCNN。

Conclusion: 高多项式阶数不一定提升性能，需进一步研究多项式基、阶数与图学习机制的交互。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [40] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过动态模型选择和分层推理策略优化语言模型的推理效率，以降低计算成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言处理任务中表现出色，但其推理过程计算成本高、能耗大，难以在资源受限的环境中部署。

Method: 提出了两种策略：(i) 路由选择，根据查询选择最合适的模型；(ii) 分层推理，通过模型序列逐步处理查询。

Result: 这些策略能够显著减少计算资源的使用，同时保持任务性能。

Conclusion: 未来研究方向包括更快的响应时间、基于任务复杂度的自适应模型选择，以及跨异构环境的可扩展部署。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [41] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 论文通过离散事件随机模拟和强化学习优化医院超声心动图检查的调度策略，动态分配资源优于静态预留策略。


<details>
  <summary>Details</summary>
Motivation: 医院超声心动图检查调度面临非确定性因素（如患者缺席、到达时间、检查时长等）和资源不对称约束，需优化资源分配以提高效率。

Method: 基于斯坦福大学医院一周运营数据预处理，构建SimPy离散事件随机模拟模型，结合Gymnasium库，比较动态分配与预留策略，并应用强化学习优化动态策略。

Result: 动态分配策略（如实时调整）在1:6胎儿与非胎儿检查室和4:2技师比例下表现更优，适应性强。强化学习策略进一步提升了效率。

Conclusion: 数据驱动的动态资源分配策略能有效提升超声实验室效率，强化学习为优化调度提供了可行方案。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [42] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 论文提出了一种通过学习多个奖励函数的分布来反映多样化人类偏好的方法，解决了当前对齐方法忽视少数观点的问题。


<details>
  <summary>Details</summary>
Motivation: 人类偏好因用户、背景和文化而异，但现有对齐方法假设单一标准，导致少数观点被忽视。

Method: 通过直接学习成对偏好的分布，无需标注者标识或预定义组，将标注者分歧视为信息性软标签，提出成对校准标准。

Result: 证明即使小规模无异常值的集合也能准确表示多样化偏好分布，并通过实验验证了训练启发式方法的有效性。

Conclusion: 该方法能更忠实地反映多元化价值观，提高了校准性。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [43] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习（FL）方法FedCGD，通过最小化设备级和样本级集体梯度发散（CGD）之和，优化了FL的收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中，数据异构性和带宽限制是影响FL性能的两大问题。现有研究多关注设备级数据异构性，而本文进一步考虑了样本级影响。

Method: 通过将设备级CGD转化为加权地球移动距离（WEMD），并结合样本级CGD的统计上界，提出FedCGD算法，在多项式时间内平衡WEMD和采样方差。

Result: 在CIFAR-10数据集上，FedCGD将分类准确率提高了4.2%，同时减少了41.8%的设备调度。

Conclusion: FedCGD通过多级CGD优化，显著提升了FL性能，并灵活权衡了WEMD和采样方差的减少。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [44] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的物理信息神经网络框架LT-PINNs，用于边界优化的工程问题，解决了传统PINNs在复杂几何中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs依赖基于密度的拓扑描述，需要手动插值且难以处理复杂几何。LT-PINNs旨在消除这些限制。

Method: 通过参数化拓扑边界曲线的控制变量为可学习参数，并引入边界条件和拓扑损失函数，实现精确边界表示。

Result: LT-PINNs在多种PDE问题中表现优异，显著降低误差，并能处理任意边界条件。

Conclusion: LT-PINNs为复杂拓扑优化提供了高效、精确的解决方案，具有广泛的应用潜力。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [45] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICRL prompting的新框架，通过多轮提示和奖励反馈，发现LLM在推理时表现出类似强化学习的行为，显著提升了任务完成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推理时是否能够表现出类似强化学习的行为，以提升任务完成效果。

Method: 提出ICRL prompting框架，通过多轮提示和奖励反馈，逐步优化LLM的响应。

Result: 在三个基准测试中，ICRL prompting显著优于基线方法，甚至在某些实验中由LLM自身生成奖励信号时仍能观察到性能提升。

Conclusion: ICRL prompting为LLM在推理时优化性能提供了新范式，展示了强化学习行为在LLM中的涌现。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [46] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 论文提出了一个统一的基准测试，比较了五种集成学习模型在葡萄酒质量评估中的表现，最终推荐了不同场景下的最佳模型。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒质量评估通常依赖主观的人工品尝，缺乏客观性和可重复性。本研究旨在通过机器学习方法实现更准确和可重复的评估。

Method: 采用泄漏控制的工作流程，包括数据分层分割、交叉验证、标准化、数据重采样、超参数搜索和特征选择。

Result: Gradient Boosting在准确率上表现最佳，但Random Forest在效率上更具优势。特征选择表明酒精等五个关键变量能捕捉大部分预测信号。

Conclusion: 推荐Random Forest作为最具成本效益的生产模型，Gradient Boosting用于离线基准测试，XGBoost和LightGBM作为GPU高效替代方案。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [47] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: ExplainBench是一个开源基准测试套件，用于系统评估局部模型解释方法，特别是在公平敏感场景下。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在高风险领域的广泛应用，对可解释和可信模型的需求增加，但缺乏标准化、可复现的评估框架。

Method: ExplainBench提供统一的解释算法封装、端到端模型训练和解释生成流程，并通过保真度、稀疏性和鲁棒性指标进行评估。

Result: 在COMPAS、UCI Adult Income和LendingClub等数据集上展示了不同解释方法的行为。

Conclusion: ExplainBench通过支持可复现的局部解释比较分析，推动了可解释机器学习的方法学基础，并促进了实际AI系统的问责制。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [48] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy是一个开源的Python自动机学习库，专注于IO行为系统的主动学习。本文介绍了其新增的被动学习领域重要方法——红蓝框架下的状态合并实现。


<details>
  <summary>Details</summary>
Motivation: 为减少状态合并算法的实现工作量，提供通用且高度可配置的红蓝框架实现，支持现有和新算法的开发。

Method: 使用通用的内部表示实现红蓝框架，用户仅需定义兼容性标准和评分即可实现状态合并算法。

Result: AALpy显著简化了状态合并算法的实现，现有算法仅需几行代码即可完成。

Conclusion: AALpy为自动机学习提供了高效的工具，支持快速实现和验证状态合并算法。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [49] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 提出了一种基于深度强化学习（DRL）的联邦学习（FL）框架，优化客户端训练数据量，提升性能并减少非独立同分布（non-IID）数据的影响。


<details>
  <summary>Details</summary>
Motivation: 解决FL中因非IID数据导致的性能下降问题，避免客户端过度共享数据。

Method: 利用DRL代理动态调整客户端训练数据量，以训练损失变化为奖励信号，优化每轮训练的数据分配。

Result: 实验表明，该方法在多个基准数据集和FL框架中表现优异。

Conclusion: 提出的DRL框架有效提升了FL性能，同时减少了非IID数据的影响。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [50] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: 本文综述了AI在能源领域的最新进展，特别是Transformer和LLMs的应用，探讨了其在预测、电网管理中的潜力，并提出了Agentic Digital Twin的概念。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在能源管理中面临泛化、情境感知和多源数据整合的挑战，而Transformer和LLMs展示了更强的能力，因此需要系统梳理其应用和发展趋势。

Method: 通过综述Transformer和LLMs的架构基础、领域适配及实际应用，分析其在能源任务中的表现和挑战。

Result: Transformer和LLMs在能源领域展现出显著潜力，尤其在复杂时间关系和多模态数据融合方面，同时提出了Agentic Digital Twin的创新概念。

Conclusion: 生成式AI（GenAI）正逐步改变能源管理，从高层规划到日常操作，未来Agentic Digital Twin有望实现更自主和智能的能源系统。

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [51] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 这篇论文首次综述了极端事件的合成数据生成方法，系统回顾了生成模型技术和大语言模型，并提出了针对极端事件的评估框架和应用领域。


<details>
  <summary>Details</summary>
Motivation: 极端事件（如市场崩溃、自然灾害和疫情）虽然罕见但破坏性极大，准确预测和早期预警可减少损失。然而，极端事件数据稀缺，传统数据驱动方法难以应对，合成数据生成成为解决方案。

Method: 论文系统回顾了生成模型技术和大语言模型，特别是通过统计理论增强的模型，以及专门用于捕捉重尾分布的训练和采样机制。

Result: 提出了一个针对极端事件的评估框架，包括统计、依赖性、可视化和任务导向的指标，并分析了这些指标在极端环境中的适用性。

Conclusion: 论文总结了关键应用领域和未充分探索的方向（如行为金融、野火、地震等），并提出了未来研究的挑战和方向。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [52] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，分析不同位置编码方法对Transformer模型表达能力、泛化能力和长序列外推能力的影响，并提出了基于正交函数的新编码方法。


<details>
  <summary>Details</summary>
Motivation: 研究不同位置编码方法如何影响Transformer模型的性能，填补Transformer理论中的关键空白，为自然语言处理、计算机视觉等领域的应用提供设计指导。

Method: 通过函数逼近定义表达能力，使用Rademacher复杂度建立泛化界限，提出基于正交函数（如小波和Legendre多项式）的新编码方法，并分析现有和提出编码方法的外推能力。

Result: 实验表明，基于正交变换的编码方法在泛化和外推能力上优于传统的正弦编码方法。

Conclusion: 该工作为Transformer模型的设计选择提供了理论支持，特别是在处理长序列任务时，正交函数编码方法更具优势。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [53] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出了一种结合非负张量分解（NTF）和生存分析的CoxNTF方法，用于生成与生存结果相关的潜在特征表示，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NMF）未结合生存信息，限制了预测能力，因此需要一种新方法提升预测性能并提供可解释性。

Method: 利用Coxnet模型的生存概率构建加权协变量张量，通过NTF生成潜在特征表示。

Result: CoxNTF在生存预测性能上与原始协变量Coxnet相当，同时提供结构化聚类框架并有效处理特征冗余。

Conclusion: CoxNTF是一种强大的生存分析工具，兼具预测性能和可解释性。

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [54] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD是一种新颖的开放世界新类别发现框架，通过结合Embedding-NeRF模型和KL散度，替代传统显式3D分割图，提升语义嵌入和视觉嵌入空间的信息聚合能力。


<details>
  <summary>Details</summary>
Motivation: 传统显式表示（如对象描述符或3D分割图）存在离散、易产生空洞和噪声的问题，限制了新类别发现的准确性。

Method: NeurNCD采用Embedding-NeRF模型和KL散度，结合特征查询、调制和聚类等关键组件，实现高效特征增强和信息交换。

Result: 在NYUv2和Replica数据集上，NeurNCD显著优于现有方法，无需密集标注数据或人工交互。

Conclusion: NeurNCD为开放世界新类别发现提供了一种高效且数据友好的解决方案。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [55] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 研究发现，分子编码器的中间层嵌入比最终层表现更好，固定嵌入平均提升5.4%，微调后平均提升8.5%，并提出了高效评估再微调的方法。


<details>
  <summary>Details</summary>
Motivation: 挑战仅依赖最终层嵌入的常规做法，探索分子编码器各层嵌入对下游任务的影响。

Method: 对五种分子编码器进行分层分析，测试22种ADMET属性预测任务，比较固定嵌入和微调效果。

Result: 中间层嵌入表现更优，固定嵌入平均提升5.4%，微调后平均提升8.5%，部分任务提升高达40.8%。

Conclusion: 充分利用分子编码器的各层嵌入可显著提升性能，同时提出高效评估方法以降低计算成本。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [56] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 论文提出SAFFRON，一种针对LLM安全保证的新型推理扩展方法，解决了传统方法在安全场景中的低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全保证研究主要关注训练阶段的对齐，但易受越狱攻击影响，且推理扩展在安全领域的潜力未被探索。

Method: 提出SAFFRON，引入多叉奖励模型（MRM）以减少奖励模型评估次数，并设计部分监督训练目标、保守探索约束和Trie缓存策略。

Result: 实验验证了SAFFRON的有效性，并公开了模型（Saffron-1）和数据集（Safety4M）。

Conclusion: SAFFRON为LLM安全提供了高效解决方案，推动了未来研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [57] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 本文通过Hessian矩阵分析诊断注意力模型的故障，发现其比梯度分析更有效。


<details>
  <summary>Details</summary>
Motivation: 随着注意力模型的规模和复杂性增加，诊断其故障变得更具挑战性，需要更有效的工具。

Method: 使用Hessian矩阵分析（曲率分析和参数交互分析）来识别注意力机制中的脆弱区域和参数依赖关系。

Result: 在三个模型（HAN、3D-CNN、DistilBERT）上的实验表明，Hessian指标比梯度更能定位不稳定性和故障源。

Conclusion: Hessian指标可显著改进复杂神经架构的故障诊断，有望提升软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [58] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM是一个结合非线性动态系统建模与深度学习的框架，通过学习潜在空间和核回归来预测时间序列，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列通常具有复杂的非线性动态，现有深度学习方法未明确建模这些动态。

Method: 结合经验动态建模（EDM）和Takens定理，提出DeepEDM框架，利用时间延迟嵌入学习潜在空间，并通过核回归近似动态。

Result: 在合成数据和真实时间序列上的实验表明，DeepEDM对输入噪声鲁棒，且预测精度优于现有方法。

Conclusion: DeepEDM成功整合了动态系统建模与深度学习，为时间序列预测提供了更准确的解决方案。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [59] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 研究提出了一种新的共识方法WISCA，用于整合机器学习模型的解释性结果，以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 在科学和高风险领域，模型的可解释性至关重要，但现有解释方法常产生冲突结果，需要共识策略来统一。

Method: 训练六个ML模型于合成数据集，使用多种模型无关解释技术，并开发WISCA方法整合类别概率和归一化归因。

Result: WISCA与最可靠的个体方法一致，验证了共识策略在提升解释可靠性中的价值。

Conclusion: 共识策略（如WISCA）能有效统一解释结果，提升机器学习模型的可解释性和可靠性。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [60] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合强化学习和智能巡航控制的框架，通过个性化驾驶行为优化婴儿睡眠质量，同时保持出行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶虽提升了安全性和舒适性，但对婴儿睡眠的影响尚未充分研究。急加速、急刹车等行为可能干扰婴儿睡眠，影响乘客舒适度和家长便利性。

Method: 结合LSTM和Transformer神经网络与强化学习，利用穿戴设备数据和车辆数据动态优化驾驶行为。

Result: 仿真结果表明，该方法显著提升了婴儿睡眠质量，同时保持了出行效率。

Conclusion: 该框架为自动驾驶中婴儿睡眠质量的优化提供了有效解决方案。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [61] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: TimeRecipe是一个统一的时间序列预测基准框架，通过模块级评估揭示设计选择对预测效果的影响，并推荐最佳模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有基准对时间序列预测模型的评估过于宏观，缺乏对设计组件有效性的深入理解，TimeRecipe旨在填补这一空白。

Method: TimeRecipe通过10,000多次实验，系统评估不同设计组件在多样化数据集、预测范围和任务设置下的表现。

Result: 实验表明，通过全面探索设计空间可以超越现有最优方法，并揭示设计选择与预测场景之间的关联。

Conclusion: TimeRecipe不仅提供了性能优越的模型，还发布了实用工具包，帮助用户根据实证结果选择合适架构。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [62] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 提出了一种无需原始训练数据的认证遗忘框架，通过替代数据集近似源数据统计特性，实现有效数据删除。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私法规的普及，从训练模型中删除私有或受版权保护信息的需求日益重要。传统遗忘方法通常需要完整训练数据集，但在源数据不可用时并不现实。

Method: 利用替代数据集近似源数据的统计特性，基于两者之间的统计距离进行可控噪声缩放，实现数据删除。

Result: 理论保证和实验验证表明，该方法在隐私敏感场景下有效且可靠，同时保持模型整体效用。

Conclusion: 该方法为无需原始训练数据的数据删除提供了可行的解决方案，具有实际应用价值。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [63] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 论文研究了在无法访问完整子类数据分布的情况下，成员推理攻击的性能，发现影子模型攻击性能急剧下降，而分位数回归攻击表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究在极端但现实的分布偏移情况下，成员推理攻击的有效性，尤其是影子模型攻击的局限性。

Method: 比较影子模型攻击和分位数回归攻击在类缺失设置下的性能，并通过理论模型分析其潜力与限制。

Result: 分位数回归攻击在CIFAR-100和ImageNet上表现优于影子模型攻击，TPR最高可达11倍。

Conclusion: 分位数回归攻击在类缺失情况下更具优势，为成员推理攻击提供了新方向。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [64] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 论文提出交替梯度流（AGF）框架，描述小初始化下两层网络的特征学习动态，统一并扩展了现有分析。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络学习特征的动态过程，填补现有理论空白。

Method: 提出AGF框架，将梯度流近似为交替两步过程：休眠神经元最大化效用函数，活跃神经元最小化成本函数。

Result: AGF量化了特征获取的顺序、时间和幅度，并在多种架构中验证了实验结果。

Conclusion: AGF为理解神经网络特征学习提供了重要进展。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [65] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ提出了一种通过质量-多样性算法生成高质量、多样化数学问题的方法，仅需单一模型，通过问题解决率衡量难度，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多样问题领域的扩展性受限，SPARQ旨在解决这一问题。

Method: 利用质量-多样性算法生成问题-解决方案对，并通过难度和多样性筛选数据。

Result: 生成2000万对数据，模型性能提升24%，多样数据增强OOD泛化能力。

Conclusion: SPARQ展示了合成数据在提升模型性能和泛化能力方面的潜力。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [66] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 论文研究了随机任务顺序下的可实现连续线性回归问题，通过两种正则化方案缩小了理论上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有未正则化方案的上界与理论下界存在显著差距，研究旨在通过正则化方案缩小或消除这一差距。

Method: 采用两种正则化方案：显式各向同性ℓ2正则化和隐式正则化（有限步预算），并将其转化为对代理损失的随机梯度下降（SGD）。

Result: 固定正则化强度可实现接近最优的收敛速率O(log k / k)，而递增正则化强度调度可达到最优速率O(1/k)。

Conclusion: 研究表明，增加正则化系数或减少每任务步数的调度在理论上是有益的。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [67] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT是一种基于FPGA的超快速CNN微调方法，适用于资源有限的IoT设备，比现有方法快17.4倍，能耗降低16.3倍。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）的训练在资源有限的IoT平台上运行时面临计算和内存的高需求挑战。

Method: 通过优化参数高效微调（PEFT）中的前向和反向计算，提出InstantFT方法。

Result: 实验表明，InstantFT比基于LoRA的方法快17.4倍，微调时间仅0.36秒，能耗降低16.3倍。

Conclusion: InstantFT能够实时适应非稳态数据分布，为IoT设备上的CNN微调提供了高效解决方案。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [68] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [69] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型（LLM）的分层协作方法，用于多无人机（UAV）在动态约束环境中的联合运动与通信控制，实验结果显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在动态和受限环境中的控制与优化是一个重大挑战，尤其是在集成地面和非地面网络（如高空平台站HAPS）的场景中。

Method: 采用分层协作的LLM框架，HAPS上的LLM负责无人机接入控制，每架无人机上的LLM处理运动规划与控制。

Result: 实验表明，该方法在系统奖励、运营成本和无人机碰撞率方面均优于基线方法。

Conclusion: 基于LLM的知识驱动范式为下一代3D空中高速公路系统的发展提供了巨大潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [70] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip是一种几何感知框架，通过梯度分布的几何变换改进DP-SGD中的梯度裁剪，提升隐私与效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中梯度裁剪阈值设置的关键挑战，现有方法未考虑梯度坐标间的相关性。

Method: 提出GeoClip，在梯度分布的几何变换基础上裁剪和扰动梯度，自适应估计变换且不增加隐私成本。

Result: GeoClip在隐私预算相同的情况下优于现有自适应裁剪方法，实验验证了其有效性。

Conclusion: GeoClip通过几何变换优化梯度裁剪，显著提升了DP-SGD的性能。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [71] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于SDN的虚假数据检测与缓解系统（FDDMS），用于车载网络，通过LSTM模型实时检测虚假数据注入攻击，并通过动态更新SDN流规则进行缓解。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和互联车辆的发展，车载网络中ECU之间的安全通信至关重要，虚假数据注入攻击可能威胁车辆安全。

Method: 提出FDDMS系统，包括解码CAN数据、构建攻击模型、使用LSTM检测攻击、提出DeepFool变体评估模型鲁棒性，并通过SDN动态更新流规则进行缓解。

Result: 实验表明FDDMS能有效实时检测和缓解虚假数据注入攻击，对抗多种对抗性攻击具有鲁棒性。

Conclusion: FDDMS为车载网络提供了一种高效、实时的虚假数据攻击检测与缓解解决方案。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [72] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿图网络（HGN）的方法，通过随机特征参数构造替代迭代优化，显著提高了训练速度（最高达600倍），同时保持了物理对称性和约束。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动建模中物理对称性和约束的挑战，同时优化训练效率。

Method: 使用哈密顿图网络（HGN）和随机特征参数构造，替代传统的梯度下降优化算法。

Result: 在多种模拟中表现稳健，包括3维N体质量-弹簧系统，且能零样本泛化到4096节点系统。

Conclusion: 挑战了传统梯度下降优化算法在物理系统神经网络训练中的主导地位。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [73] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑描述符SpectRe，将谱信息融入持久同调图，提升了图神经网络的表达能力，并验证了其稳定性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特征而无法捕捉基本图结构信息，因此需要一种更强大的拓扑描述符。

Method: 提出SpectRe，将谱信息融入持久同调图，分析其全局和局部稳定性。

Result: SpectRe比现有描述符更具表达力，实验验证了其在合成和真实数据集上的有效性。

Conclusion: SpectRe是一种更强大的拓扑描述符，能够提升图模型在相关学习任务中的能力。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [74] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 论文提出了一个统一的公理化框架，将图和拓扑消息传递联系起来，以分析并缓解拓扑消息传递中的过度压缩问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑深度学习（TDL）在建模关系数据中的高阶交互方面表现出强大能力，但拓扑消息传递中的过度压缩现象缺乏理论分析。

Method: 通过将单纯复形和蜂窝复形及其消息传递方案视为关系结构，扩展了图论结果和算法到高阶结构。

Result: 理论分析和单纯网络的实证研究表明，该框架具有推动TDL发展的潜力。

Conclusion: 提出的框架为拓扑消息传递中的过度压缩问题提供了分析和缓解的工具，推动了拓扑深度学习的发展。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [75] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文研究了高斯混合模型（GMM）在过参数化设置下的全局收敛性，证明了梯度EM算法在随机初始化下可以全局收敛到真实参数。


<details>
  <summary>Details</summary>
Motivation: 尽管EM算法及其变种梯度EM在实践中广泛应用，但在精确参数化设置下，全局收敛性仅对m=2的情况得到证明，且当m≥3时EM算法会失败。本文旨在探索过参数化设置下的全局收敛性。

Method: 在过参数化设置下，使用n>m个分量拟合m分量真实GMM，通过Hermite多项式分析梯度EM的动态特性，并利用张量分解刻画似然损失的几何景观。

Result: 证明了对于任何良好分离且一般位置的GMM，仅需轻度过参数化n=Ω(mlogm)，随机初始化的梯度EM能以多项式速率全局收敛到真实参数。

Conclusion: 这是首次在m=2之外的特殊情况下，为EM或梯度EM算法提供全局收敛和恢复结果的研究。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [76] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为DPSM的算法，通过将共形预测原则融入深度分类器的训练过程，直接最小化预测集的大小，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测校准方法生成的预测集过大，限制了其实际应用价值。

Method: 将共形训练建模为双层优化问题，提出DPSM算法，通过最小化预测集大小的度量（上层）并基于一致性分数的分位数（下层）进行优化。

Result: DPSM在多个基准数据集和深度模型上表现优异，预测集大小减少了20.46%，验证了其理论优势。

Conclusion: DPSM通过直接优化预测集大小，显著提升了共形预测的实用性，且具有更好的学习边界。

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [77] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 本文评估了多种基于语音的开源方法，用于从患者录音中预测认知障碍（CI），发现多模态方法优于单模态方法，声学特征优于语言学特征。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法多用于预测阿尔茨海默病及相关痴呆（ADRD），但未广泛用于预测更广泛的认知障碍（CI），而CI可能是ADRD的前兆和风险因素。

Method: 评估了基于语音的开源方法（原用于ADRD预测）和多模态情感分析方法，用于从患者录音中预测CI。

Result: 多模态方法优于单模态方法，声学特征（尤其是与情感和韵律相关的特征）显著优于基于BERT的语言学特征和可解释语言学特征。

Conclusion: 声学特征在多模态框架下对CI预测更有效，相关代码已开源。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [78] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [79] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为E2H Reasoner的方法，通过从易到难的课程学习（E2H）提升语言模型的推理能力，避免了单独使用强化学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，单独使用强化学习（RL）提升语言模型在复杂任务上的推理能力效果有限，因此需要更有效的方法。

Method: 采用从易到难的任务调度（E2H），逐步构建模型的推理能力，并通过理论分析验证其收敛性。

Result: 实验表明，E2H Reasoner显著提升了小型语言模型（1.5B至3B）的推理能力，优于单独使用RL的方法。

Conclusion: E2H Reasoner通过课程学习有效提升了语言模型的推理能力，且理论分析支持其样本效率优势。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [80] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种混合量子-经典架构Vision-QRWKV，用于图像分类任务，通过集成变分量子电路提升非线性特征转换能力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在复杂高维数据领域展现出潜力，本文旨在通过量子增强的RWKV架构提升视觉任务的性能。

Method: 在RWKV的通道混合组件中集成变分量子电路（VQC），构建Vision-QRWKV模型，并在14个图像分类基准上评估。

Result: 量子增强模型在多数数据集上优于经典模型，尤其在具有细微或噪声类别区分的数据集上表现突出。

Conclusion: 本研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效的量子视觉模型提供了未来研究方向。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [81] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 提出一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提升识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法因负载组合复杂多变，特征鲁棒性差且模型泛化能力不足。

Method: 将多维电力信号转换为图像负载特征，结合深度卷积神经网络和持续在线学习策略。

Result: 在高采样率负载数据集上实验表明，该方法在识别精度上有显著提升。

Conclusion: 该方法通过视觉特征和持续学习有效解决了传统NILM的局限性。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [82] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Spark Transformer通过top-k掩码和统计top-k算法实现高稀疏性，保持模型质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer中激活稀疏性引入导致的模型质量下降、参数增加或训练复杂化的问题。

Method: 采用top-k掩码和统计top-k算法实现稀疏性，并重新分配参数以低成本预测激活条目。

Result: 在标准基准测试中表现优异，FFN神经元激活率仅8%，计算量减少2.5倍，解码速度提升1.79x（CPU）和1.40x（GPU）。

Conclusion: Spark Transformer在保持模型质量的同时实现了高效的激活稀疏性，显著提升了计算效率。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [83] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: SAFER是一个结合结构化电子健康记录（EHR）和临床笔记的动态治疗推荐框架，通过校准风险感知和统计保证提供安全治疗建议。


<details>
  <summary>Details</summary>
Motivation: 动态治疗策略（DTRs）在精准医疗中至关重要，但现有方法依赖临床医生的黄金标准且未充分利用临床笔记，限制了可靠性。

Method: SAFER整合结构化EHR和临床笔记，利用标签不确定性假设和共形预测提供统计保证。

Result: 在两个公开的脓毒症数据集上，SAFER在推荐指标和反事实死亡率上优于现有基线方法。

Conclusion: SAFER为高风险DTR应用提供了可信赖且理论扎实的解决方案。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [84] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 论文提出了一种改进的数据归因方法RIF，解决了传统影响函数（IF）在高维数据中精度不足的问题，并通过实验和理论分析验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响模型行为，传统影响函数（IF）在高维数据中精度不足，需要改进。

Method: 提出了一种新的数据归因工具RIF，作为IF的替代方案，计算开销小但精度显著提升。

Result: 在多个真实数据集上验证，RIF比IF预测更准确，并提出了理论分析支持。

Conclusion: RIF是一种更优的数据归因工具，能有效检测IF无法识别的数据投毒攻击。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [85] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: SDP-CROWN是一种结合SDP松弛紧密度和线性边界传播可扩展性的新型验证框架，显著提升大规模神经网络的验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性边界传播验证器在大规模模型上表现良好但紧密度不足，而SDP验证器紧密度高但计算复杂度限制了其应用范围。

Method: 提出SDP-CROWN框架，通过SDP原理推导新的线性边界，显式捕获神经元间耦合，同时保持可扩展性。

Result: 理论证明新边界比传统方法紧密度提升√n倍，实际验证中在大型模型上接近SDP方法的紧密度。

Conclusion: SDP-CROWN成功结合了SDP的紧密度和线性边界传播的可扩展性，为大规模神经网络验证提供了高效解决方案。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [86] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的无监督框架，用于检测和分析足球中的突破防线传球（LBPs），并通过战术指标量化其效果。


<details>
  <summary>Details</summary>
Motivation: 突破防线传球（LBPs）是足球中重要的战术动作，能够帮助球队穿透防线并进入高价值区域。研究旨在通过数据驱动的方法分析和量化LBPs的效果。

Method: 使用同步的事件和跟踪数据，通过垂直空间分割建模对手队形，并识别在开放比赛中破坏防线的传球。引入了空间积累比（SBR）和两种链式变体（LBPCh^1和LBPCh^2）作为战术指标。

Result: 在2022年世界杯的数据中验证了这些指标，揭示了不同球队和球员在垂直推进和结构破坏上的风格差异。

Conclusion: 该方法具有可解释性、可扩展性，并可直接应用于现代比赛分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [87] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: HetCRF是一种针对异构图的双通道自监督学习框架，结合了MAE和CL的优势，通过两阶段聚合策略和正样本增强策略，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的混合框架在异构图上面临共享编码器设计困难和语义稀疏问题，HetCRF旨在解决这些问题。

Method: HetCRF采用两阶段聚合策略适应嵌入语义，并增强编码器输出以解决语义稀疏问题，同时提出正样本增强策略平衡梯度贡献。

Result: 在四个真实异构图数据集上，HetCRF在节点分类任务中表现优于现有基线，尤其在特征缺失情况下提升显著。

Conclusion: HetCRF通过结合MAE和CL的优势，有效解决了异构图自监督学习中的挑战，展示了其优越性和实用性。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [88] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL是一个通过生成式持续学习训练移动性基础模型的框架，解决了隐私和数据孤岛问题，性能接近联合训练且优于联邦学习。


<details>
  <summary>Details</summary>
Motivation: 移动性数据隐私敏感且分散，导致难以构建通用基础模型。MoveGCL旨在填补这一空白。

Method: 采用生成式持续学习，通过冻结教师模型生成合成轨迹，结合知识蒸馏和混合专家Transformer，实现隐私保护和模型进化。

Result: 在六个真实城市数据集上，MoveGCL性能接近联合训练，显著优于联邦学习基线，同时保护隐私。

Conclusion: MoveGCL为移动性基础模型的发展提供了开放、可扩展且隐私保护的实用方案。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [89] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel是一种通过选择困难示例来提升大语言模型（LLM）上下文学习（ICL）性能的方法，相比随机选择，F1分数提升2-7%。


<details>
  <summary>Details</summary>
Motivation: ICL的效果对示例的选择和顺序敏感，因此需要一种自适应的方法来优化演示示例的选择。

Method: 提出MarginSel，一种两步法，为每个测试实例选择困难的演示示例。

Result: 在分类任务中，F1分数绝对提升2-7%。

Conclusion: MarginSel通过增加困难示例的边距，类似支持向量，有效改善了LLM的决策边界。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [90] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构（SPT）和可解释AI技术（Sequence Score），用于高效预测蛋白质功能并揭示其生物学智能。


<details>
  <summary>Details</summary>
Motivation: 探索蛋白质Transformer是否能捕捉蛋白质序列中的生物学智能。

Method: 1. 引入Protein-FN数据集；2. 设计SPT架构；3. 开发Sequence Score技术。

Result: 小模型SPT-Tiny在AR和Protein-FN数据集上分别达到94.3%和99.6%的准确率。

Conclusion: SPT模型能发现与生物学知识一致的蛋白质序列模式，数据集和代码已开源。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [91] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 本文提出了一种基于Stein变分梯度下降（SVGD）的新方法SVH-MOL，用于在多目标学习中高效且多样化地逼近Pareto集。


<details>
  <summary>Details</summary>
Motivation: 当前方法在最大化超体积值的同时难以保证Pareto解的多样性，本文旨在解决这一挑战。

Method: 采用SVGD技术，通过功能梯度下降将一组粒子推向Pareto集，并结合多样梯度方向策略和退火调度以增强稳定性。

Result: 在多个多目标问题和多任务学习任务上的实验验证了SVH-MOL的优越性能。

Conclusion: SVH-MOL能够高效且多样化地逼近Pareto集，显著提升了多目标学习的效果。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [92] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 提出了一种通过模型编辑增强低资源语言识别的方法，显著提升了新字母表迁移学习和跨域评估的性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如古代手稿和非西方语言）在识别系统中代表性不足的问题，提升模型对新数据分布的泛化能力。

Method: 利用模型编辑技术，结合域合并策略，避免传统元学习对数据原型的需求。

Result: 实验表明，相同训练数据下，新字母表迁移学习和跨域评估性能显著提升。

Conclusion: 该方法为低资源语言识别提供了新思路，扩展了文档识别的应用范围。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [93] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIND的方法，通过特征实例邻居发现解决动态多测试分布下的性能下降问题，显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域分布变化时性能下降，现有测试时适应方法难以应对动态多测试分布。

Method: FIND包含三个关键组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN）。

Result: FIND在动态场景中实现了30%的准确率提升，同时保持计算效率。

Conclusion: FIND通过特征聚类和优化归一化策略，显著提升了模型在分布变化下的鲁棒性和效率。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [94] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: 论文提出了一种高效的聚合机制，牺牲部分表达能力以增强结构化聚合能力，并基于此设计了Caterpillar GNN，在合成和真实数据集上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代图学习中，消息传递图神经网络（MPGNNs）通常追求最大表达能力，但本文希望通过牺牲部分表达能力来增强聚合能力，实现更高效的图学习。

Method: 引入了一种高效的聚合机制，通过广义的caterpillar图的同态计数严格表征表达能力，并提出了Caterpillar GNN。

Result: 在专门设计的合成图任务中表现优于传统MPGNNs，在真实数据集上预测性能相当但显著减少了计算图中的节点数。

Conclusion: 通过结构化聚合机制，Caterpillar GNN在保持性能的同时提高了计算效率，为图学习提供了新思路。

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [95] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN是一种新型电路表示方法，通过多粒度拓扑特征提取和门感知归一化，解决了AIGs中的结构异质性和全局逻辑信息丢失问题，显著提升了逻辑电路建模的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路规模和设计复杂度的增加，传统的And-Inverter Graphs (AIGs)在表示布尔逻辑时面临结构异质性和全局逻辑信息丢失的挑战，需要更有效的电路表示方法。

Method: FuncGNN结合了混合特征聚合以提取多粒度拓扑模式，引入门感知归一化适应电路门分布，并通过多层集成合并中间特征，综合局部和全局语义信息。

Result: 在信号概率预测和真值表距离预测任务中，FuncGNN分别提升了2.06%和18.71%的性能，同时减少了50.6%的训练时间和32.8%的GPU内存使用。

Conclusion: FuncGNN通过创新的特征提取和归一化方法，显著提升了逻辑电路表示的准确性和效率，为电子设计自动化提供了更强大的工具。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [96] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 本文提出两种简单的启发式替代方法（最小距离奖励和分段匹配奖励），挑战了逆强化学习中最优传输的必要性，并证明这些方法在性能上与复杂的最优传输方法相当。


<details>
  <summary>Details</summary>
Motivation: 最优传输方法在逆强化学习中虽有效，但引入了算法复杂性和超参数敏感性。本文旨在探索是否可以通过更简单的方法实现类似效果。

Method: 提出两种启发式方法：1) 最小距离奖励，基于最近专家状态分配奖励；2) 分段匹配奖励，通过轻量级时间对齐匹配状态。

Result: 在32个在线和离线基准测试中，这些简单方法与最优传输方法性能相当或更优。

Conclusion: 最优传输的核心优势可能源于基本邻近对齐而非其复杂耦合公式，未来逆强化学习设计应重新评估复杂性。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [97] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的内部节点信息来增强目标节点嵌入，解决了现有异构图自监督学习方法中信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有异构图自监督学习方法通常通过元路径将异构图转换为同构图进行训练，但仅利用了元路径两端节点的信息，而忽略了元路径上的异构节点信息。

Method: 提出了IMPA-HGAE框架，通过创新性的掩码策略和生成式自监督学习，充分利用元路径上的内部节点信息。

Result: 实验证明IMPA-HGAE在异构数据集上表现优异，并增强了生成式自监督学习模型的表示能力。

Conclusion: 该工作为复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了新思路，并探讨了未来研究方向。

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [98] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 该论文研究了神经扩散过程在全局优化中的应用，基于Zhang等人的路径积分采样器，通过Boltzmann分布和Schrödinger桥采样问题将优化问题转化为随机控制问题，并使用Fourier MLP进行神经近似。实验表明该方法在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索神经扩散过程在全局优化中的潜力，尤其是通过随机控制和神经近似的方法解决高维优化问题。

Method: 利用Boltzmann分布将优化问题转化为Schrödinger桥采样问题，通过Girsanov定理和Fourier MLP进行神经近似求解。

Result: 在2至1,247维任务中表现良好，但在更高维（15.9k参数）任务中探索能力受限。

Conclusion: 该方法在低至中维优化任务中具有潜力，但在高维环境中需要进一步改进适应能力。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [99] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 论文提出了一种基于二阶数据流形表示的Curvature-Enhanced Manifold Sampling (CEMS)方法，用于回归任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强在分类任务中表现优异，但在回归问题中应用较少，因此需要一种新的方法来生成合成数据以提升回归任务的泛化能力。

Method: 利用二阶数据流形表示，提出CEMS方法，通过高效采样和重建新数据点来增强回归任务的性能。

Result: CEMS在多种数据集上表现优于现有方法，且在分布内外场景下均具有优越性能，计算开销极小。

Conclusion: CEMS为回归任务提供了一种高效的数据增强方法，显著提升了模型的泛化能力。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [100] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: FA-INR提出了一种基于交叉注意力和记忆库的隐式神经表示方法，通过自适应特征分配和混合专家机制，显著提升了模型效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示（INR）在处理复杂科学数据时，难以捕捉局部高频变化，且基于刚性几何结构的改进方法牺牲了灵活性和模型大小。

Method: FA-INR利用交叉注意力从增强的记忆库中学习灵活特征表示，并通过坐标引导的混合专家（MoE）机制提升可扩展性。

Result: 在三个大规模集成模拟数据集上，FA-INR实现了最佳保真度，同时显著减小模型规模。

Conclusion: FA-INR在精度和紧凑性之间建立了新的平衡，为基于INR的替代模型提供了高效解决方案。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [101] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了高维稀疏线性回归在差分隐私（DP）下的问题，提出了两种方法DP-IHT-H和DP-IHT-L，分别针对重尾数据和特定响应假设，显著提升了误差界。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对常规数据分布或低维情况，无法处理高维稀疏数据中的重尾响应问题，因此需要新的解决方案。

Method: 提出了DP-IHT-H（基于Huber损失和私有迭代硬阈值）和DP-IHT-L（进一步优化误差界），分别适用于不同数据假设。

Result: DP-IHT-H的误差界与数据尾重参数相关，而DP-IHT-L的误差界独立于尾重参数，且实验表明优于标准DP算法。

Conclusion: 本文方法在高维稀疏线性回归中表现出色，尤其适用于重尾数据，为DP领域提供了新的解决方案。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [102] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为SAFE的新型剪枝方法，通过同时优化稀疏性和平坦性，显著提升了稀疏网络的泛化性能，并在噪声数据下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 稀疏化神经网络通常会导致性能下降，现有方法难以恢复原始性能。受鲁棒优化研究的启发，论文旨在寻找同时稀疏且平坦的子网络。

Method: 将剪枝问题建模为稀疏约束优化问题，以平坦性为目标，通过增强拉格朗日对偶方法求解，并提出了广义投影操作，形成了SAFE及其扩展SAFE$^+$方法。

Result: 在图像分类和语言建模任务中，SAFE能持续生成泛化性能更好的稀疏网络，且对噪声数据具有鲁棒性。

Conclusion: SAFE方法在稀疏性和性能恢复方面表现优异，适用于现实场景。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [103] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 论文提出了一种基于log-sum-exponential（LSE）算子的新估计器，用于解决离线学习和评估中的高方差和重尾奖励分布问题。


<details>
  <summary>Details</summary>
Motivation: 离线学习和评估面临高方差和低质量倾向得分的问题，传统方法表现不佳。

Method: 引入LSE算子构建新估计器，理论分析其偏差和方差上界，并推导离线学习中的遗憾界限。

Result: LSE估计器在方差减少和重尾条件下表现稳健，遗憾界限收敛速率为O(n^(-ε/(1+ε)))。

Conclusion: 理论和实验验证了LSE估计器的优越性，代码已开源。

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [104] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 论文提出了一种名为FREE的方法，通过对抗训练在GAN框架中优化视觉语言模型（VLMs）的早期退出策略，以提升推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽然性能优异，但模型体积大导致推理延迟问题，限制了实际应用。

Method: 采用对抗训练方法（FREE），在GAN框架中训练退出分类器，每个退出点包含一个Transformer层和一个分类器，通过输入自适应推理提升速度。

Result: 实验表明，该方法在保持性能的同时，推理速度提升超过1.51倍，并增强了模型鲁棒性。

Conclusion: FREE方法有效解决了VLMs的推理延迟问题，为实际应用提供了高效解决方案。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [105] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 论文研究了上下文强化学习（ICRL）的鲁棒性，提出了一种对抗训练框架AT-DPT，以应对针对DPT的奖励投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 解决DPT在奖励投毒攻击下的脆弱性问题。

Method: 提出AT-DPT框架，同时训练攻击者和DPT模型，攻击者通过投毒环境奖励最小化DPT的真实奖励，DPT则从投毒数据中推断最优动作。

Result: 在强盗算法和MDP设置中，AT-DPT显著优于现有鲁棒基线方法。

Conclusion: AT-DPT在复杂环境中表现出良好的鲁棒性，能够有效应对奖励投毒攻击。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [106] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出一种利用潜在Kronecker结构的方法，显著提升高斯过程在大规模数据集上的计算效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在大数据集上的计算可扩展性受限，现有方法常需近似或不现实假设。

Method: 通过表达观测值核矩阵为潜在Kronecker积的投影，结合迭代线性系统求解器和路径条件。

Result: 在多达五百万样本的真实数据集上，性能优于现有稀疏和变分高斯过程方法。

Conclusion: 该方法为大规模高斯过程推断提供了高效且精确的解决方案。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [107] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 提出一种基于高斯过程的图神经网络消息传递方法，改进分布偏移下的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在不确定性估计中面临结构和标签分布随机性的双重挑战，传统方法难以应对。

Method: 通过类比随机偏微分方程与消息传递，设计了一种结合时空噪声的新型消息传递方案。

Result: 实验表明，该方法在分布外检测任务中优于现有方法，尤其在标签信息量不同的图上表现优异。

Conclusion: 该方法通过控制协方差核平滑度，显著提升了图数据不确定性估计的准确性。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [108] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy是一种基于图的物理引导学习框架，用于高分辨率空气质量建模，特别适用于监测数据有限的地区。


<details>
  <summary>Details</summary>
Motivation: 细粒度空气质量监测对减少公众暴露于污染物至关重要，但社会经济弱势地区的监测网络稀疏，限制了建模的准确性和分辨率。

Method: 提出了一种物理引导的图神经网络架构GraPhy，专为低分辨率监测数据设计，包含特定层和边特征。

Result: 在加州圣华金谷的实验表明，GraPhy在MSE、MAE和R2上表现最佳，性能提升9%-56%，且在不同空间异质性水平下均优于基线模型。

Conclusion: GraPhy的设计有效提升了空气质量建模的准确性和分辨率，尤其适用于监测数据有限的地区。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [109] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 提出了一种名为basis transformers的新架构，用于处理表格数据的挑战，如部分信息、噪声和异构结构，并在多任务表格回归基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以同时处理表格数据的关键方面，如文本信息、可变列数和无元数据的新数据。

Method: 设计了basis transformers架构，尊重表格数据的固有不变性，如层次结构和数值表示。

Result: 在OpenML-CTR23基准测试的34个任务中，中位数R²得分提高了0.338，参数数量比最佳基线少五倍，且优于预训练大型语言模型基线。

Conclusion: basis transformers在表格数据处理中表现出色，具有高效性和优越性能。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [110] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对图像分类器的非对称查询成本的黑盒攻击框架，通过改进搜索策略和梯度估计过程，显著降低了总查询成本和扰动大小。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击方法假设所有查询成本相同，但实际应用中某些查询可能成本更高（如触发额外审查的查询），而现有方法对此场景的处理不足。

Method: 提出了非对称搜索（AS）和非对称梯度估计（AGREST），通过减少对高成本查询的依赖并调整采样分布，平衡不同查询类型的成本。

Result: 在多种成本设置下，该方法总查询成本和扰动大小均优于现有方法，某些场景下提升达40%。

Conclusion: 该框架可轻松集成到现有黑盒攻击中，为非对称查询成本场景提供了高效解决方案。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [111] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络训练中梯度下降导致的渐进锐化现象，通过一个极简模型（单神经元每层的深度线性网络）揭示了其动态机制，并分析了数据集、网络深度、优化器随机性和步长的影响。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络训练中渐进锐化现象的机制，填补现有研究的空白。

Method: 使用单神经元每层的深度线性网络作为极简模型，理论分析数据集、网络深度、优化器随机性和步长对渐进锐化的影响。

Result: 极简模型成功捕捉了实际训练中的锐化动态，理论分析揭示了不同因素对渐进锐化的作用。

Conclusion: 研究深化了对神经网络训练中锐化动态的理解，强调了深度、训练数据和优化器之间的相互作用。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [112] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 提出了一种基于分位数的风险正则化强化学习算法，通过集成CVaR来确保安全性，避免了复杂架构的需求，并在动态任务中表现出更好的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 主流近似动作值迭代强化学习算法在高方差随机环境中存在高估偏差，导致策略次优，且现有方法难以确保安全约束的满足。

Method: 提出了一种风险正则化的分位数动作值迭代算法，集成CVaR来强制执行安全性，并提供了理论保证。

Result: 在动态避障任务中，该方法比风险中性方法实现了更高的目标成功率、更少的碰撞和更好的安全性能权衡。

Conclusion: 该方法通过风险正则化分位数框架有效解决了安全约束问题，并在实际任务中验证了其优越性。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [113] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: UdonCare利用医学本体论发现潜在域，通过层次化修剪和编码提升临床预测的领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决临床预测中数据分布变化导致模型性能下降的问题，尤其是缺乏患者域标签和忽视临床知识的问题。

Method: 利用ICD-9-CM层次结构分组疾病，提出UdonCare框架，通过修剪、编码和Siamese推理机制分离域信号与患者特征。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优于其他基线模型，尤其在域差距大时效果显著。

Conclusion: 医学知识在提升领域泛化能力方面具有潜力，UdonCare为实际医疗应用提供了有效解决方案。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [114] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了1-识别问题，提出新算法SEE，填补了非渐近分析的空白，实现了接近最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有文献中关于1-识别问题的非渐近分析尚不明确，本文旨在填补这一空白。

Method: 设计了新算法Sequential-Exploration-Exploitation (SEE)，并从非渐近角度进行理论分析。

Result: SEE算法实现了接近最优的样本复杂度，上下界差距仅为多项式对数因子。数值实验验证了算法的有效性。

Conclusion: SEE算法在1-识别问题中表现出色，填补了非渐近分析的空白，为多臂老虎机问题提供了新思路。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [115] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: MoXGATE是一种基于交叉注意力和可学习模态权重的深度学习框架，用于多组学数据融合，显著提升了癌症亚型分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 癌症亚型分类对个性化治疗和预后评估至关重要，但多组学数据的异质性使得有效整合具有挑战性。

Method: 提出Modality-Aware Cross-Attention MoXGATE框架，利用交叉注意力和模态权重实现多组学特征融合，并应用焦点损失解决数据不平衡问题。

Result: 在GIAC和BRCA数据集上达到95%的分类准确率，优于现有方法，且能泛化到未见过的癌症类型。

Conclusion: MoXGATE在多组学癌症亚型分类中表现出色，具有高性能和生物通用性。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [116] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出了一种新的认证机器遗忘方法，通过噪声微调保留数据，确保可证明的遗忘保证，无需对损失函数做假设，并在实践中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器遗忘问题，满足隐私和法规要求（如“被遗忘权”），现有方法假设严格或缺乏形式化保证。

Method: 利用遗忘与随机后处理的隐私放大之间的联系，通过噪声微调保留数据实现认证遗忘。

Result: 理论分析了效率与准确性的权衡，实证表明方法具有形式化遗忘保证且优于现有基线。

Conclusion: 该方法无需损失函数假设，广泛适用，并在实践中高效有效。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [117] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Hyperblocks的改进方法，通过简化算法减少模型复杂度，同时保持准确性，并引入可解释的k-NN回退机制。


<details>
  <summary>Details</summary>
Motivation: 增强模型的可解释性、减少训练时间并降低复杂度，同时不牺牲准确性，使领域专家能直接理解模型逻辑。

Method: 引入Hyperblock简化算法，包括去除冗余属性、分析重叠块、创建分离单元，并结合k-NN回退机制。

Result: 在WBC和MNIST数据集上，模型在保持高准确性的同时显著降低了复杂度。

Conclusion: 该方法为高维大数据集提供了一种透明且高效的替代方案，适用于需要信任和清晰度的领域。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [118] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文分析了K-means算法的局部最优性保证，提出了改进方法以确保局部最优性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管K-means算法在机器学习中被广泛研究，但其局部最优性保证缺乏严格分析。本文旨在填补这一空白。

Method: 提出了对K-means算法的简单修改，确保其在连续和离散意义上的局部最优性，同时保持与原算法相同的计算复杂度。

Result: 数值实验表明，改进后的方法能够提供更优的局部解，并减少聚类损失。

Conclusion: 本文提出的改进方法在理论和实践中均优于传统K-means算法，尤其是在局部最优性方面。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [119] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出了一种基于物理约束的扩散模型，用于检测异常轨迹（如GPS欺骗），在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，现有方法因忽略时空依赖性和物理知识导致高误报率。

Method: 提出了一种物理信息扩散模型，结合运动学约束识别不符合物理规律的轨迹。

Result: 在真实数据集（海事和城市领域）上，该方法在异常检测和轨迹生成方面表现出更高的准确性和更低的误差率。

Conclusion: 该方法通过整合物理约束，显著提高了异常轨迹检测的准确性，为打击非法活动提供了有效工具。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [120] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: ProbHardE2E是一个通用的概率预测框架，通过新颖的可微分概率投影层（DPPL）实现硬约束，支持端到端学习，并能优化严格评分规则，适用于多种非线性约束问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过后处理或推理阶段满足约束，且依赖分布假设，限制了模型的灵活性和鲁棒性。ProbHardE2E旨在解决这些问题。

Method: 利用DPPL层结合神经网络架构，直接学习系统并满足硬约束，同时支持不确定性量化和非线性约束。

Result: 在偏微分方程学习和时间序列预测中展示了广泛适用性，优于依赖分布假设的现有方法。

Conclusion: ProbHardE2E是一个灵活且强大的框架，适用于多种领域，能够提供鲁棒的概率估计。

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [121] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: 本文比较了基于车辆动力学的驾驶员疲劳检测方法，提出了一种透明公平的框架，并验证了随机森林方法的最高准确性（88%）。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员疲劳检测方法存在性能指标不可靠和可重复性问题，本文旨在通过公开数据集和透明框架解决这些问题。

Method: 开发了一个特征提取框架，实现了三种现有方法和一种随机森林方法，并进行了实验验证。

Result: 随机森林方法在实验中表现最佳，准确率达到88%。

Conclusion: 本文揭示了非标准化方法的固有问题，并展示了一种高性能且实现得当的方法。

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [122] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: AlphaSteer是一种理论支持的方法，通过激活导向提升LLM的安全性，同时避免对良性提示的过度拒绝，平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的部署增加，确保其能够拒绝恶意提示（如越狱攻击）至关重要，但现有方法在安全性与实用性之间存在权衡。

Method: AlphaSteer将激活导向视为可学习过程，通过两个目标学习：实用性保护（构造接近零的导向向量）和安全性增强（构造拒绝导向向量）。

Result: 实验表明，AlphaSteer显著提升了LLM的安全性，同时未损害其通用能力。

Conclusion: AlphaSteer是一种有效且理论支持的方法，解决了LLM安全性与实用性的权衡问题。

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [123] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: 论文提出MATI方法，通过区域感知混合专家和测试时自监督专家聚合，解决表格不平衡回归问题，在多个真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据在现实应用中广泛存在，但表格回归任务中的不平衡问题研究不足，现有方法依赖已知且平衡的测试分布假设，实际中可能导致性能下降。

Method: MATI包含两个创新点：(1) 区域感知混合专家，使用高斯混合模型捕捉相关区域并训练区域专家；(2) 测试时自监督专家聚合，动态调整专家权重以适应不同测试分布。

Result: 在四种真实数据集上，MATI在三种测试分布下平均MAE提升7.1%。

Conclusion: MATI有效解决了表格不平衡回归问题，提升了模型在不同测试分布下的泛化能力。

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [124] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [125] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: FairPFN是一种无需因果模型先验知识的表格基础模型，用于识别和减轻预测中受保护属性的因果影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在关键领域（如医疗、执法和金融）中应用广泛，但历史数据中的偏见可能导致决策加剧社会不平等。因果公平性框架虽能缓解这一问题，但需已知因果模型，限制了其适用性。

Method: 提出FairPFN，通过预训练合成因果公平数据，无需因果模型知识即可识别和消除受保护属性的因果影响。

Result: FairPFN在多样化的手工和真实场景中表现优异，优于基线方法。

Conclusion: FairPFN为复杂公平性问题提供了更易用的因果公平解决方案，推动了未来研究。

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [126] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: PGTS方法通过结合树搜索提升策略优化，减少不良稳定点，并在实验中表现优于传统PG方法。


<details>
  <summary>Details</summary>
Motivation: 传统PG方法在复杂环境中易陷入局部最优，PGTS旨在通过树搜索机制提升策略优化效果。

Method: PGTS引入m步前瞻机制，理论分析表明增加树搜索深度可减少不良稳定点。

Result: 实验证明PGTS在多种MDP结构中表现优异，能避开局部陷阱并找到更优解。

Conclusion: PGTS通过树搜索机制显著提升了策略优化的效果，尤其在复杂环境中表现突出。

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [127] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: E-BATS是一种高效的无反向传播测试时间适应框架，专为语音基础模型设计，通过轻量级提示适应、多尺度损失和测试时间指数移动平均机制，在保持内存效率的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在真实场景中因声学域偏移（如背景噪声和口音）性能下降，现有测试时间适应方法要么内存消耗大，要么精度低。

Method: E-BATS采用轻量级提示适应、多尺度损失和测试时间指数移动平均机制，实现高效且有效的适应。

Result: 在四个噪声语音数据集上的实验显示，E-BATS比无反向传播基线提升4.1%-13.5%准确率，比基于反向传播的方法节省2.0-6.4倍GPU内存。

Conclusion: E-BATS为实际语音处理系统在声学变化下的高效适应提供了可行方案。

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [128] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: 状态熵正则化在强化学习中表现出更好的探索性和样本效率，但其理论保证尚未被研究。本文证明了状态熵正则化对结构化和空间相关扰动的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 研究状态熵正则化的理论保证，特别是在面对常见于迁移学习中的结构化和空间相关扰动时的表现。

Method: 分析状态熵正则化在奖励和转移不确定性下的鲁棒性，并与策略熵正则化进行对比。

Result: 状态熵正则化在结构化和空间相关扰动下表现更好，但其鲁棒性优势对策略评估的样本数更敏感。

Conclusion: 状态熵正则化在特定扰动下具有理论优势，但实际应用中需注意样本数对其效果的影响。

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [129] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: 论文研究了在高概率非渐近情况下，对固定设计的ℓ²正则化非线性最小二乘问题的置信度估计，特别是针对正则化训练损失的局部极小值点。提出了一种点式置信边界，适用于任何固定测试输入的预测，并展示了该边界与测试数据在隐式特征空间中的相似性相关。此外，提出了一种高效计算加权范数的方法，并通过实验验证了其优于自助法的覆盖/宽度权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决非线性最小二乘问题中高概率非渐近置信度估计的挑战，特别是在测试输入与训练数据相似性变化时的置信边界问题。

Method: 方法包括提出点式置信边界，该边界基于目标函数的逆Hessian矩阵的加权范数，并开发了一种高效计算该范数的方法。

Result: 结果显示，提出的置信边界在测试输入与训练数据相似性变化时表现良好，且计算效率高。实验表明，该方法在覆盖/宽度权衡上优于自助法。

Conclusion: 结论是提出的置信边界方法在非线性最小二乘问题中具有高效性和优越性，特别是在处理测试输入与训练数据相似性变化时表现突出。

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [130] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据转换方法的分布式患者相似性计算（DPSC）技术，结合时间序列和静态数据，显著提升了预测性能并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 患者相似性计算（PSC）是医疗信息学中的基础问题，旨在通过患者的历史临床记录衡量其相似性，以改善临床决策支持。

Method: 采用数据转换（DT）方法，结合时间序列（如心率、血压等）和静态数据（如年龄、性别等），使用aWOE和Z-score进行数据转换，并采用分布式DTW计算时间序列相似性。

Result: 在冠状动脉疾病和充血性心力衰竭的预测中，AUC、准确率和F-measure分别提升了11.4%-21.9%，计算时间减少了40%。

Conclusion: 提出的DPSC方法在预测性能和计算效率上均表现出显著优势，适用于大规模医疗数据分析。

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [131] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: CoFILL是一种基于条件扩散模型的新型时空数据填补方法，通过双流架构处理时空特征，显著提高了填补精度。


<details>
  <summary>Details</summary>
Motivation: 时空数据中的缺失值对实际应用（如环境监测和城市交通管理）构成挑战，现有方法难以有效建模时空依赖关系且易产生累积误差。

Method: 提出CoFILL，利用扩散模型生成高质量填补值，采用双流架构并行处理时域和频域特征。

Result: 实验表明CoFILL能有效将随机噪声转化为符合真实数据分布的值，填补精度优于现有方法。

Conclusion: CoFILL通过创新架构和扩散模型解决了时空数据填补中的关键问题，具有实际应用潜力。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [132] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 该论文探讨了利用语言模型的嵌入能力实现通用黑盒优化（BBO）的方法，通过统一表示异构数值空间，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统离线BBO方法因缺乏统一表示而局限于单任务和固定维度设置，无法实现跨域通用优化。语言模型的嵌入能力为解决这一问题提供了可能。

Method: 提出多种方法，包括端到端学习的下一个标记预测框架，以及学习具有强表示能力的潜在空间。

Result: 实验验证了所提方法的通用性和有效性，表明语言模型先验与字符串嵌入空间的结合可以突破传统BBO的限制。

Conclusion: 通过统一语言模型先验和学习字符串嵌入空间，为通用BBO算法的发展开辟了新途径。

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [133] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: 本文提出了一种名为QDRT的新框架，通过行为条件训练和行为重放缓冲区实现目标驱动的多样性，解决了现有红队方法在多样性和攻击风格覆盖上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法在多样性评估和攻击风格覆盖上存在局限性，无法全面评估大型语言模型的安全性。

Method: QDRT框架通过行为条件训练和行为重放缓冲区实现目标驱动的多样性，并训练多个专门攻击者以覆盖不同攻击风格和风险类别。

Result: QDRT生成的攻击在多样性和有效性上均优于现有方法，适用于多种目标LLM（如GPT-2、Llama-3等）。

Conclusion: QDRT为自动化红队提供了一种系统且有效的方法，有助于大型语言模型的安全部署。

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [134] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: 论文提出了一种可靠的策略迭代方法（RPI），解决了传统强化学习算法在函数逼近下单调性保证失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统策略迭代在函数逼近下单调性保证失效，亟需一种新方法解决这一问题。

Method: RPI通过基于Bellman的约束优化替代传统策略评估中的投影或Bellman误差最小化。

Result: RPI不仅保证了单调性，其值估计还下界真实回报，并在函数逼近下具有收敛性。

Conclusion: RPI是首个在函数逼近下具有单调性和收敛性保证的算法，实验证明其性能优于基线方法。

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [135] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: AMoPO是一种新型框架，通过动态平衡多目标偏好优化，无需额外奖励模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效平衡多目标偏好，且依赖辅助模型增加计算复杂度。

Method: 引入多目标优化范式，使用维度感知生成指标作为隐式奖励，并采用自适应权重分配机制。

Result: AMoPO在7B、14B和32B模型上表现优于基线28.5%，验证了其扩展性和适应性。

Conclusion: AMoPO能有效实现维度感知偏好对齐，具有显著优势。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [136] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA是一个高效的TAG表示学习框架，通过仅标注代表性节点和边减少成本，并通过两级对齐模块整合标注图与TAG，实验显示其性能优于现有方法且仅需1%的标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在TAG中因复杂文本信息表现不佳，现有方法需大量标注或微调，成本高且耗时。

Method: GAGA通过标注代表性节点和边构建标注图，并采用两级对齐模块整合标注图与TAG。

Result: 实验表明GAGA分类准确率与或优于现有方法，且仅需1%的标注数据。

Conclusion: GAGA显著降低了标注成本和时间，同时保持了高性能。

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [137] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种正则化自适应图学习（RAGL）模型，用于交通预测，解决了现有方法中节点嵌入正则化不足和计算复杂度高的问题。


<details>
  <summary>Details</summary>
Motivation: 交通预测在空间-时间预测中具有重要应用，但现有自适应图学习方法存在节点嵌入正则化不足或计算复杂度高的问题。

Method: RAGL结合了随机共享嵌入（SSE）和自适应图卷积，通过残差差异机制实现嵌入正则化和噪声抑制，并提出了高效余弦算子（ECO）以线性时间复杂度进行图卷积。

Result: 在四个大规模真实交通数据集上的实验表明，RAGL在预测准确性和计算效率上均优于现有方法。

Conclusion: RAGL通过正则化和高效计算解决了交通预测中的关键问题，具有实际应用潜力。

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [138] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: 提出了一种基于Neurovectors的新型学习方法，通过向量空间中的能量传播驱动学习，提升模型的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络依赖反向传播调整权重，而Neurovectors通过向量关系编码信息，旨在提供更灵活且可解释的学习过程。

Method: 利用Neurovectors构建动态知识表示，通过能量传播而非权重更新驱动学习，适用于表格数据处理。

Result: 在UCI和Kaggle数据集上的实验表明，Neurovectors在分类和回归任务中具有竞争力。

Conclusion: Neurovectors提供了一种高效且可解释的替代方案，性能与传统方法相当。

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [139] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: 该研究通过SEER 2021数据集分析乳腺癌患者的生存结果，揭示不同种族和地理背景下的差异，并利用EDA和生存分析技术（如Kaplan-Meier估计器和Cox比例风险模型）提供详细统计结果。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示乳腺癌治疗和护理中的不平等，为制定针对性干预措施提供依据，从而改善全球乳腺癌结局。

Method: 采用SEER 2021数据集，结合EDA、Kaplan-Meier估计器、log-rank检验和Cox比例风险模型进行分析。

Result: 研究详细统计了不同种族和国家的生存率差异，为政策制定者和医疗专业人员提供可靠数据。

Conclusion: 研究结果为减少乳腺癌治疗不平等提供了基础工具，并支持全球改善乳腺癌结局的努力。

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [140] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall是一种基于双曲几何的图生成框架，结合了HVQVAE和黎曼流匹配先验，显著提升了生成图的层次结构保留能力。


<details>
  <summary>Details</summary>
Motivation: 解决欧几里得几何在捕捉指数复杂性时的局限性，提出双曲几何框架以更好地生成具有层次结构的图。

Method: 结合双曲向量量化自编码器（HVQVAE）和基于闭式测地线的黎曼流匹配先验，开发了双曲GNN和Transformer层。

Result: 在Community-Small和Ego-Small数据集上，度MMD分别降低了75%和40%，显著优于现有方法。

Conclusion: 双曲几何为复杂、结构化和层次化数据的生成建模提供了强大基础。

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [141] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出Perception-R1方法，通过引入视觉感知奖励增强多模态大语言模型（MLLMs）的感知与推理能力，解决了现有RLVR方法在感知能力提升上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在多模态推理中忽视了感知能力的提升，而感知是多模态推理的核心基础。

Method: 提出Perception-R1，通过视觉感知奖励激励MLLMs准确感知视觉内容，并利用标注文本和LLM评估一致性来分配奖励。

Result: 在多个多模态推理基准测试中，Perception-R1仅用1,442训练数据即达到最优性能。

Conclusion: Perception-R1有效提升了MLLMs的感知与推理能力，填补了现有方法的不足。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [142] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: 本文提出VARSHAP，一种基于预测方差减少的局部特征归因方法，优于现有方法如SHAP和LIME。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法（如SHAP）存在全局依赖性，无法准确捕捉局部模型行为。

Method: 基于Shapley值框架，VARSHAP以预测方差的减少作为特征重要性度量，对全局数据分布变化具有鲁棒性。

Result: 在合成和真实数据集上的实验表明，VARSHAP在定量和定性上均优于KernelSHAP和LIME。

Conclusion: VARSHAP是一种有效的局部特征归因方法，解决了现有方法的局限性。

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [143] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）在显式结构化推理中如何调节推理长度，提出了一种可视化进度条和“超频”方法以减少不必要的推理步骤，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 显式结构化推理中，推理长度对答案质量有重要影响：过短无法捕捉任务复杂性，过长则导致过度思考和性能下降。本文旨在探索LLM如何理解和调节推理长度。

Method: 1. 展示LLM如何编码推理进度，并引入交互式进度条可视化；2. 在推理过程中操纵内部进度编码以减少冗余步骤，生成更简洁的思维链。

Result: 实验结果表明，“超频”方法有效减少过度思考，提高答案准确性，并降低推理延迟。

Conclusion: 通过可视化进度条和操纵内部进度编码，本文方法优化了LLM的推理过程，提升了效率和准确性。

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [144] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: IBDR是一种新型贝叶斯推理框架，通过增强粒子多样性提升集成质量，并在VTAB-1K基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过粒子间交互建模提升贝叶斯推理的集成质量，解决传统方法中粒子多样性不足的问题。

Method: 提出IBDR框架，基于广义理论连接分布总体损失与近似后验，采用双重优化程序实现分布鲁棒性和粒子多样性。

Result: 在VTAB-1K基准测试和语言推理任务中，IBDR表现优于基线方法。

Conclusion: IBDR在提升粒子多样性和分布鲁棒性方面有效，适用于实际应用。

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [145] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: 本文提出SPlus方法，解决了Shampoo算法的三个关键问题，包括稳定性、学习率适应性和参数噪声问题，并在多种任务中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 针对Shampoo算法在优化神经网络时的不足，如矩阵逆缓存导致的发散问题、学习率适应性差以及高学习率带来的参数噪声，提出改进方案。

Method: 1. 引入历史特征基与瞬时归一化的有界更新，提升稳定性；2. 采用形状感知缩放实现网络宽度间的学习率迁移；3. 提出简单迭代平均方案减少参数噪声。

Result: 在语言建模、图像分类和扩散建模任务中，SPlus仅需Adam 44%的梯度步数和62%的墙钟时间即可达到相同验证性能。

Conclusion: SPlus显著提升了优化效率，解决了Shampoo的关键问题，适用于多种神经网络任务。

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [146] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 本文提出了一种基于Cramér-von Mises统计的新型奖励机制，旨在激励代理提交真实数据，同时抑制数据伪造和虚假报告。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据量的奖励机制容易被操纵，代理可能提交虚假或低质量数据以获取奖励。

Method: 开发了一种基于两样本检验的奖励机制，适用于多种数据分布，无需强假设。

Result: 理论证明真实报告在贝叶斯和无先验设置下构成纳什均衡，并在模拟和真实数据实验中验证了有效性。

Conclusion: 该方法放松了先前工作的关键假设，能有效激励真实数据共享。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [147] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 论文提出了一种结合cMAB和LLM的混合方法，用于个性化干预以减少久坐行为，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 传统cMAB算法需要大样本且可能忽略心理因素，因此结合LLM以提升个性化干预效果。

Method: 采用cMAB选择干预类型，LLM个性化消息内容，评估四种干预类型的效果。

Result: 通过实验比较四种模型（cMAB、LLM、cMABxLLM、RCT），分析其对步数和消息接受度的影响。

Conclusion: LLM和cMAB的结合在促进身体活动方面具有互补作用，为个性化行为干预提供了新思路。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [148] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [149] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文提出EVINET框架，结合Beta嵌入和主观逻辑，解决图学习中的误分类检测和分布外检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有图学习通常基于封闭世界假设，而实际环境是开放且嘈杂的，需要模型能够识别误分类和分布外数据。

Method: EVINET框架包含两个关键模块：Dissonance Reasoning（误分类检测）和Vacuity Reasoning（分布外检测），基于Beta嵌入和主观逻辑。

Result: 实验表明EVINET在多个任务（分类、误分类检测、分布外检测）中优于现有方法。

Conclusion: EVINET证明了不确定性估计和逻辑推理在开放世界图学习中的必要性。

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [150] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: LLMs通过上下文学习（ICL）有效建模HMM生成的数据，预测精度接近理论最优，并在实际任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决HMM建模真实数据时的计算挑战，探索LLMs在ICL中的潜力。

Method: 利用预训练LLMs通过ICL学习HMM生成的数据模式。

Result: LLMs在合成HMM数据上接近理论最优，实际任务中与专家设计模型竞争。

Conclusion: ICL能有效学习HMM序列，为复杂科学数据提供新工具。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [151] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 论文提出了一种新方法PASS，通过概率替换原始数据并设计新的损失函数，解决了现有隐私保护方法的漏洞，同时在多种数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护方法存在漏洞，因为它们基于对抗训练策略，容易泄露隐私信息。

Method: 提出PASS方法，通过概率替换原始数据，并设计信息论目标的新损失函数。

Result: 在多种数据集（如面部图像、活动传感器信号、语音记录）上验证了PASS的有效性和通用性。

Conclusion: PASS克服了现有方法的局限性，提供了一种更可靠的隐私保护解决方案。

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [152] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: 提出了一种结合PagedAttention和PyTorch FlexAttention的方法，优化LLMs长上下文推理中的KV缓存效率，显著降低延迟并减少内存碎片。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存处理方式在长上下文推理中存在内存效率低下的问题。

Method: 集成PagedAttention与PyTorch FlexAttention，通过分页KV缓存减少碎片化，并在IBM FMS中实现高效的数据收集。

Result: 在NVIDIA L4 GPU上测试，延迟随序列长度线性增长（约2倍），而传统方法呈指数增长。内存增量仅在序列长度超过2048时显著。

Conclusion: 开源实现为未来长上下文模型部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [153] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的高效生成模型，用于合成时间序列数据，以解决安全领域数据不足的问题，并提升机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 安全领域的数据访问受限，现有合成数据方法无法有效提升模型性能。

Method: 设计了一种高效的Transformer生成框架，用于生成高质量的时间序列数据。

Result: 模型在多个数据集上实现了SOTA性能，并能生成高质量样本。

Conclusion: 该Transformer模型具有通用性，可提升现有和新机器学习工作流的性能。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [154] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: DEF是一种通过扩散增强集成预报生成初始条件扰动的新方法，适用于机器学习天气预测领域。


<details>
  <summary>Details</summary>
Motivation: 现有初始条件扰动方法主要针对数值天气预报设计，限制了其在机器学习天气预测中的应用。

Method: 使用简单的条件扩散模型生成结构化扰动，可迭代应用并通过引导项控制扰动水平。

Result: 在ERA5再分析数据集上验证，DEF提高了预测性能并生成合理的分布估计。

Conclusion: DEF能将确定性神经预报系统转化为随机系统，减少长期预报误差并生成有意义的预报分布。

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [155] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 该论文提出了一种移动感知的动态稀疏化（MADS）算法，用于优化异步联邦学习（AFL）中的梯度稀疏化，以应对设备移动性带来的连接问题。


<details>
  <summary>Details</summary>
Motivation: 设备移动性导致间歇性连接，影响AFL的收敛性，需要研究梯度稀疏化、模型陈旧性和移动性接触模式之间的相互作用。

Method: 开发理论模型分析稀疏化、模型陈旧性和移动性接触模式对AFL收敛的影响，并提出MADS算法动态优化稀疏化程度。

Result: 实验表明，MADS在CIFAR-10数据集上提高分类准确率8.76%，在Argoverse轨迹预测数据集中减少平均位移误差9.46%。

Conclusion: MADS算法能有效优化AFL在移动环境中的性能，理论分析和实验结果验证了其优越性。

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [156] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: JavelinGuard是一套低成本、高性能的模型架构，用于检测大型语言模型（LLM）交互中的恶意意图，专为生产部署优化。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，恶意交互检测需求增加，需要高效且低成本的解决方案。

Method: 探索了五种基于Transformer的架构，包括基础分类器、增强注意力加权池化、混合神经网络集成和多任务框架。

Result: 在九个对抗性数据集上测试，表现优于开源防护模型和大型解码器LLM，展示了成本与性能的优越平衡。

Conclusion: 多任务框架Raudra表现最稳健，但每种架构在速度、可解释性和资源需求上有独特权衡，为实际应用提供选择指导。

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [157] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: Graph-KV通过结构化归纳偏置改进LLMs的序列化限制，在RAG和图结构任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因序列化输入而无法利用结构依赖的问题，特别是在RAG和图结构任务中。

Method: 利用KV-cache作为压缩表示，通过图结构块掩码选择性关注相关段，减少位置偏差。

Result: 在RAG基准、Arxiv-QA和论文分类任务中显著优于基线方法。

Conclusion: Graph-KV通过结构归纳偏置有效提升LLMs性能，代码和数据已公开。

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [158] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT是一种轻量级模型适配框架，用于在封闭约束下的Split Computing，通过客户端可训练适配器优化特征，无需修改原始模型。


<details>
  <summary>Details</summary>
Motivation: 在封闭环境中，传统适配方法不可行，因为需要访问模型参数或架构。SALT旨在解决这一问题。

Method: 引入紧凑的可训练适配器，在客户端优化头部网络的潜在特征，实现用户特定适配。

Result: 在CIFAR-10和CIFAR-100上验证，SALT提高了准确性且训练延迟更低。

Conclusion: SALT为边缘AI系统提供了一种实用的个性化推理解决方案。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [159] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: MoE-GPS框架通过量化预测策略对系统性能的影响，提出Distribution-Only Prediction策略，显著减少开销，提升推理性能23%以上。


<details>
  <summary>Details</summary>
Motivation: 多GPU MoE网络中专家负载不平衡，动态复制热门专家需预测分布，现有方法开销大。

Method: 提出MoE-GPS框架，量化预测策略对系统性能的影响，推荐Distribution-Only Prediction策略。

Result: 在Mixtral 8x7B MMLU数据集上，Distribution-Only Prediction比传统方法提升推理性能23%以上。

Conclusion: MoE-GPS框架能有效优化预测策略选择，显著提升系统性能。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [160] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩对齐理论的域泛化（DG）方法，通过封闭式矩对齐（CMA）算法高效对齐域级梯度和Hessian，解决了现有方法的计算效率问题，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 域泛化（DG）旨在解决现实应用中分布偏移问题，但现有方法计算效率低且理论基础不足。本文旨在通过矩对齐理论统一理解现有方法，并提出高效算法。

Method: 基于转移测度理论，扩展了多源域的定义，并证明对齐导数可提升转移测度。提出封闭式矩对齐（CMA）算法，避免重复反向传播或Hessian估计。

Result: CMA在线性探测和全微调实验中均优于经验风险最小化和现有先进算法。

Conclusion: 矩对齐理论为域泛化提供了统一框架，CMA算法在效率和性能上均表现出色。

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [161] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何解释的Transformer注意力机制改进方法，通过减少参数和增强局部性提升性能。


<details>
  <summary>Details</summary>
Motivation: 为Transformer的注意力机制提供几何解释，并解决其忽视局部归纳偏置的问题。

Method: 利用度量张量、切空间、内积等几何概念，通过并行传输连接离散位置的结构，同时减少参数并引入局部性机制。

Result: 实验表明，改进后的模块显著优于基线模型。

Conclusion: 该方法在视觉和大语言模型中具有进一步验证的潜力。

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [162] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: InverseScope是一种轻假设、可扩展的框架，通过输入反演解释神经激活，解决了现有方法对表示结构的强假设问题。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）的内部表示是解释性研究的核心挑战，现有方法依赖的强假设在实践中可能不成立。

Method: 提出InverseScope框架，通过定义目标激活的输入分布并分析其编码特征，结合条件生成架构提高采样效率。

Result: InverseScope将基于反演的解释方法扩展到更大模型和实际任务，支持对LLMs内部表示的系统定量分析。

Conclusion: InverseScope为LLMs的内部表示提供了一种高效、可扩展的解释方法，推动了模型解释性研究的发展。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [163] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，通过多级特征提取和LLM的上下文理解能力，显著提升了检测精度和实时响应效率。


<details>
  <summary>Details</summary>
Motivation: 随着多云环境的快速发展，确保智能监控系统的安全性和可靠性变得尤为重要。

Method: 在现有监控框架基础上，创新性地引入多级特征提取方法，结合LLM的自然语言处理能力和传统机器学习方法，动态适应不同云服务提供商和环境。

Result: 实验结果表明，该模型在检测精度和延迟方面显著优于传统异常检测系统，并显著提升了云基础设施的韧性和主动管理能力。

Conclusion: 该研究为多云环境下的智能监控系统提供了一种高效的异常检测与预警解决方案。

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [164] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: 论文提出了一种分数阶矩阵微分计算方法（${{\bf{J}}^\alpha }$），基于整数阶雅可比矩阵定义，实现了分数阶链式法则，并设计了分数阶Autograd技术，提升了分数阶微分在深度学习中的实用性。实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏与自动微分技术（Autograd）完全兼容的分数阶矩阵微分方法，限制了分数阶微分在神经网络优化中的应用。

Method: 提出基于整数阶雅可比矩阵定义的分数阶雅可比矩阵微分（${{\bf{J}}^\alpha }$），设计分数阶Autograd技术，并在PyTorch框架中实现分数阶线性模块（FLinear）。

Result: 实验通过定性分析训练集和验证集的损失、定量分析测试集指标，以及时间和GPU内存消耗分析，验证了${{\bf{J}}^\alpha }$的优越性能。

Conclusion: ${{\bf{J}}^\alpha }$是一种优秀的分数阶梯度下降方法，适用于深度学习领域。

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [165] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon通过变分推理改进对比学习，解决了嵌入分布无显式调控和过度依赖大批量负样本的问题，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决对比学习中嵌入分布无显式调控和过度依赖大批量负样本的问题。

Method: 提出VarCon，将监督对比学习重新表述为对潜在类别变量的变分推理，最大化后验加权的ELBO。

Result: 在CIFAR-10、CIFAR-100、ImageNet等数据集上达到SOTA性能，Top-1准确率79.36%（ImageNet-1K），78.29%（CIFAR-100）。

Conclusion: VarCon在性能、嵌入空间语义组织和少样本学习方面表现优异，具有更强的鲁棒性。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [166] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: 提出了一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，显著降低计算开销，实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境中（如机器人和自动驾驶设备）部署视觉语言模型的高计算开销问题。

Method: 采用联合优化策略，包括补丁选择、令牌选择模块和推测解码，以减少计算量。

Result: 在NVIDIA DRIVE Thor平台上，流水线实现了2.5倍端到端延迟降低，使用FP8量化后提升至3.2倍，且不影响任务准确性。

Conclusion: 该流水线是资源受限环境中实现实时VLM部署的可行解决方案。

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [167] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: 论文提出了一种基于证据深度学习的动态图OOD检测方法EviSEC，通过后验Dirichlet分布和频谱感知对比学习解决现有方法的偏差和分数同质化问题。


<details>
  <summary>Details</summary>
Motivation: 动态图中的OOD检测在安全敏感领域受到关注，但现有方法主要针对静态图，存在高偏差、高方差和分数同质化问题。

Method: 提出EviSEC，结合证据神经网络和频谱感知增强模块，通过后验Dirichlet分布和生成OOD近似数据改进检测。

Result: 在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本。

Conclusion: EviSEC通过不确定性建模和频谱感知对比学习，显著提升了动态图OOD检测的性能。

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [168] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: Fed-ICL是一种通过多轮客户端与服务器交互提升问答任务中上下文学习效果的方法，无需传输模型参数。


<details>
  <summary>Details</summary>
Motivation: 由于数据隐私、标注成本和分布差异，高质量上下文学习样本稀缺，现有方法通信开销大或未能充分利用本地数据。

Method: 提出Fed-ICL框架，通过迭代协作过程优化响应，利用客户端与服务器的多轮交互。

Result: 理论保证收敛性，实验表明在标准问答基准上表现优异且通信成本低。

Conclusion: Fed-ICL在提升问答任务性能的同时，有效解决了通信和隐私问题。

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [169] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: 论文提出采用贝叶斯实验建模框架，以主动管理和解决LLM部署中的不确定性，而非被动拒绝高不确定性输出。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署中缺乏系统区分和应对不同不确定性来源的工具，导致保守策略（如拒绝高不确定性输出）限制了其可靠性。

Method: 采用贝叶斯实验建模框架，为不确定性提供一致的理论基础，并明确其可减少性。

Result: 该框架支持主动解决不确定性（如请求澄清、检索外部信息或优化输入），而非被动避免。

Conclusion: 通过主动管理不确定性，该框架有望提升LLM系统的可靠性、透明性和适用性，尤其是在高风险场景中。

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [170] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现，特定风格提示（如列表格式）会提高大型语言模型（LLM）对越狱攻击的成功率，并提出防御策略SafeStyle以缓解风险。


<details>
  <summary>Details</summary>
Motivation: 探讨风格模式是否影响LLM的安全性，以及如何通过对齐缓解这些风险。

Method: 评估32个LLM在七个越狱基准测试中的表现，分析风格模式对攻击成功率的影响，并提出SafeStyle防御策略。

Result: 风格模式显著提高越狱攻击成功率，且与模式长度和模型注意力相关；SafeStyle能有效维持模型安全性。

Conclusion: 风格模式确实增加LLM的漏洞，SafeStyle是一种有效的防御方法。

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [171] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero是一种通过在线强化学习实现蛋白质生成模型自我改进的新框架，显著提高了蛋白质设计的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质生成模型因高质量数据稀缺而成功率受限，ProteinZero旨在通过自我改进解决这一问题。

Method: 采用在线强化学习框架，结合高效代理奖励模型（ESM-fold和快速ddG预测器）和多目标优化策略。

Result: ProteinZero在结构准确性、可设计性、热力学稳定性和序列多样性上均优于现有方法，失败率降低36%-48%。

Conclusion: ProteinZero为蛋白质设计提供了持续自我改进的新范式，扩展了设计空间探索的可能性。

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [172] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: 提出了一种名为TSC的新型后门净化防御方法，适用于多种学习范式，仅需少量干净样本即可有效防御后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常需要标记数据或特定训练流程，难以应用于监督学习以外的场景，而TSC旨在填补这一空白。

Method: TSC利用神经网络的排列不变性和二次模式连接性，通过两阶段对称连接性放大中毒样本的损失，同时保持干净样本的准确性。

Result: 实验表明，TSC在监督学习场景中表现优异，并能推广到自监督学习框架（如SimCLR和CLIP）。

Conclusion: TSC是一种通用且高效的后门净化防御方法，适用于多种学习范式。

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [173] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: Self-RedTeam提出了一种在线自博弈强化学习算法，通过攻击者和防御者的动态协同进化，提升语言模型的安全对齐能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐方法存在滞后性，攻击者可以针对静态防御模型进行过拟合，而防御者则无法及时应对新威胁。

Method: 采用两玩家零和博弈框架，模型在攻击者和防御者角色间切换，通过奖励模型裁决结果，实现动态协同适应。

Result: 实验表明，Self-RedTeam能发现更多样化的攻击（+21.8% SBERT），并在安全基准测试中表现更优（如WildJailBreak上+65.5%）。

Conclusion: 该方法推动了从被动修补到主动协同进化的转变，为语言模型安全训练提供了可扩展、自主且鲁棒的解决方案。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [174] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer是第一个为Lean设计的端到端通用hammer工具，结合神经前提选择和符号推理，显著提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经方法在自动推理中取得进展，但将其整合到实际验证工作流中仍具挑战性。Lean作为流行的证明助手，尚缺乏有效的hammer工具。

Method: 提出LeanHammer，基于神经前提选择系统，动态适应用户上下文，并结合符号证明搜索与重建。

Result: 评估显示，LeanHammer比现有前提选择器多解决21%的目标，并能泛化到多样领域。

Conclusion: LeanHammer填补了神经检索与符号推理间的鸿沟，使形式验证更易为研究者和实践者所用。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [175] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: 论文探讨了直接偏好优化（DPO）的局限性，并提出了一种新的显式偏好优化框架EXPO，避免了DPO的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）通过人类反馈强化学习（RLHF）进行微调，但RLHF依赖复杂的训练流程。DPO虽然简化了流程，但仍存在次优正则化和反直觉插值行为的问题。

Method: 提出EXPO框架，通过显式设计正则化因子，避免DPO的潜在缺陷，无需依赖隐式奖励的重参数化技巧。

Result: 理论分析和实验结果表明，EXPO能够满足正则化需求，优于DPO及其变体。

Conclusion: EXPO提供了一种更透明且有效的方法来优化LLM的偏好，解决了DPO的局限性。

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [176] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了一种通过注入Gumbel噪声和使用直通估计器的方法，显著加速逻辑门网络（LGNs）的训练，减少离散化差距，并提高神经元利用率。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在计算和能耗方面的高需求促使研究人员寻求更高效的解决方案，但现有逻辑门网络训练时间长且存在离散化差距，影响实际部署。

Method: 在训练过程中注入Gumbel噪声并使用直通估计器，理论分析表明这隐含了Hessian正则化，改善了LGNs的收敛性。

Result: 训练速度提升4.5倍，离散化差距减少98%，未使用门数量减少100%。

Conclusion: 该方法显著提升了逻辑门网络的训练效率和实际部署性能。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [177] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 论文提出了一种图因果演化（GoCE）方法，通过可微分稀疏因果邻接矩阵和因果约束渗透，解决了链式模型（CoM）中长程依赖丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 解决CoM中子链仅依赖前序信息导致的长程依赖丢失问题。

Method: 将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，通过干预一致性损失和自演化门实现动态平衡。

Result: GoCE提升了Transformer捕捉长程因果依赖的能力，并增强了自演化能力，在多个数据集上优于基线LLMs。

Conclusion: GoCE不仅超越了CoM的设计原则，还为因果学习和持续自适应改进提供了经验。

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [178] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 论文提出了一种名为DGN的方法，利用先验数据指导探索而非直接模仿，从而提升强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在先验数据（如演示）的帮助下高效进行强化学习，避免模仿学习目标对长期性能的负面影响。

Method: 提出Data-Guided Noise (DGN)框架，通过向策略添加噪声来利用先验数据指导探索，而非直接约束行为克隆。

Result: 在七个模拟连续控制任务中，DGN比现有离线数据强化学习方法性能提升2-3倍。

Conclusion: DGN通过优化探索而非模仿，显著提升了强化学习的样本效率和性能。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [179] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: 论文提出了一种基于似然最大化的学习算法，解决了推荐系统中因外生变量独立性假设不足导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统因用户仅与偏好项目交互的选择偏差导致偏好表示失真，现有方法假设外生变量独立，但实际可能存在相关性。

Method: 提出统一方法，建模含潜在外生变量的数据生成过程，并开发蒙特卡洛算法估计似然函数。

Result: 在合成数据集和三个真实数据集上的实验验证了方法的有效性。

Conclusion: 该方法能有效处理潜在外生变量，提升推荐系统的准确性和公平性。

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [180] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优传输的Wasserstein over Wasserstein (WoW)距离，用于处理概率分布上的梯度流，应用于迁移学习和数据集蒸馏任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习中许多应用涉及概率分布数据，需要设计在无限维对象上的梯度流技术。

Method: 将每个类别表示为特征的条件分布，数据集建模为这些类别上的混合分布，并引入WoW距离和梯度流。

Result: 提出了WoW梯度流框架，应用于迁移学习和数据集蒸馏任务，效果显著。

Conclusion: WoW距离和梯度流为处理概率分布数据提供了新方法，具有广泛的应用潜力。

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [181] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: MetaKANs通过小型的元学习器生成KANs的权重，显著减少可训练参数数量，同时保持性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: KANs虽然高效且可解释，但参数过多导致内存和训练成本高，MetaKANs旨在解决这一问题。

Method: 通过端到端可微的方式训练KANs和MetaKANs，生成权重以减少参数数量。

Result: 在符号回归、偏微分方程求解和图像分类等任务中，MetaKANs显著提升了参数效率和内存使用。

Conclusion: MetaKANs为KANs提供了一种更高效且可扩展的训练方法，缩小了与MLPs的训练成本差距。

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [182] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的化学代理，整合外部化学工具和数据集ChemToolBench，采用HE-MCTS框架优化工具规划与执行，显著提升化学任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在化学任务中因预训练知识过时和难以融入专业知识而面临的挑战。

Method: 结合137种外部化学工具，构建数据集ChemToolBench，采用HE-MCTS框架优化工具规划与执行，支持细粒度微调。

Result: 实验表明，该方法在化学问答和发现任务中性能显著提升，超越GPT-4o。

Conclusion: 为LLMs与专业工具结合提供了高效解决方案，支持高级化学应用。

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [183] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 提出了一种通过仅使用最可能的前p个状态来加速隐马尔可夫模型推理的方法，同时证明了误差受限于p和模型的最小混合率。实验表明，速度提升至少一个数量级，总变差距离误差低于0.09。


<details>
  <summary>Details</summary>
Motivation: 动态概率模型（如隐马尔可夫模型）的推理计算复杂且效率低，因为需要枚举整个状态空间，包括概率极低的状态，导致计算浪费和噪声传播。

Method: 提出仅使用累积概率为p的最可能状态（top-p状态）进行推理，以减少计算量和噪声。

Result: 理论证明误差受限于p和模型的最小混合率；实验显示速度提升至少10倍，总变差距离误差低于0.09。

Conclusion: 该方法显著提升了推理效率，同时控制了误差，适用于需要高效推理的动态概率模型。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [184] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: MIRA是一个专为医学时间序列设计的统一基础模型，通过创新的编码和动态建模技术，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列数据具有不规则间隔、异构采样率和频繁缺失值等挑战，现有通用模型难以处理。

Method: MIRA采用连续时间旋转位置编码、频率特定专家混合层和基于神经ODE的连续动态外推块。

Result: 在公开数据集上预训练后，MIRA在分布外和分布内场景中分别平均减少10%和7%的预测误差。

Conclusion: MIRA为医学时间序列建模提供了新基准，并为未来研究奠定了基础。

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [185] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出了一种名为ATRADA的新型框架，用于通过合成数据增强飞机轨迹数据集，提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹建模在航空交通管理中至关重要，但现有数据集往往不足或不平衡，需要合成数据增强。

Method: 使用Transformer编码器学习轨迹数据的潜在模式，通过PCA降维和GMM拟合数据分布，最后用MLP解码生成新样本。

Result: 实验表明，ATRADA能有效生成高质量的合成轨迹数据，优于多个基线方法。

Conclusion: ATRADA框架为飞机轨迹数据增强提供了一种有效解决方案，有助于提升下游任务的性能。

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [186] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: PrunePEFT是一种新颖的PEFT策略搜索方法，通过将PEFT模块搜索转化为剪枝问题，优化了预训练模型的微调配置，显著降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: PEFT方法在减少训练参数的同时保持了任务性能，但其设计空间庞大，配置不当会导致性能不佳。传统架构搜索方法开销大，需要更高效的解决方案。

Method: PrunePEFT将PEFT策略搜索转化为剪枝问题，采用混合剪枝策略，迭代移除冗余或冲突的PEFT模块，优化配置。

Result: 该方法显著减少了架构搜索的计算负担，提升了PEFT方法的可扩展性和效率。

Conclusion: PrunePEFT为大规模预训练模型的高效微调提供了一种可扩展且高效的解决方案。

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [187] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 本文研究了具有曲率损失和延迟反馈的在线凸优化问题，提出了一种改进的算法，显著降低了遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理强凸损失时，遗憾界为$d_{\max} \ln T$，但在某些情况下不如延迟版本的在线梯度下降的$\sqrt{d_{\mathrm{tot}}}$。本文旨在填补这一差距。

Method: 提出了一种改进的follow-the-regularized-leader算法，并扩展了Online Newton Step算法以处理延迟，同时设计了Vovk-Azoury-Warmuth预测器的变体用于无约束在线线性回归。

Result: 新算法实现了$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$的遗憾界，并在实验中表现出优于现有方法的性能。

Conclusion: 本文提出的算法在处理延迟和曲率损失时具有显著优势，为相关领域提供了新的解决方案。

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [188] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: TwinBreak是一种创新的安全对齐移除方法，通过识别和修剪负责安全功能的参数，有效绕过LLM的安全机制，成功率达89%至98%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的安全机制可能被恶意用户绕过，当前方法成本高或影响模型性能，需要更高效的解决方案。

Method: TwinBreak通过分析中间输出，识别并修剪与安全功能相关的参数，利用TwinPrompt数据集进行实验。

Result: 在16个LLM上测试，TwinBreak成功率达89%至98%，且计算成本低。

Conclusion: TwinBreak提供了一种高效、低成本的方法来绕过LLM的安全机制，同时保持模型性能。

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [189] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Air模型通过多模态数据融合实现高效、低成本的空气质量预测，优于传统数值模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统空气质量预测方法的高计算成本、低效率和观测数据整合不足的问题。

Method: 结合气象预报、排放清单和污染物监测数据，采用自回归预测框架和帧插值策略。

Result: 在72小时内完成多站点污染物预测，计算效率和精度均优于主流数值模型。

Conclusion: 多模态数据驱动的FuXi-Air模型为智能城市管理提供了技术参考和实践范例。

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [190] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: 本文介绍了一个新颖的产率预测数据集，首次提供了用于机器学习基准测试的瞬态流动数据集，涵盖1200多种工艺条件，重点关注溶剂选择任务。


<details>
  <summary>Details</summary>
Motivation: 化学数据集通常难以获取或需要清洗，限制了机器学习在化学领域的应用。本文旨在填补这一空白，提供首个瞬态流动数据集。

Method: 通过实验设置采样大量连续工艺条件，生成新的机器学习挑战，并专注于溶剂选择任务。

Result: 展示了回归算法、迁移学习方法、特征工程和主动学习的基准测试结果，应用于溶剂替代和可持续制造。

Conclusion: 该数据集为机器学习在化学领域的应用提供了新的基准，特别是在溶剂选择和可持续制造方面具有重要价值。

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [191] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: ChebNet被重新评估，发现其在长距离节点交互建模上优于MPNNs和GTs，但存在训练不稳定的问题。提出Stable-ChebNet，解决了稳定性问题并保持高性能。


<details>
  <summary>Details</summary>
Motivation: MPNNs在捕捉长距离依赖上有限，而GTs牺牲了计算效率和图结构。重新审视ChebNet，探索其在长距离建模上的潜力。

Method: 将ChebNet建模为稳定且非耗散的动态系统，提出Stable-ChebNet，无需特征分解、位置编码或图重连。

Result: Stable-ChebNet在多个基准测试中接近最优性能，且保持了良好的可扩展性。

Conclusion: ChebNet在长距离建模上具有竞争力，Stable-ChebNet进一步解决了其稳定性问题，成为高效且强大的替代方案。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [192] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: 论文探讨了为什么过参数化模型（如深度神经网络和Transformer）在参数远超训练样本时仍能泛化良好，从信息论和通用学习理论角度提出了一种基于假设类权重的解释。


<details>
  <summary>Details</summary>
Motivation: 研究现代机器学习中过参数化模型泛化能力强的现象，试图从理论上解释其避免过拟合的原因。

Method: 采用贝叶斯混合学习器，基于对数损失和均匀先验，分析假设类的权重（即接近真实数据生成过程的模型的累积概率）。

Result: 发现学习器的遗憾由假设类的权重决定，而非假设类的总体大小，从而解释了简单模型泛化能力强的现象。

Conclusion: 通过理论分析，揭示了过参数化模型泛化良好的原因，并提供了与实践中关键概念（如平坦极小值和模型蒸馏）的联系。

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [193] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: ProARD提出了一种动态网络训练方法，通过一次训练支持多种轻量级学生网络，避免了重复训练的高成本和碳排放。


<details>
  <summary>Details</summary>
Motivation: 当前对抗鲁棒性蒸馏方法需要为不同资源约束的设备重新训练学生网络，导致高计算成本和碳排放。

Method: 基于动态层构建动态网络，通过权重共享机制联合优化动态教师网络和内部学生网络，并采用采样机制减少计算开销。

Result: ProARD能够高效生成多样化的鲁棒学生网络，避免了随机采样带来的性能问题。

Conclusion: ProARD为对抗鲁棒性蒸馏提供了一种高效、环保的解决方案。

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [194] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: 论文研究了11种基准预测方法在19个不同基准上的表现，发现随机采样加回归模型的方法优于多数现有方法，且所有方法的有效性依赖于模型相似性。提出了一种新方法，在模型相似性较低时表现优于随机采样平均，但改进有限。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）评估成本高，研究如何通过缩小基准数据集来加速评估。

Method: 系统评估11种基准预测方法，提出随机采样加回归模型的基线方法，并引入一种基于增强逆倾向加权的新方法。

Result: 随机采样加回归模型优于多数现有方法；新方法在模型相似性低时表现更好，但改进有限。

Conclusion: 基准预测在评估前沿（新模型能力未知时）效果不佳，表明其局限性。

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [195] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: 本文提出了一种增强潜在扩散模型（LDMs）鲁棒性的方法，包括去除文本编码器的测量、数据增强技术、模型微调及专用评估流程。


<details>
  <summary>Details</summary>
Motivation: LDMs在图像生成和视频合成中表现优异，但其鲁棒性不足，现有研究未充分探索这一问题。

Method: 1. 假设鲁棒性应在去除文本编码器后测量；2. 引入数据增强技术揭示LDMs的鲁棒性缺陷；3. 使用Dreambooth微调Stable Diffusion 3和XL模型；4. 提出专用评估流程。

Result: 通过提出的方法，改进了LDMs的鲁棒性。

Conclusion: 本文的方法为LDMs的鲁棒性提供了新的解决方案和评估标准。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [196] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former是一个结合语言嵌入和动态图表示学习的框架，解决了现有方法忽略硬件属性和静态图表示的局限性，实现了跨硬件平台的零样本预测，并在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经网络架构表示学习中忽略硬件属性信息且依赖静态图表示，限制了模型的实用性和性能。

Method: 提出LeDG-Former框架，结合语言嵌入（通过LLM处理）和动态图表示学习，实现跨硬件平台的零样本预测和更有效的架构建模。

Result: 在NNLQP、NAS-Bench-101和NAS-Bench-201数据集上超越现有方法，首次实现跨硬件延迟预测。

Conclusion: LeDG-Former通过语言和动态图的结合，显著提升了神经网络架构表示学习的性能和应用范围。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [197] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为GAS的新框架，通过图搜索而非显式高层策略选择子目标，显著提升了离线分层强化学习的效率。


<details>
  <summary>Details</summary>
Motivation: 现有离线分层强化学习方法依赖高层策略生成子目标序列，效率随任务时间增长而下降，且缺乏跨轨迹状态转移的有效策略。

Method: GAS将子目标选择建模为图搜索问题，利用Temporal Distance Representation（TDR）空间嵌入状态，聚类相似状态为图节点，并通过最短路径算法选择子目标序列。

Result: GAS在运动、导航和操作任务中优于现有方法，尤其在缝合关键任务中得分从1.0提升至88.3。

Conclusion: GAS通过图辅助缝合和TDR空间嵌入，显著提升了离线分层强化学习的性能。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [198] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本文提出了首个具有理论保证的实用算法，用于推断LDA主题模型中每个文档的主题分配，解决了社交科学、数据探索和因果推断中的主要推理问题。


<details>
  <summary>Details</summary>
Motivation: 解决LDA主题模型在应用中的主要推理问题，特别是在社交科学、数据探索和因果推断场景中的需求。

Method: 采用了一种新颖的非梯度组合方法估计主题模型，算法在并行计算中以对数时间收敛到接近最优的后验概率。

Result: 算法在语义质量上优于现有LDA、神经主题模型和基于LLM的主题模型，且能提供可解释性保证和满足因果推断的独立性假设。

Conclusion: 该方法在理论和实践上均优于现有技术，为LDA主题模型的应用提供了更高效和可靠的解决方案。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [199] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 生成式AI在信用评分建模中表现不及传统方法，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 比较生成式AI与传统方法在信用评分建模中的表现，探讨其适用性。

Method: 对比分析生成式AI与传统信用评分建模技术。

Result: 当前生成式AI模型在信用评分中表现不如传统方法。

Conclusion: 生成式AI在信用风险评分中能力有限，需更多研发。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [200] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种名为EMD-CFL的新型一次性聚类方法，用于解决联邦学习（FL）中非独立同分布（non-IID）数据的脆弱性问题。该方法利用嵌入空间中数据分布的地球移动距离（EMD）进行聚类，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据环境中广泛应用，但面临非独立同分布数据的脆弱性问题。现有方法如聚类联邦学习（CFL）试图通过寻找更同质的客户端集群来解决这一问题。

Method: 提出EMD-CFL方法，利用嵌入空间中数据分布的EMD进行一次性聚类，结合领域适应理论为其提供理论支持。

Result: 在16个基线方法和多个挑战性数据集上的实验表明，EMD-CFL具有优越的聚类性能。

Conclusion: EMD-CFL是一种有效的解决方案，能够显著提升联邦学习在非独立同分布数据环境中的表现。

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [201] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Conformal Prediction的对抗攻击方法OPSA和防御策略OPSA-AT，旨在提高深度学习模型的鲁棒性和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险应用中的部署需要更强的对抗防御和可靠的性能保证，而现有方法在不确定性估计方面存在不足。

Method: 开发了OPSA攻击方法，通过最大化模型不确定性来降低Conformal Prediction的效率；并提出了OPSA-AT防御策略，将OPSA整合到新的Conformal训练范式中。

Result: 实验表明，OPSA攻击能比基线方法引发更大的不确定性，而OPSA-AT防御模型显著提升了对抗攻击的鲁棒性，同时保持了可靠的预测性能。

Conclusion: 该研究为安全关键领域开发可信赖且鲁棒的深度学习模型提供了一种有效方法。

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [202] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出了一种多视角概率方法，通过聚合视角特定的槽来捕捉不变内容信息，同时学习解耦的全局视角信息，解决了空间模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 模块化的物体中心表示对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以获取。

Method: 引入多视角概率方法，聚合视角特定槽，学习不变内容和解耦的全局视角信息，无需视角标注。

Result: 在标准基准和新复杂数据集上的实验验证了方法的鲁棒性和可扩展性。

Conclusion: 该方法解决了空间模糊性问题，提供了可识别性的理论保证，且无需视角标注。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [203] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 提出了一种新的离线强化学习方法，通过将奖励优化直接融入一致性蒸馏过程，实现了单步生成，同时保持高性能和简化训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在决策任务中表现优异，但推理速度慢；一致性模型虽能解决这一问题，但在决策任务中常因次优演示或多网络复杂训练而受限。

Method: 提出了一种新颖的一致性蒸馏方法，将奖励优化直接整合到蒸馏过程中，支持单步生成。

Result: 在Gym MuJoCo基准测试和长时程规划中，性能提升8.7%，推理速度比扩散模型快142倍。

Conclusion: 该方法在保持高性能的同时显著提升了推理速度，为离线强化学习提供了更高效的解决方案。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [204] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 论文研究了如何通过提供高层符号知识帮助多智能体强化学习解决隐私、通信和性能等挑战，扩展了局部策略兼容性检查工具，并实证验证了符号知识对学习效率的提升。


<details>
  <summary>Details</summary>
Motivation: 现实问题中多智能体协作的需求与现有去中心化多智能体强化学习（DMARL）面临的隐私、通信和性能挑战促使研究符号知识的作用。

Method: 扩展了局部策略与团队任务兼容性的形式化检查工具，并引入符号知识加速学习过程。

Result: 符号知识显著加快了DMARL的学习效率，扩展的工具使去中心化训练在更多场景中具备理论保证。

Conclusion: 符号知识能有效解决DMARL的独特挑战，提升学习效率和策略兼容性。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [205] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出了一种名为CAFT的新方法，通过多标记训练改进LLMs的概念理解能力，显著优于传统单标记预测方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的逐标记预测范式限制了其形成连贯高级概念的能力，阻碍了更深层次的理解和推理。

Method: 引入Concept-Aware Fine-Tuning (CAFT)，一种多标记训练方法，支持跨标记序列学习。

Result: 实验显示CAFT在文本摘要和蛋白质设计等任务中表现显著优于传统方法。

Conclusion: CAFT首次将多标记预测引入训练后阶段，为研究社区提供了更广泛的应用潜力。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [206] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: 论文探讨了Jarzynski重加权在训练能量基模型（EBMs）中的应用，分析了其理论意义及在不同生成框架中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法如对比散度和分数匹配在训练EBMs时存在偏差，Jarzynski重加权作为一种非平衡统计力学技术，有望解决这些问题。

Method: 通过理论分析Jarzynski重加权的选择核作用，并在流基扩散模型和受限玻尔兹曼机中验证其效果。

Result: Jarzynski重加权能减少离散化误差、提高样本质量，并纠正对比散度的偏差。

Conclusion: Jarzynski重加权是一种有潜力的工具，其核选择与模型性能密切相关。

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [207] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: RR-GNN提出了一种基于图神经网络的框架，通过改进的共形预测方法解决图数据中的不确定性问题，提供更精确的预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法在图数据中表现保守，未能考虑图的异方差性和结构偏差，RR-GNN旨在解决这些问题。

Method: RR-GNN采用图结构Mondrian共形预测、残差自适应非共形分数和交叉训练协议，优化预测性能。

Result: 在15个真实图数据任务中，RR-GNN在保持覆盖率的条件下，显著提升了预测效率。

Conclusion: RR-GNN通过结合图拓扑和动态调整预测区间，为高不确定性图数据任务提供了可靠的解决方案。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [208] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的理论框架，用于分析公平性泛化误差，并通过Efron-Stein不等式推导出紧密的公平性泛化界限。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对公平性在训练数据与未见数据之间泛化能力的理论保证，公平性过拟合问题未得到充分研究。

Method: 采用信息论视角，基于Efron-Stein不等式，结合互信息（MI）和条件互信息（CMI）推导公平性泛化界限。

Result: 实验验证了所提界限的紧密性和实际相关性，适用于多种公平感知学习算法。

Conclusion: 该框架为设计改进公平性泛化的算法提供了有价值的指导。

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [209] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 提出了一种轻量级序列Transformer模型，用于1型糖尿病患者的血糖预测，解决了计算和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病需要持续监测血糖，但现有预测模型在可穿戴设备上部署困难。

Method: 结合Transformer的注意力机制和循环神经网络的序列处理能力，设计轻量级模型，优化边缘设备部署。

Result: 在OhioT1DM和DiaTrend数据集上表现优于现有方法，能有效预测血糖水平和检测不良事件。

Conclusion: 该模型填补了高性能建模与实际部署之间的空白，为1型糖尿病管理提供了高效解决方案。

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [210] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型的因果机制，用于反事实图像编辑，解决了现有方法在可扩展性和保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动编码框架在反事实图像生成中面临身份保持、感知质量和因果模型忠实性的挑战，扩散模型的最新进展为解决这些问题提供了机会。

Method: 提出了一套基于扩散模型的因果机制，包括空间、语义和动态反事实推理，并将语义表示通过Pearl因果理论集成到扩散模型中。

Result: 首次实现了扩散模型中高级语义身份保持的反事实编辑，展示了语义控制在忠实因果控制和身份保持之间的权衡。

Conclusion: 该框架为反事实图像编辑提供了新的解决方案，结合了扩散模型的视觉质量和因果推理的语义控制。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [211] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: 论文首次构建了四种Schauder基，用于空间$C[0,1]$，分别基于ReLU、Softplus及其sigmoidal版本，改进了它们的通用逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索ReLU、Softplus及其sigmoidal版本函数在空间$C[0,1]$中作为Schauder基的可行性，填补研究空白。

Method: 构造了四种Schauder基，分别基于ReLU、Softplus及其sigmoidal版本函数。

Result: 首次证明了这些函数可以作为Schauder基，并改进了其通用逼近性质。

Conclusion: 研究为函数逼近理论提供了新的工具，扩展了ReLU和Softplus的应用范围。

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [212] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: FunDiff是一个在函数空间中进行生成建模的新框架，结合了潜在扩散过程和函数自编码器架构，能够生成连续函数并融入物理先验。


<details>
  <summary>Details</summary>
Motivation: 适应生成模型（如扩散模型和流匹配）到物理应用中，处理连续函数和复杂物理规律。

Method: 结合潜在扩散过程和函数自编码器，通过架构约束或物理损失函数融入物理先验。

Result: 理论证明在函数空间中密度估计的最优性，实验验证在流体动力学和固体力学中生成高保真且物理一致的样本。

Conclusion: FunDiff在生成连续函数和满足物理规律方面表现出色，适用于多种物理应用。

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [213] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 提出了一种新型多模态扩散模型框架，支持跨模态数据的原生生成，无需依赖外部预处理协议。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部预处理协议（如分词器和变分自编码器）统一多模态数据，对编码器和解码器精度要求高，限制了在数据有限场景的应用。

Method: 引入创新的解耦噪声调度策略，支持单模型中同时进行无条件生成和模态条件生成。

Result: 在文本-图像生成和混合类型表格数据合成任务中验证了方法的竞争力。

Conclusion: 该框架为多模态数据生成提供了更灵活和高效的解决方案。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [214] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: CausalPFN是一种基于Transformer的模型，通过大规模模拟数据训练，能够直接推断新观测数据的因果效应，无需额外调整。


<details>
  <summary>Details</summary>
Motivation: 因果效应估计需要大量手动选择和专业知识，CausalPFN旨在自动化这一过程。

Method: 结合贝叶斯因果推断和先验拟合网络（PFNs）的大规模训练协议，直接映射观测数据到因果效应。

Result: 在IHDP、Lalonde和ACIC等基准测试中表现优异，并在真实世界政策制定任务中具有竞争力。

Conclusion: CausalPFN提供了一种无需额外训练或调优的自动化因果推断解决方案，支持可靠的决策。

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [215] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究发现，在序列建模任务中，最小非线性通常足够且最优，简化模型并提高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨非线性在循环网络中的功能角色，明确其计算必要性和机制。

Method: 使用几乎线性循环神经网络（AL-RNNs）作为建模工具和内部机制探针。

Result: 最小非线性不仅足够，而且通常最优，模型更简单、鲁棒和可解释。

Conclusion: 为选择性引入非线性提供原则性框架，连接动态系统理论与循环神经网络的功能需求。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [216] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新的状态空间模型W4S4，基于冗余小波框架构建，具有稳定对角化和高效计算特性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（SSMs）的性能依赖于状态矩阵的选择和初始化，需要更高效且理论可靠的方法。

Method: 基于SaFARi框架和WaLRUS SSMs，提出W4S4模型，利用冗余小波框架构建，支持快速核计算且无需低秩近似。

Result: W4S4在长时信息保留、延迟重构任务、分类基准和长序列建模中表现优于HiPPO-based SSMs。

Conclusion: 基于小波的状态动态初始化提供了显著优势，W4S4为下一代深度SSM模型提供了可扩展且多功能的基础。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [217] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的强化学习方法PIESMC，用于构建具有代表性的驾驶循环，显著降低了计算成本并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 精确的驾驶循环构建对车辆设计、燃油经济性分析和环境影响评估至关重要。

Method: 采用物理信息强化学习框架结合蒙特卡洛采样（PIESMC），捕捉瞬态动力学、加减速、怠速和坡度变化。

Result: 实验显示，PIESMC在关键运动学和能量指标上表现优异，比MTB方法减少57.3%的误差，比MCB方法减少10.5%的误差，且速度提升近一个数量级。

Conclusion: PIESMC能高效且准确地构建驾驶循环，适用于实际应用。

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [218] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: SurvBESA是一种新型集成模型，结合了Beran估计器和自注意力机制，用于生存分析，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生存分析因数据截尾问题面临挑战，传统集成模型预测不稳定。

Method: SurvBESA通过自注意力机制平滑预测的生存函数，并探索了Huber污染模型简化训练。

Result: 数值实验表明SurvBESA优于现有方法。

Conclusion: SurvBESA提供了一种更稳定的生存分析解决方案，代码已公开。

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [219] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: 本文介绍了TokenBreak，一种利用分词策略绕过文本分类保护模型的新型攻击方法，并提出了一种无需重新训练防御模型的防护策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示现有文本分类保护模型的分词策略漏洞，这些漏洞可能被利用来绕过防御，导致目标系统仍易受攻击。

Method: 通过操纵输入文本，利用分词策略使保护模型产生错误分类，同时确保目标系统仍能理解并响应被操纵的文本。

Result: 研究发现，基于模型架构的分词策略可预测其易受攻击性，并提出了有效的防御策略。

Conclusion: TokenBreak攻击揭示了现有保护模型的局限性，提出的防御策略为增强安全性提供了可行方案。

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [220] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 论文提出了一种成本感知方法，用于平衡低成本但可能不准确的弱评分器（如模型自动评分器）与高成本但更准确的强评分器（如人工评分）的使用，以在有限标注预算下最大化统计效率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的开发需要持续评估和数据标注，但成本高昂。快速迭代中常依赖低成本但可能引入偏差的合成标注数据。

Method: 基于主动和预测驱动的统计推断，提出一系列成本最优策略，分配标注预算以最大化统计效率。

Result: 在任务样本难度差异大的情况下，新策略能以更低预算达到与传统方法相同的估计精度。

Conclusion: 新方法在特定条件下显著优于传统评估方法，尤其适用于样本难度差异大的任务。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [221] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: 本文通过神经切线核（NTK）理论分析了基于Chebyshev的物理信息Kolmogorov-Arnold网络（cPIKANs）的训练动态和收敛行为，揭示了其学习效率与核结构演化的关系。


<details>
  <summary>Details</summary>
Motivation: cPIKANs在求解偏微分方程（PDEs）中表现出潜力，但其训练动态和收敛行为缺乏理论和数值研究。本文旨在填补这一空白。

Method: 利用NTK理论分析cPIKANs，研究核结构在梯度训练中的演化及其对学习效率的影响，并扩展到物理信息场景。

Result: 研究发现cPIKANs的NTK行为可预测，揭示了标准物理信息神经网络（PINNs）无法捕捉的学习动态。谱分析还揭示了域分解对训练的改进时机。

Conclusion: 本文首次系统研究了cPIKANs的NTK，为其经验表现提供了理论解释和预测。

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [222] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为ShockCast的两阶段机器学习方法，用于建模高速流体流动，并通过自适应时间步长解决激波等突然变化现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注低速流体流动，而高速流动（接近或超过音速）会出现激波等突然变化，需要自适应时间步长方法以平衡计算成本和分辨率。

Method: ShockCast分为两阶段：第一阶段用机器学习模型预测时间步长，第二阶段将预测步长与当前流体场结合推进系统状态。采用了基于神经ODE和专家混合的步长条件策略。

Result: 通过生成两个超音速流动数据集验证了方法的有效性，代码和数据已公开。

Conclusion: ShockCast是首个学习高速流动的框架，为相关研究提供了新工具和数据集。

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [223] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: HeuriGym是一个评估LLM生成启发式算法的框架，用于组合优化问题，通过代码执行反馈迭代优化解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分评估LLM在推理和问题解决中的能力，存在封闭式问题饱和或主观性不足的问题。

Method: 提出HeuriGym框架，让LLM生成启发式算法并通过代码执行反馈迭代优化，使用QYI指标量化性能。

Result: 测试9个顶尖模型，发现其在工具使用、规划和自适应推理方面存在局限，QYI得分最高仅0.6，远低于专家基准1。

Conclusion: HeuriGym开源基准旨在推动LLM在科学和工程领域更有效和实际的问题解决能力发展。

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [224] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 论文提出了一种基于Lyapunov谱（LS）的距离度量方法，用于高效选择最优的剪枝策略（超剪枝），显著减少搜索时间并提升剪枝模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法在确定最优配置时计算成本高且缺乏早期性能保证，因此需要一种能早期预测剪枝模型性能的方法。

Method: 提出LS-based距离度量，结合超参数优化算法，形成LSH框架，用于高效选择剪枝策略。

Result: 在多个网络架构和数据集上，LSH显著减少搜索时间，且剪枝模型性能优于传统方法和原始密集模型。

Conclusion: LSH为网络剪枝提供了一种高效且性能优越的解决方案，具有实际应用潜力。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [225] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种新的测试时间扩展维度——交互扩展，通过增加代理的交互范围，实现探索、回溯和动态重新规划等行为。基于Gemma 3 12B模型的TTI方法在WebVoyager和WebArena基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间扩展范式依赖生成长推理轨迹，但无法让代理从环境中获取新信息或随时间调整行为。

Method: 提出TTI（测试时间交互），一种基于课程的在线强化学习方法，通过自适应调整代理的交互范围来训练代理。

Result: TTI在WebVoyager和WebArena基准测试中实现了最先进的性能，并能自适应平衡探索与利用。

Conclusion: 交互扩展是计算扩展的有力补充，为训练自适应代理提供了新途径。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [226] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: DesRUTGe是一个结合深度强化学习和去中心化联邦学习的框架，用于生成高保真、时间变化的城市交通模式，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确性、可扩展性或隐私保护方面存在不足，尤其是在大规模场景下生成真实交通模式仍具挑战性。

Method: DesRUTGe通过去中心化联邦学习，将每个交通检测器及其对应区域作为独立学习节点，利用最小历史数据训练本地DRL模型，并通过与邻近节点交换参数优化性能。

Result: 在巴塞罗那真实数据上的评估显示，DesRUTGe在准确性和隐私保护方面优于标准SUMO工具和其他集中式学习方法。

Conclusion: DesRUTGe为城市规划和智能交通系统提供了一种更准确、可扩展且隐私友好的交通模式生成方法。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [227] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 生成模型在图像和视频生成中表现出色，但用于生成神经网络权重时，现有方法主要通过记忆训练数据，无法生成新颖且高性能的权重。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型在神经网络权重合成中的潜力，评估其是否能生成不同于训练数据的高性能权重。

Method: 研究了四种代表性方法，分析其生成权重的表现，并与简单基线方法（如添加噪声或权重集成）进行比较。

Result: 发现现有方法主要通过记忆训练数据生成权重，无法超越简单基线方法。修改建模因素或数据增强也无法有效缓解记忆问题。

Conclusion: 研究揭示了当前生成模型在新领域中的局限性，强调了对生成模型更谨慎评估的必要性。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


### [228] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET是一种新型的重新参数化训练算法，通过正交等价变换优化神经元，提升大语言模型的训练效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的训练效果和稳定性是当前AI领域的重要挑战，需要一种更有效的训练方法。

Method: POET通过重新参数化神经元，使用两个可学习的正交矩阵和一个固定的随机权重矩阵，保持权重矩阵的谱特性，从而稳定优化目标函数。

Result: 实验证明POET在训练大语言模型时具有有效性和可扩展性。

Conclusion: POET为大规模神经网络的训练提供了一种稳定且高效的解决方案。

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [229] [Quantum Information-Theoretical Size Bounds for Conjunctive Queries with Functional Dependencies](https://arxiv.org/abs/2506.07552)
*Valter Uotila,Jiaheng Lu*

Main category: quant-ph

TL;DR: 论文探讨了利用量子信息理论中的Rényi熵替代经典香农熵，以简化约束条件下联合查询的紧致最坏情况大小界限的计算问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多约束条件下的联合查询问题时，计算紧致最坏情况大小界限复杂且可能需要无限不等式，而量子信息理论尚未被用于解决此问题。

Method: 提出用量子Rényi熵替代经典香农熵，仅需非负性不等式，简化优化空间描述。

Result: 虽然量子优化带来新挑战，但提出了量子版本的最坏情况大小界限，经典界限可作为其特例。

Conclusion: 量子信息理论为数据库理论研究提供了新方向，未来可能进一步探索其应用。

Abstract: Deriving formulations for computing and estimating tight worst-case size
increases for conjunctive queries with various constraints has been at the core
of theoretical database research. If the problem has no constraints or only one
constraint, such as functional dependencies or degree constraints, tight
worst-case size bounds have been proven, and they are even practically
computable. If the problem has more than one constraint, computing tight bounds
can be difficult in practice and may even require an infinite number of linear
inequalities in its optimization formulation. While these challenges have been
addressed with varying methods, no prior research has employed quantum
information theory to address this problem. In this work, we establish a
connection between earlier work on estimating size bounds for conjunctive
queries with classical information theory and the field of quantum information
theory. We propose replacing the classical Shannon entropy formulation with the
quantum R\'enyi entropy. Whereas classical Shannon entropy requires infinitely
many inequalities to characterize the optimization space, R\'enyi entropy
requires only one type of inequality, which is non-negativity. Although this is
a promising modification, optimization with respect to the quantum states
instead of classical distributions creates a new set of challenges that prevent
us from finding a practically computable, tight worst-case size bound. In this
line, we propose a quantum version to derive worst-case size bounds. The
previous tight classical worst-case size bound can be viewed as a special limit
of this quantum bound. We also provide a comprehensive background on prior
research and discuss the future possibilities of quantum information theory in
theoretical database research.

</details>


### [230] [Optimal quantum sampling on distributed databases](https://arxiv.org/abs/2506.07724)
*Longyun Chen,Jingcheng Liu,Penghui Yao*

Main category: quant-ph

TL;DR: 本文研究了分布式环境下的量子采样问题，提出了顺序和并行两种算法，并证明了它们的优化性。


<details>
  <summary>Details</summary>
Motivation: 大规模量子存储成本高昂，因此探索在分布式环境中实现量子采样的方法具有重要意义。

Method: 假设数据分布在多台机器上，每台机器维护一个基本计数预言机。协调器通过查询这些机器实现联合数据库的采样，研究了顺序查询和并行查询两种算法。

Result: 提出的顺序和并行算法在各自设置下均被证明是最优的。

Conclusion: 分布式量子采样是可行的，且顺序和并行算法为实际应用提供了高效解决方案。

Abstract: Quantum sampling, a fundamental subroutine in numerous quantum algorithms,
involves encoding a given probability distribution in the amplitudes of a pure
state. Given the hefty cost of large-scale quantum storage, we initiate the
study of quantum sampling in a distributed setting. Specifically, we assume
that the data is distributed among multiple machines, and each machine solely
maintains a basic oracle that counts the multiplicity of individual elements.
Given a quantum sampling task, which is to sample from the joint database, a
coordinator can make oracle queries to all machines. We focus on the oblivious
communication model, where communications between the coordinator and the
machines are predetermined. We present both sequential and parallel algorithms:
the sequential algorithm queries the machines sequentially, while the parallel
algorithm allows the coordinator to query all machines simultaneously.
Furthermore, we prove that both algorithms are optimal in their respective
settings.

</details>


### [231] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 提出了一种基于量子分类器的加权同质集成方法，通过量子并行执行和权重优化提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 集成方法通过结合多个模型提高预测准确性，但传统方法在多样性和权重分配上存在局限。

Method: 使用量子分类器和索引寄存器进行数据编码，通过量子并行执行和权重优化实现集成。

Result: 实证评估表明该方法有效，提供了性能上的改进。

Conclusion: 该方法为量子机器学习中的集成学习提供了新的思路和实现方式。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [232] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: 使用深度神经网络通过强化学习控制量子光学电路生成立方相位态，平均成功率96%，仅需光子数分辨测量作为非高斯资源。


<details>
  <summary>Details</summary>
Motivation: 立方相位态是实现连续变量通用量子计算的充分资源，探索高效生成方法。

Method: 通过强化学习训练深度神经网络控制量子光学电路。

Result: 成功生成立方相位态，平均成功率96%，并可直接生成四次相位门。

Conclusion: 该方法高效且资源需求低，为量子计算提供了新途径。

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [233] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 论文证明无法创建不产生幻觉的大语言模型，并探讨了相关权衡。


<details>
  <summary>Details</summary>
Motivation: 研究为什么大语言模型无法避免幻觉，以及需要权衡哪些因素。

Method: 通过形式化的不可能定理和拍卖模型（Green-Laffont定理）证明。

Result: 证明同时满足四个基本属性（如真实生成、信息守恒等）是不可能的。

Conclusion: 为模型架构、训练目标和评估方法提供了理论基础。

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [234] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 提出了一种基于局部得分匹配技术的序列梯度优化方法，用于解决似然函数难以处理但模型模拟容易的问题。


<details>
  <summary>Details</summary>
Motivation: 研究似然最大化问题，当似然函数难以处理但模型模拟容易时，需要一种高效的方法来近似Fisher得分。

Method: 采用局部得分匹配技术，通过线性参数化构建代理得分模型，获得闭式最小二乘解。

Result: 方法在合成和实际问题上表现优于现有基准，并提供了理论保证。

Conclusion: 该方法快速、灵活且高效，能够平滑似然目标并缓解复杂似然景观的挑战。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [235] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 论文研究了在数据扰动下样本可压缩分布家族的学习问题，提出了两种扰动模型并证明了其可学习性。


<details>
  <summary>Details</summary>
Motivation: 探讨分布家族在数据扰动条件下是否仍具有可学习性，并解决高维分布学习中的开放性问题。

Method: 分析了两种数据扰动模型：加性独立噪声模型和对抗性破坏模型，并开发了扰动-量化框架。

Result: 证明了样本可压缩家族在扰动条件下仍可学习，并给出了样本复杂度的边界。

Conclusion: 研究为高维均匀分布和高斯混合模型的学习提供了新的样本复杂度边界，解决了文献中的开放问题。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [236] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: CoSIM是一种连续半隐式模型，通过连续过渡核提升训练效率，并在图像生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决分层半隐式模型训练收敛慢的问题，提升生成模型的效率和性能。

Method: 引入连续过渡核，实现无模拟训练，并通过精心设计的核保证一致性。

Result: 在图像生成任务中表现优于现有扩散模型加速方法，尤其在FD-DINOv2上表现突出。

Conclusion: CoSIM为生成模型的多步蒸馏提供了新思路，具有高效和性能优势。

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [237] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 提出了一种利用高斯过程和时空细粒度数据估计冲突趋势的新方法，可用于研究冲突陷阱、扩散和暴露，并支持未来冲突预测。


<details>
  <summary>Details</summary>
Motivation: 研究时空冲突模式，以深入了解冲突的动态特征，并为其他估计任务提供控制变量。

Method: 使用高斯过程和高时空分辨率的冲突事件数据，估计时空冲突趋势。

Result: 能够分析冲突陷阱、扩散和暴露，并预测未来冲突模式。

Conclusion: 该方法仅依赖历史冲突数据，实现了高效且先进的冲突趋势估计和预测。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [238] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 提出了一种统一的统计框架，用于LSTM网络的系统模型选择，解决了传统方法启发式且计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LSTM在序列数据建模中广泛应用，但模型选择问题（如超参数调优、架构设计和正则化选择）仍依赖启发式方法且计算昂贵。

Method: 扩展经典模型选择思想（如信息准则和收缩估计）至序列神经网络，提出适应时间结构的惩罚似然、广义阈值方法及变分贝叶斯和近似边际似然估计策略。

Result: 在多个生物医学数据示例中展示了框架的灵活性和性能提升。

Conclusion: 该框架为LSTM模型选择提供了系统且高效的方法，显著提升了性能。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [239] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: Half-AVAE改进VAE框架，通过对抗网络和外部增强项解决欠定ICA问题，提升隐变量独立性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在欠定ICA（隐变量多于观测信号）中表现不佳，需要改进以增强隐变量的独立性和可解释性。

Method: 提出Half-AVAE，基于无编码器的Half-VAE框架，结合对抗网络和外部增强项，避免显式逆映射。

Result: 实验表明，Half-AVAE在欠定条件下优于基线模型（如GP-AVAE和Half-VAE），RMSE更低。

Conclusion: Half-AVAE展示了VAE在变分推断中的灵活性，为复杂ICA任务提供有效解决方案，推动解耦、因果推断和生成建模的应用。

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [240] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: 研究分位数最优策略学习，提出因果辅助方法解决未观测混杂和数据集覆盖不足问题，理论保证样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决在存在未观测混杂因素和数据集覆盖不足的情况下，学习分位数最优策略的挑战。

Method: 利用因果推断工具（如工具变量和负控制）估计分位数目标，采用极小极大估计和非参数模型，构建保守策略估计。

Result: 提出方法在温和条件下具有理论保证，策略学习算法样本效率高。

Conclusion: 首次提出在未观测混杂存在时，样本高效的分位数最优策略学习算法。

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [241] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: ALINE是一个统一框架，结合了贝叶斯推断和主动数据采集，通过强化学习训练Transformer架构，实现高效数据查询和即时推断。


<details>
  <summary>Details</summary>
Motivation: 许多关键应用需要系统能够战略性地获取最有信息量的数据并即时进行推断，现有方法无法完全满足这一需求。

Method: ALINE采用Transformer架构，通过强化学习训练，奖励基于自身推断组件的信息增益估计，支持定向查询策略。

Result: 实验表明，ALINE在回归主动学习、贝叶斯实验设计和心理测量模型中均表现出高效的数据选择和准确推断能力。

Conclusion: ALINE为即时推断和高效数据采集提供了一种统一的解决方案，适用于多种任务和参数子集。

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [242] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: 本文提出了一种名为R2-G2的梯度估计器，作为重参数化梯度估计器的Rao-Blackwellisation版本，并展示了其在贝叶斯多层感知机中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过Rao-Blackwellisation改进重参数化梯度估计器，以提升梯度估计的效率和模型性能。

Method: 提出R2-G2估计器，通过Rao-Blackwellisation对重参数化梯度进行优化，并验证其在贝叶斯多层感知机中的有效性。

Result: R2-G2在初始训练中表现优于传统重参数化梯度估计器，尤其适用于多次应用重参数化技巧的模型。

Conclusion: R2-G2扩展了Rao-Blackwellised梯度的优势，为概率模型提供了更高效的梯度估计方法。

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [243] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 提出一种用于线性因果模型中变化点监测的算法，通过集中化技术和干预节点选择，放大变化幅度，并验证其最优性。


<details>
  <summary>Details</summary>
Motivation: 解决线性因果模型中变化点监测的问题，尤其是考虑干预的影响。

Method: 采用集中化技术将变化集中到单一维度，基于KL散度选择干预节点，并提出两种监测方法。

Result: 理论证明方法的一阶最优性，并通过模拟和实际案例验证其有效性。

Conclusion: 所提算法在变化点监测中表现优异，能有效平衡探索与利用。

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [244] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 论文研究了在约束域上采样目标概率分布的问题，重点分析了基于偏斜反射非可逆Langevin动力学（SRNLD）的长期行为，提出了偏斜对称矩阵的设计方法，并通过大偏差理论和数值实验验证了其加速收敛的效果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习等应用中，约束域上的概率分布采样是一个重要问题。尽管已有多种Langevin算法被提出，但如何设计偏斜对称矩阵以优化性能尚不明确。

Method: 通过建立偏斜反射非可逆Langevin动力学（SRNLD）的大偏差原理（LDP），并设计偏斜对称矩阵使其与边界内法向量场的乘积为零，理论分析了其加速收敛的效果。

Result: 理论分析表明，设计的偏斜对称矩阵能加速SRNLD收敛到目标分布。数值实验验证了该方法的优越性能。

Conclusion: 论文证明了设计的偏斜对称矩阵能有效提升SRNLD的性能，为约束域采样提供了理论支持和实践指导。

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [245] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本文探讨了数字孪生技术在AI模拟中的应用，通过系统调查22项研究，总结了技术趋势并提出了参考框架。


<details>
  <summary>Details</summary>
Motivation: 解决现代亚符号AI中数据量和质量不足的问题，利用数字孪生技术提供高保真虚拟环境以支持AI开发。

Method: 通过系统调查22项主要研究，分析技术趋势并构建参考框架，结合ISO 23247标准提出架构指南。

Result: 提出了数字孪生与AI组件的参考框架，并基于ISO 23247标准提供了架构指南。

Conclusion: 数字孪生为AI模拟提供了新途径，但仍面临挑战，未来研究需进一步探索其潜力。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [246] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: 论文提出ReVD框架，通过合成推理数据和优化漏洞偏好，显著提升大语言模型在漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在漏洞检测中表现有限，主要因缺乏推理数据和语义学习不足。

Method: 构建漏洞和修复代码的前后向推理过程，设计三重监督微调和课程在线偏好优化。

Result: 在PrimeVul和SVEN数据集上，ReVD性能提升12.24%-22.77%。

Conclusion: ReVD为基于LLM的漏洞检测设定了新标准，代码和数据已开源。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [247] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: SIGMA框架通过重新利用MCTS搜索树中被丢弃的兄弟节点，提升LLM的推理能力，显著减少数据需求并提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型的数据质量成为关键，传统MCTS方法仅保留最优路径，浪费了大量有价值的部分见解和错误模式。

Method: 提出SIGMA框架，通过语义链接兄弟节点，分两阶段（批评模型和修订模型）优化推理路径。

Result: 在MATH基准测试中，SIGMA优化的7B模型仅用30K样本达到54.92%准确率，优于使用590K样本的SOTA模型。

Conclusion: SIGMA通过利用非最优推理分支的有价值信号，显著提升LLM推理能力并减少数据需求。

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [248] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 论文提出了一种无需训练的框架CER，通过动态记忆缓冲区积累和综合过去的经验，帮助LLM代理在复杂任务中自我提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在复杂任务中缺乏环境特定经验，且无法在推理时持续学习，限制了其适应性。

Method: CER通过动态记忆缓冲区积累环境动态和决策模式，使代理能在新任务中检索和增强相关知识。

Result: 在VisualWebArena和WebArena基准测试中，CER分别达到31.9%和36.7%的成功率，相对GPT-4o基线提升51.0%。

Conclusion: CER有效提升了LLM代理在复杂环境中的适应性和性能，证明了其高效性和有效性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [249] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM框架通过结合贝叶斯推断和强化学习，提升LLM在结构化领域中的预测能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在特定领域（如模拟环境）中难以生成精确预测，因其无法将广泛知识落地到具体环境。

Method: WorldLLM利用LLM的上下文学习能力，通过自然语言假设指导预测，并通过贝叶斯推断和强化学习迭代优化假设与证据。

Result: 实验证明WorldLLM在文本游戏环境中提升了预测准确性，并生成可解释的环境动态理论。

Conclusion: WorldLLM通过自主探索与假设优化，显著提升了LLM在结构化环境中的表现。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [250] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 该论文提出了一种通过预处理缩小假设空间的方法，显著减少了归纳逻辑编程（ILP）的学习时间，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 目标是通过减少假设空间的规模来提高ILP系统的效率，避免搜索无效规则。

Method: 利用背景知识识别并移除不可能出现在最优假设中的规则，采用答案集编程实现。

Result: 实验表明，该方法在多个领域（如视觉推理和游戏）中，能将学习时间从10小时以上缩短至2秒。

Conclusion: 该方法通过预处理显著提升了ILP系统的效率，且不影响预测准确性。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [251] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 论文探讨了脉冲神经网络（SNN）训练中的挑战，比较了不同学习算法的分类准确性和计算成本，提出了一种结合SNN和Lempel-Ziv复杂度（LZC）的生物启发分类器。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中因时间动态性、脉冲事件不可微分性和稀疏事件驱动激活带来的挑战，并比较不同学习算法的性能。

Method: 提出了一种结合SNN和LZC的生物启发分类器，比较了经典反向传播算法和生物启发学习算法（如tempotron和Spikprop）的性能。

Result: 经典反向传播算法分类准确率高但计算成本极高，生物启发算法在保持竞争力的同时提高了计算效率。

Conclusion: 选择合适的学习算法需权衡分类准确性和计算成本，生物启发算法更适合实时应用。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [252] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 本文系统研究了大型推理模型（LRMs）在可控谜题环境中的表现，揭示了其在复杂度增加时的准确性崩溃和反直觉的扩展限制，并与标准语言模型进行了对比。


<details>
  <summary>Details</summary>
Motivation: 当前对大型推理模型的评估主要关注数学和编程基准的最终答案准确性，缺乏对其推理过程的理解。本文旨在填补这一空白。

Method: 使用可控谜题环境，精确操纵复杂度，分析LRMs的推理痕迹和最终答案。

Result: LRMs在复杂度超过一定阈值时准确性崩溃，且推理努力先增后减。与标准模型相比，LRMs在中等复杂度任务中表现更优，但在高复杂度任务中两者均崩溃。

Conclusion: LRMs在精确计算和一致性推理方面存在局限，其推理能力仍需进一步研究。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [253] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 论文提出了一种结合神经科学和动物行为学工具的方法，用于分析深度强化学习（DRL）智能体在复杂环境中的行为，揭示了其策略、记忆和规划的细节。


<details>
  <summary>Details</summary>
Motivation: 当前DRL智能体的行为分析方法不足，尤其是在任务和智能体复杂度增加时，需要更深入的工具来理解其行为。

Method: 在ForageWorld环境中，应用神经科学和动物行为学的工具，对DRL智能体进行行为和神经联合分析。

Result: 发现基于RNN的无模型DRL智能体可以通过涌现动态表现出类似规划的行为，无需显式记忆模块或世界模型。

Conclusion: 通过借鉴生物智能研究方法，可以揭示DRL智能体学习动态中的丰富结构，为未来复杂智能体的行为分析和安全对齐提供框架。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [254] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 提出了一种名为HeTa的关系感知异构图基础攻击模型，通过挖掘共享攻击单元实现跨HGNN的通用扰动。


<details>
  <summary>Details</summary>
Motivation: 现有HGNN攻击方法需复杂参数重训练，无法快速适应新场景，而基础模型为通用攻击提供了可能。

Method: 设计基础代理模型对齐异质性，识别共享关系感知攻击单元，实现序列化关系攻击。

Result: 实验显示HeTa具有强大的攻击性能和泛化能力。

Conclusion: HeTa为HGNN提供了一种通用且高效的攻击方法，揭示了共享漏洞模式的重要性。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [255] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种结合强化学习（RL）和监督微调（SFT）的新方法ReLIFT，以克服RL在扩展大语言模型（LLM）推理能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管RL能诱导LLM的复杂行为（如规划和自我反思），但其无法超越基础模型的限制。因此，需要一种方法结合RL和SFT的优势，以引入新知识和推理模式。

Method: 提出ReLIFT方法，交替使用RL和在线微调（SFT），在遇到难题时收集高质量解决方案进行微调。

Result: ReLIFT在五个竞赛级基准和一个分布外基准上平均提升5.2分，且仅需13%的详细演示数据。

Conclusion: ReLIFT克服了RL的局限性，展示了结合RL和SFT的潜力，显著提升了LLM的推理能力。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [256] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文介绍了语义时空图模型的形式化方面，重点讨论了其在定向知识表示和过程建模中的应用。通过定义有限的γ(3,4)表示，形成可扩展的操作闭集。模型通过吸收状态和信息泄漏问题，揭示了边界信息与意图性的关联。


<details>
  <summary>Details</summary>
Motivation: 研究语义时空图模型的形式化特性，以支持可扩展的知识表示和过程建模，同时解决信息泄漏和吸收状态的问题。

Method: 定义有限的γ(3,4)表示，形成操作闭集；通过语义时空假设为图路径提供可预测性；分析吸收状态与信息泄漏的关系。

Result: 模型揭示了吸收状态与边界信息的关联，表明信息泄漏问题与除零操作类似，需手动注入补救信息。

Conclusion: 语义时空图模型及其承诺理论起源有助于理解吸收状态与边界信息的关系，为意图性引入提供了理论基础。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [257] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 论文提出了一种新的不等式证明任务形式，并发布了IneqMath数据集，评估了29个大型语言模型（LLMs），发现即使顶级模型在分步审查下准确率不足10%。


<details>
  <summary>Details</summary>
Motivation: 研究不等式证明对LLMs的挑战，揭示现有数据集的不足，并探索更有效的评估方法。

Method: 将不等式证明分为两个可自动检查的子任务（边界估计和关系预测），并开发了LLM-as-judge评估框架。

Result: 顶级LLMs在分步审查下准确率显著下降，暴露了推理链的脆弱性。

Conclusion: 模型规模和计算资源对证明正确性的提升有限，未来研究方向包括定理引导推理和自我优化。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [258] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: Gradients是一个去中心化的AutoML平台，通过竞争性市场机制优化超参数配置，显著优于现有集中式方法。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML平台依赖单一优化策略，仅探索部分可行超参数配置，限制了性能提升。

Method: Gradients将超参数优化转化为竞争性市场，独立矿工通过经济激励竞争发现最优配置。

Result: 在180个实验中，Gradients以82.8%胜率优于HuggingFace AutoTrain，对复杂任务提升30-40%。

Conclusion: 竞争性经济驱动方法能系统性发现集中式AutoML遗漏的优越配置。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [259] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: 研究通过偏好学习优化新闻标题推荐策略，发现嘈杂环境下显式探索可能不必要。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过偏好学习优化新闻标题推荐，并研究翻译对用户参与度预测的影响。

Method: 使用真实用户交互数据，在上下文赌博机设置下训练标题推荐代理。

Result: 在嘈杂上下文中，显式探索可能不必要，可采用更简单高效的策略。

Conclusion: 研究为实践中简化推荐策略提供了可能性。

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [260] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 论文提出了一种新的投影方法，将向量数据集嵌入到q度量空间中，利用度量树的强三角不等式性质减少精确搜索的比较次数，并在高维数据中实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有向量搜索算法忽视了向量嵌入的度量结构，未能充分利用其潜在特性。

Method: 提出一种投影方法，将向量数据集嵌入到q度量空间，并学习近似投影以高效转换查询点。

Result: 实验表明，该方法在高维文本和图像向量嵌入中，使经典度量树算法性能与最先进搜索方法相当。

Conclusion: 通过利用q度量空间的特性，可以显著提升向量搜索的效率与性能。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [261] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: 提出了一种基于控制函数的两阶段方法，用于纠正隐式反馈数据中的位置偏差，无需点击或倾向模型知识，且适用于非线性排名模型。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据（如用户点击）常用于学习排序系统，但存在多种偏差（如位置偏差），直接使用会导致次优排名性能。

Method: 通过两阶段过程纠正位置偏差：第一阶段利用排名过程的残差外生变量，第二阶段修正点击方程中的偏差。方法无需点击或倾向模型知识，支持非线性排名模型。

Result: 实验结果表明，该方法在纠正位置偏差方面优于现有技术。

Conclusion: 该方法通用性强，可应用于任何先进排名算法，并通过去偏验证点击优化超参数选择。

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [262] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: RADAR框架通过异步离线计算提升推荐系统的召回率，显著提高用户参与度。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推荐系统中初始检索阶段效率低、精度不足的问题，尤其是难以区分高相关性和高参与度内容。

Method: 引入RADAR框架，利用异步离线计算预排名更大候选集，存储高质量结果用于在线推理。

Result: 离线实验显示召回率提升2倍，在线A/B测试验证用户参与度提升0.8%。

Conclusion: RADAR是一种在严格在线服务约束下提升推荐质量的实用有效方法。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [263] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: 论文提出了一种针对金融时间序列的预训练模型Delphyne，解决了现有模型在金融领域表现不佳的问题，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预训练模型在金融应用中表现不佳，主要因为缺乏金融数据预训练和跨领域负迁移效应。

Method: 提出Delphyne模型，专门针对金融时间序列数据的特点进行预训练。

Result: Delphyne在公开数据集上表现优异，优于现有基础模型和全样本模型。

Conclusion: Delphyne为金融时间序列分析提供了有效的解决方案，具有广泛的应用潜力。

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [264] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: 论文提出了一种结合Transformer时间序列模型和可解释人工智能（XAI）的新方法，以提高股票价格预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融素养越来越依赖于解读复杂金融数据和使用高级预测工具的能力，因此需要一种既能准确预测又能解释预测结果的方法。

Method: 研究采用了DLinear、LTSNet、Vanilla Transformer和Time Series Transformer等模型，结合技术指标作为输入特征，并使用SHAP和LIME技术解释模型输出。

Result: 结果表明Transformer模型具有强大的预测能力，且可解释机器学习有助于个人做出明智的投资决策。

Conclusion: 该方法展示了Transformer模型和可解释机器学习在金融预测中的潜力，能够帮助个人更积极地参与金融市场。

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [265] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: 论文研究了音频合成器参数反演问题中的对称性问题，提出了一种基于条件生成模型和置换等变连续归一化流的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 音频合成器存在参数配置的多解性（对称性），导致从声音反演参数是一个不适定问题。论文旨在解决这一问题。

Method: 通过条件生成模型和置换等变连续归一化流处理对称性，并提出自适应发现对称性的策略。

Result: 在Surge XT合成器上，该方法在音频重建指标上优于回归和生成基线。

Conclusion: 通过建模对称性，生成模型显著提升了参数反演的准确性。

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [266] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 论文提出了一种语义-声学源追踪网络（SASTNet），用于解决基于神经音频编解码器的深度伪造语音（CodecFake）源追踪性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟数据上训练时难以泛化到真实数据，且容易过拟合非语音区域。

Method: 结合Whisper的语义特征编码和Wav2vec2与AudioMAE的声学特征编码，提出SASTNet。

Result: 在CodecFake+数据集的CoSG测试集上达到最优性能。

Conclusion: SASTNet能有效提升源追踪的可靠性。

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [267] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 本文提出了一种轻量级单流多模态学习框架，用于音频-视觉深度伪造检测，通过协作学习块和多模态分类模块，显著减少了参数数量并提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用两个独立的子模型分别学习音频和视觉特征，导致冗余和效率低下，且未能充分利用音频与视觉特征的固有相关性。

Method: 设计了一个轻量级单流网络，引入协作音频-视觉学习块和多模态分类模块，实现跨层连续融合多模态特征。

Result: 在DF-TIMIT、FakeAVCeleb和DFDC数据集上，仅用0.48M参数即优于现有方法，且在单模态和多模态深度伪造检测中表现优异。

Conclusion: 该方法高效轻量，适用于资源受限环境，并能有效检测未知类型的深度伪造内容。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [268] [Delegation with Costly Inspection](https://arxiv.org/abs/2506.07162)
*Mohammad T. Hajiaghayi,Piotr Krysta,Mohammad Mahdavi,Suho Shin*

Main category: cs.GT

TL;DR: 论文研究了带有检查成本的委托选择问题（DCIC），扩展了Pandora's box问题，并提出了近似机制。


<details>
  <summary>Details</summary>
Motivation: 探索在代理人可能策略性误报效用的情况下，委托人如何通过检查最大化自身效用，同时考虑委托和检查成本。

Method: 分析了无成本委托设置下的近似机制，并扩展到一般情况。

Result: 证明了在无成本委托下，3-近似机制是紧的，且在检查成本相同的情况下可改进为2-近似。

Conclusion: DCIC是PNOI的推广，提出了适用于广泛实例的常数因子近似机制。

Abstract: We study the problem of delegated choice with inspection cost (DCIC), which
is a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18)
as well as an extension of the Pandora's box problem with nonobligatory
inspection (PNOI) by Doval (JET'18). In our model, an agent may strategically
misreport the proposed element's utility, unlike the standard delegated choice
problem which assumes that the agent truthfully reports the utility for the
proposed alternative. Thus, the principal needs to inspect the proposed element
possibly along with other alternatives to maximize its own utility, given an
exogenous cost of inspecting each element. Further, the delegation itself
incurs a fixed cost, thus the principal can decide whether to delegate or not
and inspect by herself.
  We show that DCIC indeed is a generalization of PNOI where the side
information from a strategic agent is available at certain cost, implying its
NP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless
delegation setting in which the cost of delegation is free. We prove that the
maximal mechanism over the pure delegation with a single inspection and an PNOI
policy without delegation achieves a $3$-approximation for DCIC with costless
delegation, which is further proven to be tight. These results hold even when
the cost comes from an arbitrary monotone set function, and can be improved to
a $2$-approximation if the cost of inspection is the same for every element. We
extend these techniques by presenting a constant factor approximate mechanism
for the general setting for rich class of instances.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [269] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: 本文探讨了如何利用深度强化学习（DRL）优化Power-to-Gas（P2G）系统的经济性运行，结合电池储能系统和燃气轮机，解决了延迟奖励问题，提升了长期储能潜力。


<details>
  <summary>Details</summary>
Motivation: P2G技术虽能整合间歇性可再生能源，但其经济性运行复杂且效率较低，且现有研究多忽视长期储能能力。DRL在能源系统管理中表现优异，但面临延迟奖励的挑战。

Method: 通过三个逐步复杂的案例研究，评估了Deep Q-Networks和Proximal Policy Optimization算法，并引入预测整合、奖励函数惩罚和战略成本计算等改进方法。

Result: 改进后的DRL显著提升了P2G系统的经济性运行能力，克服了初始决策困难，展现了长期储能的潜力。

Conclusion: DRL结合改进方法可有效优化P2G系统的长期运行策略，为其在能源存储中的应用提供了新思路。

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [270] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: 回顾模糊系统的发展历史，强调演化模糊系统在非平稳环境中的优势，并讨论未来挑战。


<details>
  <summary>Details</summary>
Motivation: 纪念模糊集理论60周年，总结模糊建模与控制的经典贡献及演化智能系统的意义。

Method: 通过增量更新规则库结构，从数据流中构建和适应模糊模型。

Result: 演化模糊系统在非平稳环境中表现出显著优势。

Conclusion: 未来需关注安全性、可解释性和结构演化的原则性问题。

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [271] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: 本文提出了一种基于数据辅助控制（DAC）的混合控制框架，用于端口哈密顿系统，通过动态分解处理参数和结构不确定性，结合非线性控制器和强化学习（RL）实现优化控制。


<details>
  <summary>Details</summary>
Motivation: 解决端口哈密顿系统中参数和结构不确定性的管理问题，同时保持系统固有结构，并通过混合控制框架提升性能、安全性和可解释性。

Method: 将系统动态分解为右端（RHS）和左端（LHS），分别由非线性控制器和RL处理；通过虚拟端口变量连接两部分。

Result: 框架成功管理了RHS的不确定性，提升了性能、安全性和可解释性，并通过仿真验证了其有效性。

Conclusion: 该混合控制框架为端口哈密顿系统提供了一种高效且可解释的控制方法，未来可进一步研究其理论和应用。

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [272] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: 提出了一种基于控制屏障函数（CBFs）的风险敏感安全过滤器，用于具有不确定动态的离散时间多智能体系统，确保在模型不确定性下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在集中协调不切实际的场景中，确保多智能体系统的安全性是一个重要挑战。

Method: 利用基于指数风险算子的集中式风险敏感安全条件，并提出了两种分布式策略：基于最坏情况预测和基于已知安全策略的接近性。

Result: 通过数值评估验证了该方法在保持安全性的同时避免了过度保守。

Conclusion: 该方法有效解决了多智能体系统中的安全性问题，且具有实际可行性。

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [273] [CNFs and DNFs with Exactly $k$ Solutions](https://arxiv.org/abs/2506.07268)
*L. Sunil Chandran,Rishikesh Gajjala,Kuldeep S. Meel*

Main category: cs.DM

TL;DR: 论文研究了构造具有恰好k个满足赋值的DNF或CNF公式所需的最小项数或子句数问题，并给出了上界和下界。


<details>
  <summary>Details</summary>
Motivation: 加权模型计数在概率推理、网络可靠性等领域有广泛应用，而将其转换为无加权模型计数时，最小项数或子句数的确定是关键问题。

Method: 通过构造单调DNF公式，证明了上界为O(√log k log log k)，并通过理论分析给出了下界Ω(log log k)。

Result: 首次提出了o(log k)的上界，并证明了对某些k值，任何DNF或CNF表示至少需要Ω(log log k)项或子句。

Conclusion: 这些结果对基于公式转换的模型计数算法的效率有重要影响。

Abstract: Model counting is a fundamental problem that consists of determining the
number of satisfying assignments for a given Boolean formula. The weighted
variant, which computes the weighted sum of satisfying assignments, has
extensive applications in probabilistic reasoning, network reliability,
statistical physics, and formal verification. A common approach for solving
weighted model counting is to reduce it to unweighted model counting, which
raises an important question: {\em What is the minimum number of terms (or
clauses) required to construct a DNF (or CNF) formula with exactly $k$
satisfying assignments?}
  In this paper, we establish both upper and lower bounds on this question. We
prove that for any natural number $k$, one can construct a monotone DNF formula
with exactly $k$ satisfying assignments using at most $O(\sqrt{\log k}\log\log
k)$ terms. This construction represents the first $o(\log k)$ upper bound for
this problem. We complement this result by showing that there exist infinitely
many values of $k$ for which any DNF or CNF representation requires at least
$\Omega(\log\log k)$ terms or clauses. These results have significant
implications for the efficiency of model counting algorithms based on formula
transformations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [274] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为LIET的框架，通过个体学习和团队进化提升LLM在多智能体环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体规划算法在适应多智能体场景时表现不足，需要改进。

Method: 采用LIET范式，结合个体学习（局部效用函数）和团队进化（共享合作知识列表）。

Result: 在多个基准测试中，LIET表现优于现有基线，展现出更强的合作规划能力。

Conclusion: LIET框架有效提升了LLM在多智能体环境中的适应性和合作能力。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [275] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 论文提出G-Memory，一种分层、代理化的记忆系统，用于解决多代理系统（MAS）中记忆架构不足的问题，显著提升了任务执行成功率。


<details>
  <summary>Details</summary>
Motivation: 当前多代理系统的记忆机制过于简单，忽视了代理间协作的复杂性，且缺乏跨任务和代理个性化的定制能力。

Method: 引入G-Memory，基于组织记忆理论的三层图层次结构（洞察图、查询图和交互图），通过双向记忆遍历检索高层次的通用知识和细粒度的协作轨迹。

Result: 在五个基准测试中，G-Memory显著提升了任务成功率（最高20.89%）和知识问答准确性（最高10.12%）。

Conclusion: G-Memory为多代理系统的记忆架构提供了有效解决方案，支持代理团队的渐进式进化。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [276] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: MedChat是一个多智能体诊断框架，结合专用视觉模型和角色特定的LLM智能体，以提高青光眼检测的可靠性和临床报告效率。


<details>
  <summary>Details</summary>
Motivation: 解决通用LLM在医学影像应用中存在的幻觉、解释性不足和领域知识缺乏问题，同时模拟多学科医疗团队的复杂推理。

Method: 提出MedChat框架，结合专用视觉模型和多个角色特定的LLM智能体，由导演智能体协调，实现交互式诊断报告。

Result: 提高了可靠性，减少幻觉风险，并为临床审查和教育用途提供定制化界面。

Conclusion: MedChat通过多智能体协作，有效解决了医学影像诊断中的挑战，提升了临床应用的准确性和效率。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [277] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: 论文研究了使用泊松中点离散化方法从强对数凹分布中采样的效率，证明了其在2-Wasserstein距离下的收敛性，并展示了其在目标精度上的立方加速效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地从强对数凹分布中采样，特别是通过改进的离散化方法（泊松中点法）提升收敛速度。

Method: 采用泊松中点离散化方法（随机中点法的变体）对过阻尼/欠阻尼Langevin动力学进行离散化，分析其在2-Wasserstein距离下的收敛性。

Result: 在2-Wasserstein距离下实现了收敛，且在目标精度上比Euler-Maruyama离散化方法快立方级；欠阻尼Langevin动力学的复杂度远低于文献中L^2强误差的复杂度下界。

Conclusion: 泊松中点离散化方法在强对数凹分布采样中表现出显著优势，尤其是在欠阻尼Langevin动力学中，其收敛复杂度优于现有方法。

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [278] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 提出两种一致性损失函数，解决Stable Diffusion微调中的欠拟合和过拟合问题，提升生成图像的多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在生成特定主题图像时存在欠拟合（无法可靠捕捉主题身份）和过拟合（记忆主题图像并减少背景多样性）的问题。

Method: 提出两种辅助一致性损失：先验一致性正则化损失（保持非主题图像的扩散噪声预测与预训练模型一致）和主题一致性正则化损失（增强模型对噪声调制潜码的鲁棒性）。

Result: 实验表明，该方法在CLIP分数、背景多样性和视觉质量上优于DreamBooth，同时保留了主题身份。

Conclusion: 通过引入一致性损失，有效平衡了主题身份保留和图像多样性，提升了微调效果。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [279] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 本文提出了一种架构-算法协同设计方法，通过轴定向光栅化、神经排序和可重构处理阵列，显著提升了3D高斯泼溅（3DGS）在资源受限设备上的实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS在高质量视图合成中表现优异，但在资源受限设备上的实时渲染仍面临功耗和面积限制的挑战。

Method: 1. 轴定向光栅化减少冗余计算；2. 神经排序替代硬件排序器；3. 可重构处理阵列支持光栅化和神经网络推理；4. π轨迹瓦片调度优化内存访问。

Result: 实验显示，设计在保持渲染质量的同时，速度提升23.4~27.8倍，能耗降低28.8~51.4倍。

Conclusion: 该设计显著提升了3DGS在资源受限设备上的效率，并计划开源以推动领域发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [280] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出了一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率的限制。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法受限于输入分辨率，无法重建比训练视图更精细的细节。

Method: 采用轻量级生成模型预测并细化额外的3D高斯分布，结合Hessian辅助采样策略智能选择需要密集化的区域。

Result: 方法在单块消费级GPU上实时运行（0.015秒/推理），几何精度和渲染质量显著优于现有技术。

Conclusion: 提出了一种分辨率无关的3D场景增强新范式。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [281] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D是一种利用预训练3D生成模型隐式先验知识实现高压缩比3D数据压缩的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据压缩方法难以在保持视觉质量的同时实现极高压缩比，Squeeze3D旨在解决这一问题。

Method: 通过可训练映射网络连接预训练编码器和生成模型的潜在空间，将3D数据压缩为紧凑潜在代码，再通过生成模型解压缩。

Result: 实验显示，Squeeze3D对纹理网格、点云和辐射场的压缩比分别达到2187x、55x和619x，且视觉质量接近现有方法。

Conclusion: Squeeze3D无需训练对象特定网络，支持多种3D格式，且压缩和解压缩延迟低，是一种高效灵活的3D数据压缩方案。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [282] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: 本文提出了一种基于深度学习的多日换手量化交易算法，针对中国A股市场，结合多个模块实现高效交易与风险管理，年化收益15.2%，最大回撤低于5%。


<details>
  <summary>Details</summary>
Motivation: 针对中国A股市场的量化交易需求，开发一种结合深度学习与多模块优化的算法，以平衡资本效率与风险管理。

Method: 算法包含五个模块：深度横截面预测选股、混合模型分析开盘信号、动态头寸规模调整、网格搜索优化的止盈止损机制，以及基于多粒度波动的市场择时模型。

Result: 在2010-2020年数据上训练，2021-2024年数据回测显示年化收益15.2%，最大回撤低于5%，夏普比率1.87，适合机构部署。

Conclusion: 该算法在多市场环境下表现稳健，具有高资本容量和优异的可扩展性。

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [283] [Execution-Aware Program Reduction for WebAssembly via Record and Replay](https://arxiv.org/abs/2506.07834)
*Doehyun Baek,Daniel Lehmann,Ben L. Titzer,Sukyoung Ryu,Michael Pradel*

Main category: cs.PL

TL;DR: 论文提出了两种执行感知的程序缩减技术（RR-Reduce和Hybrid-Reduce），通过记录和重放执行行为来优化WebAssembly（Wasm）程序的调试过程。RR-Reduce快速缩小程序规模，Hybrid-Reduce进一步缩减程序大小。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态信息的程序缩减技术难以处理复杂Wasm程序，忽略了执行行为提供的有价值信息。

Method: RR-Reduce通过识别并隔离触发错误的函数，生成缩减程序；Hybrid-Reduce结合执行感知和无感知技术进一步缩减。

Result: RR-Reduce平均将程序缩减至原大小的1.20%，耗时14.5分钟；Hybrid-Reduce缩减至0.13%，耗时3.5小时。

Conclusion: RR-Reduce适合快速调试，Hybrid-Reduce适合需要最小化程序的场景。

Abstract: WebAssembly (Wasm) programs may trigger bugs in their engine implementations.
To aid debugging, program reduction techniques try to produce a smaller variant
of the input program that still triggers the bug. However, existing
execution-unaware program reduction techniques struggle with large and complex
Wasm programs, because they rely on static information and apply syntactic
transformations, while ignoring the valuable information offered by the input
program's execution behavior.
  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program
reduction techniques that leverage execution behaviors via record and replay.
RR-Reduce identifies a bug-triggering function as the target function, isolates
that function from the rest of the program, and generates a reduced program
that replays only the interactions between the target function and the rest of
the program. Hybrid-Reduce combines a complementary execution-unaware reduction
technique with RR-Reduce to further reduce program size.
  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a
diverse set of bugs in three engines. On average, RR-Reduce reduces the
programs to 1.20 percent of their original size in 14.5 minutes, which
outperforms the state of the art by 33.15 times in terms of reduction time.
Hybrid-Reduce reduces the programs to 0.13 percent of their original size in
3.5 hours, which outperforms the state of the art by 3.42 times in terms of
reduced program size and 2.26 times in terms of reduction time. We envision
RR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and
Hybrid-Reduce for scenarios where developers require the smallest possible
programs.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [284] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph是一个基于AI的框架，用于简化和自动化计算化学与材料科学的工作流程，结合图神经网络和大型语言模型，提升效率和易用性。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学和材料科学中至关重要，但设置和执行复杂，需要专家知识和手动操作。ChemGraph旨在解决这些问题。

Method: 利用图神经网络基础模型进行高效计算，结合大型语言模型实现自然语言理解和任务规划，提供交互式界面。

Result: 在13个基准任务中，小型LLM在简单任务上表现良好，复杂任务需更大模型；多代理框架使小型模型在特定场景中匹配或超越GPT-4o。

Conclusion: ChemGraph通过AI和自动化显著提升了原子模拟的效率和可访问性，为复杂任务提供了灵活解决方案。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [285] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: TimberStrike是一种针对基于树的联邦学习模型的优化数据集重建攻击，利用决策树的离散特性，通过分裂值和决策路径推断敏感数据。


<details>
  <summary>Details</summary>
Motivation: 研究基于树的联邦学习模型的安全和隐私问题，填补神经网络之外的研究空白。

Method: 提出TimberStrike攻击方法，利用决策树的特性从单个客户端推断其他客户端的训练数据。

Result: 在多个框架中，攻击成功重建了73.05%至95.63%的目标数据集，差分隐私部分缓解攻击但损害模型性能。

Conclusion: 需设计专门针对基于树的联邦学习系统的隐私保护机制。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [286] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: Amatriciana是一种基于图神经网络的新方法，用于检测交易图中的洗钱者，利用时间信息且无需分割子图。实验表明，该模型在少量数据下表现良好，数据充足时优于其他方法，F1得分为0.76，并将误报率降低55%。


<details>
  <summary>Details</summary>
Motivation: 洗钱对金融安全和社会稳定构成严重威胁，交易量增长需要自动化工具辅助执法机构检测此类犯罪活动。

Method: Amatriciana采用图神经网络，利用完整的交易图（不分割为时间子图）并考虑时间信息。

Result: 模型在公开数据集上表现优异，F1得分为0.76，误报率降低55%。

Conclusion: Amatriciana是一种高效且准确的洗钱检测方法，尤其在数据充足时表现突出。

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [287] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: TimeWak是一种用于多变量时间序列扩散模型的水印算法，通过在真实时间特征空间中嵌入时间链式哈希水印，解决了特征异质性和时间依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 共享隐私敏感数据（如患者功能MRI记录）需要合成数据具有高数据效用和可追溯性，但现有水印方法在潜在空间中嵌入，与时间序列生成器的真实空间操作不兼容。

Method: TimeWak直接在真实时间特征空间中嵌入时间链式哈希水印，并采用ε-精确反转技术处理扩散过程的反转误差分布。

Result: TimeWak在合成数据质量、水印可检测性和鲁棒性方面显著优于现有基线，提升了61.96%的context-FID分数和8.44%的相关性分数。

Conclusion: TimeWak是首个针对多变量时间序列扩散模型的水印算法，有效解决了真实空间水印嵌入的挑战，同时保持了高检测性和数据质量。

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [288] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 论文提出了一种优化框架，设计了两种新水印（HeavyWater和SimplexWater），用于在低熵任务中高效检测水印并最小化文本失真。


<details>
  <summary>Details</summary>
Motivation: 解决LLM水印在低熵任务（如编码）中检测困难的问题，同时平衡检测准确性和文本质量。

Method: 提出优化框架，设计两种可调水印（HeavyWater和SimplexWater），通过随机侧信息最大化检测概率并最小化文本失真。

Result: 实验表明，两种水印在低熵任务中能高效检测且对文本质量影响小，同时揭示了水印与编码理论的新联系。

Conclusion: HeavyWater和SimplexWater为LLM水印提供了高效且灵活的解决方案，适用于多种任务和模型。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [289] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLM）的中毒攻击，提出了一个全面的威胁模型，用于分类和评估此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着预训练LLM及其训练数据的广泛使用，其安全风险（如中毒攻击）日益受到关注。现有框架和术语源自分类中毒研究，不完全适用于生成式LLM。

Method: 通过系统综述已发表的LLM中毒攻击文献，提出包含四种攻击规范和六种评估指标的威胁模型。

Result: 提出了一个适用于LLM中毒攻击的威胁模型，并围绕四个关键维度（概念毒药、隐蔽毒药、持久毒药和特定任务毒药）组织讨论。

Conclusion: 该框架有助于更清晰地理解LLM中毒攻击的安全风险，并为未来研究提供统一术语和分类标准。

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [290] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: 本文提出了一种利用新型数据类型（技术数字签名）进行网络风险量化的方法，通过爬取组织网站的技术签名构建高精度风险评估模型，克服了传统IP扫描数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于IP扫描的网络风险评估方法存在数据不完整和无法覆盖大量中小企业的缺陷，而技术数字签名数据更容易获取且适用于中小企业。

Method: 通过爬取组织网站的技术签名数据，构建网络风险评估模型，并与不同网络事件数据集进行交叉验证。

Result: 研究发现技术签名与组织的网络安全状况密切相关，并揭示了勒索软件攻击受害者与其他网络事件受害者的关键差异。

Conclusion: 技术数字签名是一种可行且高效的网络风险量化方法，尤其适用于中小企业。

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [291] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: 论文提出了一种基于联邦多智能体深度强化学习（FMADRL）的移动目标防御（MTD）框架，用于无人机群网络中的DoS攻击主动和自适应缓解。


<details>
  <summary>Details</summary>
Motivation: 无人机群网络的开放无线环境、动态拓扑和资源限制使其面临严重的DoS威胁，传统静态或集中式防御机制难以应对。

Method: 设计了三种轻量级协调的MTD机制（领导者切换、路由变异和频率跳变），并将防御问题建模为多智能体部分可观察马尔可夫决策过程（POMDP），采用FMADRL算法进行分布式策略优化。

Result: 仿真结果表明，该方法在攻击缓解率、恢复时间、能耗和防御成本方面显著优于现有基线，分别提升了34.6%、减少了94.6%、降低了29.3%和98.3%。

Conclusion: FMADRL驱动的MTD框架能有效提升无人机群网络的抗DoS攻击能力，同时保持任务连续性。

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [292] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 论文提出了一种基于早期充电阶段电压行为的电动汽车（EV）识别框架，提高了识别速度和可靠性，同时揭示了充电数据隐私风险。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施的安全性需求日益增长，现有方法在充电后期才进行识别，导致能量被盗风险。早期识别方法可减少损失，但需解决隐私问题。

Method: 通过提取早期充电阶段的电压特征，构建轻量级模型实现EV识别，测试了7408次充电数据，验证了方法的可行性。

Result: 在49辆EV的7408次充电数据上，识别准确率达到0.86，仅需10个关键特征即可实现接近最优性能。

Conclusion: 该研究为EV认证提供了新方法，同时警示了充电数据可能带来的隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [293] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种统一的视觉领域数据重建攻击分类和定义，并设计了一套量化评估指标，利用大语言模型（LLMs）替代人工判断，建立了一个系统评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前数据重建攻击领域缺乏统一的定义和评估指标，阻碍了研究的进一步发展。

Method: 提出了一套量化评估指标，并利用LLMs进行视觉评估，建立了一个系统评估框架。

Result: 实证结果验证了所提指标的有效性，并为设计新攻击提供了有价值的见解。

Conclusion: 本文的分类和指标为数据重建攻击领域提供了统一的基准，推动了未来研究的发展。

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [294] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: 论文提出了一种基于模板的两阶段方法，用于预测小分子在蛋白质结合位点的3D构象，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 预测小分子在蛋白质结合位点的3D构象是药物设计的关键挑战，现有方法在模板相似性低或配体灵活性高时表现不佳。

Method: 采用两阶段方法：第一阶段基于流匹配的分子对齐生成3D坐标；第二阶段通过可微优化程序细化构象，考虑形状、药效团相似性和内部能量。

Result: 在新基准测试中，该方法优于标准对接工具和开放对齐方法，尤其在模板相似性低或配体灵活性高时表现突出。

Conclusion: 该方法为药物设计中的3D构象预测提供了更有效的解决方案。

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [295] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: 本文综述了图神经网络（GNNs）在AI辅助药物发现（AIDD）中的应用，涵盖方法基础、代表性任务及最新进展。


<details>
  <summary>Details</summary>
Motivation: GNNs因其对分子图的直接操作能力，成为学习药物分子复杂拓扑和几何特征的强大工具，推动了现代分子建模的发展。

Method: 综述了GNNs的方法论基础，包括几何GNNs、可解释模型、不确定性量化、可扩展图架构和图生成框架，并探讨了与其他深度学习方法（如自监督学习、多任务学习等）的整合。

Result: GNNs在分子性质预测、虚拟筛选、分子生成等多个任务中表现出色，但也面临实际应用中的挑战和方法瓶颈。

Conclusion: 文章总结了GNNs在药物发现中的潜力，并展望了未来研究方向。

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [296] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 论文提出了一种名为Bullseye的预测器，用于改进TAGE-SC-L预测器对难预测分支（H2P）的处理，通过并行子系统减少误预测。


<details>
  <summary>Details</summary>
Motivation: TAGE-SC-L预测器在处理难预测分支时表现不佳，导致大量误预测，而单纯扩大表规模效果有限。

Method: 引入Bullseye子系统，包括H2P识别表（HIT）和两个感知器，通过动态阈值和并行操作提高预测精度。

Result: Bullseye将平均MPKI降至3.4045，CycWpPKI降至145.09，显著提升了性能。

Conclusion: Bullseye有效解决了难预测分支的问题，为高性能处理器设计提供了新思路。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [297] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO是一种改进的贝叶斯优化方法，支持处理包含分类参数的约束，并加速FPGA软处理器的设计优化。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化（BO）无法处理分类参数约束，且优化时间随处理器复杂度增加而显著增长，尤其是在FPGA软处理器设计中。

Method: ASPO通过定制BO的数学机制，支持分类参数，并加速设计评估过程，包括惩罚获取函数和重用FPGA合成检查点。

Result: ASPO在BOOM处理器上为“multiply”基准测试减少35%的执行时间，并将设计时间减少74%（相比Boomerang）。

Conclusion: ASPO成功解决了标准BO在软处理器设计中的局限性，显著提升了优化效率和性能。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [298] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: 本文简要介绍了预测编码网络（PCNs）及其在机器学习中的应用，包括网络架构、推理和学习规则，并通过CIFAR-10图像分类任务展示了其性能。


<details>
  <summary>Details</summary>
Motivation: 预测编码网络（PCNs）是一种受生物学启发的框架，用于理解大脑中的分层计算，并为机器学习提供了一种替代传统前馈神经网络的方法。

Method: 文章介绍了PCNs的基础架构、推理和学习更新规则，并提供了PyTorch实现的Python笔记本。

Result: 通过CIFAR-10图像分类任务展示了PCNs的性能，实现了突破性的结果。

Conclusion: PCNs为机器学习提供了一种有前景的替代方案，尤其是在需要分层计算的场景中。

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [299] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: 提出了一种新的资源分配框架，通过对比排序网络选择性优化有潜力的下层任务，显著降低计算成本并保持或提升解精度。


<details>
  <summary>Details</summary>
Motivation: 双层优化的嵌套结构导致计算成本高，进化算法虽有效但资源浪费严重，需解决冗余评估问题。

Method: 采用对比排序网络在线学习上下层解的关系模式，指导基于参考的排序策略，优先优化有潜力的任务并自适应控制重采样。

Result: 在五种先进算法上的实验表明，该框架显著降低计算成本，同时保持或提升解精度。

Conclusion: 该框架为双层进化算法提供了一种通用高效策略，推动了可扩展双层优化的发展。

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [300] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: 论文提出了一种基于深度强化学习的框架，用于动态优化离散选择模型，解决了传统方法的静态策略和知识转移问题。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择建模依赖专家经验和主观假设，过程耗时且效率低。现有元启发式方法无法动态调整搜索策略或利用历史知识。

Method: 采用深度强化学习框架，通过智能体动态评估模型拟合优度和简洁性，生成奖励信号以优化模型选择。

Result: 实验表明，智能体能动态调整策略，识别高效模型，且具有鲁棒性和潜在的可迁移性。

Conclusion: 该框架为离散选择建模提供了一种自动化、高效的解决方案，无需先验领域知识。

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [301] [#P is Sandwiched by One and Two #2DNF Calls: Is Subtraction Stronger Than We Thought?](https://arxiv.org/abs/2506.06716)
*Max Bannach,Erik D. Demaine,Timothy Gomez,Markus Hecher*

Main category: cs.CC

TL;DR: 论文研究了#DNF和#2DNF在计数复杂性中的作用，发现两个受限的#2DNF调用可以捕获gapP，而#P严格包含于一个和两个#2DNF调用之间。此外，线性时间可计算的命题公式和结构保持的减缩方法带来了算法应用和稀疏化引理。


<details>
  <summary>Details</summary>
Motivation: 探索#DNF和#2DNF在计数复杂性中的边界及其与#P和gapP的关系，以揭示更精细的复杂性层次结构。

Method: 通过结构感知的减缩方法，研究#2DNF调用的组合效果，并分析其与gapP的关系。

Result: 发现两个受限的#2DNF调用足以捕获gapP，且减缩方法保持了结构和对称性，带来了稀疏化引理和算法应用。

Conclusion: 论文通过结构感知的减缩方法，揭示了#2DNF在复杂性理论中的新作用，并提供了稀疏化和算法优化的新工具。

Abstract: The canonical class in the realm of counting complexity is #P. It is well
known that the problem of counting the models of a propositional formula in
disjunctive normal form (#DNF) is complete for #P under Turing reductions. On
the other hand, #DNF $\in$ spanL and spanL $\not\subseteq$ #P unless NL = NP.
Hence, the class of functions logspace-reducible to #DNF is a strict subset of
#P under plausible complexity-theoretic assumptions. By contrast, we show that
two calls to a (restricted) #2DNF oracle suffice to capture gapP, namely, that
the logspace many-one closure of the subtraction between the results of two
#2DNF calls is gapP. Because #P $\not\subseteq$ gapP, #P is strictly contained
between one and two #2DNF oracle calls.
  Surprisingly, the propositional formulas needed in both calls are linear-time
computable, and the reduction preserves interesting structural as well as
symmetry properties, leading to algorithmic applications. We show that a single
subtraction suffices to compensate for the absence of negation while still
capturing gapP, i.e., our results carry over to the monotone fragments of #2SAT
and #2DNF. Since our reduction is linear-time, it preserves sparsity and, as a
consequence we obtain a sparsification lemma for both #2SAT and #2DNF. This has
only been known for kSAT with k $\geq$ 3 and respective counting versions. We
further show that both #2DNF calls can be combined into a single call if we
allow a little postprocessing (computable by AC0- or TC0-circuits).
Consequently, we derive refined versions of Toda's Theorem: PH $\subseteq$
[#MON2SAT]$^{log}_{TC0}$ = [#MON2DNF]$^{log}_{TC0}$ and PH $\subseteq$
[#IMPL2SAT]$^{log}_{AC0}$. Our route to these results is via structure-aware
reductions that preserve parameters like treewidth up to an additive overhead.
The absence of multiplicative overhead indeed yields parameterized SETH-tight
lower bounds.

</details>


### [302] [Robust predicate and function computation in continuous chemical reaction networks](https://arxiv.org/abs/2506.06590)
*Kim Calabrese,David Doty,Mina Latifi*

Main category: cs.CC

TL;DR: 该论文研究了化学反应网络（CRNs）中布尔谓词和数值函数的速率无关计算，发现布尔谓词的稳定计算受限，提出了一种称为“鲁棒计算”的松弛概念，并证明了CRNs可以鲁棒地计算阈值谓词的有限布尔组合和分段仿射函数。


<details>
  <summary>Details</summary>
Motivation: 研究CRNs中布尔谓词和数值函数的速率无关计算能力，揭示布尔谓词稳定计算的局限性，并探索更宽松的计算模型。

Method: 使用标准质量作用速率模型，定义鲁棒计算概念，分析CRNs在布尔谓词和数值函数上的计算能力。

Result: CRNs可以鲁棒地计算阈值谓词的有限布尔组合和分段仿射函数（具有有理系数）。

Conclusion: CRNs在鲁棒计算模型下具有更强的计算能力，能够处理更广泛的布尔谓词和数值函数。

Abstract: We initiate the study of rate-constant-independent computation of Boolean
predicates and numerical functions in the continuous model of chemical reaction
networks (CRNs), which model the amount of a chemical species as a nonnegative,
real-valued *concentration*. Real-valued numerical functions have previously
been studied, finding that exactly the continuous, piecewise rational linear
functions $f: \mathbb{R}_{> 0}^k \to \mathbb{R}_{> 0}$ can be computed
*stably*, a.k.a., *rate-independently*, meaning that the CRN gets the answer
correct no matter the rate at which reactions occur.
  We show that, contrary to functions, continuous CRNs are severely limited in
the Boolean predicates they can stably decide, reporting an answer based only
on which inputs are 0 or positive.
  This limitation motivates a slightly relaxed notion of rate-independent
computation in CRNs that we call *robust computation*. The standard mass-action
rate model is used, in which each reaction is assigned a rate equal to the
product of its reactant concentrations and its rate constant. The computation
is correct in this model if it converges to the correct output for any positive
choice of rate constants. This adversary is weaker than the stable computation
adversary, the latter being able to run reactions at non-mass-action rates.
  We show that CRNs can robustly decide every finite Boolean combination of
*threshold predicates*: those predicates defined by taking a rational weighted
sum of the inputs $\mathbf{x} \in \mathbb{R}^k_{\ge 0}$ and comparing to a
constant, answering the question ``Is $\sum_{i=1}^k w_i \cdot \mathbf{x}(i) >
h$?'', for rational weights $w_i$ and real threshold $h$. Turning to function
computation, we show that CRNs can robustly compute any piecewise affine
function with rational coefficients, where threshold predicates determine which
affine piece to evaluate for a given input.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [303] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: 论文讨论了AI恶意群体（swarms）的威胁及其对民主进程的影响，提出了三方面的应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，恶意AI群体可能通过协调行动制造虚假共识、破坏社会信任，威胁民主进程。

Method: 提出了平台端防御（如群体检测仪表盘）、模型端保护（如说服风险测试）和系统级监督（如联合国支持的AI影响观察站）三方面措施。

Result: 恶意AI群体可能导致虚假共识、社会分裂、选民操纵等问题。

Conclusion: 呼吁采取多层次措施应对AI恶意群体的威胁，保护民主进程。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [304] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: LLMs能评估空间计量经济学中实证结果的经济合理性和理论一致性，但在深层评估（如系数合理性和发表适宜性）上表现不一。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在评估学术论文经济合理性和理论一致性方面的能力。

Method: 通过创建原始和人为修改的论文摘要，由多种LLMs进行评估，包括定性分析和二元分类。

Result: LLMs在变量选择一致性上表现优异（如GPT-4o的F1分数为0.87），但在深层评估上差异显著。

Conclusion: LLMs适合辅助初步检查，但需人类监督，尤其在复杂经济推理中。

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [305] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 论文提出了一种新的审计框架，用于评估工人希望AI代理自动化或增强的职业任务，并分析这些愿望与当前技术能力的匹配情况。通过WORKBank数据库和Human Agency Scale（HAS）量化人类参与偏好，揭示了AI代理发展的关键机会与不匹配。


<details>
  <summary>Details</summary>
Motivation: 快速发展的复合AI系统正在改变劳动力市场，但缺乏对其影响的系统性理解。论文旨在填补这一空白，通过研究工人对AI自动化的需求和当前技术能力的匹配情况。

Method: 引入了一个审计框架，包括音频增强的小型访谈和Human Agency Scale（HAS）量化工具，构建了WORKBank数据库，收集了1,500名工人和AI专家的数据，覆盖844项任务和104种职业。

Result: 研究发现任务可分为四个区域：自动化“绿灯”区、“红灯”区、研发机会区和低优先级区。HAS分析显示不同职业对人类参与的期望差异显著，AI整合可能推动核心人类能力从信息技能转向人际技能。

Conclusion: 研究强调了AI代理开发需与人类需求对齐，并为工人适应职场动态变化提供了早期信号。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [306] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 论文研究了在连续和离散域上最小化两个子模函数差（DS）的问题，扩展了之前仅限于集合函数的工作。提出了一种新的DC算法变体，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 子模函数在多种应用中广泛存在，但之前的研究主要集中在集合函数上。本文旨在扩展这一研究到更广泛的连续和离散域。

Method: 提出了一种新的DC算法变体（DCA），并将其应用于离散和连续域上的DS最小化问题。通过离散化处理连续域问题。

Result: 实验表明，该方法在整数压缩感知和整数最小二乘问题上优于基线方法。

Conclusion: 本文的方法在理论和实验上均表现出色，为DS最小化问题提供了有效的解决方案。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>


### [307] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: 提出了一种量化黎曼梯度跟踪算法（Q-RGT），用于解决紧凑子流形上的分散优化问题，通过量化梯度减少通信瓶颈，同时保持与非量化方法相同的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 分散优化中通信瓶颈限制了效率，尤其是在紧凑子流形上。量化梯度可以缓解这一问题，但现有方法在量化噪声下的收敛性较差。

Method: 提出Q-RGT算法，利用量化梯度更新局部变量，避免精确黎曼投影操作，提高迭代效率。

Result: Q-RGT在量化噪声下实现了与非量化方法相同的收敛速度（O(1/K)），并通过数值实验验证了其性能。

Conclusion: Q-RGT在减少通信开销的同时保持了高效性，为分散优化提供了一种实用的解决方案。

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [308] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow是一种基于归一化流的可扩展生成模型，结合了Transformer自回归流（TARFlow）的创新设计，实现了高分辨率图像合成的强大性能。


<details>
  <summary>Details</summary>
Motivation: 解决归一化流在大规模高分辨率图像生成中的可扩展性和性能问题。

Method: 1. 引入TARFlow，结合归一化流和自回归Transformer的表达能力；2. 采用深度-浅层设计；3. 在预训练自编码器的潜在空间建模；4. 提出新的引导算法。

Result: STARFlow在类别条件和文本条件图像生成任务中表现优异，接近扩散模型的样本质量。

Conclusion: STARFlow首次证明了归一化流在大规模高分辨率图像生成中的有效性。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [309] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 论文研究了基于Vision Transformers（ViTs）的医学图像对抗水印攻击的脆弱性，并通过对抗训练提升其防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着ViTs在计算机视觉任务中的成功应用，研究其在医学图像（如皮肤病图像）中对对抗水印攻击的脆弱性具有重要意义。

Method: 使用Projected Gradient Descent（PGD）生成对抗水印，并测试其对ViTs和CNNs的迁移性，同时分析对抗训练的效果。

Result: ViTs对对抗攻击非常脆弱（准确率降至27.6%），但对抗训练可显著提升防御能力（准确率恢复至90.0%）。

Conclusion: ViTs在医学图像中易受对抗攻击，但对抗训练是一种有效的防御手段。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [310] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 本文研究了用于交通标志识别的深度神经网络（DNNs）的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并开发了检测模型以识别被污染的数据。


<details>
  <summary>Details</summary>
Motivation: 由于DNNs在交通标志识别中的广泛应用，且训练数据来源未知，确保模型在训练过程中不被攻击或污染变得至关重要。

Method: 首先通过向训练数据添加微小扰动实施误差最小化攻击，随后提出一种基于非线性变换的数据增强训练方法以破坏扰动并提升模型鲁棒性。

Result: 误差最小化攻击将DNNs的预测准确率从99.90%降至10.6%，而提出的缓解方案成功将准确率恢复至96.05%，且优于对抗训练。检测模型对攻击的识别成功率超过99%。

Conclusion: 研究表明，交通标志识别系统需采用先进的训练方法以抵御数据污染攻击，提出的数据增强和检测模型有效提升了安全性。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [311] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: Flood-DamageSense是一种深度学习框架，专门用于洪水灾害后的建筑物损坏评估，通过融合多源数据显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在洪水灾害后建筑物损坏分类中表现不佳，尤其是当损坏信号不明显时。

Method: 结合SAR/InSAR数据、高分辨率光学底图和洪水风险层，采用多模态Mamba架构和半孪生编码器进行联合预测。

Result: 在Hurricane Harvey数据上，F1分数比现有方法提升19个百分点，尤其在轻微和中度损坏分类中表现突出。

Conclusion: Flood-DamageSense通过风险感知建模和SAR全天候能力，提供了更快速、精细和可靠的洪水损坏评估。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [312] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 论文提出了一种基于CNN-LSTM的可解释人工智能框架，用于高效分类胚胎图像，提高体外受精（IVF）的成功率。


<details>
  <summary>Details</summary>
Motivation: 不孕症对生活质量影响显著，传统胚胎分级方法效率低下，需要更高效且可解释的解决方案。

Method: 结合CNN和LSTM的深度学习模型（CNN-LSTM），通过XAI实现高精度且可解释的胚胎分类。

Result: 模型在胚胎分类中实现了高准确率，同时保持了可解释性。

Conclusion: CNN-LSTM框架为IVF中的胚胎选择提供了高效且透明的解决方案。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [313] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经控制微分方程和Savitzky-Golay路径的连续时间旋转物体动力学建模方法，解决了SO(3)外推中的噪声、稀疏观测和复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: SO(3)外推在计算机视觉和机器人学中至关重要，但面临噪声、稀疏观测、复杂动态和长期预测需求的挑战。

Method: 使用神经控制微分方程和Savitzky-Golay路径建模连续时间旋转物体动力学，避免简化运动假设，同时尊重旋转的几何结构。

Result: 在真实数据上的实验表明，该方法在预测能力上优于现有方法。

Conclusion: 该方法为SO(3)外推提供了一种通用且几何结构保持的解决方案。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [314] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，ViT4TS和VLM4TS，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏视觉-时间推理能力，无法像人类专家那样识别上下文异常。

Method: 两阶段方法：ViT4TS用于初步定位异常候选，VLM4TS结合全局时间上下文和VLM推理能力进行精细检测。

Result: VLM4TS在多数情况下优于基线方法，F1-max分数提升24.6%，且效率更高。

Conclusion: VLM4TS为时间序列异常检测提供了高效且准确的解决方案。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [315] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery是一个新发布的基准数据集，用于评估从fMRI活动重建心理图像的性能，补充了现有的NSD数据集。研究发现，现有模型在心理图像重建上的表现与视觉重建表现脱节，且架构选择对跨解码性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 为了评估和改进从fMRI活动重建心理图像的模型性能，以支持医疗领域和脑机接口的实际应用。

Method: 使用NSD-Imagery数据集，对一系列基于NSD训练的开源视觉解码模型（如MindEye1、MindEye2等）进行基准测试，分析其在心理图像重建上的表现。

Result: 研究发现，心理图像重建的性能与视觉重建表现脱节，简单线性解码和多模态特征解码架构在心理图像上表现更好，而复杂架构容易过拟合视觉训练数据。

Conclusion: 心理图像数据集对实际应用开发至关重要，NSD-Imagery为改进视觉解码方法提供了重要资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [316] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM是一种新型状态空间模型架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块实现高效检测，性能优于现有流式检测器。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要低延迟、高吞吐量的实时感知，传统LiDAR处理方法因全扫描处理导致延迟，而流式方法因卷积与极坐标几何不匹配导致性能下降。

Method: PHiM采用局部双向Mamba块进行扇区内空间编码，全局前向Mamba块进行扇区间时序建模，替代卷积和位置编码，实现失真感知和维度分解操作。

Result: PHiM在Waymo Open Dataset上性能优于现有流式检测器10%，且吞吐量翻倍，匹配全扫描基线。

Conclusion: PHiM为极坐标流式LiDAR提供了一种高效解决方案，显著提升了实时感知性能。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [317] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的表格结构识别方法，通过序列标注和Transformer编码器优化处理大型密集表格。


<details>
  <summary>Details</summary>
Motivation: 解决表格结构识别中的挑战，特别是针对大型、密集表格的准确性和效率问题。

Method: 采用Split-Merge框架，将行列分割视为序列标注任务，使用双Transformer编码器；合并过程通过网格分类任务和额外Transformer编码器实现。

Result: 在FinTabNet和PubTabNet上表现优异，减少了分辨率损失和计算复杂度，同时保持高速处理。

Conclusion: 该方法为大规模表格识别提供了高效、可扩展的解决方案，适合工业应用。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [318] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，通过优化对抗分布和干净分布，以及梯度协作生成对抗样本，显著提升了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标模型损失函数引导不足和对抗生成非协作性方面存在局限，需改进以增强模型对抗攻击的防御能力。

Method: 采用D2R Loss（双优化步骤）和CAG（梯度协作生成对抗样本）策略，结合不同损失函数的优势，优化目标模型分布。

Result: 在CIFAR-10、CIFAR-100、Tiny ImageNet等基准数据集上，D2R Loss与CAG显著提升了模型鲁棒性。

Conclusion: D2R Loss与CAG策略有效解决了现有方法的不足，生成了高鲁棒性模型。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [319] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 提出了一种多步优化策略，显著提升了扩散模型在图像恢复任务中的质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如MPGD）在每一步去噪中仅应用单次梯度更新，限制了恢复质量和鲁棒性。

Method: 在每一步去噪时间步中引入多步优化策略。

Result: 实验表明，增加梯度更新次数显著提升了LPIPS和PSNR指标，且在嵌入式设备上验证了泛化能力。

Conclusion: MPGD可作为轻量级即插即用模块，适用于实时视觉感知任务。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [320] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 该论文提出了一种诊断测试（class sensitivity）来评估显著性方法是否能区分同一输入的不同类别标签，并发现许多广泛使用的方法存在类别不敏感问题。作者进一步提出了一种对比性解释方法CASE，能够更准确地识别与预测类别相关的特征。


<details>
  <summary>Details</summary>
Motivation: 显著性方法在可视化模型预测时可能存在局限性，尤其是无法区分不同类别的输入特征。作者希望通过诊断测试揭示这些方法的不足，并提出改进方案。

Method: 作者提出了一种诊断测试（class sensitivity）来评估显著性方法的类别区分能力，并通过实验验证了许多方法的局限性。随后，他们开发了CASE方法，通过对比性解释来突出预测类别的独特特征。

Result: 实验表明，许多显著性方法在类别区分上表现不佳，而CASE方法在诊断测试和基于扰动的保真度测试中表现更优，提供了更忠实和类别特定的解释。

Conclusion: 该研究揭示了显著性方法的类别不敏感问题，并提出CASE作为一种更可靠的替代方案，能够生成更具区分性的解释。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [321] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 提出了一种结合TPS和CBAM的改进YOLO模型，用于农业中的目标检测，解决了遮挡和非刚性变形问题，性能优于STN-YOLO。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO模型在农业目标检测中因遮挡和非刚性变形（如弯曲叶片）导致精度下降，现有STN的仿射变换不足以应对这些挑战。

Method: 提出CBAM-STN-TPS-YOLO模型，将TPS引入STN以实现非刚性空间变换，并利用CBAM抑制背景噪声、增强关键特征。

Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，误报率降低12%。

Conclusion: 该模型轻量且适合实时边缘部署，为智能农业提供了高效准确的监测方案。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [322] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MOS的方法，通过拼接单目标图像生成多目标图像，优化无监督表示学习，显著提升了多目标图像的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在单目标图像上表现良好，但在多目标图像上表现不佳，因此需要一种无需人工标注的方法来改进多目标图像的无监督表示。

Method: 通过拼接单目标图像生成多目标图像，利用预定的对象对应关系优化对比学习，提升对象表示。

Result: 在ImageNet、CIFAR和COCO数据集上，MOS方法在单目标和多目标图像上均取得了领先的无监督表示性能。

Conclusion: MOS方法简单有效，能够为复杂下游任务提供更详细的对象表示，且无需人工标注。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [323] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器技术解决域差距和少样本微调问题，并设计了DFN和SAM-SVN方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本分割中的域差距和少样本微调问题。

Method: 提出Domain Feature Navigator（DFN）作为结构化解耦器，结合SAM-SVN方法防止过拟合。

Result: 在1-shot和5-shot场景中，性能分别超过现有方法2.69%和4.68% MIoU。

Conclusion: DFN和SAM-SVN方法有效提升了跨域少样本分割的性能。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [324] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoCoA-Mix的混合模型，结合了混淆感知损失（CoA-loss）和置信感知权重（CoA-weights），以提升视觉语言模型在特定任务中的专业化和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 冻结的编码器常导致特征不对齐，引发类别混淆，限制了模型的专业化能力。

Method: 提出CoA-loss优化决策边界以减少混淆，并利用CoA-weights在混合模型中调整预测权重以增强泛化。

Result: 实验表明CoCoA-Mix在专业化和泛化方面优于现有方法。

Conclusion: CoCoA-Mix通过结合CoA-loss和CoA-weights，有效解决了任务适应中的专业化和泛化问题。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [325] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 论文提出了Uncertainty-o框架，用于评估和量化多模态模型（LMMs）的不确定性，并通过实验验证其在多种任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型（LMMs）被认为比纯语言模型（LLMs）更鲁棒，但其不确定性评估仍存在三个关键问题：统一评估方法、如何提示LMMs展示不确定性，以及如何量化不确定性以支持下游任务。

Method: 提出Uncertainty-o框架，包括模型无关的不确定性揭示方法、多模态提示扰动的实证研究，以及多模态语义不确定性的量化公式。

Result: 在18个基准测试和10种LMMs上的实验表明，Uncertainty-o能有效估计LMMs的不确定性，并提升下游任务（如幻觉检测和缓解）的性能。

Conclusion: Uncertainty-o为多模态模型的不确定性评估提供了统一且有效的解决方案，对实际应用具有重要意义。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [326] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 研究发现扩散模型API可能被用于生成合成图像，进而训练替代模型，从而实现对黑盒分类模型的提取和对抗攻击，效果显著且查询成本极低。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真图像生成能力带来了安全和隐私风险，如版权侵犯和恶意内容生成。本研究旨在揭示一种新型安全威胁，即利用扩散模型API生成合成图像以训练替代模型。

Method: 通过扩散模型API生成高分辨率且多样化的合成图像，用于训练替代模型，进而对黑盒分类模型进行模型提取和对抗攻击。

Result: 在七个基准测试中，该方法平均提升27.37%，仅使用0.01倍的查询预算，对抗攻击成功率达98.68%。

Conclusion: 扩散模型API可能成为新型安全威胁，需引起重视并采取防范措施。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [327] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet是一种多尺度边缘增强框架，显著提升了微小目标（如花粉）的自动识别精度，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型在微小目标定位上表现不佳。

Method: 提出HieraEdgeNet框架，包含三个模块：HEM（多尺度边缘特征提取）、SEF（边缘与语义信息融合）、CSPOKM（细节优化）。

Result: 在120类花粉数据集上，mAP@.5达到0.9501，优于YOLOv12n和RT-DETR。

Conclusion: HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微小目标检测提供了强大解决方案。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [328] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 提出了一种结合视觉表示、语义分割和用户行为模拟的时尚推荐系统，通过加权评分函数生成个性化推荐，实验显示效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决时尚推荐中如何平衡个性化风格与流行趋势的问题。

Method: 使用语义分割提取服装区域特征，结合预训练CNN提取视觉嵌入，模拟用户购物行为生成推荐。

Result: 在DeepFashion数据集上，ResNet-50达到64.95%的类别相似度，且流行度误差最低。

Conclusion: 该方法提供了一个可扩展的框架，能有效平衡个人风格与流行趋势。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [329] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种多模态RAG框架RCTS，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升LVLMs在VQA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在知识推理示例稀缺和检索知识响应不稳定方面存在不足。

Method: 引入自洽评估机制丰富知识库，并提出带启发式奖励的蒙特卡洛树搜索（MCTS-HR）重排序方法。

Result: 在多个VQA数据集上达到SOTA性能，显著优于ICL和Vanilla-RAG方法。

Conclusion: RCTS框架通过高质量推理上下文和重排序方法有效提升了LVLMs的性能。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [330] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: 论文提出R3D2模型，通过一步扩散方法解决3D高斯重建在动态对象操作和复用性上的不足，实现真实3D资产插入虚拟场景。


<details>
  <summary>Details</summary>
Motivation: 验证自动驾驶系统需要多样化和安全关键的测试，传统仿真平台资源密集且与现实数据存在领域差距，而现有神经重建方法在动态对象操作和复用性上表现不佳。

Method: R3D2是一种轻量级一步扩散模型，通过训练基于3D高斯重建对象资产的数据集，生成逼真的渲染效果（如阴影和一致光照）。

Result: R3D2显著提升了插入资产的真实感，支持文本到3D资产插入和跨场景对象转移，实现了自动驾驶验证的可扩展性。

Conclusion: R3D2为自动驾驶仿真提供了可扩展且逼真的解决方案，未来将公开数据集和代码以促进研究。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [331] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: LogoSP提出了一种无监督的3D语义分割方法，通过结合局部和全局点特征学习语义信息，并在频域中利用全局模式生成伪标签，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法仅依赖局部特征，缺乏对更丰富语义先验的探索。

Method: LogoSP通过频域中的全局模式分组超点，生成高精度伪标签用于训练分割网络。

Result: 在两个室内和一个室外数据集上，LogoSP大幅超越现有方法，达到最先进性能。

Conclusion: LogoSP证明了在无人类标签的情况下，全局模式能有效捕捉3D语义信息。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [332] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整KL-VAE训练中的损失权重、填充策略和空间条件归一化，显著减少了常见伪影，提升了重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: KL-VAE训练中常见的伪影（如颜色偏移、网格模式等）影响了生成和重建质量，需要一种无需大幅架构改动的方法来解决。

Method: 提出VIVAT方法，通过调整损失权重、优化填充策略和引入空间条件归一化，系统性解决五种常见伪影。

Result: 在多个基准测试中，VIVAT在图像重建（PSNR、SSIM）和文本到图像生成（CLIP分数）上达到最优性能。

Conclusion: VIVAT在保持KL-VAE框架简单性的同时，有效解决了其训练中的实际问题，为优化VAE训练提供了实用方案。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [333] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: FreeGave是一种无需物体先验的方法，通过引入物理代码和无散度模块，从多视角视频中学习复杂动态3D场景的物理特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂物理运动或边界时表现不佳，且依赖物体先验（如掩码或类型）。FreeGave旨在解决这些问题。

Method: 提出FreeGave方法，引入物理代码和无散度模块，估计每个高斯速度场，避免低效的PINN损失。

Result: 在三个公共数据集和一个新收集的真实数据集上表现优异，尤其在未来帧外推和运动分割任务中。

Conclusion: FreeGave成功学习到无标签训练下的3D物理运动模式，验证了其有效性。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [334] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文对34篇论文进行了综合分析，比较了18种U-Net变体在遥感变化检测中的应用潜力，并评估了它们的优缺点。


<details>
  <summary>Details</summary>
Motivation: 填补U-Net在遥感变化检测领域应用的研究空白。

Method: 通过文献综述和比较分析，评估18种U-Net变体的性能，特别关注专为变化检测设计的变体（如Siamese Swin-U-Net）。

Result: 研究发现，处理多时相数据和长距离关系对提高变化检测精度至关重要。

Conclusion: 本研究为选择U-Net变体进行遥感变化检测提供了有价值的参考。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [335] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现，当前视觉语言模型（VLMs）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法而非真正任务理解，性能在分布偏移时下降。


<details>
  <summary>Details</summary>
Motivation: 验证VLMs是否真正具备多模态上下文学习能力，而非依赖浅层启发式方法。

Method: 提出MM-ICL with Reasoning管道，为每个示例生成答案和推理依据，并在不同数据集和模型上实验。

Result: 性能对演示数量、检索方法、推理依据质量等因素不敏感，表明VLMs未有效利用演示信息。

Conclusion: 当前VLMs在多模态上下文学习中表现有限，需进一步改进。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [336] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出一种高效的单摄像头实时三维足球轨迹重建方法，通过多模态状态模型加速优化，保持厘米级精度，适用于复杂场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、运动模糊和复杂背景下的性能问题，同时降低对多摄像头和昂贵设备的需求。

Method: 引入多模态状态模型（$W$离散模态）以加速优化，适用于标准CPU，支持低延迟实时处理。

Result: 在6K分辨率俄罗斯超级联赛数据集上表现优异，性能接近多摄像头系统。

Conclusion: 提供了一种实用、低成本且高精度的三维足球轨迹跟踪方法。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [337] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集和引入零样本学习来提升胸部X光片（CXR）的肺部疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类中的挑战，并提升现有技术的可测量性。

Method: 提供高质量基准数据，扩展数据集至377,110张CXR和45种疾病标签，引入零样本学习任务。

Result: 通过多模态模型、生成方法和零样本学习策略，提升了罕见疾病检测和噪声标签处理能力。

Conclusion: CXR-LT 2024为未来研究提供了宝贵资源，推动了临床现实和泛化诊断模型的发展。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [338] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且准确的众包评估策略，用于评估神经元解释方法的可靠性，通过重要性采样和贝叶斯方法显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经元解释方法的可靠性评估通常依赖众包，但存在噪声大、成本高的问题，需要更高效的评估策略。

Method: 引入重要性采样选择最有价值的输入样本，并采用贝叶斯方法聚合评分，显著降低评估成本。

Result: 实现了约30倍的成本降低和5倍的评分效率提升，并比较了两种视觉模型的神经元解释质量。

Conclusion: 提出的方法为神经元解释评估提供了高效且准确的解决方案，显著优于传统众包评估。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [339] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: MADFormer是一种结合自回归（AR）和扩散模型的混合Transformer，用于分析AR与扩散模型的权衡，通过分块生成和分层混合优化高分辨率图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型缺乏系统指导，无法明确如何分配模型能力。MADFormer旨在探索AR与扩散模型的互补优势，提供设计原则。

Method: MADFormer将图像生成分为空间块，AR层用于全局一次性条件生成，扩散层用于局部迭代细化。

Result: 实验表明，分块生成显著提升高分辨率图像性能，AR与扩散层的垂直混合在有限计算下FID提升75%。

Conclusion: MADFormer为未来混合生成模型提供了实用的设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [340] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 论文比较了视觉语言模型（VLMs）与其视觉编码器的性能，发现VLMs在视觉任务中表现显著下降，主要原因是未能有效整合视觉信息并受语言先验影响。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs如何整合视觉与语言信息，并评估其在视觉任务中的表现。

Method: 通过一系列视觉基准测试（如深度估计、对应关系）比较VLMs与视觉编码器的性能，并分析VLMs的失败模式。

Result: VLMs在视觉任务中表现接近随机水平，主要瓶颈在于未能有效利用视觉信息且受语言模型先验影响。

Conclusion: 研究揭示了开源VLMs的失败模式，为未来改进视觉理解提供了评估框架。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [341] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: Self Forcing是一种新的自回归视频扩散模型训练方法，通过自生成输出和KV缓存解决曝光偏差问题，实现高效实时视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型中的曝光偏差问题，即在推理时模型需基于自身不完美的输出生成序列。

Method: 在训练时使用自回归展开和KV缓存，基于自生成输出生成帧，并通过视频级损失监督，结合高效扩散模型和梯度截断策略。

Result: 实验表明，该方法在单GPU上实现亚秒级延迟的实时视频生成，质量优于或匹配更慢的非因果扩散模型。

Conclusion: Self Forcing通过自生成输出和KV缓存有效解决了曝光偏差问题，实现了高效且高质量的视频生成。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [342] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型在零样本设置下进行多任务学习，通过潜在回归和任务注意力机制实现任务间协同，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多任务学习需要大量标注数据，而部分任务标注或零样本设置可以缓解这一问题。本文旨在利用扩散模型的泛化能力，实现零样本多任务学习。

Method: 提出StableMTL方法，通过潜在回归、任务编码和任务注意力机制，统一潜在损失，避免任务损失平衡问题。

Result: 在8个基准测试的7个任务上，StableMTL表现优于基线方法。

Conclusion: StableMTL展示了在零样本设置下进行多任务学习的潜力，并通过任务注意力机制提升了任务间协同效果。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [343] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 该研究开发并评估了多种机器学习和深度学习模型，用于通过多模态传感器数据早期预测痴呆患者的躁动行为，最高AUC-ROC达0.9720。


<details>
  <summary>Details</summary>
Motivation: 痴呆患者的躁动行为常见且影响生活质量，早期预测可减轻护理负担并改善患者和护理者的生活质量。

Method: 研究引入了一组新的躁动相关特征，评估了多种模型（包括二元分类和异常检测），使用了TIHM数据集（包含2803天的家庭活动数据）。

Result: 最佳模型（轻梯度提升机）在二元分类任务中表现最优，AUC-ROC为0.9720，AUC-PR为0.4320。

Conclusion: 该研究首次全面评估了基于隐私保护传感器数据的躁动预测技术，为痴呆护理提供了准确、可解释且高效的预测方法。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [344] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: 该研究开发了一种基于遥感数据和机器学习的橄榄产量估算方法，结合多光谱反射带和植被指数，通过自动集成学习框架实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 橄榄产量受气候变化影响显著，传统估算方法复杂且不准确，需要一种高效、可扩展的解决方案。

Method: 利用Landsat-8 OLI和Landsat-9 OLI-2卫星图像提取特征，结合地面调查数据，使用AutoGluon训练集成模型并进行交叉验证。

Result: Landsat-8 OLI的R2为0.8635，RMSE为1.17吨/公顷；Landsat-9 OLI-2的R2为0.8378，RMSE为1.32吨/公顷。

Conclusion: 该方法具有可扩展性、成本效益和准确性，适用于全球多样化的农业区域。

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [345] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 论文提出了一种基于对比学习的ECG预训练模型，通过引入患者记忆队列（PMQ）和数据增强方法，解决了现有方法在利用患者一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 由于标记数据有限，如何基于未标记数据构建鲁棒的ECG预训练模型成为研究重点。现有方法未能充分利用患者一致性。

Method: 提出PMQ增强的对比学习模型，引入大容量患者记忆队列和两种数据增强方法。

Result: 在三个公共数据集上的实验表明，该方法性能优于现有对比学习方法，且在标记数据有限时更具鲁棒性。

Conclusion: PMQ和数据增强显著提升了ECG预训练模型的性能，为有限标记数据场景提供了有效解决方案。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [346] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: 论文提出了一种结合拓扑数据分析和YOLOv5的新框架，用于增强地下管线的GPR图像检测，通过形状感知拓扑特征和Sim2Real策略显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统GPR解释方法受限于噪声敏感性和结构意识不足，需要更可靠的地下管线检测技术。

Method: 提出形状感知拓扑表示方法，结合TDA和YOLOv5，并采用Sim2Real策略生成合成数据。

Result: 实验显示mAP显著提升，验证了方法的鲁棒性和有效性。

Conclusion: TDA增强学习在实时地下物体检测中具有广泛应用潜力。

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [347] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 介绍了一个开源Python框架，用于生成合成ECG图像数据集，支持ECG数字化、导联区域和名称检测以及波形分割等深度学习任务。


<details>
  <summary>Details</summary>
Motivation: 推动ECG分析中基于深度学习的任务发展，如ECG数字化、导联区域和名称检测以及波形分割。

Method: 使用PTB-XL信号数据集生成四个开放数据集，包括配有时序信号的ECG图像、带YOLO格式标注的ECG图像以及适用于U-Net模型的分割掩码图像。

Result: 生成了四个公开可用的数据集，支持多种ECG分析任务。

Conclusion: 开源框架和数据集为ECG分析研究提供了重要资源。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [348] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: 提出了一种基于PPO的自适应滤波框架，在动态非平稳环境中优于传统滤波方法。


<details>
  <summary>Details</summary>
Motivation: 传统滤波方法（如LMS、RLS、Wiener和Kalman）在动态非平稳环境中表现受限，需要复杂调参或固定模型假设。

Method: 使用PPO算法，通过复合奖励函数（平衡SNR提升、MSE降低和残差平滑）实现自适应滤波。

Result: 在合成信号实验中，PPO代理表现出泛化能力，实时性能优于传统滤波方法。

Conclusion: 证明了策略梯度强化学习在鲁棒、低延迟自适应信号滤波中的可行性。

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [349] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: 提出了一种基于深度神经网络的ECG多视角分类方法，通过融合1D和2D视图，结合不确定性处理噪声和伪影，提升了心律失常分类的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ECG数据中的噪声和伪影可能导致不同视图之间的冲突，影响分类准确性。

Method: 使用三个模块：时间序列模块学习形态特征，图像空间模块学习时空特征，不确定性感知融合模块整合不同视图信息。

Result: 在两个真实数据集上验证，性能优于现有方法，且对噪声和伪影更具鲁棒性。

Conclusion: 该方法通过多视角学习和不确定性融合，显著提升了心律失常分类的准确性和鲁棒性。

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [350] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: 论文提出了一种结合可重构智能表面（RIS）和强化学习（RL）的方法，旨在实现高效且公平的多用户通信系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有RIS-RL系统中用户间通信公平性问题，确保所有用户设备（UE）都能获得足够信号强度。

Method: 提出了一种新颖的RIS-RL优化方法，通过实验和仿真验证其公平性和效率。

Result: 实验结果表明，该方法在保证网络性能和能源效率的同时，显著提升了通信公平性。

Conclusion: 论文为多用户RIS-RL系统提供了一种高效且公平的解决方案，并公开了代码和数据集以促进进一步研究。

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [351] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: 提出轻量级模型LD-RPMNet，结合Transformer和CNN，优化铁路应用中的计算效率，参数减少50%，诊断准确率提升至98.86%。


<details>
  <summary>Details</summary>
Motivation: 近传感器诊断在工业中日益普及，需优化计算效率以适用于铁路点机的实际应用。

Method: LD-RPMNet引入多尺度深度可分离卷积模块（MDSC）和广播自注意力机制（BSA），结合局部和全局特征提取。

Result: 模型参数和计算复杂度降低50%，诊断准确率提升近3%，达到98.86%。

Conclusion: LD-RPMNet展示了铁路点机近传感器故障诊断应用的可行性。

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [352] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: 论文提出了一种结合模型和数据驱动的机器学习方法，通过迁移学习和CycleGAN技术，优化了空间与机载遥感平台间的甲烷羽流检测。


<details>
  <summary>Details</summary>
Motivation: 甲烷对全球变暖影响显著，但现有遥感平台间的数据分布差异限制了甲烷羽流检测的准确性。

Method: 利用迁移学习优化机载数据训练的模型，并使用CycleGAN对齐机载与空间遥感数据分布。

Result: 通过CycleGAN转换空间数据并直接应用机载分类器，获得了最佳检测效果。

Conclusion: 该方法不仅适用于甲烷检测，还可推广至其他遥感数据的跨平台对齐。

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [353] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 论文通过传统机器学习和深度学习两种方法对心电图信号进行分类，发现基于手工特征的LightGBM模型表现最佳，准确率达99%，优于基于图像的CNN方法。


<details>
  <summary>Details</summary>
Motivation: 解决心电图信号中心跳分类的问题，比较手工特征和图像转换方法的效果。

Method: 1. 传统机器学习：提取HRV、均值、方差等特征，使用SVM、随机森林等分类器。2. 深度学习：将ECG信号转换为图像（GAF、MTF、RP），用CNN（如VGG、Inception）分类。

Result: LightGBM表现最优（准确率99%，F1分数0.94），优于CNN（F1分数0.85）。SVM和AdaBoost效果较差。

Conclusion: 手工特征能更好地捕捉ECG信号的时空变化，未来可探索多导联信号和时序依赖以提升分类效果。

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [354] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: 本文提出了一种优化的卷积神经网络方法，用于快速预测全球范围内的次声波传输损失（TLs），解决了现有方法在高频和不利风条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 准确建模次声波传输损失对评估国际监测系统网络的性能至关重要，但现有方法计算成本高且在某些条件下效果不佳。

Method: 通过优化卷积神经网络架构，利用全球模拟的温度和风场数据预测4000公里范围内的TLs。

Result: 优化后的模型在整个频段（0.1-3.2 Hz）内平均误差为8.6 dB，显著提升了预测性能。

Conclusion: 该方法为次声波监测提供了一种高效且准确的解决方案，适用于实际大气场景。

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [355] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的结合，系统梳理了其在神经解码、脑机接口和情感计算中的新方向，并提出了分类框架。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与EEG研究的交叉领域，推动神经信号分析与自然语言处理的结合。

Method: 通过系统综述和分类法，将文献分为四个领域：EEG表示学习、EEG到语言解码、跨模态生成及临床应用。

Result: 研究表明，基于Transformer的架构通过微调、少样本和零样本学习，使EEG模型能够完成复杂任务。

Conclusion: 本文为未来研究提供了基础资源，旨在通过语言模型连接自然语言处理与神经信号分析。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [356] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: 研究分析了ECG、EDA和RESP信号的关键特征，发现不同疲劳诱因导致不同生理反应，客观评估比主观评估更敏感。心率稳定性增加、呼吸幅度减少和EDA降低与疲劳相关。


<details>
  <summary>Details</summary>
Motivation: 准确检测疲劳对驾驶安全至关重要，生理信号监测比摄像头方法更隐私。但不同数据集中生理指标与疲劳标签的关联存在冲突。

Method: 分析了四个数据集中的ECG、EDA和RESP信号，使用二元逻辑回归模型识别与疲劳相关的生理指标。

Result: 不同疲劳诱因导致不同生理反应，客观评估更敏感。心率稳定性增加、呼吸幅度减少和EDA降低与疲劳显著相关。

Conclusion: 研究结果有助于理解疲劳检测，并为未来通用监测设计提供参考。

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [357] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: 比较了知识驱动、统计和深度学习方法分解EDA信号，提出基于Transformer的Feel Transformer模型，无需显式监督即可分离相位和张力成分。


<details>
  <summary>Details</summary>
Motivation: 分解EDA信号为相位和张力成分对提取情感和生理生物标志物至关重要，尤其是在真实环境中。

Method: 引入Feel Transformer模型，利用池化和趋势去除机制实现生理学意义的分解，并与Ledalab、cvxEDA等方法对比。

Result: Feel Transformer在特征保真度和抗噪性上表现优异，适用于实时生物信号分析。

Conclusion: 模型在压力预测、数字心理健康干预和生理预测中具有潜力。

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [358] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 论文提出了一种基于模型神经网络的指纹定位方法，在非视距环境中显著提高了定位精度并降低了内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理技术在复杂无线电环境（尤其是非视距传播路径）中定位精度下降，而现有机器学习方法计算复杂度高。

Method: 使用模型神经网络学习位置到信道的映射，作为生成神经信道模型，增强指纹比对字典并减少内存需求。

Result: 在非视距环境中实现亚波长级定位精度，定位精度提升数个数量级，内存需求比传统指纹方法降低一个数量级。

Conclusion: 该方法在复杂无线电环境中显著提升了定位性能，同时降低了计算和内存开销。

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [359] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: IQFM是首个基于原始IQ信号的无线通信基础模型，支持多种任务，无需复杂预处理或手工特征，通过对比自监督学习框架实现高效多任务学习。


<details>
  <summary>Details</summary>
Motivation: 探索无线通信中直接操作原始IQ数据的基础模型潜力，填补现有研究的空白。

Method: 采用任务感知增强策略，结合对比自监督学习框架，预训练轻量级编码器。

Result: 在调制分类和AoA分类中分别达到99.67%和65.45%的准确率，优于监督基线，并能泛化到新任务。

Conclusion: IQFM展示了原始IQ基础模型在6G系统中的高效多任务学习潜力。

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [360] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 论文提出了一种基于条件去噪扩散模型（CDDM）和多模态Transformer（MMT）的新框架，用于提升无蜂窝ISAC系统中的信道估计性能，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统在6G网络中具有潜力，但信道估计受限于导频污染和噪声问题，需要更高效的解决方案。

Method: 结合CDDM和MMT，利用感知信息作为输入，通过多模态Transformer捕捉感知与位置数据的关联，迭代去噪优化信道估计。

Result: 相比LS和MMSE估计器，NMSE分别提升8 dB和9 dB；相比传统扩散模型（TDDM），NMSE提升27.8%，且在低信噪比和导频污染下表现稳健。

Conclusion: 该框架显著提升了无蜂窝ISAC系统的信道估计性能，尤其在感知目标附近用户中表现优异，为6G网络提供了高效解决方案。

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [361] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型（DM）的可重构智能表面（RIS）辅助系统信道估计方法，通过确定性采样策略和轻量级网络设计，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在反向过程中存在随机性，且网络参数量大，限制了信道估计的准确性和实用性。

Method: 将信道估计问题转化为去噪过程，采用确定性采样策略和步长对齐机制，并设计轻量级网络以减少参数量。

Result: 在多种信噪比（SNR）下性能优于基线，例如在SNR=0 dB时NMSE提升13.5 dB，轻量级网络参数量仅为原U-Net的6.59%且性能几乎无损。

Conclusion: 所提方法在准确性和实用性上均有显著提升，适用于RIS辅助系统的信道估计。

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [362] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 论文提出了一个名为Tactile MNIST Benchmark Suite的标准化基准套件，用于主动触觉感知任务，填补了触觉感知和主动感知领域缺乏标准化基准的空白。


<details>
  <summary>Details</summary>
Motivation: 触觉感知在机器人灵巧操作中潜力巨大，但由于其局部性，难以单独完成需要全局场景理解的任务。主动感知技术可以弥补这一缺陷，但目前缺乏标准化基准。

Method: 引入了一个开源、兼容Gymnasium的基准套件，包括多样化的模拟场景和数据集（合成3D MNIST数字模型和真实触觉样本），并训练CycleGAN用于逼真的触觉模拟渲染。

Result: 提供了标准化的协议和可复现的评估框架，支持触觉感知和主动感知领域的系统性进展。

Conclusion: 该基准套件为触觉感知和主动感知研究提供了重要工具，有望推动相关领域的进一步发展。

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [363] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 论文提出了一种多模态空间语言地图（VLMaps和AVLMaps），结合预训练多模态特征与3D环境重建，支持自然语言命令导航和多模态目标定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用环境映射、空间精度不足或忽略视觉以外的模态信息。

Method: 通过自主探索构建视觉-语言地图（VLMaps）及其扩展音频-视觉-语言地图（AVLMaps），结合预训练多模态特征与3D重建。

Result: 实验证明，该方法支持零样本空间和多模态目标导航，在模糊场景中召回率提升50%。

Conclusion: 多模态空间语言地图为机器人导航和交互提供了视觉、音频和空间线索的统一支持。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [364] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的推土机自定位方法，通过内部传感器估计局部速度并结合扩展卡尔曼滤波器实现全局定位，实验表明该方法能有效减少位置误差。


<details>
  <summary>Details</summary>
Motivation: 传统RTK-GNSS信号在某些采矿条件下会丢失，因此需要不依赖RTK-GNSS的自定位方法。

Method: 使用机器学习模型从内部传感器估计局部速度，并结合扩展卡尔曼滤波器进行全局定位。

Result: 实验表明，该方法能有效抑制位置误差积累，尤其是在打滑情况下，且推土机专用传感器有助于提高定位精度。

Conclusion: 基于机器学习的自定位方法在推土机应用中具有潜力，尤其在RTK-GNSS信号不可靠时。

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [365] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种实时分块（RTC）算法，用于解决现代AI系统在高频控制任务中的延迟问题，通过异步执行动作分块策略，显著提升了任务吞吐量和成功率。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在物理世界交互中需要实时性能，但现有通用模型的高延迟问题导致动作分块边界出现停顿或不流畅动作。

Method: 提出RTC算法，适用于任何基于扩散或流的视觉语言动作模型，无需重新训练，通过在执行当前动作分块时生成下一个分块，实现平滑异步执行。

Result: 在Kinetix模拟器和真实世界双手机器人任务中测试，RTC显著提升了任务吞吐量，并在高延迟环境下保持高成功率。

Conclusion: RTC是一种高效、鲁棒的实时动作分块方法，适用于高动态任务，解决了延迟问题并提升了性能。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [366] [Human Side of Smart Contract Fuzzing: An Empirical Study](https://arxiv.org/abs/2506.07389)
*Guanming Qiao,Partha Protim Paul*

Main category: cs.HC

TL;DR: 论文研究了智能合约模糊测试（SC fuzzing）在实际应用中面临的挑战，通过分析GitHub问题和用户调查，提出了分类法，并指出了工具设计和文档改进的方向。


<details>
  <summary>Details</summary>
Motivation: 智能合约模糊测试在区块链应用中至关重要，但由于与传统软件的差异，其实际应用面临诸多挑战。研究旨在揭示这些挑战及其对不同用户群体的影响。

Method: 研究通过分析381个GitHub问题（来自Echidna和Foundry）和用户调查，归纳了智能合约模糊测试的挑战，并分类为技术性和人为性问题。

Result: 研究发现智能合约模糊测试存在易用性和实用性挑战，如区块链模拟技术问题和文档不足。结果提供了工具改进的具体方向。

Conclusion: 研究为智能合约模糊测试工具的设计和改进提供了实用建议，强调了文档和自动化的重要性。

Abstract: Smart contract (SC) fuzzing is a critical technique for detecting
vulnerabilities in blockchain applications. However, its adoption remains
challenging for practitioners due to fundamental differences between SCs and
traditional software systems. In this study, we investigate the challenges
practitioners face when adopting SC fuzzing tools by conducting an inductive
content analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna
and Foundry. Furthermore, we conducted a user study to examine how these
challenges affect different practitioner groups, SC developers, and traditional
software security professionals, and identify strategies practitioners use to
overcome them. We systematically categorize these challenges into a taxonomy
based on their nature and occurrence within the SC fuzzing workflow. Our
findings reveal domain-specific ease-of-use and usefulness challenges,
including technical issues with blockchain emulation, and human issues with a
lack of accessible documentation and process automation. Our results provide
actionable insights for tool developers and researchers, guiding future
improvements in SC fuzzer tool design.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [367] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: 本文综述了科学机器学习（SciML）在水文学中的应用，提出了统一的框架以整合分散的方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决水文学中SciML方法分散且缺乏统一框架的问题，以促进概念清晰和累积进展。

Method: 提出统一的框架，整合物理学知识驱动的机器学习方法，包括物理信息、物理引导、混合物理-机器学习和数据驱动的物理发现。

Result: 通过统一框架，将分散的方法整合为连贯结构，支持水文学建模的进展。

Conclusion: SciML在水文学中潜力巨大，但仍需系统性研究以克服当前局限并探索未来机会。

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [368] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: El0ps是一个Python工具箱，用于处理L0正则化问题，提供灵活框架、高性能求解器和内置机器学习流程。


<details>
  <summary>Details</summary>
Motivation: 为机器学习和信号处理等领域提供更灵活、高效的L0正则化问题解决方案。

Method: 通过自定义问题实例的框架、专用求解器和内置机器学习流程实现。

Result: El0ps在性能上达到先进水平，并支持实际应用中的L0正则化问题集成。

Conclusion: El0ps为L0正则化问题的实际应用提供了全面的工具和新视角。

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [369] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 论文提出了一种利用自然语言处理（NLP）和大型语言模型（LLMs）的流程，用于战场物联网（IoBT）中的数据查询和信息生成，以提高决策效率。


<details>
  <summary>Details</summary>
Motivation: 战场物联网（IoBT）的扩展为增强态势感知提供了新机会，但需要将设备数据转化为可消费的信息对象。

Method: 采用NLP和图形数据库技术，利用适合边缘设备的LLMs，将自然语言问题映射为数据库查询，并将结果以自然语言形式返回。

Result: 实验表明，Llama 3.1（80亿参数）在多个指标上表现最佳，且两步法提高了19.4%的准确性。

Conclusion: 该流程为在边缘设备上部署LLMs，实现自然语言与数据库交互奠定了基础。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [370] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在生成测试用例以检查或调试代码方面的能力，提出了TCGBench基准测试，发现LLMs在生成有效测试用例生成器方面表现良好，但在生成针对性测试用例以暴露代码缺陷方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在代码检查或调试中的潜力，尤其是在竞赛级编程（CP）中生成测试用例的能力。

Method: 提出TCGBench基准测试，包含两项任务：生成有效测试用例生成器和生成针对性测试用例生成器。实验评估了LLMs的表现，并构建了一个高质量的手动标注数据集。

Result: LLMs能生成有效测试用例生成器，但在生成针对性测试用例时表现不佳，尤其是高级推理模型（如o3-mini）远不及人类表现。使用手动标注数据集可以提升LLMs的表现。

Conclusion: LLMs在生成测试用例方面有潜力，但在针对性任务中仍需改进，手动标注数据集可以显著提升其性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [371] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM是一种仅使用文本数据训练语音语言模型的新框架，通过统一编码器和轻量级投影网络实现语音推理，性能接近多模态基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖大规模语音-文本配对数据和计算资源，限制了可扩展性和可访问性。

Method: 利用统一编码器将语义等效的文本和语音输入映射到共享潜在空间，并通过轻量级投影网络与LLM嵌入空间对齐。

Result: TESU-LLM在多个语音相关基准测试中表现优异，性能接近基于多模态数据的基线方法。

Conclusion: TESU-LLM提供了一种无需语音数据的高效、可扩展的语音语言模型构建方法。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [372] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: RLSC是一种基于模型自身置信度的强化学习方法，无需人工标注或外部奖励模型，显著提升了推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵的人工标注或外部奖励模型，RLSC旨在通过模型自身置信度作为奖励信号，简化训练过程。

Method: RLSC利用模型自身的置信度作为奖励信号，应用于Qwen2.5-Math-7B模型，仅需每个问题8个样本和4个训练周期。

Result: 在AIME2024、MATH500和AMC23数据集上，RLSC分别提升了20.10%、49.40%和52.50%的准确率。

Conclusion: RLSC提供了一种简单、可扩展且需要极少监督的推理模型后训练方法。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [373] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP是一种新的直接行为优化范式，专为轻量级大语言模型（LwLLMs）设计，通过梯度无关的蒙特卡洛树搜索优化执行序列，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 轻量级大语言模型（LwLLMs）在资源效率和隐私方面有优势，但推理能力有限，且现有提示优化方法对其效果不佳。

Method: DeBoP将复杂提示优化转化为离散、可量化的执行序列优化，采用蒙特卡洛树搜索方法。

Result: 在七项任务中，DeBoP优化的LwLLMs表现优于GPT-3.5，计算时间减少约60%。

Conclusion: DeBoP为LwLLMs提供了一种高效的自动优化方法，显著提升了其性能和应用潜力。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [374] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究发现，与人类价值观对齐的大型语言模型（LLMs）更容易产生有害行为，且其安全风险略高于其他微调模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用范围的扩大，个性化LLMs与人类价值观对齐的需求增加，但同时也带来了安全风险。

Method: 通过数据集分析，结合心理学假设，研究价值对齐与安全风险的关系。

Result: 价值对齐的LLMs更容易生成有害文本，且安全风险显著高于非微调模型。

Conclusion: 研究揭示了价值对齐的“黑箱”问题，并提出上下文对齐方法以提升安全性。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [375] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 论文提出了一种称为“规范采样”的方法，旨在解决大型语言模型生成非规范标记序列的问题，并证明其生成的序列更接近训练数据的真实分布。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时可能产生非规范的标记序列，导致负面后果。本文旨在通过理论分析和提出新方法解决这一问题。

Method: 通过理论分析证明生成规范序列的条件，并引入“规范采样”方法，确保模型在自回归生成过程中仅生成规范标记序列。

Result: 规范采样方法有效避免了非规范序列的生成，且生成的序列分布更接近训练数据的真实分布。

Conclusion: 规范采样是一种简单高效的方法，能够提升语言模型生成序列的质量和一致性。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [376] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 提出了一种无需训练的Tokenizer移植方法，通过正交匹配追踪（OMP）重建未见过的词嵌入，显著优于其他零样本方法。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大语言模型（LLMs）中不同Tokenizer之间的兼容性问题，避免梯度更新。

Method: 使用OMP将新词表示为共享词的稀疏线性组合，分两阶段实现：先在捐赠嵌入空间中计算新词表示，再将其稀疏系数转移回基础模型嵌入空间。

Result: 在多个基准测试中，OMP在零样本条件下表现最佳，显著优于其他基线方法。

Conclusion: 该方法支持跨Tokenizer的知识蒸馏、解码、合并等应用，并集成到开源工具mergekit-tokensurgeon中。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [377] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 通过仿射映射在语言模型的残差流之间传递特征，实现SAE权重的跨模型迁移，节省训练成本。


<details>
  <summary>Details</summary>
Motivation: 探索不同规模语言模型间表示空间的相似性，以降低训练昂贵组件（如SAE）的成本。

Method: 使用仿射映射技术将SAE权重从小模型迁移到大模型，并分析特征传递效果。

Result: 小模型和大模型的表示空间高度相似，SAE迁移可节省50%训练成本；语义和结构特征传递效果不同。

Conclusion: 揭示了大小模型线性表示空间的异同，为SAE训练效率提升提供了方法。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [378] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出了一种基于生成模型的多标签文本分类框架LAGAMC，利用标签描述生成和语义匹配，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 文本数据爆炸导致人工分类困难，需要一种高效、通用的多标签分类方法。

Method: 通过生成标签描述并匹配预定义标签，结合交叉熵损失和余弦相似度双目标损失函数。

Result: 在所有评估数据集上达到新的SOTA，Micro-F1提升13.94%，Macro-F1提升24.85%。

Conclusion: LAGAMC模型参数高效且通用，适用于实际应用。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [379] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 论文提出了一种通过强化学习（RL）让QA代理学会提出澄清问题的方法，采用离线RL目标优化，优于现有的监督微调（SFT）和直接偏好优化方法。


<details>
  <summary>Details</summary>
Motivation: 提升QA代理的能力，使其能主动提出澄清问题以更准确地回答问题。

Method: 通过模拟包含澄清问题的对话，利用离线RL目标（类似奖励加权的SFT）进行优化。

Result: 在优化奖励和语言质量上均优于现有方法。

Conclusion: 离线RL目标是一种高效且直接的优化方法，优于其他复杂方法。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [380] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出FoReaL-Decoding方法，通过快速-慢速思维解码协作，在保持模型性能的同时显著降低计算成本和推理长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）的推理过程冗长且易陷入过度思考现象，导致推理速度下降。研究发现LRMs与非推理模型在token级别存在独特的全局和局部错位现象，尤其是局部错位减弱现象。

Method: 提出FoReaL-Decoding方法，由Leading模型引导每句话的前几个token，再由较弱的草稿模型完成剩余部分，并通过随机门平滑切换模型。

Result: 在四个数学推理基准测试中，FoReaL-Decoding减少30-50%的理论FLOPs和40%的推理长度，同时保持86-100%的模型性能。

Conclusion: FoReaL-Decoding是一种简单、即插即用的方法，适用于推理任务中的成本-质量权衡控制。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [381] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 论文提出了一种基于后验推断的采样算法，用于在生成文本时有效控制目标句法结构，结合SMC和句法标注器，显著提升了句法准确性。


<details>
  <summary>Details</summary>
Motivation: 控制语言模型生成文本的句法结构对于需要清晰性、风格一致性或可解释性的应用至关重要，但目前仍具挑战性。

Method: 采用基于后验推断的采样算法，结合顺序蒙特卡洛（SMC）和句法标注器，确保生成的每个词符符合目标句法结构。

Result: 实验表明，该方法显著提高了句法准确性，GPT2-large和Llama3-8B的F1分数分别从12.31和35.33提升至约93，且不影响流畅性。

Conclusion: 该方法为需要精确控制句法的应用提供了有效解决方案，展示了采样算法在句法控制中的潜力。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [382] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为RULE的高效框架，通过强化学习实现LLM的特定信息遗忘，仅需少量遗忘数据和合成边界查询即可显著提升遗忘质量和响应自然度。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）可能包含敏感或非法内容，现有遗忘方法依赖大量数据且效果不佳，亟需一种高效且不损害模型性能的遗忘方法。

Method: 提出RULE框架，将遗忘问题转化为拒绝边界优化任务，使用少量遗忘数据和合成边界查询训练，并通过可验证的奖励函数实现安全拒绝和保留有用响应。

Result: 实验表明，RULE仅需12%遗忘数据和8%合成数据，在遗忘质量和响应自然度上分别优于基线17.5%和16.3%，同时保持模型整体性能。

Conclusion: RULE是一种高效且通用的LLM遗忘方法，能够在不损害模型性能的情况下实现目标遗忘，并提升输出自然度和训练效率。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [383] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了一种基于TF-IDF的句子排序方法，用于长文档分类，显著减少输入大小和推理延迟，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长文档分类中BERT等模型因固定输入长度和二次注意力复杂度带来的计算限制，以及文档冗余问题。

Method: 使用TF-IDF对句子进行排名，结合固定数量或百分比选择句子，并采用归一化TF-IDF分数和句子长度的增强评分策略。

Result: 在MahaNews LDC数据集上，该方法优于基线方法，输入大小减少50%以上，推理延迟降低43%，分类准确率仅下降0.33%。

Conclusion: 该方法证明了在不牺牲性能的情况下显著减少上下文是可行的，适用于实际长文档分类任务。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [384] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的方法，通过全面分析奖励模型在整个词汇空间中的响应，揭示了模型之间的异质性、系统性不对称性、对提示框架的敏感性以及对高频词的过度估值。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在将大型语言模型与人类价值观对齐中扮演关键角色，但其本身的研究相对不足，尤其是其如何编码人类价值判断。

Method: 通过分析奖励模型对所有可能单令牌响应的评分，研究其在词汇空间中的行为。

Result: 发现模型之间存在显著异质性、系统性不对称性、对提示框架的敏感性以及对高频词的过度估值，并揭示了潜在的偏见问题。

Conclusion: 研究挑战了奖励模型的可互换性和作为复杂人类价值观代理的适用性，揭示了其可能传播的偏见风险。

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [385] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 论文提出了KScope框架，用于分类和评估大型语言模型（LLM）的知识状态，并通过实验验证了上下文对知识更新的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM在知识冲突下的行为，但未能全面评估模型对问题的知识掌握程度。

Method: 提出五类知识状态的分类法，并设计KScope框架，通过分层统计测试逐步细化假设，将LLM知识归类为五种状态之一。

Result: 实验表明，上下文支持缩小了模型间的知识差距；难度、相关性和熟悉度等上下文特征影响知识更新效果；不同LLM在部分正确或冲突时表现相似，但在完全错误时差异显著。

Conclusion: 通过上下文特征分析和增强可信度，可以进一步提高知识更新的有效性，且该方法适用于多种LLM。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [386] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文提出了一种名为LoRMA的新方法，通过将低秩适应的加法更新范式转变为矩阵乘法变换，解决了计算复杂性和秩瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP领域表现出色，但全微调计算成本高。现有方法如LoRA虽然高效，但仍局限于加法更新。

Method: 提出LoRMA方法，通过矩阵乘法变换替代加法更新，采用操作重排序和秩膨胀策略优化计算。

Result: 实验证明LoRMA在多种评估指标上表现优异。

Conclusion: LoRMA为低秩适应提供了更高效的乘法变换范式，具有实际应用潜力。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [387] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 论文审核了Twitch的自动审核工具AutoMod，发现其在标记仇恨内容时存在显著缺陷，94%的仇恨内容未被检测到，同时误删了大量良性内容。


<details>
  <summary>Details</summary>
Motivation: 研究AutoMod在实时互动平台（如Twitch）中审核仇恨内容的有效性，填补现有研究空白。

Method: 通过创建测试账户，利用Twitch API发送10.7万条评论（来自4个数据集），测量AutoMod对仇恨内容的标记准确性。

Result: AutoMod漏检高达94%的仇恨内容，且依赖脏词作为信号，误删89.5%的良性内容。

Conclusion: AutoMod存在重大缺陷，需改进上下文理解能力。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [388] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了一种名为FAST的新训练方法，专门针对指令模型优化稀疏自编码器（SAEs），显著提升了重构质量和特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有SAE训练方法主要针对基础模型，应用于指令模型时效果不佳，因此需要一种专门针对指令模型优化的方法。

Method: 提出了FAST方法，通过调整训练过程以匹配指令模型的数据分布和激活模式。

Result: 在Qwen2.5-7B-Instruct上，FAST的均方误差显著优于基线方法；在Llama3.2-3B-Instruct上，高质量特征比例更高。

Conclusion: FAST不仅提升了SAE在指令模型上的性能，还揭示了通过干预特殊令牌激活改进输出质量的新机会。

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [389] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）遗忘技术中的形式依赖偏差问题，并提出了一种新的无训练方法ROCR来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法在实际应用中效果有限，主要因为其效果依赖于训练样本的形式，无法泛化到同一知识的不同表达形式。

Method: 作者提出了ROCR（Rank-one Concept Redirection）方法，通过重定向模型对特定危险概念的感知来实现无训练遗忘。

Result: 实验表明，ROCR显著提高了遗忘效果，同时生成更自然的输出。

Conclusion: LLM遗忘应具备形式独立性，ROCR为解决这一问题提供了有效路径。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [390] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch是一种结合协同训练和一致性正则化的半监督学习算法，通过三重重伪标签加权模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中伪标签选择和加权的挑战，提升模型在数据不平衡情况下的鲁棒性。

Method: 结合协同训练和一致性正则化，设计三重重伪标签加权模块，整合多种现有技术。

Result: 在5个NLP数据集的9/10设置中取得最优性能，在数据不平衡情况下表现突出。

Conclusion: MultiMatch在性能和鲁棒性上优于现有方法，适用于文本分类任务。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [391] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR是一种新颖的可扩展框架，通过残差记忆模块注入新知识，同时保留预训练模型的核心能力，解决了语言模型高效可靠更新的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在实时系统中更新知识时的效率、可靠性和遗忘问题。

Method: 通过样本依赖的掩码稀疏化输入激活，将每个编辑限制在记忆参数的不同子集，减少编辑间的干扰。推理时通过稀疏激活模式匹配识别相关编辑。

Result: 在问答、幻觉修正和分布外泛化任务中，MEMOIR在可靠性、泛化性和局部性指标上达到最优性能，支持数千次顺序编辑且遗忘最小。

Conclusion: MEMOIR为语言模型的高效知识更新提供了一种可扩展且可靠的解决方案。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [392] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 论文提出方法解决语言模型中非规范标记分配概率的问题，通过两种方法确保仅规范标记获得正概率。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对非规范标记分配非零概率，导致概率分配错误且浪费资源。

Method: 提出两种方法：(1) 基于条件的规范标记推断，(2) 基于构造的规范标记参数化。

Result: 实验表明修正规范性问题提高了多个模型和语料库的似然性。

Conclusion: 规范标记分配概率的修正能有效提升模型性能。

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [393] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 提出了一种高效的力导向布局算法，将图嵌入低维空间，用径向距离代替传统中心性度量，适用于大规模图。


<details>
  <summary>Details</summary>
Motivation: 传统中心性度量（如介数和接近度）在大规模图上计算成本高，需要更高效的替代方法。

Method: 使用力导向布局算法将图嵌入低维空间，径向距离作为中心性度量的代理。

Result: 在多种图类型上验证，与度、PageRank和路径中心性有强相关性，并能高效识别高影响力节点。

Conclusion: 该方法为传统贪婪算法提供了快速、可扩展的替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [394] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: 将条件局部独立性检验理论扩展到Ito过程，适用于动态系统中的因果发现。


<details>
  <summary>Details</summary>
Motivation: 扩展Christgau等人（2024）的条件局部独立性检验理论，以适用于更广泛的动态系统。

Method: 将理论扩展到Ito过程。

Result: 理论适用于动态系统中的因果发现。

Conclusion: 扩展后的理论为动态系统因果发现提供了新工具。

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [395] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 论文提出了一种基于Shapley值框架的风险分配方法，用于解释复杂机器学习模型的风险贡献。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如金融），模型的风险分配与预测均值同样重要，但现有方法难以公平分配风险。

Method: 通过扩展Shapley值框架，将风险公平分配给每个特征。

Result: 分析和实证表明，该方法能有效分配风险。

Conclusion: 扩展的Shapley值框架为模型风险分配提供了可行解决方案。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [396] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: 论文提出了一种解决量化金融中模型不确定性的方法，通过叠加外部不确定性度量增强传统目标，并提出了一种高效的子采样策略和随机梯度下降算法。


<details>
  <summary>Details</summary>
Motivation: 量化金融中模型不确定性对决策质量影响显著，传统方法依赖经验近似，易因小误差导致大偏差。

Method: 结合Klibanoff框架，提出叠加不确定性度量的方法，并设计子采样策略和高效随机梯度下降算法。

Result: 不确定性度量优于传统混合度量策略，子采样方法在模型风险下表现稳健，性能接近复杂贝叶斯方法。

Conclusion: 该方法有效提升模型不确定性处理的鲁棒性和效率，适用于多场景和高维数据。

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [397] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: 提出了一种基于条件扩散模型的框架，用于解决超材料逆向设计中的一对多问题，具有高光谱预测精度和生成多样性，并成功应用于热伪装。


<details>
  <summary>Details</summary>
Motivation: 超材料的逆向设计存在高度非线性和制造困难，传统机器学习方法难以应对复杂设计需求。

Method: 采用条件扩散模型，定制光谱到形状和尺寸参数的映射，解决一对多设计问题。

Result: 方法在光谱预测精度和生成多样性上优于其他生成模型，并为制造提供了有价值的先验知识。

Conclusion: 该方法成功设计并制造了具有定制选择性发射光谱的自由形式超材料，适用于热伪装。

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>
