<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 90]
- [cs.AI](#cs.AI) [Total: 7]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.CR](#cs.CR) [Total: 8]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 15]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [A New Normalization Form for Limited Distinct Attributes](https://arxiv.org/abs/2510.02865)
*Niko S. Snell,Rayen C. Lee*

Main category: cs.DB

TL;DR: 提出了一种新的数据库规范化方法LDNF，用于约束只能拥有有限数量值的属性，解决非有限不同属性导致的数据异常和查询不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据库中，非有限不同属性会导致数据异常和查询不准确，而现有的规范化方法未能有效约束这类属性的值域限制。

Method: 提出有限不同范式(LDNF)，通过强制属性符合有限数量值来将非有限不同属性转换为有限不同属性。

Result: LDNF方法能够与现有范式配合使用，填补了当前规范化方法的空白。

Conclusion: LDNF是必要的规范化改进，为约束属性值域提供了正式的方法论。

Abstract: In modern databases, the practice of data normalization continues to be
important in improving data integrity, minimizing redundancies, and eliminating
anomalies. However, since its inception and consequent improvements, there have
been no attempts to document a method which constrains the values of attributes
capable of only possessing a limited quantity of values. These non-limited
distinct attributes pose a problem throughout many relational databases as they
have the potential to cause data anomalies and query inaccuracies. Thus, a new
database normalization method, Limited Distinct Normal Form (LDNF), is
necessary in order to improve upon the currently established data normalization
process. In brief, LDNF is a method which turns non-limited distinct attributes
into limited distinct attributes by forcing the attributes to conform to a
limited quantity of values. Utilizing LDNF in tandem with existing normal forms
fulfills a need in normalization that is otherwise not present when only using
current methods. A formal approach to LDNF is therefore proposed.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ElasticMoE是一个用于MoE LLM的弹性扩展框架，通过解耦推理执行和内存操作，实现细粒度、低延迟、零停机扩展，显著提升了在动态云环境中部署大规模MoE LLM的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型扩展策略存在粒度粗、延迟长、成本高的问题，无法适应云部署中常见的突发性、短时流量模式，需要一种更精细、低延迟的扩展方案。

Method: 通过解耦推理执行与内存操作，使用HBM管理模块实现权重和KV缓存的零拷贝重映射，采用点对点传输使新加速器上线而不中断服务，基于虚拟内存的专家重分布机制避免缓冲区重分配。

Result: 在Ascend NPU上的评估显示，ElasticMoE实现了高达9倍的扩展延迟降低、2倍的扩展期间吞吐量提升，并显著改善了SLO达成率。

Conclusion: ElasticMoE通过支持细粒度、并发的扩展操作，最小化服务中断，推进了大规模MoE LLM在动态云环境中的实际部署可行性。

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [3] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: GRNND是首个GPU并行化的RNN-Descent算法，通过无序邻居传播策略和GPU架构优化，显著提升了大规模高维数据下近似最近邻图的构建效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和维度的增加，RNN-Descent算法在图构建阶段的复杂度急剧上升，变得非常耗时，甚至影响后续查询处理。

Method: 提出了GRNND算法，采用无序邻居传播策略避免同步更新陷阱，利用warp级协作操作和双缓冲邻居池进行高效内存访问，实现高度并行化的邻居更新。

Result: GRNND在性能上显著优于现有CPU和GPU方法，相比GPU方法加速2.4-51.7倍，相比CPU方法加速17.8-49.8倍。

Conclusion: GRNND成功解决了RNN-Descent在大规模高维数据下的性能瓶颈问题，为近似最近邻图构建提供了高效的GPU并行解决方案。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [4] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: TridentServe是一个新的扩散模型服务系统，采用动态阶段级服务范式，通过自动优化模型部署和请求路由，显著提升服务效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散管道服务系统采用静态、手动、管道级的资源分配方式，无法适应不同阶段和不同请求的资源需求差异，导致效率低下。

Method: 提出动态阶段级服务范式，自动推导管道部署的放置计划和请求处理的分派计划，协同优化模型和请求的资源分配。

Result: 实验表明，TridentServe在各种工作负载下，相比现有工作，SLO达成率提升2.5倍，平均/P95延迟降低3.6倍/4.1倍。

Conclusion: 动态阶段级服务范式能有效解决扩散管道服务中的资源分配不匹配问题，显著提升服务性能。

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [5] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 研究稀疏矩阵并行计算库的能效，在保持高性能的同时优化GPU计算的能源消耗，证明优化算法和数据移动可同时减少求解时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 满足现代高性能计算平台对可持续性的日益增长需求，在已证明性能优势的基础上进一步提供能源效率分析。

Method: 开发准确运行时能耗测量方法和工具，对库的核心组件进行能耗分析，优化GPU计算并最小化跨内存和计算节点的数据移动。

Result: 确认优化GPU计算和减少数据移动可同时降低求解时间和能耗，在标准基准测试中相比同类软件框架具有显著优势。

Conclusion: 该稀疏矩阵计算库不仅性能优越，在能源效率方面也表现出色，为大规模科学应用提供了高性能且可持续的解决方案。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [6] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 本文分析了云处理地球观测大数据时的能源效率问题，指出了当前平台在能源监控、数据管理、资源分配和任务调度方面的不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量激增和云计算广泛应用，能源效率和碳足迹问题日益突出，但现有云处理平台主要关注数据可访问性和计算可行性，忽视了能源效率。

Method: 通过分析当前地球观测大数据处理现状，对比其他大数据领域的成功能源效率策略，识别出关键差距并提出改进方案。

Result: 识别出四个主要差距：能源监控机制不足、数据管理缺乏能源意识、资源分配未充分考虑能源效率、任务调度缺少能源效率标准。

Conclusion: 建议开发能源感知的性能监控框架、优化基础设施编排技术、采用能源高效的任务调度方法，以在保持处理性能的同时降低能耗和环境影

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [7] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda是一个GPU加速的PyRadiomics库扩展，通过将关键几何计算卸载到GPU硬件，显著减少医学图像三维形状特征提取的处理时间。


<details>
  <summary>Details</summary>
Motivation: 解决从医学图像中提取三维形状特征的计算挑战，特别是处理大型体积数据集时的性能瓶颈问题。

Method: 在Python和C/CUDA中实现，通过GPU硬件加速关键几何计算，同时保持与原始PyRadiomics API的完全兼容性。

Result: 在典型计算集群、预算设备和家用设备上的测试证明，在所有场景下都能显著提高处理效率，支持高通量AI流水线。

Conclusion: PyRadiomics-cuda提供了透明加速功能，实现了高效、可扩展的放射组学分析，为医学图像处理提供了重要的性能优化解决方案。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [8] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: iDDS是一个面向大规模分布式科学计算的智能工作流编排系统，集成了数据感知执行、条件逻辑和可编程工作流，支持复杂动态处理管道的自动化。


<details>
  <summary>Details</summary>
Motivation: 扩展传统工作负载和数据管理能力，为大型科学实验提供更灵活、自动化的分布式计算解决方案。

Method: 采用模块化消息驱动架构，集成PanDA和Rucio等系统，支持模板驱动工作流和Python函数即任务模型。

Result: 成功应用于多个真实场景：ATLAS实验的磁带资源优化、Rubin天文台的大规模DAG工作流、机器学习超参数优化、物理分析主动学习和AI辅助探测器设计。

Conclusion: iDDS通过统一工作负载调度、数据移动和自适应决策，降低了操作开销，实现了跨异构基础设施的可重现高吞吐量工作流。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Even Faster Kernel Matrix Linear Algebra via Density Estimation](https://arxiv.org/abs/2510.02540)
*Rikhav Shah,Sandeep Silwal,Haike Xu*

Main category: cs.DS

TL;DR: 本文改进了基于核密度估计(KDE)的核矩阵线性代数任务算法，包括矩阵-向量积、矩阵-矩阵积、谱范数和矩阵元素和的计算，显著降低了时间复杂度对n和ε的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理核矩阵的线性代数任务时，对数据点数量n和误差ε的依赖较高，特别是当通过KDE查询而非直接读取矩阵元素时，需要更高效的计算方法。

Method: 使用核密度估计(KDE)方法改进核矩阵相关计算，通过优化算法设计降低对n和ε的多项式依赖，特别是减少了计算核矩阵元素和时对n的依赖。

Result: 相比现有最佳算法(特别是Backurs等人2021年的工作)，新算法显著降低了ε的多项式依赖，并在计算核矩阵元素和时减少了对n的依赖。

Conclusion: 基于KDE的方法在核矩阵线性代数任务中具有显著优势，但相关下界分析表明存在计算复杂性限制，暗示了KDE方法在这些问题中的局限性。

Abstract: This paper studies the use of kernel density estimation (KDE) for linear
algebraic tasks involving the kernel matrix of a collection of $n$ data points
in $\mathbb R^d$. In particular, we improve upon existing algorithms for
computing the following up to $(1+\varepsilon)$ relative error: matrix-vector
products, matrix-matrix products, the spectral norm, and sum of all entries.
The runtimes of our algorithms depend on the dimension $d$, the number of
points $n$, and the target error $\varepsilon$. Importantly, the dependence on
$n$ in each case is far lower when accessing the kernel matrix through KDE
queries as opposed to reading individual entries.
  Our improvements over existing best algorithms (particularly those of
Backurs, Indyk, Musco, and Wagner '21) for these tasks reduce the polynomial
dependence on $\varepsilon$, and additionally decreases the dependence on $n$
in the case of computing the sum of all entries of the kernel matrix.
  We complement our upper bounds with several lower bounds for related
problems, which provide (conditional) quadratic time hardness results and
additionally hint at the limits of KDE based approaches for the problems we
study.

</details>


### [10] [Near-Optimal Fault-Tolerant Strong Connectivity Preservers](https://arxiv.org/abs/2510.02562)
*Gary Hoppenworth,Thatchaphol Saranurak,Benyu Wang*

Main category: cs.DS

TL;DR: 该论文提出了针对有向图的k-容错连通性保持器的新构造，将边数从之前的O(k2^k n^{2-1/k})改进到O(k4^k n log n)，几乎填补了与最优下界Ω(2^k n)之间的差距。


<details>
  <summary>Details</summary>
Motivation: 在有向图中，k-容错连通性保持器的现有构造与理论下界之间存在显著的Ω(n^{1-1/k})差距，而在无向图中这个差距早已被解决。

Method: 作者开发了新的构造方法，能够构建具有O(k4^k n log n)边的k-容错连通性保持器，并在多项式时间内实现构造。

Result: 成功将边数上界从O(k2^k n^{2-1/k})改进到O(k4^k n log n)，对于常数k而言，这个结果在log n因子内是最优的。

Conclusion: 该工作几乎完全解决了有向图k-容错连通性保持器的边数复杂度问题，并展示了k-连通性保持器中对k的指数依赖不是固有的。

Abstract: A $k$-fault-tolerant connectivity preserver of a directed $n$-vertex graph
$G$ is a subgraph $H$ such that, for any edge set $F \subseteq E(G)$ of size
$|F| \le k$, the strongly connected components of $G - F$ and $H - F$ are the
same. While some graphs require a preserver with $\Omega(2^{k}n)$ edges
[BCR18], the best-known upper bound is $\tilde{O}(k2^{k}n^{2-1/k})$ edges
[CC20], leaving a significant gap of $\Omega(n^{1-1/k})$. In contrast, there is
no gap in undirected graphs; the optimal bound of $\Theta(kn)$ has been
well-established since the 90s [NI92].
  We nearly close the gap for directed graphs; we prove that there exists a
$k$-fault-tolerant connectivity preserver with $O(k4^{k}n\log n)$ edges, and we
can construct one with $O(8^{k}n\log^{5/2}n)$ edges in $\text{poly}(2^{k}n)$
time.
  Our results also improve the state-of-the-art for a closely related object; a
\textit{$k$-connectivity preserver} of $G$ is a subgraph $H$ where, for all $i
\le k$, the strongly $i$-connected components of $G$ and $H$ agree. By a known
reduction, we obtain a $k$-connectivity preserver with $O(k4^{k}n\log n)$
edges, improving the previous best bound of $\tilde{O}(k2^{k}n^{2-1/(k-1)})$
[CC20]. Therefore, for any constant $k$, our results are optimal to a $\log n$
factor for both problems.
  Lastly, we show that the exponential dependency on $k$ is not inherent for
$k$-connectivity preservers by presenting another construction with $O(n
\sqrt{kn})$ edges.

</details>


### [11] [Congestion bounds via Laplacian eigenvalues and their application to tensor networks with arbitrary geometry](https://arxiv.org/abs/2510.02725)
*Sayan Mukherjee,Shinichiro Akiyama*

Main category: cs.DS

TL;DR: 该论文研究了将任意图的顶点嵌入到树结构中的问题，提出了基于拉普拉斯特征值的拥塞上下界，并开发了基于谱聚类的启发式算法来寻找低拥塞嵌入。


<details>
  <summary>Details</summary>
Motivation: 图顶点嵌入树结构是计算机科学和物理学中的重要问题，特别是在张量网络收缩中具有应用价值。研究拥塞最小化的嵌入有助于理解张量网络收缩的内存复杂度。

Method: 使用拉普拉斯特征值分析建立拥塞上下界：λ₂(G)·2n/9 ≤ cng(G) ≤ λₙ(G)·2n/9。开发了基于层次谱聚类的收缩启发式算法来寻找低拥塞嵌入。

Result: 在超立方体、网格、随机图和张量网络表示等不同图结构上进行了数值比较，验证了拥塞边界的有效性。启发式算法在稀疏图上表现良好。

Conclusion: 拥塞边界提供了张量网络收缩内存复杂度的上下界，基于谱聚类的启发式算法为寻找低拥塞嵌入提供了实用方法。

Abstract: Embedding the vertices of arbitrary graphs into trees while minimizing some
measure of overlap is an important problem with applications in computer
science and physics. In this work, we consider the problem of bijectively
embedding the vertices of an $n$-vertex graph $G$ into the leaves of an
$n$-leaf rooted binary tree $\mathcal{B}$. The congestion of such an embedding
is given by the largest size of the cut induced by the two components obtained
by deleting any vertex of $\mathcal{B}$. The congestion $\mathrm{cng}(G)$ is
defined as the minimum congestion obtained by any embedding. We show that
$\lambda_2(G)\cdot 2n/9\le \mathrm{cng} (G)\le \lambda_n(G)\cdot 2n/9$, where
$0=\lambda_1(G)\le \cdots \le \lambda_n(G)$ are the Laplacian eigenvalues of
$G$. We also provide a contraction heuristic given by hierarchically spectral
clustering the original graph, which we numerically find to be effective in
finding low congestion embeddings for sparse graphs. We numerically compare our
congestion bounds on different families of graphs with regular structure
(hypercubes and lattices), random graphs, and tensor network representations of
quantum circuits. Our results imply lower and upper bounds on the memory
complexity of tensor network contraction in terms of the underlying graph.

</details>


### [12] [On the Enumeration of all Unique Paths of Recombining Trinomial Trees](https://arxiv.org/abs/2510.02727)
*Ethan Torres,Ramavarapu Sreenivas,Richard Sowers*

Main category: cs.DS

TL;DR: 提出了一种用于重组三叉树的高效枚举算法，利用树的对称性和弱组合的双射关系，将搜索空间指数级缩减，使处理更深层次的树成为可能。


<details>
  <summary>Details</summary>
Motivation: 重组三叉树在期权定价、物流和反馈控制中广泛应用，但深度为D的树会产生O(3^D)条轨迹，使得穷举枚举不可行。需要开发高效的算法来处理更深层次的树以获得更精确的数值近似。

Method: 利用时间齐次动态下的两种对称性：(i)节点的平移不变性；(ii)允许路径与编码弱组合的有序元组之间的规范双射。引入质量转移枚举算法，通过滑动整数"质量"通过基数元组来生成每个路径等价类的唯一代表，同时隐式计数相关的弱组合。

Result: 算法将搜索空间指数级缩减，能够处理明显更深的树。推导出组合计数表达式的上界，得出算法成本的理论下界约为O(D^{1/2}1.612^D)。实证测试证实了该界限，显示出较小的常数开销和相对于经典广度优先遍历的显著加速。

Conclusion: 该算法框架与Motzkin路径和Narayana型细化存在结构联系，为路径依赖泛函提供了新的分析工具和精化的枚举公式。

Abstract: Recombining trinomial trees are a workhorse for modeling discrete-event
systems in option pricing, logistics, and feedback control. Because each node
stores a state-dependent quantity, a depth-$D$ tree naively yields
$\mathcal{O}(3^{D})$ trajectories, making exhaustive enumeration infeasible.
Under time-homogeneous dynamics, however, the graph exhibits two exploitable
symmetries: (i) translational invariance of nodes and (ii) a canonical
bijection between admissible paths and ordered tuples encoding weak
compositions. Leveraging these, we introduce a mass-shifting enumeration
algorithm that slides integer "masses" through a cardinality tuple to generate
exactly one representative per path-equivalence class while implicitly counting
the associated weak compositions. This trims the search space by an exponential
factor, enabling markedly deeper trees -- and therefore tighter numerical
approximations of the underlying evolution -- to be processed in practice. We
further derive an upper bound on the combinatorial counting expression that
induces a theoretical lower bound on the algorithmic cost of approximately
$\mathcal{O}\bigl(D^{1/2}1.612^{D}\bigr)$. This correspondence permits direct
benchmarking while empirical tests, whose pseudo-code we provide, corroborate
the bound, showing only a small constant overhead and substantial speedups over
classical breadth-first traversal. Finally, we highlight structural links
between our algorithmic/combinatorial framework and Motzkin paths with
Narayana-type refinements, suggesting refined enumerative formulas and new
potential analytic tools for path-dependent functionals.

</details>


### [13] [Pareto-optimal Non-uniform Language Generation](https://arxiv.org/abs/2510.02795)
*Moses Charikar,Chirag Pabbaraju*

Main category: cs.DS

TL;DR: 本文研究了极限语言生成中的帕累托最优性，提出了一个算法，其生成时间几乎达到帕累托最优，即在任何其他算法中，如果某个语言的生成时间更短，必然导致其他语言的生成时间更长。


<details>
  <summary>Details</summary>
Motivation: 现有工作在非均匀语言生成方面存在生成时间严格次优的问题，需要寻找最优的生成时间边界。

Method: 提出了一个算法框架，其生成时间t*(L)几乎达到帕累托最优，并适应性地扩展到噪声和代表性生成等实际场景。

Result: 开发了帕累托最优的非均匀生成算法，在噪声和代表性生成设置中也实现了帕累托最优性。

Conclusion: 帕累托最优性是非均匀生成能达到的最佳结果，所提出的算法框架在多个实际场景中都实现了这一最优性。

Abstract: Kleinberg and Mullainathan (2024) recently proposed an interesting model for
language generation in the limit: Given a countable collection of languages,
and an adversary enumerating the strings of some language $L$ from the
collection, the objective is to generate new strings from the target language,
such that all strings generated beyond some finite time are valid. Li, Raman
and Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform
generation guarantees in this model, giving algorithms that generate new valid
strings from $L$ after seeing a number of distinct input strings $t(L)$ that
depends only on $L$ (and the collection), but not the enumeration order.
However, for both these works, the language-wise generation times $t(L)$ of the
algorithm can be strictly sub-optimal.
  In this work, we study Pareto-optimality of non-uniform language generation
in the limit. We propose an algorithm, whose generation times $t^\star(L)$ are
(almost) Pareto-optimal: any other algorithm whose generation time for some
language $L$ is strictly smaller than $t^\star(L)$, must satisfy that its
generation time for some other language $L'$ is strictly worse than
$t^\star(L')$. Pareto-optimality is essentially the best that one can achieve
for non-uniform generation. Our algorithmic framework conveniently adapts to
further give Pareto-optimal non-uniform generation algorithms in the
practically motivated settings of noisy as well as representative generation.

</details>


### [14] [Low Recourse Arborescence Forests Under Uniformly Random Arcs](https://arxiv.org/abs/2510.02950)
*J Niklas Dahlmeier,D Ellis Hershkowitz*

Main category: cs.DS

TL;DR: 研究在边插入过程中维护最大基数树形图森林的问题，目标是尽量减少回溯（已维护解中改变的边数）。这是最大基数匹配问题的树形图版本。


<details>
  <summary>Details</summary>
Motivation: 探索在动态图环境中维护最大基数树形图时如何最小化回溯成本，这是最大基数匹配问题的树形图扩展，具有理论和实际意义。

Method: 在插入模型中分析问题：一方面观察对抗性边到达的最坏情况回溯下界；另一方面针对随机边到达情况设计算法。

Result: 发现对抗性边到达可能导致Ω(m·n)的回溯（与平凡上界O(m·n)匹配）；对于均匀随机到达的边，给出了期望回溯为O(m·log²n)的算法。

Conclusion: 该问题在对抗性设置下回溯成本很高，但在随机设置下可以实现显著更好的回溯性能，表明随机性在动态图算法中的重要作用。

Abstract: In this work, we study how to maintain a forest of arborescences of maximum
arc cardinality under arc insertions while minimizing recourse -- the total
number of arcs changed in the maintained solution. This problem is the
"arborescence version'' of max cardinality matching.
  On the impossibility side, we observe that even in this insertion-only model,
it is possible for $m$ adversarial arc arrivals to necessarily incur $\Omega(m
\cdot n)$ recourse, matching a trivial upper bound of $O(m \cdot n)$. On the
possibility side, we give an algorithm with expected recourse $O(m \cdot \log^2
n)$ if all $m$ arcs arrive uniformly at random.

</details>


### [15] [Oracle-based Uniform Sampling from Convex Bodies](https://arxiv.org/abs/2510.02983)
*Thanh Dang,Jiaming Liang*

Main category: cs.DS

TL;DR: 提出了基于交替采样框架的MCMC算法，用于在凸体上均匀采样，通过拒绝采样高效实现受限高斯预言机


<details>
  <summary>Details</summary>
Motivation: 解决在凸体上均匀采样的计算效率问题，特别是在仅能访问投影预言机或分离预言机的情况下

Method: 基于交替采样框架/近端采样器，使用增强分布上的Gibbs采样，通过拒绝采样实现受限高斯预言机

Result: 建立了在Rényi散度或χ²散度度量下的非渐近复杂度界限

Conclusion: 该算法能够在仅访问投影或分离预言机的情况下，高效获得凸体上的无偏均匀采样

Abstract: We propose new Markov chain Monte Carlo algorithms to sample a uniform
distribution on a convex body $K$. Our algorithms are based on the Alternating
Sampling Framework/proximal sampler, which uses Gibbs sampling on an augmented
distribution and assumes access to the so-called restricted Gaussian oracle
(RGO). The key contribution of this work is the efficient implementation of RGO
for uniform sampling on $K$ via rejection sampling and access to either a
projection oracle or a separation oracle on $K$. In both oracle cases, we
establish non-asymptotic complexities to obtain unbiased samples where the
accuracy is measured in R\'enyi divergence or $\chi^2$-divergence.

</details>


### [16] [Smooth Trade-off for Tensor PCA via Sharp Bounds for Kikuchi Matrices](https://arxiv.org/abs/2510.03061)
*Pravesh K. Kothari,Jeff Xu*

Main category: cs.DS

TL;DR: 本文解决了张量PCA问题中的对数因子必要性争议，证明了基于Kikuchi层级的谱算法在信号强度λ ≥ Θ_r(1)·n^{-r/4}·ℓ^{1/2-r/4}时就能成功，移除了之前算法中必需的√log n因子。


<details>
  <summary>Details</summary>
Motivation: 研究张量PCA问题中信号恢复所需的最小信号强度，特别关注对数因子是否必要，这对理解多项式时间算法能否在更低信号强度下恢复信号至关重要，也与量子加速问题相关。

Method: 使用基于Kikuchi层级的谱算法，通过随机矩阵理论中的非渐近技术进行分析。

Result: 证明了谱算法在λ ≥ Θ_r(1)·n^{-r/4}·ℓ^{1/2-r/4}时就能成功区分信号张量和随机高斯张量，移除了之前算法中必需的√log n因子。

Conclusion: 成功解决了WAM19和Bandeira的猜想，为张量PCA问题的平滑时间-复杂度权衡提供了理论基础，这对量子加速候选问题的研究具有重要意义。

Abstract: In this work, we revisit algorithms for Tensor PCA: given an order-$r$ tensor
of the form $T = G+\lambda \cdot v^{\otimes r}$ where $G$ is a random symmetric
Gaussian tensor with unit variance entries and $v$ is an unknown boolean vector
in $\{\pm 1\}^n$, what's the minimum $\lambda$ at which one can distinguish $T$
from a random Gaussian tensor and more generally, recover $v$? As a result of a
long line of work, we know that for any $\ell \in \N$, there is a $n^{O(\ell)}$
time algorithm that succeeds when the signal strength $\lambda \gtrsim
\sqrt{\log n} \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$. The question of whether the
logarithmic factor is necessary turns out to be crucial to understanding
whether larger polynomial time allows recovering the signal at a lower signal
strength. Such a smooth trade-off is necessary for tensor PCA being a candidate
problem for quantum speedups[SOKB25]. It was first conjectured by [WAM19] and
then, more recently, with an eye on smooth trade-offs, reiterated in a blogpost
of Bandeira.
  In this work, we resolve these conjectures and show that spectral algorithms
based on the Kikuchi hierarchy \cite{WAM19} succeed whenever $\lambda \geq
\Theta_r(1) \cdot n^{-r/4} \cdot \ell^{1/2-r/4}$ where $\Theta_r(1)$ only hides
an absolute constant independent of $n$ and $\ell$. A sharp bound such as this
was previously known only for $\ell \leq 3r/4$ via non-asymptotic techniques in
random matrix theory inspired by free probability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: CWM是一个320亿参数的开源代码世界模型，通过在Python解释器和Docker环境中训练观察-行动轨迹，结合多任务推理强化学习，显著提升了代码生成和理解能力。


<details>
  <summary>Details</summary>
Motivation: 为了超越仅从静态代码训练的限制，通过世界建模方法增强代码生成中的推理和规划能力，为研究者提供探索计算环境中代码生成的测试平台。

Method: 在Python解释器和Docker环境中进行大规模观察-行动轨迹的中期训练，并进行多任务推理强化学习，包括可验证编码、数学和多轮软件工程环境。

Result: CWM在多项基准测试中表现优异：SWE-bench Verified 65.8%、LiveCodeBench 68.6%、Math-500 96.6%、AIME 2024 76.0%。模型能够进行智能编码、Python代码逐步执行模拟，并展示推理能力。

Conclusion: CWM为代码世界建模研究提供了强大的基础，展示了世界模型在增强代码生成、推理和规划方面的潜力，并发布了多个训练阶段的模型检查点以支持进一步研究。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [18] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: T2L-Agent是一个项目级端到端框架，通过从模块到具体漏洞行的渐进式范围缩小，结合运行时证据和AST代码分块，实现精确的漏洞定位和修复。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法存在孤立分析代码、处理长上下文困难、仅提供函数或文件级粗粒度检测等问题，无法为工程师提供精确的行级定位和针对性修复指导。

Method: T2L-Agent采用多轮反馈机制，结合Agentic Trace Analyzer融合运行时证据（崩溃点、堆栈跟踪、覆盖率差异）和基于AST的代码分块，实现迭代精炼。

Result: 在T2L-ARVO基准测试中，T2L-Agent达到58.0%的检测率和54.8%的行级定位率，显著优于基线方法。

Conclusion: 该框架和基准测试将基于LLM的漏洞检测从粗粒度识别推向可部署、鲁棒、精确的诊断，减少噪声并加速开源软件工作流中的修复过程。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [19] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: 提出了AP2O-Coder方法，通过构建错误笔记本来逐步优化LLM的代码生成能力，自适应地纠正不同类型的错误，在减少偏好数据使用的同时提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法主要关注通过成功/失败信号来提升LLM的编程能力，但忽略了失败代码中的深层错误类型。

Method: 构建错误笔记本，逐步按错误类型优化LLM，并在训练过程中自适应地重放错误类型以适应LLM不断变化的弱点。

Result: 在0.5B到34B参数的代码和通用LLM上进行实验，AP2O-Coder将代码生成性能提升高达3%（pass@k指标），同时使用更少的偏好数据。

Conclusion: AP2O-Coder方法能够有效提升LLM的代码生成能力，通过针对性地纠正错误类型来实现更好的优化效果。

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [20] [Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions](https://arxiv.org/abs/2510.02404)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.SE

TL;DR: 本文分析了FaaS环境中的资源配置问题，提出了影响函数设计、配置、运行成本和性能保证的因素分类法，并对现有资源配置文献进行了全面综述，指出了研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算平台缺乏透明度，迫使开发者依赖专家知识或经验进行资源配置决策，这使得在满足性能约束的同时实现最优资源配置成为一项非平凡任务。开源框架允许独立配置函数资源，进一步增加了复杂性。

Method: 识别FaaS环境中资源配置技术的不同方面，提出影响函数设计、配置、运行成本和性能保证的因素分类法，并对现有资源配置文献进行分析和综述。

Result: 提出了一个全面的资源配置因素分类法，系统分析了当前函数配置研究现状，识别了现有研究中的空白领域。

Conclusion: 需要进一步研究来增强函数配置能力，加强无服务器计算环境的功能，以推动其更广泛的采用。

Abstract: The serverless cloud computing model offers a framework where the service
provider abstracts the underlying infrastructure management from developers. In
this serverless model, FaaS provides an event-driven, function-oriented
computing service characterised by fine-grained, usage-based pricing that
eliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,
and Cloud Run Functions require developers to configure their function(s) with
minimum operational resources for its successful execution. This resource
allocation influences both the operational expense and the performance quality
of these functions. However, a noticeable lack of platform transparency forces
developers to rely on expert knowledge or experience-based ad-hoc decisions to
request desired function resources. This makes optimal resource configuration a
non-trivial task while adhering to performance constraints. Furthermore, while
commercial platforms often scale resources like CPU and network bandwidth
proportional to memory, open-source frameworks permit independent configuration
of function resources, introducing additional complexity for developers aiming
to optimise their functions. These complexities have directed researchers to
resolve developer challenges and advance towards an efficient server-less
execution model. In this article, we identify different aspects of resource
configuration techniques in FaaS settings and propose a taxonomy of factors
that influence function design, configuration, run-time cost, and performance
guarantees. We conduct an analysis of existing literature on resource
configuration to present a comprehensive review of current studies on function
configuration. We also identify existing research gaps and suggest future
research directions to enhance function configuration and strengthen the
capabilities of serverless computing environments to drive its broader
adoption.

</details>


### [21] [Product Manager Practices for Delegating Work to Generative AI: "Accountability must not be delegated to non-human actors"](https://arxiv.org/abs/2510.02504)
*Mara Ulloa,Jenna L. Butler,Sankeerti Haniyur,Courtney Miller,Barrett Amos,Advait Sarkar,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 研究微软产品经理对GenAI的采用情况、使用案例、感知效益与障碍，以及任务委托框架和角色适应实践


<details>
  <summary>Details</summary>
Motivation: 了解GenAI如何改变软件产品经理的工作，填补现有研究主要关注开发者而忽视产品经理的空白

Method: 在微软进行混合方法研究：885名产品经理调查、731名产品经理的遥测数据分析、15名产品经理访谈

Result: 提供了产品经理的GenAI采用率、使用案例、感知效益与障碍，以及任务委托框架和角色适应实践

Conclusion: 讨论了GenAI工作流程采用对软件开发角色的更广泛影响

Abstract: Generative AI (GenAI) is changing the nature of knowledge work, particularly
for Product Managers (PMs) in software development teams. While much software
engineering research has focused on developers' interactions with GenAI, there
is less understanding of how the work of PMs is evolving due to GenAI. To
address this gap, we conducted a mixed-methods study at Microsoft, a large,
multinational software company: surveying 885 PMs, analyzing telemetry data for
a subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:
(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and
barriers and; (2) a framework capturing how PMs assess which tasks to delegate
to GenAI; (3) PMs adaptation practices for integrating GenAI into their roles
and perceptions of how their role is evolving. We end by discussing
implications on the broader GenAI workflow adoption process and software
development roles.

</details>


### [22] [ZeroFalse: Improving Precision in Static Analysis with LLMs](https://arxiv.org/abs/2510.02534)
*Mohsen Iranmanesh,Sina Moradi Sabet,Sina Marefat,Ali Javidi Ghasr,Allison Wilson,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.SE

TL;DR: ZeroFalse是一个结合静态分析和LLM的框架，通过将静态分析器输出作为结构化合约，并添加流敏感追踪、上下文证据和CWE特定知识，来减少SAST工具的误报，同时保持覆盖率。


<details>
  <summary>Details</summary>
Motivation: SAST工具在现代软件开发中很重要，但过多的误报削弱了开发者的信任并需要昂贵的手动分类。

Method: 将静态分析器输出视为结构化合约，通过LLM进行裁决前，丰富流敏感追踪、上下文证据和CWE特定知识。

Result: 在OWASP Java Benchmark上F1分数达到0.912，在OpenVuln数据集上达到0.955，召回率和精确率均超过90%。CWE专用提示始终优于通用提示，推理导向的LLM提供最可靠的精确率-召回率平衡。

Conclusion: ZeroFalse是增强SAST可靠性并支持其集成到真实CI/CD流水线中的实用且可扩展的方法。

Abstract: Static Application Security Testing (SAST) tools are integral to modern
software development, yet their adoption is undermined by excessive false
positives that weaken developer trust and demand costly manual triage. We
present ZeroFalse, a framework that integrates static analysis with large
language models (LLMs) to reduce false positives while preserving coverage.
ZeroFalse treats static analyzer outputs as structured contracts, enriching
them with flow-sensitive traces, contextual evidence, and CWE-specific
knowledge before adjudication by an LLM. This design preserves the systematic
reach of static analysis while leveraging the reasoning capabilities of LLMs.
We evaluate ZeroFalse across both benchmarks and real-world projects using ten
state-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on
the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall
and precision above 90%. Results further show that CWE-specialized prompting
consistently outperforms generic prompts, and reasoning-oriented LLMs provide
the most reliable precision-recall balance. These findings position ZeroFalse
as a practical and scalable approach for enhancing the reliability of SAST and
supporting its integration into real-world CI/CD pipelines.

</details>


### [23] [Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices](https://arxiv.org/abs/2510.02585)
*Majid Dashtbani,Ladan Tahvildari*

Main category: cs.SE

TL;DR: 本文通过分析微服务基准测试中的自动扩缩容问题，识别了在软件生命周期不同阶段（架构、实现、部署）的关键考虑因素，并验证了这些因素对自动扩缩容性能的重要影响。


<details>
  <summary>Details</summary>
Motivation: 微服务已成为云原生系统的主流架构，但实现有效的自动扩缩容仍然面临挑战。现有基准测试往往忽视设计、实现和部署等基础方面，难以在真实条件下评估扩缩容方法。

Method: 将多种最先进的自动扩缩容方法应用于广泛使用的微服务基准测试，识别实践中的扩缩容考虑因素，并按软件生命周期阶段（架构、实现、部署）进行分类，使用Sock-Shop基准进行验证。

Result: 研究发现忽视关键生命周期问题会降低自动扩缩容器性能，而解决这些问题可以实现更稳定和高效的扩缩容。评估了多种扩缩容策略（基于阈值、控制理论、学习、黑盒优化和依赖感知）。

Conclusion: 生命周期感知的工程实践对于释放微服务系统中自动扩缩容的全部潜力至关重要，强调了在架构、实现和部署阶段考虑扩缩容问题的重要性。

Abstract: Microservices have become the dominant architectural paradigm for building
scalable and modular cloud-native systems. However, achieving effective
auto-scaling in such systems remains a non-trivial challenge, as it depends not
only on advanced scaling techniques but also on sound design, implementation,
and deployment practices. Yet, these foundational aspects are often overlooked
in existing benchmarks, making it difficult to evaluate autoscaling methods
under realistic conditions. In this paper, we identify a set of practical
auto-scaling considerations by applying several state-of-the-art autoscaling
methods to widely used microservice benchmarks. To structure these findings, we
classify the issues based on when they arise during the software lifecycle:
Architecture, Implementation, and Deployment. The Architecture phase covers
high-level decisions such as service decomposition and inter-service
dependencies. The Implementation phase includes aspects like initialization
overhead, metrics instrumentation, and error propagation. The Deployment phase
focuses on runtime configurations such as resource limits and health checks. We
validate these considerations using the Sock-Shop benchmark and evaluate
diverse auto-scaling strategies, including threshold-based, control-theoretic,
learning-based, black-box optimization, and dependency-aware approaches. Our
findings show that overlooking key lifecycle concerns can degrade autoscaler
performance, while addressing them leads to more stable and efficient scaling.
These results underscore the importance of lifecycle-aware engineering for
unlocking the full potential of auto-scaling in microservice-based systems.

</details>


### [24] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: RedCodeAgent是首个自动化红队代理，通过自适应记忆模块和定制工具箱，系统性地发现代码代理的安全漏洞，在模拟沙盒环境中评估执行结果，显著优于现有红队方法。


<details>
  <summary>Details</summary>
Motivation: 当前静态安全基准和红队工具无法覆盖新兴现实风险场景，特别是不同越狱工具组合效应的边界条件，需要更有效的安全评估方法。

Method: 开发RedCodeAgent红队代理，包含自适应记忆模块和定制工具箱，能动态选择最有效的红队工具组合，并在模拟沙盒环境中评估代码代理执行结果。

Result: 在多个先进代码代理、多样化风险场景和编程语言上的评估显示，RedCodeAgent始终优于现有方法，攻击成功率更高、拒绝率更低且效率高，在实际代码助手中发现了之前未识别的安全风险。

Conclusion: RedCodeAgent通过自动化和优化红队流程，实现了对代码代理的可扩展、自适应和有效的安全评估。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [25] [Automatic Building Code Review: A Case Study](https://arxiv.org/abs/2510.02634)
*Hanlong Wan,Weili Xu,Michael Rosenberg,Jian Zhang,Aysha Siddika*

Main category: cs.SE

TL;DR: 提出了一种基于BIM和LLM的自动化建筑规范审查框架，通过智能代理从异构文件中提取几何、时间表和系统属性，并采用COMcheck API和RAG推理两种机制进行规范检查。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限或农村地区建筑官员面临的人工审查设计文档劳动密集、易出错且成本高的问题，利用BIM和LLM技术实现自动化规范审查。

Method: 开发了智能代理驱动框架，整合BIM数据提取与自动化验证，采用RAG和MCP代理管道，通过COMcheck API和RAG推理两种互补机制进行建筑规范检查。

Result: 案例演示显示框架能自动提取几何属性、解析操作时间表并验证照明限额，GPT-4o在效率和稳定性方面表现最佳，MCP代理管道在严谨性和可靠性上优于RAG推理管道。

Conclusion: 该研究推进了ACR研究，展示了可扩展、可互操作且生产就绪的方法，将BIM与权威规范审查工具连接起来。

Abstract: Building officials, particularly those in resource-constrained or rural
jurisdictions, face labor-intensive, error-prone, and costly manual reviews of
design documents as projects increase in size and complexity. The growing
adoption of Building Information Modeling (BIM) and Large Language Models
(LLMs) presents opportunities for automated code review (ACR) solutions. This
study introduces a novel agent-driven framework that integrates BIM-based data
extraction with automated verification using both retrieval-augmented
generation (RAG) and Model Context Protocol (MCP) agent pipelines. The
framework employs LLM-enabled agents to extract geometry, schedules, and system
attributes from heterogeneous file types, which are then processed for building
code checking through two complementary mechanisms: (1) direct API calls to the
US Department of Energy COMcheck engine, providing deterministic and
audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling
flexible interpretation where coverage is incomplete or ambiguous.
  The framework was evaluated through case demonstrations, including automated
extraction of geometric attributes (such as surface area, tilt, and insulation
values), parsing of operational schedules, and validation of lighting
allowances under ASHRAE Standard 90.1-2022. Comparative performance tests
across multiple LLMs showed that GPT-4o achieved the best balance of efficiency
and stability, while smaller models exhibited inconsistencies or failures.
Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in
rigor and reliability. This work advances ACR research by demonstrating a
scalable, interoperable, and production-ready approach that bridges BIM with
authoritative code review tools.

</details>


### [26] [Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing](https://arxiv.org/abs/2510.02718)
*Ali Ghanbari,Sasan Tavakkol*

Main category: cs.SE

TL;DR: DM#是一种基于傅里叶分析的深度神经网络突变测试加速技术，通过量化突变体行为进行聚类，仅测试代表性突变体来大幅减少测试成本。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络突变分析成本高昂，因为需要测试大量突变体和大型数据集。

Method: 利用傅里叶分析量化突变体行为，将相似行为的突变体聚类，仅测试每个聚类的代表性突变体。

Result: 在14个DNN模型上评估，平均加速28.38%，突变分数误差仅0.72%，相比其他方法误差显著降低。

Conclusion: DM#能有效加速DNN突变测试，在保持准确性的同时大幅减少计算成本。

Abstract: Deep neural network (DNN) mutation analysis is a promising approach to
evaluating test set adequacy. Due to the large number of generated mutants that
must be tested on large datasets, mutation analysis is costly. In this paper,
we present a technique, named DM#, for accelerating DNN mutation testing using
Fourier analysis. The key insight is that DNN outputs are real-valued functions
suitable for Fourier analysis that can be leveraged to quantify mutant behavior
using only a few data points. DM# uses the quantified mutant behavior to
cluster the mutants so that the ones with similar behavior fall into the same
group. A representative from each group is then selected for testing, and the
result of the test, e.g., whether the mutant is killed or survived, is reused
for all other mutants represented by the selected mutant, obviating the need
for testing other mutants. 14 DNN models of sizes ranging from thousands to
millions of parameters, trained on different datasets, are used to evaluate DM#
and compare it to several baseline techniques. Our results provide empirical
evidence on the effectiveness of DM# in accelerating mutation testing by
28.38%, on average, at the average cost of only 0.72% error in mutation score.
Moreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation
score error compared to random mutant selection, boundary sample selection, and
random sample selection techniques, respectively, while generally offering
comparable speed-up.

</details>


### [27] [Automated Repair of OpenID Connect Programs (Extended Version)](https://arxiv.org/abs/2510.02773)
*Tamjid Al Rahat,Yanju Chen,Yu Feng,Yuan Tian*

Main category: cs.SE

TL;DR: AuthFix是一个基于LLM的自动修复引擎，专门用于修复OpenID Connect实现中的安全漏洞，通过故障定位、补丁合成和验证三个组件，成功修复了74%的测试漏洞。


<details>
  <summary>Details</summary>
Motivation: OpenID Connect虽然广泛采用，但存在严重安全漏洞导致经济损失和安全漏洞，需要有效的自动修复方案来解决领域特定复杂性和精确故障定位的挑战。

Method: AuthFix采用反例引导的修复方法，整合故障定位、补丁合成和补丁验证三个关键组件，使用新颖的Petri网模型检查器来确保补丁正确性。

Result: 在23个OpenID漏洞的数据集上，AuthFix成功为17个漏洞（74%）生成了正确补丁，其中大部分补丁在语义上等同于开发者编写的修复。

Conclusion: AuthFix证明了基于LLM的自动修复方法在OpenID安全漏洞修复中的有效性，能够生成高质量且语义正确的补丁。

Abstract: OpenID Connect has revolutionized online authentication based on single
sign-on (SSO) by providing a secure and convenient method for accessing
multiple services with a single set of credentials. Despite its widespread
adoption, critical security bugs in OpenID Connect have resulted in significant
financial losses and security breaches, highlighting the need for robust
mitigation strategies. Automated program repair presents a promising solution
for generating candidate patches for OpenID implementations. However,
challenges such as domain-specific complexities and the necessity for precise
fault localization and patch verification must be addressed. We propose
AuthFix, a counterexample-guided repair engine leveraging LLMs for automated
OpenID bug fixing. AuthFix integrates three key components: fault localization,
patch synthesis, and patch verification. By employing a novel Petri-net-based
model checker, AuthFix ensures the correctness of patches by effectively
modeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates
that AuthFix successfully generated correct patches for 17 out of 23 bugs
(74%), with a high proportion of patches semantically equivalent to
developer-written fixes.

</details>


### [28] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: C2|Q>是一个硬件无关的量子软件开发框架，通过将经典代码转换为量子可执行程序，使经典软件工程师能够更容易地使用量子计算。


<details>
  <summary>Details</summary>
Motivation: 当前量子开发环境要求开发者处理软件栈的低层细节，使得经典软件工程师难以使用量子计算。

Method: 采用模块化软件工程原则，将工作流分为三个核心模块：编码器（问题分类、生成量子兼容格式、构建量子电路）、部署模块（生成电路、基于保真度、运行时间和成本推荐硬件）和解码器（将量子输出解释为经典解决方案）。

Result: 编码器模块完成率达93.8%，硬件推荐模块能正确选择最多56量子比特的设备，完整工作流处理了434个Python代码片段和100个JSON输入，完成率分别为93.8%和100%。在NISQ硬件上的案例研究中，相比手动实现减少了近40倍的工作量。

Conclusion: C2|Q>框架成功地将量子软件开发抽象化，显著降低了经典软件工程师使用量子计算的门槛。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [29] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出一个猜想：代码表示越容易解析，模型性能越好。通过GramTrans方法将上下文无关语言转换为LL(1)类表示，实验证明解析难度与模型性能强相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用多种代码表示（纯文本、语法规则序列、语法树序列），但缺乏对解析难度与模型效果关系的系统性理解。

Method: 提出GramTrans方法，通过分层冲突消除算法将上下文无关语言转换为LL(1)类表示，在语法简洁性和标记效率之间实现灵活权衡。

Result: 在Python和Java上使用三个代码生成模型评估，GramTrans在多个基准测试中相比基线表示均带来显著改进。

Conclusion: 解析难度与模型性能强相关，GramTrans通过降低解析难度有效提升了代码生成模型的性能。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [30] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 该论文通过稀疏自编码器分解LLM表示，识别代码正确性方向，发现这些方向能可靠预测错误代码，且代码生成成功依赖于关注测试用例而非问题描述。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，理解其内部正确性机制对安全部署至关重要。

Method: 应用稀疏自编码器分解LLM表示，使用t统计量选择预测方向，通过分离分数确定引导方向，并通过引导、注意力分析和权重正交化分析机制特性。

Result: 代码正确性方向能可靠预测错误代码，修正能力虽统计显著但涉及修复错误与保留正确代码之间的权衡。成功的代码生成依赖于关注测试用例。

Conclusion: 基础模型中识别的方向在指令微调后仍保持有效性，表明预训练期间学习的代码正确性机制在微调中被重新利用。提出了三个实际应用：提示策略应优先测试用例、预测方向可作为错误警报、预测器可指导选择性引导。

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [31] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: AUTOPROBE是一种模型无关的方法，通过动态选择LLM内部最有信息量的表示来评估代码正确性，在编译性、功能性和安全性评估方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预选层和token位置的表示，限制了在不同模型架构和任务间的泛化能力。需要一种能动态选择重要内部表示的方法来提升代码正确性评估的鲁棒性。

Method: 使用基于注意力的机制学习隐藏状态的重要性分数，聚焦最相关特征，然后将加权表示聚合并传递给探测分类器来预测代码正确性。

Result: 在多个基准测试和代码LLM上的实验表明，AUTOPROBE始终优于基线方法。安全性评估比最先进白盒方法高18%，编译性和功能性评估分别比其它方法高19%和111%。

Conclusion: 动态选择重要内部信号使AUTOPROBE成为评估各种LLM生成代码正确性的鲁棒且可泛化的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [32] [Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications](https://arxiv.org/abs/2510.02991)
*Carlos Albuquerque,Filipe F. Correia*

Main category: cs.SE

TL;DR: 本文提出了三种监控云原生应用的设计模式：分布式追踪、应用指标和基础设施指标，旨在解决分布式系统中可观测性碎片化和根因分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构日益分布式和易变，诊断系统问题变得更加困难，需要处理碎片化的可观测性和更复杂的根因分析。

Method: 基于先前工作，引入三种设计模式：分布式追踪（改进跨服务请求流的可见性）、应用指标（结构化应用性能监控）、基础设施指标（监控运行环境）。

Result: 这些模式源自行业实践和可观测性框架，为软件从业者提供指导。

Conclusion: 三种设计模式有助于提升云原生应用的可靠性和可维护性，支持延迟分析、异常检测和资源监控。

Abstract: Observability helps ensure the reliability and maintainability of
cloud-native applications. As software architectures become increasingly
distributed and subject to change, it becomes a greater challenge to diagnose
system issues effectively, often having to deal with fragmented observability
and more difficult root cause analysis. This paper builds upon our previous
work and introduces three design patterns that address key challenges in
monitoring cloud-native applications. Distributed Tracing improves visibility
into request flows across services, aiding in latency analysis and root cause
detection, Application Metrics provides a structured approach to instrumenting
applications with meaningful performance indicators, enabling real-time
monitoring and anomaly detection, and Infrastructure Metrics focuses on
monitoring the environment in which the system is operated, helping teams
assess resource utilization, scalability, and operational health. These
patterns are derived from industry practices and observability frameworks and
aim to offer guidance for software practitioners.

</details>


### [33] [Patterns for Teaching Agile with Student Projects -- Team and Project Setup](https://arxiv.org/abs/2510.03005)
*Daniel Pinho,Petr Pícha,Filipe Correia,Přemek Brada*

Main category: cs.SE

TL;DR: 提出一个用于教授敏捷软件开发（ASD）的大学课程模式语言，重点关注团队和项目设置阶段，包含五个具体模式。


<details>
  <summary>Details</summary>
Motivation: 现有关于ASD教学的文献缺乏可操作建议，过于关注框架或扩展到非软件开发领域，需要提供实用的教学指导。

Method: 基于作者在高等教育环境中的教学经验，开发模式语言，特别关注团队和项目设置阶段，提出了五个具体模式。

Result: 提出了五个聚焦团队和项目设置的模式：限制团队规模、缩小项目范围、非关键业务项目、自组织团队、团队选择主题。

Conclusion: 这些模式为开发完整的ASD教学模式语言提供了起点，有助于改善大学敏捷软件开发课程的教学效果。

Abstract: Higher education courses teaching about agile software development (ASD) have
increased in commonality as the ideas behind the Agile Manifesto became more
commonplace in the industry. However, a lot of the literature on how ASD is
applied in the classroom does not provide much actionable advice, focusing on
frameworks or even moving beyond the software development area into teaching in
an agile way. We, therefore, showcase early work on a pattern language that
focuses on teaching ASD practices to university students, which stems from our
own experiences as educators in higher education contexts. We present five
patterns, specifically focused on team and project setup phase: Capping Team
Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling
Teams, and Team Chooses Topic as a starting point for developing the overall
pattern language.

</details>


### [34] [Investigating The Smells of LLM Generated Code](https://arxiv.org/abs/2510.03029)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 该研究提出基于场景的方法评估LLM生成代码的质量，发现LLM生成的代码比专业编写的参考代码有更多代码异味，平均异味增加63.34%，其中Codex表现最差(84.97%)，Falcon相对最好(42.28%)。


<details>
  <summary>Details</summary>
Motivation: 目前关于LLM生成代码的研究主要集中在功能正确性上，而对代码质量的研究较少，需要评估LLM生成代码在不同场景下的质量表现。

Method: 使用基于场景的方法，通过测量代码异味来评估代码质量，将测试数据集按代码主题和任务复杂度划分为不同子集，比较四种先进LLM(Gemini Pro、ChatGPT、Codex、Falcon)生成的Java代码与专业编写的参考代码。

Result: LLM生成的代码异味发生率显著高于参考代码，平均增加63.34%，其中实现异味增加73.35%，设计异味增加21.42%。任务越复杂、涉及面向对象等高级主题时，代码异味增加越多。

Conclusion: LLM在不同编码任务复杂度和主题上的表现与相应场景下人工编写代码的质量高度相关，但LLM生成代码的质量明显低于人工编写代码。

Abstract: Context: Large Language Models (LLMs) are increasingly being used to generate
program code. Much research has been reported on the functional correctness of
generated code, but there is far less on code quality.
  Objectives: In this study, we propose a scenario-based method of evaluating
the quality of LLM-generated code to identify the weakest scenarios in which
the quality of LLM generated code should be improved.
  Methods: The method measures code smells, an important indicator of code
quality, and compares them with a baseline formed from reference solutions of
professionally written code. The test dataset is divided into various subsets
according to the topics of the code and complexity of the coding tasks to
represent different scenarios of using LLMs for code generation. We will also
present an automated test system for this purpose and report experiments with
the Java programs generated in response to prompts given to four
state-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.
  Results: We find that LLM-generated code has a higher incidence of code
smells compared to reference solutions. Falcon performed the least badly, with
a smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)
and finally Codex (84.97%). The average smell increase across all LLMs was
63.34%, comprising 73.35% for implementation smells and 21.42% for design
smells. We also found that the increase in code smells is greater for more
complex coding tasks and for more advanced topics, such as those involving
object-orientated concepts.
  Conclusion: In terms of code smells, LLM's performances on various coding
task complexities and topics are highly correlated to the quality of human
written code in the corresponding scenarios. However, the quality of LLM
generated code is noticeably poorer than human written code.

</details>


### [35] [Refactoring Towards Microservices: Preparing the Ground for Service Extraction](https://arxiv.org/abs/2510.03050)
*Rita Peixoto,Filipe F. Correia,Thatiane Rosa,Eduardo Guerra,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文提出了一个包含7种重构模式的目录，专门用于支持向微服务架构的迁移，重点关注依赖关系的处理。


<details>
  <summary>Details</summary>
Motivation: 随着组织从单体系统向微服务过渡，现有方法主要提供架构级指导，忽视了代码级的挑战和依赖关系，导致迁移过程仍然手动且劳动密集。

Method: 开发了一个重构目录，整合了文献中识别的重构模式，提供结构化的逐步方法来解决代码级依赖问题。

Result: 为开发者提供了系统化的迁移指南，简化了迁移过程，并为潜在自动化奠定了基础。

Conclusion: 该工作通过提供结构化方法，使开发者能够高效有效地实施微服务迁移，解决了现有方法在代码级系统化方面的关键差距。

Abstract: As organizations increasingly transition from monolithic systems to
microservices, they aim to achieve higher availability, automatic scaling,
simplified infrastructure management, enhanced collaboration, and streamlined
deployments. However, this migration process remains largely manual and
labour-intensive. While existing literature offers various strategies for
decomposing monoliths, these approaches primarily focus on architecture-level
guidance, often overlooking the code-level challenges and dependencies that
developers must address during the migration. This article introduces a
catalogue of seven refactorings specifically designed to support the transition
to a microservices architecture with a focus on handling dependencies. The
catalogue provides developers with a systematic guide that consolidates
refactorings identified in the literature and addresses the critical gap in
systematizing the process at the code level. By offering a structured,
step-by-step approach, this work simplifies the migration process and lays the
groundwork for its potential automation, empowering developers to implement
these changes efficiently and effectively.

</details>


### [36] [State Field Coverage: A Metric for Oracle Quality](https://arxiv.org/abs/2510.03071)
*Facundo Molina,Nazareno Aguirre,Alessandra Gorla*

Main category: cs.SE

TL;DR: 提出了一种名为状态字段覆盖的新指标，用于评估测试预言的质量，该指标衡量预言在测试执行期间可能访问的对象状态字段比例。


<details>
  <summary>Details</summary>
Motivation: 现有指标要么无法为预言改进提供全面基础，要么仅适用于特定类型的预言，限制了通用性。评估预言质量对于提高测试过程的整体有效性至关重要。

Method: 实现了一种静态计算状态字段覆盖指标的机制，该指标高效且能通过识别未检查的状态字段为改进测试预言提供直接指导。

Result: 通过对273个表示不变式和249,027个测试断言的实验评估，结果显示状态字段覆盖与预言的故障检测能力（通过变异得分衡量）有强相关性。

Conclusion: 状态字段覆盖是评估预言质量的合适指标，因为它与预言的故障检测能力密切相关。

Abstract: The effectiveness of testing in uncovering software defects depends not only
on the characteristics of the test inputs and how thoroughly they exercise the
software, but also on the quality of the oracles used to determine whether the
software behaves as expected. Therefore, assessing the quality of oracles is
crucial to improve the overall effectiveness of the testing process. Existing
metrics have been used for this purpose, but they either fail to provide a
comprehensive basis for guiding oracle improvement, or they are tailored to
specific types of oracles, thus limiting their generality.
  In this paper, we introduce state field coverage, a novel metric for
assessing oracle quality. This metric measures the proportion of an object's
state, as statically defined by its class fields, that an oracle may access
during test execution. The main intuition of our metric is that oracles with a
higher state field coverage are more likely to detect faults in the software
under analysis, as they inspect a larger portion of the object states to
determine whether tests pass or not.
  We implement a mechanism to statically compute the state field coverage
metric. Being statically computed, the metric is efficient and provides direct
guidance for improving test oracles by identifying state fields that remain
unexamined. We evaluate state field coverage through experiments involving 273
representation invariants and 249,027 test assertions. The results show that
state field coverage is a well-suited metric for assessing oracle quality, as
it strongly correlates with the oracles' fault-detection ability, measured by
mutation score.

</details>


### [37] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: LLMs在代码任务中表现良好，但其理解程序含义的机制尚不明确。研究发现代码通过结构语义和命名两个渠道传递信息，去除命名会严重影响意图级任务，甚至影响执行任务，表明当前基准测试奖励命名模式记忆而非真正的语义推理。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何理解代码含义，区分结构语义和人类可解释命名在代码理解中的作用，揭示当前基准测试可能存在的缺陷。

Method: 引入语义保留的混淆技术，创建ClassEval-Obf基准测试，系统性地抑制命名线索同时保持行为不变。

Result: 混淆暴露了标识符泄漏问题，ClassEval-Obf减少了性能差距膨胀，削弱了记忆捷径，为评估LLMs代码理解和泛化能力提供了更可靠的基础。

Conclusion: 当前LLMs在代码任务中的表现部分依赖于命名模式记忆，而非真正的语义推理，需要更可靠的基准测试来评估其代码理解能力。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [38] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 提出两种LLM策略（bug abstention和patch validation）来减少自动化程序修复系统中的噪音，提高修复成功率


<details>
  <summary>Details</summary>
Motivation: 解决工业级自动化程序修复系统中，向开发者展示不太可能成功的补丁所导致的噪音问题，避免浪费开发者时间和削弱对自动化代码更改的信任

Method: 引入两种互补的LLM策略：bug abstention策略排除系统不太可能修复的错误，patch validation策略拒绝不太可能是良好修复的补丁

Result: 在Google代码库的174个人工报告错误上，两种策略组合可将成功率提高39个百分点；在空指针异常和sanitizer报告的错误上，patch validation也提高了平均单样本成功率

Conclusion: 这种双策略方法为可靠、工业规模的自动化程序修复系统部署提供了实用路径

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [39] [Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems](https://arxiv.org/abs/2510.02487)
*Ahmed Danladi Abdullahi,Erfan Bahrami,Tooska Dargahi,Mohammed Al-Khalidi,Mohammad Hammoudeh*

Main category: cs.NI

TL;DR: 本文综述了6G智能交通系统的机遇与挑战，重点关注信任、安全和隐私问题，特别探讨了量子技术在增强安全性和引入新漏洞方面的双重作用。


<details>
  <summary>Details</summary>
Motivation: 随着6G技术的发展，智能交通系统有望带来革命性的交通变革，但面临诸多安全和隐私挑战，需要建立全面的安全框架来确保系统的可靠部署。

Method: 提出了6G-ITS中不同攻击模型的分类法，比较了5G-ITS和6G-ITS的安全威胁，并提供了潜在的缓解解决方案。

Result: 识别了6G-ITS中的关键安全漏洞，强调了量子技术既通过量子密钥分发增强安全性，又引入新漏洞的双重影响。

Conclusion: 迫切需要建立涵盖物理基础设施保护、网络协议安全、数据管理保障、应用安全措施和信任管理系统的多层面综合安全框架，以有效缓解新兴安全隐私风险。

Abstract: The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.

</details>


### [40] [L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications](https://arxiv.org/abs/2510.02682)
*Haoran Wan,Kyle Jamieson*

Main category: cs.NI

TL;DR: L4Span是一种新的RAN设计，通过抽象RAN队列复杂性，将RAN队列状态与端到端低延迟信令关联，在毫秒级时间尺度预测RAN队列占用并执行ECN标记，显著降低延迟同时保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着交互式应用对低延迟网络的需求增长，有线宽带ISP已部署先进的队列占用信令机制，但蜂窝无线接入网络(RAN)在这方面落后，需要一种能在RAN中部署的低延迟解决方案。

Method: 提出L4Span设计，抽象RAN队列复杂性，提供简单接口将RAN队列状态与端到端低延迟信令连接；在毫秒级时间尺度预测RAN队列占用，为低延迟和经典流量执行ECN标记；设计轻量级，对RAN修改最小，保持3GPP和O-RAN兼容性。

Result: 在srsRAN开源软件上实现原型；评估显示L4Span在各种无线信道条件下，将低延迟和经典流量的单向延迟降低高达98%，同时保持接近线速的吞吐量。

Conclusion: L4Span通过简单接口有效解决了RAN中的低延迟问题，显著改善延迟性能且易于部署，为无线网络中的低延迟通信提供了实用解决方案。

Abstract: Design for low latency networking is essential for tomorrow's interactive
applications, but it is essential to deploy incrementally and universally at
the network's last mile. While wired broadband ISPs are rolling out the leading
queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),
another important last mile to many users, lags behind these efforts. This
paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN
queueing in a simple interface, thus tying the queue state of the RAN to
end-to-end low-latency signaling all the way back to the content server. At
millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and
performs ECN marking for both low-latency and classic flows. L4Span is
lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN
compliant for maximum ease of deployment. We implement a prototype on the
srsRAN open-source software in C++. Our evaluation compares the performance of
low-latency as well as classic flows with or without the deployment of L4Span
in various wireless channel conditions. Results show that L4Span reduces the
one-way delay of both low-latency and classic flows by up to 98 %, while
simultaneously maintaining near line-rate throughput. The code is available at
https://github.com/PrincetonUniversity/L4Span.

</details>


### [41] [FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways](https://arxiv.org/abs/2510.02800)
*Rohith Reddy Vennam,Maiyun Zhang,Raghav Subbaraman,Deepak Vashist,Dinesh Bharadia*

Main category: cs.NI

TL;DR: 提出FSMA协议，通过轻量级自由信号啁啾(FreeChirp)实现无同步、链路感知的传输，解决非地面网络中碰撞和动态链路问题


<details>
  <summary>Details</summary>
Motivation: 低轨卫星和无人机等非地面网络面临两大挑战：大覆盖范围导致频繁碰撞，移动网关造成动态链路，需要无同步的链路感知传输方案

Method: FSMA协议引入网关控制的轻量级FreeChirp信号，确保节点仅在信道空闲且链路可靠时传输，无需同步或复杂调度

Result: 使用25个商用LoRa设备和无人机移动网关测试，吞吐量提高2倍，包接收率提升2-5倍，能效改善5倍；大规模仿真支持5000+设备/卫星过境

Conclusion: FSMA是实现可扩展、高能效、可靠非地面网络物联网的实用方案

Abstract: The proliferation of Low Earth Orbit (LEO) satellites for universal IoT
applications and the growing use of drones in emergency services, agriculture,
and military operations highlight the transformative potential of
non-terrestrial networks (NTN). However, these networks face two key
challenges: (1) large coverage footprints that create frequent collisions and
(2) moving gateways that cause dynamic links and demand synchronization-free,
link-aware transmissions. Existing random access schemes such as ALOHA, CSMA,
and BSMA fail in this setting, suffering from high collision rates, hidden
terminals, or excessive gateway energy overhead. We propose Free Signal
Multiple Access (FSMA), a gateway-controlled protocol that introduces a
lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes
transmit only when the channel is idle and when links are reliable, thereby
reducing collisions and enabling link-aware access without the need for
synchronization or complex scheduling. We evaluate FSMA using 25 commercial
LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x
higher throughput, 2x to 5x better packet reception ratio, and 5x improved
energy efficiency compared to the baselines. Large-scale simulations with a
custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices
per satellite pass. These results establish FSMA as a practical step toward
scalable, energy-efficient, and reliable NTN IoT networks.

</details>


### [42] [DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol](https://arxiv.org/abs/2510.02895)
*Akihisa Takahashi,Yoshito Tobe*

Main category: cs.NI

TL;DR: DH-EAC是一种纯量子协议，用于在由多个量子局域网组成的广域量子网络中公平匿名地分配稀缺的纠缠资源。


<details>
  <summary>Details</summary>
Motivation: 现有的Dicke态纯量子MAC协议主要针对单个量子局域网，难以扩展到广域动态环境，且需要避免后选择协调。

Method: 采用两层纯量子抽签机制：外层选择获胜的量子局域网，内层选择每个获胜局域网内的获胜节点。通过测量固定获胜集合和配额，无需经典往返通信。

Result: DH-EAC在纠缠访问控制空间中提供了可实现的平衡点，在纯量子竞争解决、匿名性和多量子局域网网络可扩展性之间取得平衡。

Conclusion: 该协议为广域量子网络中的公平匿名纠缠分配提供了可行的解决方案。

Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a
pure-quantum protocol for fair and anonymous allocation of scarce entanglement
across wide-area quantum networks composed of many quantum LANs (QLANs). Prior
Dicke-state-based pure-quantum MACs resolve contention by local measurements
without classical signaling, but they mainly target a single QLAN under static
conditions; extending them to wide-area, dynamic settings while avoiding
post-selection reconciliation remains open. DH-EAC adopts a two-layer
pure-quantum lottery: the outer layer selects winning QLANs and the inner layer
selects winning nodes within each winning QLAN. A key design principle is that
both the winning set and the per-QLAN quota are fixed by measurements alone, so
the contention loop requires no classical round trip. The protocol thus aims to
jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and
fairness (bias suppression under heterogeneous QLAN sizes). We also provide
analytical models for success probability and latency under a standard i.i.d.
loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke
within one QLAN and a classical GO-driven allocator - using a minimal,
reproducible set of scenarios. Metrics include success probability, end-to-end
latency, throughput, and Jain's fairness index. The results indicate that
DH-EAC offers an implementable design point in the space of entanglement access
control, balancing pure-quantum contention resolution, anonymity, and
scalability for multi-QLAN networks.

</details>


### [43] [Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network](https://arxiv.org/abs/2510.02958)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Lau Sian Lun*

Main category: cs.NI

TL;DR: 该论文利用真实世界多运营商路测数据集，通过序列深度学习模型（GRU、LSTM、Transformer）预测密集城市蜂窝网络中的切换行为，显著减少了乒乓切换和不必要切换。


<details>
  <summary>Details</summary>
Motivation: 密集城市蜂窝网络中高小区密度、用户移动性和多样化服务需求增加了不必要的切换和乒乓效应的可能性，需要有效的切换管理解决方案。

Method: 将切换预测建模为序列问题，评估GRU、LSTM和Transformer架构，在仅使用RSRP和使用所有特征两种设置下进行实验。

Result: 基于GRU的模型实现了98%的乒乓切换减少和46.25%的不必要切换减少，推理时间仅0.91秒，适合实时边缘部署。

Conclusion: 该方法在移动性鲁棒性和用户体验质量方面相比传统3GPP A3算法有显著提升，为5G及后续网络的智能移动性管理提供了有效解决方案。

Abstract: Efficient handover management remains a critical challenge in dense urban
cellular networks, where high cell density, user mobility, and diverse service
demands increase the likelihood of unnecessary handovers and ping-pong effects.
This paper leverages a real-world, multi-operator drive-test dataset of 30,925
labelled records collected within a 2 km area around Sunway City to investigate
sequence-based deep learning approaches for handover detection and avoidance.
We formulate handover prediction as a sequence problem and evaluate Gated
Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer
architectures under Reference Signal Received Power (RSRP)-only and all-feature
settings. The integration of multi-dimensional features significantly enhanced
handover performance in dense urban cellular networks. The proposed GRU-based
model achieved a remarkable 98% reduction in ping-pong handovers, alongside a
46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only
approach which yielded a 22.19% reduction. Furthermore, the model demonstrated
a 46% improvement in Time of Stay (ToS), indicating more stable user
connections. With an inference time of just 0.91 seconds, the solution proves
highly efficient and well-suited for real-time edge deployment scenarios.
Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate
significant gains in mobility robustness and user Quality of Experience (QoE)
improvement. The dataset is released to foster reproducibility and further
research in intelligent mobility management for 5G and beyond.

</details>


### [44] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 本文提出了一种自动生成数字孪生的方法，用于高效准确验证电信网络软件，符合ITU-T自主网络架构的实验子系统要求。


<details>
  <summary>Details</summary>
Motivation: 随着电信网络软件使用的增加，需要更高效的测试验证方法。数字孪生虽能提供测试环境，但传统配置需要大量时间和人工。

Method: 开发自动生成数字孪生的方法，基于ITU-T自主网络架构的实验子系统，实现高效配置和执行。

Result: 实验结果显示该方法可行，能自动创建高效的数字孪生，且精度足以纳入现有验证流程。

Conclusion: 自动生成数字孪生方法为电信网络软件验证提供了可行的解决方案，可显著减少配置时间和人工成本。

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了一种用于极值预测的数据增强框架，结合GANs和SMOTE等数据增强技术，使用Conv-LSTM和BD-LSTM深度学习模型进行多步预测，重点关注极端值区域。


<details>
  <summary>Details</summary>
Motivation: 极值预测在金融和气候变化等领域具有重要应用价值，但面临数据不平衡的挑战。本研究旨在通过数据增强技术改善极值预测性能。

Method: 使用Conv-LSTM和BD-LSTM深度学习模型进行多步预测，结合GANs和SMOTE数据增强技术，并提出了基于相关性函数的极端值数据增强策略。

Result: SMOTE-based策略在短期和长期预测中均表现最佳，Conv-LSTM在周期性稳定数据中表现更好，BD-LSTM在混沌或非平稳序列中表现更优。

Conclusion: 数据增强技术能有效提升极值预测性能，SMOTE方法具有更好的适应性，不同深度学习模型在不同数据类型中具有互补优势。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [46] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: OpenTSLM是一个将时间序列作为原生模态集成到预训练LLM中的时间序列语言模型家族，通过两种架构（SoftPrompt和Flamingo）解决了LLM无法处理时间序列的问题，在医疗时间序列推理任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM在多模态数据处理方面表现出色，但在医学应用中无法处理时间序列数据，这限制了其在临床信息合成和数字健康应用中的潜力。

Method: 提出了两种架构：OpenTSLM-SoftPrompt通过软提示将可学习时间序列标记与文本标记连接；OpenTSLM-Flamingo通过交叉注意力机制集成时间序列和文本。在三个数据集（HAR-CoT、Sleep-CoT、ECG-QA-CoT）上进行基准测试。

Result: OpenTSLM在所有任务中均优于基线方法，在睡眠分期任务中达到69.9 F1，在HAR任务中达到65.4 F1，而仅微调的纯文本模型分别为9.05和52.2。即使是1B参数的OpenTSLM模型也超过了GPT-4o。OpenTSLM-Flamingo在处理长序列时表现更优且内存需求稳定。

Conclusion: OpenTSLM成功将时间序列作为原生模态集成到LLM中，在医疗时间序列推理任务中表现出强大的推理能力，为时间序列语言建模提供了有效的解决方案。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [47] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: RainSeer是一个结构感知的降雨场重建框架，利用雷达反射率作为物理结构先验，通过两阶段架构解决雷达数据与地面降雨观测之间的空间对齐和物理转换问题。


<details>
  <summary>Details</summary>
Motivation: 现有空间插值方法往往会过度平滑关键结构，无法捕捉急剧变化和局部极端值，影响洪水预报、水文建模和气候分析的准确性。

Method: 采用物理信息驱动的两阶段架构：结构到点映射器进行空间对齐，将中尺度雷达结构投影到局部地面降雨；地理感知降雨解码器通过因果时空注意力机制捕捉水凝物下降过程中的语义转换。

Result: 在两个公共数据集（RAIN-F和MeteoNet）上的评估显示，相比最先进基线方法，MAE降低了超过13.31%，并显著提高了重建降雨场的结构保真度。

Conclusion: RainSeer通过利用雷达反射率作为物理结构先验，有效解决了降雨场重建中的空间对齐和物理转换挑战，在保持结构细节方面表现出色。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [48] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 提出Advisor Models，一种轻量级参数化策略，通过强化学习训练，在上下文中向黑盒模型发出自然语言指导指令，实现动态优化。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型作为黑盒服务部署，无法修改模型权重，只能通过提示进行定制。静态提示优化产生单一固定提示，无法适应不同输入、用户或环境。

Method: 训练轻量级参数化策略（advisor），使用强化学习在上下文中反应式地发出自然语言指导指令。advisor作为第二个小模型位于输入和模型之间，基于环境奖励信号按实例调整行为。

Result: 在涉及推理和个性化的多个领域中，Advisor Models优于静态提示优化器，能够发现环境动态并提高下游任务性能。还展示了advisor在黑盒模型间的可迁移性，以及框架在保持对分布外输入鲁棒性的同时实现专业化的能力。

Conclusion: Advisor Models为黑盒系统提供了可学习的接口，其中advisor充当参数化、环境特定的记忆。通过Advisor Models对黑盒模型进行动态优化是实现个性化和环境适应性AI的有前景方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [49] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出了基于预测市场的训练数据选择方法，通过市场机制自动整合多种信号（不确定性、稀有性、多样性等），使用单一流动性参数控制选择集中度，并引入价格-长度规则处理token预算约束。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据选择方法需要手动组合多种异质信号（不确定性、稀有性、多样性等），权重设置缺乏理论依据且通常采用临时方案。

Method: 使用成本函数预测市场（LMSR）为每个样本定价，信号作为交易者，通过价格-长度规则ρ=p/ℓ^γ处理token预算，并加入轻量级多样性头提高覆盖度。

Result: 在GSM8K（60k token预算）上，市场选择器与强基线性能相当，同时降低了种子方差，选择开销小于0.1 GPU小时；在AGNews（保留5-25%数据）上实现竞争性准确率，并改善了平衡性和稳定性。

Conclusion: 该框架在固定计算预算下统一了多信号数据筛选，适用于提示级推理和分类任务，提供了透明可控的聚合机制。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [50] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 本文研究了后训练量化(PTQ)在安全关键环境中可能导致的极端性能下降问题，通过知识蒸馏和强化学习探索了网络和比特宽度策略对的最坏情况性能影响。


<details>
  <summary>Details</summary>
Motivation: 后训练量化虽然能降低计算复杂度和内存使用，但在推理过程中可能因输入分布变化导致性能急剧下降，这在安全关键环境中尤其需要关注。

Method: 使用知识蒸馏和强化学习来学习网络和比特宽度策略对，分析量化下的最坏情况潜在性能下降。

Result: 发现了存在"有害"的网络-策略对，多个实例显示准确率下降10-65%，而"鲁棒"对应物仅下降<2%。通过系统实验和分析，初步探索了最高脆弱点。

Conclusion: 研究结果强调了在现实世界部署中需要谨慎，并鼓励在深度学习领域进行更严格的鲁棒性检验和安全考虑。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [51] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE是一种流式数据子集选择方法，通过维护梯度几何的紧凑Frequent Directions草图，优先选择与共识方向对齐的示例，实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 在大型数据集上训练现代神经网络计算和能耗密集，需要更高效的训练方法。

Method: 使用Frequent Directions草图在O(ℓD)内存中维护梯度几何，通过一致性评分优先选择梯度与共识方向对齐的示例，避免N×N成对相似性和显式N×ℓ梯度存储。

Result: 在多个基准测试中，SAGE使用小子集预算训练，保持与全数据训练和最新子集选择基准相当的准确性，同时减少端到端计算和峰值内存。

Conclusion: SAGE提供了一种实用的恒定内存替代方案，补充了剪枝和模型压缩，用于高效训练。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [52] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: 本文研究了基于不确定性的模型选择策略，用于优化生物分子功效预测的集成学习，无需真实标签即可选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 上下文学习器（如TabPFN）在生物分子功效预测中表现良好，但其性能对提供的上下文高度敏感。如何在无法获得真实标签的情况下选择最佳模型进行集成是一个开放性问题。

Method: 采用不确定性引导的模型选择策略，使用TabPFN模型结合简单序列特征，通过预测的分位数间范围（IQR）作为不确定性度量，选择并平均具有最低平均IQR的模型集成。

Result: 在siRNA敲低功效任务中，TabPFN模型超越了专门的先进预测器。模型的预测IQR与真实预测误差呈负相关，基于最低平均IQR选择的模型集成比朴素集成或单一模型表现更优。

Conclusion: 模型不确定性是优化生物分子功效预测的强大、无需标签的启发式方法。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [53] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark是一个新型预训练框架，通过优化transformer注意力和MLP层，显著提升训练效率，实现2-6倍训练吞吐量提升和55%-83%能耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型训练时间长、能耗高的问题，现代模型需要数月计算时间和千兆瓦时电力消耗。

Method: 结合架构改进和算法增强，针对transformer注意力层和MLP层进行优化，最大化模型FLOPs利用率，同时保持与标准transformer实现的兼容性。

Result: 在3B和30B参数Llama模型上使用SlimPajama-627B数据集进行测试，在多节点H200 GPU集群上实现2-6倍训练吞吐量提升和55%-83%能耗降低。

Conclusion: 这些优化是模型和硬件无关的，可广泛应用于transformer架构，并扩展到监督微调和直接偏好优化等后训练阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [54] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 提出了Action-Controllable Factorization (ACF)，一种对比学习方法，用于从高维观察中发现可独立控制的潜在变量，解决了传统方法需要预先知道分解表示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于分解MDP的方法需要预先知道分解表示，而深度强化学习虽然能处理高维输入但无法利用分解结构。需要一种能从高维观察中发现可控因素的方法。

Method: 使用对比学习方法ACF，利用动作稀疏性（每个动作通常只影响状态变量的子集）来发现可独立控制的潜在变量。

Result: 在三个已知分解结构的基准测试（Taxi、FourRooms、MiniGrid-DoorKey）中，ACF直接从像素观察中恢复了真实可控因素，始终优于基线解纠缠算法。

Conclusion: ACF成功解决了从高维观察中发现可控因素的表征问题，为强化学习中的表示学习提供了有效方法。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [55] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 本文提出了一个通用模板，将随机学习算法适配到随机顺序模型中，同时保持其遗憾保证，并应用于延迟预测、带约束的在线学习和带切换成本的赌博机等问题。


<details>
  <summary>Details</summary>
Motivation: 随机顺序模型中的输入序列虽然渐近等价于独立同分布序列，但在有限时间内可能表现出显著的非平稳性，这会阻碍随机学习算法的性能。

Method: 提出了一个通用模板来适配随机学习算法到随机顺序模型，通过技术手段确保算法在该模型下的性能。

Result: 恢复了延迟预测、带约束在线学习和带切换成本赌博机的改进遗憾界，并证明在随机顺序模型中可学习性由VC维度而非Littlestone维度刻画。

Conclusion: 该模板成功地将随机学习算法扩展到随机顺序模型，提供了与对抗模型的重要分离，并获得了改进的理论保证。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [56] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度强化学习(DRL)和有界极值搜索(ES)的混合控制器，用于提高非线性时变系统的鲁棒性控制。


<details>
  <summary>Details</summary>
Motivation: DRL虽然能从大数据集中学习快速控制多参数系统，但在系统模型快速变化时性能会急剧下降；而有界ES能处理时变系统和未知控制方向，但收敛速度随参数增加而减慢且易陷入局部最小值。

Method: 将DRL和有界ES结合形成混合控制器：DRL利用历史数据快速学习控制多参数系统到期望设定点，有界ES确保对时变性的鲁棒性。

Result: 混合控制器的性能超过了单独使用DRL或ES的效果，在数值研究和实际粒子加速器自动调谐应用中表现出色。

Conclusion: DRL和有界ES的结合能够优势互补，既保持了DRL的快速学习能力，又增强了系统对时变性的鲁棒性。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [57] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文揭示了监督微调(SFT)与逆强化学习之间的基本等价关系，证明SFT不仅是策略模仿过程，还学习了一个隐式的密集词级奖励模型，并提出了从SFT模型中恢复密集奖励信号的方法。


<details>
  <summary>Details</summary>
Motivation: 传统上将SFT视为简单的模仿学习过程，但作者挑战这一观点，旨在证明SFT实际上是一种强大的奖励学习机制。

Method: 通过建立SFT与逆Q学习的等价关系，提出基线相对奖励函数来从SFT模型中恢复密集奖励信号，并开发了Dense-Path REINFORCE方法。

Result: Dense-Path REINFORCE方法在指令跟随基准测试中持续优于原始SFT模型。

Conclusion: SFT不仅是策略模仿，更是一种强大的奖励学习机制，为利用专家演示开辟了新可能性。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [58] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出了一种残差学习框架，通过在多块交叉阵列上顺序学习来补偿低精度权重更新的残差误差，实现在有限状态设备上的片上训练


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算加速器使用电阻交叉阵列直接在内存中进行深度神经网络计算，但有效的内存内训练通常需要至少8位电导状态才能匹配数字基线。许多有前景的忆阻器件如ReRAM由于制造限制仅提供约4位分辨率，这种有限的更新精度会显著降低训练精度

Method: 提出残差学习框架，在多块交叉阵列片上顺序学习以补偿低精度权重更新的残差误差。理论分析表明最优性差距随阵列数量增加而缩小，并实现线性收敛速率

Result: 在标准图像分类基准测试中，该方法在有限状态设置下持续优于最先进的模拟内存训练策略，成本分析确认仅产生适度的硬件开销

Conclusion: 残差学习框架能够有效解决有限状态忆阻器件的训练精度问题，为片上训练提供了可行的解决方案

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [59] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: 提出VGB算法，通过概率回溯机制改进测试时采样，提高对验证器错误的鲁棒性，在语言建模任务中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有测试时算法中，验证器的微小错误会导致生成过程中的灾难性失败，需要更鲁棒的解码策略来应对验证器错误

Method: VGB算法将自回归生成视为部分生成树上的随机游走，结合验证器和基础模型的指导，采用概率回溯机制，基于Sinclair-Jerrum随机游走理论

Result: 在合成和真实语言建模任务中，VGB在多种指标上优于基线方法

Conclusion: VGB通过理论基础的随机游走和回溯机制，有效提高了对验证器错误的鲁棒性，为过程指导的测试时采样提供了新思路

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [60] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的图生成框架SFMG，通过谱特征映射将图嵌入到黎曼流形中，利用测地流匹配来生成图，在保持性能的同时实现了30倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法通常只对齐图的谱或度分布，但忽略了特征向量诱导的几何结构和图的全局结构。

Method: 使用谱特征映射将输入图和目标图嵌入到连续黎曼流形中，定义嵌入之间的测地流，并沿着这些流匹配分布来生成输出图。

Result: 在多种基准测试中，SFMG在图元、度和谱指标上与最先进方法性能相当，同时比基于扩散的模型快30倍，并能泛化到未见过的图规模。

Conclusion: SFMG通过将谱几何与流匹配相结合，为图合成提供了一种新方法，具有更好的几何结构捕捉能力、灵活性和可扩展性。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [61] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 提出基于IATC的模型-大脑比较方法，通过跨主体映射函数评估神经网络模型作为大脑机制模型的准确性，解决了模型预测性与机制识别之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏共识的方法来比较模型激活与大脑响应，需要建立原则性的模型-大脑比较框架。

Method: 使用IATC作为最严格的函数集，在模型响应与大脑数据之间进行双向映射，评估模型能否像典型主体一样表现。

Result: IATC能够解析神经机制的细节特征（如非线性激活函数），实现神经活动的高精度预测，同时在不同脑区响应模式间实现高特异性分离。

Conclusion: IATC证明模型预测性与机制识别之间不存在固有权衡，为深度神经网络作为视觉系统模型提供了新证据，改进了模型-大脑比较方法。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [62] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 提出AttentiveGRUAE模型，基于注意力机制的GRU自编码器，用于纵向可穿戴数据的时序聚类和结果预测，在抑郁分类和聚类质量上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时学习行为特征表示、预测抑郁结果和识别行为亚型的统一模型，以从纵向可穿戴数据中获得临床可解释的见解。

Method: 使用注意力GRU自编码器联合优化三个目标：序列重构学习潜在表示、二元分类预测抑郁率、GMM软聚类识别行为亚型。

Result: 在372名参与者数据上，聚类质量（轮廓分数0.70 vs 0.32-0.70）和抑郁分类（AUC 0.74 vs 0.50-0.67）均优于基线；在332名参与者外部验证中保持稳定（轮廓分数0.63，AUC 0.61）。

Conclusion: AttentiveGRUAE在纵向睡眠数据分析中表现出色，提供了临床可解释的亚型分析和时间注意力可视化，有助于理解睡眠规律变化与抑郁风险的关系。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [63] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: 提出HOD-GNN方法，通过利用高阶节点导数来增强图神经网络表达能力，该方法在理论表达能力和实际性能上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 图神经网络表达能力有限，现有研究主要关注架构设计，而节点导数虽然在其他领域有应用，但尚未被用于增强GNN表达能力。

Method: 引入高阶导数GNN(HOD-GNN)，利用基础模型的高阶节点导数生成结构感知的节点嵌入，通过第二个GNN进行端到端训练。

Result: 理论证明HOD-GNN的表达能力与WL层次结构对齐，在流行图学习基准测试中表现出强大性能。

Conclusion: 节点导数为增强GNN表达能力提供了自然途径，HOD-GNN在表达能力和计算效率之间取得了良好平衡。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [64] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文综述了地理空间机器学习(GeoML)库的发展现状，分析了核心功能、生态系统，介绍了主要库的特点，并通过作物类型映射案例展示实际应用。


<details>
  <summary>Details</summary>
Motivation: 地理空间机器学习的发展滞后于地球观测数据的可用性，需要专门库来处理空间分辨率、光谱特性、时间序列等独特挑战。

Method: 通过分析GeoML库的演进、架构、支持数据类型和ML框架集成，讨论数据预处理、时空连接、基准测试等方法论。

Result: 介绍了TorchGeo、eo-learn、Raster Vision等流行GeoML库，展示了在作物类型映射中的实际应用效果。

Conclusion: 为从业者、开发者和研究人员提供了GeoML领域的导航指南，强调基础模型的兴起和开源地理空间软件治理的重要性。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [65] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: MINTO方法通过在目标网络和在线网络之间选择最小值作为目标估计，解决了传统目标网络更新慢和在线网络不稳定的问题，实现了更快更稳定的价值函数学习。


<details>
  <summary>Details</summary>
Motivation: 目标网络虽然稳定但学习速度慢，而使用在线网络作为目标虽然直观但会导致学习不稳定。研究者希望结合两者的优点，找到既能保持稳定又能快速学习的方法。

Method: 提出MINTO方法，使用目标网络和在线网络之间的最小值作为目标估计，通过这种简单有效的修改来缓解在线网络可能带来的高估偏差。

Result: 在在线和离线强化学习、离散和连续动作空间等多种基准测试中，MINTO都能持续提升性能表现。

Conclusion: MINTO方法能够无缝集成到多种基于价值和演员-评论家的算法中，以可忽略的成本实现更快更稳定的价值函数学习，具有广泛的适用性和有效性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [66] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [67] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: MINERVA是一种基于神经网络互信息估计的监督特征选择方法，通过两阶段过程解耦表示学习和特征选择，能有效捕捉高阶特征交互关系。


<details>
  <summary>Details</summary>
Motivation: 现有特征过滤器依赖统计成对依赖度量来建模特征-目标关系，但当目标依赖于高阶特征交互而非个体贡献时，这种方法会失效。

Method: 使用神经网络参数化互信息近似，通过精心设计的损失函数和稀疏诱导正则化器进行特征选择，采用两阶段过程分离表示学习和特征选择。

Result: 在合成和真实欺诈数据集上的实验结果表明，该方法能有效捕捉复杂特征-目标关系，并能获得精确解。

Conclusion: MINERVA方法能够有效处理传统方法难以捕捉的高阶特征交互问题，在特征选择任务中表现出色。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [68] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 提出了TabImpute，一个基于TabPFN的预训练transformer模型，用于表格数据的零样本缺失值填补，无需拟合或超参数调优。


<details>
  <summary>Details</summary>
Motivation: 表格数据中普遍存在缺失值问题，现有方法性能差异大且需要耗时的超参数调优，缺乏默认的填补方法。

Method: 基于TabPFN构建预训练transformer；引入逐条目特征化实现100倍加速；使用包含真实缺失模式的合成训练数据；建立MissBench评估基准。

Result: 在42个OpenML数据集和13种缺失模式下，TabImpute相比11种现有方法展现出稳健性能。

Conclusion: TabImpute提供准确、快速的零样本缺失值填补，无需推理时的拟合或超参数调优。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [69] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: HyperAdaLoRA是一种基于超网络加速AdaLoRA收敛的参数高效微调方法，通过注意力机制动态生成SVD参数，实现动态秩分配和快速收敛。


<details>
  <summary>Details</summary>
Motivation: AdaLoRA虽然通过SVD和奇异值剪枝实现了动态秩分配，但在训练过程中存在收敛速度慢和计算开销高的问题，需要改进。

Method: 使用基于注意力机制的超网络动态生成SVD参数(P, Λ, Q)，通过剪枝超网络生成的奇异值输出来实现动态秩分配，避免直接优化SVD组件。

Result: 在多个数据集和模型上的实验表明，该方法在不牺牲性能的情况下实现了更快的收敛速度，并在其他基于LoRA的方法上验证了广泛适用性。

Conclusion: HyperAdaLoRA通过超网络机制有效解决了AdaLoRA的收敛问题，为参数高效微调提供了更高效的解决方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [70] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: 本文提出了CLEAR方法，首次对Transformer架构进行细粒度推理能耗分析，发现注意力块的能耗与FLOPs不成比例，为构建能效优化的Transformer模型提供了组件级基准。


<details>
  <summary>Details</summary>
Motivation: LLM推理在全球范围内持续进行，已成为AI能耗的主要来源，但现有研究缺乏细粒度测量方法，仅报告粗略的模型级指标，未能将能效作为主要目标。

Method: 提出CLEAR方法（组件级能耗评估重复采样），通过重复采样解决微秒级组件执行与毫秒级能耗监测的时间不匹配问题，评估了15个涵盖四种架构类型的模型。

Result: 注意力块每FLOP消耗显著更多能量，能耗与FLOP计数不成比例，CLEAR方法能将组件级能耗方差控制在9.5%以下，捕获模型总能耗90%以上。

Conclusion: FLOPs无法准确反映组件级真实能耗，研究建立了详细的组件级能耗基准，为通过组件级优化构建能效Transformer模型提供了初步指导。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [71] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 提出了一种基于对抗自编码器的无监督深度学习框架，用于优化桥梁健康监测中的检测车辆设计，通过最小化健康与损伤状态间的Wasserstein距离来提升损伤检测灵敏度。


<details>
  <summary>Details</summary>
Motivation: 现有基于车辆-桥梁耦合响应的桥梁健康监测方法受车辆机械和动态特性影响较大，限制了检测效果，需要优化检测车辆设计来提升损伤敏感性。

Method: 使用对抗自编码器重构加速度响应的频域表示，通过最小化健康与损伤状态间Wasserstein距离来优化车辆轮胎悬挂系统的质量和刚度参数，采用Kriging元模型高效近似目标函数。

Result: 研究发现频率比在0.3-0.7之间的车辆检测效果最佳，接近共振的车辆性能较差；较轻车辆需要较低固有频率以获得最优检测效果。

Conclusion: 这是首个系统优化驱动式传感平台的研究，提出了专用检测车辆的设计方案，显著提升了桥梁损伤检测的灵敏度。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [72] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: TutorBench是一个专门评估LLMs辅导能力的基准数据集，包含1,490个专家标注的样本，涵盖解释生成、反馈提供和提示生成三个核心辅导任务，并采用样本特定评分标准进行自动评估。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地使用LLMs作为学习辅助工具，需要开发能够识别学生核心需求、具有适应性、提供个性化指导且准确可靠的AI辅导模型。

Method: 构建了包含高中和AP课程内容的1,490个样本数据集，涵盖三个辅导任务：自适应解释生成、作业反馈提供和主动学习提示生成。采用基于样本特定评分标准的LLM自动评估方法。

Result: 评估了16个前沿LLMs，结果显示所有模型得分均低于56%，在辅导技能相关标准上的通过率低于60%。Claude模型在支持主动学习方面表现最佳，但在其他任务上落后。

Conclusion: 当前前沿LLMs在展示完整辅导技能方面存在不足，TutorBench为开发下一代AI辅导系统提供了全面且未饱和的基准测试平台。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [73] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 本文证明对于一类置换等变学习规则（包括SGD、Adam等），训练过程在神经元间诱导双Lipschitz映射，强烈约束神经元分布的拓扑结构。学习率η存在拓扑临界点η*：低于η*时训练保持所有拓扑结构，高于η*时允许拓扑简化，降低模型表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究神经元网络在梯度下降学习过程中的拓扑结构变化，揭示学习率对神经元分布拓扑约束的影响，理解深度学习训练动态的拓扑机制。

Method: 通过数学分析证明置换等变学习规则诱导神经元间的双Lipschitz映射，建立学习率与拓扑约束的定量关系，结合边缘稳定性现象分析训练动态。

Result: 发现学习率存在拓扑临界点η*，区分两种训练模式：保守拓扑保持模式和拓扑简化模式。训练动态分为两个阶段：平滑优化阶段和拓扑简化阶段。

Conclusion: 该理论独立于具体架构或损失函数，为深度学习研究提供了通用的拓扑分析方法，揭示了学习率对模型表达能力的根本影响。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [74] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数集中现象的低精度浮点格式ECF8，通过理论分析和实验验证，在保持无损计算的同时实现了显著的内存节省和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型参数规模达到数千亿，低精度计算变得不可或缺。传统方法存在数值稳定性、内存占用和硬件效率等问题，需要开发无需反量化的低精度浮点格式。

Method: 通过理论和实证研究揭示GenAI权重中的指数集中现象，证明其源于随机梯度下降诱导的α-稳定分布。基于此设计ECF8格式，包含熵感知编码和GPU优化解码的无损压缩框架。

Result: 在高达671B参数的LLM和DiT模型上测试，实现了26.9%的内存节省和177.1%的吞吐量加速，同时保持模型输出的完全无损。

Conclusion: 指数集中现象是训练模型的一个统计规律，为FP8时代的无损低精度浮点设计提供了原则性路径。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [75] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 该论文对神经算子进行分类，提出解释其预测机制的方法，展示其在学习物理规律方面的能力，并指出需要开发更通用的解释方法和结合物理原理的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在函数空间映射学习方面表现出色，但其学习机制的理解仍不足。本文旨在深入理解神经算子的工作机制，特别是在学习物理规律方面的能力。

Method: 将神经算子分为两类：空间域模型（在网格上学习）和函数域模型（在函数基上学习）。提出了解释神经算子预测过程的方法，并构建了双空间多尺度模型。

Result: 研究表明神经算子能够从数据中学习隐藏的物理模式，简单的双空间多尺度模型可以达到最先进的性能。但解释方法仅限于特定情况。

Conclusion: 需要开发通用的解释方法，双空间多尺度模型在学习复杂物理方面具有巨大潜力，需要建立将已知物理原理融入神经算子的原则性框架以改善泛化能力。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [76] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak是一个将遗传编程与大型语言模型相结合的新框架，旨在提高启发式演化的效率、透明度和适应性，通过知识提取、热启动种群生成和自然语言解释来解决复杂启发式的可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 在动态和大规模场景中，最有效的启发式往往高度复杂，这阻碍了可解释性、减缓了收敛速度，并限制了跨任务的可迁移性。

Method: EvoSpeak从高质量GP启发式中学习并提取知识，利用这些知识：(i)生成热启动种群以加速收敛，(ii)将不透明的GP树转化为简洁的自然语言解释，(iii)实现跨相关任务的知识迁移和偏好感知启发式生成。

Result: 在动态柔性作业车间调度问题上的实验表明，EvoSpeak能产生更有效的启发式，提高演化效率，并提供增强可用性的人类可读报告。

Conclusion: 通过将GP的符号推理能力与LLMs的解释和生成优势相结合，EvoSpeak推动了现实世界优化问题中智能、透明和用户对齐的启发式的发展。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [77] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 该论文提出了GRAFT框架，将基于拒绝采样的微调方法统一为GRAFT，并证明其隐含地执行PPO算法。进一步提出P-GRAFT在中间噪声级别进行分布整形，以及无奖励的逆噪声校正方法，在多个生成任务上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型虽然能有效捕捉训练数据分布，但需要根据下游应用需求通过奖励函数来调整分布。传统策略梯度方法在扩散模型中由于边缘似然难以计算而受限，需要新的解决方案。

Method: 提出GRAFT框架统一基于拒绝采样的微调方法，证明其隐含执行PPO；引入P-GRAFT在中间噪声级别进行分布整形；提出逆噪声校正方法改进流模型。

Result: 在文本到图像生成、布局生成、分子生成和无条件图像生成等任务上验证有效性。Stable Diffusion 2在T2I基准测试中VQAScore优于策略梯度方法，相对基础模型提升8.81%；无条件图像生成中FID改进且FLOPs/图像更低。

Conclusion: GRAFT框架为扩散模型提供有效的分布整形方法，P-GRAFT通过中间噪声级别整形实现更有效的微调，逆噪声校正可在无显式奖励情况下改进流模型性能。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [78] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出了RAMAC框架，将表达性生成actor与分布critic结合，在离线强化学习中实现风险敏感学习，在保持高回报的同时降低尾部风险。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，离线强化学习需要提供高回报且避免灾难性尾部风险。现有风险规避方法过于保守且限制策略类别，而表达性策略仅在风险中性设置中使用。

Method: RAMAC框架耦合表达性生成actor与分布critic，通过生成路径区分组合目标（分布风险和BC损失），在复杂多模态场景中实现风险敏感学习。

Result: 在Stochastic-D4RL任务上，RAMAC使用扩散和流匹配actor，在保持强回报的同时，在大多数任务上实现了CVaR0.1的持续提升。

Conclusion: RAMAC框架成功解决了离线强化学习中风险敏感性与策略表达性之间的权衡问题，为安全关键应用提供了有效的解决方案。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [79] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 提出TSLT-Net，一种基于时空Transformer的轻量级入侵检测系统，专门用于无人机网络，在ISOT数据集上达到99.99%多分类准确率和100%异常检测准确率。


<details>
  <summary>Details</summary>
Motivation: 无人机在商业、工业和民用领域的广泛应用带来了网络安全挑战，现有入侵检测机制缺乏对动态资源受限环境的适应性和泛化能力。

Method: 利用自注意力机制建模网络流量的时间模式和空间依赖关系，包含简化的预处理流程，支持多类攻击分类和二元异常检测的单一架构。

Result: 在包含230万条标记记录的ISOT无人机异常检测数据集上，TSLT-Net达到99.99%多分类准确率和100%二元异常检测准确率，内存占用仅0.04MB，可训练参数9722个。

Conclusion: TSLT-Net是无人机网络实时网络安全的高效可扩展解决方案，特别适合部署在任务关键型无人机系统的边缘设备上。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [80] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: 提出CST-AFNet，一种基于双重注意力的深度学习框架，用于物联网网络中的鲁棒入侵检测，在Edge-IIoTset数据集上达到99.97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了复杂的网络安全挑战，需要针对异构、资源受限和分布式环境设计专门的入侵检测方案。

Method: 结合多尺度CNN进行空间特征提取，双向GRU捕获时间依赖，以及通道和时间双重注意力机制来增强对关键模式的关注。

Result: 在包含15种攻击类型和良性流量的Edge-IIoTset数据集上，模型达到99.97%的准确率，宏平均精确率、召回率和F1分数均超过99.3%。

Conclusion: CST-AFNet是复杂物联网和工业物联网环境中实时网络威胁检测的强大可扩展解决方案。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [81] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 论文提出了一种分析超参数损失曲面渐近结构的新理论和方法，发现了随机搜索在最优解附近的新分布规律，并开发了相应的分析工具。


<details>
  <summary>Details</summary>
Motivation: 超参数对模型能力影响巨大，但现代模型规模过大无法进行广泛搜索。现有工具很少能理解超参数损失曲面结构，需要新的理论工具来分析超参数优化。

Method: 开发基于随机搜索的新技术来揭示损失曲面的渐近结构，发现随机搜索最优分数的新分布，其参数正好定义了渐近区域损失曲面的特征。

Result: 发现了损失曲面在接近最优解时的简单结构特征，如有效维度和最佳可能损失，并推导出随机搜索的渐近定律，能够解释和推断其收敛性。

Conclusion: 这些新工具能够进行新的分析，如最佳性能的置信区间或确定有效超参数数量，相关工具已在GitHub上开源提供。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [82] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文提出了一种深度时间序列预测的性能上限估计方法，发现了最小预测误差与窗口级序列模式复杂度之间的指数关系（准确率定律），用于识别饱和任务和指导大模型训练。


<details>
  <summary>Details</summary>
Motivation: 由于时间序列预测具有部分可观测和不确定性的本质，存在非零误差下界。为了明确研究目标和避免在饱和任务上浪费精力，需要估计深度时间序列预测的性能上限。

Method: 通过对2800多个新训练的深度预测器进行严格的统计测试，分析窗口级序列属性与预测性能的关系，发现了准确率定律。

Result: 发现了最小预测误差与窗口级序列模式复杂度之间存在显著的指数关系，该准确率定律能有效识别广泛使用的基准测试中的饱和任务。

Conclusion: 提出的准确率定律为深度时间序列预测提供了性能上限估计方法，能够指导大时间序列模型的训练策略，为未来研究提供有价值的见解。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [83] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何布朗运动（GBM）和Dale定律的生物启发生成模型，使用乘法更新规则进行图像生成，在MNIST等数据集上展示了生成能力。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降与生物学习机制不一致，受Dale定律启发（抑制性和兴奋性突触在学习过程中不交换角色），开发生物启发的学习技术。

Method: 从几何布朗运动的随机微分方程出发，离散化反向时间SDE得到乘法更新规则，提出乘法去噪分数匹配形式化方法，训练基于分数的图像生成模型。

Result: 在MNIST、Fashion MNIST和Kuzushiji数据集上展示了新方案的生成能力，生成了从对数正态密度开始的样本。

Conclusion: 这是首个基于几何布朗运动、采用乘法更新的生物启发生成模型，为生物启发学习技术开辟了新途径。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [84] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 提出了一种新颖的鲁棒属性图聚类方法RAGC，通过混合协作增强和对比样本自适应差分感知来解决现有方法在边级嵌入增强和对比样本处理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对比属性图聚类方法主要依赖边作为辅助信息获取节点级嵌入表示，仅关注节点级嵌入增强，忽视了边级嵌入增强以及不同粒度下节点级和边级嵌入增强的交互作用。同时，它们通常平等对待所有对比样本对，忽略了难易正负样本对之间的显著差异，限制了判别能力。

Method: 提出RAGC方法，包含混合协作增强和对比样本自适应差分感知两个模块。HCA同时执行节点级和边级嵌入表示与增强，建立更全面的相似性度量标准；CSADA利用高置信度伪标签信息自适应识别所有对比样本对，并通过创新的权重调制函数差异化处理。两个模块在良性循环中相互增强。

Result: 在六个基准数据集上的综合图聚类评估表明，所提出的RAGC方法相比几种最先进的CAGC方法具有更好的效果。

Conclusion: RAGC通过混合协作增强和对比样本自适应差分感知策略，有效提升了表示学习的判别能力，在属性图聚类任务中表现出优越性能。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [85] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow是一个创新的LLM服务系统，通过抢占式请求调度和主动KV缓存管理，显著提升流式文本生成的响应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统存在非抢占式请求调度和被动内存管理的僵化问题，导致资源利用率低、请求处理并行度差，特别是在请求突发时表现不佳。

Method: 采用动态优先级调度机制，基于实时令牌缓冲区占用率和令牌消耗率来调度请求；同时主动在GPU和CPU内存间传输KV缓存，并通过I/O与计算重叠来最小化抢占开销。

Result: 在Llama3-8B和Qwen2.5-32B模型上的多GPU测试显示，TokenFlow实现了82.5%的有效吞吐量提升，P99 TTFT降低80.2%，且不影响整体令牌吞吐量。

Conclusion: TokenFlow通过创新的调度和缓存管理策略，显著提升了LLM流式服务的性能，为实时交互应用提供了更优的解决方案。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [86] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 提出自监督机器学习框架SIT-FUSE，通过融合多传感器卫星数据和太阳诱导荧光数据，无需标记数据集即可检测和绘制有害藻华严重程度和物种分类。


<details>
  <summary>Details</summary>
Motivation: 解决在标签稀缺环境下进行有害藻华监测的挑战，推进自监督学习在全球水生生物地球化学中的操作化应用。

Method: 采用自监督表示学习和分层深度聚类，融合VIIRS、MODIS、Sentinel-3、PACE等卫星反射率数据与TROPOMI太阳诱导荧光数据。

Result: 在墨西哥湾和南加州验证显示，与总浮游植物、Karenia brevis、Alexandrium spp.和Pseudo-nitzschia spp.测量结果高度一致。

Conclusion: 该框架推进了标签稀缺环境下的可扩展有害藻华监测，并通过分层嵌入实现探索性分析，是自监督学习在水生生物地球化学领域操作化的重要一步。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [87] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [88] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 该研究评估了模型消融技术对LLM安全性的影响，发现在推理时通过简单的激活编辑可以移除拒绝敏感方向，测试了不同安全预训练检查点的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究常见的LLM安全干预措施（如拒绝训练或元标签训练）是否能在推理时的激活编辑下保持有效，评估模型消融技术对安全性的影响。

Method: 使用模型消融技术移除拒绝敏感方向，在SmolLM2-1.7B的20个系统上进行控制评估，通过100个平衡的有害和无害提示测试，使用多个评判者分类响应为拒绝或非拒绝。

Result: 研究产生了检查点级别的特征描述，确定了哪些数据中心安全组件在消融后保持鲁棒，量化了评判者选择对评估结果的影响。

Conclusion: 研究为将推理时编辑整合到安全评估中提供了一个实用协议，揭示了安全预训练在不同检查点对激活编辑的鲁棒性差异。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [89] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: 本文证明了深度ReLU网络在梯度下降训练下可实现最优泛化率，解决了现有方法中要么获得次优O(1/√n)率，要么对网络深度有指数依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降在深度神经网络中是否能达到核方法中建立的最小最大最优泛化率，现有结果要么是次优的，要么只适用于平滑激活函数且对网络深度有指数依赖。

Method: 通过仔细权衡优化误差和泛化误差，在数据满足NTK可分性假设下，使用梯度下降训练深度ReLU网络，并开发了新的技术来控制参考模型附近的激活模式。

Result: 证明了过风险率为~O(L⁴(1+γL²)/(nγ²))，这与最优SVM型率~O(1/(nγ²))在深度相关因子内对齐，仅对深度有多项式依赖。

Conclusion: 本文建立了深度ReLU网络在梯度下降下的最优泛化率，关键技术创新是对深度ReLU网络在梯度下降训练中激活模式的更精细控制，实现了更尖锐的Rademacher复杂度界限。

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [90] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub是一个社区平台，旨在统一黑盒优化方法、基准测试和Python API，促进跨领域研究协作。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化研究在不同领域（如AutoML和材料信息学）中分散，缺乏统一平台来整合方法和基准测试。

Method: 提供统一的Python API、贡献者包注册表和Web界面，通过开源代码库实现平台功能。

Result: 建立了集中化的社区平台，提高了黑盒优化方法的可搜索性和跨领域研究协作。

Conclusion: OptunaHub通过促进贡献和应用的良性循环，有望推动黑盒优化领域的统一发展。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [91] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文提出了一种改进的在线共形预测方法，通过使用更广泛的函数类别来量化预测区间的相关性，而不是简单的二元评估，从而产生更窄的预测区间同时保持覆盖有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的在线共形预测方法在阈值更新步骤中只关注预测区间的有效性（真实值是否在区间内），而忽略了区间的相关性。本文旨在利用这一被忽视的方面来改进方法。

Method: 提出在阈值更新步骤中用更广泛的函数类别替换二元评估（内部/外部），这些函数使用真实值来量化预测区间的相关性，有助于防止阈值突变，可能产生更窄的预测区间。

Result: 在真实世界数据集上的实验结果表明，这些函数可以产生比现有OCP方法更紧的区间，同时保持覆盖有效性。

Conclusion: 通过量化预测区间相关性的新方法，能够在不牺牲覆盖有效性的前提下获得更窄的预测区间，改进了在线共形预测的性能。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [92] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出分层Group DRO方法，解决传统方法对组内分布偏移的脆弱性问题，在包含少数群体分布偏移的新基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法容易受到伪相关性的影响，特别是在测试数据分布偏移时。现有方法如Group DRO虽然对组间偏移具有鲁棒性，但对组内分布偏移（尤其在样本有限的少数群体中）仍然脆弱。

Method: 提出分层扩展的Group DRO方法，同时处理组间和组内不确定性，为多层次的分布偏移提供鲁棒性。

Result: 在新设计的模拟现实少数群体分布偏移的基准测试中，该方法表现出强大的鲁棒性，而现有鲁棒学习方法均失败；同时在标准基准测试中也取得更优性能。

Conclusion: 结果表明，扩大模糊集以更好地捕捉组间和组内分布不确定性非常重要，这为解决伪相关性研究中的重要挑战提供了新思路。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [93] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: FlexiQ是一种自适应混合精度量化方案，通过选择性应用低比特计算到小值范围的特征通道，并实时调整低比特通道比例，有效管理推理工作负载波动。


<details>
  <summary>Details</summary>
Motivation: 神经网络加速器成本高昂且难以实时扩展资源来处理工作负载波动，需要一种能够自适应调整计算精度的量化方法。

Method: 采用选择性低比特计算策略，对值范围小的特征通道应用低比特计算，使用高效的比特降低方法最小化量化误差，并实时调整低比特通道比例。

Result: 在11个卷积和基于transformer的视觉模型上，FlexiQ的4位模型平均精度提高6.6%，优于四种最先进的量化技术。50%4位模型仅损失0.6%精度，达到100%4位模型相对于8位模型40%的加速效果。

Conclusion: FlexiQ在NPU和GPU上引入最小运行时开销，证明了其硬件效率和整体性能优势，能够有效平衡精度与延迟的权衡。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [94] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出一种在训练过程中对状态空间模型进行降维的方法，利用Hankel奇异值分析识别并保留高影响力维度，在保持表达能力的同时显著加速优化。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型在处理长序列任务时面临表达能力和计算负担的平衡问题，需要找到既能最大化表达能力又能限制计算成本的方法。

Method: 基于控制理论中的Hankel奇异值分析，在训练过程中识别高能量状态维度并进行平衡截断，适用于线性时不变SSM和选择性模型。

Result: 实验表明，训练过程中的降维显著加速了优化过程，压缩后的模型保留了任务关键结构，性能优于直接训练的小维度模型。

Conclusion: 从大模型开始并在训练过程中缩小的SSM能够在保持计算效率的同时获得更高的性能表现。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [95] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 该论文将视觉自回归模型重新定义为迭代优化框架，将其视为构建拉普拉斯风格潜在金字塔的前向过程和从小步数粗到细重建的后向过程，连接了VAR与去噪扩散模型，并分析了三个关键设计选择对效率和保真度的影响。


<details>
  <summary>Details</summary>
Motivation: 重新审视视觉自回归模型，将其置于迭代优化框架下，以更好地理解其效率和保真度的来源，并建立与去噪扩散模型的联系。

Method: 将VAR形式化为确定性前向过程（构建潜在金字塔）和学习的后向过程（粗到细重建），通过控制实验量化三个设计因素：学习潜在空间中的细化、将预测视为代码索引的离散分类、按空间频率划分任务。

Result: 识别了三个关键设计选择对VAR效率和保真度的贡献，并展示了该框架可扩展到置换不变图生成和概率性中期天气预报。

Conclusion: 提出的框架为VAR提供了实用接口，使其能够利用扩散生态系统工具，同时保持少步、尺度并行的生成能力。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [96] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出了SASL框架，一种可解释的个性化健康预测方法，结合线性回归和主题特定交互，通过稀疏建模和LightGBM混合提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和梯度提升模型在预测健康结果时牺牲了可解释性，且未能充分处理个体间差异。

Method: 使用普通最小二乘回归与主题特定交互，通过迭代后向特征消除构建稀疏模型，并采用回归-阈值方法优化序数目标的F1分数，结合LightGBM进行置信度门控。

Result: 在CH-2025数据集上，SASL-LightGBM混合框架达到与黑盒方法相当的预测性能，但参数更少、透明度更高。

Conclusion: SASL框架在保持可解释性的同时实现了良好的预测性能，为临床实践提供了清晰可行的见解。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [97] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: 提出Karma模型，一种结合知识引导和频率自适应学习的电池健康预测方法，通过双流深度学习架构分别捕捉长期低频退化趋势和短期高频动态，显著提升电池容量估计和剩余使用寿命预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型虽然能捕捉时间退化特征，但缺乏知识引导，导致长期健康预测不可靠。电池退化行为复杂，存在非线性、噪声、容量再生等现象，需要更准确和鲁棒的预测方法。

Method: 使用信号分解获得不同频段的电池信号，开发双流深度学习架构：一个流捕捉长期低频退化趋势，另一个流建模高频短期动态。通过粒子滤波器优化知识模型参数，确保物理一致性和可靠预测。

Result: 在两个主流数据集上，Karma相比最先进算法平均误差分别降低50.6%和32.6%，展现出优越的鲁棒性、泛化性和预测性能。

Conclusion: Karma模型通过知识引导和频率自适应学习，为电池健康预测提供了更可靠、物理一致的解决方案，具有在多样化应用中实现更安全、可靠电池管理的潜力。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [98] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: 提出了RoiRL方法，一种轻量级的离线强化学习替代方案，用于改进大型语言模型的推理能力，无需真实奖励标签，训练速度比现有方法快2.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要真实奖励，而测试时强化学习(TTRL)虽然使用多数投票奖励但计算成本高，需要在线RL和参考模型。

Method: RoiRL通过优化加权对数似然目标，无需维护参考模型，采用离线迭代强化学习，显著降低内存和计算需求。

Result: 实验结果显示RoiRL训练速度比TTRL快2.5倍，在推理基准测试中表现更优。

Conclusion: RoiRL为无需标签的自改进LLMs提供了一条可扩展的路径，实现了更稳定高效的训练。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [99] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: DMark是首个专门为扩散大语言模型设计的数字水印框架，解决了传统水印方法在非自回归解码模型上的失效问题，通过三种策略实现高检测率和文本质量保持。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型提供比自回归模型更快的生成速度且质量相当，但现有水印方法因其非顺序解码特性而失效，需要专门设计的水印方案。

Method: 提出三种互补策略：预测水印（使用模型预测的令牌）、双向水印（利用扩散解码特有的前后依赖关系）、预测-双向水印（结合前两种方法最大化检测强度）。

Result: 在多个dLLM上的实验显示，DMark在1%误报率下达到92.0-99.5%的检测率，而现有方法的简单适应只能达到49.6-71.2%，同时保持文本质量。

Conclusion: DMark证明了在非自回归语言模型中实现有效水印是可行的，并对文本操作具有鲁棒性。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [100] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出了Cell-MNN模型，一种端到端的编码器-解码器架构，通过学习局部线性化的ODE来建模细胞分化动力学，解决了现有方法计算成本高、缺乏基因交互解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 细胞分化动力学建模对理解癌症等疾病至关重要，但现有方法依赖计算昂贵的预处理和多阶段训练，且无法发现明确的基因交互作用。

Method: 采用编码器-解码器架构，潜在表示是局部线性化的ODE，控制从干细胞到组织细胞的演化动力学。模型完全端到端（除标准PCA预处理外），ODE表示明确学习生物学一致且可解释的基因交互。

Result: 在单细胞基准测试中达到竞争性性能，在扩展到更大数据集和跨多个数据集联合训练方面超越现有最先进基线，同时学习到可解释的基因交互，并通过TRRUST数据库验证。

Conclusion: Cell-MNN为细胞分化动力学建模提供了计算高效、可扩展且生物学可解释的解决方案，在性能和可解释性方面均优于现有方法。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [101] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: FeDABoost是一个新颖的联邦学习框架，通过动态提升机制和自适应梯度聚合策略，在非IID设置下提高性能并增强公平性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布数据设置下性能不佳和公平性差的问题，特别是提升表现不佳客户端的训练效果。

Method: 结合多类AdaBoost算法的权重机制，在聚合时给本地错误率低的客户端分配更高权重；同时动态调整焦点损失参数来提升表现不佳客户端的训练，强调难分类样本。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上的实验表明，FeDABoost相比FedAvg和Ditto方法，在公平性方面有显著提升，同时保持有竞争力的性能。

Conclusion: FeDABoost框架通过自适应聚合和动态提升机制，有效改善了联邦学习在非IID设置下的公平性和性能表现。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [102] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 提出了一种基于检索的随机稀疏采样方法，用于处理可变长度的医疗时间序列分类问题，该方法通过通道内相似性加权窗口预测，在概率空间聚合结果，提高了可解释性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列分析面临数据稀疏、噪声和长度变化大的挑战，现有方法中随机稀疏采样能处理变长信号，检索增强方法能提高可解释性和对噪声的鲁棒性。

Method: 将随机稀疏采样框架推广到检索增强分类，通过通道内相似性加权窗口预测，在概率空间聚合结果，生成凸的序列级分数和明确的证据追踪。

Result: 在四个医疗中心收集的iEEG数据上评估，方法实现了有竞争力的分类性能，并为从业者提供了更高的透明度和可解释性。

Conclusion: 该方法展示了在可靠和可解释的临床变长时间序列分类方面的潜力。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [103] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文首次从风险感知决策的角度对持续强化学习进行理论分析，提出了与持续学习兼容的遍历风险度量新类别。


<details>
  <summary>Details</summary>
Motivation: 现有的持续强化学习研究主要关注风险中性决策，而忽略了风险感知决策在持续学习环境中的重要性。作者发现经典风险度量理论与持续设置不兼容，需要新的理论框架。

Method: 作者扩展了风险度量理论，引入了遍历风险度量这一新类别，使其与持续学习环境兼容，并通过案例研究和实证结果验证了该方法的有效性。

Result: 研究表明遍历风险度量在风险感知持续学习中具有直观吸引力和理论合理性，为持续强化学习提供了新的理论基础。

Conclusion: 本文为风险感知持续强化学习建立了首个正式理论框架，提出的遍历风险度量解决了经典风险度量理论与持续设置不兼容的问题，为这一新兴领域的发展奠定了基础。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [104] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow是一个新颖的上下文感知流匹配框架，通过整合局部组织结构和配体-受体通信模式来推断空间组学数据中的组织动态轨迹。


<details>
  <summary>Details</summary>
Motivation: 从纵向空间组学数据推断轨迹对于理解发育、再生、修复、疾病进展和治疗反应中的结构和功能组织变化动态至关重要。

Method: ContextFlow将局部组织组织和配体-受体通信模式整合到转移可能性矩阵中，正则化最优传输目标，从而生成统计一致且生物学有意义的轨迹。

Result: 在三个数据集上的评估显示，ContextFlow在推理准确性和生物学一致性方面持续优于最先进的流匹配方法。

Conclusion: ContextFlow是一个可推广的框架，用于从纵向空间组学数据建模时空动态，生成具有生物学意义的轨迹。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [105] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 提出了一个统一的框架，用于在无标签测试数据情况下评估模型在分布偏移下的泛化能力，包括数据集中心评估和模型中心评估两种场景。


<details>
  <summary>Details</summary>
Motivation: 在真实世界部署中，当标记测试数据不可用时，评估模型在分布偏移下的泛化能力至关重要。

Method: 利用模型预测的两个内在属性——置信度（反映预测确定性）和分散度（捕获预测类别的多样性），构建了基于置信度、分散度和混合指标的评估方法。

Result: 混合指标在数据集中心和模型中心评估设置中始终优于单方面指标，特别是预测矩阵的核范数在各种任务中提供稳健且准确的性能。

Conclusion: 这些发现为部署场景中的无监督模型评估提供了实用且可推广的基础。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [106] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 提出基于迁移学习的方法预测船舶轴功率，使用高频数据预训练模型，再用低频午报数据微调，相比仅用午报数据训练，在不同类型船舶上均显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 随着全球海运增长，能源优化对降低成本和提高运营效率至关重要。轴功率直接影响燃油消耗，但高质量传感器数据获取困难且成本高，午报数据成为可行替代方案。

Method: 采用迁移学习方法，先用一艘船的高频数据预训练模型，然后用其他船的低频每日午报数据进行微调。在姊妹船、相似船和不同船上进行测试。

Result: 实验显示，相比仅用午报数据训练的模型，平均绝对百分比误差在姊妹船上降低10.6%，在相似船上降低3.6%，在不同船上降低5.3%。

Conclusion: 迁移学习方法能有效利用高频数据提升低频午报数据的轴功率预测精度，在不同类型船舶上均取得显著改进。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [107] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: 提出BrainIB++框架，将信息瓶颈原理应用于图神经网络，从rs-fMRI数据中识别精神分裂症的信息性脑区子图，实现高精度诊断和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖特征工程引入人为偏差，深度学习模型缺乏可解释性，限制了精神疾病诊断模型的临床应用。

Method: 基于信息瓶颈原理的端到端图神经网络框架BrainIB++，在模型训练过程中自动识别信息性脑区子图。

Result: 在三个多队列精神分裂症数据集上优于九种现有方法，诊断准确率高且具有泛化能力，识别出的子图与已知临床生物标志物一致。

Conclusion: BrainIB++框架在保持高诊断性能的同时提供了可解释性，识别出视觉、感觉运动和高级认知功能网络的异常，具有临床实用价值。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [108] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出了一种离线逆强化学习的分布框架，联合建模奖励函数和回报分布的不确定性，通过最小化一阶随机支配违规来捕获专家行为的丰富结构。


<details>
  <summary>Details</summary>
Motivation: 传统IRL方法只能恢复确定性奖励估计或仅匹配期望回报，无法捕获专家行为中的丰富结构，特别是在学习奖励分布方面。

Method: 通过最小化一阶随机支配违规，将失真风险度量整合到策略学习中，从而恢复奖励分布和分布感知策略。

Result: 在合成基准、真实世界神经行为数据和MuJoCo控制任务上的实证结果表明，该方法恢复了表达性奖励表示并实现了最先进的模仿性能。

Conclusion: 该分布框架适用于行为分析和风险感知模仿学习，能够有效捕获专家行为的复杂结构。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [109] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了DMIS框架，用于在条件不精确监督下训练鲁棒的扩散模型，通过分解目标为生成和分类组件来处理噪声、模糊或不完整的标签问题。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型的训练通常依赖大规模数据集，这些数据集中不可避免地包含条件输入的不精确信息（如噪声、模糊或不完整的标签），导致条件不匹配和生成质量下降。

Method: 基于似然最大化推导出统一框架，将目标分解为生成组件（建模不精确标签分布）和分类组件（利用扩散分类器推断类后验概率），并通过优化的时间步采样策略提高效率。

Result: 在多种不精确监督形式下的广泛实验表明，DMIS能够持续生成高质量和类区分性的样本，涵盖图像生成、弱监督学习和噪声数据集压缩等任务。

Conclusion: DMIS是扩散模型中首个系统研究不精确监督问题的框架，能够有效处理条件输入中的不精确信息，提升生成质量。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [110] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 本文提出了首个在差分隐私条件下计算Wasserstein重心的方法，在合成数据、MNIST和大规模人口数据集上验证了其准确性与隐私保护的平衡。


<details>
  <summary>Details</summary>
Motivation: Wasserstein重心在机器学习、统计学和计算机图形学中有广泛应用，但实践中输入数据常来自敏感数据集，需要差分隐私保护。

Method: 开发了在差分隐私约束下计算Wasserstein重心的算法。

Result: 在合成数据、MNIST和大规模美国人口数据集上的实验表明，该方法能生成高质量的私有重心，具有良好的准确性与隐私权衡。

Conclusion: 这是首个在差分隐私框架下计算Wasserstein重心的可行方法，为处理敏感数据提供了隐私保护解决方案。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [111] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于平衡符号图的轻量级变压器式神经网络，用于区分癫痫患者和健康受试者的EEG信号分类。


<details>
  <summary>Details</summary>
Motivation: EEG信号样本具有固有的反相关性，可以通过符号图中的负边很好地建模。需要构建轻量且可解释的神经网络来区分癫痫患者和健康受试者。

Method: 通过展开平衡符号图上的谱去噪算法构建变压器式神经网络，利用Lanczos近似在映射的正图上高效实现理想低通滤波器，从数据中学习最优截止频率。

Result: 实验表明该方法在EEG信号分类上达到了与代表性深度学习方案相当的分类性能，同时使用了显著更少的参数。

Conclusion: 该方法提供了一种参数效率高的EEG信号分类解决方案，在保持性能的同时大幅减少了模型复杂度。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [112] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 提出了CHORD框架，通过设备-云协作实现个性化混合精度量化，在移动设备上部署序列推荐模型，无需本地重训练即可实现动态模型适应和加速推理。


<details>
  <summary>Details</summary>
Motivation: 移动设备能力提升使得在设备上直接部署重排序模型成为可能，但资源异构性需要模型压缩。现有量化方法忽略了设备特定用户兴趣，导致推荐准确性下降，而设备端微调又带来额外计算负担。

Method: CHORD框架在异构设备间分发随机初始化模型，通过云端的辅助超网络模块识别用户特定关键参数。进行多粒度参数敏感性分析，通过设备端混合精度量化实现动态模型适应，仅用每通道2位编码量化策略来最小化通信开销。

Result: 在三个真实世界数据集和两个流行骨干网络上的实验表明，CHORD在准确性、效率和适应性方面表现优异。

Conclusion: CHORD框架成功解决了设备端推荐模型部署中个性化与资源适应的平衡问题，通过设备-云协作实现了无需重训练的混合精度量化部署。

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [113] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: 开发了贝叶斯E(3)等变机器学习势函数，通过联合能量-力负对数似然损失函数实现不确定性量化，在主动学习、OOD检测和校准任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习势函数缺乏有效的不确定性量化能力，限制了其在主动学习、校准和分布外检测中的可靠性。

Method: 提出贝叶斯E(3)等变MLPs，采用迭代重分层多体消息传递，引入联合能量-力负对数似然损失函数，系统评估多种贝叶斯方法。

Result: 贝叶斯MLPs在保持最先进模型精度的同时，实现了不确定性引导的主动学习、OOD检测和能量/力校准。

Conclusion: 贝叶斯等变神经网络为开发具有不确定性感知能力的大规模原子模拟MLPs提供了强大框架。

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [114] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: ZeroShotOpt是一个基于预训练模型的通用黑盒优化方法，通过离线强化学习在大量优化轨迹上训练，实现了在2D到20D连续黑盒优化任务上的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化(BO)的性能依赖于手动调整的超参数，这些参数在不同问题场景下难以泛化。需要一种更通用、样本效率更高的优化方法。

Method: 使用离线强化学习在12种BO变体收集的大规模优化轨迹上进行预训练。通过生成数百万个基于高斯过程的合成函数来扩展预训练数据，学习可迁移的优化策略。

Result: ZeroShotOpt在广泛的未见基准测试中实现了稳健的零样本泛化，匹配或超越了包括BO在内的领先全局优化器的样本效率。

Conclusion: 该方法提供了一个可重用的基础框架，可用于未来的扩展和改进，为黑盒优化提供了新的解决方案。

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [115] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: 该研究评估了SAC、GAC和TQC算法在参数化动作空间中的性能，发现PAGAC在训练速度和回报方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同强化学习算法在高维决策任务中的表现，特别是在参数化动作空间中，消除对循环网络的需求。

Method: 使用完全可观测环境，在Platform-v0和Goal-v0基准上测试离散动作与连续动作参数空间的关联，通过Microsoft NNI进行超参数优化，并修改GAC和TQC代码库以确保可复现性。

Result: PAGAC在所有算法中表现最佳，在Platform游戏中5000回合训练时间为41:24，Robot Soccer Goal游戏中为24:04，获得最高回报，展现出最快的训练速度和稳定性。

Conclusion: PAGAC在复杂动作空间中具有速度和稳定性优势，相比PASAC和PATQC表现出更高的效率和可靠性，适合需要快速收敛和鲁棒性能的任务。未来可探索熵正则化与截断方法的混合策略。

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [116] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 提出了一个统一的双解码器深度强化学习框架UD3RL，用于解决接近旅行商问题，将决策分为节点选择和路径点确定两个子任务，在解质量和运行时间上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在解决旅行商问题方面取得了进展，但针对接近旅行商问题的研究较少，主要挑战在于其基于邻域的访问准则，即只要进入节点邻域即视为访问。

Method: 使用离散化方案构建马尔可夫决策过程，提出UD3RL框架，采用节点解码器和位置解码器分别处理两个子任务，引入k近邻子图交互策略增强空间推理，定制REINFORCE算法进行训练。

Result: UD3RL在解质量和运行时间上优于传统方法，在不同问题规模、空间分布和半径范围上表现出强泛化能力，对动态环境具有鲁棒性。

Conclusion: UD3RL框架能有效解决接近旅行商问题，具有良好的泛化性能和实际应用价值。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [117] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: 提出了一种用于图对齐问题的链式GNN方法，通过训练一系列GNN网络迭代优化相似度矩阵，结合节点对架构捕获全局结构模式，在合成基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在图组合优化问题中表现不如传统优化方法的问题，提升GNN在图对齐这一NP难问题上的实际应用价值。

Method: 采用链式训练过程，每个GNN学习迭代优化前一个网络生成的相似度矩阵；使用节点对架构而非单个节点架构，以捕获标准消息传递网络无法表示的全局结构模式。

Result: 在合成基准测试中，链式GNN比现有方法准确率提高3倍以上，能解决常规图中所有竞争方法都失败的实例；与传统优化后处理结合后，在图对齐基准测试中显著优于最先进求解器。

Conclusion: 链式GNN方法通过迭代优化和全局结构捕获，成功提升了GNN在图对齐问题上的性能，填补了GNN在组合优化问题中的性能差距。

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [118] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数蒸馏的快速蛋白质骨架生成方法，通过将采样步骤从数百步减少到少数几步，实现了20倍以上的生成速度提升，同时保持与原始模型相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散和流式生成模型在蛋白质骨架生成中表现出色，但需要数百次迭代步骤，计算成本高，限制了在大规模蛋白质发现中的应用。

Method: 采用分数身份蒸馏(SiD)技术，结合多步生成和推理时噪声调制，训练少步蛋白质骨架生成器。

Result: 蒸馏后的少步生成器实现了20倍以上的采样速度提升，同时保持了与Proteina教师模型相当的可设计性、多样性和新颖性。

Conclusion: 该方法显著降低了推理成本，使基于扩散的模型更接近实际的蛋白质工程应用，实现了大规模计算机蛋白质设计。

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [119] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一种用于图神经网络的自适应节点特征选择方法，通过在训练过程中识别和移除不必要的特征来提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据引入了复杂的依赖关系，传统特征重要性度量方法可能不适用。需要一种能够衡量特征对模型输出贡献的方法，用于解释决策、降低维度，甚至通过消除无用变量来提高性能。

Method: 基于验证性能在特征值置换时的变化来确定相关特征。该方法与模型和任务无关，在训练过程中基于干预的方法确定特征重要性，并跟踪特征被逐步移除时相关性的演变。

Result: 经验结果验证了该方法对不同图架构的灵活性，以及对更具挑战性的图学习设置的适应性。

Conclusion: 该方法不仅能在训练结束时返回特征重要性分数，还能监控特征是否被有效消除，并可用此技术评估其他指标。

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [120] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: AdaBet是一种无需梯度的层选择方法，通过分析激活空间的拓扑特征来选择重要层进行重训练，在移动设备上实现高效模型适配。


<details>
  <summary>Details</summary>
Motivation: 在边缘和移动设备上使用预训练神经网络时，需要高效适应用户特定的运行时数据分布，但现有方法依赖标签数据、完整反向传播或服务器端元训练，不适合资源受限设备。

Method: 通过Betti Numbers分析激活空间的拓扑特征来排名重要层，仅使用前向传播，无需标签或梯度，选择具有高学习能力的层进行重训练。

Result: 在16对基准模型和数据集上评估，AdaBet比基于梯度的方法平均分类准确率提高5%，平均峰值内存消耗减少40%。

Conclusion: AdaBet提供了一种高效、资源友好的层选择方法，适用于资源受限设备上的模型适配，无需标签或梯度信息。

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [121] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: 提出了一种基于ConvLSTM的深度学习框架，用于预测地铁系统中列车运行间隔的时空传播，通过结合计划终端间隔和历史数据，为调度员提供实时决策支持。


<details>
  <summary>Details</summary>
Motivation: 提高城市地铁系统的实时调度效率，确保服务可靠性、最大化资源利用率和提升乘客满意度，避免依赖计算密集的模拟。

Method: 使用卷积长短期记忆网络(ConvLSTM)模型，直接整合计划终端间隔和历史间隔数据，预测整个地铁线路上的列车运行间隔时空传播。

Result: 在大规模城市地铁数据集上的评估显示，该模型能够准确预测列车运行间隔动态，为实时决策提供可行见解。

Conclusion: 该框架为铁路运营商提供了一个强大且计算高效的工具，能够优化调度策略，显著提高服务一致性和乘客满意度。

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [122] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: 提出Signature-Informed Transformer (SIT)框架，通过直接优化风险感知金融目标来学习端到端的资产分配策略，在S&P 100股票数据上显著优于传统和深度学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决量化金融中稳健资产分配的关键挑战，传统深度学习预测器因目标不匹配和误差放大而失败。

Method: 使用路径签名对资产动态进行丰富几何表示，并采用签名增强注意力机制嵌入金融归纳偏置（如领先滞后效应）。

Result: 在每日S&P 100股票数据评估中，SIT明显优于传统和深度学习基线，特别是与预测后优化模型相比。

Conclusion: 投资组合感知目标和几何感知归纳偏置对于机器学习系统中的风险感知资本分配至关重要。

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [123] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 提出一种利用语言模型为反事实解释生成叙述性文本的新方法，通过知识蒸馏技术使小型语言模型能够媲美大型模型，并引入评估方法来验证叙述的准确性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释虽然能揭示模型决策过程，但通常过于技术化，非专家难以理解。需要将复杂的技术解释转化为易于理解的叙述性文本。

Method: 使用语言模型构建反事实解释的叙述性文本，采用知识蒸馏技术和精炼机制，使小型语言模型达到与大型模型相当的性能，同时保持推理能力。

Result: 提出的方法增强了学生模型的推理能力和实际性能，使其更适合实际应用场景。

Conclusion: 该管道通过语言模型生成反事实解释的叙述性文本，使解释更易于理解，同时通过知识蒸馏使小型模型具备与大型模型相当的能力，提高了实际应用价值。

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [124] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: 本文利用经典高速率量化理论研究混合专家模型在回归任务中的应用，分析了MoE模型的近似误差和统计学习特性，揭示了专家数量在逼近误差和估计误差之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 通过高速率量化理论假设来研究MoE模型，假设专家数量足够多使得输入空间区域非常小，从而能够分析MoE模型的近似误差特性。

Method: 定义基于输入空间分割的MoE模型，每个区域对应一个单参数专家作为常数预测器；在一维和多维输入情况下分别制定测试误差及其最小化方法；考虑给定分割下从训练数据学习专家参数的统计学习特性。

Result: 理论分析和实证研究表明MoE学习中的逼近误差和估计误差之间的权衡取决于专家数量；为一维输入制定了最小化测试误差的分割和专家配置；为多维输入制定了测试误差上界及其最小化方法。

Conclusion: MoE模型的学习性能受到专家数量的显著影响，需要在逼近能力和统计估计精度之间进行权衡，这为理解和设计MoE模型提供了理论指导。

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [125] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 提出一种新的主动学习获取函数，通过估计校准误差来查询样本，在利用DNN不确定性之前优先选择校准误差最高的样本，从而降低泛化误差和校准误差。


<details>
  <summary>Details</summary>
Motivation: 传统基于不确定性的主动学习方法在未校准的模型不确定性下效果不佳，特别是深度神经网络的不确定性通常未校准，导致次优泛化和高校准误差。

Method: 利用核校准误差估计器在协变量偏移下估计校准误差，提出新的获取函数，先查询校准误差最高的样本，再结合DNN不确定性进行主动学习。

Result: 理论上证明该方法能在未标记池和未见测试数据上获得有界的校准误差；实验表明在池式主动学习设置中，该方法在校准误差和泛化误差方面均优于其他基线方法。

Conclusion: 通过关注模型校准误差的主动学习策略，能够有效提升分类器的校准性能和泛化能力，特别适用于深度神经网络的不确定性校准问题。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [126] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 该论文为学习率预热提供了理论解释，证明了在广义平滑性条件下，预热策略比固定步长能实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 学习率预热已成为深度学习的普遍启发式方法，但其理论基础尚不明确，需要从理论上解释为何预热能改善训练效果。

Method: 基于广义(L0,L1)-平滑性条件，该条件以损失次优性的线性函数限制局部曲率，并证明该条件在常见神经网络架构中成立。在此假设下，分析带预热调度的梯度下降收敛性。

Result: 理论证明和实验验证都表明，在广义平滑性条件下，带预热调度的梯度下降比固定步长具有更快的收敛速度，并建立了上下复杂度界。

Conclusion: 学习率预热通过处理训练初期的曲率问题，在理论和实践中都能有效加速收敛，特别是在语言和视觉模型的训练中表现出明显优势。

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [127] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: FTTE是一个新颖的半异步联邦学习框架，通过稀疏参数更新和基于更新年龄与方差的陈旧度加权聚合，显著提升了在资源受限边缘设备上的训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限的边缘节点部署面临挑战，包括有限的内存、能源和通信带宽，传统同步和异步方法在异构大规模网络中受到拖尾设备延迟和收敛缓慢的影响。

Method: 采用稀疏参数更新和基于更新年龄与方差的陈旧度加权聚合机制，实现半异步联邦学习框架。

Result: 在包含500个客户端和90%拖尾设备的多样化实验中，FTTE比同步联邦学习（如FedAVG）收敛速度快81%，设备内存使用降低80%，通信负载减少69%，并且在挑战性场景下达到与半异步方法（如FedBuff）相当或更高的目标精度。

Conclusion: FTTE是首个适用于异构且主要资源受限边缘设备上实际部署的可扩展联邦学习解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [128] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 提出Density-QUCB算法，通过转移密度函数检测分布漂移，改进Q-learning UCB的不确定性估计，在非平稳强化学习中平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳强化学习中的分布漂移问题，传统QUCB算法在分布变化后可能利用次优奖励，需要shift-aware的方法。

Method: 提出DQUCB算法，使用转移密度函数检测分布漂移，利用似然值增强Q-learning UCB的不确定性估计质量。

Result: 理论证明DQUCB比QUCB有更好的后悔保证；实验显示在RL任务和COVID-19患者分配任务中，DQUCB比QUCB基线有更低的后悔。

Conclusion: DQUCB结合了模型无关RL的计算效率，在非平稳环境中有效处理分布漂移，实现了更好的探索-利用平衡。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [129] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: 提出了PRISM-Physics，一个用于复杂物理推理问题的过程级评估框架和基准，使用有向无环图表示解决方案，实现细粒度、可解释的评分。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准大多只评估最终答案，无法捕捉推理过程；而现有的逐步方法依赖启发式LLM评分或限制性线性假设，限制了可靠性和诊断有效性。

Method: 将解决方案表示为公式的有向无环图，明确编码中间步骤间的因果依赖关系；结合完全基于规则的符号公式等价匹配方法，确保跨不同表述的一致性验证。

Result: 评估框架与人类专家评分更一致；在先进LLM上的实验揭示了物理推理中的持续失败，而步骤级评分提供了诊断洞察和丰富的训练信号。

Conclusion: PRISM-Physics通过结合结构严谨性、理论保证和符号验证，为推进过程级评估和指导具有更深科学推理能力的模型发展提供了原则性基础。

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [130] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 本文探讨了叠加假设如何影响神经表示对齐度量，发现不同的叠加排列会干扰预测映射度量，导致对齐分数偏低。通过稀疏自编码器解耦叠加后，对齐分数显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究叠加假设与表示对齐度量之间的相互作用，验证不同叠加排列是否会干扰对齐度量，导致低估真实的对齐程度。

Method: 开发了严格置换度量依赖叠加排列的理论，使用稀疏自编码器在玩具模型中解耦叠加，并测试DNN→DNN和DNN→大脑线性回归对齐。

Result: 当用稀疏过完备潜在代码替换基础神经元时，对齐分数通常增加；在视觉领域的DNN→DNN和DNN→大脑线性回归对齐中也观察到类似提升。

Conclusion: 叠加解耦对于映射度量揭示神经代码间真实表示对齐是必要的。

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [131] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Walhström*

Main category: cs.LG

TL;DR: 使用机器学习和可穿戴传感器数据（惯性传感器和肌电图）来估计单臂哑铃弯举时的自觉用力程度评分（RPE），随机森林分类器表现最佳，准确率达到41.4%（精确匹配）和85.9%（±1 RPE误差范围内）。


<details>
  <summary>Details</summary>
Motivation: 准确估计自觉用力程度评分（RPE）可以通过个性化反馈和预防损伤来增强阻力训练效果。

Method: 收集了69组超过1000次重复的自定义数据集，使用可穿戴惯性传感器和肌电图（EMG）传感器数据，提取统计特征用于模型训练，评估了多种机器学习模型。

Result: 随机森林分类器表现最佳，精确准确率为41.4%，±1 RPE误差范围内的准确率为85.9%。EMG数据的加入相比仅使用惯性传感器略有提升，但可能受数据质量和放置敏感性的限制。离心重复时间被确定为最强的RPE预测因子。

Conclusion: 研究证明了基于可穿戴传感器的RPE估计的可行性，并确定了提高模型泛化能力的关键挑战。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [132] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了Best-of-Majority (BoM)推理策略，结合多数投票和Best-of-N的优势，在Pass@k设置下实现最优推理扩展性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略如多数投票和Best-of-N在困难任务中表现不佳，特别是在Pass@k评估设置下无法实现理想的扩展性能。

Method: 提出BoM策略，首先限制候选响应为高频样本，然后选择前k个奖励最高的响应，结合了多数投票和BoN的优点。

Result: 理论证明BoM在采样预算N=Ω̃(C*)时，遗憾为O(ε_opt+√(ε_RM²C*/k))，并建立了匹配下界证明其极小极大最优性。实验显示BoM在数学问题上优于多数投票和BoN。

Conclusion: BoM是首个在Pass@k设置下实现最优推理扩展的策略，其性能不会随N增加而下降，优于现有方法。

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [133] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 本文研究了部分可观测强化学习中特权专家蒸馏与标准RL的权衡，发现权衡取决于潜在动态的随机性，且最优潜在策略不总是最佳蒸馏目标。


<details>
  <summary>Details</summary>
Motivation: 部分可观测性是强化学习中的主要挑战，特权专家蒸馏虽然计算效率高但存在失败模式，需要系统研究其与标准RL的权衡关系。

Method: 通过理论模型（扰动块MDP）和模拟运动任务的受控实验，对比特权专家蒸馏和标准RL方法。

Result: 发现权衡取决于潜在动态的随机性，且最优潜在策略不总是最佳蒸馏目标，这为有效利用特权信息提供了新指导原则。

Conclusion: 研究结果为有效利用特权信息提供了新指南，有望提升许多实际部分可观测领域中策略学习的效率。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [134] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 本文提出Lp-Reg方法，通过保护低概率但有价值的推理火花来增强RLVR训练中的探索能力，解决了策略熵崩溃导致的训练瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法在训练过程中会出现性能平台期，原因是策略熵崩溃导致探索能力丧失。现有方法通常通过维持高熵来解决，但缺乏对有意义探索机制的深入研究。

Method: 引入低概率正则化(Lp-Reg)，通过构建启发式代理分布来正则化策略。该代理分布通过过滤噪声标记并重新归一化剩余候选者来放大推理火花的概率，然后作为软正则化目标保护这些有价值标记。

Result: Lp-Reg能够实现约1000步的稳定在线策略训练，而基线熵控制方法会崩溃。在五个数学基准测试上达到60.17%的平均准确率，比先前方法提升2.66%。

Conclusion: Lp-Reg通过保护推理火花有效解决了RLVR训练中的探索退化问题，实现了更稳定和有效的训练过程。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [135] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文首次为基于规则的智能环境领域形式化并实现了反事实解释方法，通过用户研究发现用户偏好具有高度情境依赖性：因果解释在时间压力下更受欢迎，而反事实解释在解决问题时更受青睐。


<details>
  <summary>Details</summary>
Motivation: 虽然反事实解释在可解释AI中是强大工具，但在基于规则的智能环境领域尚无成熟的生成方法，需要开发专门针对该领域的反事实解释框架。

Method: 开发了一个插件来扩展现有的智能环境解释引擎，实现了反事实解释的生成，并通过用户研究(N=17)评估反事实解释与传统因果解释的效果。

Result: 用户偏好高度依赖情境：因果解释因其语言简洁性和在时间压力下的优势而受青睐，反事实解释则因其可操作内容在用户想要解决问题时更受欢迎。

Conclusion: 本研究为智能环境提供了一种新型解释的实用框架，并为选择最有效解释类型提供了实证依据，强调了解释选择应根据具体使用情境而定。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [136] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena是一个实时开放网络代理评估平台，通过收集用户提交的任务、运行Arena式头对头比较，并使用步骤级人工反馈来揭示失败模式。研究发现当前网络代理存在三个一致的失败模式：验证码解决、弹出横幅移除和直接URL导航。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理评估局限于沙盒环境或人工任务，需要真实开放网络环境下的评估平台来理解代理的失败模式。

Method: 构建BrowserArena平台，收集用户提交的真实任务，进行头对头比较，分析步骤级人工反馈，并针对发现的失败模式构建针对性数据集进行深入研究。

Result: 识别出三个一致的失败模式：验证码解决、弹出横幅移除和直接URL导航。发现不同语言模型在这些任务上表现各异，如o4-mini使用更多策略绕过验证码，而DeepSeek-R1在验证码解决上误导用户。

Conclusion: 当前网络代理既多样化又脆弱，BrowserArena的基准测试方法为大规模评估和理解网络代理失败模式提供了有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [137] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出一种基于分布无关风险控制的方法，通过动态提前退出机制来防御恶意上下文示例攻击，在保证安全性的同时提升计算效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型能够从少量上下文示例中学习新任务，但这种灵活性带来了安全隐患：模型可能被错误或恶意的演示影响，需要内置防御机制来抵御此类攻击

Method: 定义零样本基准安全行为，应用分布无关风险控制来限制上下文样本对性能的负面影响，通过动态提前退出预测忽略关注不安全输入的注意力头

Result: 理论分析和实证结果表明，该方法能有效控制有害上下文演示的风险，同时在有益演示上获得显著的计算效率提升

Conclusion: 提出的方法能够在保持模型安全性的同时，利用有益上下文演示提升性能和效率，为LLM的安全部署提供了有效解决方案

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [138] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 研究发现大型多模态模型中的一小部分注意力头负责传递空间关系表示，这些注意力头的激活（称为函数向量）可以被提取和操作来改变模型在关系任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型展现出令人印象深刻的情境学习能力，但其支持任务学习的内在机制仍然不透明，需要深入理解模型如何编码和处理空间关系知识。

Method: 应用因果中介分析识别影响关系预测的注意力头，提取多模态函数向量，并在保持LMM参数冻结的情况下用少量训练数据微调这些向量。

Result: 提取的函数向量能提高零样本推理准确率，通过微调显著优于情境学习基线，且关系特定的函数向量可以线性组合来解决涉及未训练空间关系的类比问题。

Conclusion: LMMs在局部内部结构中编码空间关系知识，这些知识可以被系统提取和优化，这增进了对模型模块化的理解并增强了对LMMs中关系推理的控制能力。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [139] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 该论文开发了一个基于LLM的多智能体系统，用于自动化增材制造领域的合金发现过程，通过MCP协议调用热力学计算和工艺分析工具，实现自主决策和合金可打印性分析。


<details>
  <summary>Details</summary>
Motivation: 增材制造中的合金发现是一个复杂的跨学科挑战，需要材料科学、热力学模拟和实验分析等多领域专业知识。LLM智能体可以利用其广泛知识库来加速这一过程。

Method: 使用大型语言模型驱动的多智能体系统，通过模型上下文协议(MCP)调用工具进行Thermo-Calc性能图计算和熔合不足工艺图生成，能够根据工具调用结果动态调整任务轨迹。

Result: 开发的多智能体系统能够有效推理复杂用户提示，对提出的合金进行可打印性分析，并在实际环境中实现自主决策。

Conclusion: LLM驱动的智能体系统能够自动化和加速增材制造中的合金发现任务，展示了采用这种多智能体系统的优势。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [140] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 研究发现测试时扩展(TTS)在大量样本下存在收益递减，提出温度维度扩展方法，通过多温度采样显著提升LLM推理能力，使基础模型达到接近RL训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法仅通过增加样本数量来提升推理能力，但存在收益递减问题，且不同温度能解决不同子问题，单一温度采样无法充分利用模型潜力。

Method: 提出温度维度扩展方法，在不同温度下生成推理轨迹，并设计多温度投票机制来降低计算开销。

Result: 在多个模型和推理基准测试中，温度扩展比单一温度TTS额外提升7.3个百分点，使基础模型性能接近RL训练模型。

Conclusion: 测试时扩展的潜力被低估，温度扩展提供了一种简单有效的方法来释放基础模型的潜在能力。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [141] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs是一个自适应红队代理系统，通过推理增强的多步骤编排自动优化多样化红队策略，有效激发视觉语言模型的有害输出。它整合了11种新颖的多模态攻击策略和17种红队算法，在基准测试中达到最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的普及，其多模态接口引入了新的安全漏洞，现有红队方法要么局限于有限的对抗模式，要么依赖人工工程，缺乏对新兴真实世界漏洞的可扩展探索。

Method: 提出ARMs自适应红队代理，采用推理增强的多步骤编排自动优化红队策略，设计分层记忆和epsilon-greedy攻击探索算法平衡攻击多样性和有效性，整合11种多模态攻击策略和17种红队算法。

Result: 在实例和策略基准测试中，ARMs达到最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%。构建了ARMs-Bench数据集，包含超过30K红队实例，涵盖51个风险类别。

Conclusion: ARMs显著提高了红队实例的多样性，揭示了视觉语言模型的新兴漏洞。使用ARMs-Bench进行安全微调可大幅提升模型鲁棒性，同时保持通用效用，为改进多模态安全对齐提供可行指导。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [142] [The land use-climate change-biodiversity nexus in European islands stakeholders](https://arxiv.org/abs/2510.02829)
*Aristides Moustakas,Irene Christoforidi,George Zittis,Nazli Demirel,Mauro Fois,Savvas Zotos,Eirini Gallou,Valentini Stamatiadou,Elli Tzirkalli,Christos Zoumides,Kristina Košić,Aikaterini Christopoulou,Aleksandra Dragin,Damian Łowicki,Artur Gil,Bruna Almeida,Panos Chrysos,Mario V. Balzan,Mark D. C. Mansoldo,Rannveig Ólafsdóttir,Cigdem Kaptan Ayhan,Lutfi Atay,Mirela Tase,Vladimir Stojanović,Maja Mijatov Ladičorbić,Juan Pedro Díaz,Francisco Javier Expósito,Sonia Quiroga,Miguel Ángel Casquet Cano,Haoran Wang,Cristina Suárez,Paraskevi Manolaki,Ioannis N. Vogiatzakis*

Main category: q-bio.PE

TL;DR: 通过机器学习分析21个欧洲岛屿利益相关者对气候变化和土地利用变化的认知，发现温度上升和森林砍伐是最主要问题，水资源和能源问题被列为优先关注事项。


<details>
  <summary>Details</summary>
Motivation: 了解利益相关者对土地利用和气候变化的观点及知识差距，以促进气候适应和减缓措施。

Method: 咨询21个欧洲岛屿的利益相关者，收集他们对气候和土地利用变化影响生态系统服务的认知，并使用机器学习分析这些认知的影响程度。

Result: 温度是主要气候特征，森林砍伐是主要土地利用特征；水资源问题是利益相关者的首要关注点；能源相关问题（包括能源短缺和可再生能源设施问题）是气候与土地利用的综合风险；气候变化对生态系统服务的影响普遍被视为负面。

Conclusion: 利益相关者对生物多样性影响有共同认知，但能区分气候和土地利用的影响；水资源、能源和可再生能源问题需要管理措施应对。

Abstract: To promote climate adaptation and mitigation, it is crucial to understand
stakeholder perspectives and knowledge gaps on land use and climate changes.
Stakeholders across 21 European islands were consulted on climate and land use
change issues affecting ecosystem services. Climate change perceptions included
temperature, precipitation, humidity, extremes, and wind. Land use change
perceptions included deforestation, coastal degradation, habitat protection,
renewable energy facilities, wetlands, and others. Additional concerns such as
invasive species, water or energy scarcity, infrastructure problems, and
austerity were also considered. Climate and land use change impact perceptions
were analysed with machine learning to quantify their influence. The
predominant climatic characteristic is temperature, and the predominant land
use characteristic is deforestation. Water-related problems are top priorities
for stakeholders. Energy-related problems, including energy deficiency and
issues with wind and solar facilities, rank high as combined climate and land
use risks. Stakeholders generally perceive climate change impacts on ecosystem
services as negative, with natural habitat destruction and biodiversity loss
identified as top issues. Land use change impacts are also negative but more
complex, with more explanatory variables. Stakeholders share common perceptions
on biodiversity impacts despite geographic disparity, but they differentiate
between climate and land use impacts. Water, energy, and renewable energy
issues pose serious concerns, requiring management measures.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [143] [NetCAS: Dynamic Cache and Backend Device Management in Networked Environments](https://arxiv.org/abs/2510.02323)
*Joon Yong Hwang,Chanseo Park,Ikjun Yeom,Younghoon Kim*

Main category: cs.OS

TL;DR: NetCAS是一个动态分割I/O的框架，通过实时网络反馈和预计算性能配置文件，在缓存和后端设备之间智能分配I/O请求，显著提升远程存储环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着性能差距缩小，同时访问缓存和后端设备可提高吞吐量，但数据中心中远程后端存储面临不可预测的网络争用，传统基于命中率的缓存策略难以有效应对。

Method: 使用实时网络反馈和预计算性能配置文件动态调整I/O分割比例，采用低开销的批量轮询调度器来执行分割，避免每个请求的开销。

Result: 在远程存储环境中比传统缓存性能提升高达174%，在网络条件波动时比Orthus等收敛方案性能提升高达3.5倍。

Conclusion: NetCAS通过动态I/O分割策略有效应对网络争用问题，显著提升远程存储系统的性能表现。

Abstract: Modern storage systems often combine fast cache with slower backend devices
to accelerate I/O. As performance gaps narrow, concurrently accessing both
devices, rather than relying solely on cache hits, can improve throughput.
However, in data centers, remote backend storage accessed over networks suffers
from unpredictable contention, complicating this split. We present NetCAS, a
framework that dynamically splits I/O between cache and backend devices based
on real-time network feedback and a precomputed Perf Profile. Unlike
traditional hit-rate-based policies, NetCAS adapts split ratios to workload
configuration and networking performance. NetCAS employs a low-overhead batched
round-robin scheduler to enforce splits, avoiding per-request costs. It
achieves up to 174% higher performance than traditional caching in remote
storage environments and outperforms converging schemes like Orthus by up to
3.5X under fluctuating network conditions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [144] [OpenZL: A Graph-Based Model for Compression](https://arxiv.org/abs/2510.03203)
*Yann Collet,Nick Terrell,W. Felix Handte,Danielle Rozenblit,Victor Zhang,Kevin Zhang,Yaelle Goldschlag,Jennifer Lee,Daniel Riegel,Stan Angelov,Nadav Rotem*

Main category: cs.IR

TL;DR: 提出OpenZL压缩框架，通过模块化编解码器图模型实现高性能压缩，解决了通用压缩器性能不足和专用压缩器部署困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用压缩器性能提升缓慢且资源消耗大，专用压缩器虽然性能好但部署维护困难，需要一种兼顾性能和实用性的新压缩方法。

Method: 提出图模型压缩理论框架，将压缩表示为模块化编解码器的有向无环图，实现OpenZL系统支持自描述格式和通用解码器。

Result: 在真实数据集上相比最先进通用压缩器获得更好的压缩比和速度，Meta内部部署显示开发时间从数月缩短到数天。

Conclusion: OpenZL代表了现代数据密集型应用中实用、可扩展和可维护数据压缩的进步。

Abstract: Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [145] [The Computational Complexity of Almost Stable Clustering with Penalties](https://arxiv.org/abs/2510.03143)
*Kamyar Khodamoradi,Farnam Mansouri,Sandra Zilles*

Main category: cs.CC

TL;DR: 该论文研究了在具有小倍增维度的度量空间中，稳定（或扰动弹性）k-means和k-median聚类问题的复杂性。作者采用更广义的"几乎稳定"概念，并扩展到带惩罚的聚类问题，证明了某些特殊情况下可在多项式时间内求解，同时基于ETH假设证明了其他情况下存在超多项式下界。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注低维欧几里得空间中的乘法扰动弹性聚类问题，作者希望采用更广义的稳定性概念（"几乎稳定"）来研究小倍增维度度量空间中的聚类问题，并扩展到带惩罚的聚类场景。

Method: 作者采用Balcan和Liang(2016)提出的(α,ε)-扰动弹性概念，研究k-means和k-median聚类问题在具有小倍增维度的度量空间中的计算复杂性。

Result: 证明了某些特殊情况的几乎稳定k-means/k-median（带惩罚）聚类问题可在多项式时间内求解，同时基于ETH假设证明了其他情况下存在超多项式运行时间下界。

Conclusion: 在具有小倍增维度的度量空间中，几乎稳定的聚类问题在某些特殊情况下是易处理的，但在一般情况下仍然具有计算困难性，这为理解聚类问题的计算复杂性提供了新的见解。

Abstract: We investigate the complexity of stable (or perturbation-resilient) instances
of $\mathrm{k-M\small{EANS}}$ and $\mathrm{k-M\small{EDIAN}}$ clustering
problems in metrics with small doubling dimension. While these problems have
been extensively studied under multiplicative perturbation resilience in
low-dimensional Euclidean spaces (e.g., (Friggstad et al., 2019; Cohen-Addad
and Schwiegelshohn, 2017)), we adopt a more general notion of stability, termed
``almost stable'', which is closer to the notion of $(\alpha,
\varepsilon)$-perturbation resilience introduced by Balcan and Liang (2016).
Additionally, we extend our results to
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ with penalties, where
each data point is either assigned to a cluster centre or incurs a penalty.
  We show that certain special cases of almost stable
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties) are
solvable in polynomial time. To complement this, we also examine the hardness
of almost stable instances and $(1 + \frac{1}{poly(n)})$-stable instances of
$\mathrm{k-M\small{EANS}}$/$\mathrm{k-M\small{EDIAN}}$ (with penalties),
proving super-polynomial lower bounds on the runtime of any exact algorithm
under the widely believed Exponential Time Hypothesis (ETH).

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [146] [The Equilibrium Response of Atmospheric Machine-Learning Models to Uniform Sea Surface Temperature Warming](https://arxiv.org/abs/2510.02415)
*Bosong Zhang,Timothy M. Merlis*

Main category: physics.ao-ph

TL;DR: 评估多个先进机器学习气候模型在均匀海表温度升温情景下的表现，与物理模型对比发现ML模型能再现关键响应特征但存在辐射响应和陆地升温等方面的偏差


<details>
  <summary>Details</summary>
Motivation: 虽然已开发出能产生稳定多年气候模拟的ML模型，但这些模型在训练分布之外的泛化能力仍不明确，需要评估其对气候变化的响应能力

Method: 通过均匀海表温度升温这一广泛使用的气候变化基准，评估ACE2-ERA5、NeuralGCM和cBottle等ML模型，并与物理基础环流模型GFDL AM4在多个关键诊断指标上进行对比

Result: ML模型能再现物理模型响应的关键方面，特别是降水响应，但在辐射响应和陆地区域升温等方面表现出与稳健物理响应的显著偏差

Conclusion: ML模型在气候变化应用中展现出前景但存在当前局限性，需要进一步改进以实现稳健的样本外泛化

Abstract: Machine learning models for the global atmosphere that are capable of
producing stable, multi-year simulations of Earth's climate have recently been
developed. However, the ability of these ML models to generalize beyond the
training distribution remains an open question. In this study, we evaluate the
climate response of several state-of-the-art ML models (ACE2-ERA5, NeuralGCM,
and cBottle) to a uniform sea surface temperature warming, a widely used
benchmark for evaluating climate change. We assess each ML model's performance
relative to a physics-based general circulation model (GFDL's AM4) across key
diagnostics, including surface air temperature, precipitation, temperature and
wind profiles, and top-of-the-atmosphere radiation. While the ML models
reproduce key aspects of the physical model response, particularly the response
of precipitation, some exhibit notable departures from robust physical
responses, including radiative responses and land region warming. Our results
highlight the promise and current limitations of ML models for climate change
applications and suggest that further improvements are needed for robust
out-of-sample generalization.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [147] [FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for Implementable Portfolio Management](https://arxiv.org/abs/2510.02986)
*Jian'an Zhang*

Main category: q-fin.TR

TL;DR: FR-LUX是一个强化学习框架，通过集成微观结构一致的执行成本模型、交易空间信任区域和显式制度条件化，解决了交易成本和制度转换导致的实盘表现不佳问题。


<details>
  <summary>Details</summary>
Motivation: 交易成本和制度转换是导致模拟投资组合在实盘交易中失败的主要原因，需要开发能够学习考虑成本后的交易策略并在不同波动性-流动性制度下保持稳健的方法。

Method: FR-LUX结合三个关键要素：(1) 将比例成本和冲击成本直接嵌入奖励的微观结构一致执行模型；(2) 约束库存流变化而非logits的交易空间信任区域；(3) 显式制度条件化使策略专门适应LL/LH/HL/HH状态。

Result: 在4×5制度和成本水平网格上，FR-LUX实现了最高的平均夏普比率，保持比强基线更平坦的成本-性能斜率，并在给定换手率预算下获得优越的风险回报效率。成对场景级改进严格为正且在多重检验校正后保持统计显著性。

Conclusion: 该方法提供了在凸摩擦下的最优性保证、KL信任区域下的单调改进、长期换手率边界和由比例成本引起的无为带、制度条件化策略的正价值优势以及对成本误设的鲁棒性。方法可实施且结果可复现。

Abstract: Transaction costs and regime shifts are major reasons why paper portfolios
fail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned
Learning under eXecution costs), a reinforcement learning framework that learns
after-cost trading policies and remains robust across volatility-liquidity
regimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent
execution model combining proportional and impact costs, directly embedded in
the reward; (ii) a trade-space trust region that constrains changes in
inventory flow rather than logits, yielding stable low-turnover updates; and
(iii) explicit regime conditioning so the policy specializes to LL/LH/HL/HH
states without fragmenting the data. On a 4 x 5 grid of regimes and cost levels
with multiple random seeds, FR-LUX achieves the top average Sharpe ratio with
narrow bootstrap confidence intervals, maintains a flatter cost-performance
slope than strong baselines, and attains superior risk-return efficiency for a
given turnover budget. Pairwise scenario-level improvements are strictly
positive and remain statistically significant after multiple-testing
corrections. We provide formal guarantees on optimality under convex frictions,
monotonic improvement under a KL trust region, long-run turnover bounds and
induced inaction bands due to proportional costs, positive value advantage for
regime-conditioned policies, and robustness to cost misspecification. The
methodology is implementable: costs are calibrated from standard liquidity
proxies, scenario-level inference avoids pseudo-replication, and all figures
and tables are reproducible from released artifacts.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [148] [Optimized Degree Realization: Minimum Dominating Set & Maximum Matching](https://arxiv.org/abs/2510.03176)
*Amotz Bar-Noy,Igor Kalinichev,David Peleg,Dror Rawitz*

Main category: cs.DM

TL;DR: 本文研究了度序列实现问题中的两个优化变体：最小支配集和最大匹配问题，并提出了多项式时间算法。同时给出了最小支配集大小的简洁特征描述。


<details>
  <summary>Details</summary>
Motivation: 度序列实现问题在存在多个可行解时，需要找到质量最优的实现。之前已有最大团、最大独立集和最小顶点覆盖的高效算法，但最小支配集和最大匹配问题尚未解决。

Method: 针对度序列实现问题中的最小支配集和最大匹配两个优化目标，设计了多项式时间算法来找到最优实现。

Result: 成功解决了这两个开放问题，提出了多项式时间算法，并给出了最小支配集大小的简洁特征描述。

Conclusion: 本文填补了度序列优化实现问题中的重要空白，为最小支配集和最大匹配问题提供了高效解决方案和理论特征描述。

Abstract: The Degree Realization problem requires, given a sequence $d$ of $n$ positive
integers, to decide whether there exists a graph whose degrees correspond to
$d$, and to construct such a graph if it exists. A more challenging variant of
the problem arises when $d$ has many different realizations, and some of them
may be more desirable than others. We study \emph{optimized realization}
problems in which the goal is to compute a realization that optimizes some
quality measure. Efficient algorithms are known for the problems of finding a
realization with the maximum clique, the maximum independent set, or the
minimum vertex cover. In this paper, we focus on two problems for which such
algorithms were not known. The first is the Degree Realization with Minimum
Dominating Set problem, where the goal is to find a realization whose minimum
dominating set is minimized among all the realizations of the given sequence
$d$. The second is the Degree Realization with Maximum Matching problem, where
the goal is to find a realization with the largest matching among all the
realizations of $d$. We present polynomial time realization algorithms for
these two open problems.
  A related problem of interest and importance is \emph{characterizing} the
sequences with a given value of the optimized function. This leads to an
efficient computation of the optimized value without providing the realization
that achieves that value. For the Maximum Matching problem, a succinct
characterization of degree sequences with a maximum matching of a given size
was known. This paper provides a succinct characterization of sequences with
minimum dominating set of a given size.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [149] [oRANS: Online optimisation of RANS machine learning models with embedded DNS data generation](https://arxiv.org/abs/2510.02982)
*Daniel Dehtyriov,Jonathan F. MacArt,Justin Sirignano*

Main category: physics.flu-dyn

TL;DR: 提出在线优化框架，将DNS嵌入RANS子域中动态生成训练数据，解决深度学习流体模拟中高保真数据稀缺问题，显著提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的深度学习流体模拟存在高保真数据稀缺问题，导致模型过拟合且难以泛化到新流态

Method: 在RANS域的子域中嵌入DNS模拟，RANS提供边界条件给DNS，DNS提供平均速度和湍流统计量来在线更新DL闭合模型

Result: 在线优化的RANS模型显著优于离线训练和文献校准的闭合模型，使用较小的DNS子域即可实现准确训练

Conclusion: 该框架为物理信息机器学习闭合提供了可扩展路径，使数据自适应降阶模型能够在不同流态间泛化，无需大型预计算训练数据集

Abstract: Deep learning (DL) has demonstrated promise for accelerating and enhancing
the accuracy of flow physics simulations, but progress is constrained by the
scarcity of high-fidelity training data, which is costly to generate and
inherently limited to a small set of flow conditions. Consequently, closures
trained in the conventional offline paradigm tend to overfit and fail to
generalise to new regimes. We introduce an online optimisation framework for
DL-based Reynolds-averaged Navier--Stokes (RANS) closures which seeks to
address the challenge of limited high-fidelity datasets. Training data is
dynamically generated by embedding a direct numerical simulation (DNS) within a
subdomain of the RANS domain. The RANS solution supplies boundary conditions to
the DNS, while the DNS provides mean velocity and turbulence statistics that
are used to update a DL closure model during the simulation. This feedback loop
enables the closure to adapt to the embedded DNS target flow, avoiding reliance
on precomputed datasets and improving out-of-distribution performance. The
approach is demonstrated for the stochastically forced Burgers equation and for
turbulent channel flow at $Re_\tau=180$, $270$, $395$ and $590$ with varying
embedded domain lengths $1\leq L_0/L\leq 8$. Online-optimised RANS models
significantly outperform both offline-trained and literature-calibrated
closures, with accurate training achieved using modest DNS subdomains.
Performance degrades primarily when boundary-condition contamination dominates
or when domains are too short to capture low-wavenumber modes. This framework
provides a scalable route to physics-informed machine learning closures,
enabling data-adaptive reduced-order models that generalise across flow regimes
without requiring large precomputed training datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [150] [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)
*Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: cs.RO

TL;DR: 提出了一种动作偏差感知的混合推理方法，通过选择性跳过服务器验证来降低延迟和传输开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 为支持自动驾驶和机器人操作等延迟敏感的AI应用，6G需要分布式机器学习。但行为克隆策略无法并行验证多个动作草案，因为每个动作都依赖于前一个动作的观察结果更新。

Method: 提出动作偏差感知混合推理，草案模型估计动作是否需要目标模型验证，选择性跳过服务器通信和计算。推导路径偏差阈值来平衡传输率和推理性能。

Result: 减少上行传输和服务器操作40%，端到端延迟降低33.32%，任务成功率可达目标模型单独推理的97.03%。

Conclusion: 动作偏差感知混合推理能有效降低延迟和传输开销，同时保持接近目标模型的性能，适用于分布式ML场景。

Abstract: To support latency-sensitive AI applications ranging from autonomous driving
to industrial robot manipulation, 6G envisions distributed ML, connecting
distributed computational resources in edge and cloud over hyper-reliable
low-latency communication (HRLLC). In this setting, speculative decoding can
facilitate collaborative inference of models distributively deployed: an
on-device draft model locally generates drafts and a remote server-based target
model verifies and corrects them, resulting lower latency. However, unlike
autoregressive text generation, behavior cloning policies, typically used for
embodied AI applications like robot manipulation and autonomous driving, cannot
parallelize verification and correction for multiple drafts as each action
depends on observation which needs to be updated by a previous action. To this
end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft
model estimates an action's need for verification and correction by the target
model and selectively skips communication and computation for server
operations. Action deviation shows a strong correlation with action's rejection
probability by the target model, enabling selective skipping. We derive the
path deviation threshold that balances the transmission rate and the inference
performance, and we empirically show that action deviation-aware hybrid
inference reduces uplink transmission and server operation by 40%, while
lowering end-to-end latency by 33.32% relative to hybrid inference without
skipping and achieving task success rate up to 97.03% of that of target model
only inference.

</details>


### [151] [Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data](https://arxiv.org/abs/2510.02738)
*Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa*

Main category: cs.RO

TL;DR: 提出一个基于单次人类演示生成力信息仿真数据的框架，结合顺应性策略提升视觉运动策略在接触丰富任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动策略在处理需要连续接触的机器人操作任务时，往往忽略顺应性和力的重要性，导致过大的接触力或脆弱行为。引入力信息可改善接触感知，但需要大量数据。

Method: 通过单次人类演示生成力信息仿真数据，结合顺应性策略来学习视觉运动策略。

Result: 在真实机器人任务（包括非抓取块翻转和双手物体移动）中验证，学习到的策略展现出可靠的接触保持和对新条件的适应性。

Conclusion: 该框架能够有效解决接触丰富任务中的数据稀缺问题，提升视觉运动策略在物理交互环境中的性能。

Abstract: While visuomotor policy has made advancements in recent years, contact-rich
tasks still remain a challenge. Robotic manipulation tasks that require
continuous contact demand explicit handling of compliance and force. However,
most visuomotor policies ignore compliance, overlooking the importance of
physical interaction with the real world, often leading to excessive contact
forces or fragile behavior under uncertainty. Introducing force information
into vision-based imitation learning could help improve awareness of contacts,
but could also require a lot of data to perform well. One remedy for data
scarcity is to generate data in simulation, yet computationally taxing
processes are required to generate data good enough not to suffer from the
Sim2Real gap. In this work, we introduce a framework for generating
force-informed data in simulation, instantiated by a single human
demonstration, and show how coupling with a compliant policy improves the
performance of a visuomotor policy learned from synthetic data. We validate our
approach on real-robot tasks, including non-prehensile block flipping and a
bi-manual object moving, where the learned policy exhibits reliable contact
maintenance and adaptation to novel conditions. Project Website:
https://flow-with-the-force-field.github.io/webpage/

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [152] [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](https://arxiv.org/abs/2510.03155)
*Stevens Johnson,Varun Puram,Johnson Thomas,Acsah Konuparamban,Ashwin Kannan*

Main category: q-bio.NC

TL;DR: 提出量子启发的漏积分发放(QI-LIF)模型，将动作电位起始视为概率事件，相比传统LIF模型显著降低了预测误差，特别是在高强度刺激下。


<details>
  <summary>Details</summary>
Motivation: 传统漏积分发放(LIF)模型在预测动作电位起始延迟时存在高相对误差，特别是在强刺激或快速变化刺激下，需要更准确的神经元建模方法。

Method: 基于量子理论，将动作电位起始建模为时间上的高斯波包概率事件，开发量子启发的漏积分发放(QI-LIF)模型，并与经典LIF模型进行系统比较。

Result: QI-LIF模型显著降低了动作电位起始预测的相对误差，特别是在高强度刺激下，与观察到的生物反应更加吻合。

Conclusion: 量子启发的计算框架在提高神经建模准确性方面具有潜力，对量子工程方法实现脑启发计算具有重要意义。

Abstract: Accurate modeling of neuronal action potential (AP) onset timing is crucial
for understanding neural coding of danger signals. Traditional leaky
integrate-and-fire (LIF) models, while widely used, exhibit high relative error
in predicting AP onset latency, especially under strong or rapidly changing
stimuli. Inspired by recent experimental findings and quantum theory, we
present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats
AP onset as a probabilistic event, represented by a Gaussian wave packet in
time. This approach captures the biological variability and uncertainty
inherent in neuronal firing. We systematically compare the relative error of AP
onset predictions between the classical LIF and QI-LIF models using synthetic
data from hippocampal and sensory neurons subjected to varying stimulus
amplitudes. Our results demonstrate that the QI-LIF model significantly reduces
prediction error, particularly for high-intensity stimuli, aligning closely
with observed biological responses. This work highlights the potential of
quantum-inspired computational frameworks in advancing the accuracy of neural
modeling and has implications for quantum engineering approaches to
brain-inspired computing.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [153] [Self-supervised diffusion model fine-tuning for costate initialization using Markov chain Monte Carlo](https://arxiv.org/abs/2510.02527)
*Jannik Graebner,Ryne Beeson*

Main category: astro-ph.EP

TL;DR: 使用条件扩散模型和马尔可夫链蒙特卡洛算法优化低推力航天器轨迹，通过自监督微调完成帕累托前沿并提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 间接法求解长周期低推力航天器轨迹时，由于解空间复杂和协态变量初始猜测困难，特别是在多体环境中，全局搜索和优化具有挑战性。

Method: 采用条件扩散模型表示候选最优轨迹解的分布，结合马尔可夫链蒙特卡洛算法进行自监督微调，使用随机游走Metropolis算法提出新数据，基于约束违反和任务目标函数的高效评估进行奖励加权训练。

Result: 在木星-欧罗巴圆形限制三体问题中，MCMC方法完成了部分帕累托前沿；在土星-泰坦转移任务中，相比单独的全局搜索，该方法生成了更密集且更优的帕累托前沿。

Conclusion: 该框架无需单独的数据生成阶段，能够提高样本质量并明确针对帕累托最优性，为多体环境中的低推力轨迹优化提供了有效解决方案。

Abstract: Global search and optimization of long-duration, low-thrust spacecraft
trajectories with the indirect method is challenging due to a complex solution
space and the difficulty of generating good initial guesses for the costate
variables. This is particularly true in multibody environments. Given data that
reveals a partial Pareto optimal front, it is desirable to find a flexible
manner in which the Pareto front can be completed and fronts for related
trajectory problems can be found. In this work we use conditional diffusion
models to represent the distribution of candidate optimal trajectory solutions.
We then introduce into this framework the novel approach of using Markov Chain
Monte Carlo algorithms with self-supervised fine-tuning to achieve the
aforementioned goals. Specifically, a random walk Metropolis algorithm is
employed to propose new data that can be used to fine-tune the diffusion model
using a reward-weighted training based on efficient evaluations of constraint
violations and missions objective functions. The framework removes the need for
separate focused and often tedious data generation phases. Numerical
experiments are presented for two problems demonstrating the ability to improve
sample quality and explicitly target Pareto optimality based on the theory of
Markov chains. The first problem does so for a transfer in the Jupiter-Europa
circular restricted three-body problem, where the MCMC approach completes a
partial Pareto front. The second problem demonstrates how a dense and superior
Pareto front can be generated by the MCMC self-supervised fine-tuning method
for a Saturn-Titan transfer starting from the Jupiter-Europa case versus a
separate dedicated global search.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [154] [Adaptive randomized pivoting and volume sampling](https://arxiv.org/abs/2510.02513)
*Ethan N. Epperly*

Main category: stat.ML

TL;DR: 本文重新解释自适应随机主元选择(ARP)算法，将其与体积采样分布和线性回归的主动学习算法联系起来，提出了新的分析方法和使用拒绝采样的更快速实现。


<details>
  <summary>Details</summary>
Motivation: 重新解释ARP算法，建立与体积采样分布和主动学习算法的联系，以提供新的理论分析和改进实现效率。

Method: 通过建立ARP与体积采样分布的联系，使用拒绝采样技术来加速算法实现。

Result: 提出了对ARP算法的新分析，并开发了更快的实现版本。

Conclusion: ARP算法可以通过与体积采样和主动学习的联系得到更好的理论理解，并且使用拒绝采样可以显著提高算法效率。

Abstract: Adaptive randomized pivoting (ARP) is a recently proposed and highly
effective algorithm for column subset selection. This paper reinterprets the
ARP algorithm by drawing connections to the volume sampling distribution and
active learning algorithms for linear regression. As consequences, this paper
presents new analysis for the ARP algorithm and faster implementations using
rejection sampling.

</details>


### [155] [Higher-arity PAC learning, VC dimension and packing lemma](https://arxiv.org/abs/2510.02420)
*Artem Chernikov,Henry Towsner*

Main category: stat.ML

TL;DR: 本文概述了高阶VC理论的发展，包括Haussler打包引理的推广和超图正则性引理，并证明该理论能够刻画n重乘积空间中的高阶PAC学习。


<details>
  <summary>Details</summary>
Motivation: 发展高阶VC理论（VC_n维度）来研究n重乘积空间中的学习问题，扩展传统的二元VC理论和PAC学习框架。

Method: 推广Haussler打包引理，建立切片式超图正则性引理，将高阶VC理论与乘积测度下的PAC_n学习联系起来。

Result: 证明了高阶VC理论能够刻画n重乘积空间中的高阶PAC学习，并为近期相关研究提供了理论基础。

Conclusion: 高阶VC理论为理解n重乘积空间中的学习问题提供了统一框架，近期多项研究成果都可从该理论框架中推导出来。

Abstract: The aim of this note is to overview some of our work in Chernikov, Towsner'20
(arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension),
including a generalization of Haussler packing lemma, and an associated tame
(slice-wise) hypergraph regularity lemma; and to demonstrate that it
characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product
spaces with respect to product measures introduced by Kobayashi, Kuriyama and
Takeuchi'15. We also point out how some of the recent results in
arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in
arXiv:2010.00726.

</details>


### [156] [Predictive inference for time series: why is split conformal effective despite temporal dependence?](https://arxiv.org/abs/2510.02471)
*Rina Foygel Barber,Ashwin Pananjady*

Main category: stat.ML

TL;DR: 本文研究了时间序列预测中的不确定性量化问题，分析了分形预测方法在时间序列数据中的理论性质，提出了衡量时间依赖性的新指标"切换系数"，并给出了在平稳β混合过程类上的覆盖概率的尖锐刻画。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要提供有效的预测区间，但传统分形预测方法基于数据独立同分布或可交换性的假设，而时间序列数据存在时间依赖性，这严重违反了可交换性假设。特别是当预测器具有记忆功能时，问题更加复杂。

Method: 研究分形预测在时间序列设置中的理论性质，包括预测器具有记忆的情况。引入"切换系数"来衡量时间依赖性对可交换性的违反程度，并在平稳β混合过程类上分析覆盖概率。

Result: 提出了一个理论框架，能够准确刻画分形预测方法在时间序列中的覆盖损失，该刻画在平稳β混合过程类上是尖锐的。同时开发了可能对其他依赖数据预测推断方法分析有用的工具。

Conclusion: 本文为时间序列中分形预测的理论分析提供了重要贡献，通过引入切换系数等新工具，解决了时间依赖性对预测区间覆盖性能的影响问题，为依赖数据的预测推断方法研究奠定了基础。

Abstract: We consider the problem of uncertainty quantification for prediction in a
time series: if we use past data to forecast the next time point, can we
provide valid prediction intervals around our forecasts? To avoid placing
distributional assumptions on the data, in recent years the conformal
prediction method has been a popular approach for predictive inference, since
it provides distribution-free coverage for any iid or exchangeable data
distribution. However, in the time series setting, the strong empirical
performance of conformal prediction methods is not well understood, since even
short-range temporal dependence is a strong violation of the exchangeability
assumption. Using predictors with "memory" -- i.e., predictors that utilize
past observations, such as autoregressive models -- further exacerbates this
problem. In this work, we examine the theoretical properties of split conformal
prediction in the time series setting, including the case where predictors may
have memory. Our results bound the loss of coverage of these methods in terms
of a new "switch coefficient", measuring the extent to which temporal
dependence within the time series creates violations of exchangeability. Our
characterization of the coverage probability is sharp over the class of
stationary, $\beta$-mixing processes. Along the way, we introduce tools that
may prove useful in analyzing other predictive inference methods for dependent
data.

</details>


### [157] [Beyond Linear Diffusions: Improved Representations for Rare Conditional Generative Modeling](https://arxiv.org/abs/2510.02499)
*Kulunu Dharmakeerthi,Yousef El-Laham,Henry H. Wong,Vamsi K. Potluru,Changhong He,Taosong He*

Main category: stat.ML

TL;DR: 提出了一种尾部自适应扩散模型，通过非线性漂移项和数据表示调整，在条件变量低概率区域显著改善条件分布建模效果


<details>
  <summary>Details</summary>
Motivation: 传统线性扩散模型在条件变量低概率区域建模困难，因为训练样本稀少，难以准确学习条件分布P(Y|X=x)

Method: 基于条件极值理论，采用数据驱动的非线性漂移项扩散方法，优化数据表示以降低在条件空间低概率区域的学习样本复杂度

Result: 在两个合成数据集和一个真实金融数据集上的实验表明，该方法在极端尾部条件下准确捕捉响应分布方面显著优于标准扩散模型

Conclusion: 尾部自适应扩散模型能有效解决条件变量低概率区域的建模挑战，特别适用于极端尾部事件的分析

Abstract: Diffusion models have emerged as powerful generative frameworks with
widespread applications across machine learning and artificial intelligence
systems. While current research has predominantly focused on linear diffusions,
these approaches can face significant challenges when modeling a conditional
distribution, $P(Y|X=x)$, when $P(X=x)$ is small. In these regions, few
samples, if any, are available for training, thus modeling the corresponding
conditional density may be difficult. Recognizing this, we show it is possible
to adapt the data representation and forward scheme so that the sample
complexity of learning a score-based generative model is small in low
probability regions of the conditioning space. Drawing inspiration from
conditional extreme value theory we characterize this method precisely in the
special case in the tail regions of the conditioning variable, $X$. We show how
diffusion with a data-driven choice of nonlinear drift term is best suited to
model tail events under an appropriate representation of the data. Through
empirical validation on two synthetic datasets and a real-world financial
dataset, we demonstrate that our tail-adaptive approach significantly
outperforms standard diffusion models in accurately capturing response
distributions at the extreme tail conditions.

</details>


### [158] [Learning Multi-Index Models with Hyper-Kernel Ridge Regression](https://arxiv.org/abs/2510.02532)
*Shuo Huang,Hippolyte Labarrière,Ernesto De Vito,Tomaso Poggio,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 该论文研究了深度神经网络在高维问题中优于核方法的原因，提出组合结构是关键因素。通过多索引模型和超核岭回归方法，证明了该方法能够克服维度灾难，并开发了交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高维问题上表现出色，但其理论基础尚不明确。研究者认为学习任务的组合结构是决定深度网络优势的关键因素，希望通过形式化这一想法来理解深度学习的成功。

Method: 引入超核岭回归方法，结合神经网络和核方法的优势。使用多索引模型作为简单的组合模型，并开发了交替最小化和交替梯度两种优化方法。

Result: 证明了超核岭回归能够自适应地学习多索引模型，克服维度灾难。理论分析和数值实验都验证了该方法的有效性。

Conclusion: 组合结构确实是深度网络成功的关键因素，超核岭回归为理解深度学习提供了新的理论视角，并为高维问题提供了有效的解决方案。

Abstract: Deep neural networks excel in high-dimensional problems, outperforming models
such as kernel methods, which suffer from the curse of dimensionality. However,
the theoretical foundations of this success remain poorly understood. We follow
the idea that the compositional structure of the learning task is the key
factor determining when deep networks outperform other approaches. Taking a
step towards formalizing this idea, we consider a simple compositional model,
namely the multi-index model (MIM). In this context, we introduce and study
hyper-kernel ridge regression (HKRR), an approach blending neural networks and
kernel methods. Our main contribution is a sample complexity result
demonstrating that HKRR can adaptively learn MIM, overcoming the curse of
dimensionality. Further, we exploit the kernel nature of the estimator to
develop ad hoc optimization approaches. Indeed, we contrast alternating
minimization and alternating gradient methods both theoretically and
numerically. These numerical results complement and reinforce our theoretical
findings.

</details>


### [159] [Neural Jump ODEs as Generative Models](https://arxiv.org/abs/2510.02757)
*Robert A. Crowell,Florian Krach,Josef Teichmann*

Main category: stat.ML

TL;DR: NJODEs作为伊藤过程的生成模型，通过离散观测数据学习漂移和扩散系数，无需对抗训练即可生成与真实过程分布相同的样本。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型需要对抗训练和生成样本，而NJODE框架可以直接从观测数据学习伊藤过程的参数，处理不规则采样和路径依赖动态。

Method: 使用NJODE框架从伊藤过程的离散观测中近似漂移和扩散系数，在标准正则性假设下证明参数收敛性。

Result: 在极限情况下，学习到的系数能够恢复真实参数，生成的样本与真实过程具有相同的分布。

Conclusion: NJODE提供了一个无需对抗训练、能处理不规则数据和路径依赖的生成建模框架，适用于现实世界场景。

Abstract: In this work, we explore how Neural Jump ODEs (NJODEs) can be used as
generative models for It\^o processes. Given (discrete observations of) samples
of a fixed underlying It\^o process, the NJODE framework can be used to
approximate the drift and diffusion coefficients of the process. Under standard
regularity assumptions on the It\^o processes, we prove that, in the limit, we
recover the true parameters with our approximation. Hence, using these learned
coefficients to sample from the corresponding It\^o process generates, in the
limit, samples with the same law as the true underlying process. Compared to
other generative machine learning models, our approach has the advantage that
it does not need adversarial training and can be trained solely as a predictive
model on the observed samples without the need to generate any samples during
training to empirically approximate the distribution. Moreover, the NJODE
framework naturally deals with irregularly sampled data with missing values as
well as with path-dependent dynamics, allowing to apply this approach in
real-world settings. In particular, in the case of path-dependent coefficients
of the It\^o processes, the NJODE learns their optimal approximation given the
past observations and therefore allows generating new paths conditionally on
discrete, irregular, and incomplete past observations in an optimal way.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [160] [TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT](https://arxiv.org/abs/2510.02519)
*Atonu Ghosh,Akhilesh Mohanasundaram,Srishivanth R F,Sudip Misra*

Main category: cs.CR

TL;DR: TLoRa是一个端到端架构，通过集成TCP隧道和完整的TLS 1.3握手，在LoRa上实现HTTPS通信。它使用终端集线器和网络中继在WiFi设备和互联网之间建立安全通信通道。


<details>
  <summary>Details</summary>
Motivation: 为在LoRa网络上实现安全可靠的HTTPS通信提供解决方案，填补了在LoRa上使用完整TLS进行HTTPS访问的研究空白。

Method: 采用三阶段操作：会话建立（管理TCP套接字和TLS握手）、安全隧道（在LoRa上传输加密TLS数据）和渲染（向用户交付内容）。实现轻量级TLS记录重组层和会话复用的排队机制。

Result: 在真实硬件上评估显示，TLoRa在9.9秒内成功建立TLS会话，3.58秒完成API请求，证明了在LoRa上实现HTTPS访问的可行性。

Conclusion: TLoRa是首个在LoRa上全面设计、实现和评估使用完整TLS的HTTPS访问性能的工作，提供了实用的解决方案。

Abstract: We present TLoRa, an end-to-end architecture for HTTPS communication over
LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables
a seamless and secure communication channel between WiFi-enabled end devices
and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH
tethers a WiFi hotspot and a captive portal for user devices to connect and
request URLs. The EH forwards the requested URLs to the NR using a secure
tunnel over LoRa. The NR, which acts as a server-side proxy, receives and
resolves the request from the Internet-based server. It then relays back the
encrypted response from the server over the same secure tunnel. TLoRa operates
in three phases -session setup, secure tunneling, and rendering. In the first
phase, it manages the TCP socket and initiates the TLS handshake. In the
second, it creates a secure tunnel and transfers encrypted TLS data over LoRa.
Finally, it delivers the URL content to the user. TLoRa also implements a
lightweight TLS record reassembly layer and a queuing mechanism for session
multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a
web API. Results indicate that it provides a practical solution by successfully
establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to
fulfill API requests. To the best of our knowledge, this is the first work to
comprehensively design, implement, and evaluate the performance of HTTPS access
over LoRa using full TLS.

</details>


### [161] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络和多模态融合的被动窃听检测方法，通过联邦学习在智能电网中检测难以察觉的侦察攻击。


<details>
  <summary>Details</summary>
Motivation: 智能电网面临被动窃听威胁，攻击者通过监听通信链路获取电网拓扑和运行模式信息，为后续攻击创造条件。传统检测方法难以捕捉这种微弱、短暂的信号特征。

Method: 采用图中心多模态检测器，融合物理层和行为指标，在星型子图和短时间窗口上进行检测。使用两阶段编码器：图卷积聚合空间上下文，双向GRU建模短期时间依赖。在FedProx框架下进行联邦学习训练。

Result: 模型在测试集上达到98.32%的每时间步准确率（攻击F1=0.972）和93.35%的每序列准确率，误报率仅为0.15%。

Conclusion: 结合时空上下文能够可靠检测隐蔽的侦察攻击，同时保持低误报率，适用于非独立同分布的联邦智能电网部署。

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [162] [SoK: Preconfirmations](https://arxiv.org/abs/2510.02947)
*Aikaterini-Panagiota Stouka,Conor McMenamin,Demetris Kyriacou,Lin Oshitani,Quentin Botha*

Main category: cs.CR

TL;DR: 本文对区块链预确认协议进行了系统化知识整理，提出了预确认协议的一般框架，并分析了其经济学和风险，最后调研了现实世界中的预确认协议实现。


<details>
  <summary>Details</summary>
Motivation: 传统区块链协议存在固有延迟，限制了用户体验的提升，预确认协议通过提供早期交易确认保证来改善这一问题。

Method: 采用系统化知识整理方法，建立预确认协议的核心术语和定义，提出一般框架，并分析经济学和风险因素。

Result: 构建了预确认协议的统一框架，识别了关键风险因素，并将理论与现实世界实现进行了连接。

Conclusion: 预确认协议是改善区块链用户体验的重要方向，需要系统化框架来指导协议设计和风险评估。

Abstract: In recent years, significant research efforts have focused on improving
blockchain throughput and confirmation speeds without compromising security.
While decreasing the time it takes for a transaction to be included in the
blockchain ledger enhances user experience, a fundamental delay still remains
between when a transaction is issued by a user and when its inclusion is
confirmed in the blockchain ledger. This delay limits user experience gains
through the confirmation uncertainty it brings for users. This inherent delay
in conventional blockchain protocols has led to the emergence of
preconfirmation protocols -- protocols that provide users with early guarantees
of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on
preconfirmations. We present the core terms and definitions needed to
understand preconfirmations, outline a general framework for preconfirmation
protocols, and explore the economics and risks of preconfirmations. Finally, we
survey and apply our framework to several implementations of real-world
preconfirmation protocols, bridging the gap between theory and practice.

</details>


### [163] [PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference](https://arxiv.org/abs/2510.02395)
*Hongbo Liu,Jiannong Cao,Bo Yang,Dongbin Bai,Yinfeng Cao,Xiaoming Shen,Yinan Zhang,Jinwen Liang,Shan Jiang,Mingjin Zhang*

Main category: cs.CR

TL;DR: PolyLink是一个基于区块链的去中心化AI平台，旨在解决大型语言模型服务集中化带来的信任和成本问题，通过去中心化众包架构支持异构边缘设备的模型部署和推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型服务的部署和使用高度集中化，给终端用户和开发者带来了显著的信任问题和成本负担，需要去中心化的解决方案。

Method: 提出PolyLink平台，采用去中心化众包架构支持单设备和跨设备模型部署；设计TIQE协议结合轻量级交叉编码器和LLM-as-a-Judge进行高精度推理评估；集成基于代币的激励机制。

Result: 通过地理分布式部署在异构设备上进行实际评估，结果显示推理和验证延迟实用，安全分析表明系统能够抵抗模型退化攻击和验证器损坏。

Conclusion: PolyLink成功构建了一个实用的去中心化AI平台，解决了LLM服务集中化的问题，现已开源可用。

Abstract: The rapid advancement of large language models (LLMs) in recent years has
revolutionized the AI landscape. However, the deployment model and usage of LLM
services remain highly centralized, creating significant trust issues and costs
for end users and developers. To address these issues, we propose PolyLink, a
blockchain-based decentralized AI platform that decentralizes LLM development
and inference. Specifically, PolyLink introduces a decentralized crowdsourcing
architecture that supports single-device and cross-device model deployment and
inference across heterogeneous devices at the edge. Moreover, to ensure the
inference integrity, we design the TIQE protocol, which combines a lightweight
cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference
evaluation. Lastly, we integrate a comprehensive token-based incentive model
with dynamic pricing and reward mechanisms for all participants. We have
deployed PolyLink and conducted an extensive real-world evaluation through
geo-distributed deployment across heterogeneous devices. Results indicate that
the inference and verification latency is practical. Our security analysis
demonstrates that the system is resistant to model degradation attacks and
validator corruptions. PolyLink is now available at
https://github.com/IMCL-PolyLink/PolyLink.

</details>


### [164] [On The Fragility of Benchmark Contamination Detection in Reasoning Models](https://arxiv.org/abs/2510.02386)
*Han Wang,Haoyu Li,Brian Ko,Huan Zhang*

Main category: cs.CR

TL;DR: 研究发现，大型推理模型(LRMs)的基准污染检测存在严重漏洞，模型开发者可以通过简单的训练技巧轻松规避现有检测方法，从而在排行榜上获得虚高的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前LRM排行榜促使开发者直接在基准套件上优化模型，而将评估基准纳入训练数据是获得更高排名的捷径，这导致了基准污染问题。研究旨在揭示LRM基准污染检测的脆弱性。

Method: 研究分析了两种实际污染场景：(I)基础模型通过SFT和RL演变为LRM时，发现GRPO训练能显著掩盖污染信号；(II)对先进LRM应用带CoT的SFT污染时，现有检测方法几乎随机猜测。通过实证实验和理论分析探究了检测失效的根本原因。

Result: PPO风格的重要性采样和裁剪目标是检测掩盖的根源，表明广泛的RL方法具有类似的掩盖能力。污染后的LRM对与训练集分布相似的未见样本仍有更高置信度，从而规避基于记忆的检测方法。

Conclusion: 研究揭示了LRM评估的独特脆弱性：模型开发者可轻易污染LRM以获得虚高的排行榜性能，同时留下最少污染痕迹，严重损害评估公平性和公共排行榜的完整性，亟需针对LRM的先进污染检测方法和可信评估协议。

Abstract: Leaderboards for LRMs have turned evaluation into a competition,
incentivizing developers to optimize directly on benchmark suites. A shortcut
to achieving higher rankings is to incorporate evaluation benchmarks into the
training data, thereby yielding inflated performance, known as benchmark
contamination. Surprisingly, our studies find that evading contamination
detections for LRMs is alarmingly easy. We focus on the two scenarios where
contamination may occur in practice: (I) when the base model evolves into LRM
via SFT and RL, we find that contamination during SFT can be originally
identified by contamination detection methods. Yet, even a brief GRPO training
can markedly conceal contamination signals that most detection methods rely on.
Further empirical experiments and theoretical analysis indicate that PPO style
importance sampling and clipping objectives are the root cause of this
detection concealment, indicating that a broad class of RL methods may
inherently exhibit similar concealment capability; (II) when SFT contamination
with CoT is applied to advanced LRMs as the final stage, most contamination
detection methods perform near random guesses. Without exposure to non-members,
contaminated LRMs would still have more confidence when responding to those
unseen samples that share similar distributions to the training set, and thus,
evade existing memorization-based detection methods. Together, our findings
reveal the unique vulnerability of LRMs evaluations: Model developers could
easily contaminate LRMs to achieve inflated leaderboards performance while
leaving minimal traces of contamination, thereby strongly undermining the
fairness of evaluation and threatening the integrity of public leaderboards.
This underscores the urgent need for advanced contamination detection methods
and trustworthy evaluation protocols tailored to LRMs.

</details>


### [165] [LLM-Generated Samples for Android Malware Detection](https://arxiv.org/abs/2510.02391)
*Nik Rollinson,Nikolaos Polatidis*

Main category: cs.CR

TL;DR: 本研究探索使用GPT-4.1-mini生成Android恶意软件数据，评估合成数据在恶意软件检测中的效果。结果显示合成数据能增强稀缺数据集而不影响检测精度，但单独使用效果有限。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件通过混淆和多态性不断进化，给基于签名的防御和机器学习模型带来挑战。合成数据被提出作为数据稀缺的解决方案，但大型语言模型在生成有效恶意软件数据方面的作用尚未充分探索。

Method: 使用KronoDroid数据集对GPT-4.1-mini进行微调，生成三个恶意软件家族的结构化记录。通过提示工程和后处理解决生成不一致问题，在三种设置下评估多个分类器：仅真实数据、真实加合成数据、仅合成数据。

Result: 仅使用真实数据训练达到接近完美的检测效果，添加合成数据后性能保持高位仅有轻微下降。仅使用合成数据训练效果不一，效果因恶意软件家族和微调策略而异。

Conclusion: LLM生成的恶意软件数据可以增强稀缺数据集而不损害检测准确性，但作为独立训练源仍不足够。

Abstract: Android malware continues to evolve through obfuscation and polymorphism,
posing challenges for both signature-based defenses and machine learning models
trained on limited and imbalanced datasets. Synthetic data has been proposed as
a remedy for scarcity, yet the role of large language models (LLMs) in
generating effective malware data for detection tasks remains underexplored. In
this study, we fine-tune GPT-4.1-mini to produce structured records for three
malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the
KronoDroid dataset. After addressing generation inconsistencies with prompt
engineering and post-processing, we evaluate multiple classifiers under three
settings: training with real data only, real-plus-synthetic data, and synthetic
data alone. Results show that real-only training achieves near perfect
detection, while augmentation with synthetic data preserves high performance
with only minor degradations. In contrast, synthetic-only training produces
mixed outcomes, with effectiveness varying across malware families and
fine-tuning strategies. These findings suggest that LLM-generated malware can
enhance scarce datasets without compromising detection accuracy, but remains
insufficient as a standalone training source.

</details>


### [166] [Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense](https://arxiv.org/abs/2510.02424)
*Basil Abdullah AL-Zahrani*

Main category: cs.CR

TL;DR: CADL是一个自适应欺骗框架，在CICIDS2017数据集上达到99.88%检测率和0.13%误报率，显著优于传统入侵检测系统。


<details>
  <summary>Details</summary>
Motivation: 提供比商业欺骗平台更经济高效的替代方案，传统入侵检测系统检测率较低且成本高昂。

Method: 采用集成机器学习（随机森林、XGBoost、神经网络）结合行为分析，通过协调信号总线架构实现安全组件实时情报共享。

Result: 在5万个测试样本上，CADL检测率99.88%，误报率0.13%，行为分析对攻击者画像分类准确率达89%。

Conclusion: CADL框架提供了开源、透明的替代方案，显著提升了入侵检测性能，同时保持生产就绪的误报率水平。

Abstract: This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive
deception framework achieving 99.88% detection rate with 0.13% false positive
rate on the CICIDS2017 dataset. The framework employs ensemble machine learning
(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to
identify and adapt responses to network intrusions. Through a coordinated
signal bus architecture, security components share real-time intelligence,
enabling collective decision-making. The system profiles attackers based on
temporal patterns and deploys customized deception strategies across five
escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates
that CADL significantly outperforms traditional intrusion detection systems
(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false
positive rates. The framework's behavioral analysis achieves 89% accuracy in
classifying attacker profiles. We provide open-source implementation and
transparent performance metrics, offering an accessible alternative to
commercial deception platforms costing $150-400 per host annually.

</details>


### [167] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 提出一种基于压缩/未压缩神经网络对行为的统计方法，用于实时对抗攻击检测，在多种攻击类型上实现近乎完美的检测效果并显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击检测方法难以有效检测未知攻击类型，且对不同攻击类型的检测准确率有限，需要一种更可靠的实时检测方案。

Method: 通过比较压缩和未压缩神经网络对的行为，生成对抗攻击存在性的度量指标，建立部署前的检测基线。

Result: 与最先进技术相比，该方法在多种攻击类型上实现近乎完美的检测效果，同时显著减少了误报率。

Conclusion: 该方法既可靠又实用，适用于现实世界的对抗攻击检测应用。

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [168] [Joint Bidding on Intraday and Frequency Containment Reserve Markets](https://arxiv.org/abs/2510.03209)
*Yiming Zhang,Wolfgang Ridinger,David Wozabal*

Main category: q-fin.CP

TL;DR: 提出一种优化电池储能系统在多个电力市场中参与的新方法，将一次调频备用市场与日内连续交易相结合，使用混合整数线性规划和学习分类器策略，在德国市场数据上验证了比静态策略提高4%利润的效果。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成增加供应波动性，电池储能系统成为平衡供需的可行方案。现有文献通常孤立考虑不同市场或简化日内交易的连续性，存在研究空白。

Method: 采用混合整数线性规划实现日内决策和荷电状态恢复的滚动内在算法，结合学习分类器策略确定市场间最优容量分配。

Result: 在超过一年的德国历史市场数据上进行样本外回测验证，学习分类器策略比最佳静态策略提高总利润超过4%，比简单动态基准提高超过3%，与理论完美预见策略的差距缩小到仅4%。

Conclusion: 该方法证明了在复杂多市场环境中，基于学习的动态分配策略的有效性，能够显著提升电池储能系统的经济效益。

Abstract: As renewable energy integration increases supply variability, battery energy
storage systems (BESS) present a viable solution for balancing supply and
demand. This paper proposes a novel approach for optimizing battery BESS
participation in multiple electricity markets. We develop a joint bidding
strategy that combines participation in the primary frequency reserve market
with continuous trading in the intraday market, addressing a gap in the extant
literature which typically considers these markets in isolation or simplifies
the continuous nature of intraday trading. Our approach utilizes a mixed
integer linear programming implementation of the rolling intrinsic algorithm
for intraday decisions and state of charge recovery, alongside a learned
classifier strategy (LCS) that determines optimal capacity allocation between
markets. A comprehensive out-of-sample backtest over more than one year of
historical German market data validates our approach: The LCS increases overall
profits by over 4% compared to the best-performing static strategy and by more
than 3% over a naive dynamic benchmark. Crucially, our method closes the gap to
a theoretical perfect foresight strategy to just 4%, demonstrating the
effectiveness of dynamic, learning-based allocation in a complex, multi-market
environment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [169] [WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis](https://arxiv.org/abs/2510.02320)
*Yongqi Kang,Yong Zhao*

Main category: eess.AS

TL;DR: 提出了WEE-Therapy模型，通过弱编码器集成机制和多任务学习，显著提升了心理咨询对话分析中情感识别、技术分类、风险检测和摘要等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型依赖单一语音编码器，难以捕捉心理咨询领域特有的复杂情感和专业技巧等特征，限制了AI在临床分析中的应用。

Method: 采用弱编码器集成机制，在强大基础编码器基础上集成多个轻量级专业编码器，并通过双路由策略结合稳定的领域知识和动态的专家选择。

Result: 在情感识别、技术分类、风险检测和摘要等任务上均取得显著性能提升，且参数开销最小。

Conclusion: WEE-Therapy展示了在AI辅助临床分析中的强大潜力，为计算心理学的发展提供了有效工具。

Abstract: The advancement of computational psychology requires AI tools capable of
deeply understanding counseling dialogues. Existing audio language models
(AudioLLMs) often rely on single speech encoders pre-trained on general data,
struggling to capture domain-specific features like complex emotions and
professional techniques. To address this, we propose WEE-Therapy, a multi-task
AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This
supplements a powerful base encoder with a pool of lightweight, specialized
encoders. A novel dual-routing strategy combines stable, data-independent
domain knowledge with dynamic, data-dependent expert selection. Evaluated on
emotion recognition, technique classification, risk detection, and
summarization, WEE-Therapy achieves significant performance gains across all
tasks with minimal parameter overhead, demonstrating strong potential for
AI-assisted clinical analysis.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [170] [An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels](https://arxiv.org/abs/2510.02355)
*Yubo Zhang,Jeremy Johnston,Xiaodong Wang*

Main category: eess.SY

TL;DR: 提出了一个端到端的深度学习框架用于大规模稀疏MIMO信道中的下行波束成形，核心是深度EDN架构，包含编码器、波束成形解码器和信道解码器三个模块。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中下行波束成形的复杂性和高反馈开销问题，通过深度学习实现高效的信道压缩和波束成形设计。

Method: 采用深度EDN架构：用户端编码器压缩信道为低维潜在向量，基站端波束成形解码器映射潜在向量到波束成形器，信道解码器重构信道以优化波束成形。训练策略包括半摊销学习和知识蒸馏。

Result: 通过大量仿真验证了该框架在不同网络和信道条件下的有效性，适用于远场和近场混合波束成形场景。

Conclusion: 该深度学习框架能够有效降低反馈开销，同时实现高性能的波束成形，为大规模MIMO系统提供了实用的解决方案。

Abstract: We develop an end-to-end deep learning framework for downlink beamforming in
large-scale sparse MIMO channels. The core is a deep EDN architecture with
three modules: (i) an encoder NN, deployed at each user end, that compresses
estimated downlink channels into low-dimensional latent vectors. The latent
vector from each user is compressed and then fed back to the BS. (ii) a
beamformer decoder NN at the BS that maps recovered latent vectors to
beamformers, and (iii) a channel decoder NN at the BS that reconstructs
downlink channels from recovered latent vectors to further refine the
beamformers. The training of EDN leverages two key strategies: (a)
semi-amortized learning, where the beamformer decoder NN contains an analytical
gradient ascent during both training and inference stages, and (b) knowledge
distillation, where the loss function consists of a supervised term and an
unsupervised term, and starting from supervised training with MMSE beamformers,
over the epochs, the model training gradually shifts toward unsupervised using
the sum-rate objective. The proposed EDN beamforming framework is extended to
both far-field and near-field hybrid beamforming scenarios. Extensive
simulations validate its effectiveness under diverse network and channel
conditions.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [171] [NEURODNAAI: Neural pipeline approaches for the advancing dna-based information storage as a sustainable digital medium using deep learning framework](https://arxiv.org/abs/2510.02417)
*Rakesh Thakur,Lavanya Singh,Yashika,Manomay Bundawala,Aruna Kumar*

Main category: cs.ET

TL;DR: NeuroDNAAI是一个基于深度学习的DNA存储框架，通过量子并行概念增强编码多样性和容错性，在噪声信道中实现高保真数据重建，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: DNA作为数字信息存储介质具有高密度和耐久性优势，但合成成本、测序错误和生物约束（GC含量不平衡、同聚物）限制了实际部署。

Method: 整合生物学约束与深度学习，利用量子并行概念增强编码多样性和容错性，通过噪声信道传输DNA序列并重建数据。

Result: 在基准数据集上的实验显示，NeuroDNAAI在文本和图像数据上均实现低比特错误率，显著优于传统提示或基于规则的方案。

Conclusion: 通过统一理论、工作流程和模拟，NeuroDNAAI实现了可扩展且生物学有效的档案DNA存储。

Abstract: DNA is a promising medium for digital information storage for its exceptional
density and durability. While prior studies advanced coding theory, workflow
design, and simulation tools, challenges such as synthesis costs, sequencing
errors, and biological constraints (GC-content imbalance, homopolymers) limit
practical deployment. To address this, our framework draws from quantum
parallelism concepts to enhance encoding diversity and resilience, integrating
biologically informed constraints with deep learning to enhance error
mitigation in DNA storage. NeuroDNAAI encodes binary data streams into symbolic
DNA sequences, transmits them through a noisy channel with substitutions,
insertions, and deletions, and reconstructs them with high fidelity. Our
results show that traditional prompting or rule-based schemes fail to adapt
effectively to realistic noise, whereas NeuroDNAAI achieves superior accuracy.
Experiments on benchmark datasets demonstrate low bit error rates for both text
and images. By unifying theory, workflow, and simulation into one pipeline,
NeuroDNAAI enables scalable, biologically valid archival DNA storage

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [172] [Simple Quantum Algorithm for Approximate $k$-Mismatch Problem](https://arxiv.org/abs/2510.02399)
*Ruhan Habib*

Main category: quant-ph

TL;DR: 提出了一个简单的量子算法，用于近似解决k-不匹配问题，在给定参数ε的情况下，只有当子串是(1+ε)k-不匹配时才返回匹配，时间复杂度为Õ(ε⁻¹√(mn/k))


<details>
  <summary>Details</summary>
Motivation: k-不匹配问题在经典计算中自1982年以来一直被研究，最近Jin和Nogler以及Kociumaka等人开始在量子计算环境中研究该问题，需要开发高效的量子算法

Method: 设计了一个简单的量子算法，采用近似方法解决k-不匹配问题，算法只返回(1+ε)k-不匹配的匹配结果

Result: 算法成功解决了近似k-不匹配问题，当不返回任何匹配时保证不存在k-不匹配，实现了Õ(ε⁻¹√(mn/k))的时间复杂度

Conclusion: 该量子算法为k-不匹配问题提供了一个高效的近似解决方案，在量子计算环境中改进了现有方法的性能

Abstract: In the $k$-mismatch problem, given a pattern and a text of length $n$ and $m$
respectively, we have to find if the text has a sub-string with a Hamming
distance of at most $k$ from the pattern. This has been studied in the
classical setting since 1982 and recently in the quantum computational setting
by Jin and Nogler and Kociumaka, Nogler, and Wellnitz. We provide a simple
quantum algorithm that solves the problem in an approximate manner, given a
parameter $\epsilon \in (0, 1]$. It returns an occurrence as a match only if it
is a $\left(1+\epsilon\right)k$-mismatch. If it does not return any occurrence,
then there is no $k$-mismatch. This algorithm has a time (size) complexity of
$\tilde{O}\left( \epsilon^{-1} \sqrt{\frac{mn}{k}} \right)$.

</details>


### [173] [Scalable Quantum Optimisation using HADOF: Hamiltonian Auto-Decomposition Optimisation Framework](https://arxiv.org/abs/2510.02926)
*Namasi G Sankar,Georgios Miliotis,Simon Caton*

Main category: quant-ph

TL;DR: 提出了HADOF框架，通过迭代分解QUBO哈密顿量为子问题，在有限量子比特的NISQ设备上实现大规模组合优化问题的求解。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备的量子比特数量有限，限制了量子优化算法（如QA和QAOA）在实际组合优化问题中的应用。

Method: 使用迭代策略自动将QUBO哈密顿量分解为子哈密顿量，分别用QAOA、QA或模拟退火优化，然后聚合得到全局解。

Result: 与模拟退火和CPLEX精确求解器相比，HADOF能够扩展到远超可用量子比特数的问题规模，同时保持竞争性的精度和运行时间。

Conclusion: HADOF在IBM量子计算机上的实验显示了量子优化实际应用的潜力，为解决NISQ时代量子比特限制提供了可行方案。

Abstract: Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms
used for finding approximate solutions to combinatorial problems on near-term
NISQ systems. Many NP-hard problems can be reformulated as Quadratic
Unconstrained Binary Optimisation (QUBO), which maps naturally onto quantum
Hamiltonians. However, the limited qubit counts of current NISQ devices
restrict practical deployment of such algorithms. In this study, we present the
Hamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages
an iterative strategy to automatically divide the Quadratic Unconstrained
Binary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be
optimised separately using Hamiltonian based optimisers such as QAOA, QA or
Simulated Annealing (SA) and aggregated into a global solution. We compare
HADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing
scalability to problem sizes far exceeding available qubits while maintaining
competitive accuracy and runtime. Furthermore, we realise HADOF for a toy
problem on an IBM quantum computer, showing promise for practical applications
of quantum optimisation.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [174] [Heterogeneous Graph Representation of Stiffened Panels with Non-Uniform Boundary Conditions and Loads](https://arxiv.org/abs/2510.02472)
*Yuecheng Cai,Jasmin Jelovica*

Main category: cs.CE

TL;DR: 提出了一种基于异构图神经网络（HGNNs）的加筋板结构代理模型，通过异构图表示几何变化、非均匀边界条件和多样化载荷情况，使用异构图变换器（HGT）预测冯·米塞斯应力和位移场。


<details>
  <summary>Details</summary>
Motivation: 在结构分析和优化中需要高效的代理模型，传统方法难以处理几何变化、复杂边界条件和多样化载荷场景，因此需要开发能够准确捕捉结构行为的图神经网络方法。

Method: 将加筋板结构划分为多个结构单元（如筋板和板），每个单元用三种节点类型表示：几何节点、边界节点和载荷节点。通过引入局部方向和空间关系创建异构图边，采用异构图变换器（HGT）进行预测。

Result: 在承受面载荷的板和由加筋板组成的箱梁上进行数值测试，异构图表示相比同质图表示表现出更优越的性能，能够准确预测位移和冯·米塞斯应力，有效捕捉结构行为模式和最大值。

Conclusion: 异构图表示方法在加筋板结构分析中具有显著优势，能够有效处理几何变化和复杂边界条件，为结构优化提供了可靠的代理模型工具。

Abstract: Surrogate models are essential in structural analysis and optimization. We
propose a heterogeneous graph representation of stiffened panels that accounts
for geometrical variability, non-uniform boundary conditions, and diverse
loading scenarios, using heterogeneous graph neural networks (HGNNs). The
structure is partitioned into multiple structural units, such as stiffeners and
the plates between them, with each unit represented by three distinct node
types: geometry, boundary, and loading nodes. Edge heterogeneity is introduced
by incorporating local orientations and spatial relationships of the connecting
nodes. Several heterogeneous graph representations, each with varying degrees
of heterogeneity, are proposed and analyzed. These representations are
implemented into a heterogeneous graph transformer (HGT) to predict von Mises
stress and displacement fields across stiffened panels, based on loading and
degrees of freedom at their boundaries. To assess the efficacy of our approach,
we conducted numerical tests on panels subjected to patch loads and box beams
composed of stiffened panels under various loading conditions. The
heterogeneous graph representation was compared with a homogeneous counterpart,
demonstrating superior performance. Additionally, an ablation analysis was
performed to evaluate the impact of graph heterogeneity on HGT performance. The
results show strong predictive accuracy for both displacement and von Mises
stress, effectively capturing structural behavior patterns and maximum values.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [175] [Corrosion Risk Estimation for Heritage Preservation: An Internet of Things and Machine Learning Approach Using Temperature and Humidity](https://arxiv.org/abs/2510.02973)
*Reginald Juan M. Mercado,Muhammad Kabeer,Haider Al-Obaidy,Rosdiadee Nordin*

Main category: cs.CY

TL;DR: 开发了基于物联网和LoRa通信的腐蚀监测系统，使用温度和湿度数据通过机器学习预测钢结构的腐蚀速率，为文化遗产保护提供实时监控和防护建议。


<details>
  <summary>Details</summary>
Motivation: 菲律宾圣塞巴斯蒂安大教堂等文化遗产钢结构需要准确的腐蚀预测来进行主动保护，传统监测方法成本高且资源需求大。

Method: 构建物联网硬件系统结合LoRa无线通信，收集三年数据，建立仅使用温度和相对湿度数据的机器学习框架预测腐蚀速率，通过Streamlit仪表板和ngrok隧道提供公共访问。

Result: 成功开发出仅需基本气象数据就能准确预测腐蚀率的系统，证明高级回归分析可以从简单数据中提取有效预测信息。

Conclusion: 这种最小数据方法具有可扩展性和成本效益，适用于资源有限的遗产地，能够实现全球文化遗产钢结构的主动保护，无需复杂的传感器网络。

Abstract: Proactive preservation of steel structures at culturally significant heritage
sites like the San Sebastian Basilica in the Philippines requires accurate
corrosion forecasting. This study developed an Internet of Things hardware
system connected with LoRa wireless communications to monitor heritage
buildings with steel structures. From a three year dataset generated by the IoT
system, we built a machine learning framework for predicting atmospheric
corrosion rates using only temperature and relative humidity data. Deployed via
a Streamlit dashboard with ngrok tunneling for public access, the framework
provides real-time corrosion monitoring and actionable preservation
recommendations. This minimal-data approach is scalable and cost effective for
heritage sites with limited monitoring resources, showing that advanced
regression can extract accurate corrosion predictions from basic meteorological
data enabling proactive preservation of culturally significant structures
worldwide without requiring extensive sensor networks

</details>


### [176] [Sensors in viticulture: functions, benefits, and data-driven insights](https://arxiv.org/abs/2510.03000)
*Milan Milenkovic*

Main category: cs.CY

TL;DR: 传感器数据平台可为葡萄栽培提供数据驱动的决策支持，通过实时监测、预测和警报来优化灌溉、病虫害防治、冠层管理和收获时机等关键操作。


<details>
  <summary>Details</summary>
Motivation: 将传感器和分析预测与葡萄栽培者的观察和直觉相结合，提供数据支持的决策输入，实现精准葡萄栽培原则。

Method: 利用传感器数据平台提供实时测量、预测和警报，为葡萄园管理操作提供可操作的见解和建议。

Result: 传感器平台能够增强操作效果和效率，在不需要时节省劳动力和资源，并实现风险预警和干预。

Conclusion: 传感器数据平台在葡萄栽培中具有重要功能、效益和实际应用价值，对葡萄栽培者及农业和物联网研究人员具有参考意义。

Abstract: Use of sensor and related analytical predictions can be a powerful tool in
providing data-informed input to viticulturalists' decision process,
complementing their vineyard observations and intuition. Their up-to-date
measurements, predictions, and alerts offer actionable insights and suggestions
for managing key vineyard operations, such as irrigation, disease and pest
control, canopy management, and harvest timing. In many cases, anticipatory
interventions can mitigate risks before problems become apparent. By offering
guidance on the targeting, timing, and dosage of vineyard practices, sensor
data platforms can enhance operational effectiveness and efficiency while
conserving labor and resources when they are not required. They also enable
implementation of the principles of precision viticulture - doing the right
thing, at the right time, in the right place. This paper provides a succinct
summary of the functions, benefits, and practical considerations of sensor data
platforms in viticulture. It may be of interest to viticulturalists as well as
agricultural and IoT researchers.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [177] [Quantitative Convergence Analysis of Projected Stochastic Gradient Descent for Non-Convex Losses via the Goldstein Subdifferential](https://arxiv.org/abs/2510.02735)
*Yuping Zheng,Andrew Lamperski*

Main category: math.OC

TL;DR: 该论文分析了投影随机梯度下降(SGD)在非凸损失函数上的收敛性，提出了基于梯度到Goldstein次微分距离的收敛准则，无需方差缩减即可获得收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有投影SGD分析要么使用Moreau包络（改变了梯度性质），要么需要方差缩减方法才能保证收敛。本文旨在提出一个能直接与无约束SGD比较且无需方差缩减的收敛分析框架。

Method: 使用梯度到Goldstein次微分集合的距离作为收敛度量，分析投影SGD在紧凸集上对非凸损失函数的收敛性，考虑IID数据和混合数据两种情况。

Result: 获得了期望下的渐近收敛和O(N^{-1/3})非渐近界，对于IID次高斯数据还获得了几乎必然渐近收敛和高概率O(N^{-1/5})非渐近界。

Conclusion: 提出的收敛准则能直接还原到无约束情况，且无需方差缩减即可获得收敛保证，首次为投影SGD在非凸损失函数上提供了高概率非渐近界。

Abstract: Stochastic gradient descent (SGD) is the main algorithm behind a large body
of work in machine learning. In many cases, constraints are enforced via
projections, leading to projected stochastic gradient algorithms. In recent
years, a large body of work has examined the convergence properties of
projected SGD for non-convex losses in asymptotic and non-asymptotic settings.
Strong quantitative guarantees are available for convergence measured via
Moreau envelopes. However, these results cannot be compared directly with work
on unconstrained SGD, since the Moreau envelope construction changes the
gradient. Other common measures based on gradient mappings have the limitation
that convergence can only be guaranteed if variance reduction methods, such as
mini-batching, are employed. This paper presents an analysis of projected SGD
for non-convex losses over compact convex sets. Convergence is measured via the
distance of the gradient to the Goldstein subdifferential generated by the
constraints. Our proposed convergence criterion directly reduces to commonly
used criteria in the unconstrained case, and we obtain convergence without
requiring variance reduction. We obtain results for data that are independent,
identically distributed (IID) or satisfy mixing conditions ($L$-mixing). In
these cases, we derive asymptotic convergence and $O(N^{-1/3})$ non-asymptotic
bounds in expectation, where $N$ is the number of steps. In the case of IID
sub-Gaussian data, we obtain almost-sure asymptotic convergence and
high-probability non-asymptotic $O(N^{-1/5})$ bounds. In particular, these are
the first non-asymptotic high-probability bounds for projected SGD with
non-convex losses.

</details>


### [178] [Improving Online-to-Nonconvex Conversion for Smooth Optimization via Double Optimism](https://arxiv.org/abs/2510.03167)
*Francisco Patitucci,Ruichen Jiang,Aryan Mokhtari*

Main category: math.OC

TL;DR: 本文提出了一种基于双重乐观提示函数的在线乐观梯度方法，改进了非凸优化的在线到非凸转换框架，消除了双循环结构和对数因子，在确定性和随机设置下实现了统一的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的在线到非凸转换框架存在三个主要问题：确定性方法需要复杂的双循环结构来构建提示向量，引入了额外的对数因子；随机方法需要更强的二阶矩假设；两种设置使用不同的在线学习算法。

Method: 提出基于双重乐观提示函数的在线乐观梯度方法，使用外推点的梯度作为提示，基于两个乐观假设：提示与目标梯度之差保持近似恒定，且连续更新方向因平滑性而变化缓慢。

Result: 新方法消除了双循环需求和对数因子，在标准方差有界假设下，获得了统一的复杂度 O(ε^{-1.75} + σ²ε^{-3.5})，平滑地插值了最佳确定性速率和最优随机速率。

Conclusion: 通过引入双重乐观提示函数，本文解决了现有框架的局限性，提供了更简单、更统一的非凸优化算法，在确定性和随机设置下都实现了最优或接近最优的复杂度。

Abstract: A recent breakthrough in nonconvex optimization is the online-to-nonconvex
conversion framework of \cite{cutkosky2023optimal}, which reformulates the task
of finding an $\varepsilon$-first-order stationary point as an online learning
problem. When both the gradient and the Hessian are Lipschitz continuous,
instantiating this framework with two different online learners achieves a
complexity of $\mathcal{O}(\varepsilon^{-1.75}\log(1/\varepsilon))$ in the
deterministic case and a complexity of $\mathcal{O}(\varepsilon^{-3.5})$ in the
stochastic case. However, this approach suffers from several limitations: (i)
the deterministic method relies on a complex double-loop scheme that solves a
fixed-point equation to construct hint vectors for an optimistic online
learner, introducing an extra logarithmic factor; (ii) the stochastic method
assumes a bounded second-order moment of the stochastic gradient, which is
stronger than standard variance bounds; and (iii) different online learning
algorithms are used in the two settings. In this paper, we address these issues
by introducing an online optimistic gradient method based on a novel
\textit{doubly optimistic hint function}. Specifically, we use the gradient at
an extrapolated point as the hint, motivated by two optimistic assumptions:
that the difference between the hint and the target gradient remains near
constant, and that consecutive update directions change slowly due to
smoothness. Our method eliminates the need for a double loop and removes the
logarithmic factor. Furthermore, by simply replacing full gradients with
stochastic gradients and under the standard assumption that their variance is
bounded by $\sigma^2$, we obtain a unified algorithm with complexity
$\mathcal{O}(\varepsilon^{-1.75} + \sigma^2 \varepsilon^{-3.5})$, smoothly
interpolating between the best-known deterministic rate and the optimal
stochastic rate.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [179] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: 论文探讨了在凸优化中使用扩展浮点精度来加速大规模Max-Cut问题的求解，特别是在使用共轭梯度等间接矩阵求逆方法时，扩展精度能显著减少求解时间。


<details>
  <summary>Details</summary>
Motivation: Max-Cut问题已成为评估量子与经典优化器局部搜索启发式算法的基准。与仅提供平均性能保证的局部搜索不同，Goemans-Williamson的凸半定松弛方法提供最坏情况保证，适用于构建基准和性能关键场景。

Method: 将扩展浮点精度融入凸优化的代数子程序中，特别是在使用共轭梯度等间接矩阵求逆方法时。这些方法在内部点法中用于处理超大规模问题，具有比直接方法更低的复杂度。

Result: 使用扩展精度时，求解时间会减少一个随系统规模增大的因子。在支持扩展精度的硬件架构上，能获得预期的加速效果。

Conclusion: 扩展浮点精度在大规模凸优化问题中，特别是使用间接矩阵求逆方法时，能有效加速求解过程，且加速效果随问题规模增大而增强。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [180] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 提出基于动态专家聚类和结构化压缩的统一框架，解决MoE LLMs中的负载不平衡、参数冗余和通信开销问题，在保持模型质量的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决MoE大语言模型面临的三个核心问题：负载不平衡、参数冗余和通信开销，这些因素限制了MoE模型的可扩展性和效率。

Method: 使用动态专家聚类方法，基于参数和激活相似性定期重组专家；在集群内将专家权重分解为共享基础矩阵和极低秩残差适配器；采用两阶段分层路由策略和异构精度方案。

Result: 在GLUE和WikiText-103上的评估显示，该框架在匹配标准MoE模型质量的同时，总参数减少约80%，吞吐量提升10-20%，专家负载方差降低超过三倍。

Conclusion: 结构重组是实现可扩展、高效和内存有效的MoE LLMs的原则性路径，通过动态架构重配置和结构化压缩可显著提升模型效率。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [181] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 该论文质疑当前数学基准测试对LLM推理能力的评估，指出现有基准主要来自IMO等知名竞赛，可能存在数据污染和问题类型单一的问题。作者提出了EEFSUVA新基准，使用东欧和前苏联地区较少流通的奥数题目，发现即使是先进LLM在该基准上表现也显著下降。


<details>
  <summary>Details</summary>
Motivation: 质疑当前LLM在数学基准测试上的优异表现是否真实反映了其推理能力，因为现有基准主要来自IMO等知名竞赛，可能存在数据污染和问题类型单一的问题，无法全面评估数学推理能力。

Method: 引入EEFSUVA新基准，该基准从东欧和前苏联地区较少流通的区域性和国家级奥数竞赛中精心挑选题目，这些题目难度与IMO相当但需要非标准解题技巧，且在线语料库中较少出现。

Result: 初步结果显示，即使是先进的LLM在EEFSUVA基准上的表现也显著低于其他奥数风格基准，表明现有基准可能高估了LLM的数学推理能力。

Conclusion: 需要更广泛的评估数据集来全面评估数学推理能力，并指导未来模型开发，当前基准可能无法真实反映LLM的数学推理水平。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [182] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 提出了一种基于表示和梯度的新框架，用于诊断LLM的不良行为，包括有害内容、后门投毒和知识污染等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数梯度的归因方法存在噪声信号和计算复杂性问题，难以有效诊断LLM不良行为的根本原因。

Method: 直接在模型激活空间中分析表示及其梯度，提供语义上有意义的信号，将输出与训练数据联系起来。

Result: 该方法在样本级归因方面表现优异，还能进行细粒度的token级分析，精确识别影响模型行为的特定样本和短语。

Conclusion: 这项工作为理解、审计和减轻LLM相关风险提供了一个强大的诊断工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [183] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ是一个多维评估框架，专门用于评估文档在五个特质上的表现：连贯性、严谨性、适当性、完整性和质量。该框架基于特征自动作文评分方法，扩展到机器生成文本评估，提供基于评分标准的可解释评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多为单一分数，缺乏对文档多个维度的深入分析。CRACQ旨在提供更稳定、可解释的多维度评估，超越直接使用LLM作为评判的方法。

Method: 基于特征自动作文评分方法，整合语言、语义和结构信号进行累积评估。在500个合成资助提案上训练，并与LLM作为评判的方法进行基准测试。

Result: 初步结果表明，CRACQ比直接LLM评估产生更稳定和可解释的特征级别判断，但在可靠性和领域范围方面仍存在挑战。

Conclusion: CRACQ提供了一个有前景的多维度文档评估框架，能够进行整体和特征级别的分析，但需要进一步改进可靠性和扩展领域适用性。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [184] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 本文研究了通过提示让大语言模型模拟早期知识截止日期的能力，发现提示方法在直接查询特定日期后信息时有效，但在处理因果相关但未直接询问的遗忘内容时效果不佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间预测任务中广泛使用，但其依赖预训练数据引发污染担忧，准确预测可能反映记忆而非推理，导致对其泛化能力的高估。随着基于提示的遗忘技术出现，需要研究LLMs能否通过提示模拟早期知识截止。

Method: 构建三个评估数据集来评估LLMs遗忘能力：(1)直接事实知识，(2)语义变化，(3)因果相关知识。使用提示技术模拟早期知识截止。

Result: 结果显示，基于提示的模拟知识截止在直接查询特定日期后信息时有效，但在遗忘内容与查询因果相关但未直接询问时难以诱导遗忘。

Conclusion: 这些发现强调了在将LLMs应用于时间预测任务时需要更严格的评估设置。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [185] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 提出了mini-vec2vec方法，这是一个高效且稳定的文本嵌入空间对齐替代方案，相比原有的vec2vec方法计算成本显著降低，且学习到的映射是线性变换。


<details>
  <summary>Details</summary>
Motivation: vec2vec方法虽然能找到近乎完美的对齐，但计算昂贵且不稳定，需要更简单高效的替代方案。

Method: 包含三个主要阶段：伪平行嵌入向量的初步匹配、变换拟合和迭代优化，学习到的映射是线性变换。

Result: mini-vec2vec在效率上比原始vec2vec方法高出几个数量级，同时达到或超过其性能结果。

Conclusion: 该方法具有稳定性和可解释的算法步骤，便于扩展，为在新领域和学科中的应用开辟了新机会。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [186] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 该研究应用AI和大型语言模型改进塞内加尔司法系统中法律文本的访问，成功提取了7,967条法律条款，构建了包含2,872个节点和10,774个关系的图数据库，展示了GPT-4o、GPT-4和Mistral-Large等模型在法律知识提取中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决塞内加尔司法系统中法律文档提取和组织的困难，改善司法信息访问，帮助公民和法律专业人士更好地理解权利和义务。

Method: 使用AI和大型语言模型提取法律文本，构建详细图数据库，应用高级三元组提取技术，利用GPT-4o、GPT-4和Mistral-Large等模型识别关系和元数据。

Result: 成功提取7,967条法律条款，构建包含2,872个节点和10,774个关系的图数据库，有效展示了法律文本间的互连关系。

Conclusion: 通过AI技术成功建立了支持塞内加尔公民和法律专业人士更有效理解权利和责任的坚实框架。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [187] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 通过视觉和语言模型研究语言皮层的抽象意义表征，发现多图像聚合和多释义平均能提高神经响应预测准确度，证明语言系统存在高度抽象的形式独立意义表征。


<details>
  <summary>Details</summary>
Motivation: 探讨语言皮层中意义表征的抽象程度，验证是否存在独立于语言形式的抽象意义表征。

Method: 使用视觉和语言模型建模句子神经响应，通过生成多张对应图像提取视觉嵌入，以及平均多个释义的嵌入来预测语言皮层活动。

Result: 多图像聚合和多释义平均均能提高预测准确度，丰富上下文细节的释义甚至优于原始句子嵌入，表明语言系统具有比语言模型更丰富的语义表征。

Conclusion: 语言皮层中存在高度抽象、形式独立的意义表征，其语义表征比语言模型更丰富和广泛。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [188] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 提出了一种内存增强的小型语言模型架构，通过访问大型分层参数化内存库来存储世界知识，在推理时仅提取少量上下文相关内存块，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型依赖参数扩展来提升性能，但将所有世界知识压缩到参数中既没必要（每个提示只使用一小部分知识）也不实用（边缘设备内存和计算资源有限）。

Method: 采用内存增强架构和与现有硬件范式对齐的预训练策略，构建小型语言模型访问大型分层参数化内存库，在预训练和推理时提取少量上下文相关内存块并添加到模型中。

Result: 160M参数模型增强18M参数内存（从4.6B内存库提取）可获得与超过2倍参数常规模型相当的性能，实验将参数化内存扩展到超过21B参数，证明该方法在不同transformer架构中均有效。

Conclusion: 分层前馈内存在不同transformer架构中都能稳健工作，无论是预训练时添加还是事后添加，为构建高效的小型语言模型提供了可行方案。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [189] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出了一种基于校准对数似然评分的高效方法，从多个不同LLM中选择最佳响应，无需外部验证器或人工评估


<details>
  <summary>Details</summary>
Motivation: 在多LLM系统中，现有方法依赖昂贵的外部验证器或自一致性技术，而多LLM系统虽然能产生更多样化的响应，但性能往往不如单LLM自一致性

Method: 使用校准的对数似然评分来隐式利用这些模型的固有知识和置信度，从多个不同LLM中选择最佳响应

Result: 在GSM8K、MMLU（6个子集）和ARC数据集上，在辩论和非辩论设置下分别实现了约4%、3%和5%的性能提升

Conclusion: 该方法提供了一种原则性、新颖且计算高效的方法，能够有效利用多LLM系统的潜力，在资源受限环境中实现更好的响应选择

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [190] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过简单的提示工程，可以让纯文本训练的LLM激活与视觉和听觉编码器更接近的表征，即使没有实际的多模态输入。


<details>
  <summary>Details</summary>
Motivation: 测试纯文本训练的LLM是否包含隐含的多模态规律，以及是否可以通过感官提示来激活这些潜在结构。

Method: 使用感官提示（如'看'或'听'）来引导LLM，使其在下一个token预测时表现得好像有条件于潜在的视觉或听觉证据。

Result: 发现轻量级的提示工程能够可靠地激活纯文本训练LLM中与模态相适应的表征。

Conclusion: 纯文本训练的LLM内部表征确实隐含了多模态规律，通过适当的提示可以有效地激活这些表征。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [191] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 提出了一个基于概率上下文无关文法（PCFG）的新框架来研究语言模型如何学习语法，通过合成语言训练小模型来精确控制语法复杂性、递归深度和子语法结构。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的学习动态，特别是它们如何获取语法知识，目前仍知之甚少。

Method: 使用PCFG生成的合成语言训练小型transformer模型，通过控制语法复杂性和递归深度来研究学习动态，并推导了训练损失和KL散度的递归公式。

Result: 发现transformer模型并行减少所有子语法的损失（与儿童学习不同），子语法预训练可改善小模型的最终损失，模型在深层递归结构上表现困难。

Conclusion: PCFG为研究语言模型学习提供了多功能测试平台，揭示了神经网络在表示层次语法方面的基本挑战。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [192] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 提出了一种用于上下文问答任务的不确定性量化方法，通过理论推导分离出认知不确定性，并将其解释为语义特征差距，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化研究主要关注封闭式事实问答，而上下文问答在现实应用中很重要但尚未被探索。

Method: 提出了任务无关的令牌级不确定性度量，通过分解分离认知不确定性，用完美提示的理想模型近似真实分布，并推导出认知不确定性的上界，将其解释为语义特征差距。

Result: 在多个问答基准测试中，在分布内和分布外设置下都显著优于最先进的无监督和监督不确定性量化方法，实现了高达13个百分点的PRR改进，且推理开销可忽略。

Conclusion: 该方法为上下文问答任务提供了理论上有依据的不确定性量化框架，通过语义特征差距有效捕捉认知不确定性，性能优于现有方法。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [193] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文首次对对话AI鲁棒性进行生存分析，发现突发语义漂移会显著增加对话失败风险，而渐进累积漂移则具有保护作用，能显著延长对话持续时间。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注静态基准和单轮评估，无法捕捉真实对话中的时间动态退化特性，需要新的方法来理解LLM在多轮对话中的鲁棒性。

Method: 采用生存分析框架，分析36,951个对话轮次，使用Cox比例风险模型、加速失效时间模型和随机生存森林方法，将对话失败建模为时间到事件过程。

Result: AFT交互模型表现最佳，具有优异的区分度和校准能力。突发语义漂移会灾难性地增加对话失败风险，而渐进累积漂移则能大幅降低失败风险。

Conclusion: 生存分析是评估LLM鲁棒性的强大范式，为设计弹性对话代理提供了具体见解，并挑战了对话AI系统中语义一致性必要性的普遍假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [194] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 提出Cache-to-Cache（C2C）新范式，通过直接语义通信替代文本通信，在多LLM系统中实现更高质量和更低延迟的协作。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM系统通过文本通信会丢失丰富的语义信息并产生token-by-token生成延迟，需要探索超越文本的通信方式。

Method: 使用神经网络投影和融合源模型与目标模型的KV-cache，实现直接语义传输，并通过可学习门控机制选择受益于缓存通信的目标层。

Result: C2C比单个模型平均准确率高8.5-10.5%，比文本通信范式高约3.0-5.0%，同时平均延迟加速2.0倍。

Conclusion: KV-cache是有效的模型间通信媒介，C2C范式能够充分利用模型的深层专业语义，避免中间文本生成，显著提升多LLM系统性能。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [195] [Linear RNNs for autoregressive generation of long music samples](https://arxiv.org/abs/2510.02401)
*Konrad Szewczyk,Daniel Gallo Fernández,James Townsend*

Main category: cs.SD

TL;DR: 论文提出了HarmonicRNN模型，通过改进线性RNN架构和上下文并行化技术，在原始音频波形生成任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 直接自回归生成音频波形具有挑战性，因为原始序列长度大且存在多时间尺度的重要结构。传统方法如RNN、因果卷积和自注意力机制在此任务上效果有限。

Method: 采用深度状态空间模型（线性RNN），研究不同架构选择，并利用上下文并行化技术训练长达1分钟（100万个token）的序列。

Result: HarmonicRNN在小规模数据集上获得了最先进的似然度和感知指标。

Conclusion: 线性RNN在原始音频建模中具有高效性，通过适当的架构改进和并行化技术可以显著提升性能。

Abstract: Directly learning to generate audio waveforms in an autoregressive manner is
a challenging task, due to the length of the raw sequences and the existence of
important structure on many different timescales. Traditional approaches based
on recurrent neural networks, as well as causal convolutions and
self-attention, have only had limited success on this task. However, recent
work has shown that deep state space models, also referred to as linear RNNs,
can be highly efficient in this context. In this work, we push the boundaries
of linear RNNs applied to raw audio modeling, investigating the effects of
different architectural choices and using context-parallelism to enable
training on sequences up to one minute (1M tokens) in length. We present a
model, HarmonicRNN, which attains state of the art log-likelihoods and
perceptual metrics on small-scale datasets.

</details>


### [196] [WavInWav: Time-domain Speech Hiding via Invertible Neural Network](https://arxiv.org/abs/2510.02915)
*Wei Fan,Kejiang Chen,Xiangkun Wang,Weiming Zhang,Nenghai Yu*

Main category: cs.SD

TL;DR: 提出了一种基于流可逆神经网络的音频隐写方法，通过时频损失函数和加密技术，在保持隐写音频质量的同时提高了秘密音频的恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有音频隐写方法在恢复秘密音频时质量不佳，主要由于对时频关系建模的局限性，需要改进消息嵌入和提取的可逆性。

Method: 使用基于流的可逆神经网络建立隐写音频、载体音频和秘密音频之间的直接联系，引入时频损失函数，并添加加密技术保护隐藏数据。

Result: 在VCTK和LibriSpeech数据集上的实验表明，该方法在主客观指标上优于先前方法，并对各种噪声具有鲁棒性。

Conclusion: 该方法在目标安全通信场景中具有实用价值，通过改进可逆性和添加安全措施，提升了音频隐写的性能。

Abstract: Data hiding is essential for secure communication across digital media, and
recent advances in Deep Neural Networks (DNNs) provide enhanced methods for
embedding secret information effectively. However, previous audio hiding
methods often result in unsatisfactory quality when recovering secret audio,
due to their inherent limitations in the modeling of time-frequency
relationships. In this paper, we explore these limitations and introduce a new
DNN-based approach. We use a flow-based invertible neural network to establish
a direct link between stego audio, cover audio, and secret audio, enhancing the
reversibility of embedding and extracting messages. To address common issues
from time-frequency transformations that degrade secret audio quality during
recovery, we implement a time-frequency loss on the time-domain signal. This
approach not only retains the benefits of time-frequency constraints but also
enhances the reversibility of message recovery, which is vital for practical
applications. We also add an encryption technique to protect the hidden data
from unauthorized access. Experimental results on the VCTK and LibriSpeech
datasets demonstrate that our method outperforms previous approaches in terms
of subjective and objective metrics and exhibits robustness to various types of
noise, suggesting its utility in targeted secure communication scenarios.

</details>


### [197] [SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos](https://arxiv.org/abs/2510.02916)
*Amir Dellali,Luca A. Lanzendörfer,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: SALSA-V是一个多模态视频到音频生成模型，能够从无声视频内容合成高度同步、高保真的长音频。采用掩码扩散目标和快捷损失，实现快速高质量音频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法在音频-视频同步性和长音频生成方面存在不足，需要开发能够生成高质量、同步长音频的模型。

Method: 提出掩码扩散目标实现音频条件生成和无约束长度音频序列合成，集成快捷损失实现快速生成（仅需8个采样步骤）。

Result: 在定量评估和人类听力研究中，SALSA-V在视听对齐和与视频内容同步方面显著优于现有最先进方法。

Conclusion: SALSA-V为专业音频合成任务（如Foley生成和声音设计）提供了有效解决方案，支持近实时应用。

Abstract: We propose SALSA-V, a multimodal video-to-audio generation model capable of
synthesizing highly synchronized, high-fidelity long-form audio from silent
video content. Our approach introduces a masked diffusion objective, enabling
audio-conditioned generation and the seamless synthesis of audio sequences of
unconstrained length. Additionally, by integrating a shortcut loss into our
training process, we achieve rapid generation of high-quality audio samples in
as few as eight sampling steps, paving the way for near-real-time applications
without requiring dedicated fine-tuning or retraining. We demonstrate that
SALSA-V significantly outperforms existing state-of-the-art methods in both
audiovisual alignment and synchronization with video content in quantitative
evaluation and a human listening study. Furthermore, our use of random masking
during training enables our model to match spectral characteristics of
reference audio samples, broadening its applicability to professional audio
synthesis tasks such as Foley generation and sound design.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [198] [FLOWR.root: A flow matching based foundation model for joint multi-purpose structure-aware 3D ligand generation and affinity prediction](https://arxiv.org/abs/2510.02578)
*Julian Cremer,Tuan Le,Mohammad M. Ghahremanpour,Emilia Sługocka,Filipe Menezes,Djork-Arné Clevert*

Main category: q-bio.BM

TL;DR: Flowr.root是一个等变流匹配模型，用于口袋感知的3D配体生成，同时进行结合亲和力预测和置信度估计。该模型支持从头生成、药效团条件采样、片段扩展和多端点亲和力预测。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时进行3D配体生成和亲和力预测的综合模型，以支持从命中识别到先导化合物优化的整个药物设计流程。

Method: 使用等变流匹配模型，结合大规模配体库和混合保真度的蛋白质-配体复合物进行训练，然后在精选的共晶数据集上进行精炼，并通过参数高效微调进行项目特定适应。

Result: 在无条件3D分子生成和口袋条件配体设计中达到最先进性能，生成几何真实、低应变结构。亲和力预测模块在SPINDR测试集上表现出卓越准确性，在Schrodinger FEP+/OpenFE基准测试中优于近期模型且速度优势显著。

Conclusion: Flowr.root通过整合结构感知生成、亲和力估计和属性引导采样，为基于结构的药物设计提供了全面的基础模型，覆盖从命中识别到先导化合物优化的全过程。

Abstract: We present Flowr.root, an equivariant flow-matching model for pocket-aware 3D
ligand generation with joint binding affinity prediction and confidence
estimation. The model supports de novo generation, pharmacophore-conditional
sampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,
pKi, pKd, pEC50). Training combines large-scale ligand libraries with
mixed-fidelity protein-ligand complexes, followed by refinement on curated
co-crystal datasets and parameter-efficient finetuning for project-specific
adaptation. Flowr.root achieves state-of-the-art performance in unconditional
3D molecule generation and pocket-conditional ligand design, producing
geometrically realistic, low-strain structures. The integrated affinity
prediction module demonstrates superior accuracy on the SPINDR test set and
outperforms recent models on the Schrodinger FEP+/OpenFE benchmark with
substantial speed advantages. As a foundation model, Flowr.root requires
finetuning on project-specific datasets to account for unseen
structure-activity landscapes, yielding strong correlation with experimental
data. Joint generation and affinity prediction enable inference-time scaling
through importance sampling, steering molecular design toward higher-affinity
compounds. Case studies validate this: selective CK2alpha ligand generation
against CLK3 shows significant correlation between predicted and
quantum-mechanical binding energies, while ERalpha and TYK2 scaffold
elaboration demonstrates strong agreement with QM calculations. By integrating
structure-aware generation, affinity estimation, and property-guided sampling,
Flowr.root provides a comprehensive foundation for structure-based drug design
spanning hit identification through lead optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [199] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出了一种用于脑肿瘤分类的层次化广义类别发现方法HGCD-BT，通过结合层次聚类和对比学习，能够识别训练时未见过的肿瘤类别。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法局限于预定义的固定类别，无法识别训练时未见的肿瘤类型。无监督学习缺乏利用标注数据先验知识的能力，而半监督方法假设所有潜在类别都在标注数据中。

Method: 将层次聚类与对比学习相结合，在对比学习基础上引入半监督层次聚类损失，构建层次化的广义类别发现框架。

Result: 在OpenSRH数据集上，相比最先进的GCD方法，在图像块分类准确率提升28%，特别是在识别未见肿瘤类别方面表现优异。在Digital Brain Tumor Atlas的整张切片分类中也验证了方法的通用性。

Conclusion: HGCD-BT方法能够有效识别已知和未知的脑肿瘤类别，在不同成像模态中都具有良好的通用性，为术中决策提供了更准确的分类工具。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [200] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种简单、检测器无关的框架，通过模态上下文注意力（MoCA）和查询表示对齐（QueryREPA）来对齐医学多模态检测中的对象查询表示，从而提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学多模态检测中，由于不同成像模态（如CXR、CT、MRI）的统计异质性和表示空间不连续，导致单一检测器性能下降的问题。

Method: 使用文本派生的紧凑模态令牌编码成像模态信息，通过MoCA在自注意力中传播模态上下文，并引入QueryREPA预训练阶段，使用任务特定的对比目标对齐查询表示与模态令牌。

Result: 该方法在多种模态联合训练时，持续提升平均精度（AP），且开销极小，无需架构修改。

Conclusion: 提供了一种实用的路径，通过模态感知、类别忠实的查询表示，实现鲁棒的医学多模态目标检测。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [201] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: ELMF4EggQ是一个集成学习框架，通过融合图像、形状和重量等多模态外部特征来无损评估鸡蛋等级和新鲜度，无需内部测量。


<details>
  <summary>Details</summary>
Motivation: 开发非破坏性、准确的鸡蛋质量评估方法对食品安全、产品标准和家禽生产效率至关重要。

Method: 使用预训练CNN模型提取图像特征，结合形状和重量特征，通过PCA降维、SMOTE增强和多种机器学习算法分类，最后采用集成投票机制。

Result: 多模态方法显著优于仅使用图像或表格特征的基线，在等级分类和新鲜度预测中分别达到86.57%和70.83%的准确率。

Conclusion: 该研究首次仅使用外部非侵入特征通过机器学习评估鸡蛋内部质量，并发布了首个相应标记数据集，促进了该领域的透明度和可复现性。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [202] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 该论文系统研究了视觉生成模型中影响组合泛化的设计选择，发现离散vs连续训练目标和条件信息提供程度是关键因素，并提出通过添加连续JEPA目标来改进离散模型MaskGIT的组合性能。


<details>
  <summary>Details</summary>
Motivation: 理解视觉生成模型中促进或抑制组合泛化的机制，组合泛化是生成已知概念新颖组合的关键能力。

Method: 通过受控实验系统研究不同设计选择的影响，并基于发现提出在MaskGIT离散损失基础上添加辅助连续JEPA目标的方法。

Result: 识别出两个关键因素：(i)训练目标是离散还是连续分布，(ii)条件在训练期间提供构成概念信息的程度。提出的混合方法能改进离散模型的组合性能。

Conclusion: 训练目标的离散/连续特性和条件信息的提供程度显著影响组合泛化，通过结合连续目标可以改善离散模型的组合能力。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [203] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出马尔可夫Reeb图框架，用于模拟保持基线数据中生活模式的时空轨迹，结合个体和群体层面的移动结构，生成既一致又变化的真实轨迹。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对城市规划、流行病学和交通管理至关重要，需要生成既保持生活模式又包含变化的真实轨迹。

Method: 结合个体和群体层面的移动结构，在概率拓扑模型中构建马尔可夫Reeb图框架，用于模拟时空轨迹。

Result: 在Urban Anomalies数据集（亚特兰大和柏林子集）上评估，使用Jensen-Shannon散度在群体和个体层面指标上显示出强保真度，同时保持数据和计算效率。

Conclusion: 马尔可夫Reeb图是一个可扩展的轨迹模拟框架，适用于各种城市环境。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [204] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种基于随机共振的测试时防御机制，通过引入小的平移扰动来对抗对抗性攻击，无需额外训练或网络模块


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖特征过滤或平滑，会导致信息损失。本文旨在通过随机共振增强鲁棒性同时最小化信息损失

Method: 对输入图像引入小的平移扰动，对齐变换后的特征嵌入并聚合，然后映射回原始参考图像，使用闭式公式实现

Result: 在图像分类上恢复68.1%的准确率损失，立体匹配恢复71.9%，光流恢复29.2%，在各种对抗攻击下表现优异

Conclusion: 该方法完全无需训练、架构无关、攻击无关，首次为密集预测任务建立了通用测试时防御，具有很好的通用性和实用性

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>
