<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 13]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 72]
- [stat.TH](#stat.TH) [Total: 1]
- [cs.CC](#cs.CC) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [math.OC](#math.OC) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 5]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering](https://arxiv.org/abs/2511.03393)
*Chiara Rucco,Motaz Saad,Antonella Longo*

Main category: cs.DB

TL;DR: 本文正式化ETLT和ELTL作为可重用设计模式，并引入增强变体ETLT++和ELTL++，以解决治理、质量保证和可观测性方面的持续差距。


<details>
  <summary>Details</summary>
Motivation: 传统ETL和ELT设计模式难以满足现代可扩展性、治理和实时数据处理的需求，而混合方法如ETLT和ELTL已在实践中使用，但文献缺乏最佳实践和正式认可。

Method: 在设��模式框架内系统定义ETLT和ELTL模式，概述其结构、权衡和用例，并通过嵌入显式契约、版本控制、语义管理和持续监控作为强制性设计义务，将其扩展为ETLT++和ELTL++。

Result: 提出的框架为从业者提供了构建可审计、可扩展和成本效益高的管道的结构化路线图，在多云和实时环境中统一质量执行、血缘和可用性。

Conclusion: 通过正式化ETLT和ELTL，并通过ETLT++和ELTL++增强它们，这项工作弥合了临时实践和系统设计之间的差距，为现代可信数据工程提供了可重用的基础。

Abstract: Traditional ETL and ELT design patterns struggle to meet modern requirements
of scalability, governance, and real-time data processing. Hybrid approaches
such as ETLT (Extract-Transform-Load-Transform) and ELTL
(Extract-Load-Transform-Load) are already used in practice, but the literature
lacks best practices and formal recognition of these approaches as design
patterns. This paper formalizes ETLT and ELTL as reusable design patterns by
codifying implicit best practices and introduces enhanced variants, ETLT++ and
ELTL++, to address persistent gaps in governance, quality assurance, and
observability. We define ETLT and ELTL patterns systematically within a design
pattern framework, outlining their structure, trade-offs, and use cases.
Building on this foundation, we extend them into ETLT++ and ELTL++ by embedding
explicit contracts, versioning, semantic curation, and continuous monitoring as
mandatory design obligations. The proposed framework offers practitioners a
structured roadmap to build auditable, scalable, and cost-efficient pipelines,
unifying quality enforcement, lineage, and usability across multi-cloud and
real-time contexts. By formalizing ETLT and ELTL, and enhancing them through
ETLT++ and ELTL++, this work bridges the gap between ad hoc practice and
systematic design, providing a reusable foundation for modern, trustworthy data
engineering.

</details>


### [2] [HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics](https://arxiv.org/abs/2511.03437)
*Md Mizanur Rahaman Nayan,Zheyu Li,Flavio Ponzina,Sumukh Pinge,Tajana Rosing,Azad J. Naeemi*

Main category: cs.DB

TL;DR: 提出了一种轻量级增量聚类和高度并行化的数据库搜索平台，针对资源受限环境优化，在保证性能的同时降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的蛋白质组学数据库搜索和聚类方法需要大量资源且延迟高，不适用于资源受限环境。

Method: 利用质谱分析洞察，采用桶级并行化和查询调度来减少延迟。通过一次性硬件初始化预聚类数据，支持连续数据库搜索和本地重新聚类。使用预聚类数据的启发式方法指导增量聚类。

Result: 增量聚类加速20倍，聚类误差仅增加0.3%。数据库搜索结果与最先进工具重叠率达96%。在人类基因组数据集上，1000次查询搜索仅消耗1.1μJ能量，桶级并行化实现100倍加速。

Conclusion: 该方法为资源受限环境提供了更实用高效的蛋白质组学数据分析解决方案，显著降低了能耗和延迟，同时保持了高性能。

Abstract: Database (DB) search and clustering are fundamental in proteomics but
conventional full clustering and search approaches demand high resources and
incur long latency. We propose a lightweight incremental clustering and highly
parallelizable DB search platform tailored for resource-constrained
environments, delivering low energy and latency without compromising
performance. By leveraging mass-spectrometry insights, we employ bucket-wise
parallelization and query scheduling to reduce latency. A one-time hardware
initialization with pre-clustered proteomics data enables continuous DB search
and local re-clustering, offering a more practical and efficient alternative to
clustering from scratch. Heuristics from pre-clustered data guide incremental
clustering, accelerating the process by 20x with only a 0.3% increase in
clustering error. DB search results overlap by 96% with state-of-the-art tools,
validating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the
7nm node with a compute-in-memory design. For the human genome draft dataset
(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search
consumes 1.1{\mu}J. Bucket-wise parallelization further achieves 100x speedup.

</details>


### [3] [In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines](https://arxiv.org/abs/2511.03480)
*Khalid Belhajjame,Haroun Mezrioui,Yuyan Zhao*

Main category: cs.DB

TL;DR: 提出了一种基于张量的索引机制，用于高效捕获和查询数据准备流水线的细粒度溯源信息，结合回顾性溯源和前瞻性溯源来支持各种溯源查询。


<details>
  <summary>Details</summary>
Motivation: 数据溯源在数据准备流水线中有多种应用，如调试故障流水线、解释结果、验证公平性和识别数据质量问题，但需要高效的捕获和查询机制。

Method: 利用张量以最小内存捕获数据处理操作的细粒度溯源，在记录级溯源基础上提供属性级粒度，通过将回顾性溯源与前瞻性溯源信息结合来连接数据处理操作的输入输出模式。

Result: 评估实验使用真实和合成数据表明，该方法能够高效回答广泛的溯源查询问题。

Conclusion: 结合回顾性和前瞻性溯源的张量索引机制能够有效支持数据流水线的各种溯源应用需求。

Abstract: Data provenance has numerous applications in the context of data preparation
pipelines. It can be used for debugging faulty pipelines, interpreting results,
verifying fairness, and identifying data quality issues, which may affect the
sources feeding the pipeline execution. In this paper, we present an indexing
mechanism to efficiently capture and query pipeline provenance. Our solution
leverages tensors to capture fine-grained provenance of data processing
operations, using minimal memory. In addition to record-level lineage
relationships, we provide finer granularity at the attribute level. This is
achieved by augmenting tensors, which capture retrospective provenance, with
prospective provenance information, drawing connections between input and
output schemas of data processing operations. We demonstrate how these two
types of provenance (retrospective and prospective) can be combined to answer a
broad range of provenance queries efficiently, and show effectiveness through
evaluation exercises using both real and synthetic data.

</details>


### [4] [Analytical Queries for Unstructured Data](https://arxiv.org/abs/2511.03489)
*Daniel Kang*

Main category: cs.DB

TL;DR: 这篇论文讨论了使用机器学习分析非结构化数据（特别是视频分析）时面临的数据管理系统挑战，包括查询表达、计算成本和模型错误处理等问题。


<details>
  <summary>Details</summary>
Motivation: 随着非结构化数据的指数级增长和机器学习能力的提升，需要开发能够高效处理这些数据的数据管理系统，解决查询表达、计算成本和模型可靠性等挑战。

Method: 通过用户定义函数、结构化模式和不透明查询等方式表达查询；使用近似方法优化昂贵计算；应用异常值和漂移检测处理模型错误。

Result: 数据管理社区的最新工作已经能够解决这些挑战，使分析师能够更有效地使用机器学习分析非结构化数据。

Conclusion: 机器学习在非结构化数据分析中的应用带来了新的数据管理挑战，但通过查询优化、近似计算和错误处理等方法，这些挑战正在得到有效解决。

Abstract: Unstructured data, in the form of text, images, video, and audio, is produced
at exponentially higher rates. In tandem, machine learning (ML) methods have
become increasingly powerful at analyzing unstructured data. Modern ML methods
can now detect objects in images, understand actions in videos, and even
classify complex legal texts based on legal intent. Combined, these trends make
it increasingly feasible for analysts and researchers to automatically
understand the "real world." However, there are major challenges in deploying
these techniques: 1) executing queries efficiently given the expense of ML
methods, 2) expressing queries over bespoke forms of data, and 3) handling
errors in ML methods.
  In this monograph, we discuss challenges and advances in data management
systems for unstructured data using ML, with a particular focus on video
analytics. Using ML to answer queries introduces new challenges.First, even
turning user intent into queries can be challenging: it is not obvious how to
express a query of the form "select instances of cars turning left." Second, ML
models can be orders of magnitude more expensive compared to processing
traditional structured data. Third, ML models and the methods to accelerate
analytics with ML models can be error-prone.
  Recent work in the data management community has aimed to address all of
these challenges. Users can now express queries via user-defined functions,
opaquely through standard structured schemas, and even by providing examples.
Given a query, recent work focuses on optimizing queries by approximating
expensive "gold" methods with varying levels of guarantees. Finally, to handle
errors in ML models, recent work has focused on applying outlier and drift
detection to data analytics with ML.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 欧洲高性能计算中心在CFD领域的经验总结，通过案例研究评估不同架构上的能耗表现，强调加速器和混合精度技术对降低能耗的优势。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能源效率挑战，计算需求增长和架构复杂性导致显著能耗问题，需要测量、分析和优化能源消耗。

Method: 使用代表性CFD应用（waLBerla、FLEXI/GALAEI、Neko、NekRS）进行案例研究，在CPU和GPU架构上评估能耗与时间指标。

Result: 结果显示加速器和混合精度技术在保持计算精度的同时能显著降低能耗。

Conclusion: 需要促进HPC系统的能耗测量，提高意识、教育社区，并采取行动实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [6] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [7] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM提出了一种统一的内存亲和性数据布局和DRAM地址映射方案，专门针对NPU-PIM协同执行优化，在OPT模型上实现了TTFT降低3.0倍和TTLT降低2.18倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的大语言模型解码阶段仍然存在内存密集型问题，限制了性能。NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等挑战。

Method: 采用列主序、基于分块的数据布局和可配置的DRAM映射策略，确保与NPU计算的兼容性，同时最大化PIM效率，不引入额外内存开销或带宽损失。

Result: 在OPT模型上的综合评估显示，UMDAM将首令牌时间(TTFT)降低了最高3.0倍，末令牌时间(TTLT)降低了2.18倍。

Conclusion: UMDAM显著提升了边缘设备上端到端LLM推理效率，为NPU-PIM协同执行提供了有效的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [8] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 评估三种隔离策略（cgroups和CPU绑定、Docker容器、Firecracker MicroVMs）在Duet基准测试中对性能变异性的缓解效果，发现进程隔离通常能降低假阳性，但Docker容器表现较差。


<details>
  <summary>Details</summary>
Motivation: 云环境中多租户资源竞争导致基准测试存在性能变异性，Duet基准测试通过在同一VM上并发运行两个工作负载版本来缓解此问题，但需要额外的隔离机制来处理VM内竞争。

Method: 使用Duet设置运行基准测试，同时运行噪声生成器来模拟资源竞争，比较三种隔离策略与无隔离基线的性能表现。

Result: 所有实验在噪声影响下都显示出不同的延迟分布，但进程隔离通常能降低假阳性，而Docker容器对噪声导致的性能退化更敏感。

Conclusion: 推荐对同步工作负载使用进程隔离，但应避免使用Docker容器。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [9] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 提出了一种基于谱空间的拓扑编码方法，用于分析分布式计算中无色任务的可行性，通过Stone对偶性建立了通用的分布式可计算性定理。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，为消息敌手模型下的无色任务求解提供理论基础，解释彩色和无色模型具有相同计算能力的原因。

Method: 将分布式协议执行后的全局状态视为谱空间（使用面偏序集上的Alexandrov拓扑），通过谱空间范畴中的投射极限定义极限对象，应用Stone对偶性。

Result: 建立了通用分布式可计算性定理：存在算法求解无色任务当且仅当存在与Δ兼容的谱映射f。该定理可推导出许多已知的无色可计算性定理。

Conclusion: 彩色和无色模型具有相同的计算能力，新的拓扑方法为这种等价性提供了理论解释，统一了分布式计算中的拓扑方法。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [10] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文扩展了分布式计算中无色任务的可计算性理论，将消息对手模型推广到输入依赖型对手，并给出了k-集合协议问题的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定消息对手模型，而实际分布式系统中对手行为可能依赖于输入配置。本文旨在扩展可计算性理论以处理输入依赖型对手，特别是针对条件基对手模型。

Method: 使用拓扑几何方法，将分布式计算问题建模为几何构造，分析核心弹性对手的计算能力，并研究载体映射的结构性质。

Result: 证明了输入依赖型对手的可计算性特征，核心弹性对手的计算能力等价性，以及k-集合协议问题的充要条件。

Conclusion: 本文通过几何方法为分布式任务的可计算性提供了更一般的理论框架，特别适用于输入依赖型对手场景。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Faster Weak Expander Decompositions and Approximate Max Flow](https://arxiv.org/abs/2511.02943)
*Henry Fleischmann,George Z. Li,Jason Li*

Main category: cs.DS

TL;DR: 本文提出了更快的弱扩展器分解算法和无向图近似最大流算法，通过预热启动和简化框架改进了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有弱扩展器分解和近似最大流算法存在递归深度成本和框架复杂性问题，需要更高效的方法。

Method: 1. 在计算弱扩展器分解时引入预热启动机制避免递归深度成本；2. 简化Li等人的非递归近似最大流算法框架，并适配新的弱扩展器分解原语。

Result: 获得了比现有方法更快的弱扩展器分解算法，以及接近扩展器分解方法极限的近似最大流算法。

Conclusion: 提出的预热启动和框架简化技术有效提升了弱扩展器分解和近似最大流算法的性能，达到了扩展器分解方法的理论极限附近。

Abstract: We give faster algorithms for weak expander decompositions and approximate
max flow on undirected graphs. First, we show that it is possible to "warm
start" the cut-matching game when computing weak expander decompositions,
avoiding the cost of the recursion depth. Our algorithm is also flexible enough
to support weaker flow subroutines than previous algorithms.
  Our second contribution is to streamline the recent non-recursive approximate
max flow algorithm of Li, Rao, and Wang (SODA, 2025) and adapt their framework
to use our new weak expander decomposition primitive. Consequently, we give an
approximate max flow algorithm within a few logarithmic factors of the limit of
expander decomposition-based approaches.

</details>


### [12] [Tight Better-Than-Worst-Case Bounds for Element Distinctness and Set Intersection](https://arxiv.org/abs/2511.02954)
*Ivor van der Hoog,Eva Rotenberg,Daniel Rutschmann*

Main category: cs.DS

TL;DR: 该论文研究了元素唯一性问题的实例特定下界，通过图结构表示重复元素，证明了确定性算法的最优竞争比为O(log log n)，并展示了与集合交集问题的分离。


<details>
  <summary>Details</summary>
Motivation: 经典的元素唯一性问题有Ω(n log n)的比较下界，但这个下界在输入包含大量重复元素时可能不适用。论文旨在研究对输入中重复数量敏感的比较下界。

Method: 将输入实例的重复结构表示为无向图G(I)，研究算法在所有具有同构图G的输入上的最坏情况运行时间。通过对抗性下界证明和构造竞争算法来分析问题。

Result: 证明了对于任何确定性算法，存在图G使得存在算法在所有具有同构图G的输入上比原算法快O(log log n)倍。同时给出了O(log log n)-竞争的确定性算法。

Conclusion: 元素唯一性问题的最优确定性竞争比为Θ(log log n)，而集合交集问题的最优竞争比为Θ(log n)，两者存在分离。

Abstract: The element distinctness problem takes as input a list $I$ of $n$ values from
a totally ordered universe and the goal is to decide whether $I$ contains any
duplicates. It is a well-studied problem with a classical worst-case $\Omega(n
\log n)$ comparison-based lower bound by Fredman. At first glance, this lower
bound appears to rule out any algorithm more efficient than the naive approach
of sorting $I$ and comparing adjacent elements. However, upon closer
inspection, the $\Omega(n \log n)$ bound does not apply if the input has many
duplicates. We therefore ask: Are there comparison-based lower bounds for
element distinctness that are sensitive to the amount of duplicates in the
input?
  To address this question, we derive instance-specific lower bounds. For any
input instance $I$, we represent the combinatorial structure of the duplicates
in $I$ by an undirected graph $G(I)$ that connects identical elements. Each
such graph $G$ is a union of cliques, and we study algorithms by their
worst-case running time over all inputs $I'$ with $G(I') \cong G$. We establish
an adversarial lower bound showing that, for any deterministic algorithm
$\mathcal{A}$, there exists a graph $G$ and an algorithm $\mathcal{A}'$ that,
for all inputs $I$ with $G(I) \cong G$, is a factor $O(\log \log n)$ faster
than $\mathcal{A}$. Consequently, no deterministic algorithm can be $o(\log
\log n)$-competitive for all graphs $G$. We complement this with an $O(\log
\log n)$-competitive deterministic algorithm, thereby obtaining tight bounds
for element distinctness that go beyond classical worst-case analysis.
  We subsequently study the related problem of set intersection. We show that
no deterministic set intersection algorithm can be $o(\log n)$-competitive, and
provide an $O(\log n)$-competitive deterministic algorithm. This shows a
separation between element distinctness and the set intersection problem.

</details>


### [13] [Implementation and Brief Experimental Analysis of the Duan et al. (2025) Algorithm for Single-Source Shortest Paths](https://arxiv.org/abs/2511.03007)
*Lucas Castro,Thailsson Clementino,Rosiane de Freitas*

Main category: cs.DS

TL;DR: 实现了Duan等人(2025)提出的SSSP确定性算法，虽然具有O(m log^{2/3}n)的最佳渐近复杂度，但实际性能因常数因子较大而比Dijkstra算法慢。


<details>
  <summary>Details</summary>
Motivation: 验证具有最佳渐近复杂度的SSSP算法在实际应用中的性能表现，比较理论优势与实证效果。

Method: 忠实实现Duan等人算法的C++版本，在合成稀疏随机图和真实道路网络(DIMACS基准)上测试，并与使用二叉堆的Dijkstra算法对比。

Result: 新算法尽管渐近复杂度更优，但由于常数因子显著更大，在所有测试的稀疏图规模下都比Dijkstra算法慢，包括数千万顶点的实例。

Conclusion: 理论上的渐近优势在实际应用中可能被常数因子抵消，Dijkstra算法在当前测试规模下仍更实用。

Abstract: We present an implementation and a brief experimental analysis of the
deterministic algorithm proposed by Duan et al. (2025) for the Single-Source
Shortest Path (SSSP) problem, which achieves the best known asymptotic upper
bound in the comparison-addition model, with running time $O(m \log^{2/3} n)$.
We provide a faithful C++ implementation of this algorithm, following all
structural details described in the original paper, and compare its empirical
performance with the classical Dijkstra's algorithm using binary heaps. The
experiments were conducted on both synthetic sparse random graphs and
real-world road network instances from the DIMACS benchmark. Our results show
that, despite its superior asymptotic complexity, the new algorithm presents
significantly larger constant factors, making Dijkstra's algorithm faster for
all tested sparse graph sizes, including instances with tens of millions of
vertices. Our implementation achieves $O(m \log^{2/3} n)$ expected time, due to
the use of hash tables, and some possibilities for making it worst-case are
being considered. (This is a ongoing work.)

</details>


### [14] [A Branch-and-Bound Approach for Maximum Low-Diameter Dense Subgraph Problems](https://arxiv.org/abs/2511.03157)
*Yi Zhoua,Chunyu Luoa,Zhengren Wangb,Zhang-Hua Fuc*

Main category: cs.DS

TL;DR: 提出了一种基于分解的分支定界算法，用于在图中寻找直径不超过2的最大f(·)-稠密子图，解决了传统稠密子图可能不连通的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的f(·)-稠密图可能不连通或弱连通，限制了其在凝聚子图提取应用中的实用性。需要找到既稠密又具有良好连通性（直径小）的子图。

Method: 采用分解框架将图分解为n个更小的子图，在每个子图中独立搜索。使用退化排序和两跳退化排序策略，结合具有新颖排序上界的分支定界算法求解子问题。

Result: 在139个真实世界图上的实验表明，该算法在1小时内能解决近两倍于MIP求解器和纯分支定界算法的最优实例。

Conclusion: 提出的分解框架和分支定界算法能有效解决最大直径约束的稠密子图发现问题，显著优于现有方法。

Abstract: A graph with $n$ vertices is an $f(\cdot)$-dense graph if it has at least
$f(n)$ edges, $f(\cdot)$ being a well-defined function. The notion
$f(\cdot)$-dense graph encompasses various clique models like $\gamma$-quasi
cliques, $k$-defective cliques, and dense cliques, arising in cohesive subgraph
extraction applications. However, the $f(\cdot)$-dense graph may be
disconnected or weakly connected. To conquer this, we study the problem of
finding the largest $f(\cdot)$-dense subgraph with a diameter of at most two in
the paper. Specifically, we present a decomposition-based branch-and-bound
algorithm to optimally solve this problem. The key feature of the algorithm is
a decomposition framework that breaks the graph into $n$ smaller subgraphs,
allowing independent searches in each subgraph. We also introduce decomposition
strategies including degeneracy and two-hop degeneracy orderings, alongside a
branch-and-bound algorithm with a novel sorting-based upper bound to solve each
subproblem. Worst-case complexity for each component is provided. Empirical
results on 139 real-world graphs under two $f(\cdot)$ functions show our
algorithm outperforms the MIP solver and pure branch-and-bound, solving nearly
twice as many instances optimally within one hour.

</details>


### [15] [Optimal Stopping with a Predicted Prior](https://arxiv.org/abs/2511.03289)
*Tian Bai,Zhiyi Huang,Chui Shan Lee,Dongchen Li*

Main category: cs.DS

TL;DR: 本文提出了最优停止问题中的预测先验模型，设计了一族双标准算法，在利用准确预测的同时保持最坏情况保证，改进了现有秘书模型和先知不等式算法的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 决策者通常依赖可能出错的机器学习先验知识，而现有的秘书模型（无先验）和先知不等式模型（完全信息）无法很好地处理这种情况。

Method: 提出了最优停止问题的预测先验模型，设计了一族双标准算法，结合了秘书算法和先知不等式算法的优点。

Result: 该算法族在期望接受值和接受最大值概率两个目标上都实现了改进的一致性-鲁棒性权衡，并证明了对于后者，无法同时达到最佳先知不等式算法的一致性和最佳秘书算法的鲁棒性。

Conclusion: 预测先验模型填补了最优停止理论中的空白，为实际决策问题提供了更实用的算法框架。

Abstract: There are two major models of value uncertainty in the optimal stopping
literature: the secretary model, which assumes no prior knowledge, and the
prophet inequality model, which assumes full information about value
distributions. In practice, decision makers often rely on machine-learned
priors that may be erroneous. Motivated by this gap, we formulate the model of
optimal stopping with a predicted prior to design algorithms that are both
consistent, exploiting the prediction when accurate, and robust, retaining
worst-case guarantees when it is not.
  Existing secretary and prophet inequality algorithms are either pessimistic
in consistency or not robust to misprediction. A randomized combination only
interpolates their guarantees linearly. We show that a family of bi-criteria
algorithms achieves improved consistency-robustness trade-offs, both for
maximizing the expected accepted value and for maximizing the probability of
accepting the maximum value. We further prove that for the latter objective, no
algorithm can simultaneously match the best prophet inequality algorithm in
consistency, and the best secretary algorithm in robustness.

</details>


### [16] [Improved Online Load Balancing in the Two-Norm](https://arxiv.org/abs/2511.03345)
*Sander Borst,Danish Kashaev*

Main category: cs.DS

TL;DR: 本文提出了首个突破5竞争比的在线负载均衡算法，达到了4.9843的竞争比，通过新的原始-对偶框架和相关的随机舍入方法改进了之前的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 研究在线负载均衡问题，目标是最小化机器负载的ℓ₂范数平方。之前的确定性算法竞争比为5.828，随机算法为5，需要突破5的竞争比界限。

Method: 使用新的原始-对偶框架，基于自然半定规划松弛，结合在线实现Im和Shadloo的相关随机舍入过程。

Result: 获得了4.9843的竞争比，首次突破5的界限。同时提供了对现有算法竞争比的新证明，并证明了新分数算法的最优性。

Conclusion: 新方法不仅改进了竞争比，还提供了更简单统一的证明框架，证明了多个算法的最优性，为在线负载均衡问题提供了重要进展。

Abstract: We study the online load balancing problem on unrelated machines, with the
objective of minimizing the square of the $\ell_2$ norm of the loads on the
machines. The greedy algorithm of Awerbuch et al. (STOC'95) is optimal for
deterministic algorithms and achieves a competitive ratio of $3 + 2 \sqrt{2}
\approx 5.828$, and an improved $5$-competitive randomized algorithm based on
independent rounding has been shown by Caragiannis (SODA'08). In this work, we
present the first algorithm breaking the barrier of $5$ on the competitive
ratio, achieving a bound of $4.9843$. To obtain this result, we use a new
primal-dual framework to analyze this problem based on a natural semidefinite
programming relaxation, together with an online implementation of a correlated
randomized rounding procedure of Im and Shadloo (SODA'20). This novel
primal-dual framework also yields new, simple and unified proofs of the
competitive ratio of the $(3 + 2 \sqrt{2})$-competitive greedy algorithm, the
$5$-competitive randomized independent rounding algorithm, and that of a new
$4$-competitive optimal fractional algorithm. We also provide lower bounds
showing that the previous best randomized algorithm is optimal among
independent rounding algorithms, that our new fractional algorithm is optimal,
and that a simple greedy algorithm is optimal for the closely related online
scheduling problem $R || \sum w_j C_j$.

</details>


### [17] [Hesse's Redemption: Efficient Convex Polynomial Programming](https://arxiv.org/abs/2511.03440)
*Lucas Slot,David Steurer,Manuel Wiedmer*

Main category: cs.DS

TL;DR: 本文解决了凸多项式优化的多项式时间算法问题，特别是对于四阶及以上凸多项式的最小化，开发了新的技术来证明解的有界性，即使在没有线性特征的情况下。


<details>
  <summary>Details</summary>
Motivation: 凸优化算法（如椭球法）需要事先知道包含最优解的球半径边界。对于线性规划和凸二次规划，这种边界可以通过线性方程组的经典特征得到。但对于其他问题（如半定规划），Khachiyan的例子显示最优解可能需要指数级位数的巨大系数。四阶及以上凸多项式的最小化一直是一个开放问题，既没有线性特征，也不适用Khachiyan类型的例子。

Method: 开发了新技术来证明在没有线性特征可用时的解边界。对于在单纯形上最小化凸多项式（任意阶）的程序，证明了最优解的存在意味着存在具有多项式位长的近似最优解。将这些解边界与椭球法结合。

Result: 得到了凸多项式规划的第一个多项式时间算法，解决了Nesterov提出的问题。在此之前，即使是四阶凸多项式的无约束最小化也没有已知的多项式时间算法。

Conclusion: 本文通过开发新的解边界证明技术，成功解决了凸多项式规划的多项式时间算法问题，填补了线性/二次规划与半定规划之间的重要空白。

Abstract: Efficient algorithms for convex optimization, such as the ellipsoid method,
require an a priori bound on the radius of a ball around the origin guaranteed
to contain an optimal solution if one exists. For linear and convex quadratic
programming, such solution bounds follow from classical characterizations of
optimal solutions by systems of linear equations. For other programs, e.g.,
semidefinite ones, examples due to Khachiyan show that optimal solutions may
require huge coefficients with an exponential number of bits, even if we allow
approximations. Correspondingly, semidefinite programming is not even known to
be in NP. The unconstrained minimization of convex polynomials of degree four
and higher has remained a fundamental open problem between these two extremes:
its optimal solutions do not admit a linear characterization and, at the same
time, Khachiyan-type examples do not apply. We resolve this problem by
developing new techniques to prove solution bounds when no linear
characterizations are available. Even for programs minimizing a convex
polynomial (of arbitrary degree) over a polyhedron, we prove that the existence
of an optimal solution implies that an approximately optimal one with
polynomial bit length also exists. These solution bounds, combined with the
ellipsoid method, yield the first polynomial-time algorithm for convex
polynomial programming, settling a question posed by Nesterov (Math. Program.,
2019). Before, no polynomial-time algorithm was known even for unconstrained
minimization of a convex polynomial of degree four.

</details>


### [18] [Dynamic Meta-Kernelization](https://arxiv.org/abs/2511.03461)
*Christian Bertram,Deborah Haun,Mads Vestergaard Jensen,Tuukka Korhonen*

Main category: cs.DS

TL;DR: 该论文提出了动态核化算法，将经典的平面图支配集线性核扩展到动态设置，能够在图更新时维护线性大小的核，并保持最优解不变。


<details>
  <summary>Details</summary>
Motivation: 传统核化研究主要关注静态图的预处理算法，而现实世界中的图往往是动态变化的。该研究旨在将经典的线性核化结果扩展到动态设置，以处理不断变化的图结构。

Method: 开发动态数据结构，维护近似最优的突起分解，支持图的边和孤立顶点插入删除操作，每次更新后输出对核的少量修改以保持核的性质。

Result: 实现了在平面图上支配集问题的动态线性核化，初始化时间O(n log n)，每次更新O(log n)时间，核大小保持O(OPT(G))且保持平面性。

Conclusion: 该工作将核化理论扩展到动态设置，为拓扑小图类上的问题提供了动态核化、近似算法和FPT算法的统一框架。

Abstract: Kernelization studies polynomial-time preprocessing algorithms. Over the last
20 years, the most celebrated positive results of the field have been linear
kernels for classical NP-hard graph problems on sparse graph classes. In this
paper, we lift these results to the dynamic setting.
  As the canonical example, Alber, Fellows, and Niedermeier [J. ACM 2004] gave
a linear kernel for dominating set on planar graphs. We provide the following
dynamic version of their kernel: Our data structure is initialized with an
$n$-vertex planar graph $G$ in $O(n \log n)$ amortized time, and, at
initialization, outputs a planar graph $K$ with $\mathrm{OPT}(K) =
\mathrm{OPT}(G)$ and $|K| = O(\mathrm{OPT}(G))$, where $\mathrm{OPT}(\cdot)$
denotes the size of a minimum dominating set. The graph $G$ can be updated by
insertions and deletions of edges and isolated vertices in $O(\log n)$
amortized time per update, under the promise that it remains planar. After each
update to $G$, the data structure outputs $O(1)$ updates to $K$, maintaining
$\mathrm{OPT}(K) = \mathrm{OPT}(G)$, $|K| = O(\mathrm{OPT}(G))$, and planarity
of $K$.
  Furthermore, we obtain similar dynamic kernelization algorithms for all
problems satisfying certain conditions on (topological-)minor-free graph
classes. Besides kernelization, this directly implies new dynamic
constant-approximation algorithms and improvements to dynamic FPT algorithms
for such problems.
  Our main technical contribution is a dynamic data structure for maintaining
an approximately optimal protrusion decomposition of a dynamic
topological-minor-free graph. Protrusion decompositions were introduced by
Bodlaender, Fomin, Lokshtanov, Penninkx, Saurabh, and Thilikos [J. ACM 2016],
and have since developed into a part of the core toolbox in kernelization and
parameterized algorithms.

</details>


### [19] [Online Flow Time Minimization: Tight Bounds for Non-Preemptive Algorithms](https://arxiv.org/abs/2511.03485)
*Yutong Geng,Enze Sun,Zonghan Yang,Yuhao Zhang*

Main category: cs.DS

TL;DR: 本文研究了最小化总流时间的在线调度问题，提出了随机化算法和确定性算法的竞争比结果，并探讨了kill-and-restart模型下的算法表现。


<details>
  <summary>Details</summary>
Motivation: 先前工作只关注了单机确定性算法的Ω(n)下界，留下了随机化算法和多机确定性算法的基本问题未解决。

Method: 提出了多项式时间随机化算法，竞争比为Θ(√(n/m))，并设计了确定性非抢占式算法，竞争比为O(n/m²+√(n/m)log m)。还扩展到kill-and-restart模型。

Result: 随机化算法达到了匹配的下界，改进了离线近似比。确定性算法在m≥2时表现良好，但在m=1时有强下界。kill-and-restart模型在未知n时也能突破O(n)界限。

Conclusion: 解决了随机化和确定性在线调度的基本问题，揭示了kill-and-restart模型的优势，并证明了随机化在未知n时不足以突破O(n)竞争比。

Abstract: This paper studies the classical online scheduling problem of minimizing
total flow time for $n$ jobs on $m$ identical machines. Prior work often cites
the $\Omega(n)$ lower bound for non-preemptive algorithms to argue for the
necessity of preemption or resource augmentation, which shows the trivial
$O(n)$-competitive greedy algorithm is tight. However, this lower bound applies
only to \emph{deterministic} algorithms in the \emph{single-machine} case,
leaving several fundamental questions unanswered. Can randomness help in the
non-preemptive setting, and what is the optimal online deterministic algorithm
when $m \geq 2$? We resolve both questions. We present a polynomial-time
randomized algorithm with competitive ratio $\Theta(\sqrt{n/m})$ and prove a
matching randomized lower bound, settling the randomized non-preemptive setting
for every $m$. This also improves the best-known offline approximation ratio
from $O(\sqrt{n/m}\log(n/m))$ to $O(\sqrt{n/m})$. On the deterministic side, we
present a non-preemptive algorithm with competitive ratio
$O(n/m^{2}+\sqrt{n/m}\log m)$ and prove a nearly matching lower bound.
  Our framework also extends to the kill-and-restart model, where we reveal a
sharp transition of deterministic algorithms: we design an asymptotically
optimal algorithm with the competitive ratio $O(\sqrt{n/m})$ for $m\ge 2$, yet
establish a strong $\Omega(n/\log n)$ lower bound for $m=1$. Moreover, we show
that randomization provides no further advantage, as the lower bound coincides
with that of the non-preemptive setting.
  While our main results assume prior knowledge of $n$, we also investigate the
setting where $n$ is unknown. We show kill-and-restart is powerful enough to
break the $O(n)$ barrier for $m \geq 2$ even without knowing $n$. Conversely,
we prove randomization alone is insufficient, as no algorithm can achieve an
$o(n)$ competitive ratio in this setting.

</details>


### [20] [Randomized Rounding over Dynamic Programs](https://arxiv.org/abs/2511.03490)
*Etienne Bamas,Shi Li,Lars Rohwedder*

Main category: cs.DS

TL;DR: 该论文提出了一种方法，可以在满足动态规划递推关系的问题中，近似地处理大量额外打包约束，获得近似保证为$n^\epsilon \mathrm{polylog} n$，时间复杂度为$n^{O(1/\epsilon)}$。


<details>
  <summary>Details</summary>
Motivation: 传统动态规划方法难以处理大量额外约束。该研究旨在扩展动态规划的适用范围，使其能够处理具有打包约束的复杂问题，连接动态规划与近似算法中各种看似不相关的问题。

Method: 将动态规划子问题重新解释为网络设计问题，构建强线性规划松弛，然后应用随机舍入技术。该方法适用于满足特定递推关系的动态规划问题。

Result: 获得了灵活的近似保证：通过调整参数$\epsilon$，可以在多项式时间内获得$n^\epsilon$-近似，或在拟多项式时间内获得多对数近似。该方法可应用于最短路径、最长公共子序列等经典问题。

Conclusion: 该方法成功地将动态规划与近似算法中的各种问题联系起来，甚至可间接应用于覆盖约束问题（如有向斯坦纳树）和非递推关系问题（如匹配问题变种），显著扩展了动态规划的应用范围。

Abstract: We show that under mild assumptions for a problem whose solutions admit a
dynamic programming-like recurrence relation, we can still find a solution
under additional packing constraints, which need to be satisfied approximately.
The number of additional constraints can be very large, for example, polynomial
in the problem size. Technically, we reinterpret the dynamic programming
subproblems and their solutions as a network design problem. Inspired by
techniques from, for example, the Directed Steiner Tree problem, we construct a
strong LP relaxation, on which we then apply randomized rounding. Our
approximation guarantees on the packing constraints have roughly the form of a
$(n^{\epsilon} \mathrm{polylog}\ n)$-approximation in time $n^{O(1/\epsilon)}$,
for any $\epsilon > 0$. By setting $\epsilon=\log \log n/\log n$, we obtain a
polylogarithmic approximation in quasi-polynomial time, or by setting
$\epsilon$ as a constant, an $n^\epsilon$-approximation in polynomial time.
  While there are necessary assumptions on the form of the DP, it is general
enough to capture many textbook dynamic programs from Shortest Path to Longest
Common Subsequence. Our algorithm then implies that we can impose additional
constraints on the solutions to these problems. This allows us to model various
problems from the literature in approximation algorithms, many of which were
not thought to be connected to dynamic programming. In fact, our result can
even be applied indirectly to some problems that involve covering instead of
packing constraints, for example, the Directed Steiner Tree problem, or those
that do not directly follow a recurrence relation, for example, variants of the
Matching problem.

</details>


### [21] [Engineering Algorithms for $\ell$-Isolated Maximal Clique Enumeration](https://arxiv.org/abs/2511.03525)
*Marco D'Elia,Irene Finocchi,Maurizio Patrignani*

Main category: cs.DS

TL;DR: 提出了四种剪枝启发式方法，用于高效枚举ℓ-孤立极大团，通过控制孤立度参数ℓ来过滤与外部过度连接的团，在社交网络等真实图上显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 极大团在众多应用领域中具有重要作用，但其数量庞大使得在实际应用中难以有效利用。ℓ-孤立极大团通过控制孤立度参数来过滤与外部过度连接的团，使结果更具实用性。

Method: 基于Tomita等人的极大团枚举递归算法，提出了四种剪枝启发式方法，可以单独或组合使用，丢弃保证不会产生ℓ-孤立极大团的递归搜索分支。

Result: 实验研究表明，其中两种启发式方法在具有社交网络属性的真实图上提供了显著的效率提升，优于基线方法和现有最先进方法。

Conclusion: 所提出的剪枝启发式方法能够有效提升ℓ-孤立极大团枚举的效率，特别是在真实世界图上表现优异，为实际应用提供了实用的解决方案。

Abstract: Maximal cliques play a fundamental role in numerous application domains,
where their enumeration can prove extremely useful. Yet their sheer number,
even in sparse real-world graphs, can make them impractical to be exploited
effectively. To address this issue, one approach is to enumerate
$\ell$-isolated maximal cliques, whose vertices have (on average) less than
$\ell$ edges toward the rest of the graph. By tuning parameter $\ell$, the
degree of isolation can be controlled, and cliques that are overly connected to
the outside are filtered out. Building on Tomita et al.'s very practical
recursive algorithm for maximal clique enumeration, we propose four pruning
heuristics, applicable individually or in combination, that discard recursive
search branches that are guaranteed not to yield $\ell$-isolated maximal
cliques. Besides proving correctness, we characterize both the pruning power
and the computational cost of these heuristics, and we conduct an extensive
experimental study comparing our methods with Tomita's baseline and with a
state-of-the-art approach. Results show that two of our heuristics offer
substantial efficiency improvements, especially on real-world graphs with
social network properties.

</details>


### [22] [Improved Bounds with a Simple Algorithm for Edge Estimation for Graphs of Unknown Size](https://arxiv.org/abs/2511.03650)
*Debarshi Chanda*

Main category: cs.DS

TL;DR: 提出了一种随机算法，使用Degree和Random Edge查询来估计图的平均度数，查询复杂度为Õ(α/ε²d)和Õ(1/ε²)，改进了先前的工作。同时给出了查询复杂度的下界。


<details>
  <summary>Details</summary>
Motivation: 改进Beretta等人(SODA 2026)的算法，该算法需要Õ_ε,log n(√(n/d))次查询且需要Degree、Neighbour和Random Edge查询。新算法更简单实用，不需要任何图参数作为输入。

Method: 使用随机算法，通过Degree和Random Edge查询来估计平均度数。提出了新的估计技术，算法不需要输入图的大小等参数。

Result: 算法能以Õ(α/ε²d)次Degree查询和Õ(1/ε²)次Random Edge查询获得(1±ε)精度的平均度数估计。同时证明了任何使用Degree、Neighbour和Random Edge查询的算法都需要Ω(min(d, α/d))次查询。

Conclusion: 新算法在查询复杂度和实用性方面都优于现有方法，同时建立了查询复杂度的理论下界，解决了Beretta等人提出的问题。

Abstract: We propose a randomized algorithm with query access that given a graph $G$
with arboricity $\alpha$, and average degree $d$, makes
$\widetilde{O}\left(\frac{\alpha}{\varepsilon^2d}\right)$ \texttt{Degree} and
$\widetilde{O}\left(\frac{1}{\varepsilon^2}\right)$ \texttt{Random Edge}
queries to obtain an estimate $\widehat{d}$ satisfying $\widehat{d} \in
(1\pm\varepsilon)d$. This improves the $\widetilde{O}_{\varepsilon,\log
n}\left(\sqrt{\frac{n}{d}}\right)$ query algorithm of [Beretta et al., SODA
2026] that has access to \texttt{Degree}, \texttt{Neighbour}, and
\texttt{Random Edge} queries. Our algorithm does not require any graph
parameter as input, not even the size of the vertex set, and attains both
simplicity and practicality through a new estimation technique. We complement
our upper bounds with a lower bound that shows for all valid $n,d$, and
$\alpha$, any algorithm that has access to \texttt{Degree}, \texttt{Neighbour},
and \texttt{Random Edge} queries, must make at least
$\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a
$(1\pm\varepsilon)$-multiplicative estimate of $d$, even with the knowledge of
$n$ and $\alpha$. We also show that even with \texttt{Pair} and
\texttt{FullNbr} queries, an algorithm must make
$\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a
$(1\pm\varepsilon)$-multiplicative estimate of $d$. Our work addresses both the
questions raised by the work of [Beretta et al., SODA 2026].

</details>


### [23] [An Improved Quality Hierarchical Congestion Approximator in Near-Linear Time](https://arxiv.org/abs/2511.03716)
*Monika Henzinger,Robin Münk,Harald Räcke*

Main category: cs.DS

TL;DR: 提出了第一个近线性时间算法，使用分层拥塞近似器实现O(log²n log log n)近似质量，改进了现有结果


<details>
  <summary>Details</summary>
Motivation: 解决图中拥塞近似器的计算时间与近似质量之间的权衡问题，现有算法要么运行时间长但近似质量好，要么运行时间快但近似质量差

Method: 使用分层拥塞近似器，包含O(n log n)个割；开发新的分区例程避免在大子图上递归；引入割的边界可路由性概念和改进的稀疏割预言机

Result: 实现了近线性时间算法，以高概率获得O(log²n log log n)近似质量；并行实现具有相同近似质量、多对数跨度和近线性工作量

Conclusion: 该算法在运行时间和近似质量之间取得了更好的平衡，改进了现有结果，并通过下界证明分层拥塞近似器的近似质量至少为Ω(log n)

Abstract: A congestion approximator for a graph is a compact data structure that
approximately predicts the edge congestion required to route any set of flow
demands in a network. A congestion approximator is hierarchical if it consists
of a laminar family of cuts in the graph. There is a tradeoff between the
running time for computing a congestion approximator and its approximation
quality. Currently, for an $n$-node graph there exists a polynomial time
algorithm that achieves a $O(\log^{1.5}n \log \log n)$ approximation and a
near-linear time algorithm that achieves w.h.p. a $O(\log^4 n)$ approximation.
In this paper we give the first near-linear time algorithm, that achieves
w.h.p. a $O(\log^2 n \log \log n)$ approximation, using an hierarchical
congestion approximator with $O(n \log n)$ cuts. Based on a reduction from
oblivious routing, we also present a lower bound of $\Omega(\log n)$ for the
approximation quality of hierarchical congestion approximators.
  Our algorithm can also be implemented in the parallel setting achieving the
same approximation quality, polylogarithmic span and near-linear work. This
improves upon the best prior parallel algorithm, which has a $O(\log^9n)$
approximation.
  Crucial for achieving a near linear running time is a new partitioning
routine that, unlike previous such routines, manages to avoid recursing on
large subgraphs. To achieve the improved approximation quality, we introduce
the new concept of border routability of a cut and give an improved sparsest
cut oracle for general vertex weights.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: SELF-REDRAFT框架在无测试反馈的代码生成场景中，通过平衡利用和探索策略，相比Self-Refine在相同迭代次数下表现更优，但反馈生成和判别能力仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 解决现实代码生成场景中测试用例不可用时，现有方法在贪婪利用和随机探索之间的平衡不足的问题。

Method: 基于Self-Refine构建SELF-REDRAFT框架，鼓励模型为有根本缺陷的解决方案提出新草案。

Result: SELF-REDRAFT在相同最大迭代次数下收敛时，性能始终优于Self-Refine，但反馈生成能力和判别判断仍有显著改进空间。

Conclusion: 本研究为测试时扩展中的内在探索-利用平衡建立了基准，并确定反馈和判别能力是未来改进的关键领域。

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [25] [The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review](https://arxiv.org/abs/2511.02859)
*Bianca Leech,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 系统文献综述分析了敏捷方法论向混合框架的演变，识别了实施挑战和成功因素。混合方法结合了迭代灵活性和结构化治理，在大型和受监管环境中弥补敏捷方法的局限性。


<details>
  <summary>Details</summary>
Motivation: IT项目的快速发展推动了项目管理方法论的转变，从传统瀑布式到敏捷框架，再到最近的混合模型。研究旨在了解敏捷方法如何演变为混合框架及其在实践中的应用。

Method: 采用PRISMA指导的系统文献综述方法，分析了过去8年同行评审的研究，识别关键趋势。

Result: 混合方法论源于敏捷方法在大型和受监管环境中的局限性，成功实施依赖于领导支持、定制化流程整合和持续改进机制。

Conclusion: 研究强调需要根据具体情境进行适应性调整而非采用僵化框架，为组织进行混合转型提供了实践见解。

Abstract: The rapid evolution of IT projects has driven the transformation of project
management methodologies, from traditional waterfall approaches to agile
frameworks and, more recently, hybrid models. This systematic literature review
investigates the evolution of agile methodologies into hybrid frameworks,
analysing their implementation challenges and success factors. We identify key
trends through PRISMA-guided analysis of peer-reviewed studies from the last 8
years. Hybrid methodologies emerge from agile limitations in large-scale and
regulated environments, combining iterative flexibility with structured
governance. Agile has several implementation challenges, leading to hybrid
methods, and the success hinges on leadership support, tailored process
integration, and continuous improvement mechanisms. The study explores the need
for contextual adaptation over rigid frameworks, offering practical insights
for organisations navigating hybrid transitions.

</details>


### [26] [LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models](https://arxiv.org/abs/2511.02866)
*Ahmad Tahmasivand,Noureldin Zahran,Saba Al-Sayouri,Mohammed Fouda,Khaled N. Khasawneh*

Main category: cs.SE

TL;DR: LM-Fix是一个轻量级的大语言模型故障检测和快速恢复框架，通过短测试向量和哈希引导检测位翻转故障，实现本地修复而无需完全重载。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型完整性方法通常过于笨重或缓慢，无法满足现代LLM的需求，需要一种轻量级且快速的故障检测和恢复解决方案。

Method: 运行短测试向量并通过哈希引导检查来检测位翻转故障，然后进行本地修复而不需要完全重载模型。

Result: 在多个模型上，TVL=200时检测到超过94%的单比特翻转和接近100%的多比特翻转，运行时开销约为1%到7.7%；恢复速度比重载快100倍以上。

Conclusion: LM-Fix提供了一个实用、低开销的解决方案，能够保持大语言模型在生产环境中的可靠性。

Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery
framework for faults in large language models (LLMs). Existing integrity
approaches are often heavy or slow for modern LLMs. LM-Fix runs a short
test-vector pass and uses hash-guided checks to detect bit-flip faults, then
repairs them locally without a full reload. Across multiple models, it detects
over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with
approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster
than reloading. These results show a practical, low-overhead solution to keep
LLMs reliable in production

</details>


### [27] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: AdvFusion是一种参数高效微调方法，在多语言代码模型上评估了代码生成、代码翻译和提交消息生成任务，发现不同任务和模型规模下性能表现各异。


<details>
  <summary>Details</summary>
Motivation: 探索AdvFusion方法在代码大语言模型上的扩展应用，验证其在多种软件工程任务中的有效性。

Method: 使用AdvFusion参数高效微调架构，在代码大语言模型上进行多任务评估（代码生成、代码翻译、提交消息生成），并与AdapterFusion、LoRA等方法对比。

Result: 不同任务表现不同：代码生成中AdvFusion优于AdapterFusion但不及其他PEFT方法；提交消息生成中AdapterFusion更好；代码翻译中AdvFusion整体表现较差，且模型规模越大性能差距越明显。

Conclusion: AdvFusion在不同代码任务和模型规模下表现不一致，需要根据具体任务和模型特性选择适当的参数高效微调方法。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [28] [An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering](https://arxiv.org/abs/2511.02874)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.SE

TL;DR: 本文对比了FMEA、FHA和FFIP三种安全分析方法，发现FFIP更适合现代互联系统的安全需求，并分析了这些方法在MBSE中的集成现状，指出目前缺乏统一集成框架。


<details>
  <summary>Details</summary>
Motivation: 随着系统日益复杂，在系统生命周期早期进行有效的安全分析至关重要。需要研究关键安全分析方法的能力及其在基于模型的系统工程中的集成现状。

Method: 采用两阶段方法：第一阶段对比FMEA、FHA和FFIP技术，分析其程序、优势和局限性；第二阶段回顾这些方法在MBSE中的集成研究。

Result: 分析显示FFIP在识别涌现系统行为、二阶效应和故障传播方面能力更强；MBSE集成主要集中于FMEA，FHA和FFIP集成尚不成熟；FMEA-MBSE集成可分为四类方法。

Conclusion: 目前缺乏统一的安全分析方法与MBSE集成框架，这为支持数字工程转型、实现更协同的生命周期安全管理提供了机会。

Abstract: As systems become increasingly complex, conducting effective safety analysis
in the earlier phases of a system's lifecycle is essential to identify and
mitigate risks before they escalate. To that end, this paper investigates the
capabilities of key safety analysis techniques, namely: Failure Mode and
Effects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional
Failure Identification and Propagation (FFIP), along with the current state of
the literature in terms of their integration into Model-Based Systems
Engineering (MBSE). A two-phase approach is adopted. The first phase is focused
on contrasting FMEA, FHA, and FFIP techniques, examining their procedures,
along with a documentation of their relative strengths and limitations. Our
analysis highlights FFIP's capability in identifying emergent system behaviors,
second-order effects, and fault propagation; thus, suggesting it is better
suited for the safety needs of modern interconnected systems. Second, we review
the existing research on the efforts to integrate each of these methods into
MBSE. We find that MBSE integration efforts primarily focus on FMEA, and
integration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration
efforts could be organized into four categories: model-to-model transformation,
use of external customized algorithms, built-in MBSE packages, and manual use
of standard MBSE diagrams. While our findings indicate a variety of MBSE
integration approaches, there is no universally established framework or
standard. This leaves room for an integration approach that could support the
ongoing Digital Engineering transformation efforts by enabling a more
synergistic lifecycle safety management methods and tools.

</details>


### [29] [CS Educator challenges and their solutions : A systematic mapping study](https://arxiv.org/abs/2511.02876)
*Anjali Chouhan,Sruti Srinivasa Ragavan,Amey Karkare*

Main category: cs.SE

TL;DR: 对过去5年计算机科学教育领域的研究进行结构化文献综述，分析教育者面临的挑战及应对措施，涵盖教学、情感、技术和制度等十个主题维度。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育快速发展，但教育者在教学环境中持续面临各种挑战。目前缺乏系统性的研究来分类和综合这些挑战及应对措施，不清楚哪些领域已得到充分关注，哪些仍需更多研究。

Method: 采用结构化文献综述方法，分析过去5年发表的同行评审研究论文，重点关注十个分类主题中的挑战和应对措施。

Result: 分析发现评估实践、教师培训、课堂管理和情感健康等领域存在重复性问题，同时识别了专业发展计划和政策干预等应对策略，并揭示了若干研究不足的领域。

Conclusion: 本研究整合了对计算机科学教育现状的理解，为研究人员、课程设计者和政策制定者提供了有价值的见解，旨在提高教学效果和教育者支持。

Abstract: Computer Science (CS) education is expanding rapidly, but educators continue
to face persistent challenges in teaching and learning environments.Despite
growing interest, limited systematic work exists to categorize and synthesize
the specific challenges faced by CS educators and the remedies adopted in
response.This is problematic because it remains unclear which areas have been
thoroughly addressed and which still lack sufficient scholarly attention. In
this study, we conducted a structured literature review of peer-reviewed
research papers published over the last five years, focusing on challenges and
remedies across ten categorized themes, including pedagogical, emotional,
technological, and institutional dimensions.Our analysis revealed recurring
issues in areas such as assessment practices, teacher training, classroom
management, and emotional well-being, along with various strategies such as
professional development programs and policy interventions adopted to mitigate
them while also revealing several areas that have received insufficient
attention.This review offers a consolidated understanding of the CS education
landscape, providing valuable insights for researchers, curriculum designers,
and policymakers aiming to improve teaching effectiveness and educator support.

</details>


### [30] [AgentSLA : Towards a Service Level Agreement for AI Agents](https://arxiv.org/abs/2511.02885)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.SE

TL;DR: 本文提出了基于ISO/IEC 25010标准的AI智能体质量模型和领域特定语言，用于支持AI智能体服务的SLA定义。


<details>
  <summary>Details</summary>
Motivation: AI智能体作为软件系统的关键组件，从Model-as-a-Service向Agent-as-a-Service转变，但缺乏QoS和SLA规范，导致难以确保系统质量。

Method: 开发基于ISO/IEC 25010标准的AI智能体质量模型，并设计领域特定语言来支持AI智能体服务的SLA定义。

Result: 提出了系统的质量框架和SLA定义工具，解决了AI组件质量保证的标准化问题。

Conclusion: 该研究为AI智能体的质量保证提供了标准化方法和工具支持，有助于提升智能软件系统的开发质量。

Abstract: AI components are increasingly becoming a key element of all types of
software systems to enhance their functionality. These AI components are often
implemented as AI Agents, offering more autonomy than a plain integration of
Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an
Agent-as-a-Service one, bringing new challenges to the development of smart
software systems. Indeed, while support for the design, implementation, and
deployment of those agents exist, the specification of Quality of Service (QoS)
and definition of Service Level Agreements (SLAs) aspects for those agents,
important to ensure the quality of the resulting systems, remains an open
challenge. Part of this is due to the difficulty to clearly define quality in
the context of AI components, resulting in a lack of consensus on how to best
approach Quality Assurance (QA) for these types of systems. To address this
challenge, this paper proposes both a quality model for AI agents based on the
ISO/IEC 25010 standard, and a domain specific language to support the
definition of SLAs for the services provided by these AI agents.

</details>


### [31] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: 生成式AI编程助手在遗留代码维护任务中能提高开发效率但不会提升代码理解能力，存在理解-性能差距


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI编程助手在遗留代码维护任务中对开发效率和代码理解能力的影响

Method: 在受试者内实验研究中，18名计算机科学研究生在有和没有Copilot的情况下完成功能实现任务

Result: Copilot显著减少了任务时间并增加了通过的测试用例数量，但理解分数没有差异，且理解与任务表现之间没有相关性

Conclusion: 虽然GenAI工具可以加速遗留代码库中的编程进度，但这种进步可能不会带来对代码库的更好理解

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [32] [Risk Estimation in Differential Fuzzing via Extreme Value Theory](https://arxiv.org/abs/2511.02927)
*Rafael Baez,Alejandro Olivas,Nathan K. Diamond,Marcelo Frias,Yannic Noller,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文应用极值理论(EVT)来评估差分模糊测试中遗漏或低估bug的风险，通过在真实Java库中进行实验，证明EVT能有效估计风险并节省测试成本。


<details>
  <summary>Details</summary>
Motivation: 差分模糊测试作为动态分析无法保证bug的完全检测，需要一种方法来评估在未发现bug的情况下继续测试的风险。

Method: 使用极值理论(EVT)分析差分模糊测试过程中观察到的最大差异分布，通过统计方法估计遗漏bug的风险。

Result: EVT方法在14.3%的情况下优于基线统计方法，在64.2%的情况下与基线持平，在真实Java库测试中平均节省了数千万字节码执行。

Conclusion: EVT为差分模糊测试提供了有效的风险评估框架，能够显著提高测试效率并降低测试成本。

Abstract: Differential testing is a highly effective technique for automatically
detecting software bugs and vulnerabilities when the specifications involve an
analysis over multiple executions simultaneously. Differential fuzzing, in
particular, operates as a guided randomized search, aiming to find (similar)
inputs that lead to a maximum difference in software outputs or their
behaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the
absence of bugs: from a differential fuzzing campaign that has observed no bugs
(or a minimal difference), what is the risk of observing a bug (or a larger
difference) if we run the fuzzer for one or more steps?
  This paper investigates the application of Extreme Value Theory (EVT) to
address the risk of missing or underestimating bugs in differential fuzzing.
The key observation is that differential fuzzing as a random process resembles
the maximum distribution of observed differences. Hence, EVT, a branch of
statistics dealing with extreme values, is an ideal framework to analyze the
tail of the differential fuzzing campaign to contain the risk. We perform
experiments on a set of real-world Java libraries and use differential fuzzing
to find information leaks via side channels in these libraries. We first
explore the feasibility of EVT for this task and the optimal hyperparameters
for EVT distributions. We then compare EVT-based extrapolation against baseline
statistical methods like Markov's as well as Chebyshev's inequalities, and the
Bayes factor. EVT-based extrapolations outperform the baseline techniques in
14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we
evaluate the accuracy and performance gains of EVT-enabled differential fuzzing
in real-world Java libraries, where we reported an average saving of tens of
millions of bytecode executions by an early stop.

</details>


### [33] [Assurance Case Development for Evolving Software Product Lines: A Formal Approach](https://arxiv.org/abs/2511.03026)
*Logan Murphy,Torin Viger,Alessio Di Sandro,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出了一种用于软件产品线(SPL)的变异性感知保证案例(AC)的形式化方法，支持统一的AC开发和回归分析。


<details>
  <summary>Details</summary>
Motivation: 在软件产品线中，为每个产品单独开发严格的保证案例是不可行的，且当产品线演进时难以评估变更影响。需要将AC开发和维护提升到整个产品线层面。

Method: 形式化定义了用于SPL的变异性感知AC语言，研究了基于模板的AC开发提升方法，定义了回归分析来评估SPL演进对AC的影响，并开发了基于模型的保证管理工具。

Result: 实现了一个模型驱动的保证管理工具，并通过医疗设备产品线的AC开发案例展示了方法的有效性。

Conclusion: 该方法能够为整个软件产品线同时开发单个AC，并以变异性感知的方式进行回归分析，解决了SPL环境下AC开发和维护的挑战。

Abstract: In critical software engineering, structured assurance cases (ACs) are used
to demonstrate how key system properties are supported by evidence (e.g., test
results, proofs). Creating rigorous ACs is particularly challenging in the
context of software product lines (SPLs), i.e, sets of software products with
overlapping but distinct features and behaviours. Since SPLs can encompass very
large numbers of products, developing a rigorous AC for each product
individually is infeasible. Moreover, if the SPL evolves, e.g., by the
modification or introduction of features, it can be infeasible to assess the
impact of this change. Instead, the development and maintenance of ACs ought to
be lifted such that a single AC can be developed for the entire SPL
simultaneously, and be analyzed for regression in a variability-aware fashion.
In this article, we describe a formal approach to lifted AC development and
regression analysis. We formalize a language of variability-aware ACs for SPLs
and study the lifting of template-based AC development. We also define a
regression analysis to determine the effects of SPL evolutions on
variability-aware ACs. We describe a model-based assurance management tool
which implements these techniques, and illustrate our contributions by
developing an AC for a product line of medical devices.

</details>


### [34] [Adaptive Detection of Software Aging under Workload Shift](https://arxiv.org/abs/2511.03103)
*Rafael José Moura,Maria Gizele Nascimento,Fumio Machida,Ermeson Andrade*

Main category: cs.SE

TL;DR: 提出基于机器学习的自适应软件老化检测方法，在动态工作负载环境下保持高检测性能。


<details>
  <summary>Details</summary>
Motivation: 软件老化影响长期运行系统的性能并增加故障风险，需要检测方法来应对动态工作负载条件。

Method: 比较静态模型与包含DDM和ADWIN自适应检测器的自适应模型，处理工作负载变化。

Result: 静态模型在新工作负载下性能显著下降，而ADWIN自适应模型在所有场景中F1-Score均高于0.93。

Conclusion: 自适应模型特别是ADWIN能有效应对工作负载变化，在软件老化检测中保持高精度。

Abstract: Software aging is a phenomenon that affects long-running systems, leading to
progressive performance degradation and increasing the risk of failures. To
mitigate this problem, this work proposes an adaptive approach based on machine
learning for software aging detection in environments subject to dynamic
workload conditions. We evaluate and compare a static model with adaptive
models that incorporate adaptive detectors, specifically the Drift Detection
Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept
drift scenarios and applied in this work to handle workload shifts. Experiments
with simulated sudden, gradual, and recurring workload transitions show that
static models suffer a notable performance drop when applied to unseen workload
profiles, whereas the adaptive model with ADWIN maintains high accuracy,
achieving an F1-Score above 0.93 in all analyzed scenarios.

</details>


### [35] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: 本文研究了大型代码模型(LCMs)中自动提示生成(APG)的重要性，提出了结合指令生成(IG)和多步推理(MSR)的新方法，显著提升了代码智能任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LCMs的提示设计主要依赖人工，耗时且对特定模型和任务敏感。虽然NLP领域已有自动提示生成研究，但在代码智能领域尚待探索，这限制了开发者在面对多样化任务和黑盒LCMs时的效率。

Method: 实证研究指令生成(IG)和多步推理(MSR)两个APG关键部分，评估多种APG方法在四个开源LCMs和三个代码智能任务上的表现，最终提出结合最佳IG和MSR方法的新APG方法。

Result: IG和MSR相比基础提示显著提升性能。新方法在代码翻译、代码摘要和API推荐任务上分别获得28.38%、58.11%和84.53%的平均改进，在工业数据集上API推荐的MRR提升达148.89%。

Conclusion: 自动提示生成对LCMs性能至关重要，结合IG和MSR的新方法能显著提升代码智能任务表现，为工业应用提供了有效解决方案。

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [36] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: RefAgent是一个基于LLM的多智能体框架，用于端到端软件重构，通过专门的规划、执行、测试和迭代优化智能体，显著提升重构效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在软件重构中依赖静态指令，而基于LLM的智能体能够动态适应上下文变化并自主决策，探索其在重构活动中的潜力。

Method: 引入RefAgent多智能体框架，包含负责规划、执行、测试和迭代优化的专门智能体，利用自反思和工具调用能力进行重构。

Result: 在8个开源Java项目上评估，RefAgent达到90%的中位单元测试通过率，减少52.5%的代码异味，提升8.6%的关键质量属性。相比单智能体方法，单元测试通过率提升64.7%，编译成功率提升40.1%。

Conclusion: 多智能体架构在推进自动化软件重构方面具有巨大潜力，能够有效提升重构质量和效率。

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [37] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本研究系统评估了五种最先进的模型编辑方法在代码LLMs上的表现，发现在API弃用场景下，即时编辑会导致模型性能显著下降，而顺序编辑会进一步加剧这种退化。


<details>
  <summary>Details</summary>
Motivation: LLMs在预训练后保持静态，而编程语言和API持续演进，导致生成过时或不兼容的代码。重新训练LLMs计算成本高昂，模型编辑作为一种轻量级替代方案具有潜力，但需要验证其是否能实现真正的语法和语义适应。

Method: 对五种模型编辑方法（Constrained FT、GRACE、MEMIT、PMET、ROME）在三个开源代码LLMs（CodeLlama、CodeQwen1.5、DeepSeek-Coder）上进行系统研究，在受控的API弃用场景下评估即时和顺序编辑设置。

Result: 即时编辑导致模型性能显著下降：语法有效性下降高达86个百分点，功能正确性在最佳设置下也下降45点。顺序编辑进一步加剧退化，在某些情况下模型性能完全崩溃。正确采用预期更改的情况仅占约6%。

Conclusion: 当前模型编辑方法在代码LLMs上的效果有限，大多数生成结果依赖于变通方案而非正确采用预期更改，表明需要更有效的模型编辑技术来应对API演进挑战。

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [38] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: 提出ProjectGen多智能体框架和CodeProjectEval数据集，解决项目级代码生成中的语义鸿沟、层次依赖管理和质量维护问题，在真实数据集上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有项目级代码生成研究存在数据集不真实、评估指标不可靠、用户需求与机器可解释结构间的语义鸿沟、层次依赖管理困难和质量维护挑战等局限性。

Method: 提出ProjectGen多智能体框架，将项目分解为架构设计、骨架生成和代码填充三个阶段，采用迭代优化和基于记忆的上下文管理，引入语义软件架构树(SSAT)来桥接用户需求和源代码实现。

Result: 在DevBench小规模数据集上通过52/124个测试用例，比基线方法提升57%；在CodeProjectEval数据集上通过310个测试用例，比基线提升约10倍。

Conclusion: ProjectGen框架和SSAT表示法有效解决了项目级代码生成的关键挑战，在真实数据集上实现了最先进的性能。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [39] [Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement](https://arxiv.org/abs/2511.03421)
*Shihai Wang,Tao Chen*

Main category: cs.SE

TL;DR: LQPR是一种高效自动化的性能需求量化方法，通过将量化问题转化为分类问题，使用轻量级语言诱导匹配机制，相比基于LLM的方法成本低两个数量级且效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有性能需求量化主要依赖人工方法，成本高且容易出错，需要自动化解决方案。

Method: 将性能需求量化转化为分类问题，设计轻量级语言诱导匹配机制，利用性能需求具有强模式且简洁的特点。

Result: 在多样化数据集上与9种最先进学习方法比较，LQPR在75%或更多案例中排名第一，成本低两个数量级。

Conclusion: 对于性能需求量化任务，专用方法比通用的LLM驱动方法更合适。

Abstract: Elicited performance requirements need to be quantified for compliance in
different engineering tasks, e.g., configuration tuning and performance
testing. Much existing work has relied on manual quantification, which is
expensive and error-prone due to the imprecision. In this paper, we present
LQPR, a highly efficient automatic approach for performance requirements
quantification.LQPR relies on a new theoretical framework that converts
quantification as a classification problem. Despite the prevalent applications
of Large Language Models (LLMs) for requirement analytics, LQPR takes a
different perspective to address the classification: we observed that
performance requirements can exhibit strong patterns and are often
short/concise, therefore we design a lightweight linguistically induced
matching mechanism. We compare LQPR against nine state-of-the-art
learning-based approaches over diverse datasets, demonstrating that it is
ranked as the sole best for 75% or more cases with two orders less cost. Our
work proves that, at least for performance requirement quantification,
specialized methods can be more suitable than the general LLM-driven
approaches.

</details>


### [40] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: U2F是一个认知启发的多智能体框架，通过拥抱不确定性来发现软件工程中的创新解决方案，显著提升了解决方案的新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件工程智能体主要解决明确定义的问题，往往忽略预定义框架之外的创新解决方案，无法应对开放世界软件环境中超越既定范式的新兴挑战。

Method: U2F包含两个关键组件：(1) 发现-探索-集成智能体系统，用于发掘和综合潜在解决方案；(2) 三个维度的认知增强机制：跨领域类比推理、逆向思维和外部验证，战略性地重构和扩展传统解决方案边界。

Result: 在218个真实世界软件赋能故事上的应用显示：整体新颖性提升14%，语义新颖性提升51%，可行性稳定在4.02/5.0，LLM评估器验证了这些结果。

Conclusion: 拥抱不确定性可以作为软件工程创新的催化剂，U2F框架展示了在开放世界环境中发现创新解决方案的潜力。

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [41] [Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding](https://arxiv.org/abs/2511.03549)
*Ziv Nevo,Orna Raz,Karen Yorav*

Main category: cs.SE

TL;DR: 提出了一种利用GitHub自然语言工件（如PR描述、issue讨论、提交信息）来增强LLM代码理解能力的新方法，通过提取上下文、生成代码目的解释和验证解释三个组件来提供更准确的代码理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成代码解释时缺乏软件工程上下文基础，需要利用GitHub中的自然语言工件来提供更全面的代码理解支持。

Method: 系统包含三个组件：提取和结构化GitHub上下文、使用上下文生成代码高级解释、验证解释。实现为独立工具和Model Context Protocol服务器。

Result: 用户研究表明，当生成洞察时，这些洞察通常是有帮助且非平凡的，并且没有出现幻觉现象。

Conclusion: 该方法能有效增强基于LLM的代码解释，通过GitHub上下文提供更准确和有用的代码理解洞察。

Abstract: Understanding the purpose of source code is a critical task in software
maintenance, onboarding, and modernization. While large language models (LLMs)
have shown promise in generating code explanations, they often lack grounding
in the broader software engineering context. We propose a novel approach that
leverages natural language artifacts from GitHub -- such as pull request
descriptions, issue descriptions and discussions, and commit messages -- to
enhance LLM-based code understanding. Our system consists of three components:
one that extracts and structures relevant GitHub context, another that uses
this context to generate high-level explanations of the code's purpose, and a
third that validates the explanation. We implemented this as a standalone tool,
as well as a server within the Model Context Protocol (MCP), enabling
integration with other AI-assisted development tools. Our main use case is that
of enhancing a standard LLM-based code explanation with code insights that our
system generates. To evaluate explanations' quality, we conducted a small scale
user study, with developers of several open projects, as well as developers of
proprietary projects. Our user study indicates that when insights are generated
they often are helpful and non trivial, and are free from hallucinations.

</details>


### [42] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: OpenHands Software Agent SDK是一个用于构建软件工程代理的工具包，提供灵活的代理实现、安全可靠的执行环境以及多种用户交互接口。


<details>
  <summary>Details</summary>
Motivation: 构建生产级软件工程代理是复杂任务，需要灵活的实施方案、可靠安全的执行环境以及用户交互接口。

Method: 重新设计OpenHands框架的代理组件，提供简单接口实现基础代理，支持扩展到复杂功能如自定义工具、内存管理等，并集成沙箱执行、生命周期控制、多LLM路由和安全分析。

Result: 在SWE-Bench Verified和GAIA基准测试中表现出色，为原型设计、定制应用开发和规模化部署提供实用基础。

Conclusion: OpenHands Software Agent SDK为软件工程代理的开发提供了完整解决方案，满足生产部署的各项需求。

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [43] [DecodeX: Exploring and Benchmarking of LDPC Decoding across CPU, GPU, and ASIC Platforms](https://arxiv.org/abs/2511.02952)
*Zhenzhou Qi,Yuncheng Yao,Yiming Li,Chung-Hsuan Tung,Junyao Zheng,Danyang Zhuo,Tingjun Chen*

Main category: cs.NI

TL;DR: DecodeX是一个统一的基准测试框架，用于评估不同硬件平台上的LDPC解码加速性能，揭示了并行效率和卸载开销之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虚拟化无线接入网络需要跨异构计算基板进行灵活高效的基带处理，但缺乏统一的基准测试框架来评估不同硬件平台的LDPC解码性能。

Method: 开发了DecodeX框架，集成了CPU、GPU和ASIC等多种硬件平台的LDPC解码器实现，包括内核、API和测试向量，可扩展到其他架构和配置。

Result: 系统地表征了不同平台如何编排计算（从线程和内存管理到数据移动和加速器卸载），并量化了不同物理层参数下的解码延迟，发现加速器增益强烈依赖于数据移动和工作负载粒度。

Conclusion: 跨平台基准测试可以为未来异构vRAN的自适应调度和协同设计提供指导，实现NextG无线系统的可扩展和节能基带处理。

Abstract: Emerging virtualized radio access networks (vRANs) demand flexible and
efficient baseband processing across heterogeneous compute substrates. In this
paper, we present DecodeX, a unified benchmarking framework for evaluating
low-density parity-check (LDPC) decoding acceleration across different hardware
platforms. DecodeX integrates a comprehensive suite of LDPC decoder
implementations, including kernels, APIs, and test vectors for CPUs (FlexRAN),
GPUs (Aerial and Sionna-RK), and ASIC (ACC100), and can be readily extended to
additional architectures and configurations. Using DecodeX, we systematically
characterize how different platforms orchestrate computation-from threading and
memory management to data movement and accelerator offload-and quantify the
resulting decoding latency under varying Physical layer parameters. Our
observations reveal distinct trade-offs in parallel efficiency and offload
overhead, showing that accelerator gains strongly depend on data-movement and
workload granularity. Building on these insights, we discuss how cross-platform
benchmarking can inform adaptive scheduling and co-design for future
heterogeneous vRANs, enabling scalable and energy-efficient baseband processing
for NextG wireless systems.

</details>


### [44] [Distributed Incast Detection in Data Center Networks](https://arxiv.org/abs/2511.03039)
*Yiming Zheng,Haoran Qi,Lirui Yu,Zhan Shu,Qing Zhao*

Main category: cs.NI

TL;DR: 提出了一种基于概率假设检验的分布式incast检测方法，通过分析新流到达间隔，从第一个数据包就能立即判断是否为incast流量，显著提升了检测速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 数据中心incast流量会导致严重的性能下降（如丢包和延迟增加），现有基于交换机端口出口队列长度或梯度的检测方法存在延迟检测和高错误率的问题。

Method: 采用分布式incast检测方法，利用概率假设检验和最优检测阈值，通过分析新流到达间隔来识别incast流量。

Result: 实验结果显示该方法在检测速度和推理准确性方面相比现有方法有显著提升。

Conclusion: 该方法能够从流量的初始数据包立即检测incast，解决了传统队列长度相关方法的延迟和错误率问题。

Abstract: Incast traffic in data centers can lead to severe performance degradation,
such as packet loss and increased latency. Effectively addressing incast
requires prompt and accurate detection. Existing solutions, including MA-ECN,
BurstRadar and Pulser, typically rely on fixed thresholds of switch port egress
queue lengths or their gradients to identify microburst caused by incast flows.
However, these queue length related methods often suffer from delayed detection
and high error rates. In this study, we propose a distributed incast detection
method for data center networks at the switch-level, leveraging a probabilistic
hypothesis test with an optimal detection threshold. By analyzing the arrival
intervals of new flows, our algorithm can immediately determine if a flow is
part of an incast traffic from its initial packet. The experimental results
demonstrate that our method offers significant improvements over existing
approaches in both detection speed and inference accuracy.

</details>


### [45] [CRSF: Enabling QoS-Aware Beyond-Connectivity Service Sharing in 6G Local Networks](https://arxiv.org/abs/2511.03081)
*Pragya Sharma,Amanda Xiang,Abbas Kiani,John Kaippallimalil,Tony Saboorian,Haining Wang*

Main category: cs.NI

TL;DR: 本文提出了一种用于6G网络的核心网络功能CRSF，支持跨子网络的服务发现和选择，通过QoS感知优化提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持跨子网络共享专业化服务，但目前缺乏标准化的服务发现和选择架构。

Method: 设计CRSF网络功能，将服务选择过程建模为QoS感知的优化问题，平衡服务质量指标和用户定义优先级。

Result: 在感知服务场景的仿真中，相比基线选择策略，实现了持续更高的聚合QoS。

Conclusion: CRSF为构建标准化、协作性和服务为中心的6G互联网络提供了基础且可扩展的机制。

Abstract: Sixth-generation (6G) networks are envisioned to support interconnected local
subnetworks that can share specialized, beyond-connectivity services. However,
a standardized architecture for discovering and selecting these services across
network boundaries has not existed yet. To address this gap, this paper
introduces the Central Repository and Selection Function (CRSF), a novel
network function for the 6G core that facilitates efficient inter-subnetwork
service discovery and selection. We formulate the selection process as a
QoS-aware optimization problem designed to balance service quality metrics with
user-defined priorities. We evaluate our system model through simulations for a
sensing service scenario and observe a consistently higher aggregate Quality of
Service (QoS) compared to the baseline selection strategy. The proposed CRSF
provides a foundational and extensible mechanism for building standardized,
collaborative, and service-centric interconnected networks essential for the 6G
era.

</details>


### [46] [Handover Configurations in Operational 5G Networks: Diversity, Evolution, and Impact on Performance](https://arxiv.org/abs/2511.03116)
*Moinak Ghoshal,Imran Khan,Phuc Dinh,Z. Jonny Kong,Omar Basit,Sizhe Wang,Yufei Feng,Y. Charlie Hu,Dimitrios Koutsonikolas*

Main category: cs.NI

TL;DR: 对5G网络中切换配置的深入测量研究，揭示了新的切换类型、过度激进的配置导致高信令开销、参数值多样性以及次优配置导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 5G大规模部署与4G/5G共存使切换过程更加复杂，但现有研究对5G运营网络中切换发生原因、方式及配置对性能的影响了解甚少。

Method: 通过在美国进行的四次跨州驾驶测试，历时27个月，对三大主要运营商的切换配置进行深入测量研究。

Result: 发现新的切换类型和事件、过度激进的配置导致不必要的高信令开销、参数值存在较大多样性（5G比LTE多样性显著降低）、次优配置导致切换前后性能不佳。

Conclusion: 研究结果对移动运营商持续优化5G切换配置具有重要指导意义。

Abstract: Mobility management in cellular networks, especially the handover (HO)
process, plays a key role in providing seamless and ubiquitous Internet access.
The wide-scale deployment of 5G and the resulting co-existence of 4G/5G in the
past six years have significantly changed the landscape of all mobile network
operators and made the HO process much more complex than before. While several
recent works have studied the impact of HOs on user experience, why and how HOs
occur and how HO configurations affect performance in 5G operational networks
remains largely unknown. Through four cross-country driving trips across the US
spread out over a 27-month period, we conduct an in-depth measurement study of
HO configurations across all three major US operators. Our study reveals (a)
new types of HOs and new HO events used by operators to handle these new types
of HOs, (b) overly aggressive HO configurations that result in unnecessarily
high signaling overhead, (c) large diversity in HO configuration parameter
values, which also differ across operators, but significantly lower diversity
in 5G compared to LTE, and (d) sub-optimal HO configurations/decisions leading
to poor pre- or post-HO performance. Our findings have many implications for
mobile operators, as they keep fine-tuning their 5G HO configurations.

</details>


### [47] [Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing](https://arxiv.org/abs/2511.03159)
*Shuting Qiu,Fang Dong,Siyu Tan,Ruiting Zhou,Dian Shen,Patrick P. C. Lee,Qilin Fan*

Main category: cs.NI

TL;DR: 提出CoCaR算法，通过将DNN模型分解为子模型，在移动边缘计算网络中联合优化子模型缓存和请求路由，以在资源约束下最大化推理精度。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器容量有限难以缓存所有DNN模型，且模型加载时间对用户体验的影响未被充分研究，需要更细粒度的模型缓存和路由方案。

Method: 引入动态DNN概念，将完整模型分解为相关子模型；提出基于线性规划和随机舍入的离线算法CoCaR，以及适应动态请求模式的在线算法CoCaR-OL。

Result: CoCaR相比最先进基线方法将用户请求的平均推理精度提高了46%；在线场景中CoCaR-OL在用户体验方面至少提升32.3%。

Conclusion: 动态DNN分解和联合优化缓存路由策略能有效提升边缘计算场景下的推理精度和用户体验。

Abstract: Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near
end-users, providing low-latency services and improving users' quality of
experience (QoE). However, caching all DNN models at edge servers with limited
capacity is difficult, and the impact of model loading time on QoE remains
underexplored. Hence, we introduce dynamic DNNs in edge scenarios,
disassembling a complete DNN model into interrelated submodels for more
fine-grained and flexible model caching and request routing solutions. This
raises the pressing issue of jointly deciding request routing and submodel
caching for dynamic DNNs to balance model inference precision and loading
latency for QoE optimization. In this paper, we study the joint dynamic model
caching and request routing problem in MEC networks, aiming to maximize user
request inference precision under constraints of server resources, latency, and
model loading time. To tackle this problem, we propose CoCaR, an offline
algorithm based on linear programming and random rounding that leverages
dynamic DNNs to optimize caching and routing schemes, achieving near-optimal
performance. Furthermore, we develop an online variant of CoCaR, named
CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online
request patterns. The simulation results demonstrate that the proposed CoCaR
improves the average inference precision of user requests by 46\% compared to
state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves
an improvement of no less than 32.3\% in user QoE over competitive baselines.

</details>


### [48] [Integrity Under Siege: A Rogue gNodeB's Manipulation of 5G Network Slice Allocation](https://arxiv.org/abs/2511.03312)
*Jiali Xu,Valeria Loscri,Romain Rouvoy*

Main category: cs.NI

TL;DR: 本文研究了5G网络切片分配中的完整性漏洞，攻击者可通过伪造切片请求劫持用户设备连接，导致服务质量严重下降或资源污染攻击。


<details>
  <summary>Details</summary>
Motivation: 5G网络切片技术虽然能提供定制化服务，但也引入了新的攻击面。本文旨在探索网络切片分配中的完整性漏洞，特别是攻击者如何操纵切片分配来破坏服务质量和资源完整性。

Method: 建立了基于风险分析的威胁模型，包括利用5G-EA0空加密等不安全配置。通过5G测试床进行实验评估，演示了恶意gNodeB作为中间人如何利用协议弱点伪造切片请求。

Result: 实验显示攻击可导致明显QoS下降（95%带宽减少、150%延迟增加），也可进行隐蔽的切片操纵而不产生核心网错误。资源污染攻击可导致超过60%的丢包率和UPF CPU饱和度达80%。

Conclusion: 5G网络动态资源管理的完整性安全亟需加强，提出了跨层缓解策略以保护服务等级协议和关键基础设施。

Abstract: The advent of 5G networks, with network slicing as a cornerstone technology,
promises customized, high-performance services, but also introduces novel
attack surfaces beyond traditional threats. This article investigates a
critical and underexplored integrity vulnerability: the manipulation of network
slice allocation to compromise Quality of Service (QoS) and resource integrity.
We introduce a threat model, grounded in a risk analysis of permissible yet
insecure configurations like null-ciphering (5G-EA0), demonstrating how a rogue
gNodeB acting as a Man-in-the-Middle can exploit protocol weaknesses to forge
slice requests and hijack a User Equipment's (UE) connection. Through a
comprehensive experimental evaluation on a 5G testbed, we demonstrate the
attack's versatile and severe impacts. Our findings show this integrity breach
can manifest as obvious QoS degradation, such as a 95% bandwidth reduction and
150% latency increase when forcing UE to a suboptimal slice, or as stealthy
slice manipulation that is indistinguishable from benign network operation and
generates no core network errors. Furthermore, we validate a systemic resource
contamination attack where redirecting a crowd of UE orchestrates a
Denial-of-Service, causing packet loss to exceed 60% and inducing measurable
CPU saturation (~80%) on core network User Plane Functions (UPFs). Based on
these results, we discuss the profound implications for Service Level
Agreements (SLAs) and critical infrastructure. We propose concrete, cross-layer
mitigation strategies for network operators as future work, underscoring the
urgent need to secure the integrity of dynamic resource management in 5G
networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE是一个新的形式代数定理评估基准系列，包含FATE-H和FATE-X两个组件，每个包含100个抽象代数和交换代数问题，难度从本科练习到超过博士资格考试水平。评估显示当前LLM在正式数学推理方面表现远差于竞赛数学。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在IMO等竞赛数学基准上表现出色，但这些基准无法反映现代数学研究的深度、广度和抽象性。需要建立能够评估高级数学推理能力的基准。

Method: 引入FATE基准系列，包含两个新组件FATE-H和FATE-X，每个包含100个代数问题，涵盖从本科到博士水平的难度范围。进行两阶段评估：自然语言推理和形式化推理。

Result: 最先进的LLM证明器在FATE基准上表现很差：在FATE-H上仅达到3%（pass@64）准确率，在FATE-X上为0%。模型在自然语言推理阶段比形式化推理阶段更准确。

Conclusion: FATE为研究级形式数学推理提供了稳健且具有挑战性的基准，揭示了当前模型在高级数学推理方面的显著差距。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [50] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm是一个用于动态群组推荐的框架，通过轻量级图卷积网络捕获高阶用户信息，支持实时群组形成和动态调整群组数量，相比静态方法在群组质量和推荐准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的群组推荐系统大多假设群组是静态或预定义的，不适用于现实世界中动态变化的场景。因此需要解决群组形成这一核心挑战。

Method: 采用轻量级GCN架构捕获高阶结构信号，使用随机聚类学习实现无需重新训练的自适应群组重构，并通过对比学习在动态条件下优化群组。

Result: 在多个数据集上的实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。

Conclusion: DeepForm成功解决了动态群组推荐中的群组形成问题，满足了实时性和自适应性的操作需求，为实际应用提供了有效解决方案。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [51] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 通过从在公共ARC任务上预训练的微小递归模型出发，可以在竞赛允许的计算限制内高效微调，在竞赛中仅用12,500步梯度更新就达到6.67%的半私有评估分数。


<details>
  <summary>Details</summary>
Motivation: 解决传统TRM方法在ARC竞赛中计算成本过高的问题，探索在有限计算资源下实现有效性能的方法。

Method: 先在大规模公共ARC任务上预训练7M参数的递归神经网络，然后在竞赛任务上进行完整的微调（非LoRA或仅任务嵌入微调）。

Result: 预训练模型在公共评估集上获得约10%的分数，竞赛微调后在半私有评估任务上达到6.67%的分数。

Conclusion: 证明从预训练的微小递归模型出发进行完整微调，可以在有限计算预算内实现有竞争力的性能。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [52] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 提出了一个基于AI的框架，利用海面温度和叶绿素浓度等海洋参数来预测北印度洋的潜在渔区，帮助渔民减少搜索时间、降低燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 北印度洋（包括阿拉伯海和孟加拉湾）是沿海社区的重要生计来源，但渔民在寻找高产渔场时常常面临不确定性。

Method: 使用AI辅助框架，通过海洋学参数（如海面温度和叶绿素浓度）来预测潜在渔区。

Result: 初步结果表明，该框架能够支持渔民减少搜索时间、降低燃料消耗，并促进高效的资源利用。

Conclusion: 该AI框架能够提高潜在渔区识别的准确性，为可持续捕捞实践提供区域特定的见解。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [53] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出使用大语言模型进行人体活动识别系统中的数据投毒检测和净化，通过零样本、单样本和少样本学习实现，减少对大规模标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 物联网中可穿戴设备的广泛使用需要可靠的人体活动识别技术，但机器学习模型容易受到数据投毒攻击，传统防御方法需要大量标注数据，难以适应动态环境。

Method: 使用大语言模型进行投毒检测和净化，采用角色扮演提示和逐步思考推理策略，让LLM识别传感器数据中的异常并推断干净的替代数据。

Result: 通过广泛评估验证了框架在检测精度、净化质量、延迟和通信成本方面的有效性，证明了LLM在提升可穿戴物联网系统安全性和可靠性方面的实用性。

Conclusion: 大语言模型能够为人体活动识别系统提供强大、自适应的防御机制，减少对大规模标注数据集的依赖，提高系统在动态物联网环境中的安全性和可靠性。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [54] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 使用Llama 3.1-405B大语言模型自动生成基因组数据使用案例标签，无需人工标注或训练数据集，在零样本数据引用分类任务中达到0.674的F1分数。


<details>
  <summary>Details</summary>
Motivation: 近年来需要识别特定数据集与引用它们的科学文献之间的关联，了解数据如何被使用，但人工标注成本高昂且需要开发训练数据集。

Method: 应用开源LLM Llama 3.1-405B生成结构化数据使用案例标签，并引入新的评估框架来验证方法效果。

Result: 标准模型在零样本数据引用分类任务中达到0.674的F1分数，无需预定义类别。

Conclusion: 结果有前景但受到数据可用性、提示过拟合、计算基础设施和负责任性能评估所需费用等障碍的限制。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [55] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对ROGUE多臂老虎机框架开发了ROGUE-TS算法，通过概率裁剪平衡个性化推荐与群体效应学习，在微随机化试验中实现低遗憾和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用而缺乏充分探索，限制了群体效应估计能力，这在微随机化试验中尤为重要。

Method: 开发ROGUE-TS汤普森采样算法，并引入概率裁剪程序来平衡个性化与群体学习，量化遗憾与最小探索概率之间的权衡。

Result: 在两个微随机化试验数据集上的验证表明，该方法比现有方法获得更低遗憾，通过裁剪程序保持高统计功效且不显著增加遗憾。

Conclusion: 该框架为设计微随机化试验的研究人员提供了平衡个性化与统计有效性的实用指导，能够可靠检测治疗效果同时考虑个体行为动态。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [56] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: 提出了一种结合数字孪生和图神经网络的路面健康监测框架，通过图结构建模道路网络，实现数据驱动的预测性维护。


<details>
  <summary>Details</summary>
Motivation: 传统路面管理系统多为被动响应，缺乏实时智能来预防故障和优化维护计划，需要解决复杂空间依赖性和非线性劣化问题。

Method: 将路面段和空间关系建模为图节点和边，利用无人机、传感器和激光雷达数据流输入数字孪生，采用归纳式图神经网络学习劣化模式。

Result: 在真实世界数据集上训练，模型R2达到0.3798，优于基线回归器，能有效捕捉非线性退化。

Conclusion: DT-GNN集成提高了预测精度，建立了持续改进的闭环反馈，为主动、智能和可持续的路面管理奠定了基础。

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [57] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: UserAlign是一种推理时个性化对齐方法，通过少量成对响应比较来获取用户偏好，基于逻辑赌博机中的最佳臂识别理论框架，从固定响应池中选择个性化响应。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法要么需要大量用户偏好查询，要么要求将偏好明确指定为文本输入，这在实际应用中存在限制。

Method: 基于逻辑赌博机的最佳臂识别理论框架，将用户反馈视为一致且无噪声的，通过少量成对响应比较快速识别最佳响应。

Result: 在个性化文本和图像生成等多个任务上的实验结果表明，UserAlign在实现个性化对齐方面具有有效性。

Conclusion: UserAlign提供了一种高效的推理时个性化对齐方法，能够以少量用户查询实现与用户偏好的对齐。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [58] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 将期望信息价值(EVOI)整合到Bootstrapped DQN框架中，通过测量不同网络头之间的意见差异来指导探索，在稀疏奖励的复杂环境中实现更好的探索-利用平衡。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机局部策略噪声的探索策略（如ε-greedy和Boltzmann探索）在高维状态和稀疏奖励环境中难以有效平衡探索与利用，需要更智能的探索机制。

Method: 在Bootstrapped DQN算法框架中整合期望信息价值(EVOI)，开发两种新算法，利用信息价值估计来测量不同网络头之间的意见差异，并引导探索向最有潜力的区域。

Result: 在复杂的稀疏奖励Atari游戏中，算法表现出更高的性能，更好地利用了随机网络初始化产生的不确定性，且没有引入额外的超参数。

Conclusion: 将期望信息价值整合到Bootstrapped DQN中能够有效增强深度探索能力，在复杂稀疏奖励环境中实现更好的性能，同时充分利用网络不确定性。

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [59] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出基于神经网络的双尺度超材料设计框架，通过连续表示解决异质单元设计中的兼容性问题


<details>
  <summary>Details</summary>
Motivation: 传统异质超材料设计面临巨大设计空间和相邻单元兼容性挑战，现有方法存在边界不连续或依赖固定库的限制

Method: 使用多尺度神经表示，神经网络同时输入全局和局部坐标，输出表示多尺度结构的隐式场，通过兼容性损失确保单元间连接

Result: 框架能生成任意高分辨率的超材料设计，支持无限上采样，在力学超材料、负泊松比和力学隐身等应用中验证有效性

Conclusion: 该神经网络框架为异质超材料设计提供了连续、兼容且无需预定义数据集的新方法

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [60] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI是一种基于贝叶斯样本推理(BSI)的单次图生成模型，通过在分布参数的连续空间中迭代优化图结构的信念来处理离散图结构，在分子和合成图生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在分子生成、知识图谱和网络分析中很重要，但其离散、无序的特性使传统生成模型难以处理，因此需要开发离散扩散和流匹配模型。

Method: 提出GraphBSI模型，基于贝叶斯样本推理(BSI)，在分布参数的连续空间中迭代优化图结构的信念；将BSI表述为随机微分方程(SDE)，并推导出通过分数函数近似保持边际分布的噪声控制SDE族。

Result: 在分子和合成图生成任务上表现出最先进的性能，在标准基准测试Moses和GuacaMol上超越了现有的单次图生成模型。

Conclusion: GraphBSI通过贝叶斯样本推理方法有效处理离散图结构生成问题，理论分析揭示了与贝叶斯流网络和扩散模型的联系，实验验证了其优越性能。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [61] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出了一种自适应无传感器监测方法，通过残差校正框架修正无传感器模型的系统偏差，在集装箱温湿度监测中显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统无传感器监测方法无法整合遥测信息和修正系统误差，导致预测结果与实时数据差异显著，给用户带来困惑。

Method: 引入残差校正方法作为通用框架，在观察到实时遥测数据后修正无传感器模型的系统偏差，实现自适应无传感器监测。

Result: 在348万数据点上评估，自适应无传感器模型相比基线模型在温度MAE上从2.43°C降至2.24-2.31°C，相对湿度MAE从7.99%降至5.72-7.09%；温度RMSE从3.38°C降至3.19-3.26°C，相对湿度RMSE从10.0%降至7.70-9.12%。

Conclusion: 自适应无传感器模型能够实现更准确的货物监测、早期风险检测，并减少全球航运中对完全连接的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [62] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出了一种新的分布优化算法DADO，能够利用设计变量的可分解性结构，通过图消息传递在因子间协调优化，提高离散设计空间的优化效率。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学工程中，需要在离散设计空间（如蛋白质序列）中根据特定属性进行设计。当前分布优化算法无法利用属性预测器中存在的可分解性结构，限制了优化效率。

Method: DADO算法使用软因子化的搜索分布作为学习生成模型，通过图消息传递在由联结树定义的可分解结构上进行协调优化。

Result: DADO算法能够有效利用设计变量的可分解性，在离散组合空间中实现更高效的分布优化。

Conclusion: DADO为利用可分解结构的分布优化提供了一种新方法，有望在蛋白质设计、电路布局、材料发现等科学应用中提升设计效率。

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [63] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 使用Vision Transformer (ViT)架构从隐含波动率表面预测资产未来30天的已实现波动率，探索了将图像识别技术应用于金融期权数据的可行性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在金融预测中显示出复杂性优势，但Transformer架构在期权数据中的应用仍较少探索。本文旨在开发适用于期权数据的Transformer模型。

Method: 训练Vision Transformer (ViT)架构，从单日的隐含波动率表面（增强日期信息）预测未来30天的已实现波动率。

Result: ViT能够从IV表面学习季节性模式和非线性特征，显示出模型开发的良好前景。

Conclusion: 将ViT应用于期权数据是可行的，为金融时间序列预测提供了有前景的新方向。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [64] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出了首个用于目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性和微调LLM来适应分布变化，无需依赖人工标注的理想响应。


<details>
  <summary>Details</summary>
Motivation: 企业应用中LLM代理与人类的目标驱动交互系统难以评估：数据复杂且未标注；人工标注难以规模化；自定义指标只能监测已知错误；LLM评估结果不可靠。

Method: 利用未标注交互数据的统计特性，使用微调LLM适应分布变化，开发了用户目标标注、目标完成度测量和LLM不确定性量化的无监督指标。

Result: 在开放领域和任务特定交互数据上验证了方法的有效性。

Conclusion: 该无监督评估框架为目标驱动交互系统提供了可靠且可扩展的评估方案，解决了传统评估方法的局限性。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [65] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 本文提出了一个将Transformer语言模型与广义相对论类比的几何框架，将查询和键视为诱导表示空间有效度量的机制，注意力作为离散连接实现值向量在token间的平行传输，并通过实验验证了嵌入空间曲率的存在和影响。


<details>
  <summary>Details</summary>
Motivation: 将Transformer语言模型与广义相对论进行类比，探索其几何本质，特别是嵌入空间中的曲率效应如何影响token表示的演化轨迹。

Method: 设计三个实验：(1)可视化完整段落的曲率景观，展示局部转向角在token和层间的变化；(2)通过模拟证明锐角/平角过多和长度-弦长比增加不能由维度或偶然性解释；(3)受爱因斯坦日食实验启发，在受控上下文编辑下探测偏转，展示注意力诱导曲率导致的嵌入轨迹弯曲。

Result: 实验证实了嵌入空间曲率的存在：可视化显示局部转向角变化，模拟排除了维度和偶然性解释，上下文编辑实验展示了可测量且意义一致的嵌入轨迹弯曲。

Conclusion: Transformer语言模型确实表现出类似广义相对论的几何特性，注意力机制在表示空间中诱导了曲率，导致token嵌入沿着弯曲而非直线路径演化，这为理解语言模型的内部工作机制提供了新的几何视角。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [66] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 该论文提出了一种新的图相似性度量方法——图同态失真，能够完全表征图结构，是一种完整的图嵌入方法，并通过采样实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 长期以来，图神经网络的表达能力仅通过组合特性来衡量。作者希望摆脱这一传统，提供一种基于原则的方法来度量顶点属性图之间的相似性。

Method: 提出图同态失真作为图相似性度量，通过采样方法高效计算该度量，并证明该度量可以转化为度量空间中的距离。

Result: 图同态失真能够完全区分BREC数据集中4-WL无法区分的图，在ZINC-12k数据集上表现优于先前基于同态的方法。

Conclusion: 该理论结果为图的表征开辟了新途径，将图论传统扩展到新的前沿领域。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [67] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出MSUCB算法，通过中位数均值估计器解决在线学习排序中的点击欺诈问题，在无污染时实现最优对数遗憾，在污染下遗憾仅随总污染量线性增加


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统容易受到点击欺诈等操纵攻击，这些污染反馈会误导学习过程并降低用户体验

Method: 提出MSUCB算法，采用新颖的中位数均值估计器，在无污染时表现如标准均值，在污染时通过中位数步骤过滤异常值和污染样本

Result: 在真实数据集上的实验表明，该方法始终优于先前方法，对两种最先进方法的遗憾改进分别达到97.35%和91.60%

Conclusion: MSUCB算法在无污染时实现最优性能，在污染下保持强鲁棒性，是处理在线学习排序中点击欺诈问题的有效解决方案

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [68] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 该论文提出了SparKer方法，通过稀疏、局部性和竞争性三个原则构建自组织局部核，用于在高维表示空间中检测异常，并在多个实际应用中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代AI虽然能提取丰富的表示，但这些表示的统计特性难以控制，导致异常检测方法在弱信号或罕见信号面前失效，需要一种在最小先验信息下仍能有效检测异常的方法。

Method: 提出SparKer方法，使用稀疏高斯核集成，在半监督Neyman-Pearson框架下训练，局部建模可能包含异常的样本与正常参考样本之间的似然比。

Result: 实验表明，仅包含少量核的集成就能在数千维的表示空间中识别统计显著的异常位置，证明了方法的可解释性、效率和可扩展性。

Conclusion: 通过稀疏性、局部性和竞争性原则构建的自组织局部核方法能够有效解决高维表示空间中的异常检测问题，在科学发现、开放世界新颖性检测等多个领域具有应用价值。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [69] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出了DiCoDe框架，通过投影通用引导和评论家蒸馏机制，解决多智能体环境协同设计中的可扩展性和样本效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前协同设计方法难以处理高维环境设计空间，且在联合优化中面临移动目标导致的样本效率低下问题。

Method: DiCoDe框架包含两个核心创新：投影通用引导采样技术和评论家蒸馏机制，前者探索奖励最大化环境分布并满足硬约束，后者共享强化学习评论家知识。

Result: 在仓库自动化、多智能体路径规划和风电场优化等基准测试中表现优异，在仓库设置中奖励提高39%，模拟样本减少66%。

Conclusion: DiCoDe为智能体-环境协同设计设立了新标准，是实现现实世界协同设计收益的重要步骤。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [70] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 提出CTF-IDF算法和改进的IRLBA降维方法，结合传统机器学习技术，在文本分析中实现比深度学习方法更高效、更快速且计算资源消耗更少的解决方案，同时保持可接受的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在文本分析领域的广泛应用导致计算资源和能耗急剧增加，带来严重的碳足迹问题。需要寻找更高效、计算成本更低的替代方案。

Method: 改进传统的TF-IDF算法为CTF-IDF，结合IRLBA算法进行降维处理，构建基于传统机器学习的文本分析流程。

Result: 实验结果显示，与传统深度学习方法相比，该方法在时间复杂度和模型准确性方面都有显著提升，同时大幅降低了计算资源消耗和碳足迹。

Conclusion: 传统机器学习方法结合CTF-IDF和IRLBA技术可以在文本分析中提供比深度学习方法更环保、高效的解决方案，仅需在准确率上做出微小妥协。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [71] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: 提出了一种新的梯度估计方法，通过操纵上游雅可比矩阵来减少偏差和方差，在更宽的网络中表现更好，有望扩展到大型网络。


<details>
  <summary>Details</summary>
Motivation: 反向传播需要两次前向传播和存储中间激活值，而现有的前向模式自动微分方法由于估计方差高难以扩展到小型网络之外，且缓解方法会引入显著偏差。

Method: 通过操纵上游雅可比矩阵来计算猜测方向，减少梯度估计的偏差和方差。

Result: 该方法显示出有希望的结果，随着网络宽度增加性能更好，有望扩展到更大网络。

Conclusion: 该方法通过分析偏差和方差及其与神经网络梯度低维结构的联系，提供了一种有前景的梯度估计替代方案。

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [72] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff是首个在整个生成轨迹中强制执行Fokker-Planck方程物理学的抗体生成器，通过FPE残差损失确保物理一致性，在RAbD基准测试中建立了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在两个核心挑战：缺乏动力学一致性导致物理上不可行的结构，以及由于数据稀缺和结构偏差导致的泛化能力差。

Method: 在混合流形CDR几何结构上最小化新型FPE残差损失，将局部学习的去噪分数组合成全局一致的概率流，与SE(3)-等变扩散框架中的深度生物先验协同集成。

Result: 在从头CDR-H3设计中，可变区叠加的均方根偏差为0.99Å，比先前最先进模型AbX提高25%；在更具挑战性的六CDR协同设计任务中，全链均方根偏差降低约15%，CDR-H3环的氨基酸回收率达到45.67%。

Conclusion: 通过将生成动力学与物理定律对齐，FP-AbDiff增强了鲁棒性和泛化能力，为物理忠实和功能可行的抗体设计建立了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [73] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了基于增强重叠理论的自监督对比学习理论框架，放松了条件独立假设，推导出下游性能的渐近闭界，并开发了无需额外模块的无监督表示评估指标。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习虽然取得了巨大成功，但其工作机制尚不清晰，特别是条件独立假设在实际中往往不成立，需要更实用的理论框架。

Method: 首先基于条件独立假设给出最紧边界，然后放松该假设，提出更实用的增强重叠理论，认为在激进数据增强下同类样本的支持集会更重叠，从而通过对齐正样本实现同类样本聚类。

Result: 开发了与下游性能高度一致的无监督表示评估指标，几乎不需要额外模块，代码已开源。

Conclusion: 增强重叠理论为理解对比学习机制提供了新视角，所提出的无监督评估指标能够有效衡量表示质量。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [74] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出了Static Deceptor (StaDec)和Dynamic Deceptor (DyDec)两种攻击框架，用于生成动态自适应对抗样本来评估LLM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当将LLMs应用于敏感任务时，需要系统评估其对对抗输入的鲁棒性。现有的方法依赖外部启发式方法，缺乏自动化和适应性。

Method: 利用LLM驱动的自动化流水线生成语义相似但能有效欺骗目标LLM的对抗样本，无需依赖外部启发式方法。

Result: 攻击方法能够随着LLM的进步而演进，并在攻击者未知的模型上表现出强大的可迁移性。

Conclusion: 这项工作为LLM的鲁棒性自我评估提供了系统性方法，有助于提高LLM在敏感任务中的安全性。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [75] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种名为自适应分位数重校准（AQR）的测试时适应技术，通过通道级别的分位数对齐来修改预激活分布，无需目标域先验知识或模型重训练，在各种架构和数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统领域适应方法依赖目标域先验知识或需要模型重训练的问题，以及现有测试时适应方法无法捕捉复杂激活分布且局限于特定归一化层的局限性。

Method: AQR通过通道级别的分位数对齐来修改预激活分布，采用鲁棒的尾部校准策略处理不同批次大小下的分布尾部估计问题，利用训练时计算的源域统计量实现无监督适应。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C数据集上的实验表明，AQR在多种架构下实现了稳健的适应性能，优于现有的测试时适应基线方法。

Conclusion: AQR在动态和不可预测数据分布的真实场景中具有部署潜力，能够有效提升深度学习模型在现实场景中的泛化能力。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [76] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: F2A框架通过联合预测-异常损失和检索增强生成模块，赋予时间序列基础模型异常预测能力，实现零样本异常预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于特定系统且无法适应随时间演变的异常模式，而预训练时间序列基础模型虽然具有强大的泛化能力，但尚未用于异常预测任务。

Method: 提出F2A框架：1）联合预测-异常损失，在异常时间点也能准确预测未来信号；2）检索增强生成模块，检索历史相关时段并基于此进行预测，动态适应分布变化。

Result: 在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A始终优于最先进的方法。

Conclusion: F2A通过针对性微调和动态检索，弥合了稳健TSFM零样本预测与零样本异常预测之间的差距，为实际应用提供了可扩展的零样本异常预测解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [77] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: UnCLe是一种新颖的深度学习动态因果发现方法，通过解耦器和重耦合器网络将时间序列分解为语义表示，利用自回归依赖矩阵学习变量间依赖关系，并通过时间扰动分析预测误差来估计动态因果影响。


<details>
  <summary>Details</summary>
Motivation: 从观测时间序列中揭示因果关系对理解复杂系统至关重要。现有方法主要推断静态因果图，但现实系统通常表现出动态因果关系——关系随时间演变，需要时间分辨的因果图来准确捕捉这些时间动态。

Method: 使用一对解耦器和重耦合器网络将输入时间序列解耦为语义表示；通过自回归依赖矩阵学习变量间依赖关系；通过分析时间扰动引起的数据点级预测误差来估计动态因果影响。

Result: 在静态因果发现基准测试中优于最先进的基线方法；更重要的是，在合成和真实世界动态系统（如人体运动）中能够准确捕捉和表示演化中的时间因果关系。

Conclusion: UnCLe为揭示复杂现象中潜在的、随时间变化的机制提供了一种有前景的方法。

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [78] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出了PSD框架，用于在无监督强化学习中自动发现周期性技能，通过将状态映射到圆形潜在空间来编码周期性，能够学习具有不同周期的多样化技能。


<details>
  <summary>Details</summary>
Motivation: 当前的无监督技能发现方法往往忽视技能的周期性特征，而许多机器人任务（特别是运动任务）需要在不同时间尺度上执行周期性行为，因此发现多样化周期性技能至关重要。

Method: PSD框架训练编码器将状态映射到圆形潜在空间，在潜在表示中自然编码周期性。通过捕捉时间距离，能够有效学习具有不同周期的技能。

Result: PSD能够在复杂的机器人任务中学习具有多样化周期的技能，即使在基于像素的观察下也能工作。这些学习到的技能在下游任务（如跨栏）中表现出高性能。

Conclusion: PSD成功发现了周期性行为，并且与现有技能发现方法结合可以提供更多样化的行为，扩展了智能体的技能库。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [79] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出了一种基于熵的线性注意力机制，通过理论证明熵的严格凹性，开发了线性复杂度的熵近似算法，在时空时间序列建模中实现竞争性性能并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制因二次计算复杂度限制了在长序列中的可扩展性，需要开发更高效的线性注意力方法。

Method: 基于熵理论的线性注意力机制，利用熵作为严格凹函数的性质，开发线性复杂度的熵近似算法，通过熵相等实现注意力权重计算。

Result: 在四个时空数据集上的实验表明，该方法在预测性能上具有竞争力或更优表现，同时显著减少了内存使用和计算时间。

Conclusion: 注意力机制在时空时间序列建模中的有效性可能主要源于获得适度平衡的权重分布，而非softmax的非线性，线性注意力机制是可行的替代方案。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [80] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于copula的多模态学习框架，通过建模不同模态间的复杂交互来学习联合分布，能够有效处理缺失模态问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在多种数据模态，现有方法主要通过拼接或Kronecker积简化模态间交互结构，无法充分建模复杂的高阶交互，且对潜在表示联合分布的研究不足。

Method: 使用copula统计结构作为桥梁连接联合分布和边际分布，假设每个模态服从高斯混合分布，在联合分布上应用copula模型来对齐不同模态的边际分布。

Result: 在MIMIC公开数据集上的大量实验表明，该模型在性能上优于其他竞争方法，能够为缺失模态生成准确的表示。

Conclusion: copula驱动的多模态学习框架能够有效捕捉模态间复杂交互，在处理缺失模态问题上表现出色。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [81] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U-Net进行气候统计降尺度，结合确定性U-Net主干和变分潜在空间来捕捉随机不确定性，评估了四种训练目标在降水和温度降尺度任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型受限于计算成本，通常只能生成粗空间分辨率输出，而许多气候变化影响研究需要更精细的尺度，统计降尺度可以弥合这一差距。

Method: 采用概率U-Net架构，结合确定性U-Net主干和变分潜在空间，评估了四种训练目标（afCRPS和WMSE-MS-SSIM的三种设置）进行16倍分辨率降尺度。

Result: WMSE-MS-SSIM在某些设置下对极端事件表现良好，而afCRPS能更好地捕捉跨尺度的空间变异性。

Conclusion: 不同的训练目标在气候降尺度任务中各有优势，需要根据具体应用需求选择合适的训练目标。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [82] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 提出基于VAE-MLP的轻量级物联网僵尸网络检测框架，通过变分自编码器降维后使用MLP分类器，系统评估了QAT和PTQ两种量化策略在检测性能、存储效率和推理延迟方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习检测方法计算量大，难以在资源受限的物联网设备上部署，需要开发轻量级检测模型。

Method: 使用预训练VAE编码器将高维训练数据降维到8维潜在向量，然后训练MLP分类器，并系统评估QAT和PTQ两种量化策略。

Result: PTQ在检测准确率上仅有轻微下降，实现6倍加速和21倍尺寸缩减；QAT准确率下降较明显，实现3倍加速和24倍压缩。

Conclusion: 量化技术为设备级物联网僵尸网络检测提供了实用性解决方案，其中PTQ在性能保持和效率提升方面表现更优。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [83] [Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning](https://arxiv.org/abs/2511.03238)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习识别能够长期提高城市生活质量的气候适应路径，通过集成评估模型结合降雨预测、洪水模型、交通可达性和生活质量指数，结果表明该方法优于现实世界规划策略。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致城市洪水频率和严重性增加，影响城市生活质量，而政策制定者需要应对气候变化的不确定性和城市洪水的复杂性。

Method: 使用强化学习（RL）和集成评估模型（IAM），结合降雨预测模型、洪水模型、交通可达性模型和生活质量指数。

Result: 初步结果表明，该方法能够学习最优适应措施，并且优于其他现实和真实世界的规划策略。

Conclusion: 提出的框架能够有效识别提高长期生活质量的气候适应路径，且性能优于传统规划方法。

Abstract: Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [84] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: FCDC将数据收集建模为闭环控制问题，通过在线概率模型和反馈机制动态调节样本保留，实现数据多样性和减少冗余。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统受限于数据质量和多样性，传统开环数据收集方式积累冗余样本，导致存储效率低、标注成本高和泛化能力有限。

Method: 使用在线概率模型近似已收集数据分布状态，基于似然和马氏距离等反馈信号自适应调节样本保留，动态平衡探索与利用。

Result: 在真实数据流实验中，FCDC使数据集平衡度提升25.9%，同时减少39.8%的数据存储需求。

Conclusion: 数据收集可以主动控制，将其从被动管道阶段转变为数据驱动AI核心的自调节反馈驱动过程。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [85] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: IGNO是一种新型生成神经算子框架，用于解决偏微分方程逆问题，无需标记训练数据，通过物理约束和潜在空间优化实现高维不连续系数场的准确重建。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法需要大量标记数据或局限于特定测量类型，在稀疏、噪声或高维不连续系数情况下表现不佳，限制了实际应用。

Method: IGNO将高维系数场编码到低维潜在空间，通过神经算子解码器重建系数和PDE解，仅依赖物理约束进行训练，使用基于梯度的潜在空间优化和先验归一化流模型加速。

Result: 在多种挑战性逆问题中，包括从解测量恢复不连续系数和EIT问题，IGNO均实现准确、稳定和可扩展的反演，在严重噪声下表现优异，优于现有方法并具有良好的泛化能力。

Conclusion: IGNO为计算科学领域中的挑战性逆问题提供了一个统一且强大的框架。

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [86] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习在气候变化背景下制定洪水适应政策，考虑不确定性和不同优先目标（经济vs生活质量）的影响


<details>
  <summary>Details</summary>
Motivation: 气候变化导致洪水频率和严重性增加，需要有效的适应政策制定，但长期气候影响的不确定性和政策中的规范性选择往往不明确

Method: 结合强化学习和综合评估模型，将降雨洪水模型与洪水影响（生活质量、交通、基础设施损害）联系起来，比较不同优先目标的适应路径

Result: 优先考虑生活质量而非经济影响的模型导致更高的适应支出，并在研究区域内更均匀地分配支出

Conclusion: 强化学习是识别不确定条件下适应路径和明确建模不同适应优先级的有效工具，规范性假设显著影响适应政策

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [87] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: GMoPE是一个将专家混合架构与基于提示的图学习相结合的新框架，通过专家特定提示向量和结构感知路由实现跨领域泛化，显著降低适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在跨领域和任务泛化方面存在限制，面临负迁移、可扩展性问题和适应成本高的挑战。

Method: 提出GMoPE框架，集成专家混合架构与提示学习，使用专家特定提示向量和结构感知路由，引入软正交约束促进专家多样性，采用仅提示微调策略。

Result: 在各种预训练策略和下游任务中，GMoPE持续优于最先进基线，性能接近全参数微调，同时仅需少量适应开销。

Conclusion: GMoPE为推进可泛化和高效的图基础模型提供了一个原则性和可扩展的框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [88] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 论文分析了经典熵最小化(EM)的内在机制，将其解耦为两个作用相反的部分，并提出了自适应解耦熵最小化(AdaDEM)来解决经典EM的局限性，在噪声和动态环境中的不完美监督学习任务中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 经典熵最小化(EM)在减少类别重叠、缩小领域差距和限制不确定性方面有益，但其潜力有限。需要研究EM的内在机制并解决其局限性。

Method: 将经典EM解耦为两个部分：簇聚合驱动因子(CADF)和梯度缓解校准器(GMC)。提出AdaDEM方法，通过归一化CADF带来的奖励，并使用边缘熵校准器(MEC)替代GMC。

Result: AdaDEM在噪声和动态环境中的各种不完美监督学习任务中表现优于DEM*(经典EM的上界变体)，取得了优越性能。

Conclusion: 通过解耦经典EM并设计自适应方法，可以有效解决奖励崩溃和易类偏差问题，提升在不完美监督学习中的性能。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [89] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在数据受限条件下，扩散语言模型(DLMs)通过更多训练轮次超越自回归模型(AR)，这种优势源于多阶建模、密集计算和数据增强效应。


<details>
  <summary>Details</summary>
Motivation: 探索在有限数据条件下，扩散语言模型相比传统自回归模型的性能优势及其成因。

Method: 在严格控制预训练设置下，比较DLMs和AR模型在不同数据量、模型大小和训练轮次下的表现，分析DLMs的三个优势因素。

Result: 1.7B DLM在10B独特Python token上训练，性能超越匹配设置的AR编码器；1B DLM仅用1B token在HellaSwag和MMLU上分别达到>56%和>33%准确率。

Conclusion: 扩散语言模型在数据受限场景下具有显著优势，验证交叉熵上升不一定意味着下游性能下降。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [90] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的自适应API限流策略，在Kubernetes环境中相比传统固定阈值方法提升23.7%吞吐量并降低31.4% P99延迟


<details>
  <summary>Details</summary>
Motivation: 传统限流算法难以适应动态流量模式和变化的系统负载，需要更智能的限流机制来确保系统稳定性和服务质量

Method: 设计结合DQN和A3C算法的混合架构，将限流决策过程建模为马尔可夫决策过程，通过环境交互学习最优限流策略

Result: 90天生产部署处理5亿日请求，服务降级事件减少82%，人工干预减少68%

Conclusion: 深度强化学习方法能有效实现动态限流，在保证服务质量的同时显著提升系统性能

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [91] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 提出了一种新的多参考对齐算法，通过概率建模和相对位姿边缘化来消除全局对称性，实现更直接解法和更好收敛性。


<details>
  <summary>Details</summary>
Motivation: 从分子成像到无线通信，多参考对齐问题在许多实际应用中至关重要，如冷冻电镜、计算机视觉和无线通信系统。

Method: 使用概率方法建模MRA问题，利用相对位姿作为冗余变量进行边缘化，通过循环一致性避免集中式方法的立方复杂度。

Result: 两种提出的算法在各种实验设置下都实现了更低的重建误差。

Conclusion: 该去中心化方法通过消除全局对称性，实现了计算效率提升和重建性能改进。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [92] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络与时序建模的统一框架，用于微服务架构中的异常检测和根因追踪，通过服务拓扑表示和时序演化建模来提升异常模式识别能力。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中异常检测和根因追踪的问题，传统方法难以处理复杂的服务依赖关系和动态拓扑变化。

Method: 将微服务调用链抽象为有向图，使用图卷积聚合节点特征并建模依赖关系，引入门控循环单元建模时序演化，通过多层堆叠和拼接操作联合获取结构和时序表示，定义节点级和路径级异常评分函数。

Result: 在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。

Conclusion: 为微服务异常检测提供了新的技术路径，为分布式系统智能运维奠定了方法论基础。

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [93] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种用于核方法的连续公平性方法，通过零空间投影技术处理连续保护属性，在支持向量回归中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在社会生活中的广泛应用，公平性成为重要议题。现有研究主要关注离散属性，而连续属性（特别是回归问题中的连续公平性）研究稀缺。

Method: 将迭代零空间投影方法推广到核方法，提出模型和公平性评分无关的核嵌入方法，适用于连续保护属性。

Result: 新方法与支持向量回归结合，在多个数据集上相比其他当代方法表现出竞争性或改进的性能。

Conclusion: 该方法成功扩展了连续公平性的应用范围，为处理连续保护属性提供了有效的核方法解决方案。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [94] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: SORTD是一个新颖的框架，用于高效枚举Rashomon集合中的决策树（性能相似但结构不同的树），相比现有方法将运行时间减少了两个数量级，支持可分离和全序目标函数。


<details>
  <summary>Details</summary>
Motivation: 稀疏决策树学习提供了准确且可解释的预测模型，但单一"最佳"树存在局限性。Rashomon集合（性能相似但结构不同的树集合）可以增强变量重要性分析、丰富解释性，并允许用户选择更简单的树或满足利益相关者偏好的树，而无需将这些标准硬编码到目标函数中。

Method: 提出了SORTD框架，按目标值顺序枚举Rashomon集合中的树，提供随时可用的行为。支持任何可分离和全序的目标函数，并支持使用其他可分离（和偏序）目标函数对集合进行后评估。

Result: 实验表明，SORTD相比现有技术将运行时间减少了多达两个数量级。该框架能够计算任何可分离和全序目标的Rashomon集合。

Conclusion: 这些进展使得在实际应用中探索Rashomon集合变得更加实用，为高风险应用提供了更灵活和可解释的决策树模型选择。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [95] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模块化多标签意图识别管道DMTC，通过提示工程生成合成查询、Sentence-T5编码和在线焦点对比损失训练，在交通领域应用中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注数据且难以进行细粒度多标签区分，需要消除昂贵数据收集需求同时提高多标签意图理解的准确性。

Method: DMTC管道包含三个步骤：1) 使用提示工程引导LLM生成多样化合成查询；2) 用Sentence-T5模型编码文本查询获得语义嵌入；3) 使用新颖的在线焦点对比损失训练轻量级分类器。

Result: 在海上交通应用中，DMTC实现了5.35%的汉明损失和95.92%的AUC，优于最先进的多标签分类器和LLM基线。Sentence-T5嵌入比替代编码器提高子集准确率至少3.29%，OFC损失相比标准对比目标带来额外0.98%增益。

Conclusion: 该系统能无缝将用户查询路由到特定任务模块，为无需昂贵人工标注的完全自主、意图感知的智能体奠定了基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [96] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出统一的数据-模型耦合市场机制，将数据集和模型交易作为单一系统处理，通过供需双向映射和Shapley分配实现三方共赢定价。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经济兴起，但现有定价方法仍将数据和模型交易分离或依赖偏向一方的代理中心管道，缺乏同时覆盖数据卖家、模型生产者和模型买家的对称机制。

Method: 构建统一数据-模型耦合市场：供应端映射将数据集支付转换为买家可见的模型报价，需求端映射通过Shapley分配将买家价格反向传播到数据集，形成连接四个交互的闭环系统。

Result: 证明联合算子为标准干扰函数(SIF)，保证均衡价格的存在性、唯一性和全局收敛性。实验显示高效收敛和相比代理中心及单边基线改进的公平性。

Conclusion: 所提机制实现了数据卖家、模型生产者和模型买家的三方共赢，为机器学习模型经济提供了统一的定价解决方案。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [97] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 本文提出了Adaptable HER框架，将Hindsight Experience Replay与AlphaZero结合，通过重新标注搜索树中不成功的轨迹作为监督学习信号，解决稀疏奖励环境下神经网络早期训练困难的问题。


<details>
  <summary>Details</summary>
Motivation: AlphaZero在稀疏奖励环境中，特别是在早期训练阶段，神经网络无法提供有效指导，导致性能受限。需要一种方法来改善这种情况。

Method: 提出Adaptable HER框架，灵活整合HER与AlphaZero，允许调整HER的重标注目标、策略目标和轨迹选择等属性。

Result: 实验（包括方程发现任务）表明，修改HER的可能性是有益的，并且超越了纯监督学习或强化学习的性能。

Conclusion: Adaptable HER框架通过结合HER和AlphaZero，有效解决了稀疏奖励环境下的训练问题，提高了整体性能。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [98] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: POEMS是一个可解释的多组学集成框架，通过稀疏解码保持预测性能的同时提供可解释性，解决了深度生成模型中预测性能与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 多组学数据集成对理解疾病复杂性至关重要，但现有深度生成模型要么牺牲可解释性追求预测性能，要么通过线性化解码器强制可解释性而削弱非线性表达能力。

Method: 使用稀疏连接将特征映射到潜在因子，通过专家乘积模型在共享潜在空间中实现跨组学关联，采用门控网络自适应计算各组学在表示学习中的贡献，并提出高效稀疏解码器。

Result: 在癌症亚型分型案例中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了新颖的可解释性见解。

Conclusion: 基于生物标志物的洞察和预测准确性可以在多组学表示学习中并存，POEMS框架成功解决了可解释性与性能之间的权衡问题。

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [99] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 提出了一种利用已知群对称性改进核强化学习的理论算法框架，通过不变核编码奖励和转移动态的不变性，显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实强化学习环境中存在固有对称性，可以利用这些对称性提高学习效率。

Method: 提出了对称感知的乐观最小二乘值迭代(LSVI)变体，使用不变核来编码奖励和转移动态的不变性。

Result: 在定制Frozen Lake环境和2D布局设计问题上验证了理论改进，对称感知RL比标准核方法性能显著更好。

Conclusion: 结构先验在设计更样本高效的强化学习算法中具有重要价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [100] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost是一个高效的检索增强生成系统，通过准确性保持的上下文重用实现高缓存复用，在不牺牲准确性的情况下提升预填充性能1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在预填充阶段性能下降，现有缓存技术要么保持准确性但缓存复用率低，要么提高复用率但牺牲推理质量。

Method: RAGBoost检测并发会话和多轮交互中的重叠检索项，使用高效的上下文索引、排序和去重来最大化复用，同时通过轻量级上下文提示保持推理保真度。

Result: 与现有最先进方法相比，预填充性能提升1.5-3倍，同时在多样化的RAG和智能AI工作负载中保持甚至提高了推理准确性。

Conclusion: RAGBoost实现了高缓存复用而不牺牲准确性，可无缝集成到现有LLM推理引擎中，显著提升RAG系统性能。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [101] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: NAP是一个基于注意力的模型，通过三轴注意力机制聚合多通道预测，解决了多导睡眠图信号异质性问题，在零样本泛化方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图信号具有高度异质性，现有模型依赖固定子集的模态或通道，无法充分利用其多模态特性。

Method: 引入NAP模型，使用三轴注意力机制（时间、空间、预测器级）来组合多个预测流，通过聚合预训练单通道模型的输出来适应不同输入维度。

Result: NAP持续优于单个预测器和简单集成方法，在多个数据集上实现了最先进的零样本泛化性能。

Conclusion: 该方法虽然针对睡眠分期演示，但可扩展到其他多模态生理应用。

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [102] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架来解决机器学习中的核心悖论：何时使用更少数据反而更好。研究表明，在某些条件下，精心筛选的小数据集可以超越完整数据集，并提供了数据大小和质量相关的相变曲线。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的核心矛盾：经典缩放定律认为"越多越好"，但最近的研究如LIMO和s1显示，使用经过精心筛选的小数据集反而能获得更好的性能。需要理论框架来解释这种看似矛盾的现象。

Method: 研究数据筛选策略，其中不完美的oracle根据样本难度和正确性选择训练样本。推导了在标签无关和标签感知筛选规则下测试误差的精确缩放定律曲线。

Result: 理论分析显示，在某些条件下，小型筛选数据集可以超越完整数据集。在ImageNet上的实证结果验证了理论预测，确认了筛选何时能提高准确性甚至缓解模型崩溃。

Conclusion: 该框架为最近在LLM数学推理中观察到的矛盾筛选策略提供了原则性解释，揭示了数据筛选改善泛化的具体条件和机制。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [103] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO作为PPO的可扩展替代方案，通过轨迹组间比较估计优势值而无需学习critic。研究发现：在长视野任务中学习critic仍必不可少；GRPO受益于高折扣因子；小分组规模表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO这种无需学习critic的策略梯度方法的有效性，探讨学习baseline在策略梯度方法中的必要性。

Method: 在经典单任务RL环境中对GRPO进行系统研究，通过控制实验分离baseline、折扣因子和分组采样的影响。

Result: 1) 除短视野环境外，所有无critic baseline均不如PPO；2) GRPO受益于高折扣因子；3) 小分组规模表现优于大分组。

Conclusion: 揭示了无critic方法在经典控制任务中的局限性，以及它们在特定条件下仍可作为学习价值函数可行替代方案的具体条件。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [104] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出了一种新的拜占庭鲁棒联邦学习优化方法，通过将聚合权重作为可学习参数与全局模型参数联合优化，提高了在数据异构和恶意客户端攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中恶意客户端的存在对系统鲁棒性构成严重威胁，特别是在数据分布异构的情况下，传统方法难以有效应对。

Method: 将聚合权重作为可学习参数，与全局模型参数联合优化，开发了具有强收敛保证的交替最小化算法。

Result: 在各种数据集和攻击场景下，该方法始终优于现有最先进的拜占庭鲁棒联邦学习方法，特别是在数据高度异构和恶意客户端比例较高的情况下表现更佳。

Conclusion: 所提出的自适应加权聚合方法能有效提升联邦学习在拜占庭攻击下的鲁棒性，为异构数据环境下的安全联邦学习提供了有效解决方案。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [105] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: 本文扩展了表达性神经网络(ENN)的研究，重点强调其效率、可解释性和剪枝能力。ENN使用DCT参数化自适应激活函数，通过结构化表示实现神经元功能角色识别和冗余组件检测，提出高效剪枝策略，在分类和隐式神经表示任务中达到SOTA精度，可安全剪除40%激活系数。


<details>
  <summary>Details</summary>
Motivation: 在之前证明ENN具有强大表达能力的基础上，进一步探索其效率、可解释性和剪枝能力，将信号处理概念系统整合到神经网络设计中。

Method: 使用离散余弦变换(DCT)参数化多层感知机的自适应激活函数，利用DCT的结构化和去相关特性揭示神经元功能角色，提出基于DCT系数的高效剪枝策略。

Result: 在分类和隐式神经表示任务中达到最先进精度，同时保持较少的参数数量；得益于DCT基的正交性和有界性，可安全剪除高达40%的激活系数。

Conclusion: ENN框架实现了信号处理概念与神经网络设计的有机结合，在表达能力、紧凑性和可解释性之间达到了平衡的权衡。

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [106] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 本文研究了在随机凸优化设置下平坦最小值与泛化性能之间的关系，发现平坦最小值可能产生较差的泛化性能，而尖锐最小值反而能实现最优泛化。


<details>
  <summary>Details</summary>
Motivation: 理解学习算法的泛化行为是学习理论的核心目标，现有解释认为算法成功是因为收敛到平坦最小值，但这一关联在基础设置中尚未得到严格验证。

Method: 在非负β-平滑目标的随机凸优化框架下，分析平坦最小值与泛化的关系，并研究两种锐度感知算法（SA-GD和SAM）的性能。

Result: 平坦经验最小值可能产生Ω(1)的总体风险，而尖锐最小值能实现最优泛化；SA-GD能快速收敛到平坦最小值但泛化性能差；SAM可能收敛到尖锐最小值且泛化性能同样差。

Conclusion: 平坦最小值与良好泛化性能之间的关联在基础凸优化设置中并不成立，锐度感知算法可能无法保证泛化性能的提升。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [107] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文对模仿学习的最新进展进行了综述，提出了新的分类法来反映当前研究现状和趋势，并分析了代表性工作的优缺点、评估实践以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，模仿学习在多个领域的能力和可扩展性显著提升，需要系统梳理最新进展、方法创新和实际应用，以更好地反映当前研究格局。

Method: 采用文献综述方法，提出新的分类法来组织模仿学习研究，批判性分析代表性工作的优势、局限性和评估实践。

Result: 建立了反映当前模仿学习研究层次和趋势的新分类体系，系统总结了方法创新、实际应用和评估实践。

Conclusion: 模仿学习领域取得了显著进展，但仍面临泛化、协变量偏移和演示质量等挑战，需要进一步研究解决这些关键问题。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [108] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma是一个用于表格预测的LLM模型，通过科学记数法处理数值、目标插补预训练和n-gram检索选择示例，在语义丰富的分类任务中达到SOTA，但在回归任务中数据量大时表现不如传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决将预训练LLMs应用于表格预测时的两个实际问题：不稳定的数值标记化和有限的上下文大小。

Method: 1. 使用带符号科学记数法规范化数值；2. 在12B Gemma 3模型上继续预训练，使用目标插补目标；3. 推理时使用紧凑的n-gram检索选择信息丰富的示例，适应128k令牌窗口。

Result: 在语义丰富的基准测试中，TabGemma在分类任务（低数据和高数据场景）上达到新的SOTA，性能随上下文行数单调提升；在回归任务中，小样本时具有竞争力，但数据增长时落后于传统方法。

Conclusion: LLMs在配备专用数值处理和上下文检索时，可以成为有效的表格上下文学习器，但在数值建模和长上下文扩展方面仍需进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [109] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: CPL框架通过将神经网络输出投影到物理约束集上，确保每次更新都符合物理定律，解决了神经网络求解偏微分方程时违反守恒律、熵增等物理原则的问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络在求解偏微分方程时经常违反物理定律，如产生虚假质量、漂移激波、破坏守恒性和熵增原理，需要一种能确保物理一致性的训练方法。

Method: 使用约束投影学习(CPL)框架，将网络输出投影到由守恒律、Rankine-Hugoniot平衡、熵增和正定性定义的约束集交集上；结合总变差阻尼(TVD)抑制小振荡，以及滚动课程训练确保长期预测一致性。

Result: CPL在Burgers和Euler系统上产生了稳定且物理上合法的解，守恒律达到机器精度，总变差增长消失，熵和误差保持有界，计算开销仅增加约10%。

Conclusion: CPL使物理一致性成为学习过程的内在属性，而不是依赖神经网络自发遵守物理定律。

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [110] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: 提出了Tensor-Efficient Q-Learning (TEQL)，通过改进的低秩张量分解和探索机制，在高维强化学习中提高样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高维强化学习面临计算复杂和样本效率低的问题，特别是在大规模状态-动作空间中。现有Q学习算法受维度诅咒影响，而基于张量的方法提供了更参数高效的替代方案。

Method: TEQL通过改进的块坐标下降在离散化状态-动作空间上进行低秩张量分解，结合了基于近似误差和访问次数的探索策略，以及频率惩罚项来减少过拟合。

Result: 在经典控制任务上的实验结果表明，TEQL在样本效率和总奖励方面优于传统的基于矩阵的方法和深度强化学习方法。

Conclusion: TEQL特别适用于采样成本高的资源受限应用，如空间和医疗领域，能够有效应对高维强化学习挑战。

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [111] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 提出了一种深度隐式模仿强化学习框架，结合深度强化学习和仅观测数据集的隐式模仿学习，解决了传统模仿学习需要完整状态-动作演示和最优专家的限制。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要完整的状态-动作演示和最优专家，这在实际应用中严重受限，因为许多现实场景只提供状态观测而没有对应动作，且专家表现往往不是最优的。

Method: 主要算法DIIQN使用动作推断机制通过在线探索重建专家动作，并集成动态置信机制自适应平衡专家引导和自主学习。HA-DIIQN算法处理专家和智能体具有不同动作集的情况，引入不可行性检测机制和桥接过程。

Result: DIIQN相比标准DQN获得高达130%的更高回合回报，始终优于现有无法超越专家表现的隐式模仿方法。在异构动作设置中，HA-DIIQN学习速度比基线快64%，能够利用传统方法无法使用的专家数据集。

Conclusion: 该框架通过结合深度强化学习和隐式模仿学习，有效解决了传统模仿学习的局限性，在仅观测数据和异构动作场景下表现出色，具有很好的鲁棒性。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [112] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是最早且最具影响力的强化学习算法，研究其收敛性质不仅是RL领域早期发展的主要研究课题，如今也受到越来越多的关注

Method: 基于Robbins-Siegmund定理的统一框架，使用Lean 4定理证明器进行形式化验证

Result: 成功形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性

Conclusion: 这项工作为完全形式化收敛RL结果迈出了重要一步，所开发的框架可以轻松扩展到收敛速率和其他收敛模式

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [113] [Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction](https://arxiv.org/abs/2511.03631)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 开发了一个针对中小企业的集成财务预测系统，结合应收账款预测和现金流预测，专门解决资源有限、客户基础小和数据受限的运营约束问题。


<details>
  <summary>Details</summary>
Motivation: 中小企业特别是自由职业者和早期企业面临独特的财务管理挑战，现有企业级财务工具与他们的实际需求存在差距。

Method: 系统集成了两个关键组件：预测发票付款延迟的二元分类模型，以及处理不完整和有限历史数据的多模块现金流预测模型。

Result: 原型系统已作为Web应用程序实现，并集成到Cluee平台（为自由职业者提供财务管理工具的初创公司），证明了实际可行性。

Conclusion: 该系统为中小企业财务管理提供了实用的解决方案，填补了企业级工具与中小企业实际需求之间的空白。

Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage
businesses, face unique financial management challenges due to limited
resources, small customer bases, and constrained data availability. This paper
presents the development and deployment of an integrated financial prediction
system that combines accounts receivable prediction and cash flow forecasting
specifically designed for SME operational constraints. Our system addresses the
gap between enterprise-focused financial tools and the practical needs of
freelancers and small businesses. The solution integrates two key components: a
binary classification model for predicting invoice payment delays, and a
multi-module cash flow forecasting model that handles incomplete and limited
historical data. A prototype system has been implemented and deployed as a web
application with integration into Cluee's platform, a startup providing
financial management tools for freelancers, demonstrating practical feasibility
for real-world SME financial management.

</details>


### [114] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN是一个简化的表格基础模型实现，相比TabPFN v2减少了代码复杂度，可在单GPU上1分钟内完成预训练，性能与传统机器学习基线相当。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型实现复杂（超过10,000行代码），缺乏架构文档和代码质量，难以理解和适应新实验，阻碍了学生和研究人员的可访问性。

Method: 简化TabPFN v2架构和训练循环，使用预生成的训练数据，创建轻量级实现nanoTabPFN。

Result: 在小数据设置下，nanoTabPFN在单GPU上1分钟预训练即可达到与传统机器学习基线相当的性能，比TabPFN v2预训练快160,000倍。

Conclusion: nanoTabPFN消除了对大型计算资源的需求，使表格基础模型的预训练在教育用途上变得可访问。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [115] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 提出基于机器学习的框架，用于检测医疗物联网中的恶意网络攻击和设备故障异常，评估了8种模型在三种学习范式下的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备面临严重的安全性和可靠性挑战，易受网络威胁和操作异常影响，需要有效的检测方案来保障患者安全和系统稳定。

Method: 使用包含20万条记录的数据集，评估了监督学习（XGBoost、KNN）、半监督学习（GAN、VAE）和无监督学习（One-Class SVM、Isolation Forest、GNN、LSTM Autoencoder）三类共8种机器学习模型。

Result: XGBoost在异常检测中达到99%准确率且计算开销最小（0.04秒），Isolation Forest在精度和召回率间取得良好平衡。KNN在攻击检测中实现近乎完美的性能且计算成本最低（0.05秒），VAE达到97%准确率。GAN表现最差，计算成本最高且准确率最低。

Conclusion: 该框架通过有效的异常检测策略增强了医疗物联网安全性，能够早期检测网络威胁和设备故障，预防数据泄露、减少系统停机时间，确保医疗设备持续安全运行，保护患者健康和信任。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [116] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 对有限环境中深度Q网络的研究，重点分析ε-贪婪探索策略和优先级经验回放的影响，提供强化学习实践建议。


<details>
  <summary>Details</summary>
Motivation: 研究深度Q网络中探索策略与记忆管理之间的权衡和相互作用，为资源受限环境下的强化学习提供实用指导。

Method: 通过系统实验评估ε衰减策略对学习效率的影响，比较均匀、无回放和优先级经验回放策略在多个模拟中的表现。

Result: 优先级经验回放能带来更快的收敛速度和更高的回报，不同ε衰减策略对学习效率和奖励优化有显著影响。

Conclusion: 揭示了探索策略与记忆管理在DQN训练中的关键相互作用，为资源受限环境下的稳健强化学习提供了实用建议。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [117] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出了基于逻辑回归的参数化后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来管理偏差-方差权衡，在多分类校准中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 参数化校准函数基于逻辑回归，可以从简单的理论设置中推导出来，这促使使用比标准温度缩放更具表达力的校准方法。

Method: 使用结构化正则化、鲁棒预处理和高效优化来管理多分类校准中的偏差-方差权衡，避免过拟合。

Result: 所提出的方法在现有基于逻辑回归的校准技术上取得了显著提升，成为温度缩放、向量缩放和矩阵缩放的有吸引力的替代方案。

Conclusion: 通过结构化正则化和优化技术，可以有效地管理多分类校准中的参数复杂度问题，提供高效且易于使用的开源实现。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [118] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ是一种离线到在线强化学习框架，通过行为一致性信号和自适应机制，实现从离线训练到在线微调的无缝过渡，提高策略在动态环境中的适应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习策略在动态环境中部署时，由于分布偏移和未见状态-动作对上的不可靠价值估计，往往表现不佳。需要一种方法来实现从离线到在线学习的平滑过渡。

Method: BAQ利用离线数据中的隐式行为模型，在在线微调期间提供行为一致性信号。采用双目标损失函数：(i)在不确定性高时使在线策略与离线行为对齐；(ii)随着在线经验积累逐渐放松约束。

Result: 在标准基准测试中，BAQ持续优于先前的离线到在线RL方法，实现了更快的恢复速度、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是一种可靠且实用的解决方案，能够实现真实世界策略部署的平稳过渡。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [119] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM的模拟电路尺寸优化框架AnaFlow，通过智能体协作实现高效、可解释的自动化电路设计，解决了传统方法仿真成本高和缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计仍是手工过程，设计周期长且易出错。现有AI方法需要大量耗时仿真且缺乏可解释性，阻碍了工具的广泛采用。

Method: 采用多智能体工作流，专门化的LLM智能体协作解释电路拓扑、理解设计目标，并通过人类可理解的推理迭代优化电路设计参数，结合自适应仿真策略提高样本效率。

Result: AnaFlow在两个复杂度不同的电路上成功完成全自动尺寸优化，相比纯贝叶斯优化和强化学习方法表现更优，能够从优化历史中学习避免错误并加速收敛。

Conclusion: 该框架为模拟设计空间探索提供了强大工具，开创了模拟EDA新范式，其中AI智能体作为透明的设计助手，具有内在的可解释性优势。

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [120] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 提出一种基于收缩估计的基线方法，用于改进强化学习中的策略梯度估计，通过结合每个提示和跨提示的均值来降低方差，提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习验证奖励(RLVR)中，传统使用每个提示的经验均值作为基线来中心化奖励，但在低生成次数情况下估计不准确，导致策略梯度估计方差较大。

Method: 受Stein悖论启发，提出收缩估计器，将每个提示的均值与跨提示的均值相结合，构建收缩基线作为现有方法的即插即用替代方案。

Result: 理论证明收缩基线能产生更低方差的策略梯度估计器，实证结果表明其持续优于标准经验均值基线，降低梯度更新方差并提高训练稳定性。

Conclusion: 收缩基线方法无需额外超参数或计算，能有效改进RLVR中的策略梯度估计，是现有基线方法的有效替代方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='stat.TH'></div>

# stat.TH [[Back]](#toc)

### [121] [Statistical Properties of Rectified Flow](https://arxiv.org/abs/2511.03193)
*Gonzalo Mena,Arun Kumar Kuchibhotla,Larry Wasserman*

Main category: stat.TH

TL;DR: 本文研究了整流流的结构和统计性质，包括存在性、唯一性、正则性，以及收敛速率和中心极限定理，发现在有界和无界情况下都能获得比传统非参数回归和密度估计更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 整流流作为一种定义两个分布间传输映射的方法在机器学习中很受欢迎，但支持这些方法有效性的理论结果很少。本文旨在填补这一理论空白。

Method: 分别分析有界和无界情况，研究整流流的结构性质（存在性、唯一性、正则性）和统计性质（收敛速率、中心极限定理），使用回归和密度估计等标准统计工具。

Result: 在有界和无界两种情况下，都能建立比传统非参数回归和密度估计更快的收敛速率。

Conclusion: 整流流不仅计算简单（只需标准统计工具），而且在理论上具有优越的收敛性能，为该方法在机器学习中的应用提供了坚实的理论基础。

Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for
defining a transport map between two distributions, and enjoys popularity in
machine learning, although theoretical results supporting the validity of these
methods are scant. The rectified flow can be regarded as an approximation to
optimal transport, but in contrast to other transport methods that require
optimization over a function space, computing the rectified flow only requires
standard statistical tools such as regression or density estimation. Because of
this, one can leverage standard data analysis tools for regression and density
estimation to develop empirical versions of transport maps. We study some
structural properties of the rectified flow, including existence, uniqueness,
and regularity, as well as the related statistical properties, such as rates of
convergence and central limit theorems, for some selected estimators. To do so,
we analyze separately the bounded and unbounded cases as each presents unique
challenges. In both cases, we are able to establish convergence at faster rates
than the ones for the usual nonparametric regression and density estimation.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [122] [Supersimulators](https://arxiv.org/abs/2509.17994)
*Cynthia Dwork,Pranay Tankala*

Main category: cs.CC

TL;DR: 本文证明所有随机布尔函数都存在超模拟器——一种随机多项式大小电路，其输出在随机输入下即使面对多项式更大的区分器也无法以恒定优势与真实情况区分。该结果改进了Trevisan等人的复杂性理论正则引理，通过让区分器大小界限随目标函数变化来规避模拟器大小的下界。


<details>
  <summary>Details</summary>
Motivation: 传统正则引理提供的模拟器只能欺骗较小的区分器，存在局限性。本文旨在构建能够欺骗更大区分器的模拟器，从而在复杂性理论、密码学和学习理论等应用中提供更强的工具。

Method: 基于Trevisan等人的复杂性理论正则引理，采用源自图正则文献的迭代技术，让区分器大小界限随目标函数变化，同时保持在独立于目标函数的绝对上界之下。

Result: 成功构建了超模拟器，能够欺骗多项式更大的区分器，并在多准确性和多校准预测器的应用中闭合了先前版本中存在的复杂性差距。

Conclusion: 超模拟器为计算不可区分性提供了更强的工具，改进了多校准基特征化的结果，在复杂性理论和其他相关领域具有重要应用价值。

Abstract: We prove that every randomized Boolean function admits a supersimulator: a
randomized polynomial-size circuit whose output on random inputs cannot be
efficiently distinguished from reality with constant advantage, even by
polynomially larger distinguishers. Our result builds on the landmark
complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009),
which, in contrast, provides a simulator that fools smaller distinguishers. We
circumvent lower bounds for the simulator size by letting the distinguisher
size bound vary with the target function, while remaining below an absolute
upper bound independent of the target function. This dependence on the target
function arises naturally from our use of an iteration technique originating in
the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements
thereof, known as multiaccurate and multicalibrated predictors, respectively,
as per Hebert-Johnson et al. (2018), have previously been shown to have myriad
applications in complexity theory, cryptography, learning theory, and beyond.
We first show that a recent multicalibration-based characterization of the
computational indistinguishability of product distributions actually requires
only (calibrated) multiaccuracy. We then show that supersimulators yield an
even tighter result in this application domain, closing a complexity gap
present in prior versions of the characterization.

</details>


### [123] [Efficient Testing Implies Structured Symmetry](https://arxiv.org/abs/2511.03653)
*Cynthia Dwork,Pranay Tankala*

Main category: cs.CC

TL;DR: 本文研究了从少量样本中高效测试布尔函数性质的问题，建立了高效可测试性与结构化对称性之间的等价关系，并扩展了该特征化到其他维度。


<details>
  <summary>Details</summary>
Motivation: 研究如何从少量随机样本中高效测试未知布尔函数的性质，探索计算效率与测试能力之间的关系。

Method: 使用超模拟技术，基于算法公平性文献中的方法，用小型电路模拟器逼近任意复杂函数，欺骗更大的区分器。

Result: 证明了高效可测试性质与具有结构化对称性的性质等价，这些性质仅依赖于函数在低复杂度划分各部分上的平均值。

Conclusion: 布尔函数的高效测试本质上检查与有界的小电路集合的不可区分性，该结果可推广到任意域上的高熵分布测试。

Abstract: Given a small random sample of $n$-bit strings labeled by an unknown Boolean
function, which properties of this function can be tested computationally
efficiently? We show an equivalence between properties that are efficiently
testable from few samples and properties with structured symmetry, which depend
only on the function's average values on parts of a low-complexity partition of
the domain. Without the efficiency constraint, a similar characterization in
terms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our
main technical tool is supersimulation, which builds on methods from the
algorithmic fairness literature to approximate arbitrarily complex functions by
small-circuit simulators that fool significantly larger distinguishers.
  We extend the characterization along other axes as well. We show that
allowing parts to overlap exponentially reduces their required number,
broadening the scope of the construction from properties testable with $O(\log
n)$ samples to properties testable with $O(n)$ samples. For larger sample
sizes, we show that any efficient tester is essentially checking for
indistinguishability from a bounded collection of small circuits, in the spirit
of a characterization of testable graph properties. Finally, we show that our
results for Boolean function testing generalize to high-entropy distribution
testing on arbitrary domains.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [124] [Topography, climate, land cover, and biodiversity: Explaining endemic richness and management implications on a Mediterranean island](https://arxiv.org/abs/2511.03242)
*Aristides Moustakas,Ioannis N Vogiatzakis*

Main category: q-bio.PE

TL;DR: 本研究使用人工神经网络模型分析克里特岛特有植物丰富度的驱动因素，发现物种总丰富度、海拔范围和气候变异性是最强预测因子，特有热点与高物种丰富度区域部分重叠。


<details>
  <summary>Details</summary>
Motivation: 岛屿特有性由环境、生态和进化因素共同塑造，但地形、气候和土地覆盖的相对贡献尚未完全量化，需要研究克里特岛这一地中海生物多样性热点区的特有植物丰富度驱动机制。

Method: 使用空间显式数据（物种分布、地形复杂性、气候变异性、土地覆盖和土壤特征），采用人工神经网络模型评估预测因子的相对重要性并识别特有热点区域。

Result: 物种总丰富度、海拔范围和气候变异性是特有丰富度的最强预测因子，特有热点与高物种丰富度区域仅部分重叠，表明物种总丰富度是最佳但不完美的替代指标。

Conclusion: 应优先保护山区和气候多变区域，整合生态系统服务考量，考虑岛内空间异质性，为克里特岛和其他地中海岛屿提供基于证据的保护规划框架。

Abstract: Island endemism is shaped by complex interactions among environmental,
ecological, and evolutionary factors, yet the relative contributions of
topography, climate, and land cover remain incompletely quantified. We
investigated the drivers of endemic plant richness across Crete, a
Mediterranean biodiversity hotspot, using spatially explicit data on species
distributions, topographic complexity, climatic variability, land cover, and
soil characteristics. Artificial Neural Network models, a machine learning
tool, were employed to assess the relative importance of these predictors and
to identify hotspots of endemism. We found that total species richness,
elevation range, and climatic variability were the strongest predictors of
endemic richness, reflecting the role of biodiversity, topographic
heterogeneity, and climatic gradients in generating diverse habitats and
micro-refugia that promote speciation and buffer extinction risk. Endemic
hotspots only partially overlapped with areas of high total species richness,
indicating that total species richness was the optimal from the ones examined,
yet an imperfect surrogate. These environmentally heterogeneous areas also
provide critical ecosystem services, including soil stabilization, pollination,
and cultural value, which are increasingly threatened by tourism, renewable
energy development, land-use change, and climate impacts. Our findings
underscore the importance of prioritizing mountainous and climatically variable
regions in conservation planning, integrating ecosystem service considerations,
and accounting for within-island spatial heterogeneity. By explicitly linking
the environmental drivers of endemism to both biodiversity patterns and
ecosystem function, this study provides a framework for evidence-based
conservation planning in Crete and other Mediterranean islands with similar
geological and biogeographic contexts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [125] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 提出了农业运营设计域框架，用于描述和验证自主农业系统的运营边界，解决现有ODD概念无法应对农业应用独特挑战的问题。


<details>
  <summary>Details</summary>
Motivation: 农业自动化系统在复杂多变环境中运行，需要处理驾驶和工作过程的双重约束，现有运营设计域概念无法满足农业应用的独特需求。

Method: 开发了农业运营设计域框架，包含三个核心元素：结构化描述概念、扩展的7层模型（增加过程层）、迭代验证流程。

Result: 该框架提供了创建明确且可验证农业运营设计域的一致方法，演示用例表明能支持自主农业系统环境描述的标准化和可扩展性。

Conclusion: 农业运营设计域框架为自主农业系统提供了结构化的环境描述和验证方法，有助于解决农业自动化中的复杂性和一致性挑战。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [126] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本文提出了一个用于分析ROS和ROS 2数据包的MCP服务器，通过自然语言处理机器人数据，并评估了8种不同LLM/VLM模型的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统和物理/具身AI系统是人工智能和机器人领域的关键研究方向，但两者交叉的智能体具身AI研究仍然稀缺。

Method: 开发了一个MCP服务器，内置机器人领域知识的工具，支持移动机器人轨迹、激光扫描数据、变换和时间序列数据的分析，并提供轻量级UI进行不同LLM的基准测试。

Result: 实验表明不同LLM在工具调用能力上存在显著差异，Kimi K2和Claude Sonnet 4表现最佳，工具描述模式、参数数量等因素影响成功率。

Conclusion: 该工作填补了智能体具身AI领域的空白，提供了开源工具用于机器人数据分析，并揭示了LLM工具调用能力的关键影响因素。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [127] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于学习的框架，将LLM高级任务规划器与混合模仿学习和强化学习的低级策略相结合，用于解决礼品包装这一涉及可变形物体操作的长时程任务。


<details>
  <summary>Details</summary>
Motivation: 在仓库和零售店等环境中，人机协作处理可变形物体（如纸张、袋子和织物）时，由于可变形材料的不可预测动力学和自适应力控制需求，协调机器人动作与人类协助仍然很困难。

Method: 提出了Sub-task Aware Robotic Transformer (START)方法，从人类演示中学习统一策略。关键创新在于通过子任务ID提供显式时间基础，捕捉整个包装序列的长程时间依赖性，使策略学习子目标而非简单复制运动序列。

Result: 在真实世界包装任务中实现了97%的成功率，统一的基于Transformer的策略减少了对专用模型的需求，允许受控的人类监督，并有效连接高级意图与细粒度力控制。

Conclusion: 该框架成功解决了可变形物体操作中的人机协作挑战，通过统一的Transformer策略实现了长时程任务的稳健执行。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [128] [EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: EvtSlowTV是一个从YouTube视频构建的大规模事件相机数据集，包含超过130亿个事件，用于自监督深度估计，解决了现有数据集规模小、泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机深度估计方法受限于小规模标注数据集，难以泛化到真实世界场景。需要大规模、无约束的自然场景数据集来提升模型性能。

Method: 从YouTube公开视频构建EvtSlowTV数据集，包含各种环境条件和运动场景。采用自监督学习框架，利用事件流的高动态范围特性，无需帧级标注。

Result: EvtSlowTV比现有事件数据集大一个数量级，训练后的模型在复杂场景和运动中表现出更好的泛化能力。

Conclusion: EvtSlowTV数据集为事件相机深度估计提供了大规模、多样化的训练数据，自监督方法有效利用了事件数据的异步特性和高动态范围优势。

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [129] [Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification](https://arxiv.org/abs/2511.02992)
*Mikhael Djajapermana,Moritz Reiber,Daniel Mueller-Gritschneder,Ulf Schlichtmann*

Main category: cs.CV

TL;DR: 提出了一种新的混合CNN-ViT神经架构搜索空间，用于在严格模型大小约束下寻找高效的图像分类架构。


<details>
  <summary>Details</summary>
Motivation: 现有的混合CNN-ViT架构参数量大、计算成本高，不适合tinyML部署，需要寻找更高效的混合架构。

Method: 设计了包含混合CNN和ViT块的新搜索空间，以及可搜索池化层的新池化块，用于高效特征图缩减。

Result: 在CIFAR10数据集上的实验表明，该方法产生的混合CNN-ViT架构在准确性和推理速度上都优于基于ResNet的tinyML模型。

Conclusion: 提出的搜索空间能够生成在严格模型大小约束下具有优越性能的混合CNN-ViT架构，适合tinyML部署。

Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT)
have outperformed pure CNN or ViT architecture. However, since these
architectures require large parameters and incur large computational costs,
they are unsuitable for tinyML deployment. This paper introduces a new hybrid
CNN-ViT search space for Neural Architecture Search (NAS) to find efficient
hybrid architectures for image classification. The search space covers hybrid
CNN and ViT blocks to learn local and global information, as well as the novel
Pooling block of searchable pooling layers for efficient feature map reduction.
Experimental results on the CIFAR10 dataset show that our proposed search space
can produce hybrid CNN-ViT architectures with superior accuracy and inference
speed to ResNet-based tinyML models under tight model size constraints.

</details>


### [130] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 提出基于学习的移动激光扫描点云不确定性评估框架，无需地面真值，通过几何特征预测点级不确定性


<details>
  <summary>Details</summary>
Motivation: 移动激光扫描点云在高精度应用中需要评估不确定性，但获取地面真值成本高且不可行，需要减少对地面真值的依赖

Method: 集成最优邻域估计与几何特征提取的学习框架，使用XGBoost模型进行不确定性预测

Result: 在真实数据集上验证可行，XGBoost模型精度与随机森林相当但效率提高约3倍，几何特征可有效预测基于C2C距离的点级不确定性

Conclusion: 移动激光扫描点云的不确定性是可学习的，为不确定性评估研究提供了新的基于学习的视角

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


### [131] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种新的零样本提示方法QG-CoC，用于解决多模态大语言模型在多图像场景下细粒度感知不足和推理能力下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多图像环境中存在两个关键问题：缺乏跨图像的细粒度感知能力，以及多视觉信息推理和整合能力下降。现有研究主要关注单图像或特定受限场景，缺乏对复杂多图像推理任务的系统研究。

Method: 提出QG-CoC（问题引导的标题链）方法，这是一种通用的零样本提示方法，能够处理任意数量的图像。该方法通过问题引导的方式生成图像标题链，有效整合感知和推理过程。

Result: 在各种开源和闭源MLLMs上的多图像和单图像基准测试表明，QG-CoC在不同任务中表现出竞争力，并在现有提示方法失败的挑战性场景中展现出稳健的改进。

Conclusion: QG-CoC是一种有效的通用提示方法，能够显著提升多模态大语言模型在多图像环境下的感知和推理能力，特别是在复杂场景中表现出色。

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [132] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: 提出了AAPL方法，通过引入对抗性token嵌入来解耦图像增强带来的表面视觉变化与类别相关语义表示，从而改进提示学习在零样本学习中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法如CoOp和CoCoOp在泛化到完全未见类别时表现不佳，且主要关注文本层面的修改，忽略了图像增强的潜力。

Method: AAPL方法引入对抗性token嵌入，解耦图像增强引入的表面视觉变化与类别相关语义表示，使学习到的提示能够专注于与目标类别对齐的视觉判别特征。

Result: 在11个基准数据集上的实验表明，AAPL在少样本、零样本、跨数据集和领域泛化设置中均优于现有方法。

Conclusion: AAPL通过结合图像增强和对抗性token嵌入，有效提升了提示学习的泛化能力，为视觉语言模型的零样本学习提供了新的解决方案。

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [133] [A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features](https://arxiv.org/abs/2511.02877)
*S. K. Laha*

Main category: cs.NE

TL;DR: 提出了一种结合时间延迟嵌入和随机傅里叶特征映射的新型储层计算框架，用于混沌时间序列预测，无需传统循环架构。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算依赖高维循环连接性，需要手动调整超参数，且难以捕捉混沌系统的内在几何结构。

Method: 将时间延迟嵌入与随机傅里叶特征映射结合，构建动态储层，近似非线性核变换来揭示重构相空间中的潜在动力学关系。

Result: 在Mackey-Glass方程、Lorenz系统和Kuramoto-Sivashinsky方程等典型混沌系统上，RFF-RC实现了更高的预测精度、鲁棒的吸引子重构和长期预测能力。

Conclusion: 延迟嵌入与RFF储层的结合通过将系统嵌入到丰富的特征空间中揭示了新的动力学结构，为混沌动力学建模提供了计算高效且可解释的方法。

Abstract: Forecasting chaotic time series requires models that can capture the
intrinsic geometry of the underlying attractor while remaining computationally
efficient. We introduce a novel reservoir computing (RC) framework that
integrates time-delay embedding with Random Fourier Feature (RFF) mappings to
construct a dynamical reservoir without the need for traditional recurrent
architectures. Unlike standard RC, which relies on high-dimensional recurrent
connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel
transformations that uncover latent dynamical relations in the reconstructed
phase space. This hybrid formulation offers two key advantages: (i) it provides
a principled way to approximate complex nonlinear interactions among delayed
coordinates, thereby enriching the effective dynamical representation of the
reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters
such as spectral radius and leaking rate. We evaluate the framework on
canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the
Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC
not only achieves superior prediction accuracy but also yields robust attractor
reconstructions and long-horizon forecasts. These results show that the
combination of delay embedding and RFF-based reservoirs reveals new dynamical
structure by embedding the system in an enriched feature space, providing a
computationally efficient and interpretable approach to modeling chaotic
dynamics.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [134] [SyMuPe: Affective and Controllable Symbolic Music Performance](https://arxiv.org/abs/2511.03425)
*Ilya Borovik,Dmitrii Gavrilev,Vladimir Viro*

Main category: cs.SD

TL;DR: SyMuPe框架开发情感可控的符号钢琴演奏模型PianoFlow，使用条件流匹配训练多掩码演奏修复任务，支持无条件生成和填充，结合情感分类器和文本嵌入实现情感控制，在客观和主观评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习模型实现人类水平的音乐演奏表达和情感仍然具有挑战性，需要开发更有效的情感可控演奏模型。

Method: 使用条件流匹配训练多掩码演奏修复任务，集成钢琴演奏情感分类器，使用情感加权的Flan-T5文本嵌入作为条件输入。

Result: PianoFlow在客观和主观评估中优于基于Transformer的基线模型和现有方法，达到与人类录制和转录MIDI样本相当的性能质量。

Conclusion: 该模型可集成到交互式应用中，有助于创建更易访问和引人入胜的音乐演奏系统。

Abstract: Emotions are fundamental to the creation and perception of music
performances. However, achieving human-like expression and emotion through
machine learning models for performance rendering remains a challenging task.
In this work, we present SyMuPe, a novel framework for developing and training
affective and controllable symbolic piano performance models. Our flagship
model, PianoFlow, uses conditional flow matching trained to solve diverse
multi-mask performance inpainting tasks. By design, it supports both
unconditional generation and infilling of music performance features. For
training, we use a curated, cleaned dataset of 2,968 hours of aligned musical
scores and expressive MIDI performances. For text and emotion control, we
integrate a piano performance emotion classifier and tune PianoFlow with the
emotion-weighted Flan-T5 text embeddings provided as conditional inputs.
Objective and subjective evaluations against transformer-based baselines and
existing models show that PianoFlow not only outperforms other approaches, but
also achieves performance quality comparable to that of human-recorded and
transcribed MIDI samples. For emotion control, we present and analyze samples
generated under different text conditioning scenarios. The developed model can
be integrated into interactive applications, contributing to the creation of
more accessible and engaging music performance systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [135] [Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods](https://arxiv.org/abs/2511.03020)
*Fatimo Adenike Adeniya*

Main category: cs.CR

TL;DR: 本研究提出了一个结合统计建模和机器学习的混合分析框架，用于检测和预测电子商务领域的网络攻击模式，发现节假日购物期间网络攻击更严重，CatBoost模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台面临的网络攻击日益复杂，威胁消费者信任和运营连续性，需要开发有效的检测和预测方法来应对这些威胁。

Method: 使用Verizon社区数据泄露数据集，应用Auto ARIMA进行时间序列预测和显著性检验，包括Mann-Whitney U检验和ANOVA分析季节性变化，并采用集成机器学习模型（XGBoost、LightGBM和CatBoost）进行预测分类。

Result: 发现节假日购物活动期间网络攻击显著更严重，黑五和假日季节出现反复攻击高峰，涉及个人身份信息的泄露显示更高的威胁指标。CatBoost模型表现最佳（准确率85.29%，F1分数0.2254，ROC AUC 0.8247）。

Conclusion: 该框架独特地结合了季节性预测和可解释的集成学习，能够进行时间风险预期和泄露类型分类，为主动网络安全资源分配提供见解，并为未来实时威胁检测研究指明方向。

Abstract: Cyberattacks on e-commerce platforms have grown in sophistication,
threatening consumer trust and operational continuity. This research presents a
hybrid analytical framework that integrates statistical modelling and machine
learning for detecting and forecasting cyberattack patterns in the e-commerce
domain. Using the Verizon Community Data Breach (VCDB) dataset, the study
applies Auto ARIMA for temporal forecasting and significance testing, including
a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday
shopping events experienced significantly more severe cyberattacks than
non-holiday periods. ANOVA was also used to examine seasonal variation in
threat severity, while ensemble machine learning models (XGBoost, LightGBM, and
CatBoost) were employed for predictive classification. Results reveal recurrent
attack spikes during high-risk periods such as Black Friday and holiday
seasons, with breaches involving Personally Identifiable Information (PII)
exhibiting elevated threat indicators. Among the models, CatBoost achieved the
highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247).
The framework uniquely combines seasonal forecasting with interpretable
ensemble learning, enabling temporal risk anticipation and breach-type
classification. Ethical considerations, including responsible use of sensitive
data and bias assessment, were incorporated. Despite class imbalance and
reliance on historical data, the study provides insights for proactive
cybersecurity resource allocation and outlines directions for future real-time
threat detection research.

</details>


### [136] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 测试了8个开源大语言模型的安全性和安全性状况，发现所有模型都存在普遍漏洞，多轮攻击成功率比单轮攻击高2-10倍，突显了当前开源模型在扩展交互中维护安全护栏的系统性不足。


<details>
  <summary>Details</summary>
Motivation: 评估开源权重模型的安全性和安全性状况，识别可能影响后续微调和部署的漏洞，为开发者和研究人员提供风险认知。

Method: 使用自动化对抗测试，测量每个模型对单轮和多轮提示注入和越狱攻击的弹性。

Result: 所有测试模型都存在普遍漏洞，多轮攻击成功率在25.86%到92.78%之间，比单轮基线高2-10倍。能力导向模型如Llama 3.3和Qwen 3表现出更高的多轮易感性，而安全导向设计如Google Gemma 3表现更均衡。

Conclusion: 开源模型虽然对创新至关重要，但在没有分层安全控制的情况下部署会带来实际的操作和道德风险。建议采用安全优先的设计理念和分层保护措施，确保开源模型的弹性部署。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [137] [Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models](https://arxiv.org/abs/2511.03147)
*Haotian Yin,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种针对神经符号距离函数中ODW损失的时间调度策略，通过动态调整正则化权重来平衡早期稳定性和后期细节恢复，在CAD重建任务中显著优于固定权重方法。


<details>
  <summary>Details</summary>
Motivation: FlatCAD中的固定ODW权重在训练过程中存在矛盾：早期需要强正则化来稳定优化，但后期会抑制细节恢复。需要一种动态调度策略来解决这一矛盾。

Method: 研究了多种ODW损失权重调度策略，包括常数、线性、五次函数和阶梯插值调度，以及递增预热变体，在训练过程中动态调整正则化强度。

Result: 在ABC CAD数据集上的实验表明，时间变化调度策略始终优于固定权重，相比FlatCAD基线在Chamfer距离上实现了高达35%的改进。

Conclusion: 时间调度是曲率正则化的简单而有效的扩展，能够实现更稳健的CAD重建，在早期稳定优化和后期细节恢复之间取得良好平衡。

Abstract: Neural signed distance functions (SDFs) have become a powerful representation
for geometric reconstruction from point clouds, yet they often require both
gradient- and curvature-based regularization to suppress spurious warp and
preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten
(ODW) loss as an efficient second-order prior for CAD surfaces, approximating
full-Hessian regularization at roughly half the computational cost. However,
FlatCAD applies a fixed ODW weight throughout training, which is suboptimal:
strong regularization stabilizes early optimization but suppresses detail
recovery in later stages. We present scheduling strategies for the ODW loss
that assign a high initial weight to stabilize optimization and progressively
decay it to permit fine-scale refinement. We investigate constant, linear,
quintic, and step interpolation schedules, as well as an increasing warm-up
variant. Experiments on the ABC CAD dataset demonstrate that time-varying
schedules consistently outperform fixed weights. Our method achieves up to a
35% improvement in Chamfer Distance over the FlatCAD baseline, establishing
scheduling as a simple yet effective extension of curvature regularization for
robust CAD reconstruction.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [138] [Non-Monotonicity in Fair Division of Graphs](https://arxiv.org/abs/2511.03629)
*Hadi Hosseini,Shraddha Pathak,Yu Zhou*

Main category: cs.GT

TL;DR: 该论文研究在图中公平分配顶点的问题，其中捆绑包的价值由其割值决定。重点探讨EF1公平性与转移稳定性等效率概念的兼容性，发现在不同代理数量下存在非单调关系。


<details>
  <summary>Details</summary>
Motivation: 解决团队组建和网络分区等应用中固有的非单调估值问题，其中边际价值可能为正、负或零，取决于捆绑包的组成。

Method: 使用图论方法分析顶点分配问题，研究EF1公平性与转移稳定性等效率概念的关系，并开发高效算法来寻找满足条件的分配方案。

Result: 发现EF1与转移稳定性分配的存在性与代理数量n呈非单调关系：n=2时总存在，n=3时可能不存在，n≥4时又存在。通过弱化效率要求或限制图为森林可保证任何n下的存在性。

Conclusion: 在非单调估值设置下，EF1公平性与效率概念之间存在复杂关系，但通过适当调整条件或图结构限制，可以设计高效算法找到满足要求的公平分配方案。

Abstract: We consider the problem of fairly allocating the vertices of a graph among
$n$ agents, where the value of a bundle is determined by its cut value -- the
number of edges with exactly one endpoint in the bundle. This model naturally
captures applications such as team formation and network partitioning, where
valuations are inherently non-monotonic: the marginal values may be positive,
negative, or zero depending on the composition of the bundle. We focus on the
fairness notion of envy-freeness up to one item (EF1) and explore its
compatibility with several efficiency concepts such as Transfer Stability (TS)
that prohibits single-item transfers that benefit one agent without making the
other worse-off. For general graphs, our results uncover a non-monotonic
relationship between the number of agents $n$ and the existence of allocations
satisfying EF1 and transfer stability (TS): such allocations always exist for
$n=2$, may fail to exist for $n=3$, but exist again for all $n\geq 4$. We
further show that existence can be guaranteed for any $n$ by slightly weakening
the efficiency requirement or by restricting the graph to forests. All of our
positive results are achieved via efficient algorithms.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [139] [Workday's Approach to Secure and Compliant Cloud ERP Systems](https://arxiv.org/abs/2511.02856)
*Monu Sharma*

Main category: cs.CE

TL;DR: Workday符合GDPR、SOC 2、HIPAA等全球标准，通过自动化合规功能提升安全性和效率，并探索AI、ML和区块链技术增强威胁检测和数据完整性。


<details>
  <summary>Details</summary>
Motivation: 评估Workday作为ERP解决方案在数据保护和合规方面的能力，特别是在处理金融、医疗和政府等敏感数据时的表现。

Method: 采用比较分析的方法，评估Workday的合规特性和自动化功能，包括审计追踪、行为分析和持续报告等。

Result: Workday展现出增强的风险管理、运营灵活性和违规缓解能力，被定位为安全、合规且面向未来的ERP解决方案。

Conclusion: Workday通过其合规框架和前瞻性技术集成，为安全的企业云管理设立了新标准。

Abstract: Workday's compliance with global standards -- such as GDPR, SOC 2, HIPAA, ISO
27001, and FedRAMP -- shows its ability to best protect critical financial,
healthcare, and government data.Automated compliance attributes like audit
trails, behavioral analytics, and continuous reporting improve automation of
the process and cut down on the manual effort to audit. A comparative review
demonstrates enhanced risk management, operational flexibility, and breach
mitigation. The paper also discusses potential future solutions with AI, ML and
blockchain, to enhance attackdetection and data integrity. Overall, Workday
turns out to be a secure, compliant and future-ready ERP solution. The paper
also explores emerging trends, including the integration of AI, machine
learning, and blockchain technologies to enhance next-generation threat
detection and data integrity. The findings position Workday as a reliable,
compliant, and future-ready ERP solution, setting a new benchmark for secure
enterprise cloud management.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [140] [The Structure of Cross-Validation Error: Stability, Covariance, and Minimax Limits](https://arxiv.org/abs/2511.03554)
*Ido Nachum,Rüdiger Urbanke,Thomas Weinberger*

Main category: math.ST

TL;DR: 本文研究了k折交叉验证中折数选择的理论问题，提出了新的均方误差分解方法，并证明了交叉验证在风险估计中的基本性能界限。


<details>
  <summary>Details</summary>
Motivation: 尽管交叉验证在理论上已有研究，但关于算法-分布对如何影响k折交叉验证中折数选择的理论问题仍然开放，这促使我们研究这一问题。

Method: 提出了交叉验证风险估计均方误差的新分解方法，明确捕捉了重叠折间误差估计的相关性，并引入了比典型假设稳定性更弱的平方损失稳定性概念。

Result: 证明了两个主要结果：1) 对于任何最小化经验误差的学习算法，k折CV估计总体风险的均方误差存在Ω(√k/n)的极小极大下界；2) 存在学习规则使得最大均方误差为Ω(k/n)，与单折大小为n/k的留出估计器精度匹配。

Conclusion: 这些结果界定了基于重采样的风险估计中的基本权衡：CV无法充分利用所有n个样本进行无偏风险评估，其极小极大性能被限制在k/n和√k/n两个区间之间。

Abstract: Despite ongoing theoretical research on cross-validation (CV), many
theoretical questions about CV remain widely open. This motivates our
investigation into how properties of algorithm-distribution pairs can affect
the choice for the number of folds in $k$-fold cross-validation.
  Our results consist of a novel decomposition of the mean-squared error of
cross-validation for risk estimation, which explicitly captures the
correlations of error estimates across overlapping folds and includes a novel
algorithmic stability notion, squared loss stability, that is considerably
weaker than the typically required hypothesis stability in other comparable
works.
  Furthermore, we prove:
  1. For every learning algorithm that minimizes empirical error, a minimax
lower bound on the mean-squared error of $k$-fold CV estimating the population
risk $L_\mathcal{D}$: \[ \min_{k \mid n}\; \max_{\mathcal{D}}\;
\mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} -
L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega\!\big(\sqrt{k}/n\big), \] where
$n$ is the sample size and $k$ the number of folds. This shows that even under
idealized conditions, for large values of $k$, CV cannot attain the optimum of
order $1/n$ achievable by a validation set of size $n$, reflecting an inherent
penalty caused by dependence between folds.
  2. Complementing this, we exhibit learning rules for which \[
  \max_{\mathcal{D}}\; \mathbb{E}\!\left[\big(\widehat{L}_{\mathrm{CV}}^{(k)} -
L_{\mathcal{D}}\big)^{2}\right] \;=\; \Omega(k/n), \] matching (up to
constants) the accuracy of a hold-out estimator of a single fold of size $n/k$.
  Together these results delineate the fundamental trade-off in
resampling-based risk estimation: CV cannot fully exploit all $n$ samples for
unbiased risk evaluation, and its minimax performance is pinned between the
$k/n$ and $\sqrt{k}/n$ regimes.

</details>


### [141] [The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp Characterization of the Price of Unknown Margin](https://arxiv.org/abs/2511.03708)
*Rong Jiang,Cong Ma*

Main category: math.ST

TL;DR: 本文研究了在未知边际参数α的情况下，批量非参数上下文赌博机问题，引入了后悔膨胀准则来衡量对未知α的统计代价，并开发了RoBIN算法实现接近最优的后悔膨胀。


<details>
  <summary>Details</summary>
Motivation: 研究在批量设置下，当边际参数α未知时，自适应算法相对于知道α的oracle算法的性能损失，揭示批量设置下的自适应障碍。

Method: 引入后悔膨胀准则作为性能度量，通过凸优化问题确定最优批量分配和探索策略，开发RoBIN算法实现自适应批量分箱。

Result: 发现最优后悔膨胀随T多项式增长，其指数由涉及维度、光滑性和批量预算的凸优化问题决定。当批次数超过log log T时，自适应障碍消失。

Conclusion: 在批量设置下，对未知边际参数的自适应不可避免地会产生多项式惩罚，但当批次数达到双对数级时，可以恢复oracle的后悔率。

Abstract: We study batched nonparametric contextual bandits under a margin condition
when the margin parameter $\alpha$ is unknown. To capture the statistical price
of this ignorance, we introduce the regret inflation criterion, defined as the
ratio between the regret of an adaptive algorithm and that of an oracle knowing
$\alpha$. We show that the optimal regret inflation grows polynomial with the
horizon $T$, with exponent precisely given by the value of a convex
optimization problem involving the dimension, smoothness, and batch budget.
Moreover, the minimizers of this optimization problem directly prescribe the
batch allocation and exploration strategy of a rate-optimal algorithm. Building
on this principle, we develop RoBIN (RObust batched algorithm with adaptive
BINning), which achieves the optimal regret inflation up to logarithmic
factors. These results reveal a new adaptivity barrier: under batching,
adaptation to an unknown margin parameter inevitably incurs a polynomial
penalty, sharply characterized by a variational problem. Remarkably, this
barrier vanishes when the number of batches exceeds $\log \log T$; with only a
doubly logarithmic number of updates, one can recover the oracle regret rate up
to polylogarithmic factors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [142] [Influence of Data Dimensionality Reduction Methods on the Effectiveness of Quantum Machine Learning Models](https://arxiv.org/abs/2511.03320)
*Aakash Ravindra Shinde,Jukka K. Nurminen*

Main category: quant-ph

TL;DR: 数据降维技术在量子机器学习中用于解决NISQ设备的限制，但会导致性能指标偏差，错误估计模型实际性能。研究发现使用数据降维与不使用之间存在14%-48%的准确率差异。


<details>
  <summary>Details</summary>
Motivation: 分析数据降维方法如何影响不同QML模型，解决NISQ量子设备的噪声和有限量子比特数限制，以及经典设备模拟大量量子比特的挑战。

Method: 在多个生成的数据集、量子机器学习算法、量子数据编码方法和数据降维方法上进行实验，评估准确率、精确率、召回率和F1分数等性能指标。

Result: 使用数据降维方法会导致性能指标值偏差，错误估计量子机器学习模型的真实性能。数据集特征、量子信息嵌入方法、特征降维比例、经典组件和QML模型结构等因素加剧了这一问题。

Conclusion: 数据降维方法在量子机器学习中会产生误导性的性能评估，某些数据降维方法对特定数据嵌入方法和ansatz构造表现更好。

Abstract: Data dimensionality reduction techniques are often utilized in the
implementation of Quantum Machine Learning models to address two significant
issues: the constraints of NISQ quantum devices, which are characterized by
noise and a limited number of qubits, and the challenge of simulating a large
number of qubits on classical devices. It also raises concerns over the
scalability of these approaches, as dimensionality reduction methods are slow
to adapt to large datasets. In this article, we analyze how data reduction
methods affect different QML models. We conduct this experiment over several
generated datasets, quantum machine algorithms, quantum data encoding methods,
and data reduction methods. All these models were evaluated on the performance
metrics like accuracy, precision, recall, and F1 score. Our findings have led
us to conclude that the usage of data dimensionality reduction methods results
in skewed performance metric values, which results in wrongly estimating the
actual performance of quantum machine learning models. There are several
factors, along with data dimensionality reduction methods, that worsen this
problem, such as characteristics of the datasets, classical to quantum
information embedding methods, percentage of feature reduction, classical
components associated with quantum models, and structure of quantum machine
learning models. We consistently observed the difference in the accuracy range
of 14% to 48% amongst these models, using data reduction and not using it.
Apart from this, our observations have shown that some data reduction methods
tend to perform better for some specific data embedding methodologies and
ansatz constructions.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [143] [EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture](https://arxiv.org/abs/2511.03122)
*Seunghee Han,Yeonghun Kang,Taeun Bae,Varinia Bernales,Alan Aspuru-Guzik,Jihan Kim*

Main category: cond-mat.mtrl-sci

TL;DR: EGMOF是一个混合扩散-变换器框架，通过模块化描述符中介的工作流程实现金属有机框架（MOF）的高效逆设计，解决了化学空间广阔和标记数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 传统材料设计方法面临化学空间广阔和属性标记数据稀缺的挑战，现有生成模型需要大量数据且针对每个新目标属性都需要重新训练。

Method: 采用模块化混合设计：1）一维扩散模型（Prop2Desc）将所需属性映射到化学有意义的描述符；2）变换器模型（Desc2MOF）从这些描述符生成结构。

Result: 在氢吸附数据集上，EGMOF实现了超过95%的有效性和84%的命中率，相比现有方法有效性提升达57%，命中率提升14%，且在仅1000个训练样本下仍保持高效。

Conclusion: 该工作提出了一种数据高效、可推广的MOF逆设计方法，展示了模块化逆设计工作流程在更广泛材料发现中的潜力。

Abstract: Designing materials with targeted properties remains challenging due to the
vastness of chemical space and the scarcity of property-labeled data. While
recent advances in generative models offer a promising way for inverse design,
most approaches require large datasets and must be retrained for every new
target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a
hybrid diffusion-transformer framework that overcomes these limitations through
a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into
two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired
properties to chemically meaningful descriptors followed by (2) a transformer
model (Desc2MOF) that generates structures from these descriptors. This modular
hybrid design enables minimal retraining and maintains high accuracy even under
small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95%
validity and 84% hit rate, representing significant improvements of up to 57%
in validity and 14% in hit rate compared to existing methods, while remaining
effective with only 1,000 training samples. Moreover, our model successfully
performed conditional generation across 29 diverse property datasets, including
CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models
have not. This work presents a data-efficient, generalizable approach to the
inverse design of diverse MOFs and highlights the potential of modular inverse
design workflows for broader materials discovery.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [144] [System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc](https://arxiv.org/abs/2511.03482)
*Giorgio Palma,Ivan Santic,Andrea Serani,Lorenzo Minno,Matteo Diez*

Main category: eess.SY

TL;DR: 本研究使用HDMDc和BHDMDc方法对系泊条件下的小型自主水面艇进行系统辨识，在规则和不规则迎浪条件下验证了模型预测能力，首次展示了该方法在不同海况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决带有月池的小型自主水面艇在系泊条件下的非线性系统辨识问题，月池引起的晃荡效应增加了建模难度，需要开发准确的数据驱动模型。

Method: 采用Hankel动态模态分解控制(HDMDc)及其贝叶斯扩展(BHDMDc)，基于船体运动和系泊载荷测量数据构建降阶模型，在拖曳水池中进行规则和不规则迎浪实验。

Result: HDMDc提供了准确的确定性预测，BHDMDc通过考虑超参数变异性实现了不确定性感知的模型响应表征，两种方法都能有效预测未见过的规则和不规则波浪激励下的船舶响应。

Conclusion: 基于HDMDc的降阶模型是系统辨识的可行数据驱动替代方案，首次证明了其在不同海况下的泛化能力，在重现船舶动力学方面具有高精度。

Abstract: This study addresses the system identification of a small autonomous surface
vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition
with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were
carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under
both irregular and regular head-sea wave conditions. The ASV under
investigation features a recessed moon pool, which induces nonlinear responses
due to sloshing, thereby increasing the modelling challenge. Data-driven
reduced-order models were built from measurements of vessel motions and mooring
loads. The HDMDc framework provided accurate deterministic predictions of
vessel dynamics, while the Bayesian formulation enabled uncertainty-aware
characterization of the model response by accounting for variability in
hyperparameter selection. Validation against experimental data demonstrated
that both HDMDc and BHDMDc can predict the vessel's response to unseen regular
and irregular wave excitations. In conclusion, the study shows that HDMDc-based
ROMs are a viable data-driven alternative for system identification,
demonstrating for the first time their generalization capability for a sea
condition different from the training set, achieving high accuracy in
reproducing vessel dynamics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [145] [Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models](https://arxiv.org/abs/2511.02986)
*Giovanni Palla,Sudarshan Babu,Payam Dibaeinia,James D. Pearce,Donghui Li,Aly A. Khan,Theofanis Karaletsos,Jakub M. Tomczak*

Main category: stat.ML

TL;DR: 提出了scLDM，一种用于单细胞基因表达数据的可扩展潜在扩散模型，通过多头交叉注意力块架构和扩散变换器实现高质量生成，在多种实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞基因表达的计算建模对理解细胞过程至关重要，但由于基因表达数据的计数性质和基因间复杂的潜在依赖关系，生成真实的表达谱仍然是一个主要挑战。现有生成模型通常强加人工基因排序或依赖浅层神经网络架构。

Method: 使用固定大小的潜在变量和统一的多头交叉注意力块架构，在编码器中实现置换不变池化，在解码器中实现置换等变反池化。用基于扩散变换器和线性插值的潜在扩散模型替换高斯先验，支持多条件无分类器引导。

Result: 在观察性和扰动性单细胞数据的各种实验中表现出优越性能，并在细胞级分类等下游任务中表现良好。

Conclusion: scLDM模型能够尊重数据的可交换性基本属性，通过创新的架构设计实现了高质量的单细胞基因表达数据生成。

Abstract: Computational modeling of single-cell gene expression is crucial for
understanding cellular processes, but generating realistic expression profiles
remains a major challenge. This difficulty arises from the count nature of gene
expression data and complex latent dependencies among genes. Existing
generative models often impose artificial gene orderings or rely on shallow
neural network architectures. We introduce a scalable latent diffusion model
for single-cell gene expression data, which we refer to as scLDM, that respects
the fundamental exchangeability property of the data. Our VAE uses fixed-size
latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)
architecture, which serves dual roles: permutation-invariant pooling in the
encoder and permutation-equivariant unpooling in the decoder. We enhance this
framework by replacing the Gaussian prior with a latent diffusion model using
Diffusion Transformers and linear interpolants, enabling high-quality
generation with multi-conditional classifier-free guidance. We show its
superior performance in a variety of experiments for both observational and
perturbational single-cell data, as well as downstream tasks like cell-level
classification.

</details>


### [146] [Unifying Information-Theoretic and Pair-Counting Clustering Similarity](https://arxiv.org/abs/2511.03000)
*Alexander J. Gates*

Main category: stat.ML

TL;DR: 本文提出了一个统一框架，将聚类相似性度量的两个主要家族（对计数和信息论）联系起来，揭示了它们作为观测与期望共现的加权扩展的深层关系。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类相似性度量会产生分歧甚至矛盾的评价结果，两个主要家族之间的深层分析联系尚未完全理解，需要建立统一的分析框架。

Method: 开发了一个分析框架，从两个互补视角统一两个家族：1) 作为观测与期望共现的加权扩展；2) 将对计数推广到k元组一致性，将信息论度量视为系统累积高阶共分配结构。

Result: 成功将Rand指数和互信息等度量纳入统一框架，阐明了两个家族何时以及为何会产生分歧，将其敏感性直接与加权和近似阶数联系起来。

Conclusion: 该框架为选择、解释和扩展聚类相似性度量提供了原则性基础，有助于在不同应用场景中更好地使用这些度量方法。

Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the
many existing similarity measures can produce widely divergent, sometimes
contradictory, evaluations. Clustering similarity measures are typically
organized into two principal families, pair-counting and information-theoretic,
reflecting whether they quantify agreement through element pairs or aggregate
information across full cluster contingency tables. Prior work has uncovered
parallels between these families and applied empirical normalization or
chance-correction schemes, but their deeper analytical connection remains only
partially understood. Here, we develop an analytical framework that unifies
these families through two complementary perspectives. First, both families are
expressed as weighted expansions of observed versus expected co-occurrences,
with pair-counting arising as a quadratic, low-order approximation and
information-theoretic measures as higher-order, frequency-weighted extensions.
Second, we generalize pair-counting to $k$-tuple agreement and show that
information-theoretic measures can be viewed as systematically accumulating
higher-order co-assignment structure beyond the pairwise level. We illustrate
the approaches analytically for the Rand index and Mutual Information, and show
how other indices in each family emerge as natural extensions. Together, these
views clarify when and why the two regimes diverge, relating their
sensitivities directly to weighting and approximation order, and provide a
principled basis for selecting, interpreting, and extending clustering
similarity measures across applications.

</details>


### [147] [Precise asymptotic analysis of Sobolev training for random feature models](https://arxiv.org/abs/2511.03050)
*Katharine E Fisher,Matthew TC Li,Youssef Marzouk,Timo Schorlepp*

Main category: stat.ML

TL;DR: 本文研究了Sobolev训练（同时使用函数和梯度数据进行回归）对高维过参数化随机特征模型泛化误差的影响，发现在某些情况下梯度数据并不能改善预测性能，过参数化程度应指导训练方法的选择。


<details>
  <summary>Details</summary>
Motivation: 梯度信息在实际应用中广泛可用且有用，但理论上对Sobolev训练在高维过参数化模型中的泛化误差影响了解甚少。

Method: 使用统计物理中的副本方法和算子值自由概率理论中的线性化方法，推导出训练后随机特征模型泛化误差的闭式描述。通过将梯度数据投影到有限维子空间来模拟实际实现。

Result: 对于单索引模型描述的目标函数，补充梯度数据并不能普遍改善预测性能。过参数化程度应指导训练方法的选择。

Conclusion: 研究确定了模型通过插值噪声函数和梯度数据达到最优性能的设置，揭示了梯度数据在过参数化环境中的有限效用。

Abstract: Gradient information is widely useful and available in applications, and is
therefore natural to include in the training of neural networks. Yet little is
known theoretically about the impact of Sobolev training -- regression with
both function and gradient data -- on the generalization error of highly
overparameterized predictive models in high dimensions. In this paper, we
obtain a precise characterization of this training modality for random feature
(RF) models in the limit where the number of trainable parameters, input
dimensions, and training data tend proportionally to infinity. Our model for
Sobolev training reflects practical implementations by sketching gradient data
onto finite dimensional subspaces. By combining the replica method from
statistical physics with linearizations in operator-valued free probability
theory, we derive a closed-form description for the generalization errors of
the trained RF models. For target functions described by single-index models,
we demonstrate that supplementing function data with additional gradient data
does not universally improve predictive performance. Rather, the degree of
overparameterization should inform the choice of training method. More broadly,
our results identify settings where models perform optimally by interpolating
noisy function and gradient data.

</details>


### [148] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


### [149] [Provable Separations between Memorization and Generalization in Diffusion Models](https://arxiv.org/abs/2511.03202)
*Zeqi Ye,Qijie Zhu,Molei Tao,Minshuo Chen*

Main category: stat.ML

TL;DR: 本文从统计估计和网络逼近两个互补视角，建立了双重分离理论来解释扩散模型中的记忆化现象，并基于此开发了一种剪枝方法来减少记忆化同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在多个领域取得了显著成功，但容易产生记忆化问题——复制训练数据而非生成新颖输出。这不仅限制了其创造潜力，还引发了隐私和安全担忧。虽然已有实证研究探索缓解策略，但对记忆化的理论理解仍然有限。

Method: 通过统计估计和网络逼近两个互补视角建立双重分离理论：从估计角度证明真实评分函数不最小化经验去噪损失；从逼近角度证明实现经验评分函数需要网络规模随样本量增长。基于这些洞察，开发了一种基于剪枝的方法来减少扩散变换器中的记忆化。

Result: 理论分析揭示了记忆化的根本原因：统计估计分离和网络逼近分离。实验表明，所提出的剪枝方法能够有效减少记忆化，同时保持生成质量。

Conclusion: 该研究为理解扩散模型记忆化提供了理论基础，提出的双重分离框架揭示了记忆化的本质机制，基于此开发的剪枝方法为解决记忆化问题提供了有效途径，有助于提升扩散模型的创造性和安全性。

Abstract: Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a pruning-based method that reduces memorization while maintaining
generation quality in diffusion transformers.

</details>


### [150] [RKUM: An R Package for Robust Kernel Unsupervised Methods](https://arxiv.org/abs/2511.03216)
*Md Ashad Alam*

Main category: stat.ML

TL;DR: RKUM是一个R包，用于实现鲁棒核无监督方法，通过广义损失函数估计鲁棒核协方差算子和交叉协方差算子，提供鲁棒核CCA及其影响函数分析，能有效识别异常值并降低对污染数据的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统核方法使用二次损失函数，对污染或噪声数据敏感，需要开发鲁棒核方法来提高在高维数据应用中的可靠性。

Method: 使用广义损失函数替代传统二次损失来估计鲁棒核协方差算子和交叉协方差算子，实现鲁棒核CCA及其影响函数分析。

Result: 实验表明，标准核CCA的影响函数能有效识别异常值，而RKUM实现的鲁棒核方法对数据污染的敏感性显著降低。

Conclusion: RKUM为高维数据应用提供了一个高效且可扩展的鲁棒核分析平台。

Abstract: RKUM is an R package developed for implementing robust kernel-based
unsupervised methods. It provides functions for estimating the robust kernel
covariance operator (CO) and the robust kernel cross-covariance operator (CCO)
using generalized loss functions instead of the conventional quadratic loss.
These operators form the foundation of robust kernel learning and enable
reliable analysis under contaminated or noisy data conditions. The package
includes implementations of robust kernel canonical correlation analysis
(Kernel CCA), as well as the influence function (IF) for both standard and
multiple kernel CCA frameworks. The influence function quantifies sensitivity
and helps detect influential or outlying observations across two-view and
multi-view datasets. Experiments using synthesized two-view and multi-view data
demonstrate that the IF of the standard kernel CCA effectively identifies
outliers, while the robust kernel methods implemented in RKUM exhibit reduced
sensitivity to contamination. Overall, RKUM provides an efficient and
extensible platform for robust kernel-based analysis in high-dimensional data
applications.

</details>


### [151] [Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity](https://arxiv.org/abs/2511.03606)
*Diego Martinez-Taboada,Tomas Gonzalez,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 该论文研究了向量值自归一化过程的浓度界限，突破了传统次高斯框架的限制，提供了适用于轻尾分布（如Bennett或Bernstein界限）的浓度边界，并应用于在线线性回归和（核化）线性赌博机问题。


<details>
  <summary>Details</summary>
Motivation: 虽然标量值自归一化过程的研究已很成熟，但向量值过程在次高斯框架之外的研究相对不足，而自归一化过程在序列决策和计量经济学等领域有广泛应用需求。

Method: 提出了针对轻尾分布（超越次高斯性）的向量值自归一化过程的浓度界限理论，包括Bennett或Bernstein类型的边界。

Result: 建立了向量值自归一化过程的浓度边界理论框架，能够处理更广泛的轻尾分布情况。

Conclusion: 该研究扩展了自归一化过程的理论边界，为在线线性回归和线性赌博机等应用提供了更通用的理论支撑。

Abstract: The study of self-normalized processes plays a crucial role in a wide range
of applications, from sequential decision-making to econometrics. While the
behavior of self-normalized concentration has been widely investigated for
scalar-valued processes, vector-valued processes remain comparatively
underexplored, especially outside of the sub-Gaussian framework. In this
contribution, we provide concentration bounds for self-normalized processes
with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).
We illustrate the relevance of our results in the context of online linear
regression, with applications in (kernelized) linear bandits.

</details>


### [152] [Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning](https://arxiv.org/abs/2511.03693)
*Md Ahasanul Arafath,Abhijit Kumar Ghosh,Md Rony Ahmed,Sabrin Afroz,Minhazul Hosen,Md Hasan Moon,Md Tanzim Reza,Md Ashad Alam*

Main category: stat.ML

TL;DR: 提出了一种用于结直肠癌组织病理学分级的可扩展、保护隐私的联邦学习框架，该框架在分布式训练范式中集成了多尺度特征学习。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌分级是一个关键的预后因素，但受到观察者间变异性和多机构数据共享的隐私限制的阻碍。集中式训练模型与数据治理法规冲突，并且忽视了多尺度分析的诊断重要性。

Method: 采用双流ResNetRS50骨干网络同时捕获细粒度核细节和更广泛的组织级上下文，并将其集成到使用FedProx稳定的稳健联邦学习系统中，以减轻跨多个医院异构数据分布的客户端漂移。

Result: 在CRC-HGD数据集上的广泛评估表明，该框架实现了83.5%的总体准确率，优于可比较的集中式模型（81.6%）。该系统在识别最具侵袭性的III级肿瘤方面表现出色，召回率高达87.5%。在40倍放大倍率下，准确率进一步提高到88.0%。

Conclusion: 该联邦多尺度方法不仅保护了患者隐私，还增强了模型性能和泛化能力。提出的模块化管道为可部署、隐私感知的临床AI在数字病理学中奠定了基础。

Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains
hampered by inter-observer variability and the privacy constraints of
multi-institutional data sharing. While deep learning offers a path to
automation, centralized training models conflict with data governance
regulations and neglect the diagnostic importance of multi-scale analysis. In
this work, we propose a scalable, privacy-preserving federated learning (FL)
framework for CRC histopathological grading that integrates multi-scale feature
learning within a distributed training paradigm. Our approach employs a
dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear
detail and broader tissue-level context. This architecture is integrated into a
robust FL system stabilized using FedProx to mitigate client drift across
heterogeneous data distributions from multiple hospitals. Extensive evaluation
on the CRC-HGD dataset demonstrates that our framework achieves an overall
accuracy of 83.5%, outperforming a comparable centralized model (81.6%).
Crucially, the system excels in identifying the most aggressive Grade III
tumors with a high recall of 87.5%, a key clinical priority to prevent
dangerous false negatives. Performance further improves with higher
magnification, reaching 88.0% accuracy at 40x. These results validate that our
federated multi-scale approach not only preserves patient privacy but also
enhances model performance and generalization. The proposed modular pipeline,
with built-in preprocessing, checkpointing, and error handling, establishes a
foundational step toward deployable, privacy-aware clinical AI for digital
pathology.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [153] [Min-Max Optimization Is Strictly Easier Than Variational Inequalities](https://arxiv.org/abs/2511.03052)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 本文首次证明对于无约束二次目标函数，求解min-max问题的收敛速度严格优于对应的变分不等式问题，因为min-max算法能利用变量不对称性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过求解一阶最优性条件对应的变分不等式来解决凸凹min-max问题，本文研究是否可以通过绕过这种简化来获得更快的求解速度。

Method: 使用极值多项式的锐利特征化来描述最优收敛速率，通过格林函数和共形映射计算这些多项式。

Result: 在无约束二次目标函数的标准设置中，min-max问题的一阶算法最优收敛速率严格优于对应的变分不等式问题。

Conclusion: min-max算法能够利用最小和最大变量的不对称性，这是变分不等式简化过程中丢失的关键特性，因此可以实现更快的收敛速度。

Abstract: Classically, a mainstream approach for solving a convex-concave min-max
problem is to instead solve the variational inequality problem arising from its
first-order optimality conditions. Is it possible to solve min-max problems
faster by bypassing this reduction? This paper initiates this investigation. We
show that the answer is yes in the textbook setting of unconstrained quadratic
objectives: the optimal convergence rate for first-order algorithms is strictly
better for min-max problems than for the corresponding variational
inequalities. The key reason that min-max algorithms can be faster is that they
can exploit the asymmetry of the min and max variables--a property that is lost
in the reduction to variational inequalities. Central to our analyses are sharp
characterizations of optimal convergence rates in terms of extremal polynomials
which we compute using Green's functions and conformal mappings.

</details>


### [154] [A Support-Set Algorithm for Optimization Problems with Nonnegative and Orthogonal Constraints](https://arxiv.org/abs/2511.03443)
*Lei Wang,Xin Liu,Xiaojun Chen*

Main category: math.OC

TL;DR: 本文研究了具有非负和正交约束的优化问题，提出了一种支持集算法，通过固定支持集计算闭式全局解，显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 针对具有非负和正交约束的优化问题，其中可行矩阵具有稀疏性（每行最多一个非零项），需要开发高效算法来求解这类问题。

Method: 提出支持集算法，通过固定支持集计算目标函数近端线性化子问题的闭式全局解，并设计支持集更新策略来调整非零项位置。

Result: 算法全局收敛到一阶稳定点，达到ε近似一阶稳定点的迭代复杂度为O(ε⁻²)，在非负PCA、聚类和社区检测等应用中表现优异。

Conclusion: 支持集算法能有效处理非负正交约束优化问题，具有理论保证和实际应用价值。

Abstract: In this paper, we investigate optimization problems with nonnegative and
orthogonal constraints, where any feasible matrix of size $n \times p$ exhibits
a sparsity pattern such that each row accommodates at most one nonzero entry.
Our analysis demonstrates that, by fixing the support set, the global solution
of the minimization subproblem for the proximal linearization of the objective
function can be computed in closed form with at most $n$ nonzero entries.
Exploiting this structural property offers a powerful avenue for dramatically
enhancing computational efficiency. Guided by this insight, we propose a
support-set algorithm preserving strictly the feasibility of iterates. A
central ingredient is a strategically devised update scheme for support sets
that adjusts the placement of nonzero entries. We establish the global
convergence of the support-set algorithm to a first-order stationary point, and
show that its iteration complexity required to reach an $\epsilon$-approximate
first-order stationary point is $O (\epsilon^{-2})$. Numerical results are
strongly in favor of our algorithm in real-world applications, including
nonnegative PCA, clustering, and community detection.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [155] [Modeling Headway in Heterogeneous and Mixed Traffic Flow: A Statistical Distribution Based on a General Exponential Function](https://arxiv.org/abs/2511.03154)
*Natchaphon Leungbootnak,Zihao Li,Zihang Wei,Dominique Lord,Yunlong Zhang*

Main category: stat.AP

TL;DR: 提出了一种基于实数基指数函数的新型车头时距分布模型，用于改进异质交通和混合交通中的车头时距建模精度。


<details>
  <summary>Details</summary>
Motivation: 现有车头时距分布在异质交通（不同类型车辆）和混合交通（人工驾驶与自动驾驶车辆）中无法准确反映多样化的行为特征，导致拟合效果不佳。

Method: 修改指数函数，使用实数基代替欧拉数e，增加建模灵活性，并通过归一化处理使其成为概率函数，推导出闭式方程。

Result: 使用五个公开数据集（highD、exiD、NGSIM、Waymo、Lyft）验证，在高速公路异质交通流中表现最优，在城市道路混合交通中也有良好表现。

Conclusion: 提出的分布不仅能捕捉车头时距分布的基本特征，还提供了具有物理意义的参数来描述观测车头时距的分布形状。

Abstract: The ability of existing headway distributions to accurately reflect the
diverse behaviors and characteristics in heterogeneous traffic (different types
of vehicles) and mixed traffic (human-driven vehicles with autonomous vehicles)
is limited, leading to unsatisfactory goodness of fit. To address these issues,
we modified the exponential function to obtain a novel headway distribution.
Rather than employing Euler's number (e) as the base of the exponential
function, we utilized a real number base to provide greater flexibility in
modeling the observed headway. However, the proposed is not a probability
function. We normalize it to calculate the probability and derive the
closed-form equation. In this study, we utilized a comprehensive experiment
with five open datasets: highD, exiD, NGSIM, Waymo, and Lyft to evaluate the
performance of the proposed distribution and compared its performance with six
existing distributions under mixed and heterogeneous traffic flow. The results
revealed that the proposed distribution not only captures the fundamental
characteristics of headway distribution but also provides physically meaningful
parameters that describe the distribution shape of observed headways. Under
heterogeneous flow on highways (i.e., uninterrupted traffic flow), the proposed
distribution outperforms other candidate distributions. Under urban road
conditions (i.e., interrupted traffic flow), including heterogeneous and mixed
traffic, the proposed distribution still achieves decent results.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [156] [Spatio-Temporal Attention Network for Epileptic Seizure Prediction](https://arxiv.org/abs/2511.02846)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: eess.SP

TL;DR: 提出了一种基于时空注意力网络的深度学习框架，用于癫痫患者癫痫发作的准确预测，通过建模脑电信号的复杂时空相关性并采用对抗判别器来区分发作前和发作间期的注意力模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特征工程和/或假设固定的发作前持续时间，无法有效建模脑电信号的复杂时空相关性。

Method: 使用时空注意力网络（STAN）同时建模时空相关性，并采用对抗判别器来区分发作前和发作间期的注意力模式，实现患者特异性学习。

Result: 在CHB-MIT数据集上达到96.6%的敏感性和0.011/h的误检率，在MSSM数据集上达到94.2%的敏感性和0.063/h的误检率，显著优于现有最先进方法。

Conclusion: 该框架能够可靠地在癫痫发作前至少15分钟检测到发作前状态，患者特异性窗口可延长至45分钟，为临床应用提供了足够的干预时间。

Abstract: In this study, we present a deep learning framework that learns complex
spatio-temporal correlation structures of EEG signals through a Spatio-Temporal
Attention Network (STAN) for accurate predictions of onset of seizures for
Epilepsy patients. Unlike existing methods, which rely on feature engineering
and/or assume fixed preictal durations, our approach simultaneously models
spatio-temporal correlations through STAN and employs an adversarial
discriminator to distinguish preictal from interictal attention patterns,
enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets
demonstrates 96.6\% sensitivity with 0.011/h false detection rate on CHB-MIT,
and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming
state-of-the-art methods. The framework reliably detects preictal states at
least 15 minutes before an onset, with patient-specific windows extending to 45
minutes, providing sufficient intervention time for clinical applications.

</details>


### [157] [EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding](https://arxiv.org/abs/2511.02848)
*Shantanu Sarkar,Piotr Nabrzyski,Saurabh Prasad,Jose Luis Contreras-Vidal*

Main category: eess.SP

TL;DR: EEGReXferNet是一个轻量级生成式AI框架，通过跨被试迁移学习进行EEG子空间重建，解决了传统EEG去噪方法手动干预和神经特征抑制的问题，在保持计算效率的同时提升了时空频谱分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号去噪方法需要手动干预，且在滤波/重建过程中可能抑制关键神经特征。现有生成模型方法缺乏集成的时空频谱敏感性且计算量大，不适合脑机接口等实时应用。

Method: EEGReXferNet采用模块化架构，利用邻近通道的容积传导、频带特定卷积编码和滑动窗口动态潜在特征提取，结合基于参考的缩放确保窗口间连续性，实现跨被试泛化。

Result: 该框架提升了时空频谱分辨率（平均PSD相关性≥0.95；平均频谱图RV系数≥0.85），总权重减少约45%以减轻过拟合，并保持计算效率。

Conclusion: EEGReXferNet为神经生理学和脑机接口应用提供了鲁棒的实时EEG预处理解决方案，在保持轻量化的同时显著改善了信号质量。

Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for
monitoring brain activity, but low signal-to-noise ratios (SNR) due to various
artifacts often compromise its utility. Conventional artifact removal methods
require manual intervention or risk suppressing critical neural features during
filtering/reconstruction. Recent advances in generative models, including
Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),
have shown promise for EEG reconstruction; however, these approaches often lack
integrated temporal-spectral-spatial sensitivity and are computationally
intensive, limiting their suitability for real-time applications like
brain-computer interfaces (BCIs). To overcome these challenges, we introduce
EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction
via cross-subject transfer learning - developed using Keras TensorFlow
(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume
conduction across neighboring channels, band-specific convolution encoding, and
dynamic latent feature extraction through sliding windows. By integrating
reference-based scaling, the framework ensures continuity across successive
windows and generalizes effectively across subjects. This design improves
spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean
spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate
overfitting, and maintains computational efficiency for robust, real-time EEG
preprocessing in neurophysiological and BCI applications.

</details>


### [158] [ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis](https://arxiv.org/abs/2511.02850)
*Youssif Abuzied,Hassan AbdEltawab,Abdelrhman Gaber,Tamer ElBatt*

Main category: eess.SP

TL;DR: ECGXtract是一种基于深度学习的可解释心电图特征提取方法，通过卷积神经网络提取与临床验证基准强相关的时域和形态特征，在多个实验设置中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统信号处理和黑盒机器学习方法在心电图特征提取中的局限性，开发可解释且精确的特征提取方法。

Method: 使用卷积神经网络模型，首先训练单个模型提取单一特征以确保精确性和可解释性，然后探索同时提取多个特征的可行性，包括语义分组和大规模分组策略。

Result: ECGXtract在全局特征上平均相关系数为0.80，导联II表现最佳；导联特定特征平均相关系数为0.822；在90%的特征上优于ECGdeli；语义分组对全局特征有效，但大规模分组和导联特定多输出模型性能下降。

Conclusion: 结构化分组策略可以在计算效率和模型准确性之间取得平衡，为资源有限环境下开发更可扩展和临床可解释的心电图特征提取系统铺平道路。

Abstract: This paper presents ECGXtract, a deep learning-based approach for
interpretable ECG feature extraction, addressing the limitations of traditional
signal processing and black-box machine learning methods. In particular, we
develop convolutional neural network models capable of extracting both temporal
and morphological features with strong correlations to a clinically validated
ground truth. Initially, each model is trained to extract a single feature,
ensuring precise and interpretable outputs. A series of experiments is then
carried out to evaluate the proposed method across multiple setups, including
global versus lead-specific features, different sampling frequencies, and
comparisons with other approaches such as ECGdeli. Our findings show that
ECGXtract achieves robust performance across most features with a mean
correlation score of 0.80 with the ground truth for global features, with lead
II consistently providing the best results. For lead-specific features,
ECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract
achieves superior results to the state-of-the-art open source ECGdeli as it got
a higher correlation score with the ground truth in 90% of the features.
Furthermore, we explore the feasibility of extracting multiple features
simultaneously utilizing a single model. Semantic grouping is proved to be
effective for global features, while large-scale grouping and lead-specific
multi-output models show notable performance drops. These results highlight the
potential of structured grouping strategies to balance the computational
efficiency vs. model accuracy, paving the way for more scalable and clinically
interpretable ECG feature extraction systems in limited resource settings.

</details>


### [159] [Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation](https://arxiv.org/abs/2511.02851)
*Rushuang Zhou,Yuan-Ting Zhang,M. Jamal Deen,Yining Dong*

Main category: eess.SP

TL;DR: LiteHeart是一个半监督知识蒸馏框架，通过区域感知蒸馏和跨层互信息模块，显著缩小了低成本与高成本心脏AI系统之间的诊断性能差距。


<details>
  <summary>Details</summary>
Motivation: 先进心脏AI在日常监测中受限于医疗数据和计算资源需求，低成本心脏智能系统虽然使用可穿戴设备数据，但与高成本系统存在显著性能差距。

Method: 提出LiteHeart框架，包含区域感知蒸馏模块（模拟医生关注关键ECG区域）和跨层互信息模块（对齐决策过程），采用半监督训练策略。

Result: 在覆盖38种心血管疾病的5个数据集上评估，LiteHeart将低成本与高成本系统的性能差距显著缩小，宏F1分数比现有方法提高4.27%-7.10%。

Conclusion: LiteHeart显著提升了低成本心脏智能系统的诊断能力，为使用可穿戴技术实现可扩展、经济实惠且准确的日常心脏保健铺平了道路。

Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac
monitoring is hindered by its reliance on extensive medical data and high
computational resources. Low-cost cardiac intelligence (LCCI) offers a
promising alternative by using wearable device data, such as 1-lead
electrocardiogram (ECG), but it suffers from a significant diagnostic
performance gap compared to high-cost cardiac intelligence (HCCI). To bridge
this gap, we propose LiteHeart, a semi-supervised knowledge distillation
framework. LiteHeart introduces a region-aware distillation module to mimic how
cardiologists focus on diagnostically relevant ECG regions and a cross-layer
mutual information module to align the decision processes of LCCI and HCCI
systems. Using a semi-supervised training strategy, LiteHeart further improves
model robustness under limited supervision. Evaluated on five datasets covering
over 38 cardiovascular diseases, LiteHeart substantially reduces the
performance gap between LCCI and HCCI, outperforming existing methods by 4.27%
to 7.10% in macro F1 score. These results demonstrate that LiteHeart
significantly enhances the diagnostic capabilities of low-cost cardiac
intelligence systems, paving the way for scalable, affordable, and accurate
daily cardiac healthcare using wearable technologies.

</details>


### [160] [Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring](https://arxiv.org/abs/2511.02853)
*Young-Seok Kweon,Gi-Hwan Shin,Ji-Yong Kim,Bokyeong Ryu,Seong-Whan Lee*

Main category: eess.SP

TL;DR: 提出了一种基于心电图的意识状态估计系统，使用解耦查询注意力变压器来捕捉心率变异性特征，在睡眠分期和麻醉监测任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统意识状态监测主要依赖脑电图，但存在对噪声敏感、需要受控环境等问题。心电图提供了非侵入性、可靠的替代方案，特别适合动态临床环境。

Method: 开发了consciousness-ECG transformer模型，采用解耦查询注意力机制来有效捕捉区分意识与无意识状态的心率变异性特征，并实现实时监测系统。

Result: 模型在睡眠分期和麻醉水平监测任务中分别达到0.877和0.880的准确率，AUC值分别为0.786和0.895，均优于基线模型。

Conclusion: 基于心电图的意识监测系统为脑电图方法提供了实用且鲁棒的替代方案，有望提升患者安全并增进对意识状态的理解。

Abstract: Conscious state estimation is important in various medical settings,
including sleep staging and anesthesia management, to ensure patient safety and
optimize health outcomes. Traditional methods predominantly utilize
electroencephalography (EEG), which faces challenges such as high sensitivity
to noise and the requirement for controlled environments. In this study, we
propose the consciousness-ECG transformer that leverages electrocardiography
(ECG) signals for non-invasive and reliable conscious state estimation. Our
approach employs a transformer with decoupled query attention to effectively
capture heart rate variability features that distinguish between conscious and
unconscious states. We implemented the conscious state estimation system with
real-time monitoring and validated our system on datasets involving sleep
staging and anesthesia level monitoring during surgeries. Experimental results
demonstrate that our model outperforms baseline models, achieving accuracies of
0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our
model achieves the highest area under curve values of 0.786 and 0.895 on sleep
staging and anesthesia level monitoring, respectively. The proposed system
offers a practical and robust alternative to EEG-based methods, particularly
suited for dynamic clinical environments. Our results highlight the potential
of ECG-based consciousness monitoring to enhance patient safety and advance our
understanding of conscious states.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [161] [Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies](https://arxiv.org/abs/2511.03173)
*Arsalan Muhammad,Wasiu Akande Ahmed,Omada Friday Ojonugwa,Paul Puspendu Biswas*

Main category: astro-ph.EP

TL;DR: 本文综述了地月空间活动中的轨道设计、导航和遥感技术进展，比较了四种主要转移策略，并探讨了人工智能、GNSS反射测量和先进PNT系统在地月空间探索中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统地月转移存在发射窗口固定、推进剂需求高的问题，而地球GNSS系统在地球同步轨道以外几乎没有覆盖，这限制了地月空间的自主性和环境感知能力。

Method: 通过评估速度需求、飞行时间和燃料效率来比较四种主要转移策略；利用卷积神经网络进行自动陨石坑识别和数字地形模型生成；使用深度强化学习实现下降和着陆过程中的自适应轨迹优化；研究GNSS反射测量和先进PNT架构扩展导航能力。

Result: 建立了可扩展的地月空间探索框架，支持可持续的地月探索和长期人类与机器人存在；AI技术能够降低风险和决策延迟；GNSS-R可作为双基地雷达用于月球冰、土壤特性和表面地形测绘；PNT系统支持自主交会、拉格朗日点站保持和协调卫星群操作。

Conclusion: 结合轨道设计、人工智能导航和遥感技术的发展，为可持续的地月空间探索和长期人类与机器人存在建立了可扩展的框架。

Abstract: The rapid growth of cislunar activities, including lunar landings, the Lunar
Gateway, and in-space refueling stations, requires advances in cost-efficient
trajectory design and reliable integration of navigation and remote sensing.
Traditional Earth-Moon transfers suffer from rigid launch windows and high
propellant demands, while Earth-based GNSS systems provide little to no
coverage beyond geostationary orbit. This limits autonomy and environmental
awareness in cislunar space. This review compares four major transfer
strategies by evaluating velocity requirements, flight durations, and fuel
efficiency, and by identifying their suitability for both crewed and robotic
missions. The emerging role of artificial intelligence and machine learning is
highlighted: convolutional neural networks support automated crater recognition
and digital terrain model generation, while deep reinforcement learning enables
adaptive trajectory refinement during descent and landing to reduce risk and
decision latency. The study also examines how GNSS-Reflectometry and advanced
Positioning, Navigation, and Timing architectures can extend navigation
capabilities beyond current limits. GNSS-R can act as a bistatic radar for
mapping lunar ice, soil properties, and surface topography, while PNT systems
support autonomous rendezvous, Lagrange point station-keeping, and coordinated
satellite swarm operations. Combining these developments establishes a scalable
framework for sustainable cislunar exploration and long-term human and robotic
presence.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [162] [Quantifying Weighted Morphological Content of Large-Scale Structures via Simulation-Based Inference](https://arxiv.org/abs/2511.03636)
*M. H. Jalali Kanafi,S. M. S. Movahed*

Main category: astro-ph.CO

TL;DR: 该研究比较了Minkowski泛函和条件导数矩在大尺度结构分析中的约束能力，发现CMD在约束宇宙学参数方面优于MFs，联合使用可进一步提升精度约27%。


<details>
  <summary>Details</summary>
Motivation: 比较大尺度结构的两种高阶统计量（Minkowski泛函和条件导数矩）在约束宇宙学参数方面的能力，特别关注它们对红移空间中非线性和各向异性特征的敏感性。

Method: 使用Big Sobol Sequence模拟的晕团目录，在红移z=0.5下，通过神经后验估计实现无似然推断框架，分析不同平滑尺度下的约束能力。

Result: 在平滑尺度R=15h^{-1}Mpc下，CMD对(Ω_m, σ_8)的约束精度比MFs各阶分量分别提高约(44%,52%)、(30%,45%)、(27%,17%)和(26%,17%)。MFs与CMD联合使用比单独使用MFs精度提升约27%。

Conclusion: CMD比MFs在约束宇宙学参数方面具有更强的能力，两者结合可捕获互补的各向异性敏感信息，且相对约束能力在不同参数值和平滑尺度下保持稳定。

Abstract: In this work, we perform a simulation-based forecasting analysis to compare
the constraining power of two higher-order summary statistics of the
large-scale structure (LSS), the Minkowski Functionals (MFs) and the
Conditional Moments of Derivative (CMD), with a particular focus on their
sensitivity to nonlinear and anisotropic features in redshift-space. Our
analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations
at redshift $z=0.5$, employing a likelihood-free inference framework
implemented via neural posterior estimation. At the fiducial cosmology of the
Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the
smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter
forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs
components, improving the constraint precision by ${\sim}(44\%,\,52\%)$,
${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$,
respectively. A joint configuration combining the MFs and CMD further enhances
the precision by approximately ${\sim}27\%$ compared to the standard MFs alone,
highlighting the complementary anisotropy-sensitive information captured by the
CMD in contrast to the scalar morphological content encapsulated by the MFs. We
further extend the forecasting analysis to a continuous range of cosmological
parameter values and multiple smoothing scales. Our results show that, although
the absolute forecast uncertainty for each component of summary statistics
depends on the underlying parameter values and the adopted smoothing scale, the
relative constraining power among the summary statistics remains nearly
constant throughout.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [163] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream是一种KV缓存压缩方法，可在保持模型精度的同时实现4倍内存使用改进，并在生产环境中成功部署。


<details>
  <summary>Details</summary>
Motivation: 随着100B+参数大语言模型和100k+上下文长度的普及，KV缓存对芯片内存需求激增，但现有压缩技术难以在工业部署框架中应用。

Method: 开发SnapStream KV缓存压缩方法，在静态图和连续批处理的生产环境中实现稀疏KV注意力技术。

Result: 在16路张量并行部署中实现4倍内存使用改进，在LongBench-v2、AIME24和LiveCodeBench上精度损失最小，吞吐量达1832 tokens/秒。

Conclusion: SnapStream是首个在生产推理系统中成功部署的稀疏KV注意力技术，解决了工业部署中的实际挑战。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [164] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建了首个基准测试来评估LLMs是否内化了描述现实世界人口的经验概率分布，发现LLMs在理解现实世界统计分布方面表现不佳，缺乏对观测分布的知识。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能够内化现实世界的概率分布知识，因为LLMs被宣传为强大的现实世界分布近似器，但统计学中的维度灾难理论对此提出了根本性挑战。

Method: 开发首个基准测试，直接测试LLMs是否能够获取描述现实世界人口的经验分布，涵盖经济、健康、教育和社会行为等领域。

Result: LLMs整体表现不佳，似乎无法自然内化现实世界统计数据。在Pearl因果层次框架下，语言模型缺乏观测分布知识。

Conclusion: LLMs在理解现实世界概率分布方面能力有限，根据因果层次定理，这意味着它们在干预性和反事实性知识方面也受到限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [165] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 提出一个多智能体AI框架来形式化工程设计过程，通过专业化的知识驱动智能体协作生成和优化设计候选方案，并以NACA翼型气动优化为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计方法通常需要多领域专业知识，导致复杂的协作和迭代优化过程，资源密集且效率低下。

Method: 构建包含三个核心AI智能体的框架：图本体学家使用LLM构建领域知识图谱；系统工程师根据管理者需求制定技术要求；设计工程师利用知识图谱和计算工具提出候选方案，形成迭代反馈循环。

Result: 该框架成功应用于4位数NACA翼型的气动优化，能够有效生成和优化满足技术要求的翼型设计。

Conclusion: 研究表明，配备结构化知识表示的协作AI智能体能够显著提高工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [166] [CLAX: Fast and Flexible Neural Click Models in JAX](https://arxiv.org/abs/2511.03620)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: CLAX是一个基于JAX的库，实现了使用现代梯度优化的经典点击模型，比传统EM方法快几个数量级，支持十种经典点击模型。


<details>
  <summary>Details</summary>
Motivation: 解决复杂概率图模型点击模型未系统采用梯度优化的问题，让从业者能在保持经典模型可解释性的同时利用现代深度学习框架。

Method: 用数值稳定的直接梯度优化替代EM优化，模块化设计允许集成嵌入、深度网络等组件到经典点击模型中，实现端到端优化。

Result: 在包含超过十亿用户会话的Baidu-ULTR数据集上，单GPU约2小时完成实验，比传统EM方法快几个数量级。

Conclusion: CLAX填补了梯度优化在经典点击模型中的应用空白，为行业从业者和研究人员提供了高效、可扩展的点击模型实现框架。

Abstract: CLAX is a JAX-based library that implements classic click models using modern
gradient-based optimization. While neural click models have emerged over the
past decade, complex click models based on probabilistic graphical models
(PGMs) have not systematically adopted gradient-based optimization, preventing
practitioners from leveraging modern deep learning frameworks while preserving
the interpretability of classic models. CLAX addresses this gap by replacing
EM-based optimization with direct gradient-based optimization in a numerically
stable manner. The framework's modular design enables the integration of any
component, from embeddings and deep networks to custom modules, into classic
click models for end-to-end optimization. We demonstrate CLAX's efficiency by
running experiments on the full Baidu-ULTR dataset comprising over a billion
user sessions in $\approx$ 2 hours on a single GPU, orders of magnitude faster
than traditional EM approaches. CLAX implements ten classic click models,
serving both industry practitioners seeking to understand user behavior and
improve ranking performance at scale and researchers developing new click
models. CLAX is available at: https://github.com/philipphager/clax

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [167] [Quantifying Articulatory Coordination as a Biomarker for Schizophrenia](https://arxiv.org/abs/2511.03084)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 提出了一种可解释的框架，利用发音语音特征通过特征谱差异图和加权指数衰减和来量化声带协调性，为精神分裂症提供透明、严重程度敏感的语音生物标志物。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗诊断中能力提升，但可解释性有限阻碍了临床采用。精神分裂症作为复杂疾病，需要能够捕捉症状严重程度并提供临床意义洞察的工具，而不仅仅是二元诊断。

Method: 使用特征谱差异图来区分复杂与简单协调模式，并通过加权指数衰减和来量化声带协调性。该方法能够可靠地区分不同协调模式组别。

Result: 特征谱图有效区分了复杂与简单协调模式，WSED分数可靠地分离了这些组别。WSED分数不仅与总体BPRS严重程度相关，还与阳性和阴性症状的平衡相关，反映了阳性症状患者更复杂的协调模式和阴性症状患者的相反趋势。

Conclusion: 该方法为精神分裂症提供了一个透明、严重程度敏感的语音生物标志物，推进了临床可解释的基于语音的评估工具的潜力。

Abstract: Advances in artificial intelligence (AI) and deep learning have improved
diagnostic capabilities in healthcare, yet limited interpretability continues
to hinder clinical adoption. Schizophrenia, a complex disorder with diverse
symptoms including disorganized speech and social withdrawal, demands tools
that capture symptom severity and provide clinically meaningful insights beyond
binary diagnosis. Here, we present an interpretable framework that leverages
articulatory speech features through eigenspectra difference plots and a
weighted sum with exponential decay (WSED) to quantify vocal tract
coordination. Eigenspectra plots effectively distinguished complex from simpler
coordination patterns, and WSED scores reliably separated these groups, with
ambiguity confined to a narrow range near zero. Importantly, WSED scores
correlated not only with overall BPRS severity but also with the balance
between positive and negative symptoms, reflecting more complex coordination in
subjects with pronounced positive symptoms and the opposite trend for stronger
negative symptoms. This approach offers a transparent, severity-sensitive
biomarker for schizophrenia, advancing the potential for clinically
interpretable speech-based assessment tools.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [168] [Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia](https://arxiv.org/abs/2511.02722)
*Subati Abulikemu,Puria Radmard,Michail Mamalakis,John Suckling*

Main category: q-bio.NC

TL;DR: 该研究发现精神分裂症患者的感觉-联想（AS）层级被压缩，表现为功能分化减少、神经时间尺度变平和神经网络固定点稳定性降低。


<details>
  <summary>Details</summary>
Motivation: 研究精神分裂症患者大脑功能层级组织的改变，探索其与神经计算稳定性之间的关系。

Method: 使用fMRI数据提取个体AS梯度，通过Ornstein-Uhlenbeck过程建模神经时间尺度，并利用梯度正则化的循环神经网络模拟工作记忆任务。

Result: 精神分裂症患者AS层级压缩，梯度极值区域的时间常数效应减弱，梯度范围更大的网络学习效率更高且神经状态更稳定。

Conclusion: 梯度去分化可能通过降低神经固定点稳定性来破坏精神分裂症中的神经计算，这一机制得到了经验时间尺度平坦化和模型证据的收敛支持。

Abstract: The human neocortex is functionally organised at its highest level along a
continuous sensory-to-association (AS) hierarchy. This study characterises the
AS hierarchy of patients with schizophrenia in a comparison with controls.
Using a large fMRI dataset (N=355), we extracted individual AS gradients via
spectral analysis of brain connectivity, quantified hierarchical specialisation
by gradient spread, and related this spread with connectivity geometry. We
found that schizophrenia compresses the AS hierarchy indicating reduced
functional differentiation. By modelling neural timescale with the
Ornstein-Uhlenbeck process, we observed that the most specialised, locally
cohesive regions at the gradient extremes exhibit dynamics with a longer time
constant, an effect that is attenuated in schizophrenia. To study computation,
we used the gradients to regularise subject-specific recurrent neural networks
(RNNs) trained on working memory tasks. Networks endowed with greater gradient
spread learned more efficiently, plateaued at lower task loss, and maintained
stronger alignment to the prescribed AS hierarchical geometry. Fixed point
linearisation showed that high-range networks settled into more stable neural
states during memory delay, evidenced by lower energy and smaller maximal
Jacobian eigenvalues. This gradient-regularised RNN framework therefore links
large-scale cortical architecture with fixed point stability, providing a
mechanistic account of how gradient de-differentiation could destabilise neural
computations in schizophrenia, convergently supported by empirical timescale
flattening and model-based evidence of less stable fixed points.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [169] [Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond](https://arxiv.org/abs/2511.03434)
*Botao 'Amber' Hu,Helena Rong*

Main category: cs.HC

TL;DR: 本文比较分析了AI代理间协议的信任模型，包括Brief、Claim、Proof、Stake、Reputation和Constraint六种机制，指出单一机制不足，建议采用以Proof和Stake为核心的混合信任架构。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理经济的兴起，信任从人工监督转向协议设计，但现有协议（如A2A、AP2、ERC-8004）的信任假设缺乏系统分析，特别是LLM特有的脆弱性使纯声誉或声明方法变得脆弱。

Method: 对六种信任模型进行对比研究：分析假设、攻击面和设计权衡，特别关注LLM特有的脆弱性（提示注入、从众性、幻觉等），并在安全、隐私、延迟/成本和社会鲁棒性指标下评估现有协议。

Result: 研究发现没有单一信任机制足够，需要混合方法。Proof和Stake应作为高风险操作的基础，Brief用于身份发现，Reputation提供灵活性和社交信号。

Conclusion: 提出了混合信任模型建议，以减轻声誉操纵和LLM误行为，提炼了更安全、可互操作和可扩展的代理经济的设计指南。

Abstract: As the "agentic web" takes shape-billions of AI agents (often LLM-powered)
autonomously transacting and collaborating-trust shifts from human oversight to
protocol design. In 2025, several inter-agent protocols crystallized this
shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),
and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust
assumptions remain under-examined. This paper presents a comparative study of
trust models in inter-agent protocol design: Brief (self- or third-party
verifiable claims), Claim (self-proclaimed capabilities and identity, e.g.
AgentCard), Proof (cryptographic verification, including zero-knowledge proofs
and trusted execution environment attestations), Stake (bonded collateral with
slashing and insurance), Reputation (crowd feedback and graph-based trust
signals), and Constraint (sandboxing and capability bounding). For each, we
analyze assumptions, attack surfaces, and design trade-offs, with particular
emphasis on LLM-specific fragilities-prompt injection,
sycophancy/nudge-susceptibility, hallucination, deception, and
misalignment-that render purely reputational or claim-only approaches brittle.
Our findings indicate no single mechanism suffices. We argue for
trustless-by-default architectures anchored in Proof and Stake to gate
high-impact actions, augmented by Brief for identity and discovery and
Reputation overlays for flexibility and social signals. We comparatively
evaluate A2A, AP2, ERC-8004 and related historical variations in academic
research under metrics spanning security, privacy, latency/cost, and social
robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid
trust model recommendations that mitigate reputation gaming and misinformed LLM
behavior, and we distill actionable design guidelines for safer, interoperable,
and scalable agent economies.

</details>


### [170] [A Survey of Driver Distraction and Inattention in Popular Commercial Software-Defined Vehicles](https://arxiv.org/abs/2511.02891)
*Lingyu Zhao,Yuankai He*

Main category: cs.HC

TL;DR: 本文研究了软件定义车辆中用户界面设计对驾驶员分心和注意力分散的影响，通过调查商业车辆识别增加认知负荷的UI特征，并提出减轻风险的设计策略。


<details>
  <summary>Details</summary>
Motivation: 在软件定义车辆时代，90%以上的分心驾驶事故与UI控制相关而非手机使用，但现有SDV实现大多未考虑驾驶分心和注意力分散问题，这体现在许多商业车辆中。

Method: 通过对流行商业车辆进行调研，识别可能增加认知负荷的UI特征，并评估减轻这些风险的设计策略。

Result: 调研发现需要平衡先进软件功能与驾驶员认知工效学的UI设计，UI设计对驾驶安全有重要影响。

Conclusion: 研究结果为研究人员和原始设备制造商提供了有价值的指导，有助于汽车UI领域的发展，促进软件中心汽车时代车辆安全的提升。

Abstract: As the automotive industry embraces software-defined vehicles (SDVs), the
role of user interface (UI) design in ensuring driver safety has become
increasingly significant. In crashes related to distracted driving, over 90%
did not involve cellphone use but were related to UI controls. However, many of
the existing UI SDV implementations do not consider Drive Distraction and
Inattention (DDI), which is reflected in many popular commercial vehicles. This
paper investigates the impact of UI designs on driver distraction and
inattention within the context of SDVs. Through a survey of popular commercial
vehicles, we identify UI features that potentially increase cognitive load and
evaluate design strategies to mitigate these risks. This survey highlights the
need for UI designs that balance advanced software functionalities with
driver-cognitive ergonomics. Findings aim to provide valuable guidance to
researchers and OEMs to contribute to the field of automotive UI, contributing
to the broader discussion on enhancing vehicular safety in the software-centric
automotive era.

</details>


### [171] [From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents](https://arxiv.org/abs/2511.03143)
*Erfan Shayegani,Jina Suh,Andy Wilson,Nagu Rangan,Javier Hernandez*

Main category: cs.HC

TL;DR: 提出了一个开发上下文特定共情大语言模型的框架，通过分析真实对话数据、生成合成对话和训练专家适配器，显著缩小了感知与期望共情之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型的共情表达通常是通用的，而非针对特定任务和上下文定制的，这导致了用户期望与实际体验之间的共情差距。

Method: 分析真实多轮对话数据集，开发合成多轮对话生成流程，基于上下文定义共情模式，训练上下文特定的共情专家适配器。

Result: 感知与期望共情之间的差距显著减少了72.66%，共情得分平均提高了2.43倍，专家适配器在保持共情模式方面优于系统提示。

Conclusion: 上下文特定的共情专家适配器能有效提升对话AI的共情能力，特别是在长对话中保持共情一致性方面表现优异。

Abstract: Empathy is a critical factor in fostering positive user experiences in
conversational AI. While models can display empathy, it is often generic rather
than tailored to specific tasks and contexts. In this work, we introduce a
novel framework for developing and evaluating context-specific empathetic large
language models (LLMs). We first analyze a real-world conversational dataset
consisting of 672 multi-turn conversations across 8 tasks, revealing
significant differences in terms of expected and experienced empathy before and
after the conversations, respectively. To help minimize this gap, we develop a
synthetic multi-turn conversational generation pipeline and steer responses
toward our defined empathy patterns based on the context that more closely
matches users' expectations. We then train empathetic expert adapters for
context-specific empathy that specialize in varying empathy levels based on the
recognized task. Our empirical results demonstrate a significant gap reduction
of 72.66% between perceived and desired empathy with scores increasing by an
average factor of 2.43 as measured by our metrics and reward models.
Additionally, our trained empathetic expert adapters demonstrate superior
effectiveness in preserving empathy patterns throughout conversation turns,
outperforming system prompts, which tend to dramatically diminish in impact as
conversations lengthen.

</details>


### [172] [When Generative Artificial Intelligence meets Extended Reality: A Systematic Review](https://arxiv.org/abs/2511.03282)
*Xinyu Ning,Yan Zhuo,Xian Wang,Chan-In Devin Sio,Lik-Hang Lee*

Main category: cs.HC

TL;DR: 本文系统综述了2023-2025年间生成式AI在XR领域的应用，通过PRISMA筛选分析了26篇文献，总结了应用领域和关键技术实现，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，生成式AI与XR结合展现出巨大潜力，本文旨在系统梳理这一新兴交叉领域的研究现状和发展趋势。

Method: 采用PRISMA系统筛选方法，对2023-2025年相关文献进行系统性综述，最终分析26篇核心论文。

Result: 总结了生成式AI在XR中的主要应用领域和关键技术实现，识别了当前研究趋势和空白领域。

Conclusion: 生成式AI与XR的结合具有广阔前景，本文为未来生成式XR研究提供了指导方向和参考信息。

Abstract: With the continuous advancement of technology, the application of generative
artificial intelligence (AI) in various fields is gradually demonstrating great
potential, particularly when combined with Extended Reality (XR), creating
unprecedented possibilities. This survey article systematically reviews the
applications of generative AI in XR, covering as much relevant literature as
possible from 2023 to 2025. The application areas of generative AI in XR and
its key technology implementations are summarised through PRISMA screening and
analysis of the final 26 articles. The survey highlights existing articles from
the last three years related to how XR utilises generative AI, providing
insights into current trends and research gaps. We also explore potential
opportunities for future research to further empower XR through generative AI,
providing guidance and information for future generative XR research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [173] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian García-Romero,Miquel Esplà-Gomis,Felipe Sánchez-Martínez*

Main category: cs.CL

TL;DR: 提出一种利用多语言机器翻译模型的内部表示来区分人工翻译和机器翻译句子的新方法，在非英语语言对上比现有技术准确率提升至少5个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译系统依赖的大规模平行语料库中很大部分是机器生成的翻译，过度依赖这些合成内容会显著降低翻译质量，因此需要过滤非人工翻译作为预处理步骤。

Method: 利用代理多语言机器翻译模型的内部表示来直接区分人工翻译和机器翻译句子。

Result: 实验结果显示该方法优于当前最先进技术，特别是在非英语语言对上，准确率提升至少5个百分点。

Conclusion: 该方法能有效识别机器翻译内容，为构建高质量机器翻译系统提供了重要的预处理工具。

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [174] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 本文提出了一种新的ABSA评估方法FTS-OBP，研究了小型解码器生成语言模型在低资源教育评论ABSA任务中的表现，并发布了首个教育评论ABSA公共资源集。


<details>
  <summary>Details</summary>
Motivation: ABSA研究和资源主要集中在商业领域，而在教育和医疗等高需求低资源领域存在分析需求未满足的问题。传统评估方法过于严格，无法准确反映生成模型的性能。

Method: 1) 提出FTS-OBP评估方法，支持灵活的边界变化；2) 研究小型解码器生成语言模型，探索数据无关和数据轻量微调方法；3) 提出多任务微调策略，显著提升小模型性能。

Result: 1.5-3.8B参数的小模型在仅使用200-1000个示例的情况下，在单个GPU上超越了专有大型模型，并接近基准结果。

Conclusion: 该方法成功解决了低资源领域ABSA的挑战，为教育和医疗等领域的细粒度意见挖掘提供了可行解决方案。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [175] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 该论文提出并研究了单边对话问题（1SC），即在只能记录对话一方的场景下推断和学习对话内容，包括实时重建缺失对话和生成摘要两个任务。


<details>
  <summary>Details</summary>
Motivation: 在现实场景如远程医疗、呼叫中心和智能眼镜中，通常只能记录对话的一方，这限制了对话AI的应用。论文旨在解决这种单边对话的挑战。

Method: 在MultiWOZ、DailyDialog和Candor数据集上评估提示和微调模型，使用人类A/B测试和LLM作为评判指标。关键方法包括访问未来一轮对话信息、话语长度信息以及占位符提示来减少幻觉。

Result: 研究发现：访问未来一轮对话和话语长度信息能改善重建效果；占位符提示有助于减少幻觉；大模型通过提示能生成有前景的重建结果，而小模型需要微调；无需重建缺失对话也能生成高质量摘要。

Conclusion: 1SC是一个新颖的挑战，论文展示了有前景的结果，标志着向隐私感知对话AI迈出了一步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [176] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: PolyNorm是一种基于提示的大语言模型文本归一化方法，通过自动数据管理和评估管道，在八种语言上相比生产级系统显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 传统文本归一化系统虽然准确率高，但需要大量工程工作、难以扩展、语言覆盖有限，特别是在低资源环境下。

Method: 使用基于提示的大语言模型方法，结合语言无关的自动数据管理和评估管道，减少对手动规则工程的依赖。

Result: 在八种语言的实验中，相比生产级系统持续降低了词错误率，并发布了多语言数据集PolyNorm-Benchmark。

Conclusion: PolyNorm方法能够减少手动工程工作，实现更广泛的语言适用性，为多语言文本归一化研究提供了有效解决方案。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [177] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 评估多模态大语言模型在临床任务中的"思考模式"表现，发现相比标准模式提升有限，在复杂医疗任务中表现仍不理想


<details>
  <summary>Details</summary>
Motivation: 随着"双状态"多模态大语言模型的出现，需要严格评估其增强的推理过程对临床任务性能和可靠性的影响

Method: 使用VQA-RAD和ROCOv2数据集，评估Seed1.5-VL和Gemini-2.5-Flash两个领先MLLM在四种视觉医疗任务上的"思考模式"能力

Result: 激活思考模式相比标准非思考模式的改进在大多数任务中边际有限，在开放式VQA和医学图像解读等复杂医疗任务中表现仍不理想

Conclusion: 需要领域特定的医疗数据和更先进的医学知识整合方法

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [178] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: BanglaSTEM是一个包含5000个精心挑选的孟加拉语-英语STEM领域句子对的数据集，用于训练专门处理技术术语的翻译模型，显著提高了技术内容的翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语技术问题解决上表现良好，但在孟加拉语问题上表现不佳。现有孟加拉语-英语翻译系统在处理技术术语时存在困难，经常错误翻译专业词汇，导致问题含义改变和错误答案。

Method: 构建包含5000个孟加拉语-英语STEM句子对的BanglaSTEM数据集，使用语言模型生成12000多个翻译，通过人工评估筛选出最高质量的对。基于该数据集训练T5翻译模型，并在代码生成和数学问题解决两个任务上进行测试。

Result: 结果显示技术内容的翻译准确性显著提高，使孟加拉语使用者能够更有效地使用英语导向的语言模型。

Conclusion: BanglaSTEM数据集和训练好的翻译模型已公开发布，为孟加拉语技术内容翻译提供了有效解决方案。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [179] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 提出IRIS框架，通过隐式理由（文本序列）和显式理由（语言特征）实现可解释的零样本立场检测，将立场检测建模为信息检索排序任务，无需真实理由标注即可提供内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本立场检测方法存在泛化性问题、文本与目标间缺乏连贯性、过度依赖显式推理、解释粗糙且缺乏细微差别、未明确建模推理过程导致预测难以解释等问题。

Method: IRIS框架结合隐式理由（基于文本序列）和显式理由（基于语言特征），将立场检测视为信息检索排序任务，理解不同立场隐式理由的相关性，无需真实理由标注即可提供可解释性。

Result: 在VAST、EZ-STANCE、P-Stance和RFD基准数据集上，使用50%、30%甚至10%训练数据的广泛实验证明了模型具有良好的泛化能力。

Conclusion: IRIS框架通过提出的架构和可解释设计，在零样本立场检测中实现了优异的性能和内在可解释性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [180] [The Contiguous Art Gallery Problem is in Θ(n log n)](https://arxiv.org/abs/2511.02960)
*Sarita de Berg,Jacobus Conradi,Ivor van der Hoog,Eva Rotenberg*

Main category: cs.CG

TL;DR: 本文提出了一个O(n log n)时间算法来解决连续艺术画廊问题，相比之前O(k n^5 log n)的算法实现了显著加速，并证明该问题的时间复杂度为Θ(n log n)。


<details>
  <summary>Details</summary>
Motivation: 之前三个独立工作都得到了O(k n^5 log n)的算法复杂度，暗示这可能是问题的固有复杂度。本文旨在证明这种复杂度并非最优，可以大幅改进。

Method: 在计算几何常用的实RAM模型下，设计了一个新的O(n log n)时间算法，并通过从集合交集问题归约给出了基于排序的下界。

Result: 实现了O(k n^4)倍的加速，将算法复杂度从O(k n^5 log n)降低到O(n log n)，并证明这是最优复杂度。

Conclusion: 连续艺术画廊问题的时间复杂度为Θ(n log n)，远优于之前认为的O(k n^5 log n)复杂度。

Abstract: Recently, a natural variant of the Art Gallery problem, known as the
\emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$,
the goal is to partition its boundary $\partial P$ into the smallest number of
contiguous segments such that each segment is completely visible from some
point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this
variant is polynomial-time solvable. At SoCG~2025, three independent works
presented algorithms for this problem, each achieving a running time of $O(k
n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution.
Interestingly, these results were obtained using entirely different approaches,
yet all led to roughly the same asymptotic complexity, suggesting that such a
running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model
in computational geometry, we present an $O(n \log n)$-time algorithm,
achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We
also give a straightforward sorting-based lower bound by reducing from the set
intersection problem. We thus show that the Contiguous Art Gallery problem is
in $\Theta(n \log n)$.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [181] [Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments](https://arxiv.org/abs/2511.03632)
*Cemil Vahapoglu,Timothy J. O'Shea,Wan Liu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出了一种基于多普勒感知的稀疏神经网络波束成形模型，通过通道自适应稀疏注意力机制在MU-SIMO系统中提升高移动性场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统波束成形技术（如ZFBF和MMSE）在恶劣信道条件下性能下降，而现有Transformer模型在大型OFDM网格中面临二次注意力复杂度问题，且稀疏注意力模式未考虑无线通信中的信道动态特性。

Method: 设计Doppler-aware Sparse NNBF模型，采用通道自适应稀疏注意力机制，在2D时频轴上根据信道动态配置稀疏结构，理论上保证在p个注意力头内实现全连接。

Result: 在UMa信道条件下的仿真结果显示，该模型在高移动性场景中显著优于标准稀疏NNBF基准以及传统ZFBF和MMSE波束成形技术，同时保持结构化稀疏性和可控的每个查询关注键数量。

Conclusion: 所提出的多普勒感知稀疏神经网络波束成形模型有效解决了高移动性场景中的性能问题，通过通道自适应稀疏注意力机制在保持计算效率的同时提升了波束成形性能。

Abstract: Beamforming has significance for enhancing spectral efficiency and mitigating
interference in multi-antenna wireless systems, facilitating spatial
multiplexing and diversity in dense and high mobility scenarios. Traditional
beamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean
square error (MMSE) beamforming experience performance deterioration under
adverse channel conditions. Deep learning-based beamforming offers an
alternative with nonlinear mappings from channel state information (CSI) to
beamforming weights by improving robustness against dynamic channel
environments. Transformer-based models are particularly effective due to their
ability to model long-range dependencies across time and frequency. However,
their quadratic attention complexity limits scalability in large OFDM grids.
Recent studies address this issue through sparse attention mechanisms that
reduce complexity while maintaining expressiveness, yet often employ patterns
that disregard channel dynamics, as they are not specifically designed for
wireless communication scenarios. In this work, we propose a Doppler-aware
Sparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that
incorporates a channel-adaptive sparse attention mechanism in a multi-user
single-input multiple-output (MU-SIMO) setting. The proposed sparsity structure
is configurable along 2D time-frequency axes based on channel dynamics and is
theoretically proven to ensure full connectivity within p hops, where p is the
number of attention heads. Simulation results under urban macro (UMa) channel
conditions show that Doppler-aware Sparse NNBF significantly outperforms both a
fixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional
beamforming techniques ZFBF and MMSE beamforming in high mobility scenarios,
while maintaining structured sparsity with a controlled number of attended keys
per query.

</details>
