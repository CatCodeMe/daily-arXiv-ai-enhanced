<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 10]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 6]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 12]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 本文提出并形式化了演化知识图的压缩表示方法，以支持数据管理和分析。


<details>
  <summary>Details</summary>
Motivation: 研究演化现象有助于理解实体间随时间变化的关系并预测未来趋势，而知识图能有效建模多源异构数据。

Method: 提出并形式化了演化知识图的压缩表示方法。

Result: 该方法有助于组织和管理不断演化的数据，使其更易于访问和分析。

Conclusion: 演化知识图的压缩表示为知识图管理系统提供了有效的数据组织和利用方式。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA是一种用于高性能计算（HPC）系统网络故障诊断的新框架，通过多模态数据和图分析方法实现高精度故障定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据异构性和准确性不足，无法直接适用于HPC系统的网络故障诊断。

Method: ClusterRCA结合分类器和图方法，从拓扑连接的NIC对中提取特征，构建故障图并进行随机游走以定位根因。

Result: 实验表明，ClusterRCA在HPC系统中诊断网络故障的准确性高，且在不同应用场景下表现稳健。

Conclusion: ClusterRCA为HPC系统提供了一种有效的网络故障诊断解决方案。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [3] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 论文提出了一种分布式数据分析框架，用于高效处理大规模GPU性能日志，解决了现有工具内存占用高和运行时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具处理大规模GPU性能日志时内存需求高、运行时间长，无法满足自动化工作流的需求。

Method: 通过将数据集分区为可独立分析的片段，并利用MPI并行处理，减少单节点内存压力，避免中心瓶颈。

Result: 框架成功应用于真实HPC和AI工作负载的Nsight Compute日志，能够诊断性能变异性并揭示内存传输延迟对GPU内核行为的影响。

Conclusion: 该分布式框架显著提升了大规模性能日志分析的效率和可扩展性。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [4] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: GPU内存带宽是低延迟大语言模型（LLM）推理的主要瓶颈。推测解码利用空闲GPU计算，通过轻量级草案生成K个令牌，LLM并行验证以提高令牌吞吐量。然而，对于混合专家（MoE）模型，推测解码因激活更多权重而增加数据移动和验证时间，导致性能下降。Cascade框架通过动态调整K值选择性启用推测解码，避免性能下降并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型中推测解码因权重激活增加而导致的性能下降问题，使其在实际应用中可行。

Method: 提出Cascade框架，通过轻量级指标“推测效用”动态调整K值，选择性启用推测解码以避免性能下降。

Result: Cascade在vLLM中实现，评估显示其将性能下降限制在5%（相比1.5倍），吞吐量提升7-14%。

Conclusion: Cascade使推测解码在MoE模型中变得实用，通过动态调整K值避免性能下降并提升效率。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [5] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: ParEval-Repo是一个用于评估LLM在跨GPGPU执行模型中自动翻译代码库效果的基准测试框架，结果表明LLM在小程序上可行，但在大规模代码库中面临挑战。


<details>
  <summary>Details</summary>
Motivation: GPGPU架构多样化导致编程模型和软件栈复杂化，开发者需要大量努力进行移植和优化，LLM有望减轻这一负担。

Method: 提出ParEval-Repo框架，包含多种编程模型的科学计算和AI小应用，评估开源和商业LLM在代码翻译中的表现。

Result: LLM在小程序翻译中可行，但在生成功能性构建系统和跨文件依赖时面临困难，难以扩展到大规模代码库。

Conclusion: LLM在科学应用代码翻译中具有潜力，但需进一步解决构建系统和跨文件依赖问题以支持更大规模代码库。

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [6] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 论文探讨了如何利用DaCe框架自动生成高性能代码，以应对HPC系统中硬件多样性的挑战，并通过CFD中的计算内核验证了其跨平台性能。


<details>
  <summary>Details</summary>
Motivation: 随着HPC加速器（如Nvidia和AMD GPU）的多样化，开发者为不同硬件架构编写特定代码的负担加重，影响了科学应用的可持续性。

Method: 使用DaCe框架的Stateful Dataflow Multigraph（SDFG）表示法自动生成高性能代码，并将其集成到Fortran求解器Neko中。

Result: 生成的代码在Nvidia GH200、A100和AMD MI250X GPU上表现出良好的可移植性和性能。

Conclusion: 自动代码生成是确保大规模科学应用长期可持续性的可行解决方案。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


### [7] [BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services](https://arxiv.org/abs/2506.21033)
*Zhaojiacheng Zhou,Hongze Liu,Shijing Yuan,Hanning Zhang,Jiong Lou,Chentao Wu,Jie Li*

Main category: cs.DC

TL;DR: 提出了一种基于区块链的外部知识框架，解决LLMs幻觉问题，通过协调多个知识孤岛提供可靠知识，同时确保数据安全。


<details>
  <summary>Details</summary>
Motivation: LLMs的幻觉问题日益突出，外部知识增强是解决方案，但隐私和安全问题导致知识分散且难以获取。

Method: 设计区块链框架，从本地数据提炼知识并记录在区块链上，引入声誉机制和交叉验证确保知识质量，提供API接口供LLMs检索。

Result: 实验表明，该框架在区块链环境中实现了高效的LLM服务知识共享。

Conclusion: 提出的框架有效解决了知识孤岛问题，为LLMs提供了可靠的外部知识支持。

Abstract: The hallucination problem of Large Language Models (LLMs) has increasingly
drawn attention. Augmenting LLMs with external knowledge is a promising
solution to address this issue. However, due to privacy and security concerns,
a vast amount of downstream task-related knowledge remains dispersed and
isolated across various "silos," making it difficult to access. To bridge this
knowledge gap, we propose a blockchain-based external knowledge framework that
coordinates multiple knowledge silos to provide reliable foundational knowledge
for large model retrieval while ensuring data security. Technically, we distill
knowledge from local data into prompts and execute transactions and records on
the blockchain. Additionally, we introduce a reputation mechanism and
cross-validation to ensure knowledge quality and provide incentives for
participation. Furthermore, we design a query generation framework that
provides a direct API interface for large model retrieval. To evaluate the
performance of our proposed framework, we conducted extensive experiments on
various knowledge sources. The results demonstrate that the proposed framework
achieves efficient LLM service knowledge sharing in blockchain environments.

</details>


### [8] [Bridding OT and PaaS in Edge-to-Cloud Continuum](https://arxiv.org/abs/2506.21072)
*Carlos J Barrios,Yves Denneulin*

Main category: cs.DC

TL;DR: OTPaaS框架优化数据管理，提升响应速度、安全性和能源效率，支持工业转型。


<details>
  <summary>Details</summary>
Motivation: 为工业转型提供高效、安全的数据管理平台，确保数据主权和技术可靠性。

Method: 采用PaaS模型，支持边缘和云环境，实现灵活应用管理和集成。

Result: 成功部署并解决关键挑战，适用于多种用例。

Conclusion: OTPaaS是工业数据管理的有效解决方案，兼具灵活性和可靠性。

Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides
a structured framework for the efficient management and storage of data. It
ensures excellent response times while improving security, reliability, data
and technology sovereignty, robustness, and energy efficiency, which are
crucial for industrial transformation and data sovereignty. This paper
illustrates successful deployment, adaptable application management, and
various integration components catering to Edge and Cloud environments. It
leverages the advantages of the Platform as a Service model and highlights key
challenges that have been addressed for specific use cases.

</details>


### [9] [Enabling Bitcoin Smart Contracts on the Internet Computer](https://arxiv.org/abs/2506.21327)
*Ryan Croote,Islam El-Ashi,Thomas Locher,Yvonne-Anne Pignolet*

Main category: cs.DC

TL;DR: 提出了一种在Internet Computer（IC）上执行图灵完备比特币智能合约的架构，避免了传统桥接机制的安全风险，并展示了其高效性和低成本。


<details>
  <summary>Details</summary>
Motivation: 解决比特币程序化访问的局限性，避免桥接机制的安全风险，实现更复杂的去中心化应用。

Method: 通过IC与比特币节点直接交互，无需桥接，结合比特币的概率性与IC的不可逆性。

Result: 实测显示，该架构能在几秒内完成最终确认，执行成本低，支持复杂应用。

Conclusion: 该架构为比特币智能合约提供了安全高效的解决方案，具有实际应用潜力。

Abstract: There is growing interest in providing programmatic access to the value
locked in Bitcoin, which famously offers limited programmability itself.
Various approaches have been put forth in recent years, with the vast majority
of proposed mechanisms either building new functionality on top of Bitcoin or
leveraging a bridging mechanism to enable smart contracts that make use of
``wrapped'' bitcoins on entirely different platforms.
  In this work, an architecture is presented that follows a different approach.
The architecture enables the execution of Turing-complete Bitcoin smart
contracts on the Internet Computer (IC), a blockchain platform for hosting and
executing decentralized applications. Instead of using a bridge, IC and Bitcoin
nodes interact directly, eliminating potential security risks that the use of a
bridge entails. This integration requires novel concepts, in particular to
reconcile the probabilistic nature of Bitcoin with the irreversibility of
finalized state changes on the IC, which may be of independent interest.
  In addition to the presentation of the architecture, we provide evaluation
results based on measurements of the Bitcoin integration running on mainnet.
The evaluation results demonstrate that, with finalization in a few seconds and
low execution costs, this integration enables complex Bitcoin-based
decentralized applications that were not practically feasible or economically
viable before.

</details>


### [10] [Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget](https://arxiv.org/abs/2506.21422)
*Kevin Kreutz,Philipp Wiesner,Monica Vitali*

Main category: cs.DC

TL;DR: 提出了一种基于每小时碳预算的微服务碳感知方法，通过选择最佳版本和水平扩展，优化用户体验和收入。


<details>
  <summary>Details</summary>
Motivation: 数据中心的碳足迹问题日益突出，现有策略无法适用于服务导向的云应用。

Method: 选择最适合的微服务版本和水平扩展，以在碳预算内最大化用户体验和收入。

Result: 实验表明，该方法能适应不同工作负载和碳强度变化。

Conclusion: 该方法有效解决了服务导向云应用的碳感知问题。

Abstract: The carbon footprint of data centers has recently become a critical concern.
So far, most carbon-aware strategies have focused on leveraging the flexibility
of scheduling decisions for batch processing by shifting the time and location
of workload executions. However, such approaches cannot be applied to
service-oriented cloud applications, since they have to be reachable at every
point in time and often at low latencies. We propose a carbon-aware approach
for operating microservices under hourly carbon budgets. By choosing the most
appropriate version and horizontal scaleout for each microservice, our strategy
maximizes user experience and revenue while staying within budget constraints.
Experiments across various application configurations and carbon budgets
demonstrate that the approach adapts properly to changing workloads and carbon
intensities.

</details>


### [11] [exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design](https://arxiv.org/abs/2506.21449)
*Maxim Moraru,Weiyi Xia,Zhuo Ye,Feng Zhang,Yongxin Yao,Ying Wai Li,Cai-Zhuang Wang*

Main category: cs.DC

TL;DR: exa-AMD是一个基于Python的工具，通过整合AI/ML、材料数据库和量子力学计算，加速功能材料的发现与设计。


<details>
  <summary>Details</summary>
Motivation: 加速功能材料的发现与设计过程，提高研究效率。

Method: 利用Parsl任务并行编程库，实现从笔记本电脑到超级计算机的灵活任务执行。

Result: 通过解耦工作流逻辑与执行配置，使研究人员无需为不同系统重新实现工作流即可扩展。

Conclusion: exa-AMD为功能材料研究提供了高效、灵活且可扩展的解决方案。

Abstract: exa-AMD is a Python-based application designed to accelerate the discovery
and design of functional materials by integrating AI/ML tools, materials
databases, and quantum mechanical calculations into scalable, high-performance
workflows. The execution model of exa-AMD relies on Parsl, a task-parallel
programming library that enables a flexible execution of tasks on any computing
resource from laptops to supercomputers. By using Parsl, exa-AMD is able to
decouple the workflow logic from execution configuration, thereby empowering
researchers to scale their workflows without having to reimplement them for
each system.

</details>


### [12] [Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces](https://arxiv.org/abs/2506.21467)
*Michael Johnston,Burkhard Ringlein,Christoph Hagleitner,Alessandro Pomponio,Vassilis Vassiliadis,Christian Pinto,Srikumar Venugopal*

Main category: cs.DC

TL;DR: 论文提出Discovery Space抽象，用于形式化描述工作负载配置问题，支持大规模搜索空间的结构化、鲁棒和分布式研究，并展示其通用性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在云资源部署中，如何以最低成本找到满足服务级别协议的最优配置是一个复杂问题，涉及大量参数和配置选项。

Method: 提出Discovery Space抽象，实现其具体化，并验证其在多种工作负载（如大语言模型推理和大数据分析）中的通用性。

Result: 方法支持优化器之间的安全、透明数据共享，提高搜索效率，并实现跨搜索空间的知识转移，配置搜索速度提升超过90%。

Conclusion: Discovery Space为大规模配置搜索提供了一种高效、通用的解决方案，显著提升了优化效率。

Abstract: Finding the optimal set of cloud resources to deploy a given workload at
minimal cost while meeting a defined service level agreement is an active area
of research. Combining tens of parameters applicable across a large selection
of compute, storage, and services offered by cloud providers with similar
numbers of application-specific parameters leads to configuration spaces with
millions of deployment options.
  In this paper, we propose Discovery Space, an abstraction that formalizes the
description of workload configuration problems, and exhibits a set of
characteristics required for structured, robust and distributed investigations
of large search spaces. We describe a concrete implementation of the Discovery
Space abstraction and show that it is generalizable across a diverse set of
workloads such as Large Language Model inference and Big Data Analytics.
  We demonstrate that our approach enables safe, transparent sharing of data
between executions of best-of-breed optimizers increasing the efficiency of
optimal configuration detection in large search spaces. We also demonstrate how
Discovery Spaces enable transfer and reuse of knowledge across similar search
spaces, enabling configuration search speed-ups of over 90%.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 提出了一种自适应混合排序范式，根据输入数据的实时模式动态选择最优排序算法（计数排序、基数排序或快速排序），显著提升了执行时间和效率。


<details>
  <summary>Details</summary>
Motivation: 现有排序算法无法在所有数据分布下都表现最优，因此需要一种能根据数据特征动态选择算法的解决方案。

Method: 通过特征提取模块计算数据参数（如数据量、值范围和熵），结合有限状态机和XGBoost分类器选择最优排序策略。

Result: 实验表明，该框架在合成和真实数据集上的执行时间、灵活性和效率均优于传统静态排序算法。

Conclusion: 该自适应排序框架具有可扩展性，适用于大数据分析、边缘计算和硬件受限系统等场景。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [14] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: 本文提出了一种基于本地差分隐私（LDP）的新算法，用于图分析中的k-core分解和三角形计数，显著提高了隐私保护和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模网络的发展，图分析涉及敏感数据，隐私保护成为重要问题。本地差分隐私（LDP）在个体层面保护隐私，无需信任第三方，解决了传统集中式模型的局限性。

Method: 利用图的退化性和最大度数等输入依赖的私有图属性，设计了新的LDP算法。通过私有出度定向和改进的随机响应技术，优化了误差界限。

Result: 实验表明，k-core分解的误差仅为精确值的3倍，远优于基线方法的131倍；三角形计数的近似误差降低了六个数量级，同时保持运行效率。

Conclusion: 本文提出的LDP算法在理论和实践中均表现出色，为隐私保护的图分析提供了高效且准确的解决方案。

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


### [15] [Review of Three Variants of the k-d Tree](https://arxiv.org/abs/2506.20687)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 论文探讨了平衡k-d树的构建方法，对比了三种分区技术，并分析了其中一种的双线程执行性能。


<details>
  <summary>Details</summary>
Motivation: 由于k-d树无法使用传统平衡技术（如AVL树或红黑树），需要找到高效的分区方法来构建平衡树。

Method: 提出了三种不同的分区技术，并对比其性能；同时针对其中一种技术提出了双线程执行方案。

Result: 分析了三种分区技术的计算复杂度，并验证了双线程执行的性能提升。

Conclusion: 不同分区技术对k-d树构建效率有显著影响，双线程执行能进一步提升性能。

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to find the median of a set of data for each recursive subdivision of
that set. The sort or selection used to find the median, and the technique used
to partition the set about that median, strongly influence the computational
complexity of building a k-d tree. This article describes and contrasts three
variants of the k-d tree that differ in their technique used to partition the
set, and compares the performance of those variants. In addition, dual-threaded
execution is proposed and analyzed for one of the three variants.

</details>


### [16] [A Framework for Building Data Structures from Communication Protocols](https://arxiv.org/abs/2506.20761)
*Alexandr Andoni,Shunhua Jiang,Omri Weinstein*

Main category: cs.DS

TL;DR: 提出了一种通过通信模型设计高效高维模式匹配数据结构的方法，应用于部分匹配问题，显著提升了查询时间和空间效率。


<details>
  <summary>Details</summary>
Motivation: 解决高维模式匹配问题中现有数据结构的查询时间和空间效率不足的问题。

Method: 将数据结构问题转化为无歧义Arthur-Merlin（UAM）通信复杂度问题，并开发了针对稀疏集合不相交性的一侧误差通信协议。

Result: 实现了查询时间为n^(1-1/(c log^2 c))且空间接近线性的数据结构，显著优于现有方法。

Conclusion: 该框架展示了数据依赖数据结构的能力，并为高维模式匹配问题提供了更高效的解决方案。

Abstract: We present a general framework for designing efficient data structures for
high-dimensional pattern-matching problems ($\exists \;? i\in[n], f(x_i,y)=1$)
through communication models in which $f(x,y)$ admits sublinear communication
protocols with exponentially-small error. Specifically, we reduce the data
structure problem to the Unambiguous Arthur-Merlin (UAM) communication
complexity of $f(x,y)$ under product distributions.
  We apply our framework to the Partial Match problem (a.k.a, matching with
wildcards), whose underlying communication problem is sparse set-disjointness.
When the database consists of $n$ points in dimension $d$, and the number of
$\star$'s in the query is at most $w = c\log n \;(\ll d)$, the fastest known
linear-space data structure (Cole, Gottlieb and Lewenstein, STOC'04) had query
time $t \approx 2^w = n^c$, which is nontrivial only when $c<1$. By contrast,
our framework produces a data structure with query time $n^{1-1/(c \log^2 c)}$
and space close to linear.
  To achieve this, we develop a one-sided $\epsilon$-error communication
protocol for Set-Disjointness under product distributions with
$\tilde{\Theta}(\sqrt{d\log(1/\epsilon)})$ complexity, improving on the
classical result of Babai, Frankl and Simon (FOCS'86). Building on this
protocol, we show that the Unambiguous AM communication complexity of
$w$-Sparse Set-Disjointness with $\epsilon$-error under product distributions
is $\tilde{O}(\sqrt{w \log(1/\epsilon)})$, independent of the ambient dimension
$d$, which is crucial for the Partial Match result. Our framework sheds further
light on the power of data-dependent data structures, which is instrumental for
reducing to the (much easier) case of product distributions.

</details>


### [17] [Almost Tight Additive Guarantees for \boldmath $k$-Edge-Connectivity](https://arxiv.org/abs/2506.20906)
*Nikhil Kumar,Chaitanya Swamy*

Main category: cs.DS

TL;DR: 本文研究了k边连通生成子图问题（kECSS），提出了针对偶数和奇数k的多项式时间算法，分别获得接近最优解的(k-2)和(k-3)边连通子图。结果在APX难问题中几乎最优，并显著改进了现有工作。


<details>
  <summary>Details</summary>
Motivation: 解决k边连通生成子图问题在理论和实际中的重要性，尤其是在网络设计和优化中的应用。

Method: 针对偶数和奇数k分别设计多项式时间算法，利用LP松弛的自然解，并扩展到度约束版本。

Result: 对于偶数k，获得(k-2)边连通子图，成本不超过LP最优值；对于奇数k，获得(k-3)边连通子图。结果在APX难问题中接近最优。

Conclusion: 提出的算法在成本和连通性方面均显著改进现有方法，并扩展到度约束问题，为相关领域提供了新的解决方案。

Abstract: We consider the \emph{$k$-edge connected spanning subgraph} (kECSS) problem,
where we are given an undirected graph $G = (V, E)$ with nonnegative edge costs
$\{c_e\}_{e\in E}$, and we seek a minimum-cost \emph{$k$-edge connected}
subgraph $H$ of $G$. For even $k$, we present a polytime algorithm that
computes a $(k-2)$-edge connected subgraph of cost at most the optimal value
$LP^*$ of the natural LP-relaxation for kECSS; for odd $k$, we obtain a
$(k-3)$-edge connected subgraph of cost at most $LP^*$. Since kECSS is APX-hard
for all $k\geq 2$, our results are nearly optimal. They also significantly
improve upon the recent work of Hershkowitz et al., both in terms of solution
quality and the simplicity of algorithm and its analysis. Our techniques also
yield an alternate guarantee, where we obtain a $(k-1)$-edge connected subgraph
of cost at most $1.5\cdot LP^*$; with unit edge costs, the cost guarantee
improves to $(1+\frac{4}{3k})\cdot LP^*$, which improves upon the
state-of-the-art approximation for unit edge costs, but with a unit loss in
edge connectivity.
  Our kECSS-result also yields results for the \emph{$k$-edge connected
spanning multigraph} (kECSM) problem, where multiple copies of an edge can be
selected: we obtain a $(1+2/k)$-approximation algorithm for even $k$, and a
$(1+3/k)$-approximation algorithm for odd $k$.
  Our techniques extend to the degree-bounded versions of kECSS and kECSM,
wherein we also impose degree lower- and upper- bounds on the nodes. We obtain
the same cost and connectivity guarantees for these degree-bounded versions
with an additive violation of (roughly) $2$ for the degree bounds. These are
the first results for degree-bounded \{kECSS,kECSM\} of the form where the cost
of the solution obtained is at most the optimum, and the connectivity
constraints are violated by an additive constant.

</details>


### [18] [Courcelle's Theorem for Lipschitz Continuity](https://arxiv.org/abs/2506.21118)
*Tatsuya Gima,Soh Kumabe,Yuichi Yoshida*

Main category: cs.DS

TL;DR: 该论文提出了一个关于Lipschitz连续算法的元定理，类似于Courcelle定理，适用于有界树宽图上的优化问题，并展示了其在高近似性和低Lipschitz常数上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有Lipschitz连续算法的问题特异性，提供一个通用框架，适用于多种组合优化问题。

Method: 利用元定理设计Lipschitz连续算法，针对有界树宽图和有界团宽图上的MSO_2和MSO_1约束问题，提供近似算法。

Result: 对于任何ε>0，在有界树宽图上，算法具有多项式对数Lipschitz常数和(1±ε)近似性。

Conclusion: 该元定理为Lipschitz连续算法提供了通用解决方案，优于现有方法，并扩展了应用范围。

Abstract: Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida
(FOCS'23), measures the stability of an algorithm against small input
perturbations. Algorithms with small Lipschitz continuity are desirable, as
they ensure reliable decision-making and reproducible scientific research.
Several studies have proposed Lipschitz continuous algorithms for various
combinatorial optimization problems, but these algorithms are problem-specific,
requiring a separate design for each problem.
  To address this issue, we provide the first algorithmic meta-theorem in the
field of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz
continuous analogue of Courcelle's theorem, which offers Lipschitz continuous
algorithms for problems on bounded-treewidth graphs. Specifically, we consider
the problem of finding a vertex set in a graph that maximizes or minimizes the
total weight, subject to constraints expressed in monadic second-order logic
(MSO_2). We show that for any $\varepsilon>0$, there exists a $(1\pm
\varepsilon)$-approximation algorithm for the problem with a polylogarithmic
Lipschitz constant on bounded treewidth graphs. On such graphs, our result
outperforms most existing Lipschitz continuous algorithms in terms of
approximability and/or Lipschitz continuity. Further, we provide similar
results for problems on bounded-clique-width graphs subject to constraints
expressed in MSO_1. Additionally, we construct a Lipschitz continuous version
of Baker's decomposition using our meta-theorem as a subroutine.

</details>


### [19] [On Minimizing Wiggle in Stacked Area Charts](https://arxiv.org/abs/2506.21175)
*Alexander Dobler,Martin Nöllenburg*

Main category: cs.DS

TL;DR: 论文分析了堆叠面积图中最小化摆动（wiggle）的计算复杂度，证明其NP难性，并提出精确的混合整数线性规划方法。


<details>
  <summary>Details</summary>
Motivation: 堆叠面积图广泛用于时间序列可视化，但最小化摆动以提高可读性的计算复杂度尚未被正式分析。

Method: 论文证明了摆动最小化的NP难性，提出了混合整数线性规划方法，并与启发式算法进行了实验比较。

Result: 摆动最小化问题被证明是NP难的，且难以近似；混合整数线性规划方法在实验中表现良好。

Conclusion: 论文揭示了摆动最小化的计算复杂性，为未来研究提供了理论基础和实用方法。

Abstract: Stacked area charts are a widely used visualization technique for numerical
time series. The x-axis represents time, and the time series are displayed as
horizontal, variable-height layers stacked on top of each other. The height of
each layer corresponds to the time series values at each time point. The main
aesthetic criterion for optimizing the readability of stacked area charts is
the amount of vertical change of the borders between the time series in the
visualization, called wiggle. While many heuristic algorithms have been
developed to minimize wiggle, the computational complexity of minimizing wiggle
has not been formally analyzed. In this paper, we show that different variants
of wiggle minimization are NP-hard and even hard to approximate. We also
present an exact mixed-integer linear programming formulation and compare its
performance with a state-of-the-art heuristic in an experimental evaluation.
Lastly, we consider a special case of wiggle minimization that corresponds to
the fundamentally interesting and natural problem of ordering a set of numbers
as to minimize their sum of absolute prefix sums. We show several complexity
results for this problem that imply some of the mentioned hardness results for
wiggle minimization.

</details>


### [20] [Edge Clique Partition and Cover Beyond Independence](https://arxiv.org/abs/2506.21216)
*Fedor V. Fomin,Petr A. Golovach,Danil Sagunov,Kirill Simonov*

Main category: cs.DS

TL;DR: 论文研究了基于最大独立集参数化的边团覆盖和边团划分问题，发现两者在复杂性上存在显著差异：ECP/α是固定参数可解的，而ECC/α在k≥2时是NP完全的。此外，ECC/α在参数k+ω(G)下是固定参数可解的。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，稀疏图的边团覆盖或划分的最小数量常接近最大独立集大小，因此研究基于最大独立集的参数化问题具有实际意义。

Method: 引入并研究了Edge Clique Cover Above Independent Set (ECC/α)和Edge Clique Partition Above Independent Set (ECP/α)，分析其参数化复杂性。

Result: ECP/α是固定参数可解的，而ECC/α在k≥2时是NP完全的，但在k∈{0,1}时可多项式求解。ECC/α在参数k+ω(G)下是固定参数可解的。

Conclusion: 研究揭示了两种问题在参数化视角下的复杂性差异，并为稀疏图提供了有效的算法。

Abstract: Covering and partitioning the edges of a graph into cliques are classical
problems at the intersection of combinatorial optimization and graph theory,
having been studied through a range of algorithmic and complexity-theoretic
lenses. Despite the well-known fixed-parameter tractability of these problems
when parameterized by the total number of cliques, such a parameterization
often fails to be meaningful for sparse graphs. In many real-world instances,
on the other hand, the minimum number of cliques in an edge cover or partition
can be very close to the size of a maximum independent set \alpha(G).
  Motivated by this observation, we investigate above \alpha parameterizations
of the edge clique cover and partition problems. Concretely, we introduce and
study Edge Clique Cover Above Independent Set (ECC/\alpha) and Edge Clique
Partition Above Independent Set (ECP/\alpha), where the goal is to cover or
partition all edges of a graph using at most \alpha(G) + k cliques, and k is
the parameter. Our main results reveal a distinct complexity landscape for the
two variants. We show that ECP/\alpha is fixed-parameter tractable, whereas
ECC/\alpha is NP-complete for all k \geq 2, yet can be solved in polynomial
time for k \in {0,1}. These findings highlight intriguing differences between
the two problems when viewed through the lens of parameterization above a
natural lower bound.
  Finally, we demonstrate that ECC/\alpha becomes fixed-parameter tractable
when parameterized by k + \omega(G), where \omega(G) is the size of a maximum
clique of the graph G. This result is particularly relevant for sparse graphs,
in which \omega is typically small. For H-minor free graphs, we design a
subexponential algorithm of running time f(H)^{\sqrt{k}}n^{O(1)}.

</details>


### [21] [Vantage Point Selection Algorithms for Bottleneck Capacity Estimation](https://arxiv.org/abs/2506.21418)
*Vikrant Ashvinkumar,Rezaul Chowdhury,Jie Gao,Mayank Goswami,Joseph S. B. Mitchell,Valentin Polishchuk*

Main category: cs.DS

TL;DR: 研究如何选择最优的观测点以揭示互联网图中的瓶颈容量问题，提出非自适应和自适应两种算法，并分析其近似性能。


<details>
  <summary>Details</summary>
Motivation: 解决互联网中瓶颈容量估计问题，通过选择观测点揭示图中的最大瓶颈容量。

Method: 提出非自适应和自适应两种算法，非自适应算法基于随机排列模型，自适应算法针对固定但未知的容量模型。

Result: 非自适应算法提供1-1/e近似解，自适应算法在树和平面图上给出上下界。

Conclusion: 研究为瓶颈容量估计提供了有效的观测点选择策略，并分析了不同模型的算法性能。

Abstract: Motivated by the problem of estimating bottleneck capacities on the Internet,
we formulate and study the problem of vantage point selection. We are given a
graph $G=(V, E)$ whose edges $E$ have unknown capacity values that are to be
discovered. Probes from a vantage point, i.e, a vertex $v \in V$, along
shortest paths from $v$ to all other vertices, reveal bottleneck edge
capacities along each path. Our goal is to select $k$ vantage points from $V$
that reveal the maximum number of bottleneck edge capacities.
  We consider both a non-adaptive setting where all $k$ vantage points are
selected before any bottleneck capacity is revealed, and an adaptive setting
where each vantage point selection instantly reveals bottleneck capacities
along all shortest paths starting from that point. In the non-adaptive setting,
by considering a relaxed model where edge capacities are drawn from a random
permutation (which still leaves the problem of maximizing the expected number
of revealed edges NP-hard), we are able to give a $1-1/e$ approximate
algorithm. In the adaptive setting we work with the least permissive model
where edge capacities are arbitrarily fixed but unknown. We compare with the
best solution for the particular input instance (i.e. by enumerating all
choices of $k$ tuples), and provide both lower bounds on instance optimal
approximation algorithms and upper bounds for trees and planar graphs.

</details>


### [22] [Succinct Preferential Attachment Graphs](https://arxiv.org/abs/2506.21436)
*Ziad Ismaili Alaoui,Namrata,Sebastian Wild*

Main category: cs.DS

TL;DR: 该论文设计了一种针对图数据的压缩数据结构，其空间利用率随图的压缩性自动优化，并高效支持导航操作。


<details>
  <summary>Details</summary>
Motivation: 目前图数据的压缩计算支持不完善，现有方法通常针对特定图类且空间利用率固定。本文旨在解决这一问题。

Method: 设计了一种数据结构，其空间利用率随图的压缩性动态调整，支持高效的邻接表访问操作。

Result: 在Barabási-Albert模型中，空间利用率接近实例最优；对任意图，空间利用率不超过熵压缩边列表。

Conclusion: 该数据结构在空间利用率和查询效率上取得了平衡，为图数据的压缩计算提供了通用解决方案。

Abstract: Computing over compressed data combines the space saving of data compression
with efficient support for queries directly on the compressed representation.
Such data structures are widely applied in text indexing and have been
successfully generalised to trees. For graphs, support for computing over
compressed data remains patchy; typical results in the area of succinct data
structures are restricted to a specific class of graphs and use the same,
worst-case amount of space for any graph from this class.
  In this work, we design a data structure whose space usage automatically
improves with the compressibility of the graph at hand, while efficiently
supporting navigational operations (simulating adjacency-list access).
Specifically, we show that the space usage approaches the instance-optimal
space when the graph is drawn according to the classic Barab\'asi-Albert model
of preferential-attachment graphs. Our data-structure techniques also work for
arbitrary graphs, guaranteeing a size asymptotically no larger than an
entropy-compressed edge list. A key technical contribution is the careful
analysis of the instance-optimal space usage.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 本文提出了一种用户友好的方法，利用Python和rdflib库简化Neo4j数据库与OWL的集成，以支持知识图谱的生成。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的快速扩展，需要系统化的本体生成方法，但现有方法（如KNARM）在Neo4j与OWL集成时存在技术门槛，需更易用的解决方案。

Method: 使用Python和rdflib库开发脚本，自动从Neo4j数据库（如FAERS数据）生成OWL类和公理，简化集成过程。

Result: 通过FDA FAERS数据集验证，成功实现了Neo4j与OWL的无缝集成，支持药物安全监测和公共卫生决策。

Conclusion: 该方法为快速增长的药物不良事件数据集的本体生成提供了实用解决方案，降低了技术门槛。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [24] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究总结了领域知识在需求工程中的应用，分析了75篇相关论文，提供了方法和挑战的概述，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 领域知识对需求工程至关重要，但缺乏系统化的总结和方法论支持。

Method: 采用混合搜索策略（数据库搜索和雪球法）进行系统映射研究。

Result: 分析了75篇论文，总结了领域知识的类型、质量属性和挑战，为研究和实践提供了支持。

Conclusion: 研究为知识驱动的需求工程提供了概念和方法基础，并指出了未来研究方向。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [25] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 摘要概述了机器学习（ML）驱动系统的敏捷管理现状，通过系统映射研究识别了8个框架和8个关键主题，并指出ML任务工作量估算的挑战。


<details>
  <summary>Details</summary>
Motivation: ML系统的动态特性对传统项目管理提出挑战，需探索敏捷方法的适应性。

Method: 采用混合搜索策略（数据库搜索与雪球迭代）进行系统映射研究。

Result: 识别了27篇论文，归纳出8个框架和8个关键主题，主要挑战是ML任务工作量估算。

Conclusion: 研究总结了现状并指出需更多实证验证现有成果。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [26] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍了五个基于真实场景的RAG系统，涉及多个领域，并通过用户评估总结了十二个关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在事实准确性和上下文相关性上的不足，并填补RAG系统在真实用例中缺乏实证研究的空白。

Method: 开发五个领域特定的RAG应用，结合多语言OCR、语义检索和领域适配的LLMs，并通过100名用户的网络评估。

Result: 用户评估了六个维度，总结了十二个影响RAG系统可靠性和可用性的技术、操作和伦理挑战。

Conclusion: RAG系统在真实场景中具有潜力，但仍需解决技术、操作和伦理问题以提高其可靠性。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [27] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出了一种结合人类指导和强化学习的方法，用于开发复杂的模型转换序列，显著提高了RL的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 手动开发复杂的模型转换（MT）序列容易出错且不可行，而单纯的强化学习（RL）在复杂问题中性能不佳。人类指导可以弥补这一不足。

Method: 提出了一种技术框架，将用户定义的MT映射到RL原语，并通过RL程序执行以寻找最优MT序列，同时整合不确定的人类建议。

Result: 评估表明，即使人类建议不确定，也能显著提升RL性能，并更高效地开发复杂MT序列。

Conclusion: 该方法通过权衡人类建议的确定性和及时性，为RL驱动的人机协同工程方法迈出了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [28] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: IFMA-VD框架通过构建代码行为超图并利用超边卷积提取多边关联特征，改进了漏洞检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法忽视函数间复杂多边关联，导致漏洞检测不全面。

Method: 1. 解析函数为代码属性图生成内部特征；2. 构建代码行为超图编码行为特征；3. 使用超图网络捕获多边关联知识。

Result: 在三个漏洞数据集上F-measure和Recall优于基线方法，多边关联特征增强代码表示。

Conclusion: IFMA-VD有效提升漏洞检测性能，验证了多边关联特征的重要性。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [29] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: Synthline v1 是一种改进的产品线方法，用于生成合成需求数据，通过优化提示策略和后期处理技术提升数据质量，结果显示合成数据在某些任务上优于人工数据。


<details>
  <summary>Details</summary>
Motivation: 公开可用的标记需求数据集稀缺是 AI4RE 发展的主要障碍，需要系统化的方法来控制和优化合成数据的质量。

Method: 提出 Synthline v1，结合多样本提示、自动提示优化（PACE）和后期处理技术，评估其在四种分类任务中的效果。

Result: 多样本提示显著提升数据质量和多样性，PACE 对功能分类效果显著，合成数据在安全和缺陷分类任务上优于人工数据。

Conclusion: 系统化的合成数据生成为 AI4RE 提供了实用解决方案，缓解了数据集稀缺问题。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [30] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 本文探讨了自动程序修复（APR）中链式思考（CoT）技术的应用不足，并提出了一种结合大型语言模型（LLMs）和树搜索的创新框架$T^3$，以提高修复方案的生成精度。


<details>
  <summary>Details</summary>
Motivation: 由于APR任务需要复杂的逻辑和多步推理能力，现有的CoT技术应用不足，因此需要一种更有效的方法来提升自动修复的精度和效率。

Method: 研究系统评估了几种常见的CoT技术在APR任务中的表现，并提出了$T^3$框架，将LLMs的强大推理能力与树搜索相结合。

Result: $T^3$框架显著提高了生成候选修复方案的精度，并为优化样本选择和修复策略提供了指导。

Conclusion: $T^3$框架为高效自动化调试建立了坚实的基础，展示了在APR任务中结合LLMs和树搜索的潜力。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [31] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: KOALA是一个可配置的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用数据，克服了现有工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据收集工具在代码粒度控制、编程环境事件收集和配置灵活性方面存在不足，需要更高效的解决方案。

Method: 开发KOALA插件，安装在JetBrains IDE中，配置任务、IDE功能开关和调查，收集代码快照、IDE操作和其他数据，并转换为ProgSnap2格式。

Result: 从28名学生中收集了数据，展示了工具的实际应用和数据分析潜力。

Conclusion: KOALA是一个灵活且功能强大的工具，适用于教育研究中的数据收集需求。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [32] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文探讨了微前端架构在工业场景中的适用性，通过案例研究分析了其优缺点，并指出在某些情况下其他架构可能同样有效。


<details>
  <summary>Details</summary>
Motivation: 研究微前端架构的适用性，特别是在已有微服务架构的企业中，以解决紧耦合和开发体验差的问题。

Method: 结合学术和灰色文献研究微前端现状，并在一个手工艺品市场的微服务架构中实施微前端，最后通过开发者问卷评估效果。

Result: 微前端成功实施，但研究发现单体前端等其他方案也可能达到类似效果。微前端的优势在于与现有微服务架构的兼容性和团队知识共享。

Conclusion: 微前端并非总是必要，但在特定场景（如已有微服务架构）中可能更便捷。企业应根据具体需求选择架构。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [33] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 论文提出了一种核心模型，用于整合物联网（IoT）数据与业务流程数据，以促进过程挖掘（PM）领域的数据共享与合作。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备的普及，IoT数据与业务流程数据的整合成为挑战，现有数据模型分散且难以共享。

Method: 提出一个核心模型，综合现有数据模型的关键特征，并通过Python原型进行评估。

Result: 核心模型满足常见需求，并通过多个用例验证其有效性。

Conclusion: 该模型为PM领域的数据共享与合作提供了统一框架。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [34] [Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks](https://arxiv.org/abs/2506.20762)
*Shisheng Hu,Jie Gao,Xue Qin,Conghao Zhou,Xinyu Huang,Mushu Li,Mingcheng He,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种基于切片的漂移自适应资源管理方案，用于协同集成感知与通信网络，通过数字孪生技术优化资源分配，提升服务满意度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备和感知目标空间分布的非平稳性导致的建模漂移和规划决策失效问题。

Method: 建立两个网络切片分别提供感知和通信服务，利用数字孪生技术开发漂移自适应统计模型和仿真功能。

Result: 服务满意度提升18%，资源消耗减少13.1%。

Conclusion: 该方案显著提升了资源管理效率和网络性能。

Abstract: In this paper, we propose a novel drift-adaptive slicing-based resource
management scheme for cooperative integrated sensing and communication (ISAC)
networks. Particularly, we establish two network slices to provide sensing and
communication services, respectively. In the large-timescale planning for the
slices, we partition the sensing region of interest (RoI) of each mobile device
and reserve network resources accordingly, facilitating low-complexity
distance-based sensing target assignment in small timescales. To cope with the
non-stationary spatial distributions of mobile devices and sensing targets,
which can result in the drift in modeling the distributions and ineffective
planning decisions, we construct digital twins (DTs) of the slices. In each DT,
a drift-adaptive statistical model and an emulation function are developed for
the spatial distributions in the corresponding slice, which facilitates
closed-form decision-making and efficient validation of a planning decision,
respectively. Numerical results show that the proposed drift-adaptive
slicing-based resource management scheme can increase the service satisfaction
ratio by up to 18% and reduce resource consumption by up to 13.1% when compared
with benchmark schemes.

</details>


### [35] [Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees](https://arxiv.org/abs/2506.21406)
*Tommaso Bonato,Daniele De Sensi,Salvatore Di Girolamo,Abdulla Bataineh,David Hewson,Duncan Roweth,Torsten Hoefler*

Main category: cs.NI

TL;DR: 提出了一种新的自适应路由算法flowcut switching，确保在高性能网络中实现有序数据包传输，适用于非突发流量。


<details>
  <summary>Details</summary>
Motivation: 网络延迟严重影响超级计算机应用性能，现有自适应路由算法可能导致数据包乱序，影响TCP、QUIC和RoCE等协议的性能。

Method: 提出flowcut switching算法，不同于基于突发流量的flowlet switching，该算法在任何网络条件下都能保证数据包有序传输。

Result: flowcut switching算法能够有效减少乱序数据包，提升网络性能，尤其适用于非突发流量场景（如RDMA）。

Conclusion: flowcut switching是一种高效的自适应路由算法，适用于多种网络条件，显著提升性能。

Abstract: Network latency severely impacts the performance of applications running on
supercomputers. Adaptive routing algorithms route packets over different
available paths to reduce latency and improve network utilization. However, if
a switch routes packets belonging to the same network flow on different paths,
they might arrive at the destination out-of-order due to differences in the
latency of these paths. For some transport protocols like TCP, QUIC, and RoCE,
out-of-order (OOO) packets might cause large performance drops or significantly
increase CPU utilization. In this work, we propose flowcut switching, a new
adaptive routing algorithm that provides high-performance in-order packet
delivery. Differently from existing solutions like flowlet switching, which are
based on the assumption of bursty traffic and that might still reorder packets,
flowcut switching guarantees in-order delivery under any network conditions,
and is effective also for non-bursty traffic, as it is often the case for RDMA.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Main category: cs.LG

TL;DR: SAFL是一种基于数据集大小特性的自适应联邦学习框架，通过实验揭示了数据集大小对联邦学习效果的影响，并展示了其在多模态数据中的高效性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注模型异构性和聚合技术，忽视了数据集大小特性对训练动态的根本影响。

Method: 提出了Size-Based Adaptive Federated Learning (SAFL)框架，基于数据集大小特性组织联邦学习，并在13个多模态数据集上进行实验。

Result: 实验发现：1) 1000-1500样本为最优数据集大小范围；2) 结构化数据表现优于非结构化数据；3) 大数据集性能下降。SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL填补了数据特性驱动联邦学习策略的空白，为实际部署提供了理论和实践指导。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [37] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: E-ABIN是一个用于生物网络中异常检测的可解释框架，结合机器学习和图深度学习技术，提供用户友好平台。


<details>
  <summary>Details</summary>
Motivation: 大规模组学数据的增加需要能处理复杂基因表达数据并提供可解释结果的框架。

Method: 结合支持向量机、随机森林、图自编码器和图对抗属性网络等技术。

Result: 在膀胱癌和乳糜泻案例中成功识别生物相关异常并揭示疾病机制。

Conclusion: E-ABIN是一个高效且可解释的生物网络异常检测工具。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [38] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Main category: cs.LG

TL;DR: CCUP提出了一种基于熵不对称性的推理框架，通过分层计算原则优化不确定性下的推理效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过熵不对称性（高熵上下文与低熵内容的对齐）来优化推理过程，为大脑和机器的推理提供统一理论基础。

Method: 提出分层计算框架，包括核心推理约束、资源分配原则、时间引导动态和空间层次组合，并通过形式化定理和模拟验证。

Result: 展示了CCUP对齐推理的效率提升，证明了其作为统一理论基础的潜力。

Conclusion: CCUP为大脑和机器通过递归结构-特异性对齐最小化不确定性提供了统一框架。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [39] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出了一种基于树的方法（DTS和DTS*），通过重用过去计算来优化扩散模型的推理时对齐，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高噪声水平下估计不准确且未重用历史信息，导致计算效率低下。

Method: 将推理时对齐建模为搜索问题，通过树结构传播终端奖励并迭代优化值估计。

Result: 在MNIST和CIFAR-10上，DTS以10倍更少的计算匹配基线性能；在文本生成任务中，DTS*以5倍更少的计算实现最佳样本。

Conclusion: DTS和DTS*通过重用信息提供了一种可扩展的推理时对齐方法，将额外计算转化为更好的样本。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [40] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: 论文证明了FLIPD方法在更现实的假设下的正确性，并探讨了高斯卷积替换为均匀卷积的类似结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在生成模型中表现出色，但FLIPD方法的理论基础仅在不现实的仿射子流形假设下成立。本文旨在填补这一理论空白。

Method: 通过形式化证明，验证FLIPD在现实假设下的正确性，并研究高斯卷积替换为均匀卷积时的类似结果。

Result: 证明了FLIPD在现实假设下的正确性，并发现均匀卷积也能达到类似效果。

Conclusion: 本文完善了FLIPD的理论基础，扩展了其适用范围，为LID估计提供了更坚实的支持。

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [41] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展技术在高级理论物理领域的泛化能力，提出了一种新的符号弱验证框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探究从数学推理基准（如AIME）中学到的测试时扩展技术是否适用于高级理论物理领域。

Method: 开发了一种符号弱验证框架，利用物理问题的结构优化并行扩展效果，并在TPBench和AIME上评估其性能。

Result: 新方法在TPBench上显著优于现有测试时扩展技术，同时在AIME上也表现出色。

Conclusion: 逐步符号验证是解决复杂科学问题的有效方法。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [42] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 基础模型（FMs）正在推动材料科学的变革，提供跨领域通用性和多模态AI能力，适用于数据多样化的研究挑战。本文综述了FMs的应用、数据集和工具，并讨论了其局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在材料科学中范围狭窄且需任务特定工程，而FMs具有跨领域通用性和新兴能力，更适合材料科学的多样化需求。

Method: 通过任务驱动的分类法，涵盖六个应用领域，并综述了单模态和多模态FMs、LLM代理、数据集、工具和实验平台。

Result: FMs在材料科学中取得初步成功，但仍存在通用性、可解释性、数据不平衡、安全性和多模态融合等局限性。

Conclusion: 未来研究方向包括可扩展预训练、持续学习、数据治理和可信度提升。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [43] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Main category: cs.LG

TL;DR: 论文研究了微调后LLM中关系信息的存储与提取方式，提出动态权重嫁接方法，发现信息通过提取和回忆两种路径处理。


<details>
  <summary>Details</summary>
Motivation: 探索微调后LLM中关系信息的存储位置和提取机制，现有方法（如激活修补）不适合此类分析。

Method: 使用动态权重嫁接技术，比较微调与预训练模型的权重变化。

Result: 发现关系信息在实体处理时被提取，并在预测前通过特定层回忆；某些情况下需要两种路径共同作用。

Conclusion: 信息提取和回忆路径在不同层和组件中发挥作用，任务特定注意机制和关系提取步骤是关键。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [44] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Main category: cs.LG

TL;DR: 论文研究了在训练大型语言模型时使用块缩放精度格式（如MX格式）的挑战和可行性，发现其会导致损失函数的随机不稳定性，并提出通过调整精度方案来缓解问题。


<details>
  <summary>Details</summary>
Motivation: 为了应对训练大型语言模型的高计算成本，研究低精度算术格式（如MX格式）在训练中的表现和潜在问题。

Method: 通过训练近千个语言模型，覆盖不同计算预算和精度组合，并结合小规模代理模型进行实验和消融分析。

Result: 发现MX格式训练会导致损失函数的随机不稳定性，并提出通过修改精度方案可以避免或延迟这些问题。

Conclusion: 某些混合精度配置可以恢复与全精度训练竞争的性能，为高效训练提供了新思路。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [45] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于潜在分数的生成AI框架，用于学习计算力学中非线性动力系统的随机、非局部闭合模型和本构关系。通过联合训练卷积自编码器和条件扩散模型，显著降低了采样过程的维度，同时保持了关键的物理特性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂多尺度动力系统建模中的关键挑战，尤其是在没有明确尺度分离的情况下，传统方法的确定性和局部假设可能过于局限。

Method: 联合训练卷积自编码器与条件扩散模型，在潜在空间中降低采样维度，同时保留物理特性。

Result: 数值结果表明，联合训练方法能够发现合适的潜在空间，既保证小的重建误差，又确保扩散模型在潜在空间中的良好性能。

Conclusion: 提出的随机建模框架在数值模拟中实现了显著的计算加速，同时保持了与物理空间中标准扩散模型相当的预测精度。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [46] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Main category: cs.LG

TL;DR: 论文提出了一种名为随机参数分解（SPD）的新方法，解决了现有APD方法的计算成本高和超参数敏感性问题，适用于更大、更复杂的模型。


<details>
  <summary>Details</summary>
Motivation: 现有线性参数分解框架中的主要方法APD存在计算成本高和超参数敏感性问题，限制了其应用范围。

Method: 提出随机参数分解（SPD），通过改进算法提高可扩展性和鲁棒性。

Result: SPD在更大、更复杂的模型上表现优于APD，避免了参数收缩问题，并在玩具模型中更准确地识别真实机制。

Conclusion: SPD为机制可解释性研究提供了新可能性，并开源了相关实验库。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [47] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于LLM的自动化方法，用于优化GPU内核，特别针对新架构如AMD MI300，通过多阶段进化过程实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 优化GPU内核通常需要深厚的架构知识和大量实验，尤其是在新架构或文档不足的情况下，传统方法效率低下。

Method: 采用LLM驱动的多阶段进化方法，包括选择代码版本、生成优化假设、自动实验实现，并通过外部评估系统反馈性能数据。

Result: 由于性能竞赛数据暂未公开，论文主要展示了架构设计、工作流程和定性见解，证明了LLM驱动的潜力。

Conclusion: LLM驱动的自动化方法有望在资源受限或快速演进的硬件环境中加速GPU内核优化，降低对专业知识的依赖。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [48] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Main category: cs.LG

TL;DR: 本文提出了一种基于FINN框架的通用LSTM部署方法，通过ONNX的Scan操作符和自定义转换，实现了在FPGA上高效部署LSTM模型，并在股票预测任务中验证了其性能与资源消耗的平衡。


<details>
  <summary>Details</summary>
Motivation: LSTM在时间序列任务中表现优异，但其计算复杂度限制了在资源受限环境中的实时部署。FPGA虽能高效加速AI，但现有工具主要针对前馈网络，LSTM加速通常需要完全定制实现。本文旨在填补这一空白。

Method: 利用FINN框架和ONNX的Scan操作符建模LSTM的循环计算，支持混合量化和功能验证。通过自定义转换将量化ONNX计算图映射到硬件块，生成硬件IP。

Result: 生成的量化ConvLSTM加速器在性能和资源消耗间取得平衡，且推理精度与最先进模型相当或更好。

Conclusion: 所提方法为FPGA上资源高效的RNN加速器设计铺平了道路。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [49] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 本文研究了联邦学习（FL）在概念漂移下的性能，提出了一种基于信息理论的算法来缓解性能下降，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常是动态的，而现有的FL研究多基于静态数据集，导致在概念漂移下性能下降。本文旨在解决这一问题。

Method: 通过将概念漂移建模为马尔可夫链，引入“稳态泛化误差”评估模型对未来数据的适应能力，并基于KL散度和互信息提出一种正则化算法。

Result: 实验结果表明，提出的算法在三种漂移模式（周期性、渐进性和随机性）下均优于现有方法，验证了其有效性。

Conclusion: 本文提出的方法能有效适应FL中的概念漂移，并通过性能-成本权衡分析提供了实用指导。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [50] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: Hellsemble是一种新颖的集成框架，通过根据实例难度动态分配学习任务，提升分类性能并保持高效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法（如bagging、boosting和DES）计算成本高且对异构数据适应性差，Hellsemble旨在解决这些问题。

Method: Hellsemble通过迭代将误分类实例传递给后续模型，将数据集划分为难度递增的子集，并训练专门的基础学习器。路由器模型根据实例难度分配任务。

Result: 在OpenML-CC18和Tabzilla基准测试中，Hellsemble表现优于传统集成方法。

Conclusion: 基于实例难度的动态分配是构建高效、鲁棒集成系统的有效方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [51] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级回归模型，通过分析攻击对不同DNN层的影响来检测对抗样本，具有高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本检测方法要么效果不佳，要么计算效率低，无法实时处理，因此需要一种更实用的检测方法。

Method: 训练一个轻量级回归模型，通过预测深层特征与早期特征的误差来检测对抗样本。

Result: 该方法在实验中表现出高效性和通用性，适用于图像、视频和音频等多种领域。

Conclusion: 提出的检测方法不仅高效且兼容性强，为对抗样本检测提供了实用解决方案。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [52] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 论文探讨了分布式GNN在链接预测中的性能下降问题，提出SpLPG方法，通过图稀疏化减少通信成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 分布式GNN框架在链接预测中的性能表现未被充分研究，尤其是分区训练导致的信息损失和负采样问题。

Method: 提出SpLPG方法，利用图稀疏化减少通信成本，同时解决性能下降问题。

Result: 实验表明，SpLPG在多个数据集上减少约80%通信开销，同时基本保持链接预测准确性。

Conclusion: SpLPG有效解决了分布式GNN在链接预测中的性能问题，显著降低了通信成本。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [53] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 提出了一种基于约束深度强化学习（CDRL）的方法，用于优化雷达-通信双功能系统中的时间资源分配，以提升目标通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中如何高效分配雷达跟踪和数据通信时间的问题，以提升系统性能。

Method: 采用约束深度强化学习（CDRL）框架，优化时间资源分配，确保在时间预算内最大化通信质量。

Result: 数值实验表明，CDRL方法在动态环境中能有效提升通信质量，同时满足时间约束。

Conclusion: CDRL方法为雷达-通信双功能系统提供了一种高效的时间资源分配解决方案。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [54] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 该论文研究了多功能认知雷达系统中的时间分配问题，通过深度强化学习（DDPG和SAC）寻找帕累托最优解，并比较了两种算法的性能。SAC在稳定性和样本效率上优于DDPG。


<details>
  <summary>Details</summary>
Motivation: 解决认知雷达系统中新目标扫描与已检测目标跟踪之间的时间分配问题，以平衡动态环境中的多目标竞争。

Method: 将问题建模为多目标优化问题，采用深度强化学习（DDPG和SAC）寻找帕累托最优解，并使用NSGA-II算法估计帕累托前沿上界。

Result: DDPG和SAC均能有效适应不同场景，但SAC在稳定性和样本效率上表现更优。

Conclusion: 该研究为开发更高效、自适应的认知雷达系统提供了方法，能够动态平衡多目标竞争。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [55] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 研究发现，LoRA微调显著降低了记忆风险，同时保持了任务性能，与全微调相比表现不同。


<details>
  <summary>Details</summary>
Motivation: 探索微调（尤其是LoRA微调）对记忆化的影响，弥补现有研究的不足。

Method: 使用基于相似性的记忆化度量，比较不同微调策略（LoRA与全微调）的效果。

Result: LoRA微调在模型规模和数据重复等因素上表现与预训练和全微调不同，显著降低记忆风险。

Conclusion: LoRA微调是一种更安全的参数高效方法，能减少记忆化风险且不影响性能。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [56] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Main category: cs.LG

TL;DR: Omniwise是一个端到端的自监督微调管道，首次将大型语言模型（LLM）应用于GPU内核性能预测，无需代码执行或分析工具即可预测关键性能指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的快速发展推动了人工智能的进步，但GPU内核性能预测仍是一个未被充分探索的领域。Omniwise旨在填补这一空白。

Method: Omniwise是一个模型无关的轻量级管道，使用3B参数的小型模型，直接从内核代码预测性能指标（如内存带宽、缓存命中率等）。

Result: 在AMD MI250和MI300X架构上，Omniwise的预测误差在10%以内的准确率超过90%。

Conclusion: Omniwise为开发者提供了高效的性能预测工具，通过在线推理服务器和VS Code插件无缝集成到工作流中。

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [57] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 提出了一种轻量级的输出重加权遗忘方法RWFT，无需完全重新训练即可从分类器中删除特定类别，解决了现有遗忘方法在预测未学习类别时的不足。


<details>
  <summary>Details</summary>
Motivation: 强制执行用户删除权利并减少有害或偏见预测，同时避免完全重新训练的高成本。

Method: 通过重新分配预测概率质量来遗忘特定类别，并设计新的攻击MIA-NN验证方法有效性。

Result: RWFT在现有评估指标和新提出的TV距离指标上均优于现有方法，分别提升2.79%和111.45%。

Conclusion: RWFT是一种高效且安全的遗忘方法，能够在不完全重新训练的情况下达到与完全重新训练相同的效果。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [58] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出一种多模型在线保形预测算法，通过动态选择有效模型子集，降低计算复杂度并缩小预测集大小，同时保证覆盖率和实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决多模型在线保形预测中候选模型集过大或包含低效模型导致的计算复杂度和预测集性能下降问题。

Method: 利用二分图反馈动态选择有效模型子集，并结合预测集大小和模型损失反馈优化算法。

Result: 实验验证算法能构造更小的预测集，并在真实和合成数据集上优于现有方法。

Conclusion: 所提算法在保证覆盖率的同时，显著提升了效率和预测集性能。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [59] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Main category: cs.LG

TL;DR: 论文研究了平均奖励MDP中的离线强化学习，提出了针对目标策略的复杂性度量，并开发了一种基于悲观折扣值迭代的算法。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习在平均奖励MDP中的分布偏移和非均匀覆盖问题，填补理论研究的空白。

Method: 提出了一种基于悲观折扣值迭代的算法，结合新颖的分位数裁剪技术，使用基于经验跨度的惩罚函数。

Result: 首次实现了完全单策略样本复杂度边界，适用于一般弱通信MDP，且无需先验参数知识。

Conclusion: 学习需要超越目标策略稳态分布的覆盖假设，区分了单策略复杂性度量与以往情况，并开发了接近主要结果的下界。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [60] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 本文提出了一种改进的LIME方法（DL-LIME），通过将深度学习融入采样过程，解决了传统LIME忽略特征相关性的问题，并在雷达资源管理中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在决策过程中表现出色，但其“黑盒”特性限制了可解释性。传统LIME方法忽略了特征相关性，因此需要改进。

Method: 提出DL-LIME方法，将深度学习融入LIME的采样过程，用于深度强化学习的雷达资源管理。

Result: DL-LIME在保真度和任务性能上均优于传统LIME，并揭示了雷达资源管理中的关键决策因素。

Conclusion: DL-LIME是一种有效的可解释AI方法，适用于雷达资源管理等领域，解决了传统LIME的局限性。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [61] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体框架的化学过程优化方法，利用LLM智能体自主推断操作约束并指导优化，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在操作约束不明确或不可用时效率低下，依赖主观启发式方法，亟需一种自动化解决方案。

Method: 采用多智能体框架（基于AutoGen和OpenAI的o3模型），包括约束生成、参数验证、模拟执行和优化指导智能体，分两阶段进行自主约束生成和迭代优化。

Result: 在氢化脱烷基化过程中验证，性能与传统方法相当，但计算效率更高（20分钟内收敛，比网格搜索快31倍）。

Conclusion: 该方法在操作约束不明确的新兴过程或改造应用中具有显著潜力，展示了智能体框架的优越性和实用性。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [62] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Main category: cs.LG

TL;DR: 论文提出了一种扩展传统符号规则集成的方法，通过引入可学习的稀疏线性变换，提升模型表达能力同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统符号规则集成依赖轴平行决策区域，需要精心设计的特征才能达到高精度，否则需增加规则复杂度，牺牲可解释性。

Method: 引入可学习的稀疏线性变换（形式为x^Tw≥t），通过基于迭代重加权逻辑回归的贪心优化方法学习。

Result: 实验表明，该方法在保持测试风险与现有方法相当的同时，显著降低了模型复杂度。

Conclusion: 该方法在提升模型表达能力的同时，保持了可解释性，适用于特征设计不足的场景。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [63] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Main category: cs.LG

TL;DR: MSA算法通过利用模型检查点，高效估计和消除数据点的影响，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中删除问题数据点的高计算成本问题。

Method: 提出MSA算法，利用模型检查点估计和消除数据点影响。

Result: MSA在多个基准测试中优于现有算法。

Conclusion: MSA是一种有效的数据擦除方法，提升大语言模型的灵活性。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [64] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Main category: cs.LG

TL;DR: AbMEGD是一个端到端框架，结合多尺度等变图扩散技术，用于抗体序列和结构的协同设计，解决了现有方法在几何特征捕获和新型抗原接口泛化上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法在捕获几何特征和泛化新型抗原接口方面存在局限性，无法准确捕捉分子相互作用和保持结构完整性。

Method: AbMEGD结合多尺度等变图扩散技术，利用几何深度学习，整合原子级几何特征和残基级嵌入，确保几何精度和计算效率。

Result: 实验显示，AbMEGD在氨基酸恢复率上提高了10.13%，改进百分比提升了3.32%，CDR-H3区域的均方根偏差减少了0.062Å。

Conclusion: AbMEGD在结构完整性和功能改进之间取得了平衡，为序列-结构协同设计和亲和力优化设立了新标准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [65] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为SharpZO的混合优化方法，用于在无需反向传播的情况下微调视觉语言模型，显著提升了性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决传统微调方法在内存受限的边缘设备上不可行的问题，同时避免现有无反向传播方法的高方差和性能不足。

Method: 采用两阶段优化：先通过全局探索和平滑损失函数的锐度感知进化策略阶段，再通过稀疏零阶优化进行局部搜索。

Result: 在CLIP模型上的实验显示，SharpZO平均性能提升7%，收敛速度更快。

Conclusion: SharpZO是一种高效的无反向传播微调方法，适用于边缘设备，性能显著优于现有方法。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [66] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的知识蒸馏技术，用于提升小型学生归一化流的采样质量和密度估计能力。


<details>
  <summary>Details</summary>
Motivation: 显式密度学习器在生成模型中越来越受欢迎，但其训练难度大且采样质量较低。本研究旨在探索知识蒸馏在组合归一化流中的潜力，以提升性能。

Method: 采用知识蒸馏技术，通过中间层的非传统知识传递方式，优化小型学生归一化流。

Result: 实验表明，蒸馏后的学生模型在性能上有显著提升，同时模型尺寸更小，计算效率更高。

Conclusion: 知识蒸馏可以有效提升归一化流的性能，同时减少模型复杂度，为实际应用提供了更高效的解决方案。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [67] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Main category: cs.LG

TL;DR: TRIDENT是一个多模态学习框架，结合分子SMILES、文本描述和分类功能注释，通过全局和局部对齐目标学习分子表示，在11个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多忽视分子文本和分类信息，TRIDENT旨在整合这些信息以提升分子表示学习。

Method: TRIDENT使用体积对齐目标和局部对齐目标，结合动量机制平衡全局与局部对齐。

Result: 在11个下游任务中达到最先进性能。

Conclusion: 结合SMILES、文本和分类功能注释对分子性质预测具有显著价值。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [68] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Main category: cs.LG

TL;DR: MoRA提出了一种细粒度的混合低秩自适应学习方法，通过分解每个秩-r更新为r个秩-1组件，解决了现有LoRA-based MoE方法中的干扰、冗余和路由模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中大型预训练模型面临的灾难性遗忘和任务干扰问题，尤其是现有LoRA-based MoE方法中的干扰、冗余和路由模糊问题。

Method: MoRA将每个秩-r更新分解为r个秩-1组件，每个组件作为独立专家，通过自激活和稀疏秩激活实现细粒度混合。引入秩修剪和激活预算，自适应选择稀疏混合秩。

Result: 在CLIP和大型语言模型的持续学习任务中，MoRA显著提升了持续学习效果，减少了遗忘并改善了泛化能力。

Conclusion: MoRA通过细粒度混合秩自适应学习，有效解决了持续学习中的干扰和冗余问题，提升了模型性能。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [69] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的数据选择方法RL-Selector，通过动态优化样本选择策略，减少冗余样本，提高训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练带来高计算和存储开销，现有数据选择方法未能充分利用样本间的动态关系。

Method: 引入epsilon-sample cover量化样本冗余，将数据选择问题转化为强化学习任务，设计RL-Selector动态优化选择策略。

Result: 在多个基准数据集和架构上，RL-Selector优于现有方法，显著提升训练效率和模型泛化能力。

Conclusion: RL-Selector通过动态样本选择有效减少冗余，为高效训练提供了新思路。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [70] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Main category: cs.LG

TL;DR: SSE是一种基于图的层次强化学习框架，通过强制单步子目标可达性和动态调整路径成本，解决了长时程目标任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 长时程目标任务的奖励稀疏且目标遥远，现有方法存在子目标不可行和规划效率低的问题。

Method: SSE通过约束高层决策确保子目标可达性，采用解耦探索策略和动态路径优化。

Result: 实验表明，SSE在效率和成功率上优于现有方法。

Conclusion: SSE为长时程目标任务提供了一种高效可靠的解决方案。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [71] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Main category: cs.LG

TL;DR: 提出了一种基于遗憾感知的无监督技能发现方法，通过技能生成与策略学习的极小极大博弈，提升高维环境下的效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法在探索多样性上表现良好，但在高维环境下效率不足。

Method: 将技能发现建模为技能生成与策略学习的极小极大博弈，利用遗憾评分指导技能发现，并采用可升级的技能生成器种群避免退化。

Result: 实验表明，该方法在效率和多样性上优于基线，并在高维环境中实现15%的零样本提升。

Conclusion: 通过遗憾感知和可升级技能生成器，该方法显著提升了技能发现的效率和多样性。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [72] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Main category: cs.LG

TL;DR: FedDAA是一个动态聚类联邦学习框架，旨在适应多源概念漂移并保留有用历史知识，相比现有方法在多个数据集上提升了7.84%至8.52%的准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分布的动态变化（概念漂移）导致现有方法难以区分漂移来源，从而无法选择性保留历史知识，影响模型性能。

Method: FedDAA包含三个模块：聚类数量确定模块、真实漂移检测模块和概念漂移适应模块，分别用于优化聚类数量、区分漂移类型及适应新数据。

Result: 实验表明，FedDAA在Fashion-MNIST、CIFAR-10和CIFAR-100上比现有方法提升了7.84%至8.52%的准确率。

Conclusion: FedDAA通过明确区分漂移来源并动态调整策略，有效解决了联邦学习中的多源概念漂移问题，显著提升了模型性能。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [73] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种利用知识图谱生成高质量指令数据的新方法，显著提升大语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型（LLMs）的工具使用能力，解决现有方法生成指令数据质量不足的问题。

Method: 从知识图谱中提取查询路径，转化为用户查询，并将实体关系转化为可操作工具，生成高质量指令数据。

Result: 实验表明，仅需少量合成数据微调即可显著提升LLMs的工具使用能力和整体性能。

Conclusion: 知识图谱生成的指令数据能有效提升LLMs的工具使用能力，为未来研究提供了新方向。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [74] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: CHOOSE是一种基于CoT增强的浅层Transformer框架，用于无线符号检测，通过引入自回归潜在推理步骤，显著提升浅层模型的推理能力，同时保持存储和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于ICL的Transformer模型因深度架构导致的高存储和计算成本问题，适用于资源受限的移动设备。

Method: 提出CHOOSE框架，通过自回归潜在推理步骤增强浅层Transformer（1-2层）的推理能力，无需增加模型深度。

Result: 实验表明，CHOOSE性能优于传统浅层Transformer，与深层Transformer相当，同时保持高效。

Conclusion: CHOOSE为资源受限的无线接收器实现Transformer算法提供了有前景的方向。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [75] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Main category: cs.LG

TL;DR: 论文提出了FeDa4Fair库，用于在异构客户端偏置下评估公平联邦学习方法，并发布了四个数据集和基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题因客户端数据分布的异构性而复杂化，现有方法多关注单一敏感属性，难以满足多样化的公平需求。

Method: 通过开发FeDa4Fair库，生成适用于公平联邦学习评估的表格数据集，并提供数据集、基准测试和评估函数。

Result: 发布了四个具有异构偏置的数据集和相应的基准测试工具，支持公平性方法的可复现性评估。

Conclusion: FeDa4Fair为联邦学习中的公平性研究提供了标准化工具，有助于更全面和可复现的公平性评估。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [76] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Main category: cs.LG

TL;DR: H-CMR是一种新型概念模型，通过层次化概念推理提供对概念和任务预测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有概念模型仅对最终任务预测提供解释，而概念预测本身仍为黑盒，缺乏透明性。

Method: H-CMR利用有向无环图建模概念间关系，通过神经注意力机制选择逻辑规则进行层次化推理。

Result: H-CMR在保持高性能的同时，支持概念和模型干预，显著提升推理准确性和训练效率。

Conclusion: H-CMR通过层次化概念推理实现了更全面的可解释性，同时保持了模型性能。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [77] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 提出了一种动态跳过Transformer中间层的架构，但未在验证交叉熵和FLOPs之间取得优于密集基线的效果。


<details>
  <summary>Details</summary>
Motivation: 基于中间层冗余和早期层信息聚合的观察，探索动态跳过中间层以提高效率。

Method: 使用学习的门控机制动态跳过对称的中间层块，并通过门控注意力机制防止后续令牌关注跳过的位置，同时控制残差范数和门稀疏性。

Result: 在研究的规模下，未能在验证交叉熵和FLOPs之间取得优于密集基线的效果。

Conclusion: 提出的方法未达到预期效果，但代码已开源。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [78] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种基于双扩散隐式桥（DDIB）的框架，解决单细胞扰动数据未配对问题，结合基因调控网络（GRN）和掩码机制提升生成质量，并引入新的评估指标以反映细胞异质性。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序是破坏性过程，无法捕捉同一细胞在扰动前后的表型，导致数据未配对。现有方法要么强制配对，要么忽略未扰动与扰动细胞间的关系。

Method: 基于DDIB框架学习数据分布映射，结合GRN传播扰动信号，使用掩码机制预测沉默基因，并引入双条件扩散模型（Unlasting）和新的评估指标。

Result: 有效解决了未配对数据问题，提升了生成质量，并通过新指标更好地反映了单细胞响应的异质性。

Conclusion: Unlasting框架在单细胞扰动数据分析中表现出色，结合GRN和掩码机制，为生物医学研究提供了更可靠的工具。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [79] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 本文提出了一种抗脆弱的强化学习框架，通过动态选择策略来应对无人机导航中的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法对固定扰动的处理能力有限，无法适应更广泛的分布偏移，因此需要一种更具适应性的方法。

Method: 采用基于折扣汤普森采样（DTS）的切换机制，动态选择多个鲁棒策略，以最小化对抗性引起的状态-动作-价值分布偏移。

Result: 在复杂导航环境中，该方法表现出色，路径更短且冲突更少，优于传统鲁棒强化学习方法。

Conclusion: 抗脆弱框架能有效应对未见过的对抗攻击，提升无人机导航的适应性。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [80] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种抗脆弱的强化学习框架，通过逐步增加的对抗性扰动训练RL代理，以应对观测空间中的分布外攻击，提升决策安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键系统中，RL策略易受观测空间中的分布外对抗攻击影响，导致决策不安全或次优。为解决这一问题，研究旨在设计一种能适应并抵御此类攻击的框架。

Method: 提出抗脆弱RL框架，通过模拟攻击者逐步增加观测空间扰动，利用Wasserstein距离最小化进行专家引导的批评对齐，确保价值函数分布的稳定性。

Result: 在无人机避障场景中，抗脆弱策略显著优于标准和鲁棒RL基线，对抗攻击时累积奖励提高15%，冲突事件减少30%。

Conclusion: 抗脆弱强化学习在动态威胁环境中具有理论和实践可行性，能提升决策的安全性和韧性。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [81] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的Norm-Aware Linear Attention机制，通过解耦查询和键矩阵的范数与方向，解决了线性注意力中熵差距和负值抑制问题，提升了表达能力和效率。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然降低了复杂度，但忽略了查询范数，导致熵差距，同时抑制负值影响了内积交互。

Method: 解耦查询和键矩阵为范数和方向两部分，设计范数感知核函数动态控制熵减少，并采用范数保持映射确保非负性。

Result: NaLaFormer在视觉和语言任务中性能提升高达4.2%，同时增强了表达能力和效率。

Conclusion: Norm-Aware Linear Attention机制有效解决了线性注意力的局限性，显著提升了模型性能。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [82] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Main category: cs.LG

TL;DR: DBConformer是一种双分支卷积Transformer网络，用于EEG解码，结合时间与空间特征，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有CNN-Transformer混合模型在EEG解码中难以有效整合局部与全局特征，且缺乏显式的通道建模。

Method: 提出DBConformer，包含时间分支和空间分支，分别建模长程时间依赖性和通道间关系，并引入轻量级通道注意力模块。

Result: 在五个运动想象数据集和两个癫痫检测数据集上，DBConformer性能优于10个基线模型，参数量减少八倍。

Conclusion: DBConformer具有高性能和可解释性，适用于稳健且可解释的EEG解码。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [83] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件生成对抗网络（cGAN）的框架，用于生成能绕过无人机入侵检测系统（IDS）的隐蔽对抗攻击，并通过条件变分自编码器（CVAE）检测这些攻击。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用空域的广泛应用，传统异常检测方法难以识别新型威胁，且现有方法无法区分隐蔽对抗攻击与真实异常事件。

Method: 设计了一个多类IDS分类器，并利用cGAN生成对抗样本，同时使用CVAE检测这些样本。

Result: CVAE的负对数似然方法在检测隐蔽对抗攻击方面显著优于传统马氏距离检测器。

Conclusion: 研究强调了高级概率建模在增强IDS对抗生成模型攻击能力中的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [84] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，解决了数据、计算和通信异构性问题，并在多数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据、计算和通信异构性方面面临挑战，现有方法仅依赖文本提示且忽略了联合标签-域分布偏移。

Method: 通过双提示学习（全局和局部提示）和交叉融合模块，实现个性化联邦学习。

Result: 在九种异构数据集上的实验表明，pFedDC优于现有方法。

Conclusion: pFedDC通过双提示和交叉融合有效解决了联邦学习中的异构性问题，提升了模型性能。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [85] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Main category: cs.LG

TL;DR: 提出了一种基于线性行为的神经网络压缩方法，通过合并线性激活的神经元层，实现了无损压缩至原模型大小的1/4。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络压缩方法多通过测量重要性和冗余性减少参数，但已有优化方案仍有提升空间。

Method: 基于ReLU类激活函数的线性行为理论，提出合并几乎总是激活的神经元层的方法。

Result: 在多数测试模型中实现了无损压缩至原模型大小的1/4，且与重要性剪枝方法兼容。

Conclusion: 该方法为新型神经网络压缩技术奠定了基础，可实现更小、更高效的模型。

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [86] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 论文提出了一种基于多样化小批量选择的强化学习方法，利用行列式点过程（DPP）提升实例多样性，应用于药物发现领域，显著提高了解决方案的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，评估实例（如环境交互）的成本高且耗时，而多样化探索对避免模式崩溃至关重要。药物发现领域尤其需要多样且高质量的解决方案。

Method: 引入多样化小批量选择框架，使用行列式点过程（DPP）选择多样化实例，应用于药物发现中的化学探索。

Result: 实验表明，该方法在药物发现中显著提高了生成分子的多样性和质量。

Conclusion: 多样化小批量选择框架在药物发现中具有潜力，可加速满足未满足的医疗需求。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [87] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Main category: cs.LG

TL;DR: 研究探讨了在部分参与情况下，通过引入人工智能代理（Artificial Delegates）来提升持续性投票系统的公平性和代表性。


<details>
  <summary>Details</summary>
Motivation: 现有持续性投票规则假设完全参与和完整批准信息，但现实中部分参与是常态，导致公平性和代表性不足。

Method: 研究通过引入偏好学习的人工智能代理（Artificial Delegates）来代表缺席选民，分析其对投票公平性和代表性的影响。

Result: 研究发现缺席显著影响公平性，但人工智能代理能有效缓解这些影响，并在多种场景中提升系统的稳健性。

Conclusion: 人工智能代理能够弥补部分参与带来的不足，显著提升持续性投票系统的公平性和代表性。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [88] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出了一种基于熵的复杂数据分类方法，通过SFT和蒸馏结合的方式高效微调LLMs，显著优于标准SFT，且数据使用量减少62%。


<details>
  <summary>Details</summary>
Motivation: 通用LLMs通过SFT微调在特定领域表现提升，但传统方法需要大量数据和昂贵调用。

Method: 通过单标记答案熵（ROC AUC 0.73）分类数据复杂度，结合SFT和蒸馏进行微调。

Result: 方法在平均准确率上显著优于标准SFT（0.55 vs 0.43），且与蒸馏性能相当但数据用量减少62%。

Conclusion: 提出了一种高效微调LLMs的新方法，为相关研究提供了代码和数据支持。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [89] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 提出了一种基于零样本学习和大语言模型的新方法，用于预测电子元件过时风险，解决了数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 电子元件过时导致成本增加和系统可用性中断，但现有数据不足阻碍了准确预测。

Method: 利用零样本学习和大语言模型，从表格数据中提取领域知识进行风险预测。

Result: 在两个真实数据集上验证了方法的有效性，并比较了四种大语言模型的性能。

Conclusion: 选择合适的模型对预测任务至关重要，该方法为解决数据不足问题提供了新思路。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [90] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX是一种低通信的大规模去中心化集群训练框架，结合了多种技术，显著提升了大规模模型预训练的效率。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中高通信需求对集中式集群的依赖问题，探索在慢速网络上训练超大规模模型的可能性。

Method: 结合Pipeline Parallelism、Dual Optimizer Policy、One-Step-Delay Overlap of Communication and Local Training以及Adaptive Gradient Compression Scheme。

Result: 成功在1Gbps网络上预训练107B参数模型，相比AllReduce实现357倍加速，且模型收敛几乎无退化。

Conclusion: DiLoCoX是首个成功应用于超100B参数模型的去中心化训练框架，为慢速网络上的大规模训练提供了可行方案。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [91] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Main category: cs.LG

TL;DR: 本文重新审视了k-means聚类和k-GMM的随机种子技术，提出了基于前瞻原则和多轮策略的新初始化方法，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 改进k-means和k-GMM的种子初始化技术，通过前瞻和多轮策略提升最终性能。

Method: 提出基于前瞻原则和多轮策略的种子初始化方法，优化种子选择和随机化效果。

Result: 实验表明新方法在SSE（k-means）和log-likelihood（k-GMM）上优于现有技术，且开销较小。

Conclusion: 新方法有望成为标准技术，并为理论分析开辟了新方向。

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [92] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 提出了一种新的路由框架LPR，通过聚类视角重新审视专家路由，显著改善了MoE架构中的负载不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前MoE系统存在严重的负载不平衡问题，导致模型容量和计算资源利用率低下。

Method: 提出Latent Prototype Routing (LPR)，一种基于聚类的路由框架，推广现有方法并促进专家负载平衡。

Result: 实验表明，LPR将专家负载的基尼系数从0.70降至0.035，最小-最大专家负载比从1e-6提升至0.70，实现近乎完美的负载平衡。

Conclusion: LPR在不影响下游性能的情况下，显著提升了MoE模型的负载平衡和资源利用率。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [93] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: AGTCNet是一种新型的图-时序卷积网络，用于运动想象EEG分类，显著提升了分类性能并减少了模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有BCI系统在捕获多通道EEG信号的复杂时空依赖性方面的不足，开发更鲁棒且实用的系统。

Method: 提出AGTCNet模型，利用EEG电极的拓扑结构作为归纳偏置，结合图卷积注意力网络（GCAT）学习时空EEG表示。

Result: 在BCI Competition IV Dataset 2a和EEG Motor Movement/Imagery Dataset上，AGTCNet在独立和特定被试分类中均取得最优性能。

Conclusion: AGTCNet在运动想象EEG分类中表现出高效性和实用性，为BCI部署提供了有力工具。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [94] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: DynamicBench是一个新的基准测试，用于评估大语言模型（LLMs）在实时信息处理中的能力，通过双路径检索管道和领域特定知识，显著优于GPT4o。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法满足实时信息处理的需求，DynamicBench旨在填补这一空白。

Method: 采用双路径检索管道（结合网络搜索和本地报告数据库），并引入高级报告生成系统。

Result: 在无文档和有文档辅助的场景下，分别比GPT4o高出7.0%和5.8%。

Conclusion: DynamicBench有效评估了LLMs的实时信息处理能力，并展示了其优越性能。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [95] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Main category: cs.LG

TL;DR: 论文证明了持久拉普拉斯算子的特征值在添加一个单纯形时的变化上界，提供了特征值级别的稳定性保证。


<details>
  <summary>Details</summary>
Motivation: 持久拉普拉斯算子的特征值是描述数据几何和拓扑特征的重要工具，但其在局部更新时的稳定性尚未明确，影响下游应用的可靠性。

Method: 通过证明一个统一的Lipschitz界，表明添加一个单纯形时，持久拉普拉斯算子的特征值变化不超过该单纯形边界欧几里得范数的两倍。

Result: 首次为谱拓扑数据分析提供了特征值级别的鲁棒性保证，确保局部更新下谱特征的稳定性。

Conclusion: 该结果为动态数据环境中的可靠误差控制提供了理论基础，增强了谱拓扑数据分析的实用性。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [96] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Main category: cs.LG

TL;DR: 论文提出了SMMILE和SMMILE++，首个专家驱动的医学多模态上下文学习基准，评估了15种多模态大语言模型的表现，发现其在医学任务中的上下文学习能力有限，且易受无关示例和顺序影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态上下文学习在医学领域的潜力，填补现有研究空白，评估多模态大语言模型在医学任务中的适应性。

Method: 通过专家团队构建SMMILE和SMMILE++基准，包含111个问题和1038个排列变体，覆盖6个医学专业和13种成像模态，评估15种模型的性能。

Result: 大多数模型在医学任务中表现中等或较差，上下文学习仅带来8%-9.4%的提升，且易受无关示例干扰（性能下降达9.5%），示例顺序存在近因效应（性能提升达71%）。

Conclusion: 当前多模态大语言模型在医学任务中的上下文学习能力有限，需进一步优化以减少干扰和顺序偏见。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [97] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Main category: cs.LG

TL;DR: rQdia通过增强图像正则化Q值分布，提升像素深度强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素深度强化学习中Q值分布的不稳定性问题。

Method: 使用辅助损失函数（MSE）均衡Q值分布。

Result: 在MuJoCo和Atari任务中显著提升性能，样本效率和长期训练效果均有改善。

Conclusion: rQdia成功将无模型连续控制从像素提升至超越状态编码基线的水平。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [98] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 论文探讨了在低功耗DNN计算中，通过结合细粒度错误恢复能力和硬件近似技术，提高能效。使用ROUP近似乘法器，在ResNet-8模型上实现54%的能效提升，仅损失4%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在低功耗DNN计算中通过硬件近似技术和细粒度错误恢复能力提高能效。

Method: 采用ROUP近似乘法器，通过层、滤波器和内核级别的细粒度分布策略，评估其对准确性和能效的影响。

Result: 在CIFAR-10数据集上，相比基准量化模型，能效提升54%，准确率仅损失4%；相比现有DNN近似方法，能效提升2倍且准确性更高。

Conclusion: 提出的方法在低功耗DNN计算中实现了显著的能效提升，同时保持了较高的准确性。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [99] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文提出了一种动态更新小幅度权重的微调方法NANOADAM，以减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型预训练神经网络资源消耗高，且观察到梯度与权重之间存在特定关联。

Method: 提出NANOADAM，仅动态更新小幅度权重，无需梯度计算。

Result: 在NLP和视觉任务中表现优异，支持更大学习率并减少灾难性遗忘。

Conclusion: NANOADAM是一种高效且性能优越的微调方法。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [100] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Main category: cs.LG

TL;DR: 论文提出了一种增强时间感知图注意力网络（ATGAT），通过多模块设计提升加密货币交易欺诈检测性能，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币交易欺诈检测面临复杂交易模式和严重类别不平衡的双重挑战，传统方法难以捕捉交易网络的时空依赖关系。

Method: ATGAT包含三个模块：多尺度时间差特征融合的时间嵌入模块、时空和全局上下文联合优化的三重注意力机制、以及加权BCE损失解决类别不平衡。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，比XGBoost、GCN和标准GAT分别提升9.2%、12.0%和10.0%。

Conclusion: ATGAT验证了时间感知和三重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广至其他时空图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [101] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 通过动态评估是否在每层Transformer编码器后停止上下文学习，提出了一种早期停止方法，显著加速推理速度，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决表格基础模型在推理时的高成本问题，尤其是在大数据集上。

Method: 动态评估每层Transformer编码器后是否停止上下文学习，并使用预训练的逐层解码器解码嵌入。

Result: 在34个小分类任务上，推理速度提升1.3倍，性能几乎无损；在5个更大分类任务上，速度提升2.2倍。

Conclusion: 早期退出是一种有效且实用的策略，可显著提升表格上下文学习的效率。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [102] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: 论文提出了一种名为ScalaBL的可扩展贝叶斯低秩适应方法，通过随机变分子空间推理，解决了大语言模型（LLMs）不确定性量化的问题，同时减少了额外参数的需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在幻觉和校准不足的问题，在高风险领域（如自主系统和医疗）中，量化其不确定性至关重要。现有方法因额外参数过多而难以扩展。

Method: 提出ScalaBL方法，在低秩适应（LoRA）参数的子空间中进行贝叶斯推理，利用LoRA参数作为投影矩阵，将子空间样本映射到LLM的完整权重空间，并通过随机变分推断学习所有参数。

Result: 尽管子空间维度低，ScalaBL在性能上与最先进方法相当，仅需约1000个额外参数，并能扩展到迄今为止最大的贝叶斯LLM，其基础参数是先前工作的四倍。

Conclusion: ScalaBL是一种高效且可扩展的方法，能够在大规模LLMs中实现不确定性量化，同时显著减少计算和参数开销。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [103] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为D-CHAG的分布式跨通道分层聚合方法，用于处理多通道图像模态数据，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的科学基础模型在科学发现和创新中具有巨大潜力，但当前分布式方法未完全解决图像标记化和聚合的计算密集型问题。

Method: 提出了D-CHAG方法，适用于任何模型并行策略和视觉变换器架构，结合张量并行和模型分片。

Result: 在超光谱成像和天气预报任务中，D-CHAG实现了内存使用减少75%，并在1,024个AMD GPU上使持续吞吐量翻倍。

Conclusion: D-CHAG方法显著提升了计算效率，为多通道图像模态数据的处理提供了高效解决方案。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [104] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Main category: cs.LG

TL;DR: 提出了一种名为SSCP的单步完成策略，通过增强的流匹配目标训练生成策略，实现高效的单次动作生成，结合了生成模型的表达能力和单峰策略的效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散和流匹配模型在离线强化学习中因迭代采样导致的高推理成本和训练不稳定性问题。

Method: 使用增强的流匹配目标训练SSCP，预测直接从中间流样本生成的完成向量，避免长反向传播链。

Result: 在离线、离线到在线和在线强化学习设置中表现出色，速度和适应性优于基于扩散的基线方法。

Conclusion: SSCP是一种多功能、表达力强且高效的深度强化学习框架，适用于序列决策任务。

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [105] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Main category: cs.LG

TL;DR: 研究探讨了多模态机器学习在检测双向互动中欺骗行为的有效性，结合了欺骗者和被欺骗者的数据，发现语音和面部信息的结合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据（语音和面部信息）在欺骗检测中的效果，并验证心理学理论关于面部和声音表达差异的假设。

Method: 采用早期和晚期融合方法，结合音频和视频数据（动作单元和凝视信息），分析不同模态和参与者的组合。

Result: 多模态数据结合（语音和面部信息）效果优于单模态方法，且结合双方数据时准确率最高（71%）。

Conclusion: 研究支持心理学理论，为未来双向互动研究（如心理治疗）奠定了基础。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [106] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Main category: cs.LG

TL;DR: 提出一种基于最优控制问题的ResNet训练方法，通过惩罚中间输出来优化网络结构，实现层剪枝。


<details>
  <summary>Details</summary>
Motivation: 将最优控制问题与ResNet训练结合，探索理论支持的层剪枝策略。

Method: 通过惩罚隐藏状态的中间输出（类似最优控制中的阶段成本），利用跳跃连接和输出层传播状态。

Result: 训练动态使不必要的深层残差层权重趋近于零，验证了层剪枝的可行性。

Conclusion: 该方法为理论驱动的层剪枝提供了潜在策略。

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [107] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Main category: cs.LG

TL;DR: 提出了一种电子评估主观答题脚本的高效方法，通过关键词提取和语法检查实现。


<details>
  <summary>Details</summary>
Motivation: 传统主观答题评估效率低，需自动化解决方案。

Method: 集成系统提取答题脚本关键词，与开放和封闭域关键词对比，并检查语法和拼写错误。

Result: 在100份学生答题脚本测试中，系统精度达0.91。

Conclusion: 系统高效且准确，适用于主观答题评估。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [108] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Main category: cs.LG

TL;DR: 本文提出了一种结合遗传算法（GA）和强化学习（RL）的混合方法，用于优化低存储扩展稳定性Runge-Kutta（ESRK）方法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决高精度、低存储ESRK方法在平衡准确性、稳定性和计算效率方面的挑战。

Method: 采用GA驱动的突变进行搜索空间探索，并结合RL动态优化启发式选择，实现参数减少和计算效率提升。

Result: 在基准测试中，最佳启发式方法实现了25%的IPOPT运行时间减少，同时保持数值稳定性和准确性。

Conclusion: 该方法为数值方法的启发式优化提供了新范式，拓宽了低存储Runge-Kutta方法在高保真模拟中的应用。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [109] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的分类模型，用于预测乳腺癌和宫颈癌的易感性，并结合地理位置提供医疗服务建议，旨在提高癌症筛查意识和降低死亡率。


<details>
  <summary>Details</summary>
Motivation: 由于Telangana地区女性对宫颈癌、乳腺癌和口腔癌的筛查率极低，且缺乏相关症状和筛查知识的认知，研究旨在通过技术手段提升癌症意识和筛查率。

Method: 使用决策树分类和支持向量机分类算法分别预测宫颈癌和乳腺癌的易感性，并开发系统提供就近医院建议和健康记录管理。

Result: 模型能够预测癌症易感性，系统可提供医疗服务建议和健康记录管理，有望提升筛查率和癌症知识普及。

Conclusion: 该解决方案有助于提高癌症意识，降低死亡率，并提升Telangana地区居民的癌症知识水平。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [110] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Main category: cs.LG

TL;DR: 提出了一种无监督故障诊断方法，结合多变量时间序列分析、过程挖掘和随机模拟，用于CPS中的故障建模与分类。


<details>
  <summary>Details</summary>
Motivation: 解决手动建模故障行为的复杂性和难以解释的问题，提高故障诊断的准确性和效率。

Method: 通过多变量时间序列分析检测异常，转化为事件日志，利用过程挖掘生成可解释模型，并结合随机模拟增强分析。

Result: 在Robotic Arm Dataset上验证了方法的有效性，能够建模、模拟和分类故障行为。

Conclusion: 该方法支持预测性维护和数字孪生开发，提升了CPS的可靠性和操作效率。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [111] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Main category: cs.LG

TL;DR: mTSBench是迄今为止最大的多元时间序列异常检测（MTS-AD）和无监督模型选择基准，涵盖19个数据集中的344个标记时间序列，评估了24种异常检测方法，并揭示了模型选择的重要性及其当前局限性。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测在多个领域至关重要，但由于复杂的变量间依赖关系、时间动态性和稀疏的异常标签，仍然具有挑战性。

Method: 引入mTSBench基准，评估24种异常检测方法（包括基于大型语言模型的检测器），并系统性地比较无监督模型选择技术。

Result: 结果显示没有单一检测器在所有数据集中表现优异，且当前最先进的选择方法仍远未达到最优。

Conclusion: mTSBench提供了一个统一的评估套件，以促进未来在自适应异常检测和稳健模型选择方面的研究。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [112] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Main category: cs.LG

TL;DR: 研究发现，在大型语言模型（OLMoE）的预训练过程中，即使训练损失已收敛，测试性能仍会持续提升（即“grokking”现象）。通过分析模型内部动态，揭示了从记忆到泛化的转换机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练中“grokking”现象的机制，特别是在大规模预训练模型中的表现。

Method: 在7B参数的OLMoE模型预训练过程中，计算训练损失并评估其在数学推理、代码生成等任务上的泛化能力。开发了两种新指标量化路径距离和复杂性。

Result: 验证了“grokking”现象在大规模预训练中依然存在，并揭示了路径结构化和复杂性降低是泛化延迟的机制。新指标能预测下游任务泛化性能。

Conclusion: 研究提供了对“grokking”现象的机制解释，并提出实用指标用于监控预训练模型的泛化性能，无需微调或测试。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [113] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 研究了在低资源音乐类型中，如何通过不同适配器配置优化MusicGen和Mustango模型的微调，发现卷积适配器捕捉局部细节，而Transformer适配器保留长程依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模音乐生成模型微调计算成本高，参数高效微调（PEFT）技术如适配器方法成为替代方案，但适配器设计选择多样，需探索最优配置。

Method: 研究MusicGen和Mustango模型在Hindustani古典和土耳其Makam音乐上的不同适配器配置，包括架构、位置和大小。

Result: 卷积适配器擅长捕捉局部细节，Transformer适配器保留长程依赖；Mustango生成多样但稳定性差，MusicGen训练快且质量高。

Conclusion: 中规模适配器（40M参数）在表达力和质量间取得平衡，Mustango和MusicGen各有优劣，需根据需求选择。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [114] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出了一种可微分的短时傅里叶变换（STFT）方法，通过梯度优化参数，解决了传统方法依赖离散搜索的问题，并展示了其在提升时频表示和下游任务性能上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统STFT参数调整依赖手动或启发式方法，效果不佳且计算量大，需要一种更高效的优化方法。

Method: 提出了一种统一的、可微分的STFT框架，支持基于梯度的参数优化，并可无缝集成到神经网络中进行联合优化。

Result: 实验表明，该方法在模拟和真实数据上均能显著提升时频表示的质量和下游任务的性能。

Conclusion: 可微分STFT为参数优化提供了一种高效且灵活的方法，适用于多种信号处理任务。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [115] [Guarding Offices with Maximum Dispersion](https://arxiv.org/abs/2506.21307)
*Sándor P. Fekete,Kai Kobbe,Dominik Krupke,Joseph S. B. Mitchell,Christian Rieck,Christian Scheffer*

Main category: cs.CG

TL;DR: 研究了在正交多边形中顶点守卫的分散艺术画廊问题，证明了某些情况下NP完全性，并提出了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界平面图中顶点守卫的分散问题，以最大化守卫间的最小距离。

Method: 结合NP完全性证明和多项式时间算法，包括动态规划和SAT/CP/MIP求解器。

Result: 证明了某些问题的NP完全性，提出了高效算法，并在实际应用中验证了可行性。

Conclusion: 分散艺术画廊问题在某些情况下是NP完全的，但可通过高效算法解决，实际应用表现良好。

Abstract: We investigate the Dispersive Art Gallery Problem with vertex guards and
rectangular visibility ($r$-visibility) for a class of orthogonal polygons that
reflect the properties of real-world floor plans: these office-like polygons
consist of rectangular rooms and corridors. In the dispersive variant of the
Art Gallery Problem, the objective is not to minimize the number of guards but
to maximize the minimum geodesic $L_1$-distance between any two guards, called
the dispersion distance.
  Our main contributions are as follows. We prove that determining whether a
vertex guard set can achieve a dispersion distance of $4$ in office-like
polygons is NP-complete, where vertices of the polygon are restricted to
integer coordinates. Additionally, we present a simple worst-case optimal
algorithm that guarantees a dispersion distance of $3$ in polynomial time. Our
complexity result extends to polyominoes, resolving an open question posed by
Rieck and Scheffer (CGTA 2024). When vertex coordinates are allowed to be
rational, we establish analogous results, proving that achieving a dispersion
distance of $2+\varepsilon$ is NP-hard for any $\varepsilon > 0$, while the
classic Art Gallery Problem remains solvable in polynomial time for this class
of polygons. Furthermore, we give a straightforward polynomial-time algorithm
that computes worst-case optimal solutions with a dispersion distance of $2$.
  On the other hand, for the more restricted class of hole-free independent
office-like polygons, we propose a dynamic programming approach that computes
optimal solutions. Moreover, we demonstrate that the problem is practically
tractable for arbitrary orthogonal polygons. To this end, we compare solvers
based on SAT, CP, and MIP formulations. Notably, SAT solvers efficiently
compute optimal solutions for randomly generated instances with up to $1600$
vertices in under $15$s.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [116] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Main category: math.DS

TL;DR: 提出了一种基于生成模型的结构系统识别方法，用于参数估计和验证。


<details>
  <summary>Details</summary>
Motivation: 结合实验数据与科学理论，估计复杂系统动态的方程参数，以理解和预测系统行为。

Method: 使用神经网络将随机噪声映射为物理参数，通过已知运动方程生成假加速度，并与真实数据对比；利用独立验证数据集和判别器网络验证参数。

Result: 在分析和实际实验中展示了参数估计的准确性和模型验证的有效性。

Conclusion: 该方法在非线性结构系统中实现了高精度的参数估计和模型验证。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [117] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 研究探讨了数据源多样性对加密货币预测模型性能的影响，通过整合多种数据类别，提出了一种特征降维算法，并验证了多样性对预测性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场复杂多变，传统预测模型依赖单一数据源效果有限，需探索多源数据整合以提升预测准确性。

Method: 引入Crypto100指数，整合技术指标、链上指标、情绪指标、传统市场指数和宏观经济指标，提出特征降维算法筛选关键特征。

Result: 数据源多样性显著提升预测性能，链上指标对短期和长期预测均重要，传统市场指数和宏观经济指标对长期预测更相关。

Conclusion: 多源数据整合能显著提升加密货币预测模型的准确性和稳健性，为市场驱动因素提供了新见解。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [118] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 该技术报告介绍了DCASE 2025挑战赛任务4的提交系统，通过结合额外的音频特征和标签校正系统，显著提升了音频分类性能。


<details>
  <summary>Details</summary>
Motivation: 混合音频中的细微线索难以仅通过梅尔频谱捕捉，因此需要引入额外特征和标签校正以提高分类能力。

Method: 1. 结合梅尔频谱特征与谱滚降和色度特征；2. 应用基于代理的标签校正系统减少误报；3. 优化训练数据集，去除无关样本并引入外部数据。

Result: 实验显示，该系统将CA-SDRi指标相对提升了14.7%。

Conclusion: 通过多特征融合和数据集优化，显著提升了音频分类性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [119] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文聚焦于MTEB（大规模文本嵌入基准）的工程实践，确保其可复现性和可扩展性，包括持续集成、数据集验证和社区贡献管理。


<details>
  <summary>Details</summary>
Motivation: 确保MTEB作为文本嵌入模型评估平台的可复现性和可扩展性，以维持其质量和相关性。

Method: 通过持续集成管道验证数据集完整性、自动化测试执行和结果泛化评估，同时设计增强可复现性和可用性的方案。

Result: 成功扩展MTEB，使其更全面且保持高质量，为机器学习评估框架提供实用经验。

Conclusion: 本文为基准维护者提供了确保可复现性和可用性的宝贵经验，MTEB的工程实践具有广泛参考价值。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [120] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 研究发现，尽管LLM生成的研究想法在初始阶段被认为更具新颖性，但在实际执行后，其评分显著下降，甚至低于人类专家提出的想法。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成的研究想法是否能在实际执行中产生更好的研究成果。

Method: 招募43名专家研究人员，随机分配执行LLM生成或人类专家提出的研究想法，并撰写4页短论文记录实验。所有项目由NLP专家盲审。

Result: 执行后，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性和总体评分）上评分显著下降（p < 0.05），甚至在某些指标上人类想法反超。

Conclusion: 当前LLM在生成真正有效的研究想法方面存在局限性，且缺乏执行结果时评估研究想法具有挑战性。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [121] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文探讨了通过梯度更新模拟提示效果的方法，展示了梯度下降的表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过参数更新（如微调）模拟提示的效果，以提升语言模型的泛化能力和逻辑推理能力。

Method: 提出一种元训练方法，利用梯度下降和语言模型的预测作为目标，无需真实标签。

Result: 实验表明，该方法在单次梯度更新后能部分或完全恢复提示模型的性能，并在特定任务上表现优异。

Conclusion: 结果表明梯度下降具有强大表达能力，为长上下文建模和泛化能力研究提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [122] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 该研究提出了一种轻量级方法，通过检测查询是否基于上下文文档来减少LLMs的推理时间和资源消耗，效果接近大型模型但速度更快。


<details>
  <summary>Details</summary>
Motivation: LLMs在上下文信息不足时容易生成不可靠的回答，因此需要确保回答的基于性（groundedness）。

Method: 使用轻量级编码器模型（如RoBERTa和NomicBERT）在精选数据集上微调，检测查询是否基于上下文。

Result: 这些轻量级模型在基于性检测上的准确性与Llama3 8B和GPT4o相当，但推理延迟显著降低。

Conclusion: 该方法能高效减少LLMs的资源消耗，同时保持高准确性，代码已开源。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [123] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出了一种新的偏好对齐框架，用于通过用户交互改进实时语音对话模型。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法主要针对文本语言模型，不适用于实时语音交互的复杂性（如打断、插话等）。

Method: 构建了15万对偏好数据集，结合AI反馈，利用离线对齐方法微调全双工自回归语音模型。

Result: 实验表明，通用对话反馈能显著提升语音对话模型的准确性、安全性和上下文对齐性。

Conclusion: 研究发现，动态平衡对自然实时语音对话系统至关重要。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [124] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 介绍了skLEP，首个专为评估斯洛伐克自然语言理解（NLU）模型设计的综合基准，包含九项多样化任务，并提供了模型能力的全面评估。


<details>
  <summary>Details</summary>
Motivation: 填补斯洛伐克NLU领域缺乏全面评估工具的空白，推动该领域的研究发展。

Method: 通过整理新的斯洛伐克语数据集和翻译现有英语NLU资源构建skLEP基准，并对多种语言模型进行系统评估。

Result: 发布了完整的基准数据、开源工具包和公开排行榜，促进研究的可重复性和未来发展。

Conclusion: skLEP为斯洛伐克NLU研究提供了重要工具，有望推动该领域的进一步探索。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [125] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: 提出了一种基于粒子-线程绑定策略的新方法，优化了PIC模拟中的电荷沉积（CD）子程序，显著提升了并行计算的扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 2D和3D设备级PIC模拟的高计算成本需要并行化，但传统的私有网格方法存在扩展性问题，电荷沉积（CD）子程序成为瓶颈。

Method: 采用粒子-线程绑定策略，仅需每个节点四个私有网格（分布式内存系统）或四个私有网格（共享内存系统），同时通过额外功能和标志避免线程冲突。

Result: 在共享内存和分布式内存系统（1000核）上的性能评估表明，该方法具有扩展性且硬件依赖性低。

Conclusion: 该方法显著提升了CD子程序的扩展性和性能，同时保持传统数据结构，对现有PIC代码改动极小。

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [126] [Thinning to improve two-sample discrepancy](https://arxiv.org/abs/2506.20932)
*Gleb Smirnov,Roman Vershynin*

Main category: math.PR

TL;DR: 论文提出了一种在线算法，通过丢弃少量点，将两个独立样本的差异从O(√n)降低到O(log²ᵈ n)。


<details>
  <summary>Details</summary>
Motivation: 研究如何减少从同一分布中抽取的两个独立样本之间的差异，尤其是在高维空间中。

Method: 提出了一种简单的在线算法，通过选择性丢弃部分点来实现差异的显著降低。

Result: 算法成功将差异从O(√n)降低到O(log²ᵈ n)。

Conclusion: 该方法在高维空间中有效减少了样本差异，具有实际应用潜力。

Abstract: The discrepancy between two independent samples \(X_1,\dots,X_n\) and
\(Y_1,\dots,Y_n\) drawn from the same distribution on $\mathbb{R}^d$ typically
has order \(O(\sqrt{n})\) even in one dimension. We give a simple online
algorithm that reduces the discrepancy to \(O(\log^{2d} n)\) by discarding a
small fraction of the points.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [127] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种基于混合密度网络（MDN）的方法，用于预测位错密度和应力分布，并通过统计参数改进塑性模型，实现高精度和不确定性量化的机械性能预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习在结构材料领域的应用日益重要，但现有数据整合和预测模型中的不确定性量化仍需改进。

Method: 使用混合密度网络（MDN）模型，基于大量实验数据预测位错密度分布和晶粒级应力分布，并将统计参数融入位错介导的塑性模型。

Result: 该方法显著提高了机械性能预测的准确性和可靠性，并实现了明确的不确定性量化。

Conclusion: 该方法不仅优化了合金设计，还推动了新材料在快速发展的工业中的应用。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [128] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 论文结合奥地利资本理论和重复博弈理论，分析区块链系统中矿工在不同制度条件下的策略行为，指出可变协议会提高时间偏好，破坏长期规划和合作均衡。


<details>
  <summary>Details</summary>
Motivation: 研究矿工在可变协议下的行为动机，探讨如何通过固定协议恢复战略一致性和可持续网络均衡。

Method: 采用形式化博弈论分析和奥地利经济学原理，比较可变与固定协议对矿工激励的影响。

Result: 可变协议导致矿工从生产性投资转向政治寻租和影响力博弈，而固定协议（如比特币原始协议）能降低时间偏好，增强战略一致性。

Conclusion: 协议不可变性是恢复战略一致性、企业家信心和可持续网络均衡的关键。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [129] [Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations](https://arxiv.org/abs/2506.21134)
*Jacopo Bufalino,Jose Luis Martin-Navarro,Mario Di Francesco,Tuomas Aura*

Main category: cs.CR

TL;DR: 本文分析了Kubernetes集群中网络配置对安全的影响，重点关注横向移动问题，评估了287个开源应用，发现634个配置错误，并提出了修复建议。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为容器编排的事实标准，其安全性备受关注，但网络配置对安全的影响研究较少。本文填补了这一空白。

Method: 对287个开源应用进行了全面评估，重点关注网络配置错误及其对横向移动的影响。

Result: 发现了634个配置错误，远超现有解决方案的检测能力，并与相关组织合作修复了30多个应用的问题。

Conclusion: 网络配置错误对Kubernetes安全有重大影响，需更多关注和改进。

Abstract: Kubernetes has emerged as the de facto standard for container orchestration.
Unfortunately, its increasing popularity has also made it an attractive target
for malicious actors. Despite extensive research on securing Kubernetes, little
attention has been paid to the impact of network configuration on the security
of application deployments. This paper addresses this gap by conducting a
comprehensive analysis of network misconfigurations in a Kubernetes cluster
with specific reference to lateral movement. Accordingly, we carried out an
extensive evaluation of 287 open-source applications belonging to six different
organizations, ranging from IT companies and public entities to non-profits. As
a result, we identified 634 misconfigurations, well beyond what could be found
by solutions in the state of the art. We responsibly disclosed our findings to
the concerned organizations and engaged in a discussion to assess their
severity. As of now, misconfigurations affecting more than thirty applications
have been fixed with the mitigations we proposed.

</details>


### [130] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 提出了一种隐私保护框架，结合降维技术和差分隐私，支持安全数据共享与合作，同时保护农民敏感信息。


<details>
  <summary>Details</summary>
Motivation: 解决农民因隐私问题不愿共享数据的问题，促进数据驱动农业的发展。

Method: 结合主成分分析（PCA）和拉普拉斯噪声的差分隐私技术，支持联邦学习和隐私保护数据聚合。

Result: 在真实数据集上验证了隐私保护和实用性，性能接近集中式系统。

Conclusion: 该框架有助于农民和研究者的合作，推动农业数据的安全整合与创新。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>


### [131] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Main category: cs.CR

TL;DR: ZKPROV是一种新型加密框架，用于验证大型语言模型（LLM）的数据来源，确保模型训练数据的可靠性而不泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如医疗）部署LLM时，验证其数据来源的完整性至关重要，但现有方法要么计算成本高，要么依赖可信执行环境。

Method: ZKPROV通过零知识证明将训练模型与授权数据集绑定，利用数据集签名元数据和紧凑模型参数承诺，避免验证每一步训练过程。

Result: 实验证明ZKPROV在生成和验证证明时高效且可扩展，适用于实际部署，并提供了形式化安全保障。

Conclusion: ZKPROV在保护数据集隐私的同时，确保了可信的数据来源证明，为LLM在敏感领域的应用提供了实用解决方案。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [132] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: IXAII是一个交互式可解释AI系统，整合了LIME、SHAP、Anchors和DiCE四种方法，提供定制化视图和用户控制权，提升了透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多为静态且忽视用户视角，限制了其实际效果。

Method: 开发了交互式系统IXAII，整合四种解释方法，提供定制化视图和用户控制权，并通过专家和普通用户访谈评估。

Result: IXAII通过多解释方法和可视化选项，显著提升了透明度和用户满意度。

Conclusion: IXAII为可解释AI实践和人机交互提供了新视角，解决了现有方法的局限性。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [133] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）的因果推理能力，发现其仅能进行浅层（level-1）推理，缺乏人类深层（level-2）能力。作者提出新方法G^2-Reasoner，结合通用知识和目标导向提示，显著提升LLMs的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备类似人类的真实因果推理能力，发现其局限性并提出改进方法。

Method: 通过分析LLMs的自回归机制，设计新基准CausalProbe-2024，并提出G^2-Reasoner方法，结合通用知识和目标导向提示。

Result: LLMs在CausalProbe-2024上表现显著下降，证明其仅能进行level-1推理；G^2-Reasoner显著提升了其在新鲜和反事实语境中的推理能力。

Conclusion: LLMs的因果推理能力有限，但通过结合通用知识和目标导向提示，可以迈向level-2推理，为未来研究提供新方向。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [134] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: TFT在降雨-径流建模中略优于LSTM，尤其在模拟水文过程的中段和峰值时表现更好，并能处理更长序列。TFT的可解释性提供了科学洞察，但两种模型在Caravan数据集上性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索TFT在降雨-径流建模中的表现是否优于LSTM，并评估模型在不同数据集和流域属性下的性能差异。

Method: 训练10个随机初始化的TFT和LSTM模型，应用于531个美国CAMELS流域，并在Caravan数据集的5个子集（美国、澳大利亚、巴西、英国和智利）上重复实验。评估模型性能、流域属性影响及数据集差异。

Result: TFT略优于LSTM，尤其在模拟水文过程的中段和峰值时表现更好，并能处理更长序列。TFT的可解释性有助于识别关键变量。两种模型在Caravan数据集上性能下降。

Conclusion: TFT在改进水文建模和理解方面具有潜力，但数据质量可能影响模型性能。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [135] [EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora](https://arxiv.org/abs/2506.20963)
*Fangyuan Zhang,Zhengjun Huang,Yingli Zhou,Qintian Guo,Zhixun Li,Wensheng Luo,Di Jiang,Yixiang Fang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: EraRAG是一种新型的多层Graph-RAG框架，通过分层图结构和LSH技术支持动态更新，显著提升了检索效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-RAG方法假设静态语料库，动态更新成本高，限制了在动态环境中的可扩展性。

Method: 利用基于超平面的LSH将语料库分层组织为图结构，支持局部化插入新数据，避免全局重构。

Result: 在大规模基准测试中，EraRAG的更新时间和令牌消耗减少了一个数量级，同时保持高检索精度。

Conclusion: EraRAG为持续增长的语料库提供了高效的动态更新方案，平衡了检索效率和适应性。

Abstract: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [136] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: MegaFold是一个跨平台系统，旨在加速AlphaFold3（AF3）的训练，通过缓存、高效内核和深度融合技术，显著降低了内存使用并提高了训练速度。


<details>
  <summary>Details</summary>
Motivation: AF3在蛋白质结构预测方面取得了突破，但其计算和内存密集型操作、2D注意力机制和检索增强数据管道限制了训练的可扩展性。MegaFold旨在解决这些瓶颈。

Method: MegaFold采用提前缓存减少GPU空闲时间，使用Triton内核实现内存高效的EvoAttention，并通过深度融合优化小操作符。

Result: 在NVIDIA H200和AMD MI250 GPU上，MegaFold将AF3训练的峰值内存使用降低1.23倍，训练速度分别提高1.73倍和1.62倍，并支持更长的序列训练。

Conclusion: MegaFold显著提升了现代蛋白质折叠模型的可扩展性，代码已开源。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


### [137] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Main category: q-bio.BM

TL;DR: 论文提出了一个名为CovDocker的共价对接基准，用于更好地模拟共价结合的复杂性，并展示了其在预测相互作用位点和分子转化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数对接方法和深度学习方法难以考虑共价键的形成及其相关的结构变化，因此需要一种更全面的共价对接基准。

Method: 将共价对接过程分解为三个主要任务：反应位点预测、共价反应预测和共价对接，并采用Uni-Mol和Chemformer等先进模型建立基线性能。

Result: 基准在准确预测相互作用位点和模拟共价结合中的分子转化方面表现出色，为共价药物设计提供了严格的研究框架。

Conclusion: CovDocker展示了数据驱动方法在加速选择性共价抑制剂发现中的潜力，并解决了治疗开发中的关键挑战。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [138] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 提出了首个将脉冲神经网络（SNNs）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了现有方法的空白，并展示了SNNs在能效方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数增长（如NISAR任务预计两年内生成100PB数据），能效处理对可持续数据中心运营至关重要。SNNs的事件驱动计算模型可能比传统方法节省30-100倍能源。

Method: 开发了针对缠绕相位数据的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并分析了计算复杂性和收敛性。

Result: SNNs的时间动态特性可自然建模相位解缠的空间连续性约束，为大规模InSAR处理提供了可持续的互补方法。

Conclusion: 该工作为神经形态计算与SAR干涉测量的交叉领域开辟了新研究方向，有望推动更可持续的大规模InSAR处理。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [139] [Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning](https://arxiv.org/abs/2506.21324)
*Jiechen Chen,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.NE

TL;DR: 提出了一种结合神经形态和量子计算优势的随机量子脉冲（SQS）神经元模型，解决了现有量子脉冲模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 结合神经形态计算的时间序列高效性和量子计算的指数级状态空间，以推动人工智能发展。

Method: 使用多量子比特电路实现具有内部量子记忆的脉冲单元，并通过硬件友好的局部学习规则训练SQS神经网络（SQSNN）。

Result: SQS神经元模型能够单次实现事件驱动的概率脉冲生成，且无需全局经典反向传播。

Conclusion: SQSNN模型为模块化、可扩展且可在量子硬件上训练的量子脉冲神经网络铺平了道路。

Abstract: Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [140] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: 论文提出了两个新指标（EBI和OBI）和建模框架FABRIC，首次全面分析了计算系统对生物多样性的影响。


<details>
  <summary>Details</summary>
Motivation: 生物多样性损失是一个关键问题，但计算领域对此关注不足，缺乏相关指标和框架。

Method: 引入Embodied Biodiversity Index (EBI)和Operational Biodiversity Index (OBI)指标，开发FABRIC建模框架。

Result: 评估表明，可持续计算设计需同时考虑生物多样性、碳和水。

Conclusion: 论文填补了计算领域生物多样性研究的空白，为可持续设计提供了新工具。

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [141] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Main category: math.OC

TL;DR: 该论文提出了一种新的视角，将神经网络视为偏微分方程（PDEs），并研究了抛物型和双曲型算子系数的控制与优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献对抛物型和双曲型系统的控制和优化问题已有大量研究，但对其算子系数的控制与优化问题尚未深入探讨。

Method: 提出了一种双系统公式，用于抛物型PDEs的控制与优化问题，并提供了理论证明。

Result: 证明了抛物型PDEs的控制与优化问题存在极小值，并研究了双曲型PDEs的近似控制问题的解的存在性。

Conclusion: 该研究为PDEs控制理论中的新方向奠定了基础，并为未来高效数值方案的开发提供了理论支持。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


### [142] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Main category: math.OC

TL;DR: 研究了多链马尔可夫决策过程（MDPs）的平均奖励准则下的值迭代（VI）算法，提出了改进的导航子问题解决方法，提高了收敛速度和复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 解决多链MDPs中导航子问题的挑战，以提高收敛速度和优化性能。

Method: 开发了改进的VI算法，结合平均奖励与折扣问题的联系，扩展了固定点方法到一般Banach空间。

Result: 获得了更快的收敛速度和更精确的复杂度度量，扩展了VI的理论基础。

Conclusion: 研究成果在多链MDPs中实现了更快的收敛，并为VI方法提供了新的理论基础。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [143] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Main category: q-bio.CB

TL;DR: scMamba是一种无需特征选择即可整合单细胞多组学数据的基础模型，通过基因组位置信息保留和对比学习方法显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞多组学技术的发展需要更高效的数据整合方法，避免因特征选择丢失关键生物信息。

Method: scMamba采用基于补丁的细胞标记化策略，将基因组区域视为标记，细胞视为句子，并结合对比学习和余弦相似性正则化。

Result: scMamba在多个数据集上显著优于现有方法，提升了聚类、细胞类型注释和轨迹推断等下游任务。

Conclusion: scMamba是处理大规模单细胞多组学数据的有力工具，推动了生物学发现。

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [144] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Main category: stat.ML

TL;DR: 本文通过解析方法解决了关于多元拟连接函数质量分布极值的开放问题，并推翻了相关猜想。


<details>
  <summary>Details</summary>
Motivation: 尽管拟连接函数缺乏统计解释，但其在依赖建模社区中的重要性日益增加，因此需要解决相关问题。

Method: 采用解析方法，结合先前线性规划的结果，解决了开放问题。

Result: 完全回答了原始问题，并推翻了最近的一个猜想。

Conclusion: 解析方法成功解决了拟连接函数质量分布的极值问题，为相关研究提供了新见解。

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [145] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Main category: stat.ML

TL;DR: 论文研究了平坦性/低曲率在两层过参数化ReLU网络中的隐式偏差及其对泛化的影响，发现平坦解在高维输入下泛化性能会指数级恶化。


<details>
  <summary>Details</summary>
Motivation: 研究平坦解在梯度下降训练中的稳定性及其对泛化的影响，填补现有工作仅关注单变量输入或需要插值的空白。

Method: 通过理论分析，针对多变量输入，在平坦解的泛化差距和非参数函数估计的MSE上，提出上下界证明。

Result: 平坦解虽能泛化，但随着输入维度增加，收敛速率指数级恶化，与低范数解形成鲜明对比。

Conclusion: 平坦解在高维下可能因神经元激活稀疏但权重高而导致性能下降，首次系统解释了平坦解在高维中泛化失败的原因。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [146] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Main category: stat.ML

TL;DR: 提出了一种结合流形学习和主动学习的高斯过程回归框架，以提高高维空间中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中复杂、不连续函数的回归问题，提升预测精度。

Method: 联合优化降维神经网络和潜在空间中的高斯过程回归器，通过主动学习准则最小化全局预测误差。

Result: 在合成数据上表现优于随机顺序学习，高效处理复杂函数且保持计算可行性。

Conclusion: 框架具有科学和工程应用价值，未来将关注可扩展性和不确定性感知的流形学习。

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [147] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Main category: stat.ML

TL;DR: 论文研究了因果发现算法中马尔可夫等价类的大小问题，发现放宽假设会导致等价类规模指数级增长。


<details>
  <summary>Details</summary>
Motivation: 探讨在放宽假设条件下，马尔可夫等价类的规模变化，揭示因果发现算法的局限性。

Method: 通过理论分析，证明了在稀疏随机有向无环图、均匀随机有向混合图和均匀随机有向循环图三种设置下，等价类规模的指数级下界。

Result: 在放宽假设后，马尔可夫等价类的期望规模呈指数级增长。

Conclusion: 研究结果表明，因果发现算法的局限性在放宽假设时更为显著，等价类规模大幅增加。

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [148] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang,Hayden Hampton*

Main category: stat.ML

TL;DR: STFT-VNNGP是一种混合架构，结合了TFT和VNNGP，用于解决地缘政治冲突预测中的稀疏性和突发性问题，在2023年ATD竞赛中获胜。


<details>
  <summary>Details</summary>
Motivation: 解决地缘政治冲突预测中因数据稀疏性、突发性和过度分散导致的标准深度学习模型不可靠的问题。

Method: 采用两阶段方法：TFT捕捉复杂时间动态生成多分位数预测，VNNGP进行时空平滑和不确定性量化。

Result: 在中东和美国的冲突动态预测中，STFT-VNNGP表现优于单独TFT，尤其在长期预测中更准确。

Conclusion: 该模型为从挑战性事件数据中生成更可靠情报提供了框架，代码公开以确保可复现性。

Abstract: Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [149] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

Main category: stat.ML

TL;DR: 论文提出了一种新方法，通过调整学习率和更新频率来近似多智能体强化学习在有限状态马尔可夫游戏中的动态。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在复杂环境中的动态行为，提供一种可处理的确定性近似方法。

Method: 通过同时降低学习率和增加更新频率，将智能体参数视为慢变量，受快速混合的游戏状态影响。

Result: 证明了在温和假设下，该方法收敛到一个常微分方程（ODE），提供了学习动态的确定性近似。

Conclusion: 该方法为多智能体强化学习的动态行为提供了理论支持和实用工具。

Abstract: This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [150] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Main category: stat.ML

TL;DR: 提出了一种新型变分自编码器（VAE）架构，采用球形柯西（spCauchy）潜在分布，优于传统高斯或vMF分布，提供更自然的超球面表示，避免数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统高斯或vMF分布在表示方向性数据时存在局限性，且vMF存在数值不稳定问题，需要更灵活且稳定的替代方案。

Method: 使用球形柯西分布作为潜在变量表示，通过Möbius变换实现高效可微分的重参数化，KL散度通过快速收敛的幂级数计算。

Result: spCauchy在保持灵活性的同时，避免了过正则化，提供了更高效的潜在空间利用和更稳定的训练过程。

Conclusion: spCauchy是VAE的一种有吸引力的替代方案，兼具理论优势和高维生成建模的实际效率。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [151] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

Main category: stat.ML

TL;DR: 提出了一种高效的“wild refitting”方法，用于计算惩罚非参数估计的实例均方预测误差的高概率上界，仅需单数据集和预测方法的黑盒访问。


<details>
  <summary>Details</summary>
Motivation: 为惩罚非参数估计提供一种高效且通用的预测误差上界计算方法，适用于噪声异质性和多种应用场景。

Method: 通过计算残差、对称化和缩放残差（使用预因子ρ），并重新定义和求解以当前估计为中心的修改预测问题。

Result: 在较温和条件下，证明了该方法的高概率性能保证，表明选择合适的ρ能提供预测误差的上界。

Conclusion: 该方法适用于多种问题，如非刚性运动恢复、图像修复和核方法随机草图，为设计类似程序提供了理论指导。

Abstract: We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [152] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias,Angelos Alexopoulos,Siran Liu,Petros Dellaportas*

Main category: stat.ML

TL;DR: 论文提出了基于高斯不变性的采样方法（RWM、MALA和二阶MALA），展示了其在统计效率上的改进，并通过理论分析和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统RWM和MALA方法在统计效率上存在不足，高斯不变性为解决这一问题提供了新思路。

Method: 开发了高斯不变版本的RWM、MALA和二阶MALA，利用高斯不变性解析泊松方程，构建高效控制变量。

Result: 新方法在高维潜在高斯模型中表现优异，优于现有先进方法，并提供了几何遍历性和最优接受率的理论分析。

Conclusion: 高斯不变性采样方法显著提升了统计效率，适用于复杂目标分布，具有理论和实践价值。

Abstract: We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [153] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Main category: quant-ph

TL;DR: 提出了一种混合量子-经典强化学习框架用于台湾股市板块轮动，发现量子增强模型在训练奖励上表现更好，但在实际投资指标上不如经典模型。


<details>
  <summary>Details</summary>
Motivation: 探索量子强化学习在金融领域的应用潜力，解决板块轮动问题。

Method: 结合PPO算法、经典架构（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA），并采用自动化特征工程。

Result: 量子模型训练奖励更高，但实际投资表现（累计收益、夏普比率）不如经典模型。

Conclusion: 当前奖励设计可能导致过拟合短期波动，量子电路在NISQ约束下的不稳定性和表达性加剧了问题，未来需改进奖励设计、模型正则化和验证方法。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [154] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 研究探讨了如何利用合成数据学习适用于真实数据的解耦表示，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解耦表示学习在真实图像上尚未充分发挥潜力，主要由于生成因素的关联性、分辨率限制及缺乏真实标签。

Method: 利用合成数据学习解耦表示，分析微调效果及解耦特性在迁移中的保留情况，并提出新的可解释干预度量标准。

Result: 实验表明，从合成数据迁移到真实数据的解耦表示是可行且有效的。

Conclusion: 解耦表示的部分特性可以在迁移中保留，为真实数据提供有效表示。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [155] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种逐步生成视频到音频的方法，通过分步生成特定声音事件的音频轨道，模拟传统Foley工作流程。


<details>
  <summary>Details</summary>
Motivation: 旨在全面捕捉视频中所有声音事件，并通过条件生成提升音频合成的质量。

Method: 采用分步视频到音频合成任务，结合目标文本提示和先前生成的音频轨道，利用预训练模型避免对专业配对数据的依赖。

Result: 实验表明，该方法能为单个视频生成多个语义不同的音频轨道，合成质量优于现有基线。

Conclusion: 该方法在视频到音频生成中表现出色，为复合音频合成提供了高效解决方案。

Abstract: We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [156] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 论文提出了一种经典-量子融合的生成对抗网络（GAN），用于生成彩色医学图像，解决了量子图像生成的低质量和灰度问题，并在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病诊断的机器学习需要大量高质量数据，但现有数据集存在类别不平衡、隐私问题和对象偏差等问题。量子计算虽具潜力，但现有量子图像生成方法只能生成低质量灰度图像。

Method: 通过一种新颖的经典-量子潜在空间融合技术，构建了首个能生成彩色医学图像的经典-量子GAN。

Result: 模型在图像生成质量和分类性能提升上优于经典深度卷积GAN和现有混合经典-量子GAN，且参数和训练周期大幅减少。

Conclusion: 该研究展示了量子图像生成的潜力，尤其是在量子硬件进步后，有望成为高效的数据增强工具。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.

</details>


### [157] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 提出了一种名为FGS的方法，通过忠实性指导和调度策略，在图像编辑中平衡可编辑性和忠实性，实现高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型在图像编辑中存在可编辑性与忠实性之间的权衡问题，需要一种方法在保持可编辑性的同时增强忠实性。

Method: FGS结合忠实性指导以保留输入图像信息，并引入调度策略解决可编辑性与忠实性之间的不对齐问题。

Result: 实验表明，FGS在保持可编辑性的同时显著提升了忠实性，且兼容多种编辑方法。

Conclusion: FGS为图像编辑提供了一种高效平衡可编辑性和忠实性的解决方案，适用于多样化任务。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [158] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: EgoAdapt框架通过跨模态蒸馏和策略学习，显著提升了多感知自我中心任务的效率，同时保持或超越现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现代感知模型在多感知自我中心任务中表现优异，但计算成本高，难以在资源受限环境中部署。

Method: 提出EgoAdapt框架，结合跨模态蒸馏和策略学习，适应不同任务的动作空间。

Result: 在三个数据集上测试，EgoAdapt显著降低计算量（GMACs减少89.09%）、参数（减少82.02%）和能耗（降低9.6倍），性能与或优于现有模型。

Conclusion: EgoAdapt为资源受限环境提供了一种高效的多感知自我中心任务解决方案。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [159] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: FlickCD是一种轻量级遥感变化检测模型，通过增强差异模块和多尺度特征融合，在保持高精度的同时显著降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型复杂度高且计算资源需求大，但精度提升有限。研究旨在开发高效轻量级模型，适用于卫星端处理。

Method: 提出FlickCD模型，包含增强差异模块（EDM）和局部-全局融合块（SWSA与EGSA），以多尺度捕捉语义信息。

Result: 在四个基准数据集上，FlickCD计算和存储开销降低一个数量级，性能达到或接近SOTA水平（F1损失<1%）。

Conclusion: FlickCD在性能与资源消耗间取得平衡，适用于资源受限的遥感变化检测场景。

Abstract: Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.

</details>


### [160] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 论文提出了一种用于地下矿工检测的新型热成像数据集，并评估了多种目标检测算法，为未来紧急救援应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 地下采矿作业面临重大安全挑战，需要可靠的矿工检测能力以支持机器人辅助的搜救行动，但目前缺乏适用于地下环境的数据集。

Method: 通过系统采集各种采矿活动和场景的热成像数据，创建了专用数据集，并评估了YOLOv8、YOLOv10、YOLO11和RT-DETR等算法的性能。

Result: 研究证明了热成像用于矿工检测的可行性，并提供了基线性能指标。

Conclusion: 该数据集是开发可靠热成像矿工检测系统的关键第一步，为未来研究奠定了基础。

Abstract: Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.

</details>


### [161] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 论文提出了动态、静态和混合三种基于转向运动计数（TMC）的交通信号配置方法，并通过仿真实验验证了其在不同交通模式下的性能。


<details>
  <summary>Details</summary>
Motivation: 转向运动计数数据对交通信号设计、交叉口规划和拥堵分析至关重要，但现有方法未能充分适应不同交通模式的需求。

Method: 开发了基于视觉的跟踪系统估计TMC，结合仿真工具SUMO评估信号配置性能，提出动态、静态和混合三种方法。

Result: 实验表明，90和120秒的信号周期效果最佳；动态配置在四个交叉口表现更好，混合方法在高峰和非高峰时段适应性更强。

Conclusion: 混合信号方法能根据交通模式动态调整，适用于不均匀交通分布，尤其是东西和南北交叉口对。

Abstract: The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.

</details>


### [162] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 论文提出了一种基于空间统计的目标检测模型，解决了现有模型在未检测区域不确定性量化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型的置信度估计不准确，且无法量化未检测区域的不确定性，这在自动驾驶等应用中存在安全隐患。

Method: 采用空间统计方法，将边界框数据建模为标记点过程，通过基于似然的训练提供明确的置信度估计。

Result: 通过校准评估和性能测试验证了方法的有效性。

Conclusion: 提出的统计框架为目标检测提供了更可靠的置信度估计，特别是在未检测区域的评估上。

Abstract: Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.

</details>


### [163] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 论文提出了一种改进的跨模态图像-文本检索方法，通过最大化嵌入集间的一对一匹配和引入两种损失函数，解决了稀疏监督和集合坍缩问题，并在MS-COCO和Flickr30k上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统单向量嵌入方法难以捕捉跨模态的多样关联，而基于集合的方法虽能捕捉更丰富关系，但仍面临稀疏监督和集合坍缩问题。

Method: 提出Maximal Pair Assignment Similarity优化嵌入集间的一对一匹配，并引入Global Discriminative Loss和Intra-Set Divergence Loss增强表示。

Result: 在MS-COCO和Flickr30k上实现了最先进的性能，无需依赖外部数据。

Conclusion: 通过优化嵌入集匹配和引入损失函数，有效提升了跨模态检索的性能和多样性。

Abstract: Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.

</details>


### [164] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: HalluSegBench是一个新的基准测试，用于通过反事实视觉推理评估视觉语言分割模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有评估协议主要关注标签或文本幻觉，而忽略了视觉上下文的操纵，无法全面诊断关键失败。

Method: 提出了HalluSegBench，包含1340个反事实实例对和新的量化指标。

Result: 实验表明，视觉驱动的幻觉比标签驱动的更普遍，模型常持续错误分割。

Conclusion: 反事实推理对诊断视觉语言分割模型的真实性至关重要。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [165] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 训练模型通过人体动作预测第一人称视角视频（PEVA），基于过去视频和3D身体姿态动作，利用扩散变换器学习模拟人类动作对环境的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何从第一人称视角模拟人类动作对环境的影响，解决复杂现实环境和具身行为的建模挑战。

Method: 使用自回归条件扩散变换器，基于大规模真实世界第一人称视频和身体姿态数据集Nymeria进行训练。

Result: 设计了分层评估协议，全面分析模型的预测和控制能力。

Conclusion: 初步尝试从人类视角建模复杂现实环境和具身行为，为视频预测领域提供新思路。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [166] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Main category: stat.ME

TL;DR: 提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，性能优于传统统计方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有时空维度，而现有方法在性能和泛化能力上存在局限。

Method: 使用Transformer构建框架，提出一致且渐近正态的估计器。

Result: 模拟实验显示优于基线方法；真实数据实验分析了冲突对哥伦比亚森林流失的因果效应。

Conclusion: 新框架在时空反事实估计中表现优异，具有实际应用价值。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的U-R-Veda模型，用于心脏磁共振图像的语义分割，结合了卷积变换、视觉变换器、残差连接和注意力机制，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 自动化准确的心脏图像分割是心脏疾病诊断和管理的关键步骤，现有方法在精度和信息保留方面存在不足。

Method: U-R-Veda模型整合了卷积变换、视觉变换器、残差链接、通道和空间注意力机制，以及基于边缘检测的跳跃连接，以减少信息损失并提升特征提取能力。

Result: 模型在DSC指标上达到平均95.2%的准确率，优于其他模型，尤其在右心室和左心室心肌的分割上表现突出。

Conclusion: U-R-Veda模型通过多模块结合显著提升了心脏磁共振图像的语义分割精度，为医疗图像分析提供了更可靠的自动化工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [168] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究了3D多模态大语言模型（MLLMs）在放射学报告生成（RRG）中的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术，并提出了两种基于知识的报告增强方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用3D MLLMs自动化生成放射学报告，并优化其设计以提高性能。

Method: 研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、LLMs选择和微调技术，并引入了两种知识增强方法。

Result: 在AMOS-MM数据集上，性能提升了10%，并在MICCAI 2024挑战赛中排名第二。结果显示RRG性能与LLM大小无关，且更大的体积并不总是提升性能。

Conclusion: 3D MLLMs在RRG中具有潜力，但需注意视觉输入和模型设计的匹配性。代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [169] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: 该论文提出了一种加权深度多项式逼近方法，用于处理具有不对称行为的函数，优于传统多项式逼近方法。


<details>
  <summary>Details</summary>
Motivation: 传统有理逼近方法对非光滑或奇异函数有高效逼近能力，但多项式逼近仅能实现代数收敛。近期研究表明复合多项式结构可实现指数收敛，但需要针对不对称行为设计更优方法。

Method: 引入加权深度多项式逼近框架，通过可学习的深度多项式与单侧权重结合，捕捉局部非光滑性和全局增长性。采用基于图的参数化策略进行优化。

Result: 数值实验表明，该方法在相同参数数量下优于泰勒、切比雪夫和标准深度多项式逼近。

Conclusion: 加权深度多项式逼近方法在处理不对称行为函数时表现优越，为相关领域提供了新的工具。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>
