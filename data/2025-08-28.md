<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.CR](#cs.CR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.GT](#cs.GT) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.CG](#cs.CG) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [nucl-th](#nucl-th) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: 本文研究了图数据库管理系统中递归连接查询的多核并行处理，提出了基于不同粒度morsel分配策略的设计空间，并开发了一种混合策略来优化并行性能。


<details>
  <summary>Details</summary>
Motivation: 现有的morsel驱动并行方法在图数据库中存在局限性，要么在源节点级别并行化，要么在边界级别并行化，需要找到更有效的并行策略来提升递归查询性能。

Method: 提出了morsel分配策略的设计空间，实现了混合策略（同时使用源节点和边界级别的morsel）和多源morsel策略，并在Kuzu GDBMS系统中进行了实现和评估。

Result: 混合策略能够在不同情况下捕获源morsel-only和边界morsel-only策略的优势，并在它们受限时表现更优；多源分配在查询有足够源节点时能减少扫描量。

Conclusion: 混合morsel分配策略是并行化递归查询的稳健方法，多源分配在适当条件下能带来性能收益，为图数据库的并行处理提供了有效解决方案。

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [2] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 利用生成式AI和LLM技术生成高质量合成SQL查询数据集，用于训练学习型成本模型，相比竞争方法减少45%查询量即可达到相同预测精度


<details>
  <summary>Details</summary>
Motivation: 需要真实的数据库工作负载来进行压力测试、漏洞测试以及成本和性能优化，但获取多样化SQL查询数据集存在困难

Method: 采用现代合成数据生成技术，借鉴生成式AI和LLM社区的方法，创建高质量SQL查询数据集

Result: 训练学习型成本模型时，使用该方法生成的查询比竞争方法少45%即可达到相同的预测准确性

Conclusion: 基于生成式AI的合成数据生成技术能够有效支持学习型成本模型的训练，显著减少所需查询数量

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: HAP是一种动态混合并行策略，通过分层分解MoE架构和整数线性规划优化，显著提升MoE模型推理效率


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型推理系统采用静态并行策略，无法适应不同推理场景的计算需求变化，缺乏灵活性

Method: 将MoE架构分层分解为Attention模块和Expert模块，建立专门的推理延迟模拟模型，利用整数线性规划(ILP)求解最优混合并行配置

Result: 在A100、A6000和V100 GPU平台上分别实现1.68x、1.77x和1.57x的加速比，性能优于主流TP策略

Conclusion: HAP方法具有出色的泛化能力，在Mixtral和Qwen系列等多种MoE模型配置下均能保持性能有效性

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [4] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 本文对Algorand共识协议进行了形式化建模和验证，使用概率进程演算分析其在无对抗和恶意节点攻击下的行为，通过CADP工具验证协议的安全性和局限性。


<details>
  <summary>Details</summary>
Motivation: Algorand是一个可扩展的安全无许可区块链，采用密码学自排序和二进制拜占庭共识。为了进行严格的形式化验证，需要建立其共识协议的代数模型。

Method: 使用概率进程演算对Algorand共识协议进行建模，捕获参与者行为和各共识步骤的结构化交替。通过CADP验证工具包实现等价检查的非干扰框架来分析对抗场景。

Result: 验证了协议在无对抗情况下的正确性，分析了协调恶意节点强制提交空块而非提议块的影响，揭示了协议在对抗假设下的鲁棒性和局限性。

Conclusion: 这项工作展示了形式化方法在分析区块链共识算法中的附加价值，为Algorand协议的安全验证提供了理论基础和实用工具。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [5] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: 本文探讨了生成式AI如何作为6G网络实现环境智能(AmI)的核心技术，通过生成合成数据、语义通信、预测网络状态和更新数字孪生等功能，将6G从快速网络转变为智能环境生态系统。


<details>
  <summary>Details</summary>
Motivation: 实现全球规模的环境智能需要6G网络具备实时感知、推理和行动能力，而传统AI无法完全满足这些需求，因此需要利用生成式AI来填补关键的技术空白。

Method: 回顾了生成式AI的基础模型（GANs、VAEs、扩散模型和生成式变换器），并将其与实际的AmI用例相结合，包括频谱共享、超可靠低延迟通信、智能安全和情境感知数字孪生。

Result: 研究表明生成式AI能够有效解决AmI中的关键问题，如生成合成传感器数据、翻译用户意图、预测网络条件和隐私保护的数字孪生更新。

Conclusion: 生成式AI不是外围补充，而是将6G从快速网络转变为环境智能生态系统的基础要素，但仍需解决能效、可信合成数据、联邦生成学习和标准化等开放挑战。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [6] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale是针对大语言模型服务的协调自动扩缩框架，专门解决Prefill-Decode分离架构中的资源管理挑战，显著提升GPU利用率和节省资源


<details>
  <summary>Details</summary>
Motivation: 传统自动扩缩器在现代Prefill-Decode分离架构中表现不佳，存在异构硬件利用效率低、网络瓶颈和阶段间负载不平衡等核心问题

Method: 结合拓扑感知调度器（适应异构硬件和网络约束）和基于大规模实证研究的新型指标驱动策略，使用单一稳健指标联合扩缩prefill和decode资源池

Result: 在数万GPU的大规模生产环境中部署，平均GPU利用率提升26.6个百分点，每日节省数十万GPU小时，同时满足严格的服务等级目标

Conclusion: HeteroScale通过协调的自动扩缩方法有效解决了P/D分离架构的运营挑战，实现了高效的资源管理和架构平衡

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [7] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 本文分析了IOMMU在异构计算平台中的性能干扰问题，发现IOMMU共享结构会导致不可预测的延迟，特别是对小内存事务影响显著，DMA传输延迟最高可达1.79倍。


<details>
  <summary>Details</summary>
Motivation: 随着混合关键性系统集成异构计算平台，加速器和DMA设备作为独立总线主控直接访问内存，需要确保安全性和时序可预测性。IOMMU在调节内存访问中起关键作用，但其性能干扰作用尚未充分研究。

Method: 使用Xilinx UltraScale+ ZCU104平台分析IOMMU结构中的争用效应，研究共享TLB、缓存效应和转换开销等架构特性如何引入时序不可预测性。

Result: 实验表明IOMMU引起的干扰主要影响小内存事务，转换开销显著影响执行时间。在Arm SMMUv2实现中，IOMMU干扰可使DMA事务延迟高达1.79倍（小规模传输）。

Conclusion: IOMMU共享结构会引入不可预测的延迟，特别是在小内存事务中表现明显。不同架构的IOTLB争用效应由于共享缓存原理（如预取和分层TLB结构）可能表现出相似行为。

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [8] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 本文通过引入新问题（ETE、HET、TAR(d)*等）分析了移动机器人系统中机器人能力、灯光可观测性和调度器同步性之间的复杂交互关系，扩展了14种典型机器人模型的分离映射。


<details>
  <summary>Details</summary>
Motivation: 理解移动机器人系统的计算能力是分布式计算中的基本挑战。先前工作主要关注模型间的两两分离，本文旨在探索机器人能力、灯光可观测性和调度器同步性之间更复杂的交互方式。

Method: 通过定义新的计算问题（ETE、HET、TAR(d)*等）并进行分类分析，研究在不同同步设置下内部内存和灯光的交互作用，特别关注弱同步和异步环境下的能力分离。

Result: 发现ETE问题只能在最强模型（完全同步+全互见灯光）中解决；在弱同步下内部内存不足，而完全同步可替代灯光和内存；在异步设置中揭示了FSTA和FCOM机器人之间的细粒度分离。

Conclusion: 研究结果揭示了仅通过高阶比较才能看到的结构现象，提供了新的不可能性标准，并深化了对可观测性、内存和同步性如何共同塑造移动机器人计算能力的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [9] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 本文提出了首个将调度与数字孪生技术集成的高性能计算框架，支持在部署前进行参数配置和调度决策的假设分析，评估对物理资产的影响。


<details>
  <summary>Details</summary>
Motivation: 传统调度器评估方法局限于部署后分析或模拟器，无法模拟相关基础设施。需要一种能够在部署前评估调度决策对物理系统影响的方法。

Method: 开发了首个扩展调度能力的数字孪生框架，集成多个顶级HPC系统数据集，实现外部调度模拟器集成，支持激励结构和基于机器学习的调度评估。

Result: 构建了一个数字孪生元框架，支持HPC系统的假设场景分析，能够评估可持续性和对模拟系统的影响。

Conclusion: 该工作为HPC调度提供了创新的数字孪生方法，实现了在部署前进行调度决策评估的能力，为系统可持续性和性能优化提供了新工具。

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Efficiently Coloring the Intersection of a General Matroid and Partition Matroids](https://arxiv.org/abs/2508.19473)
*Stephen Arndt,Benjamin Moseley,Kirk Pruhs,Michael Zlatin*

Main category: cs.DS

TL;DR: 提出了一个多项式时间算法，用于计算一般拟阵与k-1个划分拟阵交的着色问题，使用最多1+∑(χ(M_i)-1)种颜色，这是首个多项式时间O(1)近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决拟阵交着色问题，特别是在其中一个拟阵是一般拟阵时的多项式时间近似算法需求。

Method: 设计多项式时间算法，处理一般拟阵和划分拟阵的交集，通过颜色分配策略实现近似着色。

Result: 算法能够使用最多1+∑(χ(M_i)-1)种颜色完成着色，对于标准组合拟阵类型也能提供多项式时间O(1)近似。

Conclusion: 该算法是首个处理一般拟阵与划分拟阵交着色问题的多项式时间O(1)近似算法，具有重要的理论和实际意义。

Abstract: This paper shows a polynomial-time algorithm, that given a general matroid
$M_1 = (X, \mathcal{I}_1)$ and $k-1$ partition matroids $ M_2, \ldots, M_k$,
produces a coloring of the intersection $M = \cap_{i=1}^k M_i$ using at most
$1+\sum_{i=1}^k \left(\chi(M_i) -1\right)$ colors. This is the first
polynomial-time $O(1)$-approximation algorithm for matroid intersection
coloring where one of the matroids may be a general matroid. Leveraging the
fact that all of the standard combinatorial matroids reduce to partition
matroids at a loss of a factor of two in the chromatic number, this algorithm
also yields a polynomial-time $O(1)$-approximation algorithm for matroid
intersection coloring in the case where each of the matroids $ M_2, \ldots,
M_k$ are one of the standard combinatorial types.

</details>


### [11] [An Optimal Sorting Algorithm for Persistent Random Comparison Faults](https://arxiv.org/abs/2508.19785)
*Barbara Geissmann,Stefano Leucci,Chih-Hung Liu,Paolo Penna*

Main category: cs.DS

TL;DR: 提出了第一个在持久性比较错误模型下时间复杂度为O(n log n)的排序算法，保证最大错位O(log n)和总错位O(n)，当错误概率p<1/4时以高概率成立


<details>
  <summary>Details</summary>
Motivation: 解决在持久性随机比较错误模型下的排序问题，其中每次比较都有固定概率出错，且相同元素对的比较结果始终一致。完美排序在此模型下不可能，目标是最小化输出序列中元素的错位

Method: 开发了时间复杂度为O(n log n)的排序算法，通过解决两个相关子问题：在几乎有序序列中快速插入新元素并控制错位，以及降低近似排序序列的错位

Result: 算法在p<1/4时以高概率保证最大错位O(log n)和总错位O(n)，证明了比较错误不会增加排序的计算难度，同时证明了无法保证o(log n)最大错位或o(n)总错位

Conclusion: 该工作解决了持久性比较错误模型下排序的时间复杂度问题，表明在此错误范围内排序的计算难度与无错误情况相同，均为Θ(n log n)

Abstract: We consider the problem of sorting $n$ elements subject to persistent random
comparison errors. In this problem, each comparison between two elements can be
wrong with some fixed (small) probability $p$, and comparing the same pair of
elements multiple times always yields the same result. Sorting perfectly in
this model is impossible, and the objective is to minimize the dislocation of
each element in the output sequence, i.e., the difference between its position
in the sequence and its true rank.
  In this paper, we present the first $O(n\log n)$-time sorting algorithm that
guarantees both $O(\log n)$ maximum dislocation and $O(n)$ total dislocation
with high probability when $p<\frac{1}{4}$. This settles the time complexity
sorting with persistent comparison errors in the given range of $p$ and shows
that comparison errors do not increase its computational difficulty. Indeed,
$\Omega(n\log n)$ time is necessary to archive a maximum dislocation of $O(\log
n)$ even without comparison errors. Moreover, we prove that no algorithm can
guarantee a maximum dislocation of $o(\log n)$ with high probability, nor a
total dislocation of $o(n)$ in expectation.
  To develop our sorting algorithm, we solve two related sub-problems, which
might be of independent interest. More precisely, we show that $O(\log n)$ time
suffices to find a position in which to insert a new element $x$ in an
almost-sorted sequence $S$ of $n$ elements having dislocation at most
$d=\Omega(\log n)$, so that the dislocation of $x$ in the resulting sequence is
$O(d)$ with high probability (which can be equivalently thought as the problem
of estimating the rank of $x$ in $S$). We also show that the maximum (resp.
total) dislocation of an approximately sorted sequence $S$ of $n$ elements can
be lowered to $O(\log n)$ (resp. $O(n)$) in $O(nd)$ time, w.h.p., where $d$ is
an upper bound on the maximum dislocation of $S$.

</details>


### [12] [Optimizing Wiggle in Storylines](https://arxiv.org/abs/2508.19802)
*Alexander Dobler,Tim Hegemann,Martin Nöllenburg,Alexander Wolff*

Main category: cs.DS

TL;DR: 本文研究故事线可视化中的wiggle最小化问题，证明wiggle计数最小化是NP完全问题，并提出线性wiggle高度最小化和二次wiggle高度最小化的高效算法，以及新的字符曲线路由方法。


<details>
  <summary>Details</summary>
Motivation: 故事线可视化中现有研究主要关注交叉点最小化，但对wiggle（字符随时间垂直移动量）这一重要质量准则研究较少，需要系统研究wiggle最小化问题。

Method: 1) 证明wiggle计数最小化的NP完全性；2) 基于数学规划开发线性wiggle高度最小化和二次wiggle高度最小化算法；3) 提出保持相邻曲线距离恒定的新路由方法。

Result: 实现了所提算法，通过案例研究比较三种优化目标，使用现有基准数据并引入铁路运营中机车车辆时刻表可视化的新应用场景。

Conclusion: 本文系统研究了故事线可视化中的wiggle最小化问题，提供了理论分析和实用算法，拓展了故事线在铁路运营可视化等新领域的应用。

Abstract: A storyline visualization shows interactions between characters over time.
Each character is represented by an x-monotone curve. Time is mapped to the
x-axis, and groups of characters that interact at a particular point $t$ in
time must be ordered consecutively in the y-dimension at $x=t$. The predominant
objective in storyline optimization so far has been the minimization of
crossings between (blocks of) characters. Building on this work, we investigate
another important, but less studied quality criterion, namely the minimization
of wiggle, i.e., the amount of vertical movement of the characters over time.
Given a storyline instance together with an ordering of the characters at any
point in time, we show that wiggle count minimization is NP-complete. In
contrast, we provide algorithms based on mathematical programming to solve
linear wiggle height minimization and quadratic wiggle height minimization
efficiently. Finally, we introduce a new method for routing character curves
that focuses on keeping distances between neighboring curves constant as long
as they run in parallel. We have implemented our algorithms, and we conduct a
case study that explores the differences between the three optimization
objectives. We use existing benchmark data, but we also present a new use case
for storylines, namely the visualization of rolling stock schedules in railway
operation.

</details>


### [13] [Distributed Sparsest Cut via Eigenvalue Estimation](https://arxiv.org/abs/2508.19898)
*Yannic Maus,Tijn de Vos*

Main category: cs.DS

TL;DR: 提出了在CONGEST模型中近似稀疏割值（图传导率φ）的新改进算法，运行时间为O(log²n/φ)轮，输出值满足φ ≤ φ̃ ≤ √2.01φ，显著优于现有最快算法


<details>
  <summary>Details</summary>
Motivation: 解决分布式计算模型中图稀疏割近似问题，提供更高效、更精确的算法，同时简化实现复杂度

Method: 基于归一化拉普拉斯矩阵特征值近似，使用幂方法（power method）进行迭代计算，每次迭代可在CONGEST模型中单轮完成

Result: 算法在O(log²n/φ)轮内收敛，输出近似值φ̃满足φ ≤ φ̃ ≤ √2.01φ，适用于加权无向图，并可推广到k-way传导率

Conclusion: 该方法相比现有基于扩展器分解的技术更简单易实现，在大多数情况下显著提升了稀疏割近似算法的性能

Abstract: We give new, improved bounds for approximating the sparsest cut value or in
other words the conductance $\phi$ of a graph in the CONGEST model. As our main
result, we present an algorithm running in $O(\log^2 n/\phi)$ rounds in which
every vertex outputs a value $\tilde \phi$ satisfying $\phi \le \tilde \phi \le
\sqrt{2.01\phi}$. In most regimes, our algorithm improves significantly over
the previously fastest algorithm for the problem [Chen, Meierhans, Probst
Gutenberg, Saranurak; SODA 25]. Additionally, our result generalizes to $k$-way
conductance.
  We obtain these results, by approximating the eigenvalues of the normalized
Laplacian matrix $L:=I-\rm{Deg}^{-1/2}A\rm{Deg}^ {-1/2}$, where, $A$ is the
adjacency matrix and $\rm{Deg}$ is the diagonal matrix with the weighted
degrees on the diagonal. The previous state of the art sparsest cut algorithm
is in the technical realm of expander decompositions. Our algorithms, on the
other hand, are relatively simple and easy to implement. At the core, they rely
on the well-known power method, which comes down to repeatedly multiplying the
Laplacian with a vector. This operation can be performed in a single round in
the CONGEST model. All our algorithms apply to weighted, undirected graphs. Our
lower bounds apply even in unweighted graphs.

</details>


### [14] [Bipartite Matching with Pair-Dependent Bounds](https://arxiv.org/abs/2508.20002)
*Shaul Rosner,Tami Tamir*

Main category: cs.DS

TL;DR: 研究二分图PD匹配问题，其中每个机器可分配的作业数量取决于具体匹配的作业，目标是最大化匹配规模。分析了计算复杂度并提出了算法。


<details>
  <summary>Details</summary>
Motivation: 现实系统中不同作业对机器拥堵的容忍度不同，这种配对依赖的约束在现有匹配问题中未被研究，但对实际应用很重要。

Method: 定义二分图PD匹配模型，分析一般情况和受限实例的计算复杂度，提出最优算法和近似算法。

Result: 该问题与经典匹配问题有显著差异，在一般情况下是NP难的，但对某些受限实例可找到多项式时间算法。

Conclusion: PD匹配是一个新的重要匹配问题变体，为现实系统中的资源分配提供了更精确的建模框架，需要进一步研究算法和应用。

Abstract: Let $G=(U \cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$
represents machines. We study a new variant of the bipartite matching problem
in which each job in $U$ can be matched to at most one machine in $V$, and the
number of jobs that can be assigned to a machine depends on the specific jobs
matched to it. These pair-dependent bounds reflect systems where different jobs
have varying tolerance for congestion, determined by the specific machine they
are assigned to.
  We define a bipartite PD-matching as a set of edges $M \subseteq E$ that
satisfies these job-to-machine tolerance constraints. This variant of matching
extends well-known matching problems, however, despite its relevance to
real-world systems, it has not been studied before. We study bipartite
PD-matchings with the objective of maximizing the matching size. As we show,
the problem exhibits significant differences from previously studied matching
problems. We analyze its computational complexity both in the general case and
for specific restricted instances, presenting hardness results alongside
optimal and approximation algorithms.

</details>


### [15] [Flow-weighted Layered Metric Euclidean Capacitated Steiner Tree Problem](https://arxiv.org/abs/2508.20041)
*Thomas Bläsius,Henrik Csöre,Max Göttlicher,Elly Schmidt,Wendy Yi*

Main category: cs.DS

TL;DR: 提出了FLaMECaST问题，这是具有分层结构和容量约束的欧几里得斯坦纳树变种，证明其NP难近似，但在特定约束下设计了多项式时间的(1+1/2^n)-近似算法。


<details>
  <summary>Details</summary>
Motivation: 受分层网络启发，研究具有分层结构和容量约束的欧几里得斯坦纳树问题，旨在构建连接源点和汇点的成本最优斯坦纳森林。

Method: 设计了基于动态规划的方法，在源点位于圆上的受限情况下实现近似算法，并扩展到源点位于凸多边形的情况。

Result: 证明FLaMECaST问题是NP难近似的，但在特定约束下获得了(1+1/2^n)-近似比的算法。

Conclusion: 虽然问题一般情况很难，但在结构化约束下可以设计有效的近似算法，为分层网络中的路由问题提供了理论保证。

Abstract: Motivated by hierarchical networks, we introduce the Flow-weighted Layered
Metric Euclidean Capacitated Steiner Tree (FLaMECaST) problem, a variant of the
Euclidean Steiner tree with layered structure and capacity constraints per
layer. The goal is to construct a cost-optimal Steiner forest connecting a set
of sources to a set of sinks under load-dependent edge costs. We prove that
FLaMECaST is NP-hard to approximate, even in restricted cases where all sources
lie on a circle. However, assuming few additional constraints for such
instances, we design a dynamic program that achieves a $\left(1 +
\frac{1}{2^n}\right)$-approximation in polynomial time. By generalizing the
structural insights the dynamic program is based on, we extend the approach to
certain settings, where all sources are positioned on a convex polygon.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: 提出了Heraklit建模框架作为数字时代建模的新方法，解决传统建模面临的挑战


<details>
  <summary>Details</summary>
Motivation: 建模在科学和工程中至关重要，但数字时代需要新的基础理论来应对新挑战

Method: 引入Heraklit建模框架作为新的建模方法

Result: 提出了新的建模框架，但具体实验结果未在摘要中说明

Conclusion: 需要进一步研究建模的正确性、信息概念以及建模中的不变性描述

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [17] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT是一种基于Transformer的堆栈跟踪去重方法，通过预训练语言模型和全连接网络有效识别重复崩溃报告，显著优于现有深度学习和传统方法。


<details>
  <summary>Details</summary>
Motivation: 自动化崩溃报告系统产生大量重复报告，传统基于字符串相似性和深度学习的去重方法无法充分捕捉堆栈跟踪中的上下文和结构关系，增加了开发人员的工作负担。

Method: 提出dedupT方法：首先使用预训练语言模型对堆栈跟踪进行整体建模（而非孤立帧），然后利用其嵌入训练全连接网络来有效排序重复崩溃报告。

Result: 在四个真实数据集上的实验表明，dedupT在重复排名和唯一崩溃检测方面均优于现有方法，MRR比最佳DL基线提高15%以上，比传统方法提高9%，同时获得更高的ROC-AUC。

Conclusion: 该研究推动了现代自然语言处理技术在软件工程中的集成，为基于堆栈跟踪的崩溃报告去重提供了有效解决方案，显著减少了人工分类工作量。

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [18] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一个面向功能的代码自进化框架，用于构建多样化的代码功能语义基准测试，显著提升了嵌入模型在代码克隆检测、功能一致性识别和代码检索等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，强调语法相似性而忽略了功能理解。大型语言模型的代码嵌入是否能反映代码级别的功能语义仍不清楚，需要构建更好的基准测试来评估功能一致性。

Method: 提出了Functionality-Oriented Code Self-Evolution数据合成框架，从单个代码实例生成四种独特的变体，涵盖四个语义和语法类别，提供更广泛的代码示例来更好地反映功能差异。

Result: 在三个下游任务（代码克隆检测、代码功能一致性识别和代码检索）上的大量实验表明，使用进化数据集训练的嵌入模型性能显著提升。

Conclusion: 该数据合成框架有效且具有泛化能力，推动了代码功能理解的发展，证明了功能导向的数据合成对提升代码嵌入模型性能的重要性。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [19] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: 代码注释显著影响Stack Overflow答案的感知帮助性，块注释比行内注释更受新手青睐，而答案位置和评分等表面特征影响较小


<details>
  <summary>Details</summary>
Motivation: 理解代码注释如何影响Stack Overflow答案的感知帮助性，因为重用理解不足的代码可能导致严重问题

Method: 在线实验模拟Stack Overflow环境（n=91），比较块注释、行内注释和无注释代码的感知帮助性

Result: 块注释和行内注释都比无注释代码显著更有帮助；新手认为块注释比行内注释更有帮助；答案位置和评分等表面特征重要性较低

Conclusion: 研究结果有助于改进社区驱动平台的代码可读性，并为AI代码生成工具提供针对性提示策略，生成更易读的代码片段

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [20] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: 使用大语言模型将PL/SQL代码翻译为Java的可行性研究，通过自定义提示策略提高翻译准确性


<details>
  <summary>Details</summary>
Motivation: VT系统包含250万行PL/SQL代码，缺乏文档和自动化测试，给重构和现代化带来重大挑战

Method: 使用10个PL/SQL-Java代码对和15个Java类构建域模型，采用链式指导推理与$n$-shot提示结合的自定义提示策略，评估多个LLM模型

Result: 该方法能够有效指导LLM生成语法准确且功能正确的翻译代码

Conclusion: 研究受限于小样本量和测试案例访问限制，但为大规模遗留系统自动化现代化奠定了基础

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [21] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出基于模型的框架，用于发现和表示无头CMS的信息模式，生成中间件库以促进CMS在软件开发中的集成


<details>
  <summary>Details</summary>
Motivation: 无头CMS已成为信息系统重要组件，但缺乏有效工具来发现和管理其中高度定制化的信息，目前主要依赖耗时且易错的手动过程

Method: 开发模型驱动框架，自动发现CMS信息模式并显式表示，设计CMS模型与其他组件的交互，生成平台无关的中间件库

Result: 实现了完整的开源框架，能够自动发现CMS信息结构并生成中间件，为客户端应用提供平台无关的CMS访问

Conclusion: 该框架有效解决了无头CMS集成中的信息发现和管理难题，通过自动化方法提升了开发效率并减少了错误

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [22] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文探讨AI辅助生成式软件重用的影响，指出其与货物崇拜开发的相似性，并提出研究议程来解决相关问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI和生成式软件重用成为软件开发的核心，传统的有机开发方法正被"AI原生"方法取代，这引发了新的软件重用形式，类似于货物崇拜开发，需要研究其影响和问题。

Method: 通过讨论AI辅助生成式软件重用的含义，提出相关问题，并定义研究议程来应对这种新兴方法的核心问题。

Result: 识别了AI辅助生成式软件重用带来的潜在问题，包括开发者对AI生成代码的过度信任，以及与货物崇拜开发的相似性。

Conclusion: 需要制定研究议程来解决AI辅助生成式软件重用的核心问题，以确保这种新兴方法的可持续性和可靠性。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [23] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: 本文系统综述了生成式AI在自动驾驶系统测试中的应用，分析了91项相关研究，总结了6个主要应用类别、评估工具和现有局限性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要在大规模部署前进行广泛测试，但实现有效高效的测试仍是一个开放挑战。生成式AI因其解释上下文、推理复杂任务和生成多样化输出的能力，正被越来越多地应用于ADS测试。

Method: 系统分析了91项相关研究，将其发现综合为6个主要应用类别（主要围绕基于场景的ADS测试），并综述了其有效性，汇编了广泛的数据集、模拟器、ADS、指标和基准。

Result: 识别了27个局限性，提供了生成式AI在ADS测试中应用的概览和实践见解。

Conclusion: 本文概述了生成式AI在自动驾驶系统测试中的应用现状，突出了现有挑战，并在这个快速发展的领域中为未来研究指明了方向。

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [24] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2是基于BERT预训练语言模型的升级版本，用于检测智能合约中的恶意意图，F1分数提升至0.927，成为当前最先进的智能合约意图检测模型。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中的恶意意图可能导致重大经济损失，需要有效的检测方法来识别不安全意图。

Method: 结合BERT预训练语言模型（在16,000个真实智能合约上使用掩码语言建模目标训练）和BiLSTM多标签分类网络，保留K-means聚类意图突出机制。

Result: F1分数达到0.927，相比前代模型的0.8633有显著提升，能够区分10种不同的意图类别。

Conclusion: SmartIntentNN2在智能合约意图检测方面表现出色，是目前最先进的模型，具有更好的性能和实用性。

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 该论文探索了将地下传感器网络与非地面网络（无人机、高空平台、低轨卫星）集成的可行性，开发了蒙特卡洛模拟器评估连接性能，发现LoRa SF7适合短距离无人机通信，LR-FHSS适合大规模地下传感器网络场景。


<details>
  <summary>Details</summary>
Motivation: 解决恶劣环境下无线地下传感器网络通信可靠性下降的问题，利用非地面网络基础设施为大规模地下监测应用提供可靠的连接方案。

Method: 开发蒙特卡洛模拟器，包含多层地下衰减模型、3GPP经验路径损耗模型和两种LoRaWAN调制方案（LoRa和LR-FHSS），评估地下到非地面网络的连接性能。

Result: LoRa SF7适用于农村环境的短距离无人机通信，LR-FHSS调制因其足够的链路预算和抗干扰性，适合高空平台和低轨卫星平台的大规模地下传感器网络场景。

Conclusion: 地下到非地面网络连接的成功概率受监测环境、设备数量、埋藏深度和土壤体积含水量等因素显著影响，不同调制方案适用于不同的网络平台和应用场景。

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [26] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 本文通过三个5G定位测试平台验证了UL-TDoA定位技术，提出PSO优化算法和AI/ML数据驱动方法，在90%情况下实现1-2米定位精度，并公开数据集


<details>
  <summary>Details</summary>
Motivation: 5G NR是实现智能城市和智能工厂精确定位的关键技术，需要研究实际部署中的同步误差、多径传播和部署几何对定位精度的影响

Method: 使用开源OpenAirInterface gNB和核心网，部署三个测试平台（室内工厂和室外场景），采用UL-TDoA技术，提出ToA/TDoA滤波和基于粒子群优化的位置估计算法，以及利用CIR等非传统测量数据的AI/ML框架

Result: 在不同测试平台中，90%的情况下实现了1-2米的定位精度，证明了5G定位系统的可行性

Conclusion: 研究提供了实用的5G定位系统设计见解，公开数据集支持5G定位社区研究，展示了在复杂环境中实现精确定位的有效方法

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [27] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: 本论文首次系统性地将零信任安全框架应用于边缘通用智能中的多LLM系统，分析了安全风险并提出了模型级和系统级的零信任安全机制。


<details>
  <summary>Details</summary>
Motivation: 边缘设备通过LLM实现智能代理化后，多LLM系统的协作特性带来了传统边界安全无法解决的安全漏洞，包括不安全的LLM间通信、扩大的攻击面和跨域数据泄露等问题。

Method: 系统分析EGI环境中多LLM系统的安全风险，提出零信任多LLM框架愿景，并调研实现零信任安全的关键技术进展，将安全机制分为模型级（强身份认证、上下文感知访问控制等）和系统级（主动维护、区块链管理等）方法。

Result: 提供了首个将零信任应用于多LLM系统的系统性研究，为构建安全的边缘通用智能多LLM系统奠定了理论基础并提供了实践策略。

Conclusion: 零信任安全范式是多LLM系统在边缘通用智能环境中应对安全挑战的关键解决方案，需要进一步研究模型级和系统级的安全机制来确保系统的整体安全性。

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [28] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN是首个面向任意目的地的拥塞感知多宿主算法，通过动态选择最优路径来避免拥塞，在Linux中易于实现，并在真实环境中优于现有方案


<details>
  <summary>Details</summary>
Motivation: 当前多宿主路由器采用简单的无拥塞感知机制，无法避免拥塞路径，需要一种能动态适应连接质量并适用于任意目的地的解决方案

Method: 开发了2SYN算法，能够动态为新连接选择首选路径，即使面对之前未见过的目的地也能工作，并在Linux系统中实现

Result: 在真实世界实验中使用LTE或有线链路，2SYN能够动态适应连接质量，性能优于替代方法

Conclusion: 2SYN帮助企业通过充分利用多宿主能力来更好地管理网络

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [29] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 对4G/5G网络跨小区干扰的第一个测量研究，发现运营商基站缺乏干扰协调，导致用户设备信号质量显著恶化


<details>
  <summary>Details</summary>
Motivation: 小区间干扰管理已研究数十年，但其在实际运营网络中的效果很少被探索，需要量化分析实际网络中的跨小区干扰问题

Method: 对运营中的4G/5G网络进行测量研究，从网络部署、频道分配、时频资源分配和网络配置四个主要角度分析跨小区干扰

Result: 发现跨小区干扰普遍存在，基站缺乏干扰协调，甚至在豪华频资源未充分利用时也优先使用相同时频资源，导致互相干扰，尤其在频率选择性衰落时造成显著信号质量下降

Conclusion: 运营商网络在跨小区干扰管理方面存在重大缺口，通过有效的干扰协调策略可以实现显著的信号质量提升

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [30] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: ML-MaxProp是一种混合路由协议，通过监督机器学习增强MaxProp，在灾难应急场景中显著提升DTN网络的传输性能


<details>
  <summary>Details</summary>
Motivation: 灾难和大规模城市应急场景中，基础设施倒塌、移动性不可预测和资源严重受限导致传统网络中断，现有DTN路由协议在稀疏相遇、缓冲区短缺和连接不稳定时表现不佳

Method: 提出ML-MaxProp协议，利用监督机器学习结合上下文特征（相遇频率、跳数、缓冲区占用率、消息年龄和TTL）实时预测中继适用性，将刚性启发式转变为自适应智能

Result: 在ONE环境中使用Helsinki SPMBM移动模型进行广泛模拟，ML-MaxProp始终优于基线协议，实现更高的投递概率、更低延迟和更少开销，统计验证显示改进显著且稳健

Conclusion: ML-MaxProp不仅是渐进式改进，更是轻量级、自适应且实用的解决方案，能够在地面基础设施崩溃时维持关键任务通信

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出了一种基于正则化最小二乘的物理信息回归(PIR)方法，用于参数线性非线性动态模型的参数估计，在计算效率和精度上优于物理信息神经网络(PINN)。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性动态模型参数估计的效率问题，特别是对于参数线性模型，需要一种能够快速准确估计参数的方法来连接理论和数据。

Method: 利用参数线性模型的特点，采用正则化普通最小二乘法从时间序列数据中估计参数，该方法称为物理信息回归(PIR)。

Result: 在合成数据和真实COVID-19数据上测试表明，PIR比PINN表现更好，特别是在复杂隔室模型上，且计算速度显著更快。

Conclusion: PIR方法在参数线性非线性动态模型中提供了可靠且快速的参数估计能力，可能支持实时应用，是连接物理模型和数据的有效桥梁。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [32] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 扩展ZipNN压缩方法至低精度浮点格式(FP8/FP4)，通过分离压缩指数和尾数组件，实现高达83%的压缩比，并发现LLM中的K/V缓存张量也具有压缩潜力。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型越来越大以及部署范围扩大，减少神经网络权重的存储和传输成本变得日益重要。虽然ZipNN等无损压缩方法已在FP32和BF16高精度格式中显示出效果，但对于新兴的低精度格式(FP8/FP4)的压缩研究仍有限。

Method: 设计了一种压缩方法，将浮点数的指数和尾数组件分离并独立地使用來自低缩编码的低缩码进行压缩。对于低精度浮点格式(FP8和FP4)进行了扩展处理，并研究了大语言模型中关键值(K/V)缓存张量的压缩性。

Result: 评估结果显示，对于BF16格式实现了有62%的压缩比，对于FP8格式甚至达到83%的压缩比。同时发现LLM中的K/V缓存张量也呈现出可压缩的模式，能够在部署过程中节省内存。

Conclusion: 该研究成功将ZipNN方法扩展到低精度浮点格式，为高效推理提供了明显的存储和传输成本节省。对K/V缓存压缩性的发现为大型语言模型的内存优化开启了新方向。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [33] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT是一种新型黑盒攻击框架，通过LLM迭代优化生成隐蔽且语义自然的对抗提示，无需外部数据访问或模型检索，就能诱导模型产生过度思考的计算低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有过度思考攻击需要外部知识源进行数据投毒、依赖可检索的污染内容以及结构明显的模板，这些限制条件在现实场景中实用性有限。

Method: 提出POT框架，使用基于LLM的迭代优化来生成隐蔽且语义自然的对抗提示，完全消除对外部数据访问和模型检索的依赖。

Result: 在不同模型架构和数据集上的广泛实验表明，POT相比其他方法实现了更优越的性能。

Conclusion: POT框架成功解决了现有过度思考攻击的限制，提供了一种更实用和有效的黑盒攻击方法，能够诱导LLM产生计算低效的过度推理过程。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [34] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 提出了一种在真实分布式物联网环境中训练深度强化学习模型的新框架，利用ACK反馈信息进行模型训练，实现了高效的通信信道选择


<details>
  <summary>Details</summary>
Motivation: 现有研究很少探索在真实分布式物联网系统中使用真实数据训练DRL模型，需要填补这一研究空白

Method: 基于DRL的方法让物联网设备选择通信信道，并通过实际数据传输获得的ACK反馈信息来训练DRL模型

Result: 通过帧成功率(FSR)的性能评估，证明了所提框架的可行性和有效性

Conclusion: 该框架为在真实分布式物联网环境中训练DRL模型提供了一种有效的解决方案，展示了良好的性能表现

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [35] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame是一个插件模块，通过关联记忆缓冲区整合少量专家轨迹，显著提升离线强化学习在低质量数据上的性能，无需环境交互或修改主干架构。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常面临次优数据问题，难以获得大规模专家数据集。核心挑战是如何有效利用稀缺的专家演示和丰富的低质量数据来提升代理性能。

Method: 提出Re:Frame方法，为标准离线RL策略（如Decision Transformer）添加小型外部关联记忆缓冲区（AMB），存储专家轨迹。策略通过基于内容的关联从AMB检索专家数据并整合到决策中。

Result: 在D4RL MuJoCo任务上，仅使用60条专家轨迹（占6000条轨迹数据集的0.1%），Re:Frame在四分之三的设置中 consistently 优于强基线Decision Transformer，性能提升高达+10.7标准化点。

Conclusion: Re:Frame提供了一种简单且数据高效的方法来注入稀缺专家知识，显著改善从低质量数据集进行的离线强化学习。

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [36] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 该论文提出了NCMemo框架，首次量化半监督节点分类中的标签记忆效应，发现图同配性与记忆化呈负相关关系，并提出图重连方法有效减少记忆化同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被证明会记忆训练数据，但图神经网络(GNNs)的记忆化分析仍未被充分探索。需要量化GNNs在半监督节点分类中的标签记忆化程度。

Method: 提出NCMemo框架分析节点分类记忆化，研究图同配性与记忆化的关系，分析GNN训练动态，并探索图重连作为减轻记忆化的方法。

Result: 发现低同配性图显著增加记忆化；GNN在低同配性区域依赖记忆化来最小化训练损失；特征空间邻域标签不一致的节点更容易被记忆；图重连能有效减少记忆化且不损害性能。

Conclusion: 研究揭示了图同配性与记忆化的内在联系，提出的图重连方法既能降低记忆化又能减少隐私风险，为理解GNN学习和隐私保护部署提供了重要见解。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [37] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 提出基于奇异值分解的多源迁移学习框架，通过分解源模型为秩一组件并选择最显著组件进行聚合，仅微调主奇异值来实现高效知识迁移


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习忽略了利用在线大量可用模型的机会，现有方法在细粒度知识提取和聚合效率方面存在不足，无法处理大量源模型或高参数模型

Method: 使用奇异值分解将每个源模型分解为基本秩一组件，然后从所有源中选择最显著组件进行聚合，仅微调合并矩阵的主奇异值来适应目标任务

Result: 该方法实现了高效的迁移学习，对输入级和参数空间的扰动具有鲁棒性（如噪声或剪枝源），计算扩展性良好

Conclusion: 通过SVD分解和选择性聚合的细粒度方法，解决了多源迁移学习的效率和精度限制，为利用在线模型知识提供了有效途径

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [38] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 这篇论文介绍了图数据模型在化学科学中的应用，重点讲解了图神经网络如何用于分子、蛋白质和化学过程的分析，为化学发现提供新的计算方法。


<details>
  <summary>Details</summary>
Motivation: 化学科学中的分子、蛋白质和反应过程天然具有图结构特征，需要专门的图数据建模方法来处理这些复杂的相互关系，以推动新一代化学发现。

Method: 采用图论作为数学基础，结合图神经网络等机器学习算法，建立化学图数据模型，包括图设计基础、关键预测任务和代表性应用案例。

Result: 提供了化学科学中图数据建模的完整框架，展示了图神经网络在分子性质预测、蛋白质功能分析等领域的应用潜力。

Conclusion: 图数据建模为化学科学提供了强大的分析工具，图神经网络等学习方法将在下一代化学发现中发挥关键作用，读者通过学习这些概念可以应用图方法解决化学问题。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [39] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习模型，结合时间卷积网络和Mamba状态空间模型，仅使用RR间期数据实现房颤早期预测，可提前2小时预测且计算效率高


<details>
  <summary>Details</summary>
Motivation: 房颤是常见心律失常，早期阵发性房颤(PAF)因突发性和短暂性难以检测，但未检测的PAF会进展为持续性房颤，增加死亡风险和严重并发症。早期预测可为预防性治疗提供机会

Method: 使用轻量级深度学习模型，仅基于RR间期数据，结合时间卷积网络(TCN)进行位置编码和Mamba选择性状态空间模型，实现高效的并行序列建模

Result: 在受试者测试中达到灵敏度0.908、特异性0.933、F1分数0.930、AUROC 0.972、AUPRC 0.932。仅73.5千参数和38.3 MFLOPs，计算效率高，优于传统CNN-RNN方法

Conclusion: 该模型仅需30分钟输入数据即可提前2小时预测房颤，为预防性干预提供足够时间，在准确性和模型紧凑性方面均优于传统方法

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [40] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 本文提出了首个基于信息几何和扩散动力学的多模态大语言模型幻觉量化框架，将幻觉从定性检测转向数学基础测量


<details>
  <summary>Details</summary>
Motivation: 现有评估技术主要是启发式的，缺乏原则性量化和理论保证，无法理解幻觉如何产生、传播和跨模态交互

Method: 将MLLM输出表示为多模态图拉普拉斯算子的谱嵌入，通过特征模态分解和RKHS嵌入来量化语义失真和幻觉能量

Result: 建立了模态感知、理论可解释的度量标准，能够捕捉幻觉随时间和输入提示的演化

Conclusion: 为量化有界幻觉建立了原则性基础，将幻觉从定性风险转变为可处理、可分析的现象

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [41] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 基于LLaMA 3.2的视觉语言模型在HEP中微子相互作用分类任务上表现优于传统CNN，支持多模态推理


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多模态推理方面的潜力，特别是在高能物理实验中的中微子相互作用分类应用

Method: 使用基于LLaMA 3.2的视觉语言模型进行微调，与NOvA和DUNE实验中使用的CNN基线模型进行性能对比

Result: VLM在分类准确率、精确率、召回率和AUC-ROC等指标上达到或超过CNN性能，并能整合文本和语义上下文

Conclusion: 视觉语言模型为HEP事件分类提供了有前景的通用骨干网络，为实验性中微子物理的多模态方法开辟了新途径

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [42] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 本研究探索了量子机器学习在恶意软件分类中的应用，比较了量子多层感知器(QMLP)和量子卷积神经网络(QCNN)两种混合量子-经典模型在多个数据集上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的出现，量子机器学习为改进恶意软件检测提供了范式转换的机会，但该领域应用仍未被充分探索。

Method: 使用角度嵌入将恶意软件特征编码为量子态，QMLP通过全量子比特测量和数据重上传捕获复杂模式，QCNN通过量子卷积和池化层减少活跃量子比特实现更快训练。

Result: 二元分类准确率高达95-96%(API-Graph)、91-92%(AZ-Domain)、77%(EMBER-Domain)；多分类准确率91.6-95.7%(API-Graph)、41.7-93.6%(AZ-Class)、60.7-88.1%(EMBER-Class)。QMLP在复杂多分类任务中表现更优，QCNN训练效率更高但准确率略低。

Conclusion: 量子机器学习在恶意软件分类中展现出良好潜力，QMLP适合复杂分类任务，QCNN在训练效率方面具有优势，为量子计算在网络安全领域的应用提供了新方向。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [43] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出DETNO架构，结合Transformer神经算子和扩散模型，解决交通流预测中高频特征丢失和长期预测误差累积问题


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在交通流预测中会产生平滑预测，无法重建高频特征（如密度梯度），导致多步预测时误差快速累积，影响实时交通管理

Method: 使用带有交叉注意力机制的Transformer神经算子提供模型表达能力和超分辨率，结合基于扩散的细化组件通过渐进去噪迭代重建高频交通细节

Result: 在混沌交通数据集上的综合评估表明，该方法在扩展预测方面优于传统和基于Transformer的神经算子，能保持高频成分并提高长期预测稳定性

Conclusion: DETNO架构成功克服了标准神经算子的固有平滑限制和预测不稳定性，为长期交通预测提供了有效解决方案

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [44] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典架构用于SMILES字符串重构，通过量子编码与经典序列建模结合，实现了84%的量子保真度和60%的经典重构相似度，超越了现有量子基线方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在分子设计等生成模型中具有潜力，但现有方法在SMILES序列重构任务中存在保真度下降的问题，量子与序列任务的结合研究不足。

Method: 采用混合量子-经典架构，将量子编码技术与经典序列建模相结合，在保持量子表示表达能力的同时利用经典序列模型优势。

Result: 实现了约84%的量子保真度和60%的经典重构相似度，性能优于现有的量子基线方法。

Conclusion: 该工作为未来量子机器学习应用奠定了有前景的基础，在量子表示和经典序列模型之间取得了平衡，推动了量子感知序列模型在分子和药物发现领域的更广泛研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [45] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: 提出基于Kolmogorov-Arnold表示理论的哈密顿神经网络(KAR-HNN)，用单变量变换替代MLP，解决现有HNN对超参数敏感和能量漂移问题，在多个物理系统基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有哈密顿神经网络(HNN)通常使用多层感知机(MLP)，在处理复杂能量景观时对超参数过于敏感，导致能量漂移和长期预测稳定性差。

Method: 采用Kolmogorov-Arnold表示理论，用单变量变换替代MLP，利用局部函数逼近更好地捕捉高频和多尺度动力学，同时保持哈密顿系统的辛形式。

Result: 在弹簧-质量系统、单摆、二体和三体问题四个基准测试中，KAR-HNN显著减少了能量漂移，提高了长期预测稳定性。

Conclusion: KAR-HNN为高维、参数少的现实物理过程提供了准确稳定的建模方法，具有良好的物理一致性和可解释性。

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [46] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct在聊天或问答格式中错误判断"9.11"大于"9.8"，但在简单格式中回答正确。研究发现transformers存在奇偶注意力头专业化机制，偶数头处理数值比较，奇数头执行不兼容功能。


<details>
  <summary>Details</summary>
Motivation: 研究模型在不同格式下出现推理失败的根本机制，探索transformers内部的计算结构和冗余特性。

Method: 通过系统性干预实验，分析不同层和注意力头的功能，使用稀疏自编码器(SAE)进行特征分析，确定修复bug所需的最小注意力头数量。

Result: 发现需要Layer 10中恰好8个偶数头才能完美修复bug，8个以上成功，7个或以下完全失败。格式表示在Layer 7分离(10%特征重叠)，在Layer 10重新纠缠(80%特征重叠)。

Conclusion: 表面上的全模块需求隐藏了复杂的子结构，这对可解释性和效率具有重要意义，仅使用25%的注意力头即可实现完美修复。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [47] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 使用物理信息机器学习模型，通过载入可微分流体动力学模拟器和CNN网络，大幅减少地下存储库压力控制的计算成本，从万次模拟降到千次以内。


<details>
  <summary>Details</summary>
Motivation: 地下存储库压力控制面临地质异质性和多相流体动力学的挑战，高保真度物理模拟计算成本极高，需要开发更高效的方法来减少模拟次数。

Method: 结合可微分多相流体动力学模拟器(DPFEHM框架)与卷积神经网络(CNN)，通过转移学习先在单相稳态模拟上预训练，然后在多相模拟上细调，将过渡流体动力学物理整合到训练过程中。

Result: 在少于3000次全物理多相流模拟下实现高精度训练，较之前估计需要100万次模拟大幅减少。该方法能够更准确地预测实际注入-提取场景下的压力控制。

Conclusion: 通过物理信息机器学习流程和转移学习策略，成功将地下存储库压力控制的计算成本降到可接受范围，为复杂地质条件下的流体动力学模拟提供了高效解决方案。

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [48] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的无监督框架，用于对43种癌症类型进行聚类分析，通过基因水平和染色体水平的双重突变特征来学习统一的癌症类型嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 理解泛癌症突变景观对揭示肿瘤发生的分子机制至关重要。虽然患者级别的机器学习技术已被广泛用于识别肿瘤亚型，但基于共享分子特征对整体癌症类型进行分组的队列级别聚类主要依赖于经典统计方法。

Method: 使用COSMIC数据库的编码突变数据，为每种癌症类型构建两个互补的突变特征：基因水平特征（捕获最常见突变基因的核苷酸替换模式）和染色体水平特征（表示染色体间归一化替换频率）。通过TabNet编码器和多尺度对比学习目标（NT-Xent损失）来学习统一的癌症类型嵌入表示。

Result: 生成的潜在表示产生了具有生物学意义的癌症类型聚类，与已知的突变过程和组织起源相一致。

Conclusion: 这是对比学习在队列级别癌症聚类中的首次应用，为突变驱动的癌症亚型分析提供了一个可扩展且可解释的框架。

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [49] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 这篇论文提出了一种更高效的样本数据增帽策略，通过空间填充采样局部"柱面"状态来生成神经PDE训练数据，减少了传统时间集成方法的空时冗余性。


<details>
  <summary>Details</summary>
Motivation: 神经PDE比传统数值PDE求解器更容易进行微分、线性化、约化和不确定性量化，但通常需要长时间集成的解踏轨迹进行训练，样本效率低下。

Method: 通过空间填充采样局部"柱面"状态来生成训练数据，消除轨迹数据中的空时冗余性，并对稀有状态进行过采样以提高神经PDE的通用性。

Result: 实验表明，仅需等效于10个时间步长的数值模拟数据即可学习到准确的神经柱面运算符，如果能获得单个全轨迹模拟数据进一步提高准确性。

Conclusion: 该方法在多个PDE系统中都表现出更好的神经柱面运算符训练效果，与传统轨迹采样方法相比有明显的性能优势。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [50] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 本文提出在生成模型中引入张量分解技术，通过生成较小的张量因子而非完整张量来显著降低模型输出和参数数量，从而降低复杂仿真数据的生成成本。


<details>
  <summary>Details</summary>
Motivation: 大型复杂仿真数据集的生成通常耗时耗资源，特别是对于昂贵的实验，生成合成数据用于下游任务变得越来越合理。现有生成模型如GAN和扩散模型虽然提高了数据生成效率，但仍有进一步降低成本的空间。

Method: 针对多维数据（张量），采用内部张量分解技术，生成较小的张量因子而不是完整的张量，以此减少模型的输出规模和总体参数量。

Result: 实验表明该方法能显著降低复杂仿真数据的生成成本，同时生成的数据仍然保持实用性。

Conclusion: 张量分解技术有潜力提高生成模型的效率，特别是在生成多维数据（张量）时，能够有效降低成本同时保持数据质量。

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [51] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 该论文证明了现代神经网络架构（如预层归一化和线性注意力模块）几乎总是满射的，这意味着任何指定输出都可以通过某些输入生成，揭示了生成模型不可避免的安全漏洞和越狱风险。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否具有满射性，因为满射性意味着任何输出（包括有害内容）都可能被生成，这关系到模型安全性和越狱漏洞问题。

Method: 通过数学证明方法，分析现代神经网络基础构建模块（预层归一化网络和线性注意力模块）的满射性质。

Result: 证明了GPT风格变换器和确定性ODE求解器的扩散模型等广泛使用的生成框架对任意输出都存在逆映射，即这些架构几乎总是满射的。

Conclusion: 该研究提供了一个形式化框架，揭示了现代常用神经网络架构对广泛类别对抗攻击的不可避免的脆弱性。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [52] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 该论文研究了成员推理攻击的样本复杂度，发现在高斯均值估计场景中，攻击者需要Ω(n + n²ρ²)个参考样本才能达到完全知情攻击者的效果，这比训练算法使用的样本数n要多得多。


<details>
  <summary>Details</summary>
Motivation: 当前实践中使用的成员推理攻击通常只使用O(n)个参考样本，但攻击者可能拥有更多分布信息。研究需要多少参考样本才能实现有效的成员推理攻击，有助于评估实际攻击的风险和局限性。

Method: 在高斯均值估计的基本设置下进行分析，其中学习算法使用n个样本估计高斯分布的均值，期望误差为ρ²d。研究攻击者需要的最小参考样本数来实现成功的成员推理。

Result: 研究结果表明，对于高斯均值估计中的成员推理，攻击者需要Ω(n + n²ρ²)个参考样本才能与完全知情攻击者竞争。这是首次证明攻击者有时需要比训练算法使用的样本数多得多的样本。

Conclusion: 实践中使用的有限形式攻击（使用O(n)样本）可能低估了成员推理的可能性。当分布信息容易获取时，可能存在更好的攻击方法，这对实际隐私风险评估具有重要意义。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [53] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [54] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas是一种新算法，能够生成数据的局部邻域低维表示，并通过深度神经网络在局部嵌入和原始数据之间建立映射，同时使用拓扑失真来验证流形假设并确定维度。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习工具只能生成全局嵌入，无法提供数学上定义流形所需的局部映射，也不能评估数据集是否符合流形假设。

Method: 通过生成数据局部邻域的低维表示，训练深度神经网络在局部嵌入和原始数据之间进行映射，利用拓扑失真来验证流形假设和确定维度。

Result: 在测试数据集上成功学习了流形结构，发现许多真实数据集（包括单细胞RNA测序数据）不符合流形假设。对于符合流形假设的数据，DeepAtlas可以构建生成模型。

Conclusion: DeepAtlas能够有效验证流形假设并确定数据维度，为符合流形结构的数据集提供了生成模型，并有望将微分几何的强大工具应用于各种数据集。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [55] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出了SAFT框架来解决表格学习中分布偏移问题，将离散特征搜索转化为连续表示生成范式，通过三种机制提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 表格学习在训练和测试数据分布偏移时效果下降，需要解决分布偏移下的表格学习问题。

Method: 提出Shift-Aware Feature Transformation (SAFT)框架，包含：嵌入解相关和样本重加权的抗偏移表示、次优嵌入平均的平坦感知生成、训练测试分布对齐的归一化方法。

Result: 大量实验表明SAFT在多种真实世界分布偏移下，在鲁棒性、有效性和泛化能力方面均优于现有表格学习方法。

Conclusion: SAFT成功将表格学习重构为连续表示生成问题，通过可微分优化显著提升了分布偏移场景下的性能表现。

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [56] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE是一个通过质量对齐嵌入转移的方程生成框架，用于在低数据条件下通过蒸馏方法微调基础模型进行符号方程发现，在多个基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在大规模方程数据集上预训练后，在应用于小型领域特定数据集时常常出现负迁移和泛化能力差的问题，需要数据高效的微调框架。

Method: EQUATE结合符号-数值对齐和评估器引导的嵌入优化，将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，通过数据-方程拟合度和简洁性进行指导。

Result: 在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面始终优于最先进的基线方法，同时保持低复杂度和快速推理。

Conclusion: EQUATE为基础模型蒸馏设置中的数据高效符号回归提供了一个实用且可推广的解决方案。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [57] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 提出了PoolFlip环境和Flip-PSRO方法，通过多智能体强化学习训练防御者对抗未知攻击策略，效果比基线方法提升2倍


<details>
  <summary>Details</summary>
Motivation: 现有FlipIt框架依赖少量启发式或专门学习技术，存在脆弱性和无法适应新攻击的问题，需要更强大的自适应防御方法

Method: 开发PoolFlip多智能体环境扩展FlipIt游戏，提出Flip-PSRO多智能体强化学习方法，采用基于群体的训练和所有权效用函数

Result: Flip-PSRO防御者在面对训练中未接触的启发式攻击时，效果比基线方法提升2倍，同时保持高水平控制

Conclusion: Flip-PSRO为网络防御提供了有效的自适应决策框架，能够对抗未知和潜在自适应的对手，显著提升防御效果

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [58] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 使用Python程序表示游戏策略，通过LLM进行代码优化，在Atari游戏中达到与深度强化学习相当的性能，但训练时间和环境交互更少


<details>
  <summary>Details</summary>
Motivation: 探索程序化策略表示方法，通过代码自进化实现游戏智能体的高效学习和适应，减少对大量环境交互的依赖

Method: 将决策策略表示为Python程序，利用大语言模型根据执行轨迹和自然语言反馈进行代码优化，实现策略的自进化

Result: 在Atari游戏中性能与深度强化学习基线相当，但训练时间显著减少，环境交互次数大幅降低

Conclusion: 程序化策略表示方法为构建高效、适应性强的智能体提供了有前景的途径，能够进行复杂的长期推理

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [59] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA是一个可扩展的机器遗忘框架，针对异构时空数据扩展了SISA训练方法，通过相似性感知聚类实现高效精确的遗忘，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现代移动平台存储了大量GPS轨迹和文本数据，GDPR等隐私法规要求按需删除个人数据，但为每个删除请求从头重新训练深度模型不可行。

Method: 将行程的数值和语言特征嵌入共享潜在空间，使用相似性感知聚类将样本分配到分片中，每个分片增量训练，预测时聚合分片结果，删除时仅重新训练受影响的分片。

Result: 在10个月的真实移动日志实验中，MobText-SISA保持了基线预测准确性，在错误率和收敛速度方面始终优于随机分片方法。

Conclusion: MobText-SISA为城市规模多模态移动数据的隐私合规分析提供了实用基础，实现了精确遗忘同时保持模型性能。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [60] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs在表格数据拟合中存在严重脆弱性，任务无关的数据表示变化（如变量名更改）会导致预测结果大幅波动，即使是最先进的表格基础模型也无法完全避免这种敏感性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于数据拟合和预测生成，需要评估其作为即插即用数据拟合工具的稳健性，特别是对任务无关数据变化的敏感性。

Method: 通过改变变量名等任务无关的数据表示方式，测试LLMs在上下文学习和监督微调下的预测敏感性，并分析注意力模式来解释这种现象。

Result: 简单的变量名更改可使预测误差波动高达82%，LLMs对任务无关变化表现出显著敏感性，注意力分析显示存在非均匀注意力模式。

Conclusion: 尽管LLMs具有出色的预测能力，但目前缺乏基本的稳健性，不能作为原则性的数据拟合工具使用。

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [61] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA是一种新的参数高效微调方法，通过引入辅助LoRA模块来模拟SAM的对抗性权重扰动，在保持内存效率的同时实现更平坦的最小值，消除了SAM的双倍训练成本。


<details>
  <summary>Details</summary>
Motivation: SAM虽然能通过寻找平坦最小值来提升泛化能力，但其巨大的内存和计算开销使其不适用于大模型。直接将SAM应用于LoRA参数会限制锐度优化的效果。

Method: 提出双向低秩适应(Bi-LoRA)，包含主LoRA模块和辅助LoRA模块。主模块通过标准梯度下降适应特定任务，辅助模块通过梯度上升捕捉损失景观的锐度。

Result: 在多种任务和架构上的广泛实验表明，Bi-LoRA在提升泛化能力方面既高效又有效。

Conclusion: Bi-LoRA成功解决了SAM在大模型微调中的内存和计算效率问题，同时保持了优异的泛化性能。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [62] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种基于因果推理的反事实奖励模型，通过Counterfactual Trust Score来缓解RLHF中的偏见问题，在假新闻检测中达到89.12%准确率并减少虚假相关性


<details>
  <summary>Details</summary>
Motivation: RLHF中奖励模型会放大数据集中的潜在偏见，导致有缺陷的奖励信号和公平性下降，传统被动约束方法在因果混淆情况下会失效

Method: 结合因果推理和多模态表示学习的反事实奖励模型，包含四个组件的Counterfactual Trust Score：反事实偏移、重构不确定性、公平规则违反检测和时间奖励偏移

Result: 在多模态真假新闻数据集上达到89.12%的假新闻检测准确率，优于基线奖励模型，显著减少了虚假相关性和不公平强化信号

Conclusion: 该方法为公平感知的RLHF提供了鲁棒且可解释的解决方案，具有可调节的偏见减少阈值，提高了动态实时策略制定的可靠性

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [63] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程介绍了人工智能生成模型在合成数据生成方面的最新进展，包括大语言模型、扩散模型和生成对抗网络等技术，并提供实践案例和应用指南。


<details>
  <summary>Details</summary>
Motivation: 解决数据挖掘中遇到的数据稀缺、隐私保护和标注成本高等挑战，通过合成数据生成技术提供可扩展的解决方案。

Method: 询讲介绍合成数据生成的基础理论和最新技术，包括关键方法论和实践框架，讨论评估策略和应用场景。

Result: 参与者将获得关于如何利用生成式合成数据来提升数据挖掘研究和实践的可操作性见解。

Conclusion: 生成式模型为数据挖掘领域带来了革命性的合成数据创造能力，有效解决了多种数据挖掘挑战，具有重要的研究价值和应用前景。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [64] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出SyReM方法解决运动预测中持续学习的稳定性-可塑性困境，通过紧凑记忆缓冲和选择性记忆回放机制，在11个驾驶数据集上显著减轻灾难性遗忘并提升新场景预测精度


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在运动预测中面临灾难性遗忘问题，现有持续学习方法过度强调记忆稳定性会损害学习可塑性，需要平衡稳定性与可塑性的矛盾

Method: SyReM维护紧凑记忆缓冲表示已学知识，使用不等式约束保证记忆稳定性，同时基于损失梯度余弦相似度的选择性记忆回放机制增强学习可塑性

Result: 在INTERACTION的11个自然驾驶数据集上验证，相比非持续学习和持续学习基线，SyReM显著减轻了过去场景的灾难性遗忘，同时提高了新场景的预测精度

Conclusion: SyReM有效解决了运动预测中的稳定性-可塑性困境，通过协同记忆回放机制实现了更好的持续学习性能，代码已开源

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [65] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution (Δ-Attribution) 是一个模型无关的框架，通过差分特征归因来解释模型版本A和B之间的变化原因，提供轻量级的模型更新审计。


<details>
  <summary>Details</summary>
Motivation: 模型更新（超参数、内核、深度、求解器或数据）会改变性能，但变化的原因往往不透明。需要一种方法来解释模型版本之间的具体变化。

Method: 通过差分每特征归因：Δφ(x)=φ_B(x)-φ_A(x)，使用Δ-归因质量套件评估，包括幅度/稀疏性、一致性/偏移、行为对齐和鲁棒性等指标。通过快速遮挡/钳位在标准化空间中实例化，使用类锚定边界和基线平均。

Result: 在45个设置中评估发现：归纳偏差变化产生大的行为对齐delta（如SVC poly→rbf：BAC≈0.998，DCE≈6.6），而"表面"调整显示rank-overlap@10=1.0和DCE≈0。最明显的重新分布出现在更深的GB上（JSD≈0.357）。

Conclusion: Δ-Attribution提供了一种轻量级的更新审计方法，通过区分良性变化与行为上有意义或风险依赖转移的变化来补充准确性评估。

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [66] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS是一种受人类大脑互补学习系统启发的在线持续学习范式，通过双记忆重放机制解决DNN车辆运动预测中的灾难性遗忘问题，显著提升预测稳定性并大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 智能城市服务依赖AI，但深度神经网络在车辆运动预测中存在灾难性遗忘问题。传统方法需要大量数据收集且效率低下，无法平衡长短期经验，无法实现人类般的持续学习能力。

Method: 提出Dual-LS范式，受人类大脑互补学习系统启发，采用两种协同的记忆重放机制，加速经验检索并动态协调长短期知识表示，实现任务无关的在线持续学习。

Result: 在三个国家77.2万辆车、累计测试里程11,187公里的自然数据测试中，Dual-LS将灾难性遗忘减少74.31%，计算资源需求降低94.02%，显著提升预测稳定性且不增加数据需求。

Conclusion: Dual-LS为基于DNN的车辆运动预测提供了计算高效、人类般的持续学习适应性，适合智能城市应用，解决了传统方法在灾难性遗忘和资源效率方面的局限性。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [67] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: RLTR框架通过工具使用奖励的强化学习，解耦训练过程，专注于规划模块的单目标优化，相比端到端方法提升了8-12%的规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体训练采用端到端多目标优化，存在目标分配不平衡和可验证数据稀缺的问题，难以有效提升规划能力。

Method: 提出RLTR框架，通过基于工具使用完整性的奖励信号来直接评估工具调用序列质量，实现规划模块的单目标优化。

Result: 实验显示RLTR在规划性能上比端到端基线提升8-12%，规划能力的提升还带来整体系统最终响应质量5-6%的提高。

Conclusion: RLTR通过解耦训练和工具使用奖励机制，有效解决了LLM智能体规划能力训练的挑战，为智能体性能提升提供了新思路。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [68] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast是首个专门为金融时间序列预测设计的基础模型，通过大规模金融数据集训练，在零样本情况下表现出色，无需领域特定微调即可捕捉多样化模式，超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测对经济稳定、政策制定和可持续投资至关重要，但由于时间非平稳性、多领域多样性和不同时间分辨率等模式变化而面临挑战。现有深度学习方法容易过拟合且需要大量领域特定微调。

Method: 开发FinCast基础模型，在大规模金融数据集上进行训练，专门针对金融时间序列预测设计，具备零样本性能能力。

Result: FinCast展现出强大的零样本性能，无需领域特定微调就能有效捕捉多样化模式。全面的实证和定性评估表明其超越了现有最先进方法。

Conclusion: FinCast作为首个金融时间序列预测基础模型，具有强大的泛化能力，能够克服现有方法的局限性，为金融预测提供了新的解决方案。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [69] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA是一种新颖的模型精度估计框架，直接在logit空间中操作，通过锚点建模策略来捕捉logit分布的细微变化，从而在分布偏移下提供更准确和鲁棒的模型性能估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖softmax概率或数据相似性度量，前者存在信息损失问题，后者计算昂贵且领域特定。需要一种能够保留更丰富信息、计算高效且广泛适用的模型精度估计方法。

Method: ALSA在logit空间中初始化多个可学习锚点，每个锚点分配一个影响函数来捕捉logit的细微变化。基于理论洞察和实证观察，利用logit的聚合和分布与模型预测性能的强相关性。

Result: 在视觉、语言和图基准测试上的大量实验表明，ALSA优于基于softmax和相似性的基线方法。在显著分布偏移下表现出优异的鲁棒性。

Conclusion: ALSA作为一个实用工具，在logit空间中操作避免了信息损失，通过锚点建模策略实现了跨多种分布偏移的可靠模型评估，具有广泛的适用性和实用性。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [70] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: pFedBayesPT是一个基于视觉提示调优的细粒度实例级个性化联邦学习框架，通过贝叶斯方法处理客户端内部数据异质性，在特征和标签异质性设置下均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法假设每个客户端数据遵循单一分布，但实践中单个客户端可能包含来自多个来源或领域的数据，导致显著的客户端内部异质性和次优性能

Method: 基于视觉提示调优的实例级个性化联邦学习框架，从贝叶斯角度制定实例级提示生成，将提示后验建模为隐式分布以捕捉多样视觉语义，在半隐式变分推断框架下推导变分训练目标

Result: 在基准数据集上的广泛实验表明，pFedBayesPT在特征和标签异质性设置下始终优于现有的个性化联邦学习方法

Conclusion: 提出的pFedBayesPT框架有效解决了客户端内部数据异质性挑战，通过实例级个性化方法显著提升了联邦学习性能

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [71] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR是一个用于量化数据集内在结构特性的框架，包含规模、覆盖度、真实性和丰富度四个维度，能够识别保持泛化能力的最小基础数据集，并指导多模态数据的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 传统数据优化方法主要关注数据量和训练效率，缺乏对数据质量结构特性的理论理解，特别是在样本缩放过程中数据特性如何影响泛化的机制不明确。

Method: 提出SCAR框架从四个维度（Scale, Coverage, Authenticity, Richness）量化数据集结构特性；建立基础数据集概念，识别保持泛化能力的最小数据子集；建模单模态任务为阶跃函数，估计多模态数据集中的泛化偏差；开发SCAR指导的数据补全策略。

Result: 在多个多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据采集方面的有效性，能够实现高效、模态感知的多模态数据集扩展。

Conclusion: SCAR提供了一个原则性的数据特性量化框架，能够捕捉数据集缩放过程中的稳定结构特性，为数据理解和优化提供了理论基础和实践指导，特别是在多模态学习场景中表现出色。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [72] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 首次全面探索低功耗柔性压力分类器的设计空间，涵盖多种机器学习分类器、特征选择和神经网络简化算法，设计了1200多个柔性分类器，实现比现有方法更高精度的实时压力监测


<details>
  <summary>Details</summary>
Motivation: 传统压力监测依赖间歇性、症状导向的干预，缺乏连续、可及且成本效益高的解决方案。现有刚性硅基可穿戴设备虽能多任务处理，但不适合轻量灵活穿戴，限制了连续监测的实用性

Method: 采用柔性电子技术，设计完全定制的低精度算术电路，探索各种机器学习分类器、特征选择和神经网络简化算法，开发了1200多个柔性分类器

Result: 实现了比现有方法更高精度的实时压力分类器，同时具备低成本、可贴合性、低功耗和紧凑尺寸的优势

Conclusion: 该研究为设计实时压力分类器提供了重要见解，解决了柔性电子中复杂电路实现的挑战，推动了连续压力监测技术的发展

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [73] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文证明了适当正则化的函数可以用有理函数和有理神经网络在C¹范数下近似，并提供了关于网络宽度、深度以及有理函数次数的近似率。


<details>
  <summary>Details</summary>
Motivation: 研究有理函数和有理神经网络在C¹范数下的近似能力，特别是在符号回归和物理定律学习等应用中的重要性。

Method: 使用有理函数和有理神经网络进行函数近似，分析网络宽度、深度以及有理函数次数对近似精度的影响。

Result: 获得了在C¹范数下的近似结果，包括具体的近似率，并推广到EQL÷和ParFam架构的有理神经网络。

Conclusion: 有理函数和有理神经网络能够有效近似正则函数，为符号回归和物理定律学习提供了理论支持。

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [74] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 本文研究图上游走的度量结构，引入加权度量处理序列，基于逐步顶点距离和加权范数定义游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造，支持度量建模经典工具的应用。


<details>
  <summary>Details</summary>
Motivation: 研究图上游走的度量结构，为分析游走间相对距离的较弱测量形式（邻近度）提供理论基础，支持在网络上进行Lipschitz回归和强化学习策略开发。

Method: 引入加权度量处理序列，基于逐步顶点距离和加权范数定义游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造方法。

Result: 建立了图上游走的度量框架，提供了邻近度的表示公式和显式构造，支持Lipschitz函数从游走子空间的扩展，保持了基本性质。

Conclusion: 提出的度量框架为估计邻近度和基于探索性游走的强化学习策略开发提供了稳健方法，扩展了网络结构上的Lipschitz回归应用。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [75] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了Adam-PFN，一种用于Adam优化器超参数调优的预训练代理模型，结合新的学习曲线增强方法CDF-augment，显著提升了学习曲线外推能力和超参数优化效率。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在深度学习中被广泛使用，但其超参数调优过程繁琐且成本高昂。现有的冻结-解冻贝叶斯优化方法受限于通用代理模型，缺乏对超参数如何影响学习过程的先验知识。

Method: 开发了Adam-PFN代理模型，在TaskSet的学习曲线上进行预训练，并提出了CDF-augment学习曲线增强方法，通过人工增加训练样本来提升模型性能。

Result: 该方法在TaskSet评估任务上显著改善了学习曲线外推能力并加速了超参数优化，同时在分布外任务上也表现出强劲性能。

Conclusion: Adam-PFN结合CDF-augment增强方法为Adam优化器的超参数调优提供了有效的解决方案，特别是在低预算场景下表现出色。

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [76] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP是一种无需训练的图划分方法，通过图拉普拉斯负校正机制提取超出常规频率范围的低频信息，结合谱GNN和随机输入，仅需一次前向传播即可生成可区分嵌入，在效率和准确性方面均表现优异。


<details>
  <summary>Details</summary>
Motivation: 从图信号处理的角度发现，带有负校正的图拉普拉斯可以产生超出常规范围[0,2]的图频率，探索这些低频信息是否能编码更多关于社区结构的有效信息。

Method: 采用谱GNN作为主干网络，结合低通滤波器和负校正机制；仅输入随机信号；通过一次前向传播无需训练即可获得图嵌入；使用BIRCH聚类获得图划分结果。

Result: 实验表明，仅基于负校正机制放大[0,2]范围外的低频信息，InfraredGP就能为聚类模块生成可区分嵌入，无需训练即可获得高质量的图划分结果。在IEEE HPEC图挑战基准测试中，静态和流式图划分的效率提升16-23倍，质量与各种基线方法相当。

Conclusion: InfraredGP通过负校正机制有效利用了超出常规频率范围的低频信息，实现了无需训练的高效高质量图划分，在效率和准确性方面都表现出色。

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [77] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: 提出基于3D扩散模型的生成管道，直接合成任意大小的物理真实颗粒介质装配体，显著加速离散元法模拟的初始化阶段


<details>
  <summary>Details</summary>
Motivation: 离散元法模拟颗粒介质时，初始化阶段计算成本极高，因为涉及大位移和动能，主导总模拟时间

Method: 两阶段管道：首先训练扩散模型生成独立的3D体素网格；然后使用基于掩码输入的3D修复模型无缝拼接网格，采用多种掩码策略和2D重绘技术

Result: 实现了计算时间与样本大小的线性缩放，1.2米长的道碴轨道合成仅需20秒，相当于3小时的DEM模拟

Conclusion: 该方法能够实现物理一致、实时、可扩展的颗粒介质合成，适用于工业应用

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [78] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 提出了EUREKA框架，通过选择有趣而非最准确的特征来构建分类器，利用大语言模型评估特征有趣度，生成既具有预测性又新颖可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习追求预测准确性，但忽略了模型的有趣性和新颖性。作者希望构建使用非预期特征的分类器，即使准确率略低，但能提供新的洞察和知识发现。

Method: EUREKA框架：1）使用大语言模型对特征进行有趣度排序；2）仅选择有趣特征构建可解释分类器；3）在多个基准数据集上验证方法有效性。

Result: 在Occupancy Detection数据集中偏好湿度而非CO2和光照强度；在Twin Papers数据集中发现标题含冒号的论文更可能被引用。模型在保持有意义的准确率同时提供了新颖见解。

Conclusion: EUREKA能够识别非显而易见但仍具预测性的特征，支持在中等准确率足够但重视新颖性和可解释性的场景下进行知识发现和交流。

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [79] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging是一种基于粒子群优化的数据驱动模型融合方法，通过初始化粒子群并进行多轮迭代，有效解决了现有方法在性能和效率方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法存在性能限制：数据无关方法缺乏数据指导，梯度方法计算成本高，无梯度方法优化步数有限效果不佳。需要一种高效且可扩展的融合方案。

Method: 基于粒子群优化(PSO)算法，使用预训练模型、专家模型和稀疏化专家模型初始化粒子群，通过多轮迭代优化，最终选择全局最优粒子作为融合模型。

Result: 在不同语言模型上的实验表明，PSO-Merging总体上优于基线融合方法，提供了更高效和可扩展的解决方案。

Conclusion: PSO-Merging成功解决了现有模型融合方法的局限性，为构建多任务模型提供了一种计算效率高且性能优越的新方法。

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [80] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出了一种新的辛卷积神经网络架构，通过结合辛神经网络、适当辛分解和张量技术来保持卷积层的辛结构，并在波动方程、非线性薛定谔方程和正弦-戈登方程上验证了其优越性能


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在保持物理系统的辛结构方面存在不足，需要开发能够保持哈密顿系统辛几何结构的神经网络架构

Method: 首先引入卷积层的数学等价形式，然后使用辛神经网络参数化CNN层以确保卷积层保持辛特性，并引入辛池化层构建完整的自编码器

Result: 在波动方程、非线性薛定谔方程和正弦-戈登方程三个例子上，辛CNN的性能优于通过适当辛分解获得的线性辛自编码器

Conclusion: 所提出的辛卷积神经网络架构能够有效保持物理系统的辛结构，在哈密顿系统的数值模拟中表现出优越性能

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [81] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 一种结合有限元法和物理信息DeepONet的混合框架，用于多孔介质中的流体运移模拟，实现了高精度流场计算和快速运移推断


<details>
  <summary>Details</summary>
Motivation: 解决多孔介质中由尖锐局部源引起的流体运移问题，结合传统数值方法的精确性和深度学习方法的高效性

Method: 使用FEM求解达西流方程获得速度场，然后通过物理信息DeepONet学习源函数到溶质浓度的映射关系，采用适应性采样策略处理尖锐源引起的温度梯度

Result: 方法与参考解几乎完全一致，运算速度比传统求解器提高数量级，适合实际应用场景

Conclusion: 该混合框架成功结合了FEM的精确性和DeepONet的高效性，为处理多孔介质中的复杂流体运移问题提供了一种有效的数值解决方案

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [82] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 量子潜在分布可以提升生成模型的性能，在某些条件下能产生经典方法无法高效生成的数据分布，实验证明在GAN、扩散模型和流匹配模型中都能带来改进


<details>
  <summary>Details</summary>
Motivation: 探索量子处理器产生的潜在分布何时以及如何能够提升生成模型的性能，并验证这种量子优势的可重现性

Method: 理论证明量子潜在分布的优势条件，在合成量子数据集和QM9分子数据集上进行基准测试，使用模拟和真实的光子量子处理器，比较多种经典基线方法

Result: 量子潜在分布在GAN中相比经典基线方法能带来生成性能的提升，同时确定了与量子潜在分布兼容的扩散模型和流匹配模型架构

Conclusion: 近期量子处理器能够扩展深度生成模型的能力，量子潜在分布确实能够提供经典方法无法实现的性能优势

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [83] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 提出了一种基于结构多样性的无参数图神经网络框架SDGNN，通过结构多样性消息传递机制同时捕捉邻域结构异质性和特征语义稳定性，无需训练参数，在多个数据集上优于主流GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法依赖大量可训练参数和固定聚合规则，难以适应结构异质性强的图数据，容易导致节点表示过平滑和语义退化。

Method: 基于结构多样性理论设计统一的结构多样性消息传递机制，从结构驱动和特征驱动两个角度进行互补建模，不引入额外可训练参数。

Result: 在8个公共基准数据集和PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战性条件下均优于主流GNN方法。

Conclusion: 为无参数图神经网络设计提供了新的理论视角和通用方法，验证了结构多样性作为图表示学习核心信号的重要性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [84] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: 提出了NM-Hebb两阶段训练框架，结合神经启发的局部可塑性和距离感知监督，在多个数据集和骨干网络上显著提升CNN准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决传统CNN依赖全局梯度优化导致的过拟合、冗余滤波器和可解释性差的问题

Method: 两阶段训练：第一阶段结合交叉熵损失、Hebbian正则器和可学习神经调节器；第二阶段使用成对度量学习损失进行微调

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上Top-1准确率提升2.0-10.0个百分点，NMI提升最高0.15

Conclusion: 结合局部Hebbian可塑性和度量学习微调，使CNN不仅更准确而且更可解释，适用于资源受限和安全关键的AI部署

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [85] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC是一个二阶可微分框架，通过动态平衡强化学习和行为克隆来解决离线RL中策略约束的缩放问题，无需针对每个数据集调整超参数。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法需要针对不同任务和数据集质量精心调整超参数，这既耗时又不实用，因此需要一种自适应的方法来处理分布偏移问题。

Method: 提出了自适应策略约束缩放（ASPC）框架，使用二阶可微分方法在训练过程中动态平衡RL目标和行为克隆目标。

Result: 在4个D4RL领域的39个数据集上，ASPC使用单一超参数配置优于其他自适应约束方法和需要逐数据集调优的最先进算法，且计算开销最小。

Conclusion: ASPC提供了一种有效且实用的解决方案，能够在无需大量超参数调整的情况下实现优异的离线RL性能，具有理论性能改进保证。

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [86] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet是一个针对符号二分图链接符号预测的新型谱卷积神经网络模型，通过Gegenbauer多项式基滤波器实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单部符号图，忽略了二分图的节点异质性和独特特征，且传统谱卷积算子最初是为无符号图的正链接设计的，不适用于从已知链接推断缺失的正负链接

Method: 提出三个主要技术贡献：1）快速理论基础的节点特征初始化谱分解技术；2）基于Gegenbauer多项式基的新谱图滤波器；3）交替使用正负边Gegenbauer多项式滤波器的多层符号感知谱卷积网络

Result: 在6个基准SBG数据集上，相比11个强竞争对手，GegenNet在链接符号预测中实现了显著优越的性能（AUC提升最高4.28%，F1提升最高11.69%）

Conclusion: GegenNet通过创新的谱卷积方法有效解决了符号二分图的链接符号预测问题，证明了其在处理节点异质性和二分图特征方面的优势

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [87] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS本体概念的新型放射学报告检索方法，通过标准化医学实体提取和改进的Tversky相似度度量，在长尾医疗影像任务中优于现有嵌入方法


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP或CXR-BERT等高维文本嵌入的检索方法存在可解释性差、计算成本高、与医学知识结构化特性不匹配的问题，需要更透明、可解释的检索策略

Method: 使用RadGraph-XL和SapBERT增强管道从自由文本报告中提取标准化医学实体并链接到UMLS概念，定义基于改进加权Tversky指数的任务自适应相似度度量，考虑同义词、否定和层次关系

Result: 在MIMIC-CXR放射影像分类任务中优于最先进的嵌入检索方法，特别是在长尾场景下表现更佳，并为MIMIC-CXR生成了本体支持的新疾病标签资源

Conclusion: 该方法为临床AI系统提供了更可解释、可靠和任务特定的检索策略，特别适用于需要可解释性和领域知识整合的场景

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [88] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer是一个基于BERT的预训练模型，专门设计用于网络流量分析，通过创新的流量表示、协议语义嵌入和预训练任务，显著提升了流量分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在网络流量分类中难以捕捉包结构特征、流级行为、分层协议语义和包间上下文关系，需要专门设计的解决方案。

Method: 提出FlowletFormer模型，包含：1）连贯行为感知流量表示模型分割语义单元；2）协议栈对齐嵌入层捕获多层协议语义；3）字段特定和上下文感知预训练任务增强包间和流间学习。

Result: 实验结果显示FlowletFormer在流量表示效果、分类准确性和少样本学习能力方面显著优于现有方法，并能更好理解网络传输原理（如TCP状态连接）。

Conclusion: FlowletFormer通过有效整合领域特定的网络知识，为流量分析提供了更鲁棒和可信的框架，在多个关键指标上表现出色。

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [89] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 通过逆向动态游戏算法从多代理局部广义纳什均衡交互数据中学习参数化约束，使用MILP编码KKT条件来恢复与纳什稳定性一致的约束集合


<details>
  <summary>Details</summary>
Motivation: 从多代理交互示例中学习基础约束，以支持符合约束的动态运动规划设计

Method: 使用混合整数线性规划(MILP)编码交互代理的KKT条件，恢复与纳什稳定性一致的约束集合

Result: 算法能够学习真实安全集和非安全集的内逆近似，并在模拟和硬件实验中成功推断凸和非凸约束

Conclusion: 该方法能够从非线性动态代理的纳什均衡交互中有效学习约束，为符合约束的动态运动规划提供支持

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [90] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijit Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 提出了全局排列熵(GPE)，这是一种新的复杂度指标，不仅考虑连续段落的排列模式，还包含所有可能长度的非连续模式，通过高效算法提取完整排列分布，比标准排列熵能揭示更多结构信息。


<details>
  <summary>Details</summary>
Motivation: 标准排列熵只基于连续段落的相对顺序模式，可能忽略了时间序列中重要的非连续结构信息，需要一种更全面的复杂度度量方法。

Method: 开发全局排列熵(GPE)指标，利用新算法高效提取所有可能长度的排列模式分布，包括非连续模式，然后应用香农熵进行量化。

Result: 在合成数据集上的实验表明，GPE能够揭示标准排列熵无法获取的结构信息，提供了更全面的复杂度分析。

Conclusion: 全局排列熵是对传统排列熵的重要扩展，能够捕捉更丰富的时序结构特征，为时间序列复杂度分析提供了新的工具，并提供了Julia实现包。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [91] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 基于机器学习框架，使用实时传感器数据预测工业离心泵的短期故障，通过随机森林和XGBoost模型在5-30分钟前实现早期预警。


<details>
  <summary>Details</summary>
Motivation: 工业离心泵的故障预测对预防性维护至关重要，传统方法难以实现准确的短期故障预警，需要开发基于实时数据的预测框架。

Method: 采用滑动窗口方法（60分钟和120分钟回溯期），提取统计特征（均值、标准差、最小值、最大值、线性趋势），使用SMOTE算法处理类别不平衡，训练随机森林和XGBoost分类器。

Result: 随机森林模型在60分钟窗口下表现最佳：5分钟前召回率69.2%，15分钟前64.9%，30分钟前48.6%。120分钟窗口下，15和30分钟前召回率提升至65.6%。

Conclusion: 预测性能取决于历史数据长度和预测时间范围，不同故障模式可能在不同时间尺度上演变。该方法为实时工业监控系统提供了可解释且可扩展的预测性维护解决方案。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [92] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 通过数据驱动的马德里街道停车模拟，量化了不同协调策略对停车搜索时间的影响，提出了一种实用的Cord-Approx策略，将用户搜索时间从19.98分钟减少到6.69分钟。


<details>
  <summary>Details</summary>
Motivation: 在密集都市区域，寻找路边停车位加剧了交通拥堵。虽然基于手机的实时助手已被提出，但其有效性尚未得到充分研究。

Method: 使用马德里街道停车生态系统的数据驱动模拟，分析四种策略：无协调搜索、无非用户意识的协调停车、理想化的全知系统，以及新颖的Cord-Approx策略——该策略使用历史占用分布来延长物理距离，并通过匈牙利匹配算法进行调度。

Result: 在高保真模拟中，Cord-Approx用户平均6.69分钟找到停车位，而非应用用户需要19.98分钟。在中心枢纽区域搜索时间减少72%（67-76%），在住宅区域减少高达73%。

Conclusion: Cord-Approx策略通过概率估计非用户行为，显著减少了停车搜索时间，为解决都市停车拥堵问题提供了实用有效的解决方案。

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [93] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 语言模型在密码验证任务中表现不佳，推理能力反而会泄露机密信息，当前前沿模型不适合处理敏感信息


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在关键场景中作为自主代理部署，确保其可靠遵循用户定义规则成为关键安全问题，需要研究模型是否具备上下文鲁棒性

Method: 开发PasswordEval基准测试，测量模型在密码验证任务中的表现，通过对抗性用户压力和长对话增加测试难度

Result: 当前开源和闭源模型在此简单任务上表现不佳，推理能力不能提升性能反而会泄露机密信息

Conclusion: 当前前沿模型不适合处理机密信息，推理能力需要以不同方式训练才能在高风险场景中安全部署

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [94] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种新的自监督预训练方法，通过双层优化和平衡约束来处理异构数据，提高模型在下游任务中的适应性


<details>
  <summary>Details</summary>
Motivation: 传统自监督预训练方法将所有异构数据混合并最小化全局平均损失，无法确保模型对每个数据源都达到局部最优，需要更好的方法来处理异构数据

Method: 采用双层优化框架，通过K步梯度下降确保模型从初始状态开始对每个异构数据源都能达到局部最优，使用一阶近似方法求解，与模型无关元学习(MAML)有联系

Result: 在多领域和多语言数据集上的实验表明，该方法能显著提高自监督预训练模型在下游监督微调任务中的适应性

Conclusion: 提出的平衡约束和双层优化方法有效解决了异构数据自监督预训练问题，提高了模型的适应性能

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [95] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于梯度的示例选择算法，用于上下文学习中的提示调整和链式思维推理，通过梯度估计实现了高效的子集选择。


<details>
  <summary>Details</summary>
Motivation: 解决在固定模型参数的情况下，如何高效地从n个示例中选择k个最佳示例来优化上下文学习的性能。这在提示调整和链式思维推理中具有广泛应用。

Method: 提出一种基于输出梯度的方法，通过在输入嵌入空间中的一阶近似估计模型输出。对多个随机采样子集进行运算，并聚合结果形成每个示例的影响分数，最终选择k个最相关的示例。

Result: 算法在六个数据集上实现了小于1%的误差，将子集选择速度提升了达37.7倍，并在最大十340亿参数的模型上验证了效率。与基于输入嵌入的方法相比，性能平均提升了11%。

Conclusion: 该方法仅需预计算模型输出和梯度一次，实现了线性时间复杂度，为大规模上下文学习提供了高效的示例选择方案。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [96] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 本研究针对电商产品分类的平台异构性和现有分类法结构限制问题，开发了多模态分层分类框架，在27万+产品数据上实现了98.59%的F1分数，并提出了产品重分类流程发现细粒度类别，最终部署到商业平台中。


<details>
  <summary>Details</summary>
Motivation: 解决电商产品分类中的平台异构性问题（不同平台分类标准不一致）和现有分类法的结构限制（类别层次不完整或不一致），提升跨平台产品分类的准确性和一致性。

Method: 使用多模态分层分类框架，整合文本特征（RoBERTa）、视觉特征（ViT）和视觉-语言联合表示（CLIP）。研究早期融合、晚期融合和基于注意力的融合策略，采用动态掩码确保分类一致性。针对浅层或不一致类别，提出自监督的产品重分类流程（SimCLR、UMAP和级联聚类）。

Result: CLIP嵌入通过MLP晚期融合策略获得最高分层F1分数98.59%，优于单模态基线。产品重分类流程发现了新的细粒度类别（如鞋类子类型），聚类纯度超过86%。跨平台实验显示复杂晚期融合方法在多样化训练数据上准确率最高，而简单早期融合方法对未见平台泛化更好。

Conclusion: 成功开发了可工业规模部署的多模态分层分类框架，通过两阶段推理管道（轻量级RoBERTa阶段+GPU加速多模态阶段）在商业交易智能平台中实现成本与准确性的平衡，解决了电商产品分类的实际工业挑战。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [97] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 该研究开发了一个检测微调过程中快速转变的框架，使用分布变化检测方法和基于自然语言的序参量来量化LLM微调中的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解在狭窄有害数据集上微调LLMs时，何时以及如何出现与人类价值观广泛不一致的涌现性错位行为。

Method: 结合分布变化检测方法和由LLM评估的英文序参量，使用客观统计差异度量来量化微调过程中的相变对模型多方面的影响。

Result: 发现实际行为转变比梯度范数峰值指示的时间更晚，能够自动发现和量化基于语言的序参量，并在知识问题、政治和伦理等多个领域进行验证。

Conclusion: 该框架能够有效检测和表征微调过程中的快速转变，为理解LLM微调过程中的行为变化提供了系统化的分析方法。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [98] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony是一个去中心化的多智能体系统，通过分布式账本、Beacon选择协议和加权投票机制，让轻量级LLM在消费级GPU上协同工作，解决了集中式编排的高成本和通信限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架大多采用集中式编排，存在部署成本高、通信拓扑僵化、适应性有限等问题，需要更轻量、灵活的去中心化解决方案。

Method: 提出Symphony系统，包含三个核心机制：1）去中心化账本记录能力；2）Beacon选择协议进行动态任务分配；3）基于思维链的加权结果投票。

Result: 在推理基准测试中优于现有基线方法，实现了显著的准确率提升，并在不同容量模型间展现出良好的鲁棒性。

Conclusion: Symphony提供了一种隐私保护、可扩展、容错且低开销的去中心化编排方案，使轻量级LLM能够在消费级硬件上有效协作。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [99] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop是一个用于神经网络预测模型中人工指导的偏见缓解工具，通过从神经网络中提取决策树让用户检查和修改不公平决策逻辑，然后微调原始模型以实现更公平的预测


<details>
  <summary>Details</summary>
Motivation: 敏感属性如性别或年龄在机器学习任务中可能导致不公平预测，特别是在不考虑上下文的情况下使用时，需要一种能够选择性处理敏感属性影响的方法

Method: 从神经网络中提取决策树，允许用户检查和修改不公平的决策逻辑，然后使用这些修改来微调原始模型，实现上下文感知的偏见消除

Result: 相比其他公平性方法，FairLoop通过人工参与实现上下文感知的偏见消除，能够选择性地处理敏感属性的影响而不是统一排除它们

Conclusion: FairLoop提供了一种有效的人类引导偏见缓解方法，通过决策树可视化和人工干预，实现了对神经网络预测模型的选择性公平化处理

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [100] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: 使用大型语言模型为个性化营销邮件生成主题标题，通过离线和在线实验验证能有效提升用户参与度


<details>
  <summary>Details</summary>
Motivation: 传统电商营销邮件的标题采用固定模板，无法充分激发用户对个性化内容的兴趣，限制了邮件营销效果

Method: 利用大型语言模型生成反映邮件个性化内容的主题标题，进行离线模拟和百万级用户的在线实验

Result: 实验证明该技术能有效改善客户与邮件之间的互动参与度

Conclusion: 大型语言模型可以安全、自动化地为数百万用户生成邮件标题，提升营销邮件效果

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [101] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究通过注意力头剪枝来防御预训练语言模型中的后门攻击，提出了六种剪枝策略，实验表明梯度剪枝对语法触发攻击效果最好，强化学习和贝叶斯剪枝对风格攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成严重威胁，这些攻击通过隐蔽的恶意触发器绕过传统检测，使得事后净化变得至关重要。用户通常不了解攻击触发器，因此需要无需触发器知识或干净参考模型的防御方法。

Method: 设计了六种剪枝策略：梯度剪枝、层间方差剪枝、结构化L1/L2稀疏化梯度剪枝、随机集成剪枝、强化学习引导剪枝和贝叶斯不确定性剪枝。每种方法迭代移除信息量最少的注意力头，同时监控验证准确率以避免过度剪枝。

Result: 实验评估显示，梯度剪枝在防御语法触发攻击方面表现最佳，而强化学习和贝叶斯剪枝在抵御风格攻击方面效果更好。

Conclusion: 注意力头剪枝可以有效缓解后门攻击威胁，不同剪枝策略适用于不同类型的攻击，为无需触发器知识的后门防御提供了有效解决方案。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [102] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 本文通过将多臂老虎机强化学习算法应用于失败导向搜索(FDS)，在作业车间调度和资源受限项目调度问题上实现了显著的性能提升，比原始FDS快1.7-2.1倍，比IBM CP Optimizer快2.1-3.5倍，并改进了多个基准实例的下界。


<details>
  <summary>Details</summary>
Motivation: 失败导向搜索(FDS)是约束编程中重要的通用搜索算法，在调度问题上表现优异。研究发现FDS的搜索树最小化与多臂老虎机问题密切相关，这为应用强化学习算法优化FDS提供了理论基础。

Method: 将多臂老虎机强化学习算法应用于FDS，并针对具体问题进行改进和参数调优。在作业车间调度问题(JSSP)和资源受限项目调度问题(RCPSP)上进行评估，使用新开发的OptalCP求解器进行实验。

Result: 增强版FDS在JSSP上比原始实现快1.7倍，在RCPSP上快2.1倍；比IBM CP Optimizer 22.1中的FDS算法在JSSP上快3.5倍，在RCPSP上快2.1倍。在900秒时间限制下，改进了84个JSSP实例中78个和393个RCPSP实例中226个的最优下界，并完全解决了部分实例。

Conclusion: 基于多臂老虎机强化学习的FDS增强方法显著提升了搜索效率，在标准调度基准测试中表现出色，证明了强化学习与约束编程结合的有效性，为求解困难的调度问题提供了新的有效方法。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [103] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 本研究扩展了CybORG的Cage Challenge 2环境，新增了Patch、Isolate和Unisolate三个动作，改进了奖励信号和特征空间设计，并使用DQN和PPO算法验证了这些改进的有效性。


<details>
  <summary>Details</summary>
Motivation: 在自主网络操作(ACO)中，模拟环境对于强化学习训练至关重要。现有环境需要更准确地反映真实网络安全场景中人类操作员的能力，并提供更好的训练信号。

Method: 首先扩展CybORG环境，添加三个新动作；然后改进奖励信号和智能体特征空间设计；最后使用DQN和PPO算法在更新后的环境中进行训练验证。

Result: 研究表明CybORG可以通过添加现实功能进行扩展，同时保持其生成有效强化学习训练信号的能力。改进后的环境能够更好地支持RL智能体训练。

Conclusion: 该框架成功增强了CybORG环境的真实性和功能性，为自主网络操作中的强化学习训练提供了更有效的模拟平台，证明了环境扩展的可行性和价值。

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [104] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: 提出基于强化学习的框架，通过复合奖励函数在微调大语言模型时同时优化显性和隐性隐私保护、语义保真度和输出多样性，有效平衡数据效用和用户隐私。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统依赖大规模高质量数据集，但这些数据包含敏感个人信息。传统匿名化技术可能降低模型性能且无法有效防御利用写作风格、主题焦点等隐性信号的推理攻击，需要更强大的隐私保护机制。

Method: 使用强化学习框架微调大语言模型，采用复合奖励函数联合优化显性/隐性隐私、语义保真度和输出多样性。隐私奖励结合语义线索和基于潜在表示最小生成树的结构模式，在分布上下文中建模隐私敏感信号。

Result: 实证结果表明，该方法显著增强了作者混淆和隐私指标，同时不降低语义质量，为大语言模型时代的隐私保护数据生成提供了可扩展且模型无关的解决方案。

Conclusion: 该强化学习框架成功解决了隐私保护与数据效用之间的平衡问题，能够生成既保持实用性又降低隐私风险的合成重写文本，适用于大规模语言模型环境下的隐私保护数据生成。

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [105] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: 本文系统分析了联邦学习中梯度反转攻击的隐私风险，发现在推理模式下攻击更容易成功，而在训练模式下需要特定架构条件才能实现有效攻击。研究提出了两种新攻击方法，并首次对生产级目标检测模型进行了攻击测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注推理模式下的梯度反转攻击，但忽略了训练模式下的实际部署场景。需要系统分析不同架构和训练行为对隐私漏洞的影响，为实际联邦学习部署提供更准确的风险评估。

Method: 系统分析不同神经网络架构和训练行为对梯度反转攻击的影响；开发两种新型攻击方法，针对训练模式下的模型；首次对生产级目标检测模型进行攻击测试；建立全面的设置映射，明确不同架构选择和操作模式对隐私的影响。

Result: 发现推理模式下攻击显著简化；训练模式下成功攻击需要同时满足多个架构条件：浅层宽网络、跳跃连接、预激活归一化；提出的新攻击方法在现实训练条件下达到最先进性能；生产级目标检测模型显示出强大的内在鲁棒性。

Conclusion: 研究重新定义了梯度反转风险评估框架，提供了模型何时易受攻击、何时表现鲁棒以及何处可能存在细微泄漏的实用见解，为未来研究和部署场景中的隐私风险评估提供了重要指导。

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [106] [Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework](https://arxiv.org/abs/2508.19276)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 本文分析了NISQ时代量子计算资源管理的重要性，提出了运行时感知的量子软件开发愿景，并开发了Qonscious原型框架来实现基于动态资源评估的条件执行。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备存在量子比特数量有限、错误率高、相干时间短等限制，需要高效管理量子资源来设计和部署量子算法。

Method: 分析量子资源在NISQ设备中的角色，提出运行时感知的量子软件开发方法，并开发Qonscious原型框架支持基于动态资源评估的条件执行。

Result: 提出了量子资源估计(QRE)领域的强化方向，开发了可扩展、可靠且资源感知的量子软件开发方法。

Conclusion: 该研究为量子软件开发提供了资源感知的新范式，通过Qonscious框架展示了动态资源管理在NISQ时代的可行性，推动了量子软件工程的发展。

Abstract: Quantum computers represent a radical technological advancement in the way
information is processed by using the principles of quantum mechanics to solve
very complex problems that exceed the capabilities of classical systems.
However, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),
the available hardware presents several limitations, such as a limited number
of qubits, high error rates, and reduced coherence times. Efficient management
of quantum resources, both physical (qubits, error rates, connectivity) and
logical (quantum gates, algorithms, error correction), becomes particularly
relevant in the design and deployment of quantum algorithms. In this work, we
analyze the role of resources in the various uses of NISQ devices today,
identifying their relevance and implications for software engineering focused
on the use of quantum computers. We propose a vision for runtime-aware quantum
software development, identifying key challenges to its realization, such as
limited introspection capabilities and temporal constraints in current
platforms. As a proof of concept, we introduce Qonscious, a prototype framework
that enables conditional execution of quantum programs based on dynamic
resource evaluation. With this contribution, we aim to strengthen the field of
Quantum Resource Estimation (QRE) and move towards the development of scalable,
reliable, and resource-aware quantum software.

</details>


### [107] [Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning](https://arxiv.org/abs/2508.19327)
*Pilsung Kang*

Main category: quant-ph

TL;DR: 该论文将量子纠缠重新解释为一种"超级混杂"资源，通过因果推断框架证明量子相关性违反经典因果界限，并开发了量子DO演算用于因果特征选择，在量子机器学习中显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 重新从现代因果推断的角度理解贝尔定理揭示的量子力学与局域实在论之间的深刻冲突，探索量子纠缠作为因果资源的独特性质。

Method: 提出量化混杂强度的概念，建立物理混杂层次结构，开发基于电路的量子DO演算实现，并将其应用于量子机器学习中的因果特征选择问题。

Result: 计算验证了量子纠缠产生的相关性违反经典因果界限，在量子机器学习应用中，因果特征选择使模型鲁棒性平均绝对提升了11.3%。

Conclusion: 该框架成功连接了量子基础理论和因果人工智能，为量子相关性提供了新的实用视角，证明了量子DO演算在区分因果关系和伪相关性方面的有效性。

Abstract: Bell's theorem reveals a profound conflict between quantum mechanics and
local realism, a conflict we reinterpret through the modern lens of causal
inference. We propose and computationally validate a framework where quantum
entanglement acts as a "super-confounding" resource, generating correlations
that violate the classical causal bounds set by Bell's inequalities. This work
makes three key contributions: First, we establish a physical hierarchy of
confounding (Quantum > Classical) and introduce Confounding Strength (CS) to
quantify this effect. Second, we provide a circuit-based implementation of the
quantum $\mathcal{DO}$-calculus to distinguish causality from spurious
correlation. Finally, we apply this calculus to a quantum machine learning
problem, where causal feature selection yields a statistically significant
11.3% average absolute improvement in model robustness. Our framework bridges
quantum foundations and causal AI, offering a new, practical perspective on
quantum correlations.

</details>


### [108] [Is data-efficient learning feasible with quantum models?](https://arxiv.org/abs/2508.19437)
*Alona Sakhnenko,Christian B. Mendl,Jeanette M. Lorenz*

Main category: quant-ph

TL;DR: 该研究提出了一个分析量子机器学习数据集复杂性的框架，重点关注量子核方法在数据效率方面的优势，并开发了新的分析工具来验证量子-经典性能差距。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习领域缺乏对数据集特性的统一理解框架，需要研究量子模型是否比经典模型具有更好的数据效率，特别是在量子核方法方面。

Method: 开发了生成半人工全经典数据集的方法，使用量子核方法进行训练，并引入了从经典核方法衍生的新分析工具来研究量子-经典性能差距。

Result: 实证结果显示量子核方法相比经典对应方法能用更少的训练数据实现低错误率，提出的分析工具预测性能与实证证据高度一致。

Conclusion: 该研究为全面探索数据集复杂性铺平了道路，有助于深入理解量子核方法模型的泛化优势，为领域未来发展奠定了基础。

Abstract: The importance of analyzing nontrivial datasets when testing quantum machine
learning (QML) models is becoming increasingly prominent in literature, yet a
cohesive framework for understanding dataset characteristics remains elusive.
In this work, we concentrate on the size of the dataset as an indicator of its
complexity and explores the potential for QML models to demonstrate superior
data-efficiency compared to classical models, particularly through the lens of
quantum kernel methods (QKMs). We provide a method for generating
semi-artificial fully classical datasets, on which we show one of the first
evidence of the existence of classical datasets where QKMs require less data
during training. Additionally, our study introduces a new analytical tool to
the QML domain, derived for classical kernel methods, which can be aimed at
investigating the classical-quantum gap. Our empirical results reveal that QKMs
can achieve low error rates with less training data compared to classical
counterparts. Furthermore, our method allows for the generation of datasets
with varying properties, facilitating further investigation into the
characteristics of real-world datasets that may be particularly advantageous
for QKMs. We also show that the predicted performance from the analytical tool
we propose - a generalization metric from classical domain - show great
alignment empirical evidence, which fills the gap previously existing in the
field. We pave a way to a comprehensive exploration of dataset complexities,
providing insights into how these complexities influence QML performance
relative to traditional methods. This research contributes to a deeper
understanding of the generalization benefits of QKM models and potentially a
broader family of QML models, setting the stage for future advancements in the
field.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [109] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该论文将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法(CAA)将这些因素映射到激活方向，并提出可解释的向量干预方法来缓解安全关键行为。


<details>
  <summary>Details</summary>
Motivation: 奉承行为是LLM中的关键行为风险，但通常被当作孤立的故障模式处理。作者认为应该将其建模为心理测量特质（如情绪性、开放性和宜人性）的组合，类似于心理测量学中的因子分解。

Method: 使用对比激活加法(CAA)将激活方向映射到心理测量因素，研究不同因素组合如何导致奉承行为（如高外向性结合低尽责性）。

Result: 提出了可解释和可组合的基于向量的干预方法，包括加法、减法和投影操作，可用于缓解LLM中的安全关键行为。

Conclusion: 通过将奉承行为视为心理测量特质的几何和因果组合，提供了更深入的理解和有效的干预手段，为LLM安全性研究提供了新的视角和方法。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [110] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文提出了监控系统红队测试(MRT)工作流，用于检测自主LLM代理的隐蔽不当行为，发现代理对监控的知晓会显著降低监控可靠性，而混合分层-顺序监控框架比监控信息获取更重要。


<details>
  <summary>Details</summary>
Motivation: 随着自主LLM代理的广泛应用，需要有效监控系统来检测其隐蔽不当行为（如秘密分享私人信息），但目前缺乏系统性的红队测试方法来评估监控系统的鲁棒性。

Method: 建立了MRT工作流，包含：(1)不同级别的代理和监控情境感知；(2)规避监控的对抗策略（如提示注入）；(3)两个数据集和环境（SHADE-Arena和CUA-SHADE-Arena）；测试了现有LLM监控框架和新提出的混合分层-顺序框架。

Result: 三个关键发现：1)代理知晓监控会显著降低监控可靠性；2)混合监控框架优于基线框架，能使较弱模型可靠监控较强代理；3)在人类参与环节中，仅对预标记案例进行人工审查可将TPR提高约15%（FPR=0.01时）。

Conclusion: 建立了MRT标准工作流，揭示了LLM和人类在监控代理不当行为时的对抗鲁棒性不足，发布了代码、数据和日志以促进进一步研究。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [111] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL是一个统一的LLM强化学习范式，通过改进的GRPO算法和基于价值模型的测试时解码方法，显著提升LLM的代码推理能力，在多个编程基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法GRPO因奖励方差不足而失败，基于过程奖励模型(PRM)的验证方法存在训练数据获取困难和验证效果不佳的问题，需要新的解决方案来提升LLM的推理准确性。

Method: 采用两阶段方法：1) ReST-GRPO阶段使用优化的ReST算法筛选和组装高价值训练数据，增加GRPO采样的奖励方差；2) VM-MCTS测试时解码方法通过蒙特卡洛树搜索收集无标注的价值目标，训练价值模型为解码提供精确的过程信号和验证分数。

Result: 在APPS、BigCodeBench、HumanEval等多个编程基准测试中，ReST-RL显著优于其他强化训练基线(如原生GRPO和ReST-DPO)以及解码和验证基线(如PRM-BoN和ORM-MCTS)。

Conclusion: ReST-RL通过结合改进的GRPO算法和基于价值模型的测试时解码优化，有效提升了LLM策略的推理能力，为解决LLM推理准确性提供了有效的强化学习范式。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [112] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了从数据科学向模型科学范式转变的概念框架，强调以训练好的模型为核心进行分析，包含验证、解释、控制和接口四大支柱。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛应用，需要从数据为中心的方法转向以模型为核心的分析范式，以应对模型在不同操作环境中的行为交互、验证、解释和控制需求。

Method: 提出了模型科学的概念框架，包含四个关键支柱：验证（严格的情境感知评估协议）、解释（探索模型内部操作的各种方法）、控制（整合对齐技术来引导模型行为）和接口（开发交互式和可视化解释工具）。

Result: 建立了一个系统性的模型科学框架，为开发可信、安全且与人类对齐的AI系统提供指导。

Conclusion: 模型科学框架为AI系统的可信性、安全性和人类对齐性提供了新的研究方向和方法论基础，推动了从数据科学向模型科学的范式转变。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [113] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: 提出了聚合虚拟博弈(agg-FP)算法，通过聚合其他智能体的动作来减少动作空间，在匿名多矩阵游戏中保持收敛性并加速收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统虚拟博弈在智能体数量增加时面临联合动作空间指数增长的问题，导致奖励探索缓慢。匿名游戏结构可以缓解这一问题

Method: 开发了聚合虚拟博弈变体，每个智能体跟踪其他智能体选择每个动作的数量频率，而不是单个智能体的动作

Result: 在匿名多矩阵游戏中，agg-FP在相同条件下收敛到纳什均衡，并通过模拟验证了收敛加速效果

Conclusion: 通过动作聚合可以在不损失收敛保证的前提下显著减少动作空间，从而加速虚拟博弈算法的收敛速度

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [114] [Simple Stepsize for Quasi-Newton Methods with Global Convergence Guarantees](https://arxiv.org/abs/2508.19712)
*Artem Agafonov,Vladislav Ryspayev,Samuel Horváth,Alexander Gasnikov,Martin Takáč,Slavomir Hanzely*

Main category: math.OC

TL;DR: 本文提出了一种简单的步长调度策略，为拟牛顿方法在凸函数上提供了O(1/k)的全局收敛率保证，并在控制Hessian近似误差的条件下实现了O(1/k²)的加速收敛率。


<details>
  <summary>Details</summary>
Motivation: 拟牛顿方法虽然在实际应用中广泛使用且效率高，但其全局收敛性通常只在特定线搜索策略和强凸性假设下得到保证。本文旨在扩展拟牛顿方法的理论理解，提供更强的全局收敛保证。

Method: 引入简单的步长调度策略，控制Hessian近似的相对精度，并开发了自适应变体以增强鲁棒性。

Result: 理论分析表明该方法在凸函数上达到O(1/k)的全局收敛率，在控制Hessian近似误差时达到O(1/k²)的加速收敛率，与Nesterov加速梯度方法和立方正则化牛顿方法的最佳已知速率相匹配。实证比较验证了理论发现。

Conclusion: 所提出的方法为拟牛顿方法提供了更强的理论保证，在保持实用性的同时实现了与最优方法相当的收敛速率，并通过自适应变体进一步增强了鲁棒性。

Abstract: Quasi-Newton methods are widely used for solving convex optimization problems
due to their ease of implementation, practical efficiency, and strong local
convergence guarantees. However, their global convergence is typically
established only under specific line search strategies and the assumption of
strong convexity. In this work, we extend the theoretical understanding of
Quasi-Newton methods by introducing a simple stepsize schedule that guarantees
a global convergence rate of ${O}(1/k)$ for the convex functions. Furthermore,
we show that when the inexactness of the Hessian approximation is controlled
within a prescribed relative accuracy, the method attains an accelerated
convergence rate of ${O}(1/k^2)$ -- matching the best-known rates of both
Nesterov's accelerated gradient method and cubically regularized Newton
methods. We validate our theoretical findings through empirical comparisons,
demonstrating clear improvements over standard Quasi-Newton baselines. To
further enhance robustness, we develop an adaptive variant that adjusts to the
function's curvature while retaining the global convergence guarantees of the
non-adaptive algorithm.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [115] [Inferring geometry and material properties from Mueller matrices with machine learning](https://arxiv.org/abs/2508.19713)
*Lars Doorenbos,C. H. Lucas Patty,Raphael Sznitman,Pablo Márquez-Neila*

Main category: physics.optics

TL;DR: 使用机器学习从穆勒矩阵同时推断表面几何和材料属性，证明了即使材料类型未知也能重建物体几何和识别材料类型


<details>
  <summary>Details</summary>
Motivation: 穆勒矩阵同时编码几何和材料信息，但传统方法难以同时恢复两者，这是一个病态问题。研究探索机器学习是否能从穆勒矩阵中充分提取这些信息

Method: 使用各向同性材料球体的数据集，在五个可见波长(450-650nm)下采集全角度域的穆勒矩阵。训练机器学习模型仅以穆勒矩阵为输入来预测材料属性和表面法线

Result: 即使材料类型未知，也能预测表面法线并重建物体几何。穆勒矩阵使模型能正确识别材料类型。对角元素对材料表征关键，非对角元素对法线估计决定性

Conclusion: 穆勒矩阵包含足够信息用于同时推断表面几何和材料属性，机器学习方法能有效解决这一传统病态问题

Abstract: Mueller matrices (MMs) encode information on geometry and material
properties, but recovering both simultaneously is an ill-posed problem. We
explore whether MMs contain sufficient information to infer surface geometry
and material properties with machine learning. We use a dataset of spheres of
various isotropic materials, with MMs captured over the full angular domain at
five visible wavelengths (450-650 nm). We train machine learning models to
predict material properties and surface normals using only these MMs as input.
We demonstrate that, even when the material type is unknown, surface normals
can be predicted and object geometry reconstructed. Moreover, MMs allow models
to identify material types correctly. Further analyses show that diagonal
elements are key for material characterization, and off-diagonal elements are
decisive for normal estimation.

</details>


### [116] [Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields](https://arxiv.org/abs/2508.19751)
*Joshua R. Jandrell,Mitchell A. Cox*

Main category: physics.optics

TL;DR: 提出使用傅里叶特征网络来准确建模高频振荡光学场，相比标准MLP减少85%参数的同时将预测误差降低一个数量级


<details>
  <summary>Details</summary>
Motivation: 标准多层感知器(MLP)在处理高频振荡复值函数时存在频谱偏差问题，无法有效拟合高频正弦波，需要新的方法来准确建模光学扰动效应

Method: 将预定义的傅里叶特征（与扰动相关的正弦函数集）作为网络额外输入，将学习问题从近似复杂函数重构为寻找基函数的线性组合

Result: 在多模光纤机械压缩传输矩阵预测任务中，傅里叶特征网络将输出场振幅和相位的预测误差降低一个数量级，平均复相关系数达到0.995

Conclusion: 该方法为准确建模广泛类别的振荡物理系统提供了一种通用且鲁棒的方法

Abstract: Modelling the effects of perturbations on optical fields often requires
learning highly oscillatory complex-valued functions. Standard multi-layer
perceptrons (MLPs) struggle with this task due to an inherent spectral bias,
preventing them from fitting high-frequency sinusoids. To overcome this, we
incorporate Fourier features - a set of predefined sinusoids dependent on the
perturbation - as an additional network input. This reframes the learning
problem from approximating a complex function to finding a linear combination
of basis functions. We demonstrate this method by training a Fourier Feature
Network to predict the transmission matrix of a multimode fibre under
mechanical compression. Compared to a standard MLP, our network reduces
prediction error in the output field's amplitude and phase by an order of
magnitude, achieving a mean complex correlation of 0.995 with the ground truth,
despite using 85% fewer parameters. This approach offers a general and robust
method for accurately modelling a wide class of oscillatory physical systems.

</details>


### [117] [On-chip wave chaos for photonic extreme learning](https://arxiv.org/abs/2508.19878)
*Matthew R. Wilson,Jack A. Smith,Michael J. Strain,Xavier Porte*

Main category: physics.optics

TL;DR: 本文通过实验验证了基于波混沌微空洞的芯片级光子极限学习机的可行性，利用光子调制技术实现高效能的分类任务


<details>
  <summary>Details</summary>
Motivation: 随着可扩展和能源效率人工神经网络需求的增长，集成光子学提供了组床、并行和超高速信息处理平台，特别适合极限学习机架构

Method: 设计基于跑道式微空洞的光子极限学习机，通过波长编码输入信息，利用SU-8聚合物直接光刷技术制造微空洞，通过散射壁收集漏泰模式光作为读出层

Result: 通过高分辨率波长扫描获得了无相关和非周期性的斑点行为，在四种不同的标准分类任务中展现了良好的系统性能，并能通过测量散射壁不同部位来控制输出节点数量

Conclusion: 该光子极限学习机平台具有可优化性能的灵活性，能够根据不同任务需求调整读出层规模，为高效能人工智能硬件开发提供了新的解决方案

Abstract: The increase in demand for scalable and energy efficient artificial neural
networks has put the focus on novel hardware solutions. Integrated photonics
offers a compact, parallel and ultra-fast information processing platform,
specially suited for extreme learning machine (ELM) architectures. Here we
experimentally demonstrate a chip-scale photonic ELM based on wave chaos
interference in a stadium microcavity. By encoding the input information in the
wavelength of an external single-frequency tunable laser source, we leverage
the high sensitivity to wavelength of injection in such photonic resonators. We
fabricate the microcavity with direct laser writing of SU-8 polymer on glass. A
scattering wall surrounding the stadium operates as readout layer, collecting
the light associated with the cavity's leaky modes. We report uncorrelated and
aperiodic behavior in the speckles of the scattering barrier from a high
resolution scan of the input wavelength. Finally, we characterize the system's
performance at classification in four qualitatively different benchmark tasks.
As we can control the number of output nodes of our ELM by measuring different
parts of the scattering barrier, we demonstrate the capability to optimize our
photonic ELM's readout size to the performance required for each task.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [118] [Internally-Convex Drawings of Outerplanar Graphs in Small Area](https://arxiv.org/abs/2508.19913)
*Michael A. Bekos,Giordano Da Lozzo,Fabrizio Frati,Giuseppe Liotta,Antonios Symvonis*

Main category: cs.CG

TL;DR: 该论文改进了外平面图的网格直线绘制算法，将面积从O(n²)优化到O(n¹.⁵)，并对特定类别的外平面图提供了严格凸多边形内部面的绘制方法。


<details>
  <summary>Details</summary>
Motivation: Kant在1996年提出的算法虽然能够为外平面图生成保持嵌入的平面直线网格绘制，但其O(n²)的面积复杂度较大，需要进一步优化。同时，对于要求内部面为严格凸多边形的情况，也需要新的绘制方法。

Method: 提出了一种新的算法来计算外平面图的网格直线绘制，优化了面积复杂度。对于内部面要求严格凸多边形的情况，针对弱对偶为路径的外平面图，开发了基于最大内部面循环大小的绘制算法。

Result: 成功将外平面图的网格直线绘制面积从O(n²)降低到O(n¹.⁵)。对于特定类别的外平面图，实现了Θ(nk²)面积的严格凸多边形内部面绘制，其中k是最大内部面循环大小。

Conclusion: 该研究在外平面图绘制领域取得了显著进展，不仅大幅降低了传统绘制方法的面积复杂度，还为严格凸多边形内部面的绘制提供了有效的解决方案，对图绘制算法的发展具有重要意义。

Abstract: A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

</details>


### [119] [Visualizing Treewidth](https://arxiv.org/abs/2508.19935)
*Alvin Chiu,Thomas Depian,David Eppstein,Michael T. Goodrich,Martin Nöllenburg*

Main category: cs.CG

TL;DR: 本文研究并实现了展示图有界路径宽度或树宽度的见证绘图方法，通过绘制树分解或路径分解作为袋树结构，在每个袋中显示诱导子图，并用轨道连接顶点在多个袋中的副本。


<details>
  <summary>Details</summary>
Motivation: 为了清晰展示图的有界路径宽度或树宽度属性，需要开发有效的可视化方法来呈现树分解结构，使图的这一重要属性能够直观地被观察和理解。

Method: 实现可视化原型，使用动态规划优化小宽度图的交叉最小化，对较大宽度图采用启发式方法。提出分类法：袋内子图呈现为单/双页弧图或圆形布局，轨道用直线或轨道径向路径渲染。

Result: 开发了多种绘图范式，能够有效展示图的路径宽度和树宽度属性，通过优化的顶点布局避免边和轨道的交叉。

Conclusion: 该方法为图的有界宽度属性提供了有效的可视化解决方案，通过不同的绘图风格选择可以适应不同的可视化需求和应用场景。

Abstract: A witness drawing of a graph is a visualization that clearly shows a given
property of a graph. We study and implement various drawing paradigms for
witness drawings to clearly show that graphs have bounded pathwidth or
treewidth. Our approach draws the tree decomposition or path decomposition as a
tree of bags, with induced subgraphs shown in each bag, and with ''tracks'' for
each graph vertex connecting its copies in multiple bags. Within bags, we
optimize the vertex layout to avoid crossings of edges and tracks. We implement
a visualization prototype for crossing minimization using dynamic programming
for graphs of small width and heuristic approaches for graphs of larger width.
We introduce a taxonomy of drawing styles, which render the subgraph for each
bag as an arc diagram with one or two pages or as a circular layout with
straight-line edges, and we render tracks either with straight lines or with
orbital-radial paths.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [120] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的道岔机故障检测方法，仅需功率信号输入即可实现高精度故障分类，准确率超过99.99%，具有技术无关性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 道岔机是铁路关键安全设备，故障会导致运营中断。现有方法需要多输入和定制特征，缺乏可扩展性且受技术类型限制。

Method: 使用深度学习模型分析道岔机运动期间的功率信号模式，仅需单一输入即可分类正常和故障状态，并采用保形预测提供置信度。

Result: 实现了>99.99%的精确度，<0.01%的误报率和可忽略的漏报率，在真实环境和测试平台上验证了多种机电道岔机的可扩展性。

Conclusion: 该方法技术无关、可扩展，通过保形预测提供输出确定性指示，符合ISO-17359标准，为预防性维护提供了有效解决方案。

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [121] [Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission](https://arxiv.org/abs/2508.19910)
*Sergio Hernandez,Christophe Peucheret,Francesco Da Ros,Darko Zibar*

Main category: eess.SP

TL;DR: 通过数据驱动代模型实现直接调制激光器系统的端到端优化，在低调制功耗和小带宽下获得更优性能


<details>
  <summary>Details</summary>
Motivation: 直接调制激光器(DML)在短距离通信系统中很有吸引力，但其复杂的非线性动力学特性给建模和优化带来挑战

Method: 基于实验数据训练的数据驱动代模型，进行端到端优化，包括脏冲形成、均衡器滤波器、偏置电流和调制射频功率

Result: 在各种符号速率和传输距离下都显示出更优的性能，同时使用更低的调制射频功率、更少的滤波器派次和更小的信号带宽

Conclusion: 端到端优化方案能够有效提升DML系统性能，为短距离通信系统提供更高效的解决方案

Abstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach
intensity modulation and direct detection communication systems. However, their
complex nonlinear dynamics make the modeling and optimization of DML-based
systems challenging. In this paper, we study the end-to-end optimization of
DML-based systems based on a data-driven surrogate model trained on
experimental data. The end-to-end optimization includes the pulse shaping and
equalizer filters, the bias current and the modulation radio-frequency (RF)
power applied to the laser. The performance of the end-to-end optimization
scheme is tested on the experimental setup and compared to 4 different
benchmark schemes based on linear and nonlinear receiver-side equalization. The
results show that the proposed end-to-end scheme is able to deliver better
performance throughout the studied symbol rates and transmission distances
while employing lower modulation RF power, fewer filter taps and utilizing a
smaller signal bandwidth.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [122] [Large Language Models (LLMs) for Electronic Design Automation (EDA)](https://arxiv.org/abs/2508.20030)
*Kangwei Xu,Denis Schwachhofer,Jason Blocklove,Ilia Polian,Peter Domanski,Dirk Pflüger,Siddharth Garg,Ramesh Karri,Ozgur Sinanoglu,Johann Knechtel,Zhuorui Zhao,Ulf Schlichtmann,Bing Li*

Main category: eess.SY

TL;DR: 这篇论文综述了大语言模型在电子设计自动化中的应用，通过三个案例研究展示了LLMs在硬件设计、测试和优化中的能力，并提出了未来挖沉潜力的挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 随着雖成电路复杂度的增加，传统的设计到制造流程需要大量迭代，旷耗人力且容易出错。需要更高效的EDA解决方案来加速硬件开发。LLMs在语境理解、逻辑推理和生成能力方面的进步为整合到EDA中提供了机遇。

Method: 论文通过综述性评估方式，分析LLMs在EDA中的集成应用。重点关注LLMs的能力、限制和未来机遇。使用三个案例研究来展示LLMs在硬件设计、测试和优化中的实际能力。

Result: 研究展示了LLMs在EDA流程中的强大潜力，能够简化甚至自动化整个设计到制造工作流。案例研究证明了LLMs在各个设计阶段的应用效果。

Conclusion: 论文为研究人员提供了使用先进AI技术提升EDA的价值见解。虽然LLMs在EDA中展现了希望，但仍面临着需要解决的挑战。未来需要进一步探索LLMs在形成下一代EDA工具中的潜力。

Abstract: With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [123] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 这篇论文提出了一种轻量级的实时宏观流动预测模型，专门用于人群运动预测，在保持准确性的同时大幅提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统微观模型在密集人群中计算成本高，而现有宏观模型要么过于简单要么计算复杂，需要在预测准确性和计算效率之间找到平衡

Method: 基于行人流的内在特性，简化空间和时间处理，设计了一种轻量级的实时宏观人群预测模型

Result: 实现了推理时间减少3.6倍，同时预测准确性提高3.1%，集成到社会意识规划框架后能够支持机器人在动态环境中的高效社会合规导航

Conclusion: 高效的人群建模技术能够使机器人在不依赖高成本计算的情况下完成密集环境中的导航任务

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [124] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: 通过结合离散MAPF解算器与生成式滴析模型，提出了一种可扩展到100台机器人的高质量连续轨迹规划方法


<details>
  <summary>Details</summary>
Motivation: 解决离散MAPF方法轨迹质量低和连续优化方法维度灾难问题，寻找一种既可扩展又能产生高质量轨迹的多机器人运动规划方案

Method: 采用离散指导滴析(DGD)框架：(1)将非凸问题分解为可解的子问题，(2)结合MAPF解决方案与约束优化技术指导滴析模型，(3)包含轻量约束修复机制确保可行性

Result: 在大规模复杂环境中达到最先进性能，可扩展到100台机器人，具有高规划效率和高成功率

Conclusion: DGD框架有效结合了离散和连续方法的优势，为大规模多机器人运动规划提供了一种可扩展且高质量的解决方案

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [125] [An algorithm for accurate and simple-looking metaphorical maps](https://arxiv.org/abs/2508.19810)
*Eleni Katsanou,Tamara Mchedlidze,Antonios Symvonis,Thanos Tolias*

Main category: cs.DM

TL;DR: 本文扩展了基于力的隐喻地图算法，通过引入区域刚度、压力权重系数和支持非三角化图，显著提高了面积精度（接近完美）同时保持视觉简洁性


<details>
  <summary>Details</summary>
Motivation: 现有基于力的隐喻地图算法在视觉简洁性和面积精度之间存在权衡，面积误差可达30%。需要改进算法以在保持视觉简洁性的同时提高面积精度，并支持更一般的图结构

Method: 1. 引入区域刚度概念，根据区域压力动态调整刚度；2. 为多边形点添加压力权重系数，特别处理狭窄通道；3. 支持非三角化图，通过生成多点交汇点或引入孔洞

Result: 扩展算法能够构建面积精度接近完美的隐喻地图，在视觉简洁性方面仅有轻微牺牲，实验评估显示了显著改进

Conclusion: 提出的多维度扩展有效平衡了隐喻地图的面积精度和视觉简洁性两个优化目标，算法具有更好的通用性和实用性

Abstract: "Metaphorical maps" or "contact representations" are visual representations
of vertex-weighted graphs that rely on the geographic map metaphor. The
vertices are represented by countries, the weights by the areas of the
countries, and the edges by contacts/ boundaries among them. The accuracy with
which the weights are mapped to areas and the simplicity of the polygons
representing the countries are the two classical optimization goals for
metaphorical maps. Mchedlidze and Schnorr [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022] presented a force-based algorithm that
creates metaphorical maps that balance between these two optimization goals.
Their maps look visually simple, but the accuracy of the maps is far from
optimal - the countries' areas can vary up to 30% compared to required. In this
paper, we provide a multi-fold extension of the algorithm in [Metaphoric Maps
for Dynamic Vertex-weighted Graphs, EuroVis 2022]. More specifically:
  1. Towards improving accuracy: We introduce the notion of region stiffness
and suggest a technique for varying the stiffness based on the current pressure
of map regions.
  2. Towards maintaining simplicity: We introduce a weight coefficient to the
pressure force exerted on each polygon point based on whether the corresponding
point appears along a narrow passage.
  3. Towards generality: We cover, in contrast to [Metaphoric Maps for Dynamic
Vertex-weighted Graphs, EuroVis 2022], non-triangulated graphs. This is done by
either generating points where more than three regions meet or by introducing
holes in the metaphorical map.
  We perform an extended experimental evaluation that, among other results,
reveals that our algorithm is able to construct metaphorical maps with nearly
perfect area accuracy with a little sacrifice in their simplicity.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [126] [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](https://arxiv.org/abs/2508.19750)
*Binhui Zhang,Jianwei Ma*

Main category: stat.ML

TL;DR: Fractal Flow是一种新型标准化流架构，通过整合Kolmogorov-Arnold网络和潜在狄利克雷分配构建结构化潜在空间，并采用递归模块化设计提高表达能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升标准化流在高维密度估计和生成建模中的表达能力和可解释性，构建结构化、可解释的潜在空间并建模层次化语义聚类。

Method: 1. 整合Kolmogorov-Arnold网络和潜在狄利克雷分配(LDA)到标准化流中；2. 引入受分形生成模型启发的递归模块化设计；3. 构建结构化潜在空间和层次化语义聚类模型。

Result: 在MNIST、FashionMNIST、CIFAR-10和地球物理数据上的实验表明，Fractal Flow实现了潜在聚类、可控生成和优越的估计精度。

Conclusion: Fractal Flow通过创新的架构设计，在保持标准化流原则性框架的同时，显著提升了模型的表达能力和可解释性，在多个数据集上展现出优异的性能。

Abstract: Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

</details>


### [127] [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](https://arxiv.org/abs/2508.19841)
*Fahime Seyedheydari,Kevin Conley,Simo Särkkä*

Main category: stat.ML

TL;DR: 使用条件正则化流的概率性数据驱动代理模型，预测纳米粒子嵌入散射介质的辐射性质，提供高准确性预测和可靠的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络模型只能提供点估计，无法获得完整的后验预测分布和不确定性量化，而辐射传输模拟计算成本高。

Method: 采用条件正则化流模型，学习输入参数(吸收系数、散射系数、各向异性因子、粒子尺寸分布)到光学输出(反射率、吸收率、透射率)的条件分布。训练数据通过蒙特卡洛辐射传输模拟生成，光学性质来自Mie理论。

Result: 模型实现了高预测准确性和可靠的不确定性估计，能够生成完整的后验预测分布。

Conclusion: 该条件正则化流模型是一种高效且强大的辐射传输模拟代理，既能提供准确预测又能进行理论上有据的不确定性量化。

Abstract: We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

</details>


### [128] [The Information Dynamics of Generative Diffusion](https://arxiv.org/abs/2508.19897)
*Luca Ambrogioni*

Main category: stat.ML

TL;DR: 本文提供了一个统一的数学框架，将生成扩散模型的动态、信息论和热力学特性联系起来，揭示了生成过程本质上是受控的噪声诱导对称性破缺过程。


<details>
  <summary>Details</summary>
Motivation: 虽然生成扩散模型已成为机器学习中强大的模型类别，但对其运行机制的统一理论理解仍在发展中，需要建立整合的数学框架来连接其不同特性。

Method: 通过分析条件熵产生率与得分函数向量场散度的关系，将轨迹分支和生成分叉表征为能量景观中的对称性破缺相变，建立动态非线性滤波机制。

Result: 证明了生成带宽直接由得分函数向量场的期望散度控制，揭示了信息传递峰值对应于可能结果之间的临界转变，得分函数通过抑制与数据不兼容的波动来调节噪声带宽。

Conclusion: 生成过程本质上是受控的噪声诱导对称性破缺过程，这种综合视角为理解扩散模型的运作机制提供了强大的理论洞察。

Abstract: Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [129] [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](https://arxiv.org/abs/2508.19504)
*Kevin Song,Anand Jayarajan,Yaoyao Ding,Qidong Su,Zhanda Zhu,Sihang Liu,Gennady Pekhimenko*

Main category: cs.MA

TL;DR: 本研究提出通过优化系统环境而非改进智能体本身来提高LLM智能体成功率的方法，设计了Aegis环境优化技术，平均提升成功率6.7-12.5%


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注改进智能体本身，但忽视了系统环境对智能体性能的重要影响。智能体在复杂现实环境中的低成功率限制了实际部署

Method: 收集142个智能体轨迹（3656次交互），分析失败模式并提出6种交互失败分类法，设计Aegis环境优化技术：环境可观测性增强、通用计算卸载和推测性智能体动作

Result: Aegis技术在无需修改智能体和底层LLM的情况下，平均提升智能体成功率6.7-12.5%

Conclusion: 优化系统环境是提高LLM智能体性能的有效补充方向，环境优化可以显著提升智能体在复杂现实环境中的成功率

Abstract: Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

</details>


### [130] [Anomaly Detection in Networked Bandits](https://arxiv.org/abs/2508.20076)
*Xiaotong Cheng,Setareh Maghsudi*

Main category: cs.MA

TL;DR: 提出一种新颖的bandit算法，利用网络知识学习用户偏好和特征残差，实现个性化推荐和异常检测的同步进行


<details>
  <summary>Details</summary>
Motivation: 社交网络中异常节点可能导致严重后果，需要设计高效的在线学习算法来鲁棒地学习用户偏好并同时检测异常

Method: 通过网络知识表征用户偏好和特征信息残差，通过学习分析这些偏好和残差，为每个用户开发个性化推荐策略并同时检测异常

Result: 严格证明了算法regret的上界，并在合成和真实数据集上与多个最先进的协作上下文bandit算法进行了实验比较

Conclusion: 该方法能够有效处理社交网络中的异常检测问题，同时实现个性化推荐

Abstract: The nodes' interconnections on a social network often reflect their
dependencies and information-sharing behaviors. Nevertheless, abnormal nodes,
which significantly deviate from most of the network concerning patterns or
behaviors, can lead to grave consequences. Therefore, it is imperative to
design efficient online learning algorithms that robustly learn users'
preferences while simultaneously detecting anomalies.
  We introduce a novel bandit algorithm to address this problem. Through
network knowledge, the method characterizes the users' preferences and
residuals of feature information. By learning and analyzing these preferences
and residuals, it develops a personalized recommendation strategy for each user
and simultaneously detects anomalies. We rigorously prove an upper bound on the
regret of the proposed algorithm and experimentally compare it with several
state-of-the-art collaborative contextual bandit algorithms on both synthetic
and real-world datasets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [131] [MRExtrap: Longitudinal Aging of Brain MRIs using Linear Modeling in Latent Space](https://arxiv.org/abs/2508.19482)
*Jaivardhan Kapoor,Jakob H. Macke,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: MRExtrap通过卷积自编码器的潜在空间线性模型模拟大脑老化，利用年龄信息进行线性外推预测未来MRI扫描，在ADNI数据集上表现优于GAN基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的生成模型通常从单次扫描预测未来大脑MRI，本文发现自编码器训练的潜在空间中老化轨迹近似线性，因此探索在此空间中使用线性模型来更准确地模拟大脑老化过程。

Method: 训练卷积自编码器创建大脑MRI的潜在空间，通过估计潜在进展率β进行基于年龄的线性外推预测。提出使用群体平均和个体特定的先验线性进展率，并支持贝叶斯后验采样进行多扫描条件下的灵活更新。

Result: 在ADNI数据集上，MRExtrap准确预测老化模式，在单体积大脑老化预测方面优于GAN基线方法。潜在进展率与疾病和年龄相关的结构萎缩模式相关。

Conclusion: MRExtrap提供了一个简单而强大的方法，用于基于年龄生成3D大脑MRI，特别适用于具有多个纵向观察数据的场景，为神经退行性疾病进展研究提供了有效工具。

Abstract: Simulating aging in 3D brain MRI scans can reveal disease progression
patterns in neurological disorders such as Alzheimer's disease. Current deep
learning-based generative models typically approach this problem by predicting
future scans from a single observed scan. We investigate modeling brain aging
via linear models in the latent space of convolutional autoencoders (MRExtrap).
Our approach, MRExtrap, is based on our observation that autoencoders trained
on brain MRIs create latent spaces where aging trajectories appear
approximately linear. We train autoencoders on brain MRIs to create latent
spaces, and investigate how these latent spaces allow predicting future MRIs
through linear extrapolation based on age, using an estimated latent
progression rate $\boldsymbol{\beta}$. For single-scan prediction, we propose
using population-averaged and subject-specific priors on linear progression
rates. We also demonstrate that predictions in the presence of additional scans
can be flexibly updated using Bayesian posterior sampling, providing a
mechanism for subject-specific refinement. On the ADNI dataset, MRExtrap
predicts aging patterns accurately and beats a GAN-based baseline for
single-volume prediction of brain aging. We also demonstrate and analyze
multi-scan conditioning to incorporate subject-specific progression rates.
Finally, we show that the latent progression rates in MRExtrap's linear
framework correlate with disease and age-based aging patterns from previously
studied structural atrophy rates. MRExtrap offers a simple and robust method
for the age-based generation of 3D brain MRIs, particularly valuable in
scenarios with multiple longitudinal observations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [132] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF是一种无需训练的方法，通过融合历史视觉信息来增强VLA模型的推理质量，在多个基准测试中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型逐帧处理视觉输入，丢弃了机器人操作任务中宝贵的时间信息，对视觉噪声敏感且忽略了连续帧间的连贯性

Method: 提出时间令牌融合(TTF)方法，结合灰度像素差异分析和基于注意力的语义相关性评估，通过硬融合策略和关键帧锚定实现选择性时间令牌融合

Result: 在LIBERO上平均提升4.0个百分点(72.4% vs 68.4%)，在SimplerEnv上相对提升4.8%，在真实机器人任务上相对提升8.7%

Conclusion: TTF方法模型无关，能跨架构工作，并发现选择性重用Query矩阵可以提升性能，为直接KQV矩阵重用策略提供了新方向

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [133] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 本文提出了一种针对体育图像字幕生成的两级微调LVLM管道，解决了现有模型在体育专业术语和风格化描述方面的不足，在Super Bowl LIX中成功应用于实时体育新闻报道。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在体育领域的应用存在局限，特别是缺乏领域特定的体育术语来生成自然的人类风格描述，无法满足生产级的体育图像字幕生成需求。

Method: 采用两级微调的LVLM（大型视觉语言模型）管道，专门针对体育图像生成风格化字幕，优化了模型在体育专业术语和描述风格方面的表现。

Result: 相比其他方法，F1分数提升8-10%，BERT分数提升2-10%，具有较小的运行时内存占用和快速执行时间。在Super Bowl LIX中，以每3-5秒处理6张图像的速度为超过1000张图像生成了高精度风格化字幕。

Conclusion: 该管道成功解决了现有模型在体育图像字幕生成方面的局限性，证明了其在实时专业体育新闻报道中的实际应用价值，为体育领域的AI应用提供了有效解决方案。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [134] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 该论文调查数字水印和隐写术作为ICAO合规面部图像的防篡改解决方案，分析现有技术潜力与局限，为身份验证系统提供安全部署指导


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像在身份验证中广泛应用，但标准化也带来了篡改和深度伪造风险，传统实时检测方法无法提供捕获后保护

Method: 通过全面分析最先进的数字水印和隐写技术，评估这些方法在ICAO图像应用中的潜力和局限性

Result: 识别了关键技术权衡，提供了在标准约束下ICAO合规图像安全部署的实用指导

Conclusion: 数字水印和隐写术可作为ICAO合规面部图像的有效防篡改补充方案，在保持标准兼容性的同时提供持续验证能力

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [135] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM是一个新颖的知识整合框架，可以从异构的预训练模型中提取共识知识，无需依赖训练数据分布或网络架构的强假设，在无监督目标识别任务中表现出色且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识整合方法通常对训练数据分布和网络架构有强假设限制，只能从特定类型的模型中学习，存在数据和/或归纳偏差。如何有效利用网络上大量异构预训练模型的集体知识成为一个根本性挑战。

Method: 提出了UNIFORM框架，采用专门的投票机制在logit层面和特征层面捕获知识共识：logit层面整合能够预测目标类别的教师模型，特征层面利用在任意标签空间上学习到的视觉表示。

Result: 大量实验表明，UNIFORM相比强知识迁移基线有效提升了无监督目标识别性能，能够从100多个教师模型中受益，而现有方法在更小规模时就达到饱和。

Conclusion: UNIFORM框架成功解决了从异构预训练模型中有效整合知识的问题，无需传统方法的约束条件，展现出卓越的可扩展性和性能提升。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [136] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN是首个用于3D主动脉瘤生长预测的多模态条件网格到网格生成对抗网络，通过双分支架构结合局部KNN卷积网络和全局图卷积网络，克服了深度GCN的过度平滑问题，在几何精度和临床直径估计方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主动脉瘤进展的个性化准确预测对于及时干预至关重要，但由于需要同时建模复杂3D几何中的细微局部变形和全局解剖变化，这一任务仍然具有挑战性。

Method: 提出MCMeshGAN，采用双分支架构：新颖的局部KNN卷积网络(KCN)保持细粒度几何细节，全局图卷积网络(GCN)捕获长距离结构上下文。专用条件分支编码临床属性(年龄、性别)和目标时间间隔，生成解剖学合理、时间可控的预测。

Result: 在TAAMesh数据集(208名患者的590个多模态记录)上的广泛实验表明，MCMeshGAN在几何精度和临床重要直径估计方面始终优于最先进的基线方法。

Conclusion: 该框架为临床可部署的个性化3D疾病轨迹建模提供了稳健的一步，源代码已公开。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [137] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet是一个基于transformer的模型，通过结合未来行人轨迹和车辆速度预测来预测行人过街意图，在推理时间和性能上都达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆上路，准确预测行人过街意图变得至关重要，这有助于提高道路安全性和自动驾驶系统的可靠性。

Method: 提出TrajFusionNet模型，包含序列注意力模块(SAM)和视觉注意力模块(VAM)两个分支。SAM学习观察到的和预测的行人轨迹与车辆速度的序列表示，VAM通过在场景图像上叠加预测的行人边界框来学习视觉表示。

Result: 模型在三个最常用的行人过街意图预测数据集上都达到了最先进的性能，并且具有最低的总推理时间（包括模型运行时间和数据预处理）。

Conclusion: TrajFusionNet通过轻量级模态结合序列和视觉信息，在行人过街意图预测任务中实现了优异的性能和效率，为自动驾驶系统提供了可靠的预测解决方案。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [138] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出基于互信息的天空背景估计模型SMI，利用所有光纤光谱估计天空背景，解决传统方法依赖天空光纤平均光谱的问题


<details>
  <summary>Details</summary>
Motivation: 当前天空背景扣除主要依赖天空光纤光谱构建超天空光谱，但缺乏对目标周围环境的建模，需要更精确的天空背景估计方法

Method: SMI包含两个主要网络：第一个网络使用波长校准模块从光谱中提取天空特征，解决特征偏移问题；第二个网络采用增量训练方法最大化不同光谱表示间的互信息来捕获共同成分，同时最小化相邻光谱表示间的互信息以获得个体成分

Result: 在LAMOST光谱上的实验表明，SMI能够在观测过程中获得更好的目标天空背景，特别是在蓝端表现更佳

Conclusion: SMI模型通过互信息和增量训练方法有效解决了天空背景估计问题，相比传统方法能提供更精确的天空背景扣除

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [139] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 提出Discrete Diffusion VLA方法，使用离散扩散模型处理机器人动作生成，避免了传统自回归或连续扩散方法的局限性，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有的VLA解码器要么采用固定的自回归生成顺序，要么需要专门的连续扩散训练和迭代采样，这阻碍了统一可扩展架构的发展

Method: 使用离散扩散模型处理离散化的动作块，采用与VLM骨干网络相同的交叉熵目标进行训练，支持并行解码和自适应解码顺序

Result: 在LIBERO上达到96.3%平均成功率，SimplerEnv Fractal上71.2%视觉匹配率，SimplerEnv Bridge上49.3%总体表现，优于自回归和连续扩散基线

Conclusion: 离散扩散动作解码器支持精确的动作建模和一致的训练，为将VLA扩展到更大模型和数据集奠定了基础

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [140] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一个可训练的组合框架，通过两阶段训练流程将通用规划器与专业执行器结合，在科学计算GUI任务中实现了卓越的执行精度和跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决GUI自主代理在科学计算等专业领域中长期规划与精确执行之间的权衡问题，现有组合框架缺乏可训练性和适应性，无法从经验中学习。

Method: 两阶段训练流程：1) 专业化阶段 - 使用解耦GRPO方法为每个科学应用单独训练专家规划器；2) 泛化阶段 - 聚合成功轨迹进行监督微调，整合通用规划器(Cerebrum)与专业执行器(Cerebellum)。

Result: 在ScienceBenchmark的四个挑战性应用中显著超越基线方法，在开源模型中建立了新的最先进水平。

Conclusion: CODA框架成功解决了科学计算GUI任务中规划与执行的平衡问题，通过可训练的组合架构实现了强大的执行能力和跨领域泛化性能。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [141] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 本文提出了一个针对自然语言查询中数据库实体识别（DB-ER）的新方法，包括创建人工标注基准、数据增强技术和基于T5的专门实体识别模型，在精度和召回率上优于现有NER模型。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中数据库实体识别的挑战，现有NER模型在数据库实体识别任务上表现不佳，需要专门的方法来处理这种特定领域的实体识别问题。

Method: 1) 从流行的text-to-SQL基准创建人工标注的DB-ER基准；2) 利用SQL查询自动标注NLQ的数据增强方法；3) 基于T5架构的专门语言模型，使用序列标注和token分类两个下游任务进行微调。

Result: 提出的DB-ER标注器在精度和召回率上都优于两种最先进的NER标注器。数据增强使精度和召回率提升超过10%，T5骨干网络的微调使这些指标提升5-10%。

Conclusion: 该方法在数据库实体识别任务上表现出色，数据增强和专门模型架构的结合显著提升了性能，为自然语言查询中的数据库实体识别提供了有效的解决方案。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [142] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 这篇论文为印地语言模型提供了五个高质量的评测数据集，解决了直接翻译英语数据集无法抓取语言文化细节的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的印地语评测基准，直接翻译英语数据集无法涵盖印地语的语言和文化细节，需要专门的评测工具。

Method: 采用人工注释与翻译验证相结合的方法，创建了五个印地语LLM评测数据集：IFEval-Hi、MT-Bench-Hi、GSM8K-Hi、ChatRAG-Hi和BFCL-Hi。

Result: 通过这些数据集对支持印地语的开源LLM进行了全面基准测评，提供了它们当前能力的详细对比分析。

Conclusion: 该方法不仅有效解决了印地语LLM评测问题，还可以作为可复制的方法论例用于其他语言资源稀缺语言的基准开发。

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [143] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 提出了11Plus-Bench评估框架，发现当前MLLMs在空间推理方面表现出早期认知迹象，但与人类存在较大性能差距，且实例级表现随机，而人类表现可预测且受抽象模式复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 人类认知过程中空间推理与感知紧密交织，但多模态大语言模型(MLLMs)在这方面的能力评估尚未深入探索，需要系统评估其空间推理能力。

Method: 构建11Plus-Bench基准测试，基于真实标准化空间能力测试，包含细粒度专家标注的感知复杂度和推理过程，对14个MLLMs进行广泛实验并与人类评估对比。

Result: MLLMs表现出空间认知的早期迹象，认知努力与推理相关复杂度强相关，但实例级表现随机；人类正确率高度可预测且受抽象模式复杂度影响。

Conclusion: 当前MLLMs在空间推理方面既有新兴能力也有局限性，为模型设计提供了可操作的见解。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [144] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: 该论文提出了一种处理中互连（π²）计算范式，利用现有路由、交换和互连系统中的计算原语来实现AI工作负载，通过分析建模显示其能量缩放性能优于其他神经形态架构。


<details>
  <summary>Details</summary>
Motivation: 大型神经形态计算中，路由、交换和互连结构虽然只起支持作用，但对于大规模AI工作负载却决定了能耗和速度。现有系统存在瓶颈，需要探索新的计算范式来提升性能。

Method: 将典型AI工作负载操作映射到包交换和包路由硬件中已有的原语（延迟、因果性、超时、丢包和广播操作），利用现有缓冲和流量整形算法实现神经元模型和突触操作，并通过知识蒸馏框架训练神经网络拓扑到π²系统。

Result: 分析建模表明，π²架构的能量缩放性能随互连带宽和能量效率提升而改善，优于其他神经形态平台，预计可扩展到执行脑规模AI推理工作负载，功耗在数百瓦范围内。

Conclusion: π²计算范式通过利用互连技术趋势，能够有效解决大型神经形态计算的瓶颈问题，实现更好的能量缩放性能和可扩展性，为脑规模AI应用提供可行解决方案。

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: PLGC是一种针对联邦推荐系统的模型无关策略，通过个性化本地-全局协作解决嵌入退化问题，提高个性化表示效果


<details>
  <summary>Details</summary>
Motivation: 集中式推荐系统存在隐私泄露问题，联邦推荐系统虽然能保护隐私，但面临嵌入退化、个性化不足和维度坍塌等挑战

Method: 提出PLGC策略，将冻结的全局项目嵌入表整合到本地设备，使用神经正切核策略动态平衡本地和全局信息，结合对比目标函数减少嵌入冗余

Result: 在五个真实数据集上的实验表明，PLGC优于各种基线算法，有效缓解了嵌入退化问题

Conclusion: PLGC是一种有效的模型无关个性化训练策略，可应用于现有联邦推荐基准模型，显著提升推荐性能

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [146] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: 这篇博士论文提出了四个主要贡献来改进PIM架构：DAMOV方法论用于识别内存数据移动瓶颈，MIMDRAM硬件/软件协同设计解决可编程性问题，Proteus框架降低PUD操作延迟，以及DaPPA编程框架简化PIM编程。


<details>
  <summary>Details</summary>
Motivation: 为当前和未来系统提供工具、编程模型和系统支持，以促进PIM架构（特别是基于DRAM的解决方案）的采用，解决数据移动瓶颈和可编程性限制。

Method: 1) DAMOV：首个严谨的方法论来表征现代工作负载中的内存相关数据移动瓶颈
2) MIMDRAM：硬件/软件协同设计基底，解决PUD架构的编程灵活性限制
3) Proteus：硬件框架，通过数据感知运行时引擎降低PUD操作延迟
4) DaPPA：数据并行内存处理架构编程框架

Result: 开发了完整的工具链和方法论来优化PIM架构，包括基准测试套件、硬件加速方案和编程框架，显著改善了PIM架构的性能、能效和可编程性。

Conclusion: 该研究为PIM架构提供了全面的解决方案，从方法论到硬件实现再到编程支持，为PIM技术的实际应用和推广奠定了重要基础。

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>


### [147] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: GENIE-ASI是首个基于大语言模型的免训练模拟子电路识别方法，通过上下文学习和代码生成实现自动化识别，在简单结构中达到完美性能，在复杂电路中展现潜力。


<details>
  <summary>Details</summary>
Motivation: 传统模拟子电路识别方法需要大量人工专业知识、基于规则的编码或标注数据集，存在效率低和可扩展性差的问题。

Method: 采用两阶段方法：首先通过上下文学习从少量示例推导自然语言指令，然后将其转换为可执行的Python代码来识别SPICE网表中的子电路。

Result: 在提出的新基准测试中，GENIE-ASI在简单结构上达到F1分数1.0，中等抽象度上为0.81，复杂子电路上为0.31，表现具有竞争力。

Conclusion: 大语言模型可作为模拟设计自动化中的适应性通用工具，为模拟设计自动化中基础模型应用开辟了新的研究方向。

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [148] [Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data](https://arxiv.org/abs/2508.19683)
*Kenji Fukushima,Syo Kamata*

Main category: nucl-th

TL;DR: 这篇论文研究了使用风险拙网络(FNN)构建的拓扑不确定性(TU)在异常检测中的性能。通过顶点数据分析提取FNN隐藏层信息，并在中子星数据集上进行实验，异常检测成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 研究如何从训练好的神经网络隐藏层中提取有价值信息，并利用拓扑不确定性来进行异常检测，特别是在中子星状态方程推断这种关键应用中。

Method: 使用训练好的前向神经网络，通过顶点数据分析构建拓扑不确定性(TU)。采用中子星数据集，将数据标记为正常(k=0)和异常(k=1)推断结果，并引入交叉TU来量化不确定性。

Result: 在数值实验中，异常检测的性能依赖于FNN超参数。在最佳情况下，异常检测的成功率超过90%，能够准确识别失败的推断结果。

Conclusion: 拓扑不确定性是一种有效的方法，可以从训练好的神经网络中提取隐藏信息并用于异常检测。该方法在中子星状态方程推断等关键应用中显示出良好性能，具有进一步应用潜力。

Abstract: We study the performance of the Topological Uncertainty (TU) constructed with
a trained feedforward neural network (FNN) for Anomaly Detection. Generally,
meaningful information can be stored in the hidden layers of the trained FNN,
and the TU implementation is one tractable recipe to extract buried information
by means of the Topological Data Analysis. We explicate the concept of the TU
and the numerical procedures. Then, for a concrete demonstration of the
performance test, we employ the Neutron Star data used for inference of the
equation of state (EoS). For the training dataset consisting of the input
(Neutron Star data) and the output (EoS parameters), we can compare the
inferred EoSs and the exact answers to classify the data with the label $k$.
The subdataset with $k=0$ leads to the normal inference for which the inferred
EoS approximates the answer well, while the subdataset with $k=1$ ends up with
the unsuccessful inference. Once the TU is prepared based on the $k$-labled
subdatasets, we introduce the cross-TU to quantify the uncertainty of
characterizing the $k$-labeled data with the label $j$. The anomaly or
unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is
smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various
input data, we calculate the cross-TU and estimate the performance of Anomaly
Detection. We find that performance depends on FNN hyperparameters, and the
success rate of Anomaly Detection exceeds $90\%$ in the best case. We finally
discuss further potential of the TU application to retrieve the information
hidden in the trained FNN.

</details>
