<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.NI](#cs.NI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 62]
- [eess.IV](#eess.IV) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [math.ST](#math.ST) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CC](#cs.CC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT](https://arxiv.org/abs/2508.00341)
*Shengheng Liu,Ningning Fu,Zhonghao Zhang,Yongming Huang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: 论文提出了一种结合空中计算的联邦学习框架，通过用户调度和波束成形优化通信效率和推理精度，并提出了低复杂度调度策略。


<details>
  <summary>Details</summary>
Motivation: 物联网(IoT)的普及带来了隐私问题，联邦学习(FL)作为一种去中心化训练方法，需要解决大规模网络中用户参与受限的问题。

Method: 引入空中计算到FL框架，提出用户调度和波束成形联合优化方法，并通过差分凸技术分解非凸问题，进一步提出低复杂度调度策略。

Result: 实验表明，所提方法在聚合误差和学习性能上优于现有方法。

Conclusion: 该方法有效提升了FL在大规模网络中的通信效率和模型性能。

Abstract: The rising popularity of Internet of things (IoT) has spurred technological
advancements in mobile internet and interconnected systems. While offering
flexible connectivity and intelligent applications across various domains, IoT
service providers must gather vast amounts of sensitive data from users, which
nonetheless concomitantly raises concerns about privacy breaches. Federated
learning (FL) has emerged as a promising decentralized training paradigm to
tackle this challenge. This work focuses on enhancing the aggregation
efficiency of distributed local models by introducing over-the-air computation
into the FL framework. Due to radio resource scarcity in large-scale networks,
only a subset of users can participate in each training round. This highlights
the need for effective user scheduling and model transmission strategies to
optimize communication efficiency and inference accuracy. To address this, we
propose an integrated approach to user scheduling and receive beam steering,
subject to constraints on the number of selected users and transmit power.
Leveraging the difference-of-convex technique, we decompose the primal
non-convex optimization problem into two sub-problems, yielding an iterative
solution. While effective, the computational load of the iterative method
hampers its practical implementation. To overcome this, we further propose a
low-complexity user scheduling policy based on characteristic analysis of the
wireless channel to directly determine the user subset without iteration.
Extensive experiments validate the superiority of the proposed method in terms
of aggregation error and learning performance over existing approaches.

</details>


### [2] [Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services](https://arxiv.org/abs/2508.00426)
*Rohan Gandhi,Ankur Mallick,Ken Sueda,Rui Liang*

Main category: cs.DC

TL;DR: Tetris框架通过优化初始呼叫分配和周期性迁移呼叫，减少高负载媒体处理器（MP）的使用，提升会议服务性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有算法（如Teams使用的）容易导致MP过载，影响性能并增加成本，主要原因是忽略了呼叫CPU使用的变异性。

Method: Tetris框架结合历史数据优化初始分配，并通过线性优化周期性迁移呼叫以减少热MP的使用。

Result: 在包含1000万次呼叫的24小时跟踪中，Tetris将热MP上的参与者数量减少了至少2.5倍。

Conclusion: Tetris有效解决了MP过载问题，显著提升了性能和成本效益。

Abstract: Conference services like Zoom, Microsoft Teams, and Google Meet facilitate
millions of daily calls, yet ensuring high performance at low costs remains a
significant challenge. This paper revisits the problem of packing calls across
Media Processor (MP) servers that host the calls within individual datacenters
(DCs). We show that the algorithm used in Teams -- a large scale conferencing
service as well as other state-of-art algorithms are prone to placing calls
resulting in some of the MPs becoming hot (high CPU utilization) that leads to
degraded performance and/or elevated hosting costs. The problem arises from
disregarding the variability in CPU usage among calls, influenced by
differences in participant numbers and media types (audio/video), compounded by
bursty call arrivals. To tackle this, we propose Tetris, a multi-step framework
which (a) optimizes initial call assignments by leveraging historical data and
(b) periodically migrates calls from hot MPs using linear optimization, aiming
to minimize hot MP usage. Evaluation based on a 24-hour trace of over 10
million calls in one DC shows that Tetris reduces participant numbers on hot
MPs by at least 2.5X.

</details>


### [3] [SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments](https://arxiv.org/abs/2508.00622)
*Kapel Dev,Yash Madhwal,Sofia Shevelo,Pavel Osinenko,Yury Yanovich*

Main category: cs.DC

TL;DR: SwarnRaft是一个基于区块链的定位和共识框架，用于在GNSS信号缺失时维持无人机群的协调和数据完整性。


<details>
  <summary>Details</summary>
Motivation: 无人机群在关键应用中依赖GNSS信号，但信号可能因干扰、环境或攻击而中断，导致任务失败。

Method: 利用Raft共识算法，使无人机节点在GNSS信号缺失时仍能达成状态更新共识，并通过本地感知和WiFi通信实现。

Result: 原型系统展示了在信号丢失时通过共识重建或验证节点位置的鲁棒性，保持了群的一致性和容错性。

Conclusion: SwarnRaft为不可预测环境中的去中心化无人机操作提供了实用且安全的基础。

Abstract: Unmanned aerial vehicle (UAV) swarms are increasingly used in critical
applications such as aerial mapping, environmental monitoring, and autonomous
delivery. However, the reliability of these systems is highly dependent on
uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,
which can be disrupted in real-world scenarios due to interference,
environmental conditions, or adversarial attacks, causing disorientation,
collision risks, and mission failure. This paper proposes SwarnRaft, a
blockchain-inspired positioning and consensus framework for maintaining
coordination and data integrity in UAV swarms operating under GNSS-denied
conditions. SwarnRaft leverages the Raft consensus algorithm to enable
distributed drones (nodes) to agree on state updates such as location and
heading, even in the absence of GNSS signals for one or more nodes. In our
prototype, each node uses GNSS and local sensing, and communicates over WiFi in
a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify
the position of the failed node based on its last known state and trajectory.
Our system demonstrates robustness in maintaining swarm coherence and fault
tolerance through a lightweight, scalable communication model. This work offers
a practical and secure foundation for decentralized drone operation in
unpredictable environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [4] [From Dynamic Programs to Greedy Algorithms](https://arxiv.org/abs/2508.00776)
*Dieter van Melkebeek*

Main category: cs.DS

TL;DR: 论文展示了如何从动态规划的一般情况简单推导出经典贪心算法，适用于区间调度、背包问题和最短路径问题。


<details>
  <summary>Details</summary>
Motivation: 为本科生算法课程提供一种开发贪心算法和论证其正确性的替代方法。

Method: 通过扩展动态规划的Bellman方程，利用单调性性质确定最优解。

Result: 成功推导出区间调度、背包问题和最短路径问题的贪心算法。

Conclusion: 该方法为理解贪心算法提供了新视角，尤其在区间调度中解释了从最早开始时间到最早结束时间的顺序变化。

Abstract: We show for several computational problems how classical greedy algorithms
for special cases can be derived in a simple way from dynamic programs for the
general case: interval scheduling (restricted to unit weights), knapsack
(restricted to unit values), and shortest paths (restricted to nonnegative edge
lengths). Conceptually, we repeatedly expand the Bellman equations underlying
the dynamic program and use straightforward monotonicity properties to figure
out which terms yield the optimal value under the respective restrictions. The
approach offers an alternative for developing these greedy algorithms in
undergraduate algorithms courses and/or for arguing their correctness. In the
setting of interval scheduling, it elucidates the change in order from earliest
start time first for the memoized dynamic program to earliest finish time first
for the greedy algorithm.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: 论文提出Git-Context-Controller (GCC)，一种基于Git版本控制思想的结构化上下文管理框架，用于提升大型语言模型代理在长期任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理在长期任务（如大型软件开发）中的部署，上下文管理成为关键瓶颈。

Method: GCC将代理的上下文管理设计为类似Git的版本化内存层次结构，支持COMMIT、BRANCH、MERGE和CONTEXT等操作，实现里程碑检查点、替代计划探索和结构化反思。

Result: 在SWE-Bench-Lite基准测试中，GCC代理解决了48.00%的软件错误，优于26个竞争系统；在自复制案例中，GCC代理的任务解决率达到40.7%，而未使用GCC的仅为11.7%。

Conclusion: GCC框架显著提升了代理在长期任务中的上下文管理能力，为复杂任务提供了高效解决方案。

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [6] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（LLMs）在生成复杂Python代码时的表现，发现仅有少数模型（如GPT-4.1）能稳定生成正确代码，同时揭示了第三方库的文档问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在科学研究的代码生成中应用广泛，但其对不熟悉Python API的解析能力尚未充分研究。

Method: 通过零样本提示，测试LLMs在两种复杂任务（数据分析和合成数据生成）中的表现，并定量和定性评估代码功能性和错误。

Result: 仅少数模型能稳定生成正确代码，GPT-4.1表现最佳；同时发现第三方库文档和实现问题。

Conclusion: LLMs在科学自动化中存在局限性，需优化提示设计、完善文档并提升模型能力。

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [7] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: 本文通过系统文献综述（SLR）研究了机器学习（ML）在软件工程（SE）中的应用，总结了最佳实践、挑战和不足，强调了稳健的ML流程对提升软件质量和效率的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程系统的复杂性增加，传统方法难以应对，导致调试时间长、缺陷检测效率低等问题。ML为自动化任务提供了解决方案，但其效果依赖于流程的稳健性。

Method: 通过SLR分析最新的ML流程，包括数据预处理、特征工程、算法选择和验证技术，如SMOTE、SZZ算法、集成方法和新评估指标。

Result: 研究发现，稳健的预处理和集成方法（如随机森林和梯度提升）表现最佳，而朴素贝叶斯等简单模型在效率和可解释性上仍有价值。新指标（如BAM）在特定应用中崭露头角。

Conclusion: 本文强调了设计良好的ML流程对解决SE挑战的重要性，为研究者和实践者提供了优化软件质量和效率的实用建议，并为未来创新奠定了基础。

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [8] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文系统综述了基于LLM的代码生成代理，涵盖其自主性、任务范围扩展和工程实用性增强三大特征，并探讨了技术发展、应用场景、评估工具及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索基于LLM的代码生成代理如何通过自主性、任务范围扩展和工程实用性增强，革新软件开发范式。

Method: 通过系统调查，追溯技术发展轨迹，分类核心技术（单代理与多代理架构），并详细描述其在SDLC中的应用。

Result: 总结了主流评估基准与工具，并分析了当前挑战。

Conclusion: 提出未来基础性、长期性研究方向，以推动领域进一步发展。

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [9] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: 量化技术显著降低LLMs4Code的隐私风险，同时揭示了任务性能与隐私风险之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 研究量化技术对LLMs4Code任务性能和隐私风险的影响，以指导实际部署中的隐私保护。

Method: 对Pythia、CodeGen和GPTNeo三种模型家族应用静态和动态量化技术，评估其任务性能和隐私风险。

Result: 量化显著降低隐私风险，且任务性能与隐私风险正相关；量化大模型可能比小模型更优。

Conclusion: 量化提供了一种平衡任务性能和隐私风险的有效方法，适用于不同架构和规模的模型。

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [10] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 研究探讨了在真实应用开发中如何测试基于大语言模型的系统，发现测试策略结合了手动与自动方法，并面临模型行为不确定等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程中广泛应用，但对其集成系统的测试研究较少，本研究旨在填补这一空白。

Method: 通过分析99份学生报告，采用主题分析和结构化编码进行探索性案例研究。

Result: 测试策略包括探索性测试、单元测试和提示迭代，挑战包括集成失败、输出不可预测和提示敏感性。

Conclusion: 测试大语言模型系统需结合传统验证方法和行为感知评估，为生成组件测试提供了实践依据。

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [11] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 比较面向对象编程（OOP）和函数式编程（FP）对软件系统架构特性的影响，通过Kotlin和Scala实现数字钱包系统进行定性和定量分析。


<details>
  <summary>Details</summary>
Motivation: 随着函数式编程在软件行业中的关注度增加，研究OOP和FP对系统架构特性的影响，为开发者和组织提供选择编程范式的依据。

Method: 通过Kotlin（OOP）和Scala（FP）实现数字钱包系统，进行定性的自我民族志分析和定量的开发者调查分析。

Result: 定性分析揭示了编写代码的视角，定量分析展示了开发者对代码的阅读体验。

Conclusion: 研究结果为开发者和组织在选择编程范式时提供了参考依据。

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [12] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: GenLoc是一种基于大型语言模型（LLM）的bug定位方法，通过代码探索功能迭代分析代码库，解决bug报告与源代码之间的词汇不匹配问题，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有bug定位方法因词汇不匹配问题效果有限，需改进。

Method: 利用LLM和代码探索功能迭代分析代码库，可选向量嵌入检索相关文件。

Result: 在6个大型Java项目的9,000多个bug报告上测试，GenLoc在多个指标上优于5种先进技术，Accuracy@1平均提升60%以上。

Conclusion: GenLoc通过LLM和代码探索有效解决了词汇不匹配问题，显著提升了bug定位的准确性。

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [13] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 提出了一种基于抽象-具体化框架的方法，利用LLM生成多个候选输出，通过聚合和优化生成更一致、准确的图模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成图模型时的语法违规、约束不一致和准确性不足问题。

Method: 构建概率部分模型聚合候选输出，再优化为满足约束的具体模型。

Result: 显著提升了生成图模型的一致性和质量。

Conclusion: 该方法有效解决了LLM生成图模型的主要问题，具有实际应用价值。

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [14] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: 论文提出了ULT（UnLeakedTestbench）基准，用于更真实地评估大语言模型（LLM）在单元测试生成中的能力，解决了现有基准的数据污染和结构简单性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成基准存在数据污染和结构简单性问题，导致科学结论的可靠性不足。

Method: 通过多阶段筛选过程构建ULT，包含3,909个高复杂度的Python函数任务，并引入PLT作为对照基准。

Result: ULT上的测试生成表现显著低于其他基准（如TestEval和PLT），表明其更具挑战性。

Conclusion: ULT提供了一个更真实、更具挑战性的评估工具，有助于更准确地衡量LLM的测试生成能力。

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [15] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: 论文研究了工业环境中结对编程中的权力差距现象，提出了避免权力差距的建议。


<details>
  <summary>Details</summary>
Motivation: 理解结对编程中权力相关现象，为从业者提供改进建议。

Method: 分析了22个工业结对编程会话，使用扎根理论方法，并通过292名参与者的调查验证理论。

Result: 提出了权力差距理论，揭示了其负面影响，调查证实理论概念在实践中普遍存在。

Conclusion: 避免权力差距是结对编程的重要技能，需减少等级行为，增加平等行为。

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [16] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan是一个统一平台，无缝整合了值流分析和符号推理，提升了程序分析的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的值流分析和符号分析技术各自独立，缺乏统一的平台实现高效集成。

Method: Desyan扩展了Datalog引擎（Soufflé），集成SMT求解器，支持自动处理典型程序分析模式。

Result: Desyan在值流分析中性能领先（速度提升20倍），在符号推理中支持多种求解方式，显著提升效率（速度提升2倍）。

Conclusion: Desyan成功填补了值流与符号分析间的技术鸿沟，为程序分析提供了高效、灵活的解决方案。

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [17] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出了一种名为SPENCER的框架，结合双编码器和交叉编码器以提高代码检索的效率和准确性，并通过模型蒸馏技术减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器模型在代码检索任务中因缺乏底层交互而性能受限，需在保持效率的同时提升效果。

Method: SPENCER框架先使用双编码器缩小搜索空间，再用交叉编码器提升准确性；提出自适应模型蒸馏技术和助教选择策略。

Result: 实验表明，双编码器与交叉编码器结合优于单一双编码器模型；模型蒸馏技术减少70%推理时间，性能保留98%以上。

Conclusion: SPENCER框架有效提升了代码检索的性能和效率，模型蒸馏技术显著优化了推理速度。

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [18] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文通过实证研究发现，用户反馈中大量信息与系统问题无关，需过滤；严重问题难以仅通过反馈特征检测；机器学习方法适用于分析用户反馈。


<details>
  <summary>Details</summary>
Motivation: 理解用户反馈在大型在线服务系统中的特性，以改进基于反馈的问题检测方法。

Method: 对来自6个真实服务的50,378,766条用户反馈进行实证研究，分析内容、特征及机器学习适用性。

Result: 大部分反馈与问题无关；严重问题难以通过反馈特征检测；反馈主题分布稳定，机器学习方法可行。

Conclusion: 研究结果为大规模服务系统中基于反馈的问题检测提供了实证基础，指导实用检测方法的设计与实现。

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [19] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: MCeT是一种自动化工具，用于评估行为模型（如序列图）的正确性，并生成问题列表。它利用LLM进行多角度细粒度分析，显著提高了问题检测的准确性和数量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在AI建模中的应用增加，需要自动化工具来评估模型正确性，为工程师提供反馈并帮助AI自我改进。

Method: MCeT将图和需求分解为原子单元，进行多角度比较，并结合自一致性检查以减少LLM幻觉问题。

Result: MCeT将直接方法的精度从0.58提升至0.81，检测到的问题数量增加了90%，平均每个图发现6个新问题。

Conclusion: MCeT通过多角度分析和自一致性检查，显著提升了行为模型正确性评估的效果。

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [20] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: 研究比较了LLM生成代码与人工编写代码的内部质量属性，发现LLM代码整体缺陷更少且修复成本更低，但在复杂场景中可能引入关键问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在代码生成中的软件质量表现及其与人工编写代码的对比。

Method: 结合数据集、三种LLM配置（零样本、少样本、微调）和SonarQube工具评估代码质量，分析维护性、可靠性等指标。

Result: LLM生成代码缺陷较少且修复成本低，微调模型减少高严重性问题但降低性能；复杂问题中可能引入结构性问题。

Conclusion: LLM生成代码质量有优势，但复杂场景需系统评估，研究加深了对LLM代码生成优缺点的理解。

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [21] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: 本文提出了一种基于因果依赖分析的自动化一致性检查方法，用于验证具体流程模型与参考模型的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性检查方法缺乏语义模型比较的表达能力和自动化能力，无法满足需求。

Method: 采用因果依赖分析任务和事件的方法，并集成到语义框架中，提出了一种算法进行一致性检查。

Result: 通过案例研究验证了方法的有效性，提高了流程模型一致性验证的准确性和灵活性。

Conclusion: 研究提供了一种工具辅助的解决方案，增强了流程模型一致性验证的能力。

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [22] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: 该论文研究了动态符号执行（DSE）在组件-连接器架构语义差异分析中的应用，通过增强MontiArc-to-Java生成器收集运行时数据，评估了DSE的适用性，发现其潜力但存在可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动开发中，确保模型正确性和一致性至关重要，因此需要有效的方法来分析语义差异。

Method: 增强MontiArc-to-Java生成器，收集符号和具体执行数据，评估不同执行策略的效率、最小性和完整性。

Result: DSE在组件-连接器架构分析中显示出潜力，但可扩展性是其主要限制。

Conclusion: DSE适用于语义差异分析，但需进一步研究以提升其在大系统中的实用性。

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


### [23] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: 该研究通过分析Codeforces用户的编程竞赛表现，预测其在软件工程领域的就业潜力，并使用随机森林分类器将用户分为四个就业能力等级。


<details>
  <summary>Details</summary>
Motivation: 快速发展的科技行业需要评估程序员就业潜力的工具，研究旨在探索编程竞赛表现与就业机会之间的相关性。

Method: 通过Codeforces API收集用户数据，处理关键性能指标，并使用随机森林分类器构建预测模型，系统通过Flask部署在Render上。

Result: 模型能有效区分不同技能水平，基于编程熟练度和参与度将用户分类为四个就业能力等级。

Conclusion: 该研究为机器学习在职业评估中的应用奠定了基础，并可扩展至更广泛技术领域的就业能力预测。

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [24] [Agent Network Protocol Technical White Paper](https://arxiv.org/abs/2508.00007)
*Gaowei Chang,Eidan Lin,Chengxuan Yuan,Rizhao Cai,Binbin Chen,Xuan Xie,Yin Zhang*

Main category: cs.NI

TL;DR: ANP提出了一种面向Agentic Web的新一代通信协议，解决现有互联网基础设施对大规模Agent互联与协作的不足。


<details>
  <summary>Details</summary>
Motivation: 现有互联网基础设施主要为人类交互设计，导致Agent间数据孤岛、接口不友好和高协作成本，难以支持大规模Agent互联与协作的需求。

Method: ANP采用AI原生设计，兼容现有互联网协议，模块化可组合架构，遵循极简但可扩展原则，通过三层协议系统（身份与加密通信层、元协议协商层、应用协议层）解决问题。

Result: ANP系统性地解决了Agent身份认证、动态协商和能力发现互操作性问题。

Conclusion: ANP为Agentic Web提供了高效、兼容且可扩展的通信协议，支持未来Agent大规模互联与协作。

Abstract: With the development of large models and autonomous decision-making AI,
agents are rapidly becoming the new entities of the internet, following mobile
apps. However, existing internet infrastructure is primarily designed for human
interaction, creating data silos, unfriendly interfaces, and high collaboration
costs among agents, making it difficult to support the needs for large-scale
agent interconnection and collaboration. The internet is undergoing a profound
transformation, showing four core trends: agents replacing traditional
software, universal agent interconnection, native protocol-based connections,
and autonomous agent organization and collaboration. To align with these
trends, Agent Network Protocol (ANP) proposes a new generation of communication
protocols for the Agentic Web. ANP adheres to AI-native design, maintains
compatibility with existing internet protocols, adopts a modular composable
architecture, follows minimalist yet extensible principles, and enables rapid
deployment based on existing infrastructure. Through a three-layer protocol
system--identity and encrypted communication layer, meta-protocol negotiation
layer, and application protocol layer--ANP. systematically solves the problems
of agent identity authentication, dynamic negotiation, and capability discovery
interoperability.

</details>


### [25] [Enabling Immersive XR Collaborations over FTTR Networks (Invited)](https://arxiv.org/abs/2508.00009)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 本文探讨了FTTR中的预测带宽分配和无缝切换方案，以实现高质量的沉浸式协作体验。


<details>
  <summary>Details</summary>
Motivation: 研究FTTR作为实现室内扩展现实协作的潜在解决方案。

Method: 提出预测带宽分配和无缝切换方案。

Result: 展示了高质量的沉浸式协作体验。

Conclusion: FTTR结合预测带宽分配和无缝切换方案，能够有效支持室内扩展现实协作。

Abstract: Fiber-To-The-Room is a potential solution to achieve in-premise extended
reality collaborations. This paper explores predictive bandwidth allocation and
seamless handover schemes over FTTR, showing high-quality immersive experience
for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s).

</details>


### [26] [Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?](https://arxiv.org/abs/2508.00010)
*Ruibo Wang,Baha Eddine Youcef Belmekki,Howard H. Yang,Mohamed Slim Alouini*

Main category: cs.NI

TL;DR: 论文通过引入相对误差量化平面与球面模型的差异，提出点过程生成算法和相似性度量，推导最优平面高度表达式，为NTN建模提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 随着非地面网络（NTN）的广泛部署，网络性能分析的计算复杂度急剧增加，而平面模型因忽略地球曲率在高空NTN分析中存在偏差。

Method: 提出点过程生成算法，生成平面和球面点过程对；引入相似性度量，开发相对误差估计算法；推导最优平面高度的解析表达式。

Result: 数值结果研究了部署高度和区域对NTN建模的影响，并通过HAP和LEO卫星星座案例验证。

Conclusion: 研究为平面模型适用性提供了量化标准，并降低了计算复杂度，支持NTN的高效分析。

Abstract: With the explosive deployment of non-terrestrial networks (NTNs), the
computational complexity of network performance analysis is rapidly escalating.
As one of the most suitable mathematical tools for analyzing large-scale
network topologies, stochastic geometry (SG) enables the representation of
network performance metrics as functions of network parameters, thus offering
low-complexity performance analysis solutions. However, choosing between planar
and spherical models remains challenging. Planar models neglect Earth's
curvature, causing deviations in high-altitude NTN analysis, yet are still
often used for simplicity. This paper introduces relative error to quantify the
gap between planar and spherical models, helping determine when planar modeling
is sufficient. To calculate the relative error, we first propose a point
process (PP) generation algorithm that simultaneously generates a pair of
homogeneous and asymptotically similar planar and spherical PPs. We then
introduce several typical similarity metrics, including topology-related and
network-level metrics, and further develop a relative error estimation
algorithm based on these metrics. In addition, we derive an analytical
expression for the optimal planar altitude, which reduces computational
complexity and provides theoretical support for planar approximation. Finally,
numerical results investigate how deployment altitude and region affect NTN
modeling, with case studies on HAP and LEO satellite constellations.

</details>


### [27] [AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks](https://arxiv.org/abs/2508.00011)
*Ahmet Melih Ince,Ayse Elif Canbilen,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度确定性策略梯度（DDPG）的强化学习方法，用于优化HAPS支持的V2X网络中的信息新鲜度（AoI），提升网络可靠性。


<details>
  <summary>Details</summary>
Motivation: 6G网络需满足高可靠低延迟通信（HRLLC）需求，尤其是自动驾驶等安全关键应用。非地面网络（NTN）如HAPS因其广覆盖和低延迟优势，可增强通信可靠性。

Method: 采用DDPG算法动态优化HAPS支持的V2X网络中的AoI，实现无需集中协调的独立学习。

Result: 方法显著提升了信息新鲜度和网络可靠性，适用于基于车队的自动驾驶系统。

Conclusion: HAPS与DDPG结合的资源分配方案在AoI优化方面具有潜力，尤其适用于基础设施受限区域。

Abstract: Sixth-generation (6G) networks are designed to meet the hyper-reliable and
low-latency communication (HRLLC) requirements of safety-critical applications
such as autonomous driving. Integrating non-terrestrial networks (NTN) into the
6G infrastructure brings redundancy to the network, ensuring continuity of
communications even under extreme conditions. In particular, high-altitude
platform stations (HAPS) stand out for their wide coverage and low latency
advantages, supporting communication reliability and enhancing information
freshness, especially in rural areas and regions with infrastructure
constraints. In this paper, we present reinforcement learning-based approaches
using deep deterministic policy gradient (DDPG) to dynamically optimize the
age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.
The proposed method improves information freshness and overall network
reliability by enabling independent learning without centralized coordination.
The findings reveal the potential of HAPS-supported solutions, combined with
DDPG-based learning, for efficient AoI-aware resource allocation in
platoon-based autonomous vehicle systems.

</details>


### [28] [Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach](https://arxiv.org/abs/2508.00020)
*Ferdaous Tarhouni,Ruibo Wang,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文研究了卫星-空中-地面一体化网络（SAGIN）中高空平台（HAPs）作为中继的性能，提出了三种评估指标，并利用球形随机几何（SSG）进行低复杂度性能分析。


<details>
  <summary>Details</summary>
Motivation: 随着全球无线通信需求的增长，SAGIN的重要性日益凸显，HAPs作为中继可提升通信性能，但缺乏从中继角度评估其性能的研究。

Method: 采用球形随机几何（SSG）工具，推导了三种性能指标的解析表达式，包括端到端性能指标BREP的闭式解。

Result: 通过数值结果展示了卫星网络拓扑对性能的影响，并分析了满足短期和长期数据率需求的最小HAP传输功率。

Conclusion: SSG框架在SAGIN中具有优势，HAPs作为中继能有效提升性能，研究为动态拓扑和干扰分析提供了低复杂度解决方案。

Abstract: In recent years, the satellite-aerial-ground integrated network (SAGIN) has
become essential in meeting the increasing demands for global wireless
communications. In SAGIN, high-altitude platforms (HAPs) can serve as
communication hubs and act as relays to enhance communication performance. In
this paper, we evaluate network performance and analyze the role of HAPs in
SAGIN from the relay perspective. Based on this unique perspective, we
introduce three metrics to evaluate the performance, named the average access
data rate, the average backhaul data rate, and the backhaul rate exceedance
probability (BREP). Considering the need for dynamic topology and interference
analysis, we choose spherical stochastic geometry (SSG) as a tool and derive
analytical expressions for the above metrics to achieve low-complexity
performance evaluation. Specifically, we provide a closed-form expression for
the end-to-end performance metric BREP. Given that there is no existing
literature in the SSG field studying networks from a relay perspective, we
specifically investigate the impact of satellite network topology on
performance in our numerical results to further highlight the advantages of the
SSG framework. Additionally, we analyze the minimum HAP transmission power
required to maintain both short-term and long-term data rate demands.

</details>


### [29] [Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts](https://arxiv.org/abs/2508.00234)
*Jin Yang,Qiong Wu,Zhiying Feng,Zhi Zhou,Deke Guo,Xu Chen*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习（DRL）的QoS感知LLM路由框架，用于优化边缘LLM服务的响应质量和延迟。


<details>
  <summary>Details</summary>
Motivation: 云LLM服务存在高延迟、不稳定性和隐私问题，边缘部署LLM可提升实时性和隐私保护，但需解决路由问题。

Method: 采用动态状态抽象技术和异构图注意力网络（HAN）表示全局状态，结合动作影响估计器和定制奖励函数优化DRL决策。

Result: 实验表明，该算法显著提升了平均QoS和计算资源效率。

Conclusion: 提出的DRL框架能有效解决边缘LLM服务的动态路由问题，提升服务质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
leading to a significant increase in user demand for LLM services. However,
cloud-based LLM services often suffer from high latency, unstable
responsiveness, and privacy concerns. Therefore, multiple LLMs are usually
deployed at the network edge to boost real-time responsiveness and protect data
privacy, particularly for many emerging smart mobile and IoT applications.
Given the varying response quality and latency of LLM services, a critical
issue is how to route user requests from mobile and IoT devices to an
appropriate LLM service (i.e., edge LLM expert) to ensure acceptable
quality-of-service (QoS). Existing routing algorithms fail to simultaneously
address the heterogeneity of LLM services, the interference among requests, and
the dynamic workloads necessary for maintaining long-term stable QoS. To meet
these challenges, in this paper we propose a novel deep reinforcement learning
(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM
services. Due to the dynamic nature of the global state, we propose a dynamic
state abstraction technique to compactly represent global state features with a
heterogeneous graph attention network (HAN). Additionally, we introduce an
action impact estimator and a tailored reward function to guide the DRL agent
in maximizing QoS and preventing latency violations. Extensive experiments on
both Poisson and real-world workloads demonstrate that our proposed algorithm
significantly improves average QoS and computing resource efficiency compared
to existing baselines.

</details>


### [30] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: 提出了一种结合马尔可夫链模型和ITU-R传播模型的可扩展频谱预测框架，用于动态频谱接入。


<details>
  <summary>Details</summary>
Motivation: 频谱资源在时空上常未被充分利用，需动态频谱接入策略以利用空闲频段。

Method: 结合两状态马尔可夫链模型（捕捉时间占用模式）和ITU-R传播模型（考虑路径损耗和杂波效应），预测频谱可用性。

Result: 方法能高效预测时空频谱机会，计算成本低，适用于实时频谱管理。

Conclusion: 框架灵活，适用于不同频段和场景，为认知无线电网络提供有效支持。

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


### [31] [Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network](https://arxiv.org/abs/2508.00042)
*Athanasios Tziouvaras,Carolina Fortuna,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Marko Grobelnik,Blaž Bertalanič*

Main category: cs.NI

TL;DR: 论文提出两种无监督、模型无关的批量概念漂移检测方法，用于AI原生6G网络中模型性能的维护。


<details>
  <summary>Details</summary>
Motivation: 无线环境的非静态性（如基础设施变化、用户移动性等）会导致概念漂移，降低模型准确性，现有方法多为领域专用或对某些漂移类型效果不佳。

Method: 引入两种基于预期效用分数的无监督概念漂移检测方法，无需部署后的真实标签即可判断是否需要模型重训练。

Result: 在室外指纹定位和链路异常检测的实际用例中，两种方法比经典检测器（如ADWIN、DDM、CUSUM）性能提升20-40个百分点，F1分数达0.94和1.00，误报率降低20个百分点。

Conclusion: 提出的方法显著提升了概念漂移检测的准确性和实用性，适用于动态无线环境中的模型维护。

Abstract: AI-native 6G networks promise unprecedented automation and performance by
embedding machine-learning models throughout the radio access and core segments
of the network. However, the non-stationary nature of wireless environments due
to infrastructure changes, user mobility, and emerging traffic patterns,
induces concept drifts that can quickly degrade these model accuracies.
Existing methods in general are very domain specific, or struggle with certain
type of concept drift. In this paper, we introduce two unsupervised,
model-agnostic, batch concept drift detectors. Both methods compute an
expected-utility score to decide when concept drift occurred and if model
retraining is warranted, without requiring ground-truth labels after
deployment. We validate our framework on two real-world wireless use cases in
outdoor fingerprinting for localization and for link-anomaly detection, and
demonstrate that both methods are outperforming classical detectors such as
ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an
F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus
reducing the false alarm rate by up to 20 percentage points compared to the
best classical detectors.

</details>


### [32] [Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies](https://arxiv.org/abs/2508.00228)
*Aashay Arora,Diego Davila,Frank Würthwein,John Graham,Dima Mishin,Justas Balcas,Tom Lehman,Xi Yang,Chin Guok,Harvey Newman*

Main category: cs.NI

TL;DR: 本文研究了US-CMS Tier-2站点为应对高亮度LHC时代400 Gbps带宽需求所需的软硬件改进，重点测试了XRootD HTTP第三方复制的性能。


<details>
  <summary>Details</summary>
Motivation: 为应对未来网络流量增长和用户数据分析需求，确保软件和硬件能够支持400 Gbps带宽需求，并解决站点间延迟问题。

Method: 通过系统测试，模拟真实网络条件，探索不同主机和传输配置，包括每集群的源数量和CPU分配。

Result: 通过创建跨广域网的网络“循环”，成功模拟了真实网络条件，并测试了XRootD的性能。

Conclusion: 研究为US-CMS Tier-2站点在高带宽需求下的软硬件优化提供了重要参考。

Abstract: In anticipation of the High Luminosity-LHC era, there is a critical need to
oversee software readiness for upcoming growth in network traffic for
production and user data analysis access. This paper looks into software and
hardware required improvements in US-CMS Tier-2 sites to be able to sustain and
meet the projected 400 Gbps bandwidth demands while tackling the challenge
posed by varying latencies between sites. Specifically, our study focuses on
identifying the performance of XRootD HTTP third-party copies across multiple
400 Gbps links and exploring different host and transfer configurations. Our
approach involves systematic testing with variations in the number of origins
per cluster and CPU allocations for each origin. By replicating real network
conditions and creating network "loops" that traverse multiple switches across
the wide area network, we are able to replicate authentic network conditions

</details>


### [33] [Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study](https://arxiv.org/abs/2508.00256)
*Chuang Zhang,Geng Sun,Jiacheng Wang,Yijing Lin,Weijie Yuan,Sinem Coleri,Dusit Niyato,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 论文探讨了低空无线网络（LAWNs）的安全挑战，并提出了一种基于大型人工智能模型（LAMs）的优化框架，通过增强状态特征和设计内在奖励来提升安全通信任务的强化学习性能。


<details>
  <summary>Details</summary>
Motivation: LAWNs因其低空操作、频繁移动和依赖非授权频谱等特点，面临独特的安全挑战，传统AI方法存在局限性，需要更先进的解决方案。

Method: 提出了一种基于LAM的优化框架，利用大型语言模型（LLMs）生成增强状态特征，并设计内在奖励，以改进强化学习性能。

Result: 通过案例研究验证了该框架的有效性，仿真结果表明其在提升安全通信任务性能方面的潜力。

Conclusion: LAMs在LAWNs安全通信中具有显著优势，未来可进一步探索其应用方向。

Abstract: Low-altitude wireless networks (LAWNs) have the potential to revolutionize
communications by supporting a range of applications, including urban parcel
delivery, aerial inspections and air taxis. However, compared with traditional
wireless networks, LAWNs face unique security challenges due to low-altitude
operations, frequent mobility and reliance on unlicensed spectrum, making it
more vulnerable to some malicious attacks. In this paper, we investigate some
large artificial intelligence model (LAM)-enabled solutions for secure
communications in LAWNs. Specifically, we first explore the amplified security
risks and important limitations of traditional AI methods in LAWNs. Then, we
introduce the basic concepts of LAMs and delve into the role of LAMs in
addressing these challenges. To demonstrate the practical benefits of LAMs for
secure communications in LAWNs, we propose a novel LAM-based optimization
framework that leverages large language models (LLMs) to generate enhanced
state features on top of handcrafted representations, and to design intrinsic
rewards accordingly, thereby improving reinforcement learning performance for
secure communication tasks. Through a typical case study, simulation results
validate the effectiveness of the proposed framework. Finally, we outline
future directions for integrating LAMs into secure LAWN applications.

</details>


### [34] [Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning](https://arxiv.org/abs/2508.00261)
*Saichao Liu,Geng Sun,Chuang Zhang,Xuejie Liu,Jiacheng Wang,Changyuan Zhao,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机（UAV）辅助移动边缘计算（MEC）的系统，通过优化无人机轨迹和资源分配，提升系统性能。采用增强的深度强化学习算法DPPOIL解决多目标优化问题，仿真结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）因固定位置和服务范围受限，性能受限。无人机辅助MEC系统可扩展服务范围，提升计算能力。

Method: 提出无人机轨迹控制和资源分配的多目标优化问题（TCRAMOP），并设计增强的深度强化学习算法DPPOIL，结合模仿学习技术优化策略。

Result: 仿真结果表明，DPPOIL算法优于其他基线方法，能有效最大化卸载数量并最小化延迟和能耗。

Conclusion: 无人机辅助MEC系统结合DPPOIL算法，显著提升了系统性能，为动态环境下的资源分配提供了有效解决方案。

Abstract: Mobile edge computing (MEC) is a promising technique to improve the
computational capacity of smart devices (SDs) in Internet of Things (IoT).
However, the performance of MEC is restricted due to its fixed location and
limited service scope. Hence, we investigate an unmanned aerial vehicle
(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can
simultaneously provide computing service for multiple SDs. To improve the
performance of system, we formulated a UAV-based trajectory control and
resource allocation multi-objective optimization problem (TCRAMOP) to
simultaneously maximize the offloading number of UAVs and minimize total
offloading delay and total energy consumption of UAVs by optimizing the flight
paths of UAVs as well as the computing resource allocated to served SDs. Then,
consider that the solution of TCRAMOP requires continuous decision-making and
the system is dynamic, we propose an enhanced deep reinforcement learning (DRL)
algorithm, namely, distributed proximal policy optimization with imitation
learning (DPPOIL). This algorithm incorporates the generative adversarial
imitation learning technique to improve the policy performance. Simulation
results demonstrate the effectiveness of our proposed DPPOIL and prove that the
learned strategy of DPPOIL is better compared with other baseline methods.

</details>


### [35] [Mamba for Wireless Communications and Networking: Principles and Opportunities](https://arxiv.org/abs/2508.00403)
*Rongsheng Zhang,Ruichen Zhang,Yang Lu,Wei Chen,Bo Ai,Dusit Niyato*

Main category: cs.NI

TL;DR: Mamba模型在无线通信中展现出潜力，通过平衡计算效率与效果，革新无线网络设计。文章综述了Mamba的应用，提出两个框架并验证其性能提升，同时指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 无线网络的异构性和动态性日益增加，Mamba模型有望通过高效处理时空数据，优化无线通信设计。

Method: 分析Mamba在无线信号处理中的潜力，提出两个应用框架（替代传统算法和启用新范式），并通过案例研究验证其性能。

Result: Mamba在特征增强和计算效率方面均有显著提升，案例研究证明了其实际应用价值。

Conclusion: Mamba为无线通信带来新机遇，但仍需解决关键挑战，未来研究方向值得探索。

Abstract: Mamba has emerged as a powerful model for efficiently addressing tasks
involving temporal and spatial data. Regarding the escalating heterogeneity and
dynamics in wireless networks, Mamba holds the potential to revolutionize
wireless communication and networking designs by balancing the trade-off
between computational efficiency and effectiveness. This article presents a
comprehensive overview of Mamba' applications in wireless systems.
Specifically, we first analyze the potentials of Mamba for wireless signal
processing tasks from the perspectives of long-range dependency modeling and
spatial feature extraction. Then we propose two application frameworks for
Mamba in wireless communications, i.e., replacement of traditional algorithms,
and enabler of novel paradigms. Guided by the two frameworks, we conduct case
studies on intelligent resource allocation and joint source and channel
decoding to demonstrate Mamba's improvements in both feature enhancement and
computational efficiency. Finally, we highlight critical challenges and outline
potential research directions for Mamba in wireless communications and
networking.

</details>


### [36] [Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications](https://arxiv.org/abs/2508.00583)
*Yunting Xu,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Deepu Rajan,Liang Yu,Haibo Zhou,Abbas Jamalipour,Xianbin Wang*

Main category: cs.NI

TL;DR: 论文探讨了大型视觉模型（LVMs）在视觉智能中的基础作用及其在物联网（IoT）中的应用，提出了一种渐进式微调框架，并在无人机网络中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LVMs在视觉任务中表现出色，但其在无线通信领域的应用仍面临模型大小和重新训练的挑战，需要一种高效的方法来适应多任务优化。

Method: 提出了一种渐进式微调框架，逐步调整预训练的LVMs以适应多任务优化，并在低空经济网络（LAENets）中进行了案例研究。

Result: 在无人机网络的联合波束成形和定位任务中，所提框架优于传统CNNs，展示了LVMs在智能无线系统中的潜力。

Conclusion: LVMs在无线通信领域具有广阔的应用前景，渐进式微调框架为多任务优化提供了一种高效解决方案。

Abstract: Large vision models (LVMs) have emerged as a foundational paradigm in visual
intelligence, achieving state-of-the-art performance across diverse visual
tasks. Recent advances in LVMs have facilitated their integration into Internet
of Things (IoT) scenarios, offering superior generalization and adaptability
for vision-assisted network optimization. In this paper, we first investigate
the functionalities and core architectures of LVMs, highlighting their
capabilities across classification, segmentation, generation, and multimodal
visual processing. We then explore a variety of LVM applications in wireless
communications, covering representative tasks across the physical layer,
network layer, and application layer. Furthermore, given the substantial model
size of LVMs and the challenges of model retraining in wireless domains, we
propose a progressive fine-tuning framework that incrementally adapts
pretrained LVMs for joint optimization of multiple IoT tasks. A case study in
low-altitude economy networks (LAENets) demonstrates the effectiveness of the
proposed framework over conventional CNNs in joint beamforming and positioning
tasks for Internet of drones, underscoring a promising direction for
integrating LVMs into intelligent wireless systems.

</details>


### [37] [Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications](https://arxiv.org/abs/2508.00616)
*Mingzhe Fan,Geng Sun,Hongyang Pan,Jiacheng Wang,Jiancheng An,Hongyang Du,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机搭载智能超表面（UAV-SIMs）的通信系统，通过联合优化用户关联、无人机位置和超表面相位，提升网络容量。


<details>
  <summary>Details</summary>
Motivation: 固定超表面限制了通信性能，而移动超表面（如无人机搭载）可以灵活部署，从而增强性能。

Method: 将联合优化问题分解为三个子问题（用户关联、无人机位置、超表面相位），采用交替优化策略求解。

Result: 仿真验证了所提策略在不同场景下的有效性。

Conclusion: 无人机搭载超表面及其优化策略能显著提升通信系统性能。

Abstract: Stacked intelligent metasurfaces (SIMs) have emerged as a promising
technology for realizing wave-domain signal processing, while the fixed SIMs
will limit the communication performance of the system compared to the mobile
SIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted
communication system, where UAVs as base stations (BSs) can cache the data
processed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance
the communication performance. To this end, we formulate a UAV-SIM-based joint
optimization problem (USBJOP) to comprehensively consider the association
between UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of
UAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and
NP-hardness of USBJOP, we decompose it into three sub-optimization problems,
which are the association between UAV-SIMs and users optimization problem
(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase
shifts optimization problem (USPSOP). Then, these three sub-optimization
problems are solved by an alternating optimization (AO) strategy. Specifically,
AUUOP and ULOP are transformed to a convex form and then solved by the CVX
tool, while we employ a layer-by-layer iterative optimization method for
USPSOP. Simulation results verify the effectiveness of the proposed strategy
under different simulation setups.

</details>


### [38] [Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach](https://arxiv.org/abs/2508.00629)
*Francisco Crespo,Javier Villegas,Carlos Baena,Eduardo Baena,Sergio Fortes,Raquel Barco*

Main category: cs.NI

TL;DR: 论文提出了一种轻量级、可编程的分布式应用（dApp），用于在O-RAN架构下动态管理CPU资源，提升能效和利用率，无需修改内核或依赖专有软件。


<details>
  <summary>Details</summary>
Motivation: O-RAN的软硬件解耦和虚拟化带来了CPU资源管理的挑战，现有调度器在实时性要求下表现不佳，导致能效低下。

Method: 开发了一个部署在DU层的dApp，通过线程级遥测数据（如上下文切换、IPC、缓存指标）实时调整CPU线程亲和性、核心隔离和频率缩放。

Result: 实验表明，该方案在商用srsRAN部署中实现了显著的节能效果，且不影响实时处理性能。

Conclusion: 该dApp展示了在下一代网络中通过低延迟应用实现细粒度资源控制的潜力。

Abstract: The transition toward softwarized Radio Access Networks (RANs), driven by the
Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through
disaggregation and virtualization of base station functions. However, this
shift introduces new challenges in managing CPU resources efficiently under
strict real-time constraints. In particular, the interplay between
latency-sensitive RAN workloads and general-purpose Operating System (OS)
schedulers often leads to sub-optimal performance and unnecessary energy
consumption. This work proposes a lightweight, programmable distributed
application (dApp) deployed at the Distributed Unit (DU) level to dynamically
orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging
thread-level telemetry like context switches, Instructions Per Cycle (IPC), and
cache metrics, to adapt CPU thread affinity, core isolation, and frequency
scaling in real time. Unlike existing solutions, it requires no access to
proprietary RAN software, hardware-specific features, or kernel modifications.
Fully compliant with the O-RAN architecture and agnostic to the underlying RAN
stack, the proposed solution introduces negligible overhead while improving
energy efficiency and CPU utilization. Experimental results using a
commercial-grade srsRAN deployment demonstrate consistent power savings without
compromising real-time processing performance, highlighting the potential of
low-latency dApps for fine-grained resource control in next-generation networks

</details>


### [39] [Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience](https://arxiv.org/abs/2508.00688)
*Ruiyang Huang,Haocheng Wang,Yixuan Shen,Ning Gao,Qiang Ni,Shi Jin,Yifan Wu*

Main category: cs.NI

TL;DR: 论文提出了一种两步框架，通过节点优先级排序和多目标拓扑优化增强异构海空群网络的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 异构海空群网络在对抗环境中易受通信中断和结构弱点影响，需提升其韧性。

Method: 设计三层架构表示网络依赖关系，提出SurBi-Ranking方法动态评估节点关键性，并应用NSGA-III算法优化拓扑。

Result: SurBi-Ranking比传统方法更准确识别关键节点，优化后网络在攻击下连接性下降减少30%，任务成功率更高。

Conclusion: 该框架显著提升了网络在对抗环境中的持续连接性和任务有效性。

Abstract: Heterogeneous marine-aerial swarm networks encounter substantial difficulties
due to targeted communication disruptions and structural weaknesses in
adversarial environments. This paper proposes a two-step framework to
strengthen the network's resilience. Specifically, our framework combines the
node prioritization based on criticality with multi-objective topology
optimization. First, we design a three-layer architecture to represent
structural, communication, and task dependencies of the swarm networks. Then,
we introduce the SurBi-Ranking method, which utilizes graph convolutional
networks, to dynamically evaluate and rank the criticality of nodes and edges
in real time. Next, we apply the NSGA-III algorithm to optimize the network
topology, aiming to balance communication efficiency, global connectivity, and
mission success rate. Experiments demonstrate that compared to traditional
methods like K-Shell, our SurBi-Ranking method identifies critical nodes and
edges with greater accuracy, as deliberate attacks on these components cause
more significant connectivity degradation. Furthermore, our optimization
approach, when prioritizing SurBi-Ranked critical components under attack,
reduces the natural connectivity degradation by around 30%, achieves higher
mission success rates, and incurs lower communication reconfiguration costs,
ensuring sustained connectivity and mission effectiveness across multi-phase
operations.

</details>


### [40] [Deep Joint Source-Channel Coding for Small Satellite Applications](https://arxiv.org/abs/2508.00715)
*Olga Kondrateva,Grace Li Zhang,Julian Zobel,Björn Scheuermann,Stefan Dietzel*

Main category: cs.NI

TL;DR: 提出了一种适用于卫星通信的深度联合源信道编码（DJSCC）框架，包括基础系统DJSCC-SAT和自适应架构ADJSCC-SAT，显著减少模型存储需求并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决小卫星在低地球轨道中因通信瓶颈导致的高维数据传输问题，探索DJSCC在复杂卫星环境中的实际应用。

Method: 建立DJSCC-SAT系统，引入多状态统计信道模型；提出自适应架构ADJSCC-SAT，利用注意力模块适应多种信道状态。

Result: 在Sentinel-2多光谱数据上验证，自适应方法性能接近多专用网络，且存储需求更低，对信道估计误差更鲁棒。

Conclusion: 该框架为实际卫星任务中部署鲁棒、自适应的DJSCC系统提供了实用高效的解决方案。

Abstract: Small satellites used for Earth observation generate vast amounts of
high-dimensional data, but their operation in low Earth orbit creates a
significant communication bottleneck due to limited contact times and harsh,
varying channel conditions. While deep joint source-channel coding (DJSCC) has
emerged as a promising technique, its practical application to the complex
satellite environment remains an open question. This paper presents a
comprehensive DJSCC framework tailored for satellite communications. We first
establish a basic system, DJSCC-SAT, and integrate a realistic, multi-state
statistical channel model to guide its training and evaluation. To overcome the
impracticality of using separate models for every channel condition, we then
introduce an adaptable architecture, ADJSCC-SAT, which leverages attention
modules to allow a single neural network to adjust to a wide range of channel
states with minimal overhead. Through extensive evaluation on Sentinel-2
multi-spectral data, we demonstrate that our adaptable approach achieves
performance comparable to using multiple specialized networks while
significantly reducing model storage requirements. Furthermore, the adaptable
model shows enhanced robustness to channel estimation errors, outperforming the
non-adaptable baseline. The proposed framework is a practical and efficient
step toward deploying robust, adaptive DJSCC systems for real-world satellite
missions.

</details>


### [41] [Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE](https://arxiv.org/abs/2508.00735)
*Lucas Aubard,Johan Mazel,Gilles Guette,Pierre Chifflier*

Main category: cs.NI

TL;DR: 论文介绍了PYROLYSE工具，用于测试IP和TCP重组策略的多样性，并发现其比之前认为的更复杂，同时揭示了多个实现中的错误和安全问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决IP和TCP重组策略在不同实现中的不一致性，以及由此导致的NIDS与主机OS解释差异引发的漏洞。

Method: 使用PYROLYSE工具对23种实现进行测试，分析其重组策略的多样性，并验证其正确性。

Result: 发现14至20种不同的重组行为，报告了8个错误，包括安全漏洞（如NIDS绕过或DoS攻击）。

Conclusion: 建议NIDS等工具不应仅基于双块测试策略处理多块重叠情况，以避免安全风险。

Abstract: IP fragmentation and TCP segmentation allow for splitting large data packets
into smaller ones, e.g., for transmission across network links of limited
capacity. These mechanisms permit complete or partial overlaps with different
data on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,
i.e., the data chunk preferences that depend on the overlap types, differ
across protocol implementations. This leads to vulnerabilities, as NIDSes may
interpret the packet differently from the monitored host OSes. Some NIDSes,
such as Suricata or Snort, can be configured so that their policies are
consistent with the monitored OSes. The first contribution of the paper is
PYROLYSE, an audit tool that exhaustively tests and describes the reassembly
policies of various IP and TCP implementation types. This tool ensures that
implementations reassemble overlapping chunk sequences without errors. The
second contribution is the analysis of PYROLYSE artifacts. We first show that
the reassembly policies are much more diverse than previously thought. Indeed,
by testing all the overlap possibilities for n <= 3 test case chunks and
different testing scenarios, we observe from 14 to 20 different behaviors out
of 23 tested implementations depending on the protocol. Second, we report eight
errors impacting one OS, two NIDSes, and two embedded stacks, which can lead to
security issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was
assigned to a NIDS error. Finally, we show that implemented IP and TCP policies
obtained through chunk pair testing are usually inconsistent with the observed
triplet reassemblies. Therefore, contrarily to what they currently do, NIDSes
or other network traffic analysis tools should not apply n = 2 pair policies
when the number of overlapping chunks exceeds two.

</details>


### [42] [Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype](https://arxiv.org/abs/2508.00792)
*Aashay Arora,Diego Davila,Jonathan Guiang,Frank Würthwein,Harvey Newman,Justas Balcas,Tom Lehman,Xi Yang*

Main category: cs.NI

TL;DR: DMM是一个原型接口，连接CERN的数据管理软件Rucio与SDN服务SENSE，优化高能物理数据传输的带宽分配和监控。


<details>
  <summary>Details</summary>
Motivation: 通过结合Rucio和SENSE，优化现有全球LHC计算基础设施中的网络使用效率。

Method: 设计并实现DMM，支持基于传输优先级的带宽分配和细粒度监控。

Result: DMM成功实现了网络优化和端到端数据流监控。

Conclusion: DMM为高能物理数据传输提供了高效的网络管理解决方案。

Abstract: The Data Movement Manager (DMM) is a prototype interface that connects CERN's
data management software, Rucio, with the Sofware-Defined Networking (SDN)
service SENSE by ESNet. It enables SDN-enabled high-energy physics data flows
using the existing worldwide LHC computing grid infrastructure. A key feature
of DMM is transfer priority-based bandwidth allocation, optimizing network
usage. Additionally, it provides fine-grained monitoring of underperforming
flows by leveraging end-to-end data flow monitoring. This is achieved through
access to host-level (network interface) throughput metrics and transfer-tool
(FTS) data transfer job-level metrics. This paper details the design and
implementation of DMM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理定律的可扩展时空Transformer（ScaleSTF），用于预测大规模城市网络的动态，解决了现有模型在效能与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市网络系统涉及复杂且规则未知的过程，现有模型（如图神经网络）在计算效率与预测效果之间存在矛盾，难以应用于大规模网络。

Method: 提出了一种基于Transformer结构的可解释神经扩散方案，其注意力层由低维嵌入诱导，具有线性复杂度。

Result: 在交通流量、太阳能发电和智能电表等大规模城市系统中验证了ScaleSTF的先进性能和卓越的可扩展性。

Conclusion: ScaleSTF为大规模城市网络动态预测提供了新视角，同时兼顾了效能与效率。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [44] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 研究提出了一种结合LSTM和Transformer的混合深度学习框架，用于高效测量公路铁路平交道口（HRGC）的剖面，以解决传统方法成本高、耗时长的问题。


<details>
  <summary>Details</summary>
Motivation: HRGC的高剖面可能导致车辆卡住，传统测量方法成本高且不安全，需要更高效、经济的解决方案。

Method: 开发了三种混合深度学习模型（Transformer-LSTM、LSTM-Transformer序列和并行模型），利用IMU和GPS传感器数据与地面真实数据结合进行训练。

Result: 模型2和3表现最佳，能快速生成2D/3D HRGC剖面，显著提升安全评估效率。

Conclusion: 该深度学习框架为HRGC剖面测量提供了高效、准确的解决方案，有望提升公路和铁路安全性。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [45] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 结合贝叶斯机制检测与条件神经过程，提出R-NP模型用于德国电力市场24小时电价预测，在多种应用场景中表现均衡且优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统电价预测方法在复杂市场环境下表现不稳定，需结合机制检测与局部建模以提高预测准确性和实用性。

Method: 使用DS-HDP-HMM检测电价机制，每个机制由独立的CNP建模，最终预测为机制加权的CNP输出混合。

Result: R-NP在TOPSIS多标准评估中表现最优，尤其在2021-2023年成为最均衡的解决方案。

Conclusion: R-NP模型在电价预测中兼具准确性与实用性，优于DNN和LEAR模型。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [46] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过分阶段构建LLM，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调在边缘设备上资源消耗大的问题，同时保持数据隐私。

Method: 分阶段微调，逐步增加子模型参数容量，通过知识转移和优化初始化参数加速训练。

Result: DevFT在多个基准测试中表现优异，收敛速度提升4.59倍，通信开销减少10.67倍，性能平均提升9.07%。

Conclusion: DevFT是一种高效且兼容现有方法的联邦微调方案，适用于资源受限的边缘设备。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [47] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 比较了两种空间约束（权重相似性和激活相似性）对地形卷积神经网络的影响，发现权重相似性在鲁棒性、输入敏感性和功能定位方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究地形约束的不同实现方式对神经网络学习表示的影响，填补系统性研究的空白。

Method: 通过权重相似性（WS）和激活相似性（AS）两种约束训练地形卷积神经网络，并评估分类准确性、鲁棒性和表示的空间组织。

Result: WS在噪声鲁棒性、输入敏感性和功能定位方面优于AS和标准CNN，并影响了网络的表示几何。

Conclusion: 权重相似性约束在端到端训练中能产生更鲁棒的表示，并对特征学习和功能组织有显著影响。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [48] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估强化学习算法在部分可观测性下性能的基准框架POBAX，强调基准需覆盖多种部分可观测形式并体现记忆改进能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估简单的状态混淆形式，无法反映真实场景中的复杂部分可观测性，如视觉遮挡或未知对手意图。

Method: 提出POBAX开源库，包含多种代表性环境（如定位、视觉控制、游戏等），并验证这些任务具有记忆改进能力。

Result: POBAX提供了快速评估的工具和GPU可扩展的实验环境，支持算法在复杂部分可观测性下的性能测试。

Conclusion: POBAX为部分可观测性研究提供了全面的基准框架，有助于推动算法在真实场景中的应用。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [49] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型的无监督时间序列异常检测框架TriP-LLM，通过三分支设计整合局部和全局特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和智能制造的普及，时间序列数据的规模和复杂性增加，传统统计方法难以应对高异质性和复杂性。

Method: TriP-LLM采用三分支设计（分块、选择和全局）将时间序列编码为分块标记，利用预训练的大型语言模型处理，并通过轻量级解码器重构输入以计算异常分数。

Result: 实验表明，TriP-LLM在多个公共数据集上优于现有方法，且内存消耗更低。

Conclusion: TriP-LLM展示了大型语言模型在时间序列异常检测中的潜力，适用于GPU内存受限环境。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [50] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个结合自适应压缩和激活检查点的内存管理框架，旨在减少GPU内存占用并加速大型语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中，重计算会带来高达30%的开销，现有方法在内存优化和模型精度之间存在权衡。

Method: Adacc包含三个模块：(1) 针对LLM张量的异常值设计层特定压缩算法；(2) 使用MILP优化调度策略；(3) 引入自适应策略演化机制。

Result: 实验表明，Adacc比现有框架加速1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc通过自适应压缩和调度策略，显著提升了LLM训练效率，同时保证了模型精度。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


### [51] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LightGBM回归模型和遗传算法（GA）优化的新方法框架，用于评估COVID-19相关指标对比特币回报预测的贡献。结果表明，疫情指标显著提升了模型性能，尤其是疫苗接种数据。


<details>
  <summary>Details</summary>
Motivation: 研究的主要目标不仅是预测比特币回报，而是确定疫情相关的健康数据是否能显著提升预测准确性。

Method: 构建了包含每日比特币回报和COVID-19指标的数据集，使用GA优化模型，并通过统计方法（如Mann-Whitney U检验）比较性能。

Result: COVID-19指标显著提高了模型性能（R2增加40%，RMSE降低2%），疫苗接种数据是主要预测因子。

Conclusion: 该方法通过整合公共卫生信号扩展了金融分析工具，为投资者和政策制定者在系统性危机中提供了更精细的市场指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [52] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 论文提出了一种名为Stress-Aware Learning的弹性神经网络训练范式，通过动态调整优化行为来提升模型在稳定或不确定训练环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中的结构疲劳概念启发，旨在解决深度神经网络在训练过程中遇到的优化停滞问题。

Method: 提出Plastic Deformation Optimizer，通过内部应力信号检测优化困难，并注入自适应噪声以逃离尖锐极小值。

Result: 在六种架构、四种优化器和七个视觉基准测试中验证了方法的鲁棒性和泛化能力，计算开销小。

Conclusion: Stress-Aware Learning通过动态调整优化行为，显著提升了模型的泛化能力和鲁棒性。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [53] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: 论文提出StackLiverNet，一种可解释的堆叠集成模型，用于肝脏疾病检测，解决了现有方法的分类错误高、可解释性差等问题，并展示了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病诊断需要精确且及时的方法，现有机器学习模型存在高误分类率、可解释性差等问题，因此需要改进。

Method: 采用高级数据预处理和特征选择技术，结合随机欠采样处理类别不平衡，构建基于LightGBM元模型的堆叠集成模型。

Result: 模型测试准确率达99.89%，Cohen Kappa为0.9974，AUC为0.9993，训练和推理速度快。

Conclusion: StackLiverNet在性能和可解释性上均表现优异，适用于临床实践。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [54] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 论文提出了一种新的神经网络层变换方法，通过结构化线性算子和残差校正组件，提升训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当代神经网络缺乏结构保障，导致学习不稳定和可解释性差。

Method: 将层变换分解为结构化线性算子和残差校正组件，优化信号传播和训练动态。

Result: 实验表明，该方法改善了梯度条件、降低扰动敏感性，并增强层间鲁棒性。

Conclusion: 该方法为更稳定、透明的神经网络架构奠定了基础，同时保持表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [55] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 该研究通过PCA和变分自编码器（VAE）简化心电图（ECG）数据，提出三种新型VAE变体，在信号重建和下游预测任务中表现优异，尤其适用于小规模数据集。


<details>
  <summary>Details</summary>
Motivation: ECG信号复杂度高且个体差异大，传统深度学习方法在小数据集上效果有限，研究旨在通过特征生成方法解决这一问题。

Method: 使用PCA和自编码器（包括三种新型VAE变体：SAE、A beta-VAE和C beta-VAE）简化ECG数据，并结合LGBM分类器进行预测任务。

Result: A beta-VAE在信号重建中表现最佳（MAE为15.7±3.2 μV），SAE编码结合传统特征提升了LVEF预测（AUROC为0.901），接近CNN模型但计算资源需求更低。

Conclusion: 新型VAE编码不仅简化了ECG数据，还为小规模标注数据下的深度学习应用提供了实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [56] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN是一种结合强化学习和图神经网络的框架，用于优化自行车流量传感器的布局并提高数据稀疏环境下的流量估计精度。


<details>
  <summary>Details</summary>
Motivation: 城市自行车流量数据稀疏，传感器覆盖有限，导致流量估计困难。

Method: 结合GCN、GAT和DQN的强化学习代理，数据驱动地选择传感器位置。

Result: 在墨尔本自行车网络中，INSPIRE-GNN显著优于传统启发式方法，提升了MSE、RMSE和MAE等指标。

Conclusion: 该框架为交通规划者提供了优化传感器布局和提高数据可靠性的实用工具。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [57] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种基于权重而非激活的新方法，用于理解和监控微调后的LLM，无需依赖与训练数据分布相似的输入数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的解释方法需要分布相似的数据，限制了检测和防御新型威胁（如后门）的能力。

Method: 通过分析微调模型与基础模型权重差异的顶部奇异向量，识别新行为，并通过余弦相似度监控这些行为。

Result: 在后门模型中阻止100%攻击（假阳性率<1.2%），在遗忘模型中检测遗忘主题准确率达95.42%，并能恢复“遗忘”信息。

Conclusion: 该方法在监控和预部署审计中表现优异，适用于商业模型分析。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [58] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效且鲁棒的医学图像传输。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信技术的发展推动了远程医疗的需求，但医学数据在有限带宽和噪声信道中的高效传输仍是一大挑战。

Method: 开发了医学增强的压缩和去噪模块，通过捕捉关键语义信息实现高效带宽利用和鲁棒性。

Result: 在真实医学数据集上的实验表明，DiSC-Med在噪声信道中实现了超高带宽效率和优异的图像重建性能。

Conclusion: DiSC-Med框架为远程医疗提供了高效且鲁棒的解决方案，具有广阔的应用潜力。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [59] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出了一种将回归问题转化为强化学习问题的新方法，通过自定义奖励信号和RL算法实现函数逼近，展示了在噪声正弦波学习任务中的成功应用。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法受限于预定义的可微损失函数（如均方误差），难以处理非对称成本或复杂、不可微目标。

Method: 将模型预测视为动作，基于预测误差定义自定义奖励信号，利用Actor-Critic算法，并结合优先级经验回放、增加网络容量和位置编码等技术。

Result: RL框架不仅成功解决了回归问题，还提供了定义目标和指导学习过程的更大灵活性。

Conclusion: 强化学习为回归问题提供了一种灵活且有效的替代范式。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [60] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为BEMA的方法，通过修正EMA中的偏差，显著提升了语言模型微调的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调中的随机性（通常由小批量训练引起）会导致训练不稳定，而传统的EMA方法虽能减少随机性，但会引入偏差。

Method: 提出BEMA方法，通过修正EMA中的偏差，保留方差减少的优势。

Result: 实验表明，BEMA在多种标准语言模型基准测试中显著提升了收敛速度和最终性能。

Conclusion: BEMA是一种实用且理论支持的方法，可提高微调的稳定性和效率。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [61] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的强化学习框架，用于优化大规模推荐系统中的会话目标，显著提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖监督学习，难以优化长期目标（如会话参与度），而强化学习在大规模应用时面临动作空间大和工程复杂性的挑战。

Method: RecoMind利用现有推荐模型构建模拟环境，并通过定制探索策略高效探索大规模动作空间，简化RL策略的训练与部署。

Result: 离线模拟和在线A/B测试显示，RecoMind训练的RL策略显著优于传统监督学习方法，会话深度和观看时长分别提升4.71%和15.81%。

Conclusion: RecoMind为大规模推荐系统提供了一种可扩展的RL嵌入方法，有效优化会话用户满意度。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [62] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段框架，利用几何信息改进基础模型在噪声标签数据下的鲁棒分类性能，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在噪声标签数据下的微调需求，以及现有kNN方法利用局部几何信息的局限性。

Method: 两阶段框架：可靠性估计和可靠性加权推断，结合非负核（NNK）邻域构建和几何信息。

Result: 在CIFAR-10和DermaMNIST数据集上，方法在各种噪声条件下表现优于标准kNN和自适应邻域基线。

Conclusion: 通过引入几何信息，提出的方法显著提升了噪声标签数据下的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [63] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: 论文提出了一种新的参数高效微调方法KRAdapter，通过Khatri-Rao乘积生成权重更新，解决了LoRA在近似高有效秩矩阵时的局限性，并在多模态和大语言模型中验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: LoRA在适应高有效秩矩阵时表现不佳，尤其是在多模态和大语言模型中。

Method: 引入KRAdapter算法，利用Khatri-Rao乘积生成权重更新，以更好地处理高有效秩矩阵。

Result: KRAdapter在1B参数的视觉语言模型和8B参数的大语言模型中表现优于LoRA，尤其在未见过的常识推理任务上。

Conclusion: KRAdapter在保持LoRA计算和内存效率的同时，提供了更鲁棒的参数高效微调方法。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [64] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究发现指令微调会导致大语言模型（LLM）的置信度校准退化，标签平滑是一种有效解决方案，但在大词汇量LLM中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探讨指令微调对LLM置信度校准的影响，并寻找实用解决方案。

Method: 分析开源LLM的校准退化现象，提出标签平滑作为解决方案，并设计定制内核以减少内存消耗。

Result: 标签平滑能有效维持校准，但在大词汇量LLM中效果受限，理论及实验验证了其与模型规模的关联。

Conclusion: 标签平滑是指令微调中保持校准的有效方法，但需进一步优化以应对大词汇量LLM的挑战。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [65] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 论文介绍了一种在线辅导系统，通过多臂老虎机框架和离线策略评估，优化学生反馈策略，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 通过大规模学生数据优化反馈策略，以提高学生的即时和整体学习表现。

Method: 使用多臂老虎机（MAB）框架和离线策略评估，分析43,000种辅助动作，并设计算法选择最佳策略。进一步探索上下文老虎机（CB）策略的个性化效果。

Result: 在166,000次练习会话中验证了MAB策略显著提升学生表现，但CB策略的个性化效果有限。

Conclusion: 数据驱动的反馈系统已大规模部署，未来可进一步优化个性化策略。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [66] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 该研究提出了一种将基于性能的抗震设计视为逆工程问题的方法，通过可解释的机器学习模型直接映射设计变量与性能指标，解决了传统方法的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于性能的抗震设计存在计算效率低下的问题，研究旨在通过机器学习模型直接优化设计参数，提高效率。

Method: 采用可解释的机器学习模型映射设计变量与性能指标，并将其作为评估函数集成到遗传优化算法中，解决逆问题。

Result: 模型在多种建筑类型、几何形状和地震设计条件下表现出高准确性（R2>90%），优化算法能够识别符合工程原理的最优构件属性。

Conclusion: 该方法通过机器学习与优化算法的结合，显著提高了抗震设计的效率和准确性，适用于多样化的建筑和地震条件。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [67] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer是一种基于图不变学习的Transformer模型，旨在解决图数据分布变化下的泛化问题，通过分离不变与可变子图并优化编码模块，实现对新图的泛化表示。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在数据分布变化下泛化能力不足，图不变学习可能提供解决方案，但如何设计基于不变学习的注意力机制和编码方式仍具挑战。

Method: GOODFormer通过三个模块联合优化：1) 熵引导的不变子图分离器；2) 动态子图编码器；3) 不变学习模块，结合理论支持。

Result: 在基准数据集上，GOODFormer在分布变化下优于现有方法。

Conclusion: GOODFormer有效解决了图Transformer在分布变化下的泛化问题，为图不变学习提供了新思路。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [68] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA是一种新的数据同化方法，通过结合轻量级梯度更新和预训练生成先验，减少地球系统建模中的误差累积，优于传统变分方法。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模中误差累积和传统变分方法对高斯误差假设的局限性。

Method: PnP-DA交替使用梯度分析更新和预训练生成先验，避免复杂神经网络的反向传播。

Result: 在标准混沌测试中，PnP-DA显著减少预测误差，优于传统方法。

Conclusion: PnP-DA通过放松统计假设和利用历史数据，提供了一种高效的数据同化解决方案。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [69] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 论文提出了一种基于UMAP和敏感性分析的胚胎学方法，用于可视化语言模型在训练过程中的结构发展，揭示了新的网络机制。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型内部计算结构的形成是深度学习科学的核心问题，而敏感性分析作为一种工具尚未充分发挥其潜力。

Method: 应用UMAP对敏感性矩阵进行可视化，分析模型在训练中的结构发展。

Result: 可视化结果展示了清晰的“身体计划”，包括已知特征（如感应电路）和新发现的结构（如用于计数空格标记的“间距鳍”）。

Conclusion: 敏感性分析不仅能验证模型，还能揭示新机制，为研究复杂神经网络的发育原理提供了全面视角。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [70] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种名为BOOD的新框架，利用扩散模型生成高质量的OOD特征和图像，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在潜在空间中提取有效的OOD特征，特别是在决策边界附近，因此需要一种更高效的方法来生成OOD数据。

Method: BOOD通过学习文本条件的潜在特征空间，选择靠近决策边界的ID特征，并通过扰动生成OOD特征，再用扩散模型解码为图像。

Result: 实验显示BOOD显著优于现有方法，FPR95降低29.64%，AUROC提升7.27%。

Conclusion: BOOD提供了一种高效的OOD特征生成策略，显著提升了OOD检测性能。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [71] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC（Sheaf GNNs with PAC-Bayes Calibration）是一种结合细胞鞘消息传递的新架构，通过最优传输提升、方差减少扩散和PAC-Bayes谱正则化，解决了GNN中的过平滑问题，并在异质图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在异质图上的过平滑问题，现有鞘神经网络方法依赖静态或高参数化结构，缺乏泛化性和稳定性保证。

Method: SGPC结合细胞鞘消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化，实现半监督节点分类。

Result: 在9个同质和异质基准测试中，SGPC优于现有光谱和鞘基GNN，并提供未见节点的置信区间。

Conclusion: SGPC通过理论性能边界和端到端训练，显著提升了GNN在异质图上的表现，并提供了稳定性保证。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [72] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: OID-PPO是一种基于强化学习的室内设计优化框架，通过结合专家定义的功能和视觉准则，显著提升了布局质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 住宅室内设计对居住者满意度影响重大，但现有方法因计算成本高、数据稀缺或设计原则不足而受限。

Method: 提出OID-PPO框架，利用近端策略优化和连续家具放置策略，整合设计准则到奖励函数中。

Result: 实验表明OID-PPO在布局质量和计算效率上优于现有方法，并通过消融研究验证了设计准则的重要性。

Conclusion: OID-PPO为室内设计提供了一种高效且灵活的解决方案，同时揭示了设计准则的具体贡献。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [73] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种具有双重适应性的通用算法，通过元专家框架动态创建和聚合专家，以适应不同函数类型和环境变化。


<details>
  <summary>Details</summary>
Motivation: 现有算法缺乏通用性，只能处理单一凸函数类型且需要先验参数知识，限制了实际应用。

Method: 提出元专家框架，动态创建多个专家并通过元算法聚合，结合睡眠专家技术捕捉环境变化。

Result: 算法能同时最小化多种凸函数的自适应遗憾，并允许函数类型在轮次间切换。

Conclusion: 该框架扩展至在线复合优化，开发了通用算法以最小化复合函数的自适应遗憾。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [74] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，通过图形界面和知识图谱帮助非ML专家构建ML流程。


<details>
  <summary>Details</summary>
Motivation: 解决非ML专家在科学和工程领域构建高质量ML流程的困难。

Method: 结合知识图谱和图形界面，简化ML流程的构建。

Result: ExeKGLib提高了流程的透明度和可重用性，并确保其可执行。

Conclusion: ExeKGLib在真实用例中展示了其可用性和实用性。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [75] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Co-Reward的新型强化学习框架，通过对比语义相似问题的答案一致性作为奖励基础，解决了自奖励信号中的崩溃问题，并在多个推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管带有可验证奖励的强化学习（RLVR）在提升大语言模型（LLM）推理能力方面有潜力，但其依赖人工标注标签的问题限制了扩展性。现有自奖励方法虽能激发LLM推理潜力，但存在崩溃问题。

Method: 提出Co-Reward框架，通过为每个训练样本构造相似问题，并通过投票生成代理标签，利用对比一致性构建奖励，以增强推理一致性。

Result: 在多个推理基准测试和LLM系列中，Co-Reward表现优于其他自奖励基线，甚至超越基于真实标签的奖励，如在MATH500上提升6.8%。

Conclusion: Co-Reward通过自监督奖励机制有效避免了崩溃问题，提升了推理能力，扩展了输入样本的多样性。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [76] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种ResE-BiLSTM模型，用于预测贷款违约，通过滑动窗口技术和多种基线模型对比，展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 贷款违约预测对信用风险管理至关重要，机器学习方法可以提升预测准确性。

Method: 使用ResE-BiLSTM模型，结合滑动窗口技术，并在Freddie Mac数据集上评估，与LSTM、BiLSTM、GRU、CNN和RNN等基线模型对比。

Result: ResE-BiLSTM在准确性、精确度、召回率、F1和AUC等指标上优于基线模型。

Conclusion: ResE-BiLSTM在贷款违约预测中表现出色，具有实际应用价值。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [77] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: 论文提出了一种名为ctdGAN的条件生成对抗网络，用于解决表格数据中的类别不平衡问题，通过空间分区和新的损失函数生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的类别不平衡问题严重影响机器学习性能，现有GAN方法未考虑输入样本的向量子空间，导致生成数据位置随意且条件采样效果不佳。

Method: ctdGAN通过空间分区步骤为输入样本分配聚类标签，利用新概率采样策略和损失函数生成样本，确保生成数据分布接近原始数据。

Result: 在14个不平衡数据集上的实验表明，ctdGAN能生成高保真样本并显著提升分类准确率。

Conclusion: ctdGAN通过改进生成策略和损失函数，有效解决了表格数据中的类别不平衡问题，生成样本质量高且分类性能优越。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [78] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和图神经网络（GNNs）的新框架CoLL，用于文本属性图（TAGs）中的异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法主要关注图域内的优化目标，忽视了文本模态的互补价值，且文本特征常通过浅层嵌入技术编码，可能遗漏异常相关的语义上下文。

Method: 提出CoLL框架，通过多LLM协作进行证据增强生成，捕捉异常相关上下文，并结合带门控机制的GNN自适应融合文本特征与拓扑信息。

Result: 实验表明CoLL在AP上平均提升13.37%，显著优于现有方法。

Conclusion: CoLL为结合LLMs推进图异常检测开辟了新途径。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [79] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMUCL的端到端文本属性图异常检测方法，通过跨模态和多尺度一致性联合训练文本和图编码器，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的图数据常带有文本描述（TAGs），现有方法在文本编码和异常检测目标之间存在分离，限制了检测能力。如何整合文本和图拓扑以释放跨模态数据的潜力是一个挑战。

Method: 提出CMUCL方法，同时建模文本和图结构数据，通过跨模态和单模态多尺度一致性联合训练编码器，并设计基于不一致性挖掘的异常评分器。

Result: 实验表明，CMUCL在文本属性图异常检测中表现优异，平均准确率（AP）比次优方法提高了11.13%。

Conclusion: CMUCL通过整合文本和图拓扑，显著提升了异常检测能力，并发布了8个数据集以推动未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [80] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文研究了在线非子模优化问题，提出了两种算法（DBGD-NF和其扩展版）以改进现有方法对延迟和反馈的敏感性，并取得了更好的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法对延迟敏感且未解耦延迟与反馈的联合效应，因此需要改进。

Method: 提出了DBGD-NF算法及其扩展版，分别利用单点梯度估计和块更新机制。

Result: DBGD-NF实现了与平均延迟相关的遗憾界，扩展版进一步解耦了延迟与反馈的效应，实验验证了方法的优越性。

Conclusion: 新算法在延迟和反馈处理上优于现有方法，适用于结构化稀疏学习等场景。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [81] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出一种两阶段框架，通过信噪比阈值和滤波优化高光谱数据，结合聚类和NNLS解混，提升矿物检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱成像中弱矿物信号被噪声和冗余波段掩盖的问题。

Method: 第一阶段：信噪比阈值和Savitzky-Golay滤波；第二阶段：KMeans聚类和NNLS解混。

Result: 实验表明，该方法提高了弱矿物区域的检测精度和解混准确性。

Conclusion: 两阶段策略为地质高光谱应用提供了一种实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [82] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 论文提出了一种新的可解释性定义，旨在解决现有定义不具操作性的问题，并提供了设计可解释模型的一般蓝图和开源库。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性定义缺乏操作性，无法指导用户设计通用、可靠且稳健的可解释模型，导致研究问题不明确。

Method: 提出了一种通用且简单的可解释性新定义，涵盖现有非正式概念，并基于此设计了一个可解释模型的通用蓝图和开源库。

Result: 新定义具有操作性，揭示了设计可解释模型所需的基础属性、假设、原则、数据结构和架构特征。

Conclusion: 论文为可解释性研究提供了更清晰的方向和工具，推动了可解释AI的发展。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [83] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的方法，通过生成大量氢原子转移（HAT）反应的数据集，并比较三种图神经网络架构（SchNet、Allegro和MACE），以准确预测反应势能面和反应能垒。MACE表现最佳，可用于大规模胶原蛋白模拟。


<details>
  <summary>Details</summary>
Motivation: 氢原子转移（HAT）反应在生物过程中至关重要，但其机理尚不完全清楚。传统模拟方法（如经典力场或DFT分子动力学）难以满足量子化学精度的需求。

Method: 通过半经验方法和DFT生成HAT反应数据集，并比较三种图神经网络架构（SchNet、Allegro和MACE）的性能。MACE用于预测反应势能面和能垒。

Result: MACE在能量、力和能垒预测上表现最优，平均绝对误差为1.13 kcal/mol，适用于大规模胶原蛋白模拟。

Conclusion: 该方法可推广至其他生物分子系统，为复杂环境中的化学反应提供量子级精度的模拟。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [84] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: 研究发现，主动学习（AL）在低数据场景下效率最低，仅提升1-4%，而数据增强（DA）和半监督学习（SSL）结合随机采样可提升60%。但AL与DA和SSL结合后仍能提供额外提升，建议将其作为性能优化的最后一步。


<details>
  <summary>Details</summary>
Motivation: 探讨主动学习（AL）在实际应用中较少使用的原因，并研究其在低数据场景下的效率问题。

Method: 比较数据增强（DA）、半监督学习（SSL）和主动学习（AL）在低数据场景下的表现，并分析其组合效果。

Result: AL单独使用时效率最低，但与DA和SSL结合后仍能提供额外性能提升。

Conclusion: 建议将AL作为数据优化的最后一步，而非解决标签缺失的主要方法。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [85] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出了一种基于相似性的自建图模型（SBSCGM）和混合图神经网络（HybridGraphMedGNN），用于预测ICU患者的死亡风险和连续关键性评分，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录（EHR）中的关系结构，而准确预测ICU患者的关键性对早期干预至关重要。

Method: SBSCGM动态构建患者相似图，HybridGraphMedGNN结合GCN、GraphSAGE和GAT层学习患者表示。

Result: 在MIMIC-III数据集上，模型AUC-ROC达0.94，优于基线方法和单一GNN模型。

Conclusion: 该框架为重症监护风险预测提供了可扩展且可解释的解决方案。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [86] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP是一个用户友好的QGIS插件，通过自监督学习策略解决了深度学习在遥感中的三大挑战：大数据需求、高计算资源和强编码能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感中的应用受限于大数据需求、高计算资源和强编码能力，IAMAP旨在为非AI专家提供便捷的工具。

Method: IAMAP基于自监督学习的通用模型，支持特征提取、降维、聚类、相似性映射和模型验证等功能。

Result: IAMAP使非专家能够高效利用深度学习特征，无需GPU或大量数据。

Conclusion: IAMAP推动了高效、节能的深度学习方法的普及。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [87] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN是一种新型神经网络框架，通过分离变量和自适应谱方法解决传统PINN在高频振荡PDE中的谱偏差问题，显著提升精度并减少参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在高频振荡PDE中存在谱偏差问题，无法有效捕捉高频解分量，限制了其应用。

Method: SV-SNN通过分解多变量函数为单变量乘积、自适应傅里叶谱特征和学习频率参数，结合SVD理论框架量化谱偏差。

Result: 在多个基准问题中，SV-SNN精度提升1-3个数量级，参数减少90%，训练时间缩短60%。

Conclusion: SV-SNN有效解决了神经PDE求解中的谱偏差问题，具有显著优势。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [88] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于KAN的自适应频率选择学习架构（KFS），用于解决时间序列预测中多尺度噪声干扰和频率信息分布不均的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列在不同尺度上存在噪声干扰，且频率信息分布不均，导致多尺度表示效果不佳。

Method: 结合Kolmogorov-Arnold Networks（KAN）和Parseval定理，设计KFS框架，通过FreK模块进行频域能量分布主导频率选择，并利用时间戳嵌入对齐多尺度时间表示。

Result: 在多个真实时间序列数据集上的实验表明，KFS实现了最先进的性能。

Conclusion: KFS是一种简单而有效的架构，能够显著提升时间序列预测的准确性。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [89] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 论文探讨了强化学习在应对低成本无人机群威胁中的应用，展示了其在防御系统中的战略优势。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群的威胁对现代防御系统提出了快速战略决策的需求，需优化拦截优先级。

Method: 通过高保真模拟环境，训练强化学习代理在离散动作空间中协调多效应器，优化拦截优先级。

Result: 强化学习策略在模拟攻击场景中表现优于基于规则的基线，平均损害更低，防御效率更高。

Conclusion: 强化学习可作为防御架构的战略层，提升韧性而不取代现有控制系统。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [90] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，解决了FNOs的过参数化和不确定性量化问题，通过减少参数数量和内存占用，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: FNOs存在过参数化和缺乏原生不确定性量化的问题，限制了其在科学和工程应用中的可靠性。

Method: DINOZAUR采用扩散乘法器替代FNOs中的密集张量乘法器，并通过贝叶斯方法定义时间参数的先验分布，实现不确定性量化。

Result: 在多个PDE基准测试中，DINOZAUR表现优异，同时提供了高效的不确定性量化。

Conclusion: DINOZAUR是一种高效且可靠的神经算子方法，适用于需要不确定性量化的科学和工程问题。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [91] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv是一个基于纵向电子健康记录（EHR）的生存预测模型，通过神经控制微分方程（NCDE）提取连续时间潜在轨迹，并结合对比学习和解释方法提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要可靠的生存预测，但现有方法难以处理不规则采样的临床数据并透明地关联生存结果。

Method: TrajSurv使用NCDE提取潜在轨迹，通过对比学习对齐潜在状态与患者状态，并采用两步解释方法关联临床进展与生存结果。

Result: 在MIMIC-III和eICU数据集上，TrajSurv表现出竞争性准确性和优于现有深度学习的透明度。

Conclusion: TrajSurv为临床决策提供了准确且透明的生存预测工具。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [92] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文提出了一种动态原型（DP）的DGAD模型，用于捕捉动态图中演变的异常模式，通过选择性更新内存缓冲区和置信度伪标记实现跨域检测。


<details>
  <summary>Details</summary>
Motivation: 动态图异常检测（DGAD）在多个领域（如金融、交通、社交网络）中至关重要，但现有通用模型难以捕捉动态图中的异常演变，且新领域缺乏标注数据。

Method: DP-DGAD通过提取动态原型（正常和异常模式的演变表示）并存储在内存缓冲区中，选择性更新缓冲区以保留通用模式，同时引入新领域特定模式，最后通过异常评分器和伪标记进行检测。

Result: 在十个真实数据集上的实验表明，DP-DGAD实现了最先进的性能。

Conclusion: DP-DGAD通过动态原型和伪标记有效解决了跨域动态图异常检测的挑战。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [93] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 论文提出了一种结合GDFM和GAN的方法，用于同时合成分布式风电场的长期风电功率场景，优于单独使用GDFM或GAN的方法。


<details>
  <summary>Details</summary>
Motivation: 资源充足性研究需要合成多风电场长期风电功率场景，现有方法（如GDFM和GAN）各有局限，无法同时满足时空相关性和波形模仿需求。

Method: 结合GDFM和GAN，利用GAN提取动态因子并作为GDFM的滤波器，以同时捕捉时空和频率相关性。

Result: 在澳大利亚风电数据上的测试表明，该方法在合成风电功率场景时优于其他替代方案，更接近实际风电的统计特性。

Conclusion: GDFM与GAN的结合方法能有效合成具有实际统计特性的风电功率场景，适用于资源充足性研究。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [94] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 比较多种AI模型对临床笔记分类的性能，发现超参数调优显著提升模型表现，而数据平衡方法影响有限。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型在心理健康诊断中的分类性能，为AI辅助诊断工具提供参考。

Method: 比较传统机器学习（如随机森林、SVM）和深度学习模型（如DistilBERT、SciBERT），并评估不同过采样策略（如SMOTE）和超参数调优的效果。

Result: 超参数调优显著提升模型准确率（最高96%），过采样方法仅对BERT模型有轻微正面影响。

Conclusion: 超参数调优对模型性能至关重要，为心理健康AI诊断工具提供了实用指导。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [95] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 论文提出了一种无需手工特征的消息传递图神经网络框架MIND，用于解决网络拆解问题，并在大规模真实网络上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，增加了计算成本并引入偏差，需要一种更纯粹的数据驱动方法。

Method: 引入注意力机制和消息迭代配置文件，利用多样化合成网络训练集构建高效框架。

Result: MIND在大规模真实网络上优于现有方法，具有高效性和泛化能力。

Conclusion: MIND框架不仅适用于网络拆解，还可推广到其他复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [96] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 论文提出了一种基于分解状态空间表示的方法，用于解决和学习鲁棒马尔可夫决策过程（r-MDPs），通过利用系统组件间的不确定性独立性，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: r-MDPs通过显式建模转移动态的认知不确定性来扩展MDPs，但学习r-MDPs需要大量样本交互。本文旨在通过分解状态空间表示减少样本需求。

Method: 提出基于分解状态空间表示的方法，将非凸优化问题转化为可处理的线性规划问题，并直接学习分解模型表示。

Result: 实验结果表明，利用分解结构可显著提高样本效率，生成比现有方法更有效的鲁棒策略和更严格的性能保证。

Conclusion: 分解状态空间表示是解决和学习r-MDPs的有效方法，能够显著提升样本效率和策略性能。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [97] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: JSON-Bag模型通过JSON描述的游戏轨迹进行标记化，并使用Jensen-Shannon距离（JSD）作为度量标准，结合原型最近邻搜索（P-NNS）进行评估。在六款桌游的分类任务中表现优于基线方法，且样本效率高。


<details>
  <summary>Details</summary>
Motivation: 研究如何通用地表示游戏轨迹，并评估其在分类任务中的有效性。

Method: 使用JSON-Bag模型标记化游戏轨迹的JSON描述，结合JSD和P-NNS进行分类任务评估。

Result: 在多数任务中优于基线方法，样本效率高，且能通过自动特征提取提升准确性。

Conclusion: JSON-Bag模型能有效表示游戏轨迹，JSD与策略距离高度相关。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [98] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR提出了一种针对带噪声标签的图域自适应框架，通过双分支预训练和嵌套伪标签细化机制提升跨域学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法假设源标签干净，但实际场景中标签噪声普遍存在，严重影响性能。

Method: NeGPR采用双分支预训练（语义和拓扑分支）和嵌套细化机制，结合噪声感知正则化策略。

Result: 实验表明NeGPR在严重标签噪声下优于现有方法，准确率提升高达12.7%。

Conclusion: NeGPR通过噪声鲁棒性设计，显著提升了带噪声标签的图域自适应性能。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [99] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI SDK是一个开源工具包，用于生成高质量的合成表格数据，解决数据隐私和访问问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习依赖高质量数据，但隐私、专有利益和伦理问题限制了数据访问，合成数据成为解决方案。

Method: 基于TabularARGN自回归框架，集成差分隐私、公平性生成和自动化质量保证，支持多类型和复杂数据集。

Result: SDK在速度和可用性上表现优异，已快速部署为云服务和本地软件。

Conclusion: MOSTLY AI SDK实用性强，能有效解决数据瓶颈，推动数据民主化。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [100] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出一种多保真度分层采样方法，结合自适应机器学习元模型，高效估计小失效概率，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方差缩减技术在罕见事件分析中仍需大量模型评估，计算成本高，尤其在复杂非线性有限元模型中。

Method: 使用分层采样生成高保真数据集训练深度学习元模型，作为低保真模型，结合多保真蒙特卡罗框架估计失效概率。

Result: 应用于高层钢结构建筑，准确估计非线性响应超越概率曲线，计算成本显著降低。

Conclusion: 该方法在保证精度的同时大幅提升计算效率，适用于复杂系统的罕见事件分析。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [101] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出一种基于特征空间密度的单确定性模型方法，用于量化分布偏移和OOD检测，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法计算和存储成本高，需更高效的不确定性量化方法。

Method: 利用核密度估计的信息势场近似训练集特征空间密度，通过比较测试样本特征判断分布偏移。

Result: 在2D合成数据集和OOD检测任务中表现优于基线模型。

Conclusion: 单确定性模型方法高效且有效，适用于分布偏移和OOD检测。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [102] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型和去噪自编码器的新框架DDAE，用于表格数据异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测面临特征交互复杂和异常样本稀缺的挑战，现有方法在噪声适应性和重建映射上存在不足。

Method: 提出DDAE框架，整合扩散模型的噪声调度和对比学习到编码过程，优化异常检测。

Result: 在ADBench的57个数据集上，DDAE在半监督和无监督设置中均表现优异，PR-AUC和ROC-AUC显著提升。

Conclusion: 研究强调了噪声策略在表格异常检测中的重要性，DDAE为未来工作提供了新方向。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [103] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 论文研究了量子机器学习中变分量子电路（VQC）的性能，比较了振幅编码和角度编码模型在不同旋转门下的分类表现，发现编码方式对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的结合（QML）受到广泛关注，但量子电路的设计对模型性能的影响尚不明确，因此需要研究不同编码方式和旋转门的效果。

Method: 使用振幅编码和角度编码的VQC模型，在不同旋转门配置下训练，并在Wine和Diabetes数据集上评估分类性能。

Result: 相同拓扑结构下，最佳和最差模型的准确率差异为10%至30%，最高达41%，编码方式的选择显著影响分类性能。

Conclusion: 编码方式是VQC模型的重要超参数，旋转门的选择对模型性能有显著影响。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [104] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 研究探讨了影响学生CGPA的多变量因素，通过文献综述、问卷调查和因果分析，开发了预测模型和分类模型，并构建了一个基于Web的应用帮助学生优化学术表现。


<details>
  <summary>Details</summary>
Motivation: 学术表现受多种社会、学术和财务因素影响，研究旨在识别这些因素并开发策略以优化学生CGPA。

Method: 通过文献综述构建因果图，进行在线问卷调查（1050名学生），数据预处理后使用回归和分类模型（如岭回归和随机森林）分析，并结合可解释AI技术（SHAP、LIME）。

Result: 岭回归预测CGPA的MAE为0.12，MSE为0.023；随机森林分类准确率达98.68%。关键因素包括学习时间、奖学金、父母教育背景和先前学术表现。

Conclusion: 研究成功开发了一个Web应用，为学生提供个性化建议，帮助他们预测和改进学术表现。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [105] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: GEPAR3D是一种结合实例检测和多类分割的新方法，显著提高了CBCT中牙齿分割的精度，尤其在根尖分割上表现优异。


<details>
  <summary>Details</summary>
Motivation: CBCT中的牙齿分割，尤其是根尖等精细结构的分割，对正畸中的根吸收评估至关重要，但现有方法存在挑战。

Method: GEPAR3D将统计形状模型作为几何先验，结合深度分水岭方法，将每个牙齿建模为3D能量盆地，实现实例感知分割。

Result: 在多个测试集上，GEPAR3D的平均Dice相似系数达95.0%，召回率提升9.5%，显著优于其他方法。

Conclusion: GEPAR3D在根分割质量上大幅提升，有望为临床决策提供更准确的根吸收评估，并已开源实现和数据集。

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [106] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug是一个新颖的插件框架，通过增强基础流匹配（FM）先验来解决不适定逆问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域特定或无训练先验，FMPlug利用观察与目标对象的相似性及生成流的高斯性。

Method: 引入时间自适应预热策略和锐利高斯性正则化，充分发挥领域无关基础模型的潜力。

Result: 在图像超分辨率和高斯去模糊任务中显著优于现有方法。

Conclusion: FMPlug通过简单而强大的洞察力，为不适定逆问题提供了高效解决方案。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [107] [Are controlled unitaries helpful?](https://arxiv.org/abs/2508.00055)
*Ewin Tang,John Wright*

Main category: quant-ph

TL;DR: 论文证明了在量子计算中，控制门（cU）对许多问题并无帮助，并提出了一种“去控制”方法，将使用cU的电路转换为仅使用U的电路。


<details>
  <summary>Details</summary>
Motivation: 针对量子算法中控制门（cU）的作用进行深入研究，反驳了文献中认为去控制不可行的负面结论。

Method: 提出了一种“去控制”技术，将使用cU和cU†的电路转换为仅使用U和U†的电路，输出带有随机相位的结果。

Result: 证明了cU仅在提供U的全局相位信息时有用，且去控制后的电路在忽略全局相位时足够。

Conclusion: 去控制技术是可行的，并可用于证明在特定条件下存在伪随机酉矩阵集合。

Abstract: Many quantum algorithms, to compute some property of a unitary $U$, require
access not just to $U$, but to $cU$, the unitary with a control qubit. We show
that having access to $cU$ does not help for a large class of quantum problems.
For a quantum circuit which uses $cU$ and $cU^\dagger$ and outputs
$|\psi(U)\rangle$, we show how to ``decontrol'' the circuit into one which uses
only $U$ and $U^\dagger$ and outputs $|\psi(\varphi U)\rangle$ for a uniformly
random phase $\varphi$, with a small amount of time and space overhead. When we
only care about the output state up to a global phase on $U$, then the
decontrolled circuit suffices. Stated differently, $cU$ is only helpful because
it contains global phase information about $U$.
  A version of our procedure is described in an appendix of Sheridan, Maslov,
and Mosca [SMM09]. Our goal with this work is to popularize this result by
generalizing it and investigating its implications, in order to counter
negative results in the literature which might lead one to believe that
decontrolling is not possible. As an application, we give a simple proof for
the existence of unitary ensembles which are pseudorandom under access to $U$,
$U^\dagger$, $cU$, and $cU^\dagger$.

</details>


### [108] [Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning](https://arxiv.org/abs/2508.00024)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mario Bifulco,Carlos Andrés Durán,Cristian Bosch,Ricardo Simón Carbajo*

Main category: quant-ph

TL;DR: 论文提出了一种结合类平衡k-means蒸馏和预训练Vision Transformer嵌入的量子-经典混合方法，解决了量子支持向量机的扩展性问题，并在Fashion-MNIST和MNIST数据集上实现了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 量子支持向量机因高维量子态和硬件限制面临扩展性挑战，需要一种更高效的方法。

Method: 采用嵌入感知的量子-经典混合流程，结合类平衡k-means蒸馏和预训练Vision Transformer嵌入。

Result: 在Fashion-MNIST和MNIST数据集上，ViT嵌入实现了8.02%和4.42%的准确率提升，而CNN特征表现下降。

Conclusion: 研究表明量子核优势高度依赖嵌入选择，为可扩展的量子机器学习提供了实用路径。

Abstract: Quantum Support Vector Machines face scalability challenges due to
high-dimensional quantum states and hardware limitations. We propose an
embedding-aware quantum-classical pipeline combining class-balanced k-means
distillation with pretrained Vision Transformer embeddings. Our key finding:
ViT embeddings uniquely enable quantum advantage, achieving up to 8.02%
accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,
while CNN features show performance degradation. Using 16-qubit tensor network
simulation via cuTensorNet, we provide the first systematic evidence that
quantum kernel advantage depends critically on embedding choice, revealing
fundamental synergy between transformer attention and quantum feature spaces.
This provides a practical pathway for scalable quantum machine learning that
leverages modern neural architectures.

</details>


### [109] [Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems](https://arxiv.org/abs/2508.00027)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi,Yongli Ren,Jiayang Niu*

Main category: quant-ph

TL;DR: 提出了一种三阶段混合机器学习算法，通过压缩标签配置文件、在固定量子比特预算下优化特征选择，并使用仅需5个量子比特的量子半随机森林（QsRF）进行推荐评分，性能接近现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统使用大量稀疏语义标签描述物品，但传统量子方法需要过多量子比特，超出当前NISQ设备的处理能力且易受误差影响。

Method: 1. 使用SVD和k-means压缩标签配置文件；2. 通过深度3的QAOA优化特征选择；3. 训练100棵树的QsRF进行推荐评分。

Result: 在ICM-150/500数据集上，该方法与全特征基线性能相当。

Conclusion: 该方法在仅需5个量子比特的情况下，实现了与现有最优方法相近的性能，适用于当前NISQ设备。

Abstract: Modern recommenders describe each item with hundreds of sparse semantic tags,
yet most quantum pipelines still map one qubit per tag, demanding well beyond
one hundred qubits, far out of reach for current noisy-intermediate-scale
quantum (NISQ) devices and prone to deep, error-amplifying circuits. We close
this gap with a three-stage hybrid machine learning algorithm that compresses
tag profiles, optimizes feature selection under a fixed qubit budget via QAOA,
and scores recommendations with a Quantum semi-Random Forest (QsRF) built on
just five qubits, while performing similarly to the state-of-the-art methods.
Leveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \%
variance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A
100-tree QsRF trained on these codes matches full-feature baselines on
ICM-150/500.

</details>


### [110] [Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins](https://arxiv.org/abs/2508.00029)
*Azadeh Alavi,Sanduni Jayasinghe,Mojtaba Mahmoodian,Sam Mazaheri,John Thangarajah,Sujeeva Setunge*

Main category: quant-ph

TL;DR: 提出了一种混合量子经典多层感知器（QMLP）框架，用于解决大规模结构健康监测中的计算复杂性问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 大规模民用结构（如桥梁、管道等）的意外故障可能带来重大经济和安全隐患，现有有限元建模方法计算成本高且复杂，亟需更高效的解决方案。

Method: 采用对称正定矩阵和多项式特征嵌入传感器数据，结合参数化量子电路和经典神经网络，实现高效的逆有限元映射。

Result: 在桥梁实验中，QMLP的均方误差（MSE）为0.0000000000316，显著优于纯经典方法。

Conclusion: 量子增强方法为实时结构健康监测提供了高效、可扩展的解决方案，推动了数字孪生技术的发展。

Abstract: Large-scale civil structures, such as bridges, pipelines, and offshore
platforms, are vital to modern infrastructure, where unexpected failures can
cause significant economic and safety repercussions. Although finite element
(FE) modeling is widely used for real-time structural health monitoring (SHM),
its high computational cost and the complexity of inverse FE analysis, where
low dimensional sensor data must map onto high-dimensional displacement or
stress fields pose ongoing challenges. Here, we propose a hybrid quantum
classical multilayer perceptron (QMLP) framework to tackle these issues and
facilitate swift updates to digital twins across a range of structural
applications.
  Our approach embeds sensor data using symmetric positive definite (SPD)
matrices and polynomial features, yielding a representation well suited to
quantum processing. A parameterized quantum circuit (PQC) transforms these
features, and the resultant quantum outputs feed into a classical neural
network for final inference. By fusing quantum capabilities with classical
modeling, the QMLP handles large scale inverse FE mapping while preserving
computational viability.
  Through extensive experiments on a bridge, we demonstrate that the QMLP
achieves a mean squared error (MSE) of 0.0000000000316, outperforming purely
classical baselines with a large margin. These findings confirm the potential
of quantum-enhanced methods for real time SHM, establishing a pathway toward
more efficient, scalable digital twins that can robustly monitor and diagnose
structural integrity in near real time.

</details>


### [111] [Dimension reduction with structure-aware quantum circuits for hybrid machine learning](https://arxiv.org/abs/2508.00048)
*Ammar Daskin*

Main category: quant-ph

TL;DR: 论文提出了一种基于Schmidt分解和量子电路的混合机器学习模型，用于数据压缩和降噪。


<details>
  <summary>Details</summary>
Motivation: 通过量子电路实现数据的高效压缩，减少大规模模型的可训练参数数量。

Method: 结合Schmidt分解和量子电路，生成k秩近似表示，并与经典神经网络结合。

Result: 实验验证了量子电路能有效压缩数据并提供k秩近似。

Conclusion: 该方法为大规模模型训练提供了高效的压缩和降噪方案。

Abstract: Schmidt decomposition of a vector can be understood as writing the singular
value decomposition (SVD) in vector form. A vector can be written as a linear
combination of tensor product of two dimensional vectors by recursively
applying Schmidt decompositions via SVD to all subsystems. Given a vector
expressed as a linear combination of tensor products, using only the $k$
principal terms yields a $k$-rank approximation of the vector. Therefore,
writing a vector in this reduced form allows to retain most important parts of
the vector while removing small noises from it, analogous to SVD-based
denoising.
  In this paper, we show that quantum circuits designed based on a value $k$
(determined from the tensor network decomposition of the mean vector of the
training sample) can approximate the reduced-form representations of entire
datasets. We then employ this circuit ansatz with a classical neural network
head to construct a hybrid machine learning model. Since the output of the
quantum circuit for an $2^n$ dimensional vector is an $n$ dimensional
probability vector, this provides an exponential compression of the input and
potentially can reduce the number of learnable parameters for training
large-scale models. We use datasets provided in the Python scikit-learn module
for the experiments. The results confirm the quantum circuit is able to
compress data successfully to provide effective $k$-rank approximations to the
classical processing component.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [112] [Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers](https://arxiv.org/abs/2508.00419)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.LO

TL;DR: 论文探讨了利用大型语言模型（LLM）结合Z3求解器自动合成循环不变式的方法，在Code2Inv基准测试中实现了100%覆盖率，优于之前的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 循环不变式对程序验证至关重要，但自动合成仍具挑战性。现有方法仅适用于部分基准测试，因此研究LLM是否能提升性能。

Method: 将OpenAI的LLM（O1、O1-mini、O3-mini）与Z3求解器结合，通过生成-检查迭代优化不变式。

Result: 在133个任务中实现100%覆盖率，仅需1-2次模型提议和14-55秒时间，显著优于之前的最佳结果（107/133）。

Conclusion: LLM具有潜在逻辑推理能力，可推广至其他命令式语言，为循环不变式合成提供新思路。

Abstract: Loop invariants are essential for proving the correctness of programs with
loops. Developing loop invariants is challenging, and fully automatic synthesis
cannot be guaranteed for arbitrary programs. Some approaches have been proposed
to synthesize loop invariants using symbolic techniques and more recently using
neural approaches. These approaches are able to correctly synthesize loop
invariants only for subsets of standard benchmarks. In this work, we
investigate whether modern, reasoning-optimized large language models can do
better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled
generate-and-check pipeline with the Z3 SMT solver, using solver
counterexamples to iteratively guide invariant refinement. We use Code2Inv
benchmark, which provides C programs along with their formal preconditions and
postconditions. On this benchmark of 133 tasks, our framework achieves 100%
coverage (133 out of 133), outperforming the previous best of 107 out of 133,
while requiring only 1-2 model proposals per instance and 14-55 seconds of
wall-clock time. These results demonstrate that LLMs possess latent logical
reasoning capabilities which can help automate loop invariant synthesis. While
our experiments target C-specific programs, this approach should be
generalizable to other imperative languages.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [113] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: 该论文探讨了使用LLMs（如GPT 4oMini、GPT 4.1mini等）自动生成Python类型注释的方法，通过生成-检查-修复流程，验证了其在一致性和准确性上的表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: Python类型注释能提升代码可维护性和错误检测，但手动生成易出错且耗时。传统自动化方法存在词汇限制、行为过度近似和依赖大量标注数据等问题。

Method: 采用生成-检查-修复流程：LLM基于语法树生成注释，静态类型检查器（Mypy）验证，错误反馈迭代优化。评估了四种LLM变体在6000个代码片段上的表现。

Result: GPT 4.1mini和O3Mini表现最佳，一致性达88.6%，精确匹配和基础类型准确率分别为70.5%和79.1%，平均修复迭代次数低于1次。

Conclusion: 通用和推理优化的LLMs无需额外训练即可高效生成一致的类型注释，性能与传统深度学习方法相当，且可扩展至其他可选类型语言。

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [114] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 论文研究了去中心化联邦学习中的安全聚合问题，从信息论角度分析了通信和密钥使用的最优界限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在协议设计和计算保证上，缺乏对信息论极限的理解，尤其是在去中心化设置中。

Method: 考虑了K个完全连接的用户网络，每个用户持有私有输入，目标是安全计算所有输入的和。

Result: 确定了最优速率区域，表明每个用户必须传输至少一个符号、持有至少一个密钥符号，且所有用户需持有不少于K-1个独立密钥符号。

Conclusion: 结果为分布式学习系统中设计安全且通信高效的协议提供了理论基础。

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [115] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: 本文首次将扩散模型应用于生成LHC质子-质子碰撞事件的喷注图像，比较了基于分数的扩散模型和一致性模型的性能，发现后者在生成质量和稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用扩散模型生成高能物理实验中的喷注图像，以改进计算效率和生成准确性。

Method: 将喷注的动力学变量映射为二维图像，训练扩散模型学习喷注成分的空间分布，并比较分数扩散模型和一致性模型的性能。

Result: 一致性模型在生成质量和稳定性上优于分数扩散模型，FID评分更高。

Conclusion: 该方法为高能物理研究提供了更高效和准确的工具，一致性模型表现出显著优势。

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [116] [Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method](https://arxiv.org/abs/2508.00101)
*Alena Kopaničáková,Youngkyu Lee,George Em Karniadakis*

Main category: math.NA

TL;DR: 提出一种基于DeepONet的放气策略，加速PCG方法求解大规模线性系统的收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统放气技术依赖特征向量近似或Krylov子空间，而新方法利用算子学习生成放气子空间，提升效率。

Method: 通过DeepONet学习基函数或直接预测解，构建放气算子，并提出稀疏模式策略。

Result: 数值实验表明，该方法在多种问题和参数下有效且泛化性强。

Conclusion: DeepONet放气PCG方法在加速收敛和泛化性方面表现优异。

Abstract: We propose a new deflation strategy to accelerate the convergence of the
preconditioned conjugate gradient(PCG) method for solving parametric
large-scale linear systems of equations. Unlike traditional deflation
techniques that rely on eigenvector approximations or recycled Krylov
subspaces, we generate the deflation subspaces using operator learning,
specifically the Deep Operator Network~(DeepONet). To this aim, we introduce
two complementary approaches for assembling the deflation operators. The first
approach approximates near-null space vectors of the discrete PDE operator
using the basis functions learned by the DeepONet. The second approach directly
leverages solutions predicted by the DeepONet. To further enhance convergence,
we also propose several strategies for prescribing the sparsity pattern of the
deflation operator. A comprehensive set of numerical experiments encompassing
steady-state, time-dependent, scalar, and vector-valued problems posed on both
structured and unstructured geometries is presented and demonstrates the
effectiveness of the proposed DeepONet-based deflated PCG method, as well as
its generalization across a wide range of model parameters and problem
resolutions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [117] [Data-Driven Motion Planning for Uncertain Nonlinear Systems](https://arxiv.org/abs/2508.00154)
*Babak Esmaeili,Hamidreza Modares,Stefano Di Cairano*

Main category: eess.SY

TL;DR: 提出了一种基于数据驱动的非线性系统运动规划框架，通过构建重叠不变多面体序列实现安全路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖系统动力学模型，而本文方法仅需数据即可计算安全区域并设计状态反馈控制器。

Method: 围绕随机采样的路径点，算法识别凸可容许区域，并通过数据驱动的线性矩阵不等式问题学习多个椭球不变集及其局部状态反馈增益。这些椭球的凸包在多段仿射控制器下仍保持不变，并近似为多面体。通过验证连续凸包多面体的交集并引入中间节点确保平滑过渡。

Result: 仿真验证了该方法在复杂非线性系统中实现安全、动态可行路径的有效性。

Conclusion: 所提方法无需系统动力学模型，仅依赖数据即可实现安全运动规划。

Abstract: This paper proposes a data-driven motion-planning framework for nonlinear
systems that constructs a sequence of overlapping invariant polytopes. Around
each randomly sampled waypoint, the algorithm identifies a convex admissible
region and solves data-driven linear-matrix-inequality problems to learn
several ellipsoidal invariant sets together with their local state-feedback
gains. The convex hull of these ellipsoids, still invariant under a
piece-wise-affine controller obtained by interpolating the gains, is then
approximated by a polytope. Safe transitions between nodes are ensured by
verifying the intersection of consecutive convex-hull polytopes and introducing
an intermediate node for a smooth transition. Control gains are interpolated in
real time via simplex-based interpolation, keeping the state inside the
invariant polytopes throughout the motion. Unlike traditional approaches that
rely on system dynamics models, our method requires only data to compute safe
regions and design state-feedback controllers. The approach is validated
through simulations, demonstrating the effectiveness of the proposed method in
achieving safe, dynamically feasible paths for complex nonlinear systems.

</details>


### [118] [Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms](https://arxiv.org/abs/2508.00775)
*Andrea Martin,Ian R. Manchester,Luca Furieri*

Main category: eess.SY

TL;DR: 论文提出了一种方法，在保持最坏情况收敛性的同时，提升算法在特定问题上的平均性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险工程应用中，优化算法需具备最坏情况保证，但通常牺牲了实际常见问题的性能。本文旨在解决这一问题。

Method: 从基线线性收敛算法出发，推导出所有且仅能保持其收敛性的修改规则。

Result: 方法适用于多种算法（如梯度下降、Nesterov加速方法等），并在线性方程组和MPC应用中验证了有效性。

Conclusion: 该方法能在保留最坏情况保证的同时，显著提升特定问题的性能。

Abstract: In high-stakes engineering applications, optimization algorithms must come
with provable worst-case guarantees over a mathematically defined class of
problems. Designing for the worst case, however, inevitably sacrifices
performance on the specific problem instances that often occur in practice. We
address the problem of augmenting a given linearly convergent algorithm to
improve its average-case performance on a restricted set of target problems -
for example, tailoring an off-the-shelf solver for model predictive control
(MPC) for an application to a specific dynamical system - while preserving its
worst-case guarantees across the entire problem class. Toward this goal, we
characterize the class of algorithms that achieve linear convergence for
classes of nonsmooth composite optimization problems. In particular, starting
from a baseline linearly convergent algorithm, we derive all - and only - the
modifications to its update rule that maintain its convergence properties. Our
results apply to augmenting legacy algorithms such as gradient descent for
nonconvex, gradient-dominated functions; Nesterov's accelerated method for
strongly convex functions; and projected methods for optimization over
polyhedral feasibility sets. We showcase effectiveness of the approach on
solving optimization problems with tight iteration budgets in application to
ill-conditioned systems of linear equations and MPC for linear systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [119] [Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence](https://arxiv.org/abs/2508.00091)
*Chandler Smith,HanQin Cai,Abiy Tasissa*

Main category: math.OC

TL;DR: 本文提出了一种基于黎曼优化的框架，通过将欧几里得距离几何问题转化为半正定Gram矩阵的低秩矩阵补全任务，解决了部分成对距离恢复的问题。


<details>
  <summary>Details</summary>
Motivation: 欧几里得距离几何问题在传感器网络定位、分子构象和流形学习等领域有广泛应用，但现有方法存在局限性。

Method: 采用黎曼优化框架，将问题转化为Gram矩阵的低秩补全任务，利用非正交基的展开系数编码距离测量，并通过黎曼梯度下降实现局部线性收敛。

Result: 在伯努利采样模型下，证明了算法在特定采样概率下能实现局部线性收敛，并通过实验验证了其性能优于现有方法。

Conclusion: 该方法不仅提供了理论保证，还通过实验证明了其在实际应用中的有效性，为欧几里得距离几何问题提供了新的解决方案。

Abstract: The problem of recovering a configuration of points from partial pairwise
distances, referred to as the Euclidean Distance Geometry (EDG) problem, arises
in a broad range of applications, including sensor network localization,
molecular conformation, and manifold learning. In this paper, we propose a
Riemannian optimization framework for solving the EDG problem by formulating it
as a low-rank matrix completion task over the space of positive semi-definite
Gram matrices. The available distance measurements are encoded as expansion
coefficients in a non-orthogonal basis, and optimization over the Gram matrix
implicitly enforces geometric consistency through the triangle inequality, a
structure inherited from classical multidimensional scaling. Under a Bernoulli
sampling model for observed distances, we prove that Riemannian gradient
descent on the manifold of rank-$r$ matrices locally converges linearly with
high probability when the sampling probability satisfies $p \geq
\mathcal{O}(\nu^2 r^2 \log(n)/n)$, where $\nu$ is an EDG-specific incoherence
parameter. Furthermore, we provide an initialization candidate using a one-step
hard thresholding procedure that yields convergence, provided the sampling
probability satisfies $p \geq \mathcal{O}(\nu r^{3/2} \log^{3/4}(n)/n^{1/4})$.
A key technical contribution of this work is the analysis of a symmetric linear
operator arising from a dual basis expansion in the non-orthogonal basis, which
requires a novel application of the Hanson--Wright inequality to establish an
optimal restricted isometry property in the presence of coupled terms.
Empirical evaluations on synthetic data demonstrate that our algorithm achieves
competitive performance relative to state-of-the-art methods. Moreover, we
propose a novel notion of matrix incoherence tailored to the EDG setting and
provide robustness guarantees for our method.

</details>


### [120] [Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks](https://arxiv.org/abs/2508.00267)
*Molly Noel,Gabriel Mancino-Ball,Yangyang Xu*

Main category: math.OC

TL;DR: 本文提出了一种基于邻居采样（NS）的Adam型随机方法，用于解决非凸GCN训练问题，并通过控制变量技术减少采样误差，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: GCN训练方法缺乏理论保证或现代深度学习算法（如自适应性和动量）的关键实践元素，因此需要改进。

Method: 采用邻居采样（NS）结合Adam型随机方法，并利用控制变量技术减少采样误差。

Result: 在标准假设下，方法具有最优收敛率，实验显示其在大规模图数据集上优于传统NS-SGD方法。

Conclusion: 提出的方法在理论和实践中均表现出色，适用于大规模GCN训练。

Abstract: Graph convolutional networks (GCNs) are a powerful tool for graph
representation learning. Due to the recursive neighborhood aggregations
employed by GCNs, efficient training methods suffer from a lack of theoretical
guarantees or are missing important practical elements from modern deep
learning algorithms, such as adaptivity and momentum. In this paper, we present
several neighbor-sampling (NS) based Adam-type stochastic methods for solving a
nonconvex GCN training problem. We utilize the control variate technique
proposed by [1] to reduce the stochastic error caused by neighbor sampling.
Under standard assumptions for Adam-type methods, we show that our methods
enjoy the optimal convergence rate. In addition, we conduct extensive numerical
experiments on node classification tasks with several benchmark datasets. The
results demonstrate superior performance of our methods over classic NS-based
SGD that also uses the control-variate technique, especially for large-scale
graph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .

</details>


### [121] [Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process](https://arxiv.org/abs/2508.00816)
*Youssef Ait El Mahjoub,Jean-Michel Fourneau,Salma Alouah*

Main category: math.OC

TL;DR: 提出了一种基于单输入超状态可分解MDP（SISDMDP）的高效策略评估方法，适用于大规模状态空间和长期优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MDP中策略评估的计算复杂度问题，尤其是在无限时域设置下（如平均奖励或折扣奖励）。

Method: 结合Chiu的单输入分解和Robertazzi的单循环递归特性，定义SISDMDP，并开发基于此结构的精确策略评估方法。

Result: 该方法能够高效且精确地评估策略，适用于平均和折扣奖励MDP。

Conclusion: SISDMDP及其策略评估方法为解决大规模MDP问题提供了可扩展的解决方案。

Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in
sequential decision-making, especially when dealing with large state spaces and
long-term optimization criteria. A key step in Bellman dynamic programming
algorithms is the policy evaluation, which becomes computationally demanding in
infinite-horizon settings such as average-reward or discounted-reward
formulations. In the context of Markov chains, aggregation and disaggregation
techniques have for a long time been used to reduce complexity by exploiting
structural decompositions. In this work, we extend these principles to a
structured class of MDPs. We define the Single-Input Superstate Decomposable
Markov Decision Process (SISDMDP), which combines Chiu's single-input
decomposition with Robertazzi's single-cycle recurrence property. When a policy
induces this structure, the resulting transition graph can be decomposed into
interacting components with centralized recurrence. We develop an exact and
efficient policy evaluation method based on this structure. This yields a
scalable solution applicable to both average and discounted reward MDPs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [122] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: FedGuard是一种新型联邦学习机制，通过利用成员推断对模型偏差的高敏感性，有效识别并排除中毒模型，显著优于现有防御方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭攻击，尤其是当恶意客户端超过50%或数据高度非独立同分布时。现有防御机制通常针对特定攻击类型，效果有限。

Method: FedGuard要求客户端在训练中包含服务器指定的额外小批量数据，通过观察其置信度下降来识别中毒模型。

Result: 在90%客户端为拜占庭且数据高度非独立同分布的情况下，FedGuard显著优于现有方案，能抵御多种攻击类型。

Conclusion: FedGuard通过创新方法解决了联邦学习中的拜占庭攻击问题，具有广泛适用性和高效性。

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [123] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: 本文综述了动态二进制插桩（DBI）技术，比较了不同方法的优缺点，并评估了其性能，发现没有一种技术在所有情况下都最优。


<details>
  <summary>Details</summary>
Motivation: DBI技术在安全和多领域应用广泛，但现有方法各有局限，需系统比较其性能和适用性。

Method: 综合分析了进程级和全系统级的DBI方法，比较其插桩技术和性能。

Result: 结果表明，不同技术在不同场景下表现各异，没有绝对优势的方法。

Conclusion: DBI技术需根据具体需求选择，未来研究应关注优化和适应性。

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


### [124] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: 论文提出了一种基于证据深度学习（EDL）的分类方法，用于推断APT攻击阶段，并处理分布外输入的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统检测系统无法有效识别攻击阶段，而准确推断攻击阶段对定制响应策略至关重要。

Method: 采用EDL方法，通过输出Dirichlet分布参数来建模预测不确定性，从而推断攻击阶段并检测OOD输入。

Result: 模拟实验表明，该方法能准确推断攻击阶段并有效检测OOD输入，支持在动态环境中部署不确定性感知模型。

Conclusion: 不确定性感知模型在APT攻击阶段推断中具有可行性，适用于动态对抗环境。

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [125] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: 论文提出了一种分析LLM历史交互数据的方法和LeakSealer框架，用于检测和防御jailbreaking攻击与数据泄露。


<details>
  <summary>Details</summary>
Motivation: LLM的广泛应用带来了安全威胁，如jailbreaking和数据泄露，需要有效的防御机制。

Method: 1. 分析LLM历史交互数据生成使用地图；2. 提出LeakSealer框架，结合静态分析和动态防御。

Result: LeakSealer在ToxicChat数据集上表现最佳，PII泄露检测AUPRC达0.97。

Conclusion: LeakSealer能有效识别和防御LLM的安全威胁，优于现有基线。

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [126] [Constructive Disintegration and Conditional Modes](https://arxiv.org/abs/2508.00617)
*Nathaël Da Costa,Marvin Pförtner,Jon Cockayne*

Main category: math.ST

TL;DR: 论文探讨了贝叶斯统计中的条件化操作，通过测度分解理论形式化，并提供了构建分解的数学工具。研究发现限制密度与分解密度存在显著差异，并讨论了条件模式与分解测度模式的不一致性及其实际影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决测度分解的构建困难问题，以及在近似贝叶斯推断和贝叶斯逆问题中的应用需求。

Method: 方法包括提供数学工具构建测度分解，并在可微流形上应用这些工具找到分解密度。

Result: 结果显示限制密度与分解密度存在显著差异，且条件模式与分解测度模式不一致。

Conclusion: 结论强调两种方法（限制密度与分解密度）在不同建模场景下的实用性，并呼吁根据具体需求选择合适方法。

Abstract: Conditioning, the central operation in Bayesian statistics, is formalised by
the notion of disintegration of measures. However, due to the implicit nature
of their definition, constructing disintegrations is often difficult. A
folklore result in machine learning conflates the construction of a
disintegration with the restriction of probability density functions onto the
subset of events that are consistent with a given observation. We provide a
comprehensive set of mathematical tools which can be used to construct
disintegrations and apply these to find densities of disintegrations on
differentiable manifolds. Using our results, we provide a disturbingly simple
example in which the restricted density and the disintegration density
drastically disagree. Motivated by applications in approximate Bayesian
inference and Bayesian inverse problems, we further study the modes of
disintegrations. We show that the recently introduced notion of a "conditional
mode" does not coincide in general with the modes of the conditional measure
obtained through disintegration, but rather the modes of the restricted
measure. We also discuss the implications of the discrepancy between the two
measures in practice, advocating for the utility of both approaches depending
on the modelling context.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [127] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: 研究发现AI工具在软件开发中广泛采用，提升生产力，但对协作影响有限，团队文化和支持结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对开发者生产力和体验的真实影响。

Method: 混合方法研究，包括500多名开发者的调查及访谈和观察。

Result: AI提升效率和满意度，但对协作影响较小，团队支持是关键。

Conclusion: AI是开发者的辅助工具，团队文化和支持结构对有效整合至关重要。

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [128] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: MetaExplainer是一个神经符号框架，通过三阶段过程生成用户为中心的AI解释，提升AI系统的可解释性和信任度。


<details>
  <summary>Details</summary>
Motivation: 解决模型提供的解释与用户需求之间的差距，提升AI系统的可解释性和信任度。

Method: 采用三阶段过程：1) 用LLM分解用户问题；2) 用模型解释方法生成系统推荐；3) 合成自然语言解释。使用Explanation Ontology指导语言模型和解释方法。

Result: 在问题重构、模型解释忠实度和自然语言合成中表现优异（F1-score 59.06%，忠实度70%，上下文利用率67%）。用户研究支持其创造性和全面性。

Conclusion: MetaExplainer是一个多功能且可追溯的工具，适用于多种解释类型和领域，有望提升AI的可解释性。

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [129] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 论文探讨了表格在LLMs和MLLMs中的重要性，提出了表格输入表示的分类法，并指出了当前研究的不足。


<details>
  <summary>Details</summary>
Motivation: 表格因其复杂和灵活的结构在LLMs和MLLMs中受到关注，但缺乏通用方法，研究领域存在挑战。

Method: 通过分类法总结表格输入表示，并介绍表格理解任务。

Result: 指出了三个关键研究缺口：任务偏重检索、模型处理复杂表格能力不足、模型泛化性有限。

Conclusion: 需要进一步研究以解决表格理解中的挑战。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [130] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: NusaAksara是一个针对印尼语言及其原生脚本的公共基准测试，涵盖文本和图像模态，包含多种任务，如OCR和翻译。数据由专家构建，覆盖8种脚本和7种语言，包括低资源语言。测试显示现有NLP技术对本地脚本处理能力有限。


<details>
  <summary>Details</summary>
Motivation: 印尼语言和脚本丰富，但NLP研究多基于罗马化文本。NusaAksara旨在填补原生脚本研究的空白。

Method: 构建包含8种脚本和7种语言的基准数据集，涵盖多种任务，并通过专家验证。测试了多种模型（如GPT-4o、Llama 3.2）的性能。

Result: 现有NLP技术对印尼本地脚本处理能力极低，许多任务表现接近零。

Conclusion: NusaAksara为印尼原生脚本研究提供了重要基准，凸显了现有技术的不足。

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [131] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 论文系统评估了LLM优化技术（如剪枝、量化和token丢弃）在长上下文场景中的效果，发现这些方法的组合可能对大模型产生负面影响，并揭示了F1分数的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在资源需求和上下文窗口限制方面的挑战，探索优化技术的实际效果及其组合的影响。

Method: 系统性地对两种支持长上下文的LLM架构进行优化方法分析，并评估组合技术的性能，随后在70B参数模型上研究扩展性。

Result: 发现优化方法的组合可能对大模型产生负面影响，F1分数掩盖了问答任务中的精度-召回权衡。

Conclusion: 通过系统级分析和任务特定洞察，帮助研究者在效率、准确性和扩展性之间找到平衡。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [132] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct通过分段监督微调（S-SFT）和细粒度量化优化，解决了Transformer模型在边缘设备上部署时的计算和内存问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署大型语言模型时，自注意力的二次时间复杂度和KV缓存需求导致效率低下，现有优化方法无法兼顾TTFT和性能。

Method: 提出EdgeInfinite-Instruct，采用S-SFT策略针对长序列任务（如摘要和问答）进行微调，并通过细粒度PTQ和固定形状计算图优化部署。

Result: 实验表明，该方法在长上下文基准测试和实际移动任务中提升了性能，同时保持了边缘NPU设备的效率。

Conclusion: EdgeInfinite-Instruct在保持模型质量的同时，显著降低了计算和内存成本，适合边缘设备部署。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [133] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文研究了上下文学习（ICL）中演示无效的原因，提出基于梯度流的演示选择方法GradS，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设ICL中的演示均有效，但实际并非如此，因此探究演示无效的原因并提出改进方法。

Method: 通过梯度流和线性自注意力模型分析，提出GradS方法，利用梯度流幅度选择有效演示。

Result: 实验表明，模型层数增加会放大演示有效性差异，GradS平均提升6.8%性能。

Conclusion: GradS通过梯度流选择演示，显著提升ICL性能，验证了演示无效原因的理论推导。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [134] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN提出了一种基于代理的框架，通过节点级自主决策和检索增强生成（RAG）解决GNN中节点信息不平衡和全局语义关系缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的固定聚合机制无法处理节点信息不平衡，且忽略了全局语义关系，限制了模型的性能。

Method: ReaGAN为每个节点赋予代理能力，使其能基于内部记忆自主规划行动，并通过RAG访问全局语义相关内容。

Result: ReaGAN在少样本情境下表现出色，无需微调即可利用冻结的LLM骨干网络。

Conclusion: ReaGAN展示了代理规划和局部-全局检索在图学习中的潜力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [135] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 提出了一种名为PaPaformer的并行路径解码器架构，通过分路径训练和组合减少训练时间和参数量，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型训练耗时长且计算资源需求高，即使是小型模型也需要多GPU和数天时间。本文旨在探索快速训练和评估解码器架构的方法。

Method: 提出PaPaformer，一种并行路径解码器架构，分路径训练后组合成完整模型，减少参数量和训练时间。

Result: 该方法显著减少训练时间和参数量，同时性能有所提升，并支持路径定制以适应特定任务需求。

Conclusion: PaPaformer为高效训练语言模型提供了新思路，具有减少资源消耗和灵活定制的潜力。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [136] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX框架通过结合上下文忠实性和一致性，提出两种新指标来改进LLM的置信度估计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM置信度估计方法忽略了响应与上下文的相关性，而CRUX旨在填补这一空白。

Method: 提出CRUX框架，使用上下文熵减和统一一致性检查两种新指标。

Result: 在多个数据集上实验显示CRUX的AUROC最高。

Conclusion: CRUX有效提升了LLM置信度估计的准确性。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [137] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 论文研究了现有AI生成文本检测器在真实场景中的不足，提出了DACTYL数据集和两种分类器优化方法，发现DXO分类器在泛化性能上更优。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器在内部测试中表现良好，但在真实场景中效果不佳，缺乏对少样本和领域特定生成文本的检测能力。

Method: 提出DACTYL数据集，包含少样本和领域特定生成文本；比较了标准BCE优化和DXO优化两种分类器训练方法。

Result: DXO分类器在OOD文本上表现优于BCE分类器，泛化能力更强。

Conclusion: 研究揭示了AI生成文本检测器的改进空间，DXO优化方法在泛化性能上具有潜力。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [138] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了训练时和测试时的增强技术分类，并分析了其在临床应用中的表现与挑战。


<details>
  <summary>Details</summary>
Motivation: 医学实践中需要系统、透明且可验证的推理能力，而现有LLMs在此方面存在不足，因此推动了专门针对医学推理的LLMs研究。

Method: 通过分析60项研究（2022-2025），提出训练时（如监督微调）和测试时（如提示工程）的技术分类，并评估其在多模态数据及临床任务中的应用。

Result: 研究发现，评估标准从简单准确性转向推理质量和可视化解释性，但仍存在忠实性与合理性差距等问题。

Conclusion: 未来需发展高效、鲁棒且社会技术责任强的医学AI，解决多模态推理等挑战。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [139] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever是一种法律先例检索方法，通过提取修辞显著片段而非完整文档，结合BM25、向量数据库和交叉编码器模型，解决法律文档复杂性和数量增长的问题。


<details>
  <summary>Details</summary>
Motivation: 法律先例检索在普通法体系中至关重要，但传统方法难以应对日益复杂的法律文档数量和内容。

Method: 结合BM25、向量数据库和交叉编码器模型，通过层次化BiLSTM CRF分类器生成修辞标注，并采用互惠排名融合进行结果整合。

Result: 在IL-PCR和COLIEE 2025数据集上验证，TraceRetriever能有效应对文档数量增长，并在部分案例知识可用时提升检索效果。

Conclusion: TraceRetriever为法律先例检索提供了可靠且可扩展的解决方案，尤其适用于信息不完整的情况。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [140] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG是一个基于检索增强生成（RAG）的框架，用于预测印度法律判决，通过结合案件事实、法律条文和先例，显著提高了预测准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在印度法律背景下忽视了普通法系的核心元素（法律条文和先例），NyayaRAG旨在填补这一空白。

Method: 提出NyayaRAG框架，模拟真实法庭场景，结合案件描述、法律条文和语义检索的先例，使用领域特定管道评估输入配置。

Result: 实验表明，结合结构化法律知识显著提升了预测准确性和解释质量。

Conclusion: NyayaRAG为法律判决预测提供了更有效和可解释的方法，尤其在普通法系中表现突出。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [141] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 论文探讨了基于大型语言模型的生成代理在社会科学研究中的有效性，通过重现HEXACO人格测试实验，发现代理能部分对齐HEXACO框架，但存在模型特异性偏差。


<details>
  <summary>Details</summary>
Motivation: 验证基于角色的生成代理是否能有效代表人类群体，并评估其在社会科学研究中的潜力。

Method: 重现HEXACO人格测试实验，调查310个GPT-4驱动的代理，进行因子分析并与原始结果对比。

Result: 1）代理响应中可恢复出部分对齐HEXACO的人格结构；2）GPT-4生成的人格维度一致且可靠；3）跨模型分析显示人格分析存在模型特异性偏差。

Conclusion: 生成代理在社会科学研究中具有潜力，但需注意模型特异性和设计一致性以最大化人类人格特征的覆盖和代表性。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [142] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 提出了一种基于代理的检索增强生成（RAG）框架，用于提升放射学问答中大型语言模型（LLM）的诊断准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 传统单步检索的RAG系统在复杂临床推理任务中表现有限，需要更高效的解决方案。

Method: 采用代理RAG框架，使LLM能自主分解问题、迭代检索临床证据并动态合成回答，评估了24种不同架构和规模的LLM。

Result: 代理检索显著提升了诊断准确性（73% vs. 64%），减少了幻觉（9.4%），并在46%的案例中检索到相关临床背景。

Conclusion: 代理框架能有效提升放射学问答的事实性和准确性，尤其对中型LLM效果显著，值得进一步临床验证。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一种基于概率可达性分析的主动运行时安全框架，通过预测未来风险并提前干预，解决了现有规则系统缺乏前瞻性的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在自主能力强大的同时，其随机行为带来了难以预测的安全风险，现有规则系统反应滞后且无法处理长期依赖和分布变化。

Method: Pro2Guard将代理行为抽象为符号状态，从执行轨迹中学习离散时间马尔可夫链，运行时通过估计到达不安全状态的概率预测风险，并在风险超过阈值时触发干预。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%的早期安全干预和100%的交通违规及碰撞预测，风险预测时间最长可达38.66秒。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了代理行为的安全性，同时保持了任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [144] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），用于在满足安全性和不透明性约束的条件下学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在探索基于超属性的安全感知强化学习方面存在空白，尤其是在机器人应用中。

Method: 提出了一种结合动态Boltzmann softmax RL和HyperTWTL约束的方法，以Markov Decision Process（MDP）建模代理动态。

Result: 通过案例研究验证了方法的有效性和可扩展性，并优于两种基线RL算法。

Conclusion: 该方法为安全感知强化学习提供了一种新思路，尤其在机器人任务中表现出色。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [145] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨通过明确要求AI赋能人类并管理人类与AI之间的权力平衡，以促进安全与福祉。提出了一种参数化、可分解的目标函数，并通过算法计算或近似实现。


<details>
  <summary>Details</summary>
Motivation: 研究AI安全中的权力概念，旨在通过赋能人类和管理权力平衡，同时提升安全与福祉。

Method: 采用部分公理化方法设计目标函数，考虑人类有限理性与社会规范，并通过逆向归纳或多智能体强化学习实现。

Result: 在多种情境下验证目标函数的效果，表明其可能比直接基于效用的目标更安全。

Conclusion: 适度最大化人类权力的聚合指标可能是AI系统的有益目标，比直接效用目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [146] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部探索和外部数据，解决了RLVR方法的局限性，提升了LLM的推理能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: RLVR方法因固有的策略限制和稀疏奖励，难以突破基础LLM的能力边界，甚至导致能力边界崩溃。RL-PLUS旨在解决这一问题。

Method: RL-PLUS结合了多重重要性采样和基于探索的优势函数，以利用外部数据并引导模型探索高价值路径。

Result: RL-PLUS在六个数学推理基准测试中表现最优，并在分布外任务中显著优于现有方法，平均提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，显著提升了LLM的推理能力和泛化性能。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [147] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了多频带变滞后格兰杰因果关系（MB-VLGC）框架，解决了传统方法无法处理频率依赖性因果延迟的问题，并在多领域实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果关系方法假设固定的滞后时间，而变滞后方法（VLGC）虽允许滞后时间变化，但忽略了频率依赖性。本文旨在解决这一局限性。

Method: 提出MB-VLGC框架，通过显式建模频率依赖性因果延迟，扩展了VLGC方法，并设计了高效推断流程。

Result: 实验表明，MB-VLGC在合成和真实数据集上显著优于现有方法。

Conclusion: MB-VLGC框架具有广泛适用性，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [148] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种混合框架，结合传统XAI技术和生成式AI模型，为用户提供多模态、个性化的解释，以增强教育中AI系统的透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的自适应学习系统缺乏透明度，且XAI技术多关注技术输出而忽视用户角色和理解。本文旨在解决这一问题。

Method: 提出混合框架，整合传统XAI技术与生成式AI模型，结合用户个性化需求，生成动态、多模态的解释。

Result: 框架重新定义可解释性为动态沟通过程，并探讨了其在教育中的局限性及研究方向。

Conclusion: 目标是推动可解释AI的发展，提升透明度并支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [149] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的上下文感知解释系统，通过可视化方法满足不同用户对AI推荐的理解需求。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体的AI推荐缺乏针对用户特定需求的解释性，导致用户不理解推荐原因，从而降低了推荐的价值。

Method: 提出了一种视觉解释系统，结合用户需求和上下文，提供不同形式的解释（如技术详细版和简化版）。

Result: 系统首次在单一流程中同时调整解释风格（视觉vs.数字）和粒度（专家vs.普通用户）。

Conclusion: 通过30名X用户的公开试点验证系统对决策和信任的影响。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [150] [Asymptotically Optimal Inapproximability of E$k$-SAT Reconfiguration](https://arxiv.org/abs/2508.00276)
*Shuichi Hirahara,Naoto Ohsaka*

Main category: cs.CC

TL;DR: 本文研究了Maxmin E$k$-SAT Reconfiguration问题，提出了近似算法并证明了其近似因子的上下界。


<details>
  <summary>Details</summary>
Motivation: 研究在满足$k$-CNF公式的赋值转换过程中，如何最大化最小满足子句比例的问题。

Method: 开发了确定性近似算法，并证明了问题的PSPACE-hard近似下界。

Result: 证明了最优近似因子为$1 - \Theta\left(\frac{1}{k}\right)$，并提出了$(1-\frac{1}{k-1}-\frac{1}{k})$-因子算法。

Conclusion: 这是首个近似阈值比其NP类似问题更差的重配置问题，并引入了新的非单调测试方法。

Abstract: In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable
$k$-CNF formula $\varphi$ where each clause contains exactly $k$ literals,
along with a pair of its satisfying assignments. The objective is transform one
satisfying assignment into the other by repeatedly flipping the value of a
single variable, while maximizing the minimum fraction of satisfied clauses of
$\varphi$ throughout the transformation. In this paper, we demonstrate that the
optimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 -
\Theta\left(\frac{1}{k}\right)$. On the algorithmic side, we develop a
deterministic $\left(1-\frac{1}{k-1}-\frac{1}{k}\right)$-factor approximation
algorithm for every $k \geq 3$. On the hardness side, we show that it is
$\mathsf{PSPACE}$-hard to approximate this problem within a factor of
$1-\frac{1}{10k}$ for every sufficiently large $k$. Note that an
``$\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT,
whose approximation threshold is $1-\frac{1}{2^k}$ shown by H\r{a}stad (JACM
2001). To the best of our knowledge, this is the first reconfiguration problem
whose approximation threshold is (asymptotically) worse than that of its
$\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new
``non-monotone'' test, which is specially tailored to reconfiguration problems,
despite not being helpful in the PCP regime.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [151] [funOCLUST: Clustering Functional Data with Outliers](https://arxiv.org/abs/2508.00110)
*Katharine M. Clark,Paul D. McNicholas*

Main category: stat.ML

TL;DR: 提出了一种基于OCLUST算法的功能数据聚类方法，解决了高维和异常值问题。


<details>
  <summary>Details</summary>
Motivation: 功能数据的高维性和对异常值的敏感性给聚类带来了挑战。

Method: 扩展OCLUST算法，创建了一种鲁棒的功能数据聚类和异常值修剪方法。

Result: 在模拟和真实数据集上验证了方法的聚类和异常值识别性能。

Conclusion: 该方法在功能数据聚类和异常值处理中表现优异。

Abstract: Functional data present unique challenges for clustering due to their
infinite-dimensional nature and potential sensitivity to outliers. An extension
of the OCLUST algorithm to the functional setting is proposed to address these
issues. The approach leverages the OCLUST framework, creating a robust method
to cluster curves and trim outliers. The methodology is evaluated on both
simulated and real-world functional datasets, demonstrating strong performance
in clustering and outlier identification.

</details>


### [152] [Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.00247)
*Sergei Gleyzer,Hanh Nguyen,Dinesh P. Ramakrishnan,Eric A. F. Reinhardt*

Main category: stat.ML

TL;DR: 论文提出了一种基于加权正弦函数的Kolmogorov-Arnold网络（KAN）变体，替代传统多层感知机（MLP），在多项实验中表现优于固定频率傅里叶变换方法，与MLP性能相当。


<details>
  <summary>Details</summary>
Motivation: 探索KAN的替代方案，通过简化Kolmogorov-Arnold表示中的内外函数为加权正弦函数，以提升性能。

Method: 使用加权正弦函数替换KAN中的内外函数，固定相位为线性间隔常数，并进行理论验证和数值实验。

Result: 新方法在多项实验中优于固定频率傅里叶变换，与MLP性能相当。

Conclusion: 提出的KAN变体在理论和实验上均表现良好，为神经网络设计提供了新方向。

Abstract: The Kolmogorov-Arnold representation theorem states that any continuous
multivariable function can be exactly represented as a finite superposition of
continuous single variable functions. Subsequent simplifications of this
representation involve expressing these functions as parameterized sums of a
smaller number of unique monotonic functions. These developments led to the
proof of the universal approximation capabilities of multilayer perceptron
networks with sigmoidal activations, forming the alternative theoretical
direction of most modern neural networks.
  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an
alternative to multilayer perceptrons. KANs feature learnable nonlinear
activations applied directly to input values, modeled as weighted sums of basis
spline functions. This approach replaces the linear transformations and
sigmoidal post-activations used in traditional perceptrons. Subsequent works
have explored alternatives to spline-based activations. In this work, we
propose a novel KAN variant by replacing both the inner and outer functions in
the Kolmogorov-Arnold representation with weighted sinusoidal functions of
learnable frequencies. Inspired by simplifications introduced by Lorentz and
Sprecher, we fix the phases of the sinusoidal activations to linearly spaced
constant values and provide a proof of its theoretical validity. We also
conduct numerical experiments to evaluate its performance on a range of
multivariable functions, comparing it with fixed-frequency Fourier transform
methods and multilayer perceptrons (MLPs). We show that it outperforms the
fixed-frequency Fourier transform and achieves comparable performance to MLPs.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [153] [LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources](https://arxiv.org/abs/2508.00654)
*Rodrigo Escobar Díaz Guerrero,Jamile Mohammad Jafari,Tobias Meyer-Zedler,Michael Schmitt,Juergen Popp,Thomas Bocklitz*

Main category: cs.CE

TL;DR: LEO是一个基于网络的平台，旨在连接和管理分布式数据系统之间的链接，解决显微镜研究中数据整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 显微镜研究中，跨平台管理大量异构数据是一个主要挑战，缺乏有效工具整合这些数据以满足FAIR原则。

Method: 开发LEO平台，通过插件架构链接电子实验笔记本（ELN）和OMERO，并扩展支持其他数据源。

Result: LEO提供了一个可扩展且灵活的解决方案，适用于广泛的显微镜研究工作流程。

Conclusion: LEO填补了异构数据源整合工具的空白，支持FAIR原则，提升了数据管理的效率和可重用性。

Abstract: In the interdisciplinary field of microscopy research, managing and
integrating large volumes of data stored across disparate platforms remains a
major challenge. Data types such as bioimages, experimental records, and
spectral information are often maintained in separate repositories, each
following different management standards. However, linking these data sources
across the research lifecycle is essential to align with the FAIR principles of
data management: Findability, Accessibility, Interoperability, and Reusability.
Despite this need, there is a notable lack of tools capable of effectively
integrating and linking data from heterogeneous sources. To address this gap,
we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based
platform designed to create and manage links between distributed data systems.
LEO was initially developed to link objects between Electronic Lab Notebooks
(ELNs) and OMERO, but its functionality has since been extended through a
plugin-based architecture, allowing the integration of additional data sources.
This extensibility makes LEO a scalable and flexible solution for a wide range
of microscopy research workflows.

</details>


### [154] [Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models](https://arxiv.org/abs/2508.00804)
*Julian Lemmel,Manuel Kranzl,Adam Lamine,Philipp Neubauer,Radu Grosu,Sophie Neubauer*

Main category: cs.CE

TL;DR: 提出了一种基于实时循环学习的结构化状态空间模型（SSM）在线微调方法，能够在推理时动态更新模型参数。


<details>
  <summary>Details</summary>
Motivation: SSM通常离线训练且部署后静态运行，无法适应动态环境。本研究旨在实现SSM的在线自适应能力。

Method: 采用实时循环学习技术，在线更新线性循环单元SSM的参数。

Result: 在嵌入式汽车硬件采集的小型碳排放数据集上，该方法显著降低了推理时的预测误差。

Conclusion: 该方法适用于动态且资源受限的环境，展示了SSM在线适应的潜力。

Abstract: This paper introduces a new approach for fine-tuning the predictions of
structured state space models (SSMs) at inference time using real-time
recurrent learning. While SSMs are known for their efficiency and long-range
modeling capabilities, they are typically trained offline and remain static
during deployment. Our method enables online adaptation by continuously
updating model parameters in response to incoming data. We evaluate our
approach for linear-recurrent-unit SSMs using a small carbon emission dataset
collected from embedded automotive hardware. Experimental results show that our
method consistently reduces prediction error online during inference,
demonstrating its potential for dynamic, resource-constrained environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [155] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的模型，专注于利用眼周区域的颜色图像进行性别分类，并在两个数据集上取得了高准确率（99%和96%）。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安全、人机交互等领域至关重要，但化妆品和伪装等因素可能影响分类准确性。因此，研究专注于眼周区域以解决这一问题。

Method: 使用CNN模型分析眼周区域的颜色图像，提取关键特征进行性别分类，并在CVBL和（Female and Male）数据集上验证性能。

Result: 模型在CVBL数据集上达到99%的准确率，在（Female and Male）数据集上达到96%的准确率，且参数量较少（7,235,089）。

Conclusion: 该模型在性别分类中表现出色，具有在安全和监控等领域的实际应用潜力。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [156] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 论文提出了一种结构化图“谱系”的定义，用于构建层次化增长的图模型，支持高效的代数操作和类型构造，适用于机器学习和计算科学中的层次化架构设计。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为机器学习和计算科学中的层次化模型架构提供一种数学框架，支持高效的图操作和类型构造。

Method: 方法包括定义层次化增长的图谱系、引入双分图连接、延长映射、以及推导低成本的“骨架”代数操作和类型构造。

Result: 结果表明，这些骨架操作具有类似标准操作的代数性质，并能逼近连续极限对象，适用于深度神经网络和多网格数值方法。

Conclusion: 结论是该方法为层次化模型架构和局部算法提供了有效的数学工具，具有广泛的应用潜力。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [157] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx通过轻量级适配器Parallel-Text，将CLIP文本嵌入引入SAM，实现语义引导的分割，性能优于传统空间提示。


<details>
  <summary>Details</summary>
Motivation: 探索语义文本提示在SAM中的潜力，弥补传统空间提示的不足。

Method: 设计轻量级适配器Parallel-Text，将文本嵌入注入SAM的图像编码器，仅修改MLP并行分支。

Result: 在COD10K、COCO和ADE20K数据集上，性能优于纯空间提示基线。

Conclusion: 语义条件集成到SAM架构中，为高效适应提供了实用且可扩展的路径。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [158] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 论文提出在少样本图像分类中，利用对象的局部位置信息显著提升分类性能，并展示了通过Segment Anything Model或无监督前景提取方法实现改进。


<details>
  <summary>Details</summary>
Motivation: 少样本图像分类中，图像模糊性（如多对象或复杂背景）会显著降低性能，因此需要探索如何利用对象的局部位置信息来提升分类效果。

Method: 通过Segment Anything Model（仅需标记对象的一个像素）或无监督前景对象提取方法，利用对象的局部位置信息。

Result: 实验表明，该方法在多个基准测试中显著提升了分类性能。

Conclusion: 利用对象的局部位置信息是提升少样本图像分类性能的有效方法，且可通过简单标记或无监督方法实现。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [159] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种个性化引导方法，通过动态控制弱模型的权重插值，平衡目标分布对齐与文本编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG和AG）在少量图像微调时无法有效平衡目标分布对齐与模型原始知识保留。

Method: 利用未学习的弱模型和空文本提示，动态控制权重插值以平衡输出。

Result: 实验表明该方法能提升文本对齐和目标分布保真度，且无需额外计算开销。

Conclusion: 个性化引导方法有效解决了现有方法的局限性，实现了更好的平衡性能。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [160] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 论文提出Adapt-WeldNet框架，结合预训练架构、迁移学习和自适应优化器优化焊接缺陷检测，并引入DDIA框架提升系统透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法难以检测细微或内部缺陷，现有神经网络方法缺乏可解释性，存在安全隐患。

Method: Adapt-WeldNet系统评估预训练架构、迁移学习策略和自适应优化器；DDIA框架结合XAI技术和专家验证提升透明度。

Result: 优化了缺陷检测性能，并通过DDIA框架增强了系统的可解释性和可信度。

Conclusion: 该研究提升了焊接缺陷检测系统的性能和透明度，增强了在海洋和离岸环境中的安全性和可靠性。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [161] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 论文提出了一种混合架构$MV_{Hybrid}$，结合状态空间模型（SSMs）和ViT，用于预测病理图像中的空间基因表达，性能优于现有ViT模型。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学的高成本和复杂性限制了临床应用，而现有基于ViT的视觉基础模型在病理学中表现不佳。

Method: 引入$MV_{Hybrid}$，结合SSMs和ViT，利用负实特征值的状态空间模型捕捉低频形态模式。

Result: 在LOSO评估中，$MV_{Hybrid}$比最佳ViT模型相关性高57%，性能下降减少43%。

Conclusion: $MV_{Hybrid}$在基因表达预测和其他下游任务中表现优异，有望成为下一代病理视觉基础模型。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [162] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个基于CLIP架构的多模态多任务框架，用于预测真菌生长的发育阶段和时间戳，无需显式时间输入。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时序动态在微生物学、农业等领域至关重要，但现有视觉语言模型在捕捉时序进展方面有限。

Method: 基于CLIP架构，学习视觉-文本联合嵌入，通过分类和回归预测离散生长阶段和连续时间戳。

Result: 实验表明CLIPTime能有效建模生物进展，生成可解释的时序输出。

Conclusion: CLIPTime展示了视觉语言模型在生物监测应用中的潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [163] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接物体。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的可交互对象，但现有方法在物理合理性和点云对齐方面存在不足。

Method: 使用SDF表示部件形状，通过点云对齐损失和非穿透、移动性约束引导反向扩散过程，并引入类别信息优化对齐。

Result: 在PartNet-Mobility数据集上验证，PhysNAP在约束一致性和生成能力之间取得平衡，优于无引导基线模型。

Conclusion: PhysNAP通过物理约束和类别信息，显著提升了铰接物体生成的物理合理性和对齐效果。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [164] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 本文研究了针对人脸检测系统的生成攻击（Face Generation Attacks），首次提出了一种能够干扰坐标回归任务的Landmark Shift Attack，并提出了相应的防御措施。


<details>
  <summary>Details</summary>
Motivation: 在非受控环境下，人脸识别系统面临光照、姿态等挑战，需要依赖人脸检测模块进行对齐。研究这些模块的脆弱性有助于提升系统安全性。

Method: 提出了一种新型攻击方法（Landmark Shift Attack），干扰人脸检测器的坐标回归任务，并展示了其有效性。

Result: 实验证明了Face Generation Attacks和Landmark Shift Attack对人脸检测系统的破坏性。

Conclusion: 本文揭示了人脸检测系统的潜在漏洞，并提出了防御方案，为未来安全研究提供了方向。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [165] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN是一种针对卫星图像的SR框架，结合ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，提升语义一致性和像素级不确定性，适用于无人机和卫星系统。


<details>
  <summary>Details</summary>
Motivation: GANs在超分辨率任务中缺乏语义一致性和像素级置信度，限制了其在遥感关键应用中的可信度。

Method: 提出SU-ESRGAN，集成ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，生成像素级不确定性图。

Result: 性能与基线ESRGAN相当，在无人机数据集上表现良好，尤其在成像特征匹配时效果更佳。

Conclusion: SU-ESRGAN适用于卫星和无人机系统，强调领域感知训练在SR应用中的重要性。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>
