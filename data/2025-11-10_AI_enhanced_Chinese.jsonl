{"id": "2511.05082", "pdf": "https://arxiv.org/pdf/2511.05082", "abs": "https://arxiv.org/abs/2511.05082", "authors": ["Yiming Xie", "Hua Dai", "Mingfeng Jiang", "Pengyue Li", "zhengkai Zhang", "Bohan Li"], "title": "An Efficient Proximity Graph-based Approach to Table Union Search", "categories": ["cs.DB"], "comment": null, "summary": "Neural embedding models are extensively employed in the table union search\nproblem, which aims to find semantically compatible tables that can be merged\nwith a given query table. In particular, multi-vector models, which represent a\ntable as a vector set (typically one vector per column), have been demonstrated\nto achieve superior retrieval quality by capturing fine-grained semantic\nalignments. However, this problem faces more severe efficiency challenges than\nthe single-vector problem due to the inherent dependency on bipartite graph\nmaximum matching to compute unionability scores. Therefore, this paper proposes\nan efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS\nemploys a multi-stage pipeline that combines a novel refinement strategy, a\nfiltering strategy based on many-to-one bipartite matching. Besides, we propose\nan enhanced pruning strategy to prune the candidate set, which further improve\nthe search efficiency. Extensive experiments on six benchmark datasets\ndemonstrate that our approach achieves 3.6-6.0X speedup over existing\napproaches while maintaining comparable recall rates.", "AI": {"tldr": "\u63d0\u51faPGTUS\u65b9\u6cd5\u89e3\u51b3\u8868\u683c\u8054\u5408\u641c\u7d22\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u7ed3\u5408\u65b0\u9896\u7684\u4f18\u5316\u7b56\u7565\u548c\u8fc7\u6ee4\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53ec\u56de\u7387\u7684\u540c\u65f6\u5b9e\u73b03.6-6.0\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u591a\u5411\u91cf\u6a21\u578b\u5728\u8868\u683c\u8054\u5408\u641c\u7d22\u4e2d\u867d\u7136\u68c0\u7d22\u8d28\u91cf\u4f18\u8d8a\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u4e8c\u5206\u56fe\u6700\u5927\u5339\u914d\u8ba1\u7b97\u8054\u5408\u6027\u5206\u6570\uff0c\u9762\u4e34\u6bd4\u5355\u5411\u91cf\u6a21\u578b\u66f4\u4e25\u91cd\u7684\u6548\u7387\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u4f18\u5316\u7b56\u7565\u3001\u57fa\u4e8e\u591a\u5bf9\u4e00\u4e8c\u5206\u5339\u914d\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u53ca\u589e\u5f3a\u7684\u526a\u679d\u7b56\u7565\u6765\u4fee\u526a\u5019\u9009\u96c6\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e863.6-6.0\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u53ec\u56de\u7387\u3002", "conclusion": "PGTUS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u8054\u5408\u641c\u7d22\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2511.04853", "pdf": "https://arxiv.org/pdf/2511.04853", "abs": "https://arxiv.org/abs/2511.04853", "authors": ["Nuno dos Santos Fernandes", "Pedro Tom\u00e1s", "Nuno Roma", "Frank Winklmeier", "Patricia Conde-Mu\u00ed\u00f1o"], "title": "Marionette: Data Structure Description and Management for Heterogeneous Computing", "categories": ["cs.DC"], "comment": "5 pages, 2 figures. To be published as a short paper accepted by the\n  24th International Symposium on Parallel and Distributed Computing (ISPDC)", "summary": "Adapting large, object-oriented C++ codebases for hardware acceleration might\nbe extremely challenging, particularly when targeting heterogeneous platforms\nsuch as GPUs. Marionette is a C++17 library designed to address this by\nenabling flexible, efficient, and portable data structure definitions. It\ndecouples data layout from the description of the interface, supports multiple\nmemory management strategies, and provides efficient data transfers and\nconversions across devices, all of this with minimal runtime overhead due to\nthe compile-time nature of its abstractions. By allowing interfaces to be\naugmented with arbitrary functions, Marionette maintains compatibility with\nexisting code and offers a streamlined interface that supports both\nstraightforward and advanced use cases. This paper outlines its design, usage,\nand performance, including a CUDA-based case study demonstrating its efficiency\nand flexibility.", "AI": {"tldr": "Marionette\u662f\u4e00\u4e2aC++17\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u9762\u5411\u5bf9\u8c61C++\u4ee3\u7801\u5e93\u5728\u5f02\u6784\u5e73\u53f0\uff08\u5982GPU\uff09\u4e0a\u786c\u4ef6\u52a0\u901f\u7684\u6311\u6218\uff0c\u901a\u8fc7\u89e3\u8026\u6570\u636e\u5e03\u5c40\u4e0e\u63a5\u53e3\u63cf\u8ff0\uff0c\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6548\u3001\u53ef\u79fb\u690d\u7684\u6570\u636e\u7ed3\u6784\u5b9a\u4e49\u3002", "motivation": "\u9002\u5e94\u5927\u578b\u9762\u5411\u5bf9\u8c61C++\u4ee3\u7801\u5e93\u8fdb\u884c\u786c\u4ef6\u52a0\u901f\u975e\u5e38\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u9488\u5bf9GPU\u7b49\u5f02\u6784\u5e73\u53f0\u65f6\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u5e03\u5c40\u3001\u5185\u5b58\u7ba1\u7406\u548c\u8de8\u8bbe\u5907\u6570\u636e\u4f20\u8f93\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528C++17\u7f16\u8bd1\u65f6\u62bd\u8c61\uff0c\u89e3\u8026\u6570\u636e\u5e03\u5c40\u4e0e\u63a5\u53e3\u63cf\u8ff0\uff0c\u652f\u6301\u591a\u79cd\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u548c\u8f6c\u6362\uff0c\u540c\u65f6\u5141\u8bb8\u63a5\u53e3\u901a\u8fc7\u4efb\u610f\u51fd\u6570\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u901a\u8fc7\u57fa\u4e8eCUDA\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\uff0cMarionette\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u5b9e\u73b0\u9ad8\u6548\u7075\u6d3b\u7684\u6570\u636e\u7ed3\u6784\u7ba1\u7406\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u4ee3\u7801\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "Marionette\u4e3aC++\u4ee3\u7801\u5e93\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u7684\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7f16\u8bd1\u65f6\u62bd\u8c61\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u7075\u6d3b\u6027\uff0c\u652f\u6301\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u5404\u79cd\u4f7f\u7528\u573a\u666f\u3002"}}
{"id": "2511.04775", "pdf": "https://arxiv.org/pdf/2511.04775", "abs": "https://arxiv.org/abs/2511.04775", "authors": ["Ce Jin", "Yael Kirkpatrick", "Micha\u0142 Stawarz", "Virginia Vassilevska Williams"], "title": "Improved Additive Approximation Algorithms for APSP", "categories": ["cs.DS"], "comment": null, "summary": "The All-Pairs Shortest Paths (APSP) is a foundational problem in theoretical\ncomputer science. Approximating APSP in undirected unweighted graphs has been\nstudied for many years, beginning with the work of Dor, Halperin and Zwick\n[SICOMP'01]. Many recent works have attempted to improve these original\nalgorithms using the algebraic tools of fast matrix multiplication. We improve\non these results for the following problems.\n  For $+2$-approximate APSP, the state-of-the-art algorithm runs in\n$O(n^{2.259})$ time [D\\\"urr, IPL 2023; Deng, Kirkpatrick, Rong, Vassilevska\nWilliams, and Zhong, ICALP 2022]. We give an improved algorithm in\n$O(n^{2.2255})$ time.\n  For $+4$ and $+6$-approximate APSP, we achieve time complexities\n$O(n^{2.1462})$ and $O(n^{2.1026})$ respectively, improving the previous\n$O(n^{2.155})$ and $O(n^{2.103})$ achieved by [Saha and Ye, SODA 2024].\n  In contrast to previous works, we do not use the big hammer of\nbounded-difference $(\\min,+)$-product algorithms. Instead, our algorithms are\nbased on a simple technique that decomposes the input graph into a small number\nof clusters of constant diameter and a remainder of low degree vertices, which\ncould be of independent interest in the study of shortest paths problems. We\nthen use only standard fast matrix multiplication to obtain our improvements.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u65e0\u5411\u65e0\u6743\u56fe\u4e2d\u5168\u5bf9\u6700\u77ed\u8def\u5f84(APSP)\u95ee\u9898\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u9488\u5bf9+2\u3001+4\u548c+6\u8fd1\u4f3c\u5206\u522b\u8fbe\u5230\u4e86O(n^2.2255)\u3001O(n^2.1462)\u548cO(n^2.1026)\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u5168\u5bf9\u6700\u77ed\u8def\u5f84\u662f\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u57fa\u7840\u95ee\u9898\uff0c\u8fd1\u5e74\u6765\u8bb8\u591a\u5de5\u4f5c\u8bd5\u56fe\u901a\u8fc7\u5feb\u901f\u77e9\u9635\u4e58\u6cd5\u7b49\u4ee3\u6570\u5de5\u5177\u6539\u8fdbDor\u7b49\u4eba[SICOMP'01]\u7684\u539f\u59cb\u7b97\u6cd5\u3002\u672c\u6587\u65e8\u5728\u8fdb\u4e00\u6b65\u63d0\u5347\u8fd9\u4e9b\u8fd1\u4f3c\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u7b80\u5355\u6280\u672f\u5c06\u8f93\u5165\u56fe\u5206\u89e3\u4e3a\u5c11\u91cf\u5e38\u6570\u76f4\u5f84\u7684\u7c07\u548c\u4f4e\u5ea6\u6570\u9876\u70b9\u7684\u5269\u4f59\u90e8\u5206\uff0c\u4ec5\u4f7f\u7528\u6807\u51c6\u5feb\u901f\u77e9\u9635\u4e58\u6cd5\u83b7\u5f97\u6539\u8fdb\uff0c\u800c\u4e0d\u4f9d\u8d56\u6709\u754c\u5dee(min,+)\u4e58\u79ef\u7b97\u6cd5\u3002", "result": "\u5bf9\u4e8e+2\u8fd1\u4f3cAPSP\uff0c\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(n^2.259)\u6539\u8fdb\u5230O(n^2.2255)\uff1b\u5bf9\u4e8e+4\u548c+6\u8fd1\u4f3c\uff0c\u5206\u522b\u4eceO(n^2.155)\u548cO(n^2.103)\u6539\u8fdb\u5230O(n^2.1462)\u548cO(n^2.1026)\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u5206\u89e3\u7684\u7b80\u5355\u6280\u672f\u80fd\u591f\u6709\u6548\u6539\u8fdbAPSP\u8fd1\u4f3c\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u77ed\u8def\u5f84\u95ee\u9898\u7814\u7a76\u4e2d\u53ef\u80fd\u5177\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2511.05022", "pdf": "https://arxiv.org/pdf/2511.05022", "abs": "https://arxiv.org/abs/2511.05022", "authors": ["Charles Melvin", "N. Rich Nguyen"], "title": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems", "categories": ["cs.NI", "H.3.3; H.3.4; H.m"], "comment": "Preprint version", "summary": "PriorityFresh is a semantic, actionability-first caching policy designed for\noffline emergency warning systems. Within the AWARE system's simulation\nenvironment, PriorityFresh optimizes which alerts to retain and surface under\nconstrained connectivity. Experiments indicate improved actionability-first\nperformance without harming efficiency. A separate Priority Forecasting model\nis used only to synthesize realistic alert sequences for controlled experiments\nand does not influence caching or push decisions.", "AI": {"tldr": "PriorityFresh\u662f\u4e00\u79cd\u8bed\u4e49\u4f18\u5148\u3001\u4ee5\u53ef\u64cd\u4f5c\u6027\u4e3a\u5bfc\u5411\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u4e13\u4e3a\u79bb\u7ebf\u7d27\u6025\u9884\u8b66\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5728AWARE\u7cfb\u7edf\u6a21\u62df\u73af\u5883\u4e2d\u4f18\u5316\u53d7\u9650\u8fde\u63a5\u4e0b\u7684\u8b66\u62a5\u4fdd\u7559\u548c\u63a8\u9001\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u5728\u8fde\u63a5\u53d7\u9650\u73af\u5883\u4e0b\u80fd\u591f\u4f18\u5148\u4fdd\u7559\u548c\u63a8\u9001\u6700\u5177\u53ef\u64cd\u4f5c\u6027\u8b66\u62a5\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u79bb\u7ebf\u7d27\u6025\u9884\u8b66\u7cfb\u7edf\u7684\u6548\u80fd\u3002", "method": "\u5728AWARE\u7cfb\u7edf\u6a21\u62df\u73af\u5883\u4e2d\u5b9e\u73b0PriorityFresh\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u8bed\u4e49\u548c\u53ef\u64cd\u4f5c\u6027\u4f18\u5148\u7ea7\u6765\u4f18\u5316\u8b66\u62a5\u7f13\u5b58\uff1b\u540c\u65f6\u4f7f\u7528\u72ec\u7acb\u7684\u4f18\u5148\u7ea7\u9884\u6d4b\u6a21\u578b\u751f\u6210\u771f\u5b9e\u8b66\u62a5\u5e8f\u5217\u7528\u4e8e\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePriorityFresh\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee5\u53ef\u64cd\u4f5c\u6027\u4e3a\u5bfc\u5411\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "PriorityFresh\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u79bb\u7ebf\u7d27\u6025\u9884\u8b66\u7cfb\u7edf\u7684\u8b66\u62a5\u53ef\u64cd\u4f5c\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u7cfb\u7edf\u6548\u7387\uff0c\u4e3a\u53d7\u9650\u8fde\u63a5\u73af\u5883\u4e0b\u7684\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04824", "pdf": "https://arxiv.org/pdf/2511.04824", "abs": "https://arxiv.org/abs/2511.04824", "authors": ["Kosei Horikawa", "Hao Li", "Yutaro Kashiwa", "Bram Adams", "Hajimu Iida", "Ahmed E. Hassan"], "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents", "categories": ["cs.SE", "D.2.7"], "comment": "23 pages, 7 Tables, 5 Figuress, Submitted to ACM Transactions on\n  Software Engineering and Methodology(TOSEM)", "summary": "Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are\ntransforming the software engineering landscape. These AI-powered systems\nfunction as autonomous teammates capable of planning and executing complex\ndevelopment tasks. Agents have become active participants in refactoring, a\ncornerstone of sustainable software development aimed at improving internal\ncode quality without altering observable behavior. Despite their increasing\nadoption, there is a critical lack of empirical understanding regarding how\nagentic refactoring is utilized in practice, how it compares to human-driven\nrefactoring, and what impact it has on code quality. To address this empirical\ngap, we present a large-scale study of AI agent-generated refactorings in\nreal-world open-source Java projects, analyzing 15,451 refactoring instances\nacross 12,256 pull requests and 14,988 commits derived from the AIDev dataset.\nOur empirical analysis shows that refactoring is a common and intentional\nactivity in this development paradigm, with agents explicitly targeting\nrefactoring in 26.1% of commits. Analysis of refactoring types reveals that\nagentic efforts are dominated by low-level, consistency-oriented edits, such as\nChange Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable\n(8.5%), reflecting a preference for localized improvements over the high-level\ndesign changes common in human refactoring. Additionally, the motivations\nbehind agentic refactoring focus overwhelmingly on internal quality concerns,\nwith maintainability (52.5%) and readability (28.1%). Furthermore, quantitative\nevaluation of code quality metrics shows that agentic refactoring yields small\nbut statistically significant improvements in structural metrics, particularly\nfor medium-level changes, reducing class size and complexity (e.g., Class LOC\nmedian $\\Delta$ = -15.25).", "AI": {"tldr": "\u5bf9AI\u4ee3\u7406\u5728\u771f\u5b9e\u5f00\u6e90Java\u9879\u76ee\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u91cd\u6784\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e8615,451\u4e2a\u91cd\u6784\u5b9e\u4f8b\uff0c\u53d1\u73b0\u4ee3\u7406\u91cd\u6784\u4e3b\u8981\u5173\u6ce8\u4f4e\u5c42\u6b21\u3001\u4e00\u81f4\u6027\u5bfc\u5411\u7684\u7f16\u8f91\uff0c\u4e0e\u4eba\u7c7b\u91cd\u6784\u76f8\u6bd4\u66f4\u6ce8\u91cd\u5c40\u90e8\u6539\u8fdb\u800c\u975e\u9ad8\u5c42\u6b21\u8bbe\u8ba1\u53d8\u66f4\u3002", "motivation": "AI\u4ee3\u7406\u7f16\u7801\u5de5\u5177\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u683c\u5c40\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4ee3\u7406\u91cd\u6784\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u5e94\u7528\u3001\u4e0e\u4eba\u7c7b\u91cd\u6784\u6bd4\u8f83\u4ee5\u53ca\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u5f71\u54cd\u7684\u5b9e\u8bc1\u7406\u89e3\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\uff0c\u5206\u679012,256\u4e2apull\u8bf7\u6c42\u548c14,988\u4e2a\u63d0\u4ea4\u4e2d\u768415,451\u4e2a\u91cd\u6784\u5b9e\u4f8b\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u91cd\u6784\u662f\u8fd9\u79cd\u5f00\u53d1\u8303\u5f0f\u4e2d\u5e38\u89c1\u4e14\u6709\u610f\u5411\u7684\u6d3b\u52a8\uff0826.1%\u7684\u63d0\u4ea4\u660e\u786e\u9488\u5bf9\u91cd\u6784\uff09\uff0c\u4ee3\u7406\u91cd\u6784\u4e3b\u8981\u5173\u6ce8\u4f4e\u5c42\u6b21\u7f16\u8f91\uff08\u5982\u53d8\u91cf\u7c7b\u578b\u66f4\u653911.8%\u3001\u53c2\u6570\u91cd\u547d\u540d10.4%\uff09\uff0c\u52a8\u673a\u4e3b\u8981\u662f\u53ef\u7ef4\u62a4\u6027\uff0852.5%\uff09\u548c\u53ef\u8bfb\u6027\uff0828.1%\uff09\uff0c\u5728\u7ed3\u6784\u6307\u6807\u4e0a\u4ea7\u751f\u5c0f\u800c\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "AI\u4ee3\u7406\u91cd\u6784\u5728\u5b9e\u8df5\u4e2d\u662f\u5e38\u89c1\u4e14\u6709\u6548\u7684\u6d3b\u52a8\uff0c\u4f46\u4e0e\u4eba\u7c7b\u91cd\u6784\u76f8\u6bd4\u66f4\u6ce8\u91cd\u5c40\u90e8\u6539\u8fdb\uff0c\u5728\u4ee3\u7801\u8d28\u91cf\u6307\u6807\u4e0a\u4ea7\u751f\u79ef\u6781\u4f46\u6709\u9650\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.04686", "pdf": "https://arxiv.org/pdf/2511.04686", "abs": "https://arxiv.org/abs/2511.04686", "authors": ["Pratik Poudel"], "title": "Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 2 figures", "summary": "The Key-Value (KV) cache is integral to efficient autoregressive inference in\nlarge language models (LLMs), yet its unbounded growth in stateful multi-turn\nscenarios presents major challenges. This paper examines the interplay between\nKV cache management strategies, the architectural context limits of models like\nmeta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of\npositional encodings. Through empirical analysis using a stateful benchmarking\nframework, we show that LLM generation quality degrades sharply when the\naccumulated KV cache approaches or exceeds the model's trained context window\n(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory\nexhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via\nAttentionTop), can worsen performance if they disrupt positional coherence.\nBecause LLMs rely on consistent positional signals (e.g., RoPE), compacting a\ncache by removing non-contiguous tokens can scramble these signals and lead to\ndegenerative outputs. We further show that simple strategies preserving\ncontiguous context blocks (e.g., keeping an initial \"gist\") can yield more\ncoherent generations than complex or positionally disruptive ones. We advocate\nfor eviction techniques that respect architectural limits, preserve positional\nstructure, and view \"cache health\" holistically beyond mere size.", "AI": {"tldr": "KV\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\u5728\u63a5\u8fd1\u6216\u8d85\u8fc7\u6a21\u578b\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\u4f1a\u4e25\u91cd\u964d\u4f4eLLM\u751f\u6210\u8d28\u91cf\uff0c\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u6027\u7684\u7834\u574f\u662f\u4e3b\u8981\u95ee\u9898\uff0c\u4fdd\u6301\u8fde\u7eed\u4e0a\u4e0b\u6587\u5757\u7684\u7b80\u5355\u7b56\u7565\u4f18\u4e8e\u590d\u6742\u7684\u4f4d\u7f6e\u7834\u574f\u6027\u7b56\u7565\u3002", "motivation": "\u7814\u7a76KV\u7f13\u5b58\u5728\u72b6\u6001\u5316\u591a\u8f6e\u573a\u666f\u4e2d\u65e0\u9650\u589e\u957f\u5e26\u6765\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u6027\u5bf9LLM\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u72b6\u6001\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540cKV\u7f13\u5b58\u9a71\u9010\u7b56\u7565\uff0c\u5305\u62ec\u9ad8\u4fdd\u7559\u7387\u7b56\u7565\u548c\u4fdd\u6301\u8fde\u7eed\u4e0a\u4e0b\u6587\u5757\u7684\u7b56\u7565\u3002", "result": "\u5f53\u7d2f\u79efKV\u7f13\u5b58\u63a5\u8fd1\u6216\u8d85\u8fc7\u6a21\u578b\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\uff0cLLM\u751f\u6210\u8d28\u91cf\u6025\u5267\u4e0b\u964d\uff1b\u4fdd\u6301\u4f4d\u7f6e\u8fde\u7eed\u6027\u7684\u7b80\u5355\u7b56\u7565\u6bd4\u590d\u6742\u7b56\u7565\u4ea7\u751f\u66f4\u8fde\u8d2f\u7684\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "KV\u7f13\u5b58\u9a71\u9010\u6280\u672f\u5e94\u5c0a\u91cd\u67b6\u6784\u9650\u5236\u3001\u4fdd\u6301\u4f4d\u7f6e\u7ed3\u6784\uff0c\u5c06\"\u7f13\u5b58\u5065\u5eb7\"\u89c6\u4e3a\u8d85\u8d8a\u5355\u7eaf\u5927\u5c0f\u7684\u6574\u4f53\u6982\u5ff5\u3002"}}
{"id": "2511.05053", "pdf": "https://arxiv.org/pdf/2511.05053", "abs": "https://arxiv.org/abs/2511.05053", "authors": ["Wakuto Matsumi", "Riaz-Ul-Haque Mian"], "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs", "categories": ["cs.DC", "cs.AI", "cs.GR"], "comment": null, "summary": "Machine learning based on neural networks has advanced rapidly, but the high\nenergy consumption required for training and inference remains a major\nchallenge. Hyperdimensional Computing (HDC) offers a lightweight,\nbrain-inspired alternative that enables high parallelism but often suffers from\nlower accuracy on complex visual tasks. To overcome this, hybrid accelerators\ncombining HDC and Convolutional Neural Networks (CNNs) have been proposed,\nthough their adoption is limited by poor generalizability and programmability.\nThe rise of open-source RISC-V architectures has created new opportunities for\ndomain-specific GPU design. Unlike traditional proprietary GPUs, emerging\nRISC-V-based GPUs provide flexible, programmable platforms suitable for custom\ncomputation models such as HDC. In this study, we design and implement custom\nGPU instructions optimized for HDC operations, enabling efficient processing\nfor hybrid HDC-CNN workloads. Experimental results using four types of custom\nHDC instructions show a performance improvement of up to 56.2 times in\nmicrobenchmark tests, demonstrating the potential of RISC-V GPUs for\nenergy-efficient, high-performance computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRISC-V GPU\u7684\u6df7\u5408HDC-CNN\u52a0\u901f\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5b9a\u5236GPU\u6307\u4ee4\u4f18\u5316\u8d85\u7ef4\u8ba1\u7b97\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe56.2\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u80fd\u8017\u9ad8\uff0c\u800c\u8d85\u7ef4\u8ba1\u7b97(HDC)\u867d\u7136\u8f7b\u91cf\u4f46\u7cbe\u5ea6\u8f83\u4f4e\u3002\u6df7\u5408HDC-CNN\u52a0\u901f\u5668\u9762\u4e34\u6cdb\u5316\u6027\u548c\u53ef\u7f16\u7a0b\u6027\u5dee\u7684\u95ee\u9898\uff0cRISC-V\u67b6\u6784\u4e3a\u5b9a\u5236GPU\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u56db\u79cd\u9488\u5bf9HDC\u64cd\u4f5c\u4f18\u5316\u7684\u5b9a\u5236GPU\u6307\u4ee4\uff0c\u5728RISC-V GPU\u5e73\u53f0\u4e0a\u5904\u7406\u6df7\u5408HDC-CNN\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u5fae\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe56.2\u500d\uff0c\u8bc1\u660e\u4e86RISC-V GPU\u5728\u9ad8\u6027\u80fd\u80fd\u6548\u8ba1\u7b97\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8eRISC-V\u7684\u5b9a\u5236GPU\u6307\u4ee4\u80fd\u591f\u6709\u6548\u52a0\u901fHDC\u64cd\u4f5c\uff0c\u4e3a\u6df7\u5408HDC-CNN\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u9ad8\u6548\u5904\u7406\u65b9\u6848\u3002"}}
{"id": "2511.04826", "pdf": "https://arxiv.org/pdf/2511.04826", "abs": "https://arxiv.org/abs/2511.04826", "authors": ["Sanjeev Khanna", "Aaron Putterman", "Junkai Song"], "title": "Optimal Parallel Basis Finding in Graphic and Related Matroids", "categories": ["cs.DS", "cs.CC"], "comment": null, "summary": "We study the parallel complexity of finding a basis of a graphic matroid\nunder independence-oracle access. Karp, Upfal, and Wigderson (FOCS 1985, JCSS\n1988) initiated the study of this problem and established two algorithms for\nfinding a spanning forest: one running in $O(\\log m)$ rounds with\n$m^{\\Theta(\\log m)}$ queries, and another, for any $d \\in \\mathbb{Z}^+$,\nrunning in $O(m^{2/d})$ rounds with $\\Theta(m^d)$ queries. A key open question\nthey posed was whether one could simultaneously achieve polylogarithmic rounds\nand polynomially many queries. We give a deterministic algorithm that uses\n$O(\\log m)$ adaptive rounds and $\\mathrm{poly}(m)$ non-adaptive queries per\nround to return a spanning forest on $m$ edges, and complement this result with\na matching $\\Omega(\\log m)$ lower bound for any (even randomized) algorithm\nwith $\\mathrm{poly}(m)$ queries per round. Thus, the adaptive round complexity\nfor graphic matroids is characterized exactly, settling this long-standing\nproblem. Beyond graphs, we show that our framework also yields an $O(\\log\nm)$-round, $\\mathrm{poly}(m)$-query algorithm for any binary matroid satisfying\na smooth circuit counting property, implying, among others, an optimal $O(\\log\nm)$-round parallel algorithms for finding bases of cographic matroids.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u5728\u72ec\u7acb\u6027\u9884\u8a00\u8bbf\u95ee\u4e0b\u5bfb\u627e\u56fe\u5f62\u62df\u9635\u57fa\u7684\u5e76\u884c\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u4e00\u4e2a\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u4f7f\u7528O(log m)\u81ea\u9002\u5e94\u8f6e\u6b21\u548c\u6bcf\u8f6epoly(m)\u975e\u81ea\u9002\u5e94\u67e5\u8be2\u6765\u627e\u5230\u751f\u6210\u68ee\u6797\uff0c\u5e76\u7ed9\u51fa\u4e86\u5339\u914d\u7684\u4e0b\u754c\u3002", "motivation": "Karp\u3001Upfal\u548cWigderson\u57281985\u5e74\u63d0\u51fa\u4e86\u5bfb\u627e\u751f\u6210\u68ee\u6797\u7684\u95ee\u9898\uff0c\u4ed6\u4eec\u5efa\u7acb\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u4f46\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u662f\u5426\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u591a\u5bf9\u6570\u8f6e\u6b21\u548c\u591a\u9879\u5f0f\u67e5\u8be2\u6b21\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u786e\u5b9a\u6027\u7b97\u6cd5\u6846\u67b6\uff0c\u4f7f\u7528O(log m)\u81ea\u9002\u5e94\u8f6e\u6b21\uff0c\u6bcf\u8f6e\u6267\u884cpoly(m)\u6b21\u975e\u81ea\u9002\u5e94\u67e5\u8be2\u6765\u5bfb\u627e\u751f\u6210\u68ee\u6797\u3002", "result": "\u7b97\u6cd5\u5728O(log m)\u8f6e\u6b21\u5185\u8fd4\u56de\u751f\u6210\u68ee\u6797\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u4efb\u4f55\u7b97\u6cd5\uff08\u5373\u4f7f\u662f\u968f\u673a\u5316\u7684\uff09\u4f7f\u7528poly(m)\u67e5\u8be2\u6bcf\u8f6e\u90fd\u9700\u8981\u03a9(log m)\u8f6e\u6b21\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b8c\u5168\u523b\u753b\u4e86\u56fe\u5f62\u62df\u9635\u7684\u81ea\u9002\u5e94\u8f6e\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u6846\u67b6\u4e5f\u9002\u7528\u4e8e\u6ee1\u8db3\u5e73\u6ed1\u7535\u8def\u8ba1\u6570\u6027\u8d28\u7684\u4e8c\u5143\u62df\u9635\u3002"}}
{"id": "2511.05027", "pdf": "https://arxiv.org/pdf/2511.05027", "abs": "https://arxiv.org/abs/2511.05027", "authors": ["Zhuoling Chen", "Yi Zhong", "Martin Haenggi"], "title": "Cross-link RTS/CTS for MLO mm-Wave WLANs", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": "13 pages, 13 figures", "summary": "The directional RTS/CTS mechanism of mm-wave Wi-Fi hardly resolves the hidden\nterminal problem perfectly.This paper proposes cross-link RTS/CTS under\nmulti-link operation (MLO) to address this problem and introduces a novel point\nprocess, named the generalized RTS/CTS hard-core process (G-HCP), to model the\nspatial transceiver relationships under the RTS/CTS mechanism, including the\ndirectional case and the omnidirectional case.Analytical expressions are\nderived for the intensity, the mean interference, an approximation of the\nsuccess probability, and the expected number of hidden nodes for the\ndirectional RTS/CTS mechanism.Theoretical and numerical results demonstrate the\nperformance difference between two RTS/CTS mechanisms.The cross-link RTS/CTS\nmechanism ensures higher link quality at the cost of reduced network\nthroughput.In contrast, the directional RTS/CTS sacrifices the link quality for\nhigher throughput.Our study reveals a fundamental trade-off between link\nreliability and network throughput, providing critical insights into the\nselection and optimization of RTS/CTS mechanisms in next-generation WLAN\nstandards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8de8\u94fe\u8defRTS/CTS\u673a\u5236\u89e3\u51b3\u6beb\u7c73\u6ce2Wi-Fi\u4e2d\u7684\u9690\u85cf\u7ec8\u7aef\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u5e7f\u4e49RTS/CTS\u786c\u6838\u8fc7\u7a0b\u6a21\u578b\u6765\u5206\u6790\u7a7a\u95f4\u6536\u53d1\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u94fe\u8def\u53ef\u9760\u6027\u4e0e\u7f51\u7edc\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "motivation": "\u6beb\u7c73\u6ce2Wi-Fi\u4e2d\u7684\u5b9a\u5411RTS/CTS\u673a\u5236\u65e0\u6cd5\u5b8c\u7f8e\u89e3\u51b3\u9690\u85cf\u7ec8\u7aef\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u673a\u5236\u6765\u6539\u5584\u94fe\u8def\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u8de8\u94fe\u8defRTS/CTS\u673a\u5236\uff0c\u5e76\u5f15\u5165\u5e7f\u4e49RTS/CTS\u786c\u6838\u8fc7\u7a0b(G-HCP)\u6765\u5efa\u6a21\u7a7a\u95f4\u6536\u53d1\u5173\u7cfb\uff0c\u63a8\u5bfc\u4e86\u5f3a\u5ea6\u3001\u5e73\u5747\u5e72\u6270\u3001\u6210\u529f\u6982\u7387\u8fd1\u4f3c\u548c\u9690\u85cf\u8282\u70b9\u671f\u671b\u6570\u7b49\u5206\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u8de8\u94fe\u8defRTS/CTS\u673a\u5236\u4ee5\u964d\u4f4e\u7f51\u7edc\u541e\u5410\u91cf\u4e3a\u4ee3\u4ef7\u786e\u4fdd\u66f4\u9ad8\u7684\u94fe\u8def\u8d28\u91cf\uff0c\u800c\u5b9a\u5411RTS/CTS\u5219\u727a\u7272\u94fe\u8def\u8d28\u91cf\u6765\u83b7\u5f97\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u94fe\u8def\u53ef\u9760\u6027\u4e0e\u7f51\u7edc\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u4e3a\u4e0b\u4e00\u4ee3WLAN\u6807\u51c6\u4e2dRTS/CTS\u673a\u5236\u7684\u9009\u62e9\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2511.04849", "pdf": "https://arxiv.org/pdf/2511.04849", "abs": "https://arxiv.org/abs/2511.04849", "authors": ["Quang-Dung Nguyen", "Tri-Dung Tran", "Thanh-Hieu Chu", "Hoang-Loc Tran", "Xiangwei Cheng", "Dirk Slama"], "title": "Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach", "categories": ["cs.SE", "cs.AI", "I.2.6; I.2.7; D.2.3"], "comment": "6 pages, 3 figures", "summary": "The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in\nthe automotive industry, where software now plays a pivotal role in defining\nvehicle functionality, enabling rapid innovation of modern vehicles. Developing\nSDV-specific applications demands advanced tools to streamline code generation\nand improve development efficiency. In recent years, general-purpose large\nlanguage models (LLMs) have demonstrated transformative potential across\ndomains. Still, restricted access to proprietary model architectures hinders\ntheir adaption to specific tasks like SDV code generation. In this study, we\npropose using prompts, a common and basic strategy to interact with LLMs and\nredirect their responses. Using only system prompts with an appropriate and\nefficient prompt structure designed using advanced prompt engineering\ntechniques, LLMs can be crafted without requiring a training session or access\nto their base design. This research investigates the extensive experiments on\ndifferent models by applying various prompting techniques, including bare\nmodels, using a benchmark specifically created to evaluate LLMs' performance in\ngenerating SDV code. The results reveal that the model with a few-shot\nprompting strategy outperforms the others in adjusting the LLM answers to match\nthe expected outcomes based on quantitative metrics.", "AI": {"tldr": "\u4f7f\u7528\u7cfb\u7edf\u63d0\u793a\u548c\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86(SDV)\u7684\u53d1\u5c55\u9700\u8981\u9ad8\u6548\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff0c\u4f46\u4e13\u6709\u6a21\u578b\u67b6\u6784\u7684\u9650\u5236\u963b\u788d\u4e86LLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u8bbe\u8ba1\u7cfb\u7edf\u63d0\u793a\u7ed3\u6784\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7b56\u7565\u5f15\u5bfcLLM\u751f\u6210SDV\u4ee3\u7801\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7b56\u7565\u7684\u6a21\u578b\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u66f4\u597d\u5730\u8c03\u6574LLM\u8f93\u51fa\u4ee5\u5339\u914d\u9884\u671f\u7ed3\u679c\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u662f\u4f18\u5316LLM\u5728SDV\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u67b6\u6784\u6216\u8fdb\u884c\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.04718", "pdf": "https://arxiv.org/pdf/2511.04718", "abs": "https://arxiv.org/abs/2511.04718", "authors": ["Yue Xun", "Jiaxing Xu", "Wenbo Gao", "Chen Yang", "Shujun Wang"], "title": "Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "11 pages, 2 figures, conference", "summary": "Resting-state fMRI has become a valuable tool for classifying brain disorders\nand constructing brain functional connectivity networks\n  by tracking BOLD signals across brain regions. However, existing mod els\nlargely neglect the multi-frequency nature of neuronal oscillations,\n  treating BOLD signals as monolithic time series. This overlooks the cru cial\nfact that neurological disorders often manifest as disruptions within\n  specific frequency bands, limiting diagnostic sensitivity and specificity.\n  While some methods have attempted to incorporate frequency informa tion, they\noften rely on predefined frequency bands, which may not be\n  optimal for capturing individual variability or disease-specific alterations.\n  To address this, we propose a novel framework featuring Adaptive Cas cade\nDecomposition to learn task-relevant frequency sub-bands for each\n  brain region and Frequency-Coupled Connectivity Learning to capture\n  both intra- and nuanced cross-band interactions in a unified functional\n  network. This unified network informs a novel message-passing mecha nism\nwithin our Unified-GCN, generating refined node representations\n  for diagnostic prediction. Experimental results on the ADNI and ABIDE\n  datasets demonstrate superior performance over existing methods. The\n  code is available at https://github.com/XXYY20221234/Ada-FCN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u9891\u7387\u8fde\u63a5\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea7\u8054\u5206\u89e3\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u9891\u7387\u5b50\u5e26\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\u6765\u6355\u6349\u8de8\u9891\u6bb5\u4ea4\u4e92\uff0c\u5728\u8111\u75be\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9759\u606f\u6001fMRI\u6a21\u578b\u5927\u591a\u5ffd\u7565\u795e\u7ecf\u5143\u632f\u8361\u7684\u591a\u9891\u7279\u6027\uff0c\u5c06BOLD\u4fe1\u53f7\u89c6\u4e3a\u5355\u4e00\u65f6\u95f4\u5e8f\u5217\u5904\u7406\uff0c\u8fd9\u9650\u5236\u4e86\u8bca\u65ad\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u3002\u795e\u7ecf\u75be\u75c5\u901a\u5e38\u5728\u7279\u5b9a\u9891\u6bb5\u8868\u73b0\u51fa\u5f02\u5e38\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u9891\u5e26\uff0c\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u5dee\u5f02\u548c\u75be\u75c5\u7279\u5f02\u6027\u6539\u53d8\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7ea7\u8054\u5206\u89e3\u5b66\u4e60\u6bcf\u4e2a\u8111\u533a\u7684\u4efb\u52a1\u76f8\u5173\u9891\u7387\u5b50\u5e26\uff0c\u9891\u7387\u8026\u5408\u8fde\u63a5\u5b66\u4e60\u6355\u6349\u9891\u6bb5\u5185\u548c\u8de8\u9891\u6bb5\u7684\u7cbe\u7ec6\u4ea4\u4e92\uff0c\u6784\u5efa\u7edf\u4e00\u529f\u80fd\u7f51\u7edc\u3002\u8be5\u7f51\u7edc\u901a\u8fc7\u7edf\u4e00GCN\u4e2d\u7684\u65b0\u578b\u6d88\u606f\u4f20\u9012\u673a\u5236\u751f\u6210\u7cbe\u70bc\u8282\u70b9\u8868\u793a\u7528\u4e8e\u8bca\u65ad\u9884\u6d4b\u3002", "result": "\u5728ADNI\u548cABIDE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u9891\u7387\u5b50\u5e26\u5e76\u6355\u6349\u8de8\u9891\u6bb5\u4ea4\u4e92\uff0c\u4e3a\u8111\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u654f\u611f\u548c\u7279\u5f02\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.05067", "pdf": "https://arxiv.org/pdf/2511.05067", "abs": "https://arxiv.org/abs/2511.05067", "authors": ["Giuseppe Esposito", "Juan-David Guerrero-Balaguera", "Josie Esteban Rodriguez Condia", "Matteo Sonza Reorda", "Marco Barbiero", "Rossella Fortuna"], "title": "GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters", "categories": ["cs.DC"], "comment": null, "summary": "Graphics Processing Units (GPUs) are specialized accelerators in data centers\nand high-performance computing (HPC) systems, enabling the fast execution of\ncompute-intensive applications, such as Convolutional Neural Networks (CNNs).\nHowever, sustained workloads can impose significant stress on GPU components,\nraising reliability concerns due to potential faults that corrupt the\nintermediate application computations, leading to incorrect results. Estimating\nthe stress induced by an application is thus crucial to predict reliability\n(with\\,special\\,emphasis\\,on\\,aging\\,effects). In this work, we combine online\ntelemetry parameters and hardware performance counters to assess GPU stress\ninduced by different applications. The experimental results indicate the stress\ninduced by a parallel workload can be estimated by combining telemetry data and\nPerformance Counters that reveal the efficiency in the resource usage of the\ntarget workload. For this purpose the selected performance counters focus on\nmeasuring the i) throughput, ii) amount of issued instructions and iii) stall\nevents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5728\u7ebf\u9065\u6d4b\u53c2\u6570\u548c\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\u6765\u8bc4\u4f30GPU\u5de5\u4f5c\u8d1f\u8f7d\u5e94\u529b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4bGPU\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "GPU\u5728\u6301\u7eed\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4f1a\u4ea7\u751f\u663e\u8457\u5e94\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u7ec4\u4ef6\u6545\u969c\u548c\u8ba1\u7b97\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u51c6\u786e\u8bc4\u4f30\u5de5\u4f5c\u8d1f\u8f7d\u5e94\u529b\u4ee5\u9884\u6d4b\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5728\u7ebf\u9065\u6d4b\u53c2\u6570\u548c\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff0c\u901a\u8fc7\u6d4b\u91cf\u541e\u5410\u91cf\u3001\u6307\u4ee4\u6570\u91cf\u548c\u505c\u987f\u4e8b\u4ef6\u6765\u8bc4\u4f30GPU\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408\u9065\u6d4b\u6570\u636e\u548c\u6027\u80fd\u8ba1\u6570\u5668\u53ef\u4ee5\u51c6\u786e\u4f30\u8ba1\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u5728GPU\u4e0a\u4ea7\u751f\u7684\u5e94\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30GPU\u5de5\u4f5c\u8d1f\u8f7d\u5e94\u529b\uff0c\u4e3a\u53ef\u9760\u6027\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2511.04982", "pdf": "https://arxiv.org/pdf/2511.04982", "abs": "https://arxiv.org/abs/2511.04982", "authors": ["Tianxing Ding", "Hongyang Liu", "Yitong Yin", "Can Zhou"], "title": "Tight Bounds for Sampling q-Colorings via Coupling from the Past", "categories": ["cs.DS"], "comment": null, "summary": "The Coupling from the Past (CFTP) paradigm is a canonical method for perfect\nsampling. For uniform sampling of proper $q$-colorings in graphs with maximum\ndegree $\\Delta$, the bounding chains of Huber (STOC 1998) provide a systematic\nframework for efficiently implementing CFTP algorithms within the classical\nregime $q \\ge (1 + o(1))\\Delta^2$. This was subsequently improved to $q >\n3\\Delta$ by Bhandari and Chakraborty (STOC 2020) and to $q \\ge (8/3 +\no(1))\\Delta$ by Jain, Sah, and Sawhney (STOC 2021).\n  In this work, we establish the asymptotically tight threshold for\nbounding-chain-based CFTP algorithms for graph colorings. We prove a lower\nbound showing that all such algorithms satisfying the standard contraction\nproperty require $q \\ge 2.5\\Delta$, and we present an efficient CFTP algorithm\nthat achieves this asymptotically optimal threshold $q \\ge (2.5 + o(1))\\Delta$\nvia an optimal design of bounding chains.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u56fe\u7740\u8272\u95ee\u9898\u4e2d\u57fa\u4e8e\u8fb9\u754c\u94fe\u7684CFTP\u7b97\u6cd5\u7684\u6e10\u8fd1\u7d27\u9608\u503c\uff0c\u8bc1\u660e\u4e86\u6240\u6709\u6ee1\u8db3\u6807\u51c6\u6536\u7f29\u6027\u8d28\u7684\u6b64\u7c7b\u7b97\u6cd5\u9700\u8981q\u22652.5\u0394\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fbe\u5230\u8be5\u6700\u4f18\u9608\u503c\u7684\u6709\u6548CFTP\u7b97\u6cd5\u3002", "motivation": "CFTP\u662f\u5b8c\u7f8e\u91c7\u6837\u7684\u7ecf\u5178\u65b9\u6cd5\uff0c\u4f46\u5728\u56fe\u7740\u8272\u95ee\u9898\u4e2d\uff0c\u73b0\u6709\u57fa\u4e8e\u8fb9\u754c\u94fe\u7684CFTP\u7b97\u6cd5\u5728q\u503c\u8981\u6c42\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u627e\u5230\u6b64\u7c7b\u7b97\u6cd5\u7684\u6e10\u8fd1\u7d27\u9608\u503c\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u6700\u4f18\u7684\u8fb9\u754c\u94fe\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684CFTP\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u5728q\u2265(2.5+o(1))\u0394\u6761\u4ef6\u4e0b\u6709\u6548\u3002", "result": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u8fb9\u754c\u94fe\u7684CFTP\u7b97\u6cd5\u9700\u8981q\u22652.5\u0394\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fbe\u5230\u8be5\u6700\u4f18\u9608\u503c\u7684\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u786e\u5b9a\u4e86\u56fe\u7740\u8272\u95ee\u9898\u4e2d\u57fa\u4e8e\u8fb9\u754c\u94fe\u7684CFTP\u7b97\u6cd5\u7684\u6e10\u8fd1\u7d27\u9608\u503c\u4e3aq\u22652.5\u0394\uff0c\u4e3a\u5b8c\u7f8e\u91c7\u6837\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2511.05149", "pdf": "https://arxiv.org/pdf/2511.05149", "abs": "https://arxiv.org/abs/2511.05149", "authors": ["Cristina Olmedilla", "Jesus Escudero-Sahuquillo", "Pedro J. Garcia", "Francisco J. Quiles", "Jose Duato"], "title": "Improving Injection-Throttling Mechanisms for Congestion Control for Data-center and Supercomputer Interconnects", "categories": ["cs.NI", "cs.AR", "C.2; C.4"], "comment": "4 pages, 3 figures", "summary": "Over the past decade, Supercomputers and Data centers have evolved\ndramatically to cope with the increasing performance requirements of\napplications and services, such as scientific computing, generative AI, social\nnetworks or cloud services. This evolution have led these systems to\nincorporate high-speed networks using faster links, end nodes using multiple\nand dedicated accelerators, or a advancements in memory technologies to bridge\nthe memory bottleneck. The interconnection network is a key element in these\nsystems and it must be thoroughly designed so it is not the bottleneck of the\nentire system, bearing in mind the countless communication operations that\ngenerate current applications and services. Congestion is serious threat that\nspoils the interconnection network performance, and its effects are even more\ndramatic when looking at the traffic dynamics and bottlenecks generated by the\ncommunication operations mentioned above. In this vein, numerous congestion\ncontrol (CC) techniques have been developed to address congestion negative\neffects. One popular example is Data Center Quantized Congestion Notification\n(DCQCN), which allows congestion detection at network switch buffers, then\nmarking congesting packets and notifying about congestion to the sources, which\nfinally apply injection throttling of those packets contributing to congestion.\nWhile DCQCN has been widely studied and improved, its main principles for\ncongestion detection, notification and reaction remain largely unchanged, which\nis an important shortcoming considering congestion dynamics in current\nhigh-performance interconnection networks. In this paper, we revisit the DCQCN\nclosed-loop mechanism and refine its design to leverage a more accurate\ncongestion detection, signaling, and injection throttling, reducing control\ntraffic overhead and avoiding unnecessary throttling of non-congesting flows.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86DCQCN\u62e5\u585e\u63a7\u5236\u673a\u5236\uff0c\u6539\u8fdb\u4e86\u5176\u95ed\u73af\u8bbe\u8ba1\uff0c\u901a\u8fc7\u66f4\u51c6\u786e\u7684\u62e5\u585e\u68c0\u6d4b\u3001\u4fe1\u4ee4\u548c\u6ce8\u5165\u8282\u6d41\u6765\u51cf\u5c11\u63a7\u5236\u6d41\u91cf\u5f00\u9500\uff0c\u907f\u514d\u5bf9\u975e\u62e5\u585e\u6d41\u7684\u4e0d\u5fc5\u8981\u8282\u6d41\u3002", "motivation": "\u968f\u7740\u8d85\u7ea7\u8ba1\u7b97\u673a\u548c\u6570\u636e\u4e2d\u5fc3\u7684\u9ad8\u901f\u53d1\u5c55\uff0c\u4e92\u8fde\u7f51\u7edc\u6210\u4e3a\u5173\u952e\u7ec4\u4ef6\u3002DCQCN\u4f5c\u4e3a\u6d41\u884c\u7684\u62e5\u585e\u63a7\u5236\u6280\u672f\uff0c\u5176\u6838\u5fc3\u7684\u62e5\u585e\u68c0\u6d4b\u3001\u901a\u77e5\u548c\u53cd\u5e94\u673a\u5236\u5728\u73b0\u6709\u9ad8\u6027\u80fd\u4e92\u8fde\u7f51\u7edc\u4e2d\u5df2\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u9002\u5e94\u73b0\u4ee3\u7f51\u7edc\u62e5\u585e\u52a8\u6001\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1DCQCN\u7684\u95ed\u73af\u673a\u5236\uff0c\u6539\u8fdb\u62e5\u585e\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4f18\u5316\u4fe1\u4ee4\u673a\u5236\uff0c\u5e76\u5b8c\u5584\u6ce8\u5165\u8282\u6d41\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u6539\u8fdb\u673a\u5236\u51cf\u5c11\u4e86\u63a7\u5236\u6d41\u91cf\u5f00\u9500\uff0c\u907f\u514d\u4e86\u975e\u62e5\u585e\u6d41\u7684\u4e0d\u5fc5\u8981\u8282\u6d41\uff0c\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5bf9DCQCN\u673a\u5236\u7684\u91cd\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5e94\u5bf9\u73b0\u4ee3\u9ad8\u6027\u80fd\u4e92\u8fde\u7f51\u7edc\u4e2d\u7684\u62e5\u585e\u95ee\u9898\uff0c\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.04986", "pdf": "https://arxiv.org/pdf/2511.04986", "abs": "https://arxiv.org/abs/2511.04986", "authors": ["Mohammadreza Saeidi", "Ethan Thoma", "Raula Gaikovina Kula", "Gema Rodr\u00edguez-P\u00e9rez"], "title": "What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers", "categories": ["cs.SE"], "comment": null, "summary": "Background: Widespread use of third-party libraries makes ecosystems like\nNode Package Manager (npm) critical to modern software development. However,\nthis interconnected chain of dependencies also creates challenges: bugs in one\nlibrary can propagate downstream, potentially impacting many other libraries\nthat rely on it. We hypothesize that maintainers may not always decide to fix a\nbug, especially if the maintainer decides it falls out of their responsibility\nwithin the chain of dependencies. Aims: To confirm this hypothesis, we\ninvestigate the responsiveness of 30,340 bug reports across 500 of the most\ndepended-upon npm packages. Method: We adopt a mixed-method approach to mine\nrepository issue data and perform qualitative open coding to analyze reasons\nbehind unaddressed bug reports. Results: Our findings show that maintainers are\ngenerally responsive, with a median project-level responsiveness of 70% (IQR:\n55%-89%), reflecting their commitment to support downstream developers.\nConclusions: We present a taxonomy of the reasons some bugs remain unresolved.\nThe taxonomy includes contribution practices, dependency constraints, and\nlibrary-specific standards as reasons for not being responsive. Understanding\nmaintainer behavior can inform practices that promote a more robust and\nresponsive open-source ecosystem that benefits the entire community.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86npm\u751f\u6001\u7cfb\u7edf\u4e2d\u7ef4\u62a4\u8005\u5bf9bug\u62a5\u544a\u7684\u54cd\u5e94\u60c5\u51b5\uff0c\u53d1\u73b0\u7ef4\u62a4\u8005\u603b\u4f53\u4e0a\u54cd\u5e94\u79ef\u6781(\u4e2d\u4f4d\u657070%)\uff0c\u4f46\u67d0\u4e9bbug\u56e0\u8d21\u732e\u5b9e\u8df5\u3001\u4f9d\u8d56\u7ea6\u675f\u548c\u5e93\u7279\u5b9a\u6807\u51c6\u7b49\u539f\u56e0\u672a\u88ab\u89e3\u51b3\u3002", "motivation": "npm\u7b49\u751f\u6001\u7cfb\u7edf\u4f9d\u8d56\u94fe\u4e2d\u7684bug\u53ef\u80fd\u4f20\u64ad\u5f71\u54cd\u4e0b\u6e38\uff0c\u5047\u8bbe\u7ef4\u62a4\u8005\u53ef\u80fd\u4e0d\u603b\u662f\u4fee\u590dbug\uff0c\u7279\u522b\u662f\u5f53\u4ed6\u4eec\u8ba4\u4e3a\u8d85\u51fa\u8d23\u4efb\u8303\u56f4\u65f6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5206\u679030,340\u4e2abug\u62a5\u544a\uff0c\u6316\u6398\u4ed3\u5e93\u95ee\u9898\u6570\u636e\u5e76\u8fdb\u884c\u5b9a\u6027\u5f00\u653e\u7f16\u7801\u5206\u6790\u672a\u89e3\u51b3bug\u62a5\u544a\u7684\u539f\u56e0\u3002", "result": "\u7ef4\u62a4\u8005\u901a\u5e38\u54cd\u5e94\u79ef\u6781\uff0c\u9879\u76ee\u7ea7\u54cd\u5e94\u4e2d\u4f4d\u6570\u4e3a70%(IQR:55%-89%)\uff0c\u53cd\u6620\u4e86\u4ed6\u4eec\u5bf9\u4e0b\u6e38\u5f00\u53d1\u8005\u7684\u652f\u6301\u627f\u8bfa\u3002", "conclusion": "\u63d0\u51fa\u4e86bug\u672a\u89e3\u51b3\u539f\u56e0\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u8d21\u732e\u5b9e\u8df5\u3001\u4f9d\u8d56\u7ea6\u675f\u548c\u5e93\u7279\u5b9a\u6807\u51c6\u3002\u7406\u89e3\u7ef4\u62a4\u8005\u884c\u4e3a\u6709\u52a9\u4e8e\u4fc3\u8fdb\u66f4\u5065\u58ee\u548c\u54cd\u5e94\u8fc5\u901f\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2511.04722", "pdf": "https://arxiv.org/pdf/2511.04722", "abs": "https://arxiv.org/abs/2511.04722", "authors": ["Qianyang Li", "Xingjun Zhang", "Peng Tao", "Shaoxun Wang", "Yancheng Pan", "Jia Wei"], "title": "AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Forecasting long-term time series in IoT environments remains a significant\nchallenge due to the non-stationary and multi-scale characteristics of sensor\nsignals. Furthermore, error accumulation causes a decrease in forecast quality\nwhen predicting further into the future. Traditional methods are restricted to\noperate in time-domain, while the global frequency information achieved by\nFourier transform would be regarded as stationary signals leading to blur the\ntemporal patterns of transient events. We propose AWEMixer, an Adaptive\nWavelet-Enhanced Mixer Network including two innovative components: 1) a\nFrequency Router designs to utilize the global periodicity pattern achieved by\nFast Fourier Transform to adaptively weight localized wavelet subband, and 2) a\nCoherent Gated Fusion Block to achieve selective integration of prominent\nfrequency features with multi-scale temporal representation through\ncross-attention and gating mechanism, which realizes accurate time-frequency\nlocalization while remaining robust to noise. Seven public benchmarks validate\nthat our model is more effective than recent state-of-the-art models.\nSpecifically, our model consistently achieves performance improvement compared\nwith transformer-based and MLP-based state-of-the-art models in long-sequence\ntime series forecasting. Code is available at\nhttps://github.com/hit636/AWEMixer", "AI": {"tldr": "AWEMixer\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5c0f\u6ce2\u589e\u5f3a\u6df7\u5408\u7f51\u7edc\uff0c\u901a\u8fc7\u9891\u7387\u8def\u7531\u5668\u548c\u76f8\u5e72\u95e8\u63a7\u878d\u5408\u5757\uff0c\u5728\u65f6\u9891\u57df\u5b9e\u73b0\u7cbe\u786e\u7684\u65f6\u5e8f\u9884\u6d4b\uff0c\u5728IoT\u73af\u5883\u4e2d\u6709\u6548\u89e3\u51b3\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\u3002", "motivation": "IoT\u73af\u5883\u4e2d\u4f20\u611f\u5668\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u591a\u5c3a\u5ea6\u7279\u6027\u4f7f\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u65f6\u57df\u64cd\u4f5c\uff0c\u800c\u5085\u91cc\u53f6\u53d8\u6362\u83b7\u5f97\u7684\u5168\u5c40\u9891\u7387\u4fe1\u606f\u88ab\u89c6\u4e3a\u5e73\u7a33\u4fe1\u53f7\uff0c\u4f1a\u6a21\u7cca\u77ac\u6001\u4e8b\u4ef6\u7684\u65f6\u95f4\u6a21\u5f0f\u3002", "method": "\u63d0\u51faAWEMixer\u7f51\u7edc\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff1a1\uff09\u9891\u7387\u8def\u7531\u5668\u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u83b7\u5f97\u7684\u5168\u5c40\u5468\u671f\u6027\u6a21\u5f0f\u81ea\u9002\u5e94\u52a0\u6743\u5c40\u90e8\u5c0f\u6ce2\u5b50\u5e26\uff1b2\uff09\u76f8\u5e72\u95e8\u63a7\u878d\u5408\u5757\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u7a81\u51fa\u9891\u7387\u7279\u5f81\u4e0e\u591a\u5c3a\u5ea6\u65f6\u95f4\u8868\u793a\u7684\u9009\u62e9\u6027\u96c6\u6210\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u4e8eTransformer\u548cMLP\u7684\u6700\u65b0\u6a21\u578b\uff0c\u5728\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6301\u7eed\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AWEMixer\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u65f6\u9891\u5b9a\u4f4d\uff0c\u540c\u65f6\u5bf9\u566a\u58f0\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728IoT\u73af\u5883\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.04946", "pdf": "https://arxiv.org/pdf/2511.04946", "abs": "https://arxiv.org/abs/2511.04946", "authors": ["Lei Chen", "Erci Xu", "Yiming Sun", "Shengyu Fan", "Xianglong Deng", "Guiming Shi", "Guang Fan", "Liang Kong", "Yilan Zhu", "Shoumeng Yan", "Mingzhe Zhang"], "title": "The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective", "categories": ["cs.CR", "cs.DC"], "comment": "https://link.springer.com/chapter/10.1007/978-981-95-1021-4_25", "summary": "Fully Homomorphic Encryption (FHE) allows computations to be performed on\nencrypted data, significantly enhancing user privacy. However, the I/O\nchallenges associated with deploying FHE applications remains understudied. We\nanalyze the impact of storage I/O on the performance of FHE applications and\nsummarize key lessons from the status quo. Key results include that storage I/O\ncan degrade the performance of ASICs by as much as 357$\\times$ and reduce GPUs\nperformance by up to 22$\\times$.", "AI": {"tldr": "\u5206\u6790\u5b58\u50a8I/O\u5bf9\u5168\u540c\u6001\u52a0\u5bc6(FHE)\u5e94\u7528\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u50a8I/O\u4f1a\u663e\u8457\u964d\u4f4eASIC\u548cGPU\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u5168\u540c\u6001\u52a0\u5bc6\u5141\u8bb8\u5728\u52a0\u5bc6\u6570\u636e\u4e0a\u6267\u884c\u8ba1\u7b97\uff0c\u589e\u5f3a\u7528\u6237\u9690\u79c1\uff0c\u4f46\u90e8\u7f72FHE\u5e94\u7528\u65f6\u7684I/O\u6311\u6218\u7814\u7a76\u4e0d\u8db3", "method": "\u5206\u6790\u5b58\u50a8I/O\u5bf9FHE\u5e94\u7528\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u603b\u7ed3\u73b0\u72b6\u4e2d\u7684\u5173\u952e\u7ecf\u9a8c\u6559\u8bad", "result": "\u5b58\u50a8I/O\u53ef\u4f7fASIC\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe357\u500d\uff0cGPU\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe22\u500d", "conclusion": "\u5b58\u50a8I/O\u662fFHE\u5e94\u7528\u90e8\u7f72\u4e2d\u7684\u91cd\u8981\u74f6\u9888\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u4f18\u5316"}}
{"id": "2511.05295", "pdf": "https://arxiv.org/pdf/2511.05295", "abs": "https://arxiv.org/abs/2511.05295", "authors": ["Jon Kleinberg", "Fan Wei"], "title": "Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations", "categories": ["cs.DS", "cs.CL", "cs.DM", "cs.LG"], "comment": null, "summary": "The success of large language models (LLMs) has motivated formal theories of\nlanguage generation and learning. We study the framework of \\emph{language\ngeneration in the limit}, where an adversary enumerates strings from an unknown\nlanguage $K$ drawn from a countable class, and an algorithm must generate\nunseen strings from $K$. Prior work showed that generation is always possible,\nand that some algorithms achieve positive lower density, revealing a\n\\emph{validity--breadth} trade-off between correctness and coverage. We resolve\na main open question in this line, proving a tight bound of $1/2$ on the best\nachievable lower density. We then strengthen the model to allow \\emph{partial\nenumeration}, where the adversary reveals only an infinite subset $C \\subseteq\nK$. We show that generation in the limit remains achievable, and if $C$ has\nlower density $\\alpha$ in $K$, the algorithm's output achieves density at least\n$\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the\npartial-information setting, where the generator must recover within a factor\n$1/2$ of the revealed subset's density. We further revisit the classical\nGold--Angluin model of \\emph{language identification} under partial\nenumeration. We characterize when identification in the limit is possible --\nwhen hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in\nthe process give a new topological formulation of Angluin's characterization,\nshowing that her condition is precisely equivalent to an appropriate\ntopological space having the $T_D$ separation property.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u751f\u6210\u6781\u9650\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u6700\u4f73\u53ef\u5b9e\u73b0\u4e0b\u754c\u5bc6\u5ea6\u4e3a1/2\u7684\u7d27\u754c\uff0c\u5e76\u5c06\u7ed3\u679c\u63a8\u5e7f\u5230\u90e8\u5206\u679a\u4e3e\u8bbe\u7f6e\uff0c\u540c\u65f6\u91cd\u65b0\u5ba1\u89c6\u4e86Gold-Angluin\u8bed\u8a00\u8bc6\u522b\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u8bed\u8a00\u751f\u6210\u548c\u5b66\u4e60\u7684\u5f62\u5f0f\u7406\u8bba\u7814\u7a76\uff0c\u9700\u8981\u7406\u89e3\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u8bed\u8a00\u751f\u6210\u7684\u53ef\u884c\u6027\u53ca\u5176\u6027\u80fd\u754c\u9650\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u751f\u6210\u6781\u9650\u6846\u67b6\uff0c\u7814\u7a76\u5bf9\u6297\u65b9\u4ece\u672a\u77e5\u8bed\u8a00\u4e2d\u679a\u4e3e\u5b57\u7b26\u4e32\u65f6\u7b97\u6cd5\u7684\u751f\u6210\u80fd\u529b\uff1b\u5f15\u5165\u90e8\u5206\u679a\u4e3e\u6a21\u578b\uff0c\u5206\u6790\u5728\u4ec5\u63ed\u793a\u65e0\u9650\u5b50\u96c6\u60c5\u51b5\u4e0b\u7684\u751f\u6210\u6027\u80fd\uff1b\u91cd\u65b0\u5ba1\u89c6Gold-Angluin\u8bed\u8a00\u8bc6\u522b\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u6700\u4f73\u53ef\u5b9e\u73b0\u4e0b\u754c\u5bc6\u5ea6\u4e3a1/2\u7684\u7d27\u754c\uff1b\u5728\u90e8\u5206\u679a\u4e3e\u8bbe\u7f6e\u4e2d\uff0c\u7b97\u6cd5\u8f93\u51fa\u5bc6\u5ea6\u81f3\u5c11\u4e3a\u03b1/2\uff08\u5176\u4e2d\u03b1\u662f\u63ed\u793a\u5b50\u96c6\u5728\u8bed\u8a00\u4e2d\u7684\u5bc6\u5ea6\uff09\uff1b\u7ed9\u51fa\u4e86\u8bed\u8a00\u8bc6\u522b\u5728\u6781\u9650\u4e0b\u53ef\u884c\u7684\u7279\u5f81\u5316\u6761\u4ef6\u3002", "conclusion": "\u8bed\u8a00\u751f\u6210\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u662f\u53ef\u884c\u7684\uff0c\u5b58\u57281/2\u7684\u5bc6\u5ea6\u754c\u9650\uff1b\u90e8\u5206\u679a\u4e3e\u8bbe\u7f6e\u4e0b\u7684\u7ed3\u679c\u63a8\u5e7f\u4e86\u8fd9\u4e00\u754c\u9650\uff1bAngluin\u7684\u7279\u5f81\u5316\u6761\u4ef6\u7b49\u4ef7\u4e8e\u9002\u5f53\u62d3\u6251\u7a7a\u95f4\u5177\u6709T_D\u5206\u79bb\u6027\u8d28\u3002"}}
{"id": "2511.05238", "pdf": "https://arxiv.org/pdf/2511.05238", "abs": "https://arxiv.org/abs/2511.05238", "authors": ["Peide Li", "Liu Cao", "Lyutianyang Zhang", "Dongyu Wei", "Ye Hu", "Qipeng Xie"], "title": "EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme", "categories": ["cs.NI", "68T05, 90C26, 68M10", "I.2.11; C.2.1; C.4; G.3"], "comment": "Approx. 12 pages, 3 figures, 3 tables; focuses on 6G heterogeneous\n  radio environment digital twin construction via personalized federated\n  learning", "summary": "Radio Environment Map (REM) is transitioning from 5G homogeneous environments\nto B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),\na natural fit for this distributed task, struggles with performance degradation\nin accuracy and communication efficiency under the non-independent and\nidentically distributed (Non-IID) data conditions inherent to these new\nenvironments. This paper proposes EPFL-REMNet, an efficient personalized\nfederated framework for constructing a high-fidelity digital twin of the 6G\nheterogeneous radio environment. The proposed EPFL-REMNet employs a\"shared\nbackbone + lightweight personalized head\" model, where only the compressed\nshared backbone is transmitted between the server and clients, while each\nclient's personalized head is maintained locally. We tested EPFL-REMNet by\nconstructing three distinct Non-IID scenarios (light, medium, and heavy) based\non radio environment complexity, with data geographically partitioned across 90\nclients. Experimental results demonstrate that EPFL-REMNet simultaneously\nachieves higher digital twin fidelity (accuracy) and lower uplink overhead\nacross all Non-IID settings compared to standard FedAvg and recent\nstate-of-the-art methods. Particularly, it significantly reduces performance\ndisparities across datasets and improves local map accuracy for long-tail\nclients, enhancing the overall integrity of digital twin.", "AI": {"tldr": "EPFL-REMNet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa6G\u5f02\u6784\u65e0\u7ebf\u7535\u73af\u5883\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\uff0c\u901a\u8fc7\u5171\u4eab\u4e3b\u5e72+\u8f7b\u91cf\u4e2a\u6027\u5316\u5934\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u5728\u975eIID\u6570\u636e\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u57286G\u5f02\u6784\u65e0\u7ebf\u7535\u73af\u5883\u5730\u56fe\u6784\u5efa\u4e2d\uff0c\u9762\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u65f6\u51fa\u73b0\u7684\u6027\u80fd\u4e0b\u964d\u548c\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\"\u5171\u4eab\u4e3b\u5e72+\u8f7b\u91cf\u4e2a\u6027\u5316\u5934\"\u6a21\u578b\u67b6\u6784\uff0c\u4ec5\u4f20\u8f93\u538b\u7f29\u7684\u5171\u4eab\u4e3b\u5e72\uff0c\u5ba2\u6237\u7aef\u672c\u5730\u7ef4\u62a4\u4e2a\u6027\u5316\u5934\uff0c\u572890\u4e2a\u5ba2\u6237\u7aef\u7684\u5730\u7406\u5206\u533a\u6570\u636e\u4e0a\u6784\u5efa\u4e09\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u975eIID\u573a\u666f\u3002", "result": "\u5728\u6240\u6709\u975eIID\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6807\u51c6FedAvg\u548c\u6700\u65b0\u65b9\u6cd5\uff0cEPFL-REMNet\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6570\u5b57\u5b6a\u751f\u4fdd\u771f\u5ea6\u548c\u66f4\u4f4e\u7684\u4e0a\u884c\u5f00\u9500\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u957f\u5c3e\u5ba2\u6237\u7aef\u7684\u672c\u5730\u5730\u56fe\u7cbe\u5ea6\u3002", "conclusion": "EPFL-REMNet\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e866G\u5f02\u6784\u65e0\u7ebf\u7535\u73af\u5883\u6570\u5b57\u5b6a\u751f\u7684\u6574\u4f53\u5b8c\u6574\u6027\uff0c\u5728\u975eIID\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u7684\u53cc\u91cd\u4f18\u5316\u3002"}}
{"id": "2511.05165", "pdf": "https://arxiv.org/pdf/2511.05165", "abs": "https://arxiv.org/abs/2511.05165", "authors": ["Ahmad Hatahet", "Christoph Knieke", "Andreas Rausch"], "title": "Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9006\u5411\u5de5\u7a0b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4ece\u6e90\u4ee3\u7801\u751f\u6210\u8f6f\u4ef6\u67b6\u6784\u63cf\u8ff0\u6587\u6863\uff0c\u89e3\u51b3\u4f20\u7edf\u6587\u6863\u7f3a\u5931\u3001\u8fc7\u65f6\u6216\u4e0e\u5b9e\u73b0\u4e0d\u7b26\u7684\u95ee\u9898\u3002", "motivation": "\u8f6f\u4ef6\u67b6\u6784\u63cf\u8ff0\u6587\u6863\u5728\u5b9e\u8df5\u4e2d\u5e38\u5e38\u7f3a\u5931\u3001\u8fc7\u65f6\u6216\u4e0e\u7cfb\u7edf\u5b9e\u9645\u5b9e\u73b0\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u9700\u8981\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u83b7\u53d6\u67b6\u6784\u4fe1\u606f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u8017\u65f6\u4e14\u589e\u52a0\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5f71\u54cd\u65b0\u5f00\u53d1\u8005\u4e0a\u624b\u548c\u7cfb\u7edf\u957f\u671f\u7ef4\u62a4\u3002", "method": "\u6574\u5408\u9006\u5411\u5de5\u7a0b\u6280\u672f\u548cLLM\uff0c\u901a\u8fc7\u63d0\u53d6\u5b8c\u6574\u7ec4\u4ef6\u56fe\u3001\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u7b5b\u9009\u67b6\u6784\u91cd\u8981\u5143\u7d20\uff08\u6838\u5fc3\u7ec4\u4ef6\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4ee3\u7801\u903b\u8f91\u751f\u6210\u72b6\u6001\u673a\u56fe\u6765\u6062\u590d\u9759\u6001\u548c\u884c\u4e3a\u67b6\u6784\u89c6\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u53ef\u6269\u5c55\u4e14\u53ef\u7ef4\u62a4\u7684\u67b6\u6784\u89c6\u56fe\u8868\u793a\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u5de5\u4e13\u5bb6\u53c2\u4e0e\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u51c6\u786e\u8868\u793a\u590d\u6742\u8f6f\u4ef6\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u65f6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3001\u589e\u5f3a\u7cfb\u7edf\u7406\u89e3\u548c\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5c55\u793a\u4e86LLM\u5728\u8f6f\u4ef6\u67b6\u6784\u6587\u6863\u751f\u6210\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.04723", "pdf": "https://arxiv.org/pdf/2511.04723", "abs": "https://arxiv.org/abs/2511.04723", "authors": ["Mohamadreza Akbari Pour", "Mohamad Sadeq Karimi", "Amir Hossein Mazloumi"], "title": "Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Health prediction is crucial for ensuring reliability, minimizing downtime,\nand optimizing maintenance in industrial systems. Remaining Useful Life (RUL)\nprediction is a key component of this process; however, many existing models\nstruggle to capture fine-grained temporal dependencies while dynamically\nprioritizing critical features across time for robust prognostics. To address\nthese challenges, we propose a novel framework that integrates Temporal\nConvolutional Networks (TCNs) for localized temporal feature extraction with a\nmodified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.\nThis architecture effectively bridges short- and long-term dependencies while\nemphasizing salient temporal patterns. Furthermore, the incorporation of a\nmulti-time-window methodology improves adaptability across diverse operating\nconditions. Extensive evaluations on benchmark datasets demonstrate that the\nproposed model reduces the average RMSE by up to 5.5%, underscoring its\nimproved predictive accuracy compared to state-of-the-art methods. By closing\ncritical gaps in current approaches, this framework advances the effectiveness\nof industrial prognostic systems and highlights the potential of advanced\ntime-series transformers for RUL prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TCN\u548c\u589e\u5f3a\u578bTFT\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u7cfb\u7edf\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\uff0c\u901a\u8fc7\u591a\u65f6\u95f4\u7a97\u53e3\u65b9\u6cd5\u63d0\u9ad8\u9002\u5e94\u6027\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747RMSE\u964d\u4f4e5.5%\u3002", "motivation": "\u73b0\u6709RUL\u9884\u6d4b\u6a21\u578b\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u65e0\u6cd5\u52a8\u6001\u4f18\u5148\u5904\u7406\u5173\u952e\u7279\u5f81\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u65f6\u95f4\u5377\u79ef\u7f51\u7edc(TCN)\u8fdb\u884c\u5c40\u90e8\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408\u7531Bi-LSTM\u7f16\u7801\u5668-\u89e3\u7801\u5668\u589e\u5f3a\u7684\u6539\u8fdb\u578b\u65f6\u95f4\u878d\u5408\u53d8\u6362\u5668(TFT)\uff0c\u91c7\u7528\u591a\u65f6\u95f4\u7a97\u53e3\u65b9\u6cd5\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5e73\u5747RMSE\u964d\u4f4e\u9ad8\u8fbe5.5%\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f25\u8865\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u5de5\u4e1a\u9884\u6d4b\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u51f8\u663e\u4e86\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u53d8\u6362\u5668\u5728RUL\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.05209", "pdf": "https://arxiv.org/pdf/2511.05209", "abs": "https://arxiv.org/abs/2511.05209", "authors": ["Jorge V\u00e1zquez-P\u00e9rez", "Daniel Exp\u00f3sito-Pati\u00f1o", "Marta Losada", "\u00c1lvaro Carballido", "Andr\u00e9s G\u00f3mez", "Tom\u00e1s F. Pena"], "title": "CUNQA: a Distributed Quantum Computing emulator for HPC", "categories": ["quant-ph", "cs.DC", "D.1.3; J.2; D.2.11"], "comment": null, "summary": "The challenge of scaling quantum computers to gain computational power is\nexpected to lead to architectures with multiple connected quantum processing\nunits (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In\nparallel, there is a growing momentum toward treating quantum computers as\naccelerators, integrating them into the heterogeneous architectures of\nhigh-performance computing (HPC) environments. This work combines these two\nforeseeable futures in CUNQA, an open-source DQC emulator designed for HPC\nenvironments that allows testing, evaluating and studying DQC in HPC before it\neven becomes real. It implements the three DQC models of no-communication,\nclassical-communication and quantum-communication; which will be examined in\nthis work. Addressing programming considerations, explaining emulation and\nsimulation details, and delving into the specifics of the implementation will\nbe part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm\nis used to demonstrate and analyze the emulation of the models. To the best of\nour knowledge, CUNQA is the first tool designed to emulate the three DQC\nschemes in an HPC environment.", "AI": {"tldr": "CUNQA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u5668\uff0c\u4e13\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u8bbe\u8ba1\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u5b9e\u73b0\u4e4b\u524d\u8fdb\u884c\u6d4b\u8bd5\u3001\u8bc4\u4f30\u548c\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u673a\u5411\u591a\u91cf\u5b50\u5904\u7406\u5355\u5143\u67b6\u6784\u53d1\u5c55\uff0c\u4ee5\u53ca\u91cf\u5b50\u8ba1\u7b97\u673a\u4f5c\u4e3a\u52a0\u901f\u5668\u96c6\u6210\u5230\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u8d8b\u52bf\uff0c\u9700\u8981\u5de5\u5177\u6765\u6a21\u62df\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u5728HPC\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u3002", "method": "\u5f00\u53d1\u4e86CUNQA\u6a21\u62df\u5668\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u6a21\u578b\uff1a\u65e0\u901a\u4fe1\u3001\u7ecf\u5178\u901a\u4fe1\u548c\u91cf\u5b50\u901a\u4fe1\uff0c\u5e76\u4f7f\u7528\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u8fdb\u884c\u6f14\u793a\u548c\u5206\u6790\u3002", "result": "CUNQA\u662f\u9996\u4e2a\u5728HPC\u73af\u5883\u4e2d\u6a21\u62df\u4e09\u79cd\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u65b9\u6848\u7684\u4e13\u7528\u5de5\u5177\uff0c\u80fd\u591f\u5904\u7406\u7f16\u7a0b\u8003\u8651\u3001\u6a21\u62df\u7ec6\u8282\u548c\u5b9e\u73b0\u5177\u4f53\u95ee\u9898\u3002", "conclusion": "CUNQA\u4e3a\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6a21\u62df\u5e73\u53f0\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u5de5\u5177\u7a7a\u767d\u3002"}}
{"id": "2511.05285", "pdf": "https://arxiv.org/pdf/2511.05285", "abs": "https://arxiv.org/abs/2511.05285", "authors": ["Kenny Be\u0161ter \u0160torgel", "Cl\u00e9ment Dallard", "Vadim Lozin", "Martin Milani\u010d", "Viktor Zamaraev"], "title": "Awesome graph parameters", "categories": ["math.CO", "cs.DM", "cs.DS", "05C75 (Primary), 05D10, 05C69, 05C65, 05C85 (Secondary)"], "comment": null, "summary": "For a graph $G$, we denote by $\\alpha(G)$ the size of a maximum independent\nset and by $\\omega(G)$ the size of a maximum clique in $G$. Our paper lies on\nthe edge of two lines of research, related to $\\alpha$ and $\\omega$,\nrespectively. One of them studies $\\alpha$-variants of graph parameters, such\nas $\\alpha$-treewidth or $\\alpha$-degeneracy. The second line deals with graph\nclasses where some parameters are bounded by a function of $\\omega(G)$. A\nfamous example of this type is the family of $\\chi$-bounded classes, where the\nchromatic number $\\chi(G)$ is bounded by a function of $\\omega(G)$.\n  A Ramsey-type argument implies that if the $\\alpha$-variant of a graph\nparameter $\\rho$ is bounded by a constant in a class $\\mathcal{G}$, then $\\rho$\nis bounded by a function of $\\omega$ in $\\mathcal{G}$. If the reverse\nimplication also holds, we say that $\\rho$ is awesome. Otherwise, we say that\n$\\rho$ is awful. In the present paper, we identify a number of awesome and\nawful graph parameters, derive some algorithmic applications of awesomeness,\nand propose a number of open problems related to these notions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.05334", "pdf": "https://arxiv.org/pdf/2511.05334", "abs": "https://arxiv.org/abs/2511.05334", "authors": ["Giovanni Fiaschi", "Carlo Vitucci", "Thomas Westerb\u00e4ck", "Daniel Sundmark", "Thomas Nolte"], "title": "A Formal Model for Path Set Attribute Calculation in Network Systems", "categories": ["cs.NI", "F.2.2; G.2.2"], "comment": "8 pages, 3 figures, to be published in the proceedings of the IEEE\n  International Symposium on Networks, Computers and Communications (ISNCC'25),\n  27-28 Oct. 2025", "summary": "In graph theory and its practical networking applications, e.g.,\ntelecommunications and transportation, the problem of finding paths has\nparticular importance. Selecting paths requires giving scores to the\nalternative solutions to drive a choice. While previous studies have provided\ncomprehensive evaluation of single-path solutions, the same level of detail is\nlacking when considering sets of paths. This paper emphasizes that the path\ncharacterization strongly depends on the properties under consideration. While\nproperty-based characterization is also valid for single paths, it becomes\ncrucial to analyse multiple path sets. From the above consideration, this paper\nproposes a mathematical approach, defining a functional model that lends itself\nwell to characterizing the path set in its general formulation. The paper shows\nhow the functional model contextualizes specific attributes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u65b9\u6cd5\u6765\u8bc4\u4f30\u8def\u5f84\u96c6\u5408\uff0c\u5f3a\u8c03\u8def\u5f84\u7279\u6027\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8def\u5f84\u573a\u666f\u4e0b\u3002", "motivation": "\u5728\u56fe\u8bba\u548c\u7f51\u7edc\u5e94\u7528\u4e2d\uff0c\u8def\u5f84\u9009\u62e9\u9700\u8981\u5bf9\u5907\u9009\u65b9\u6848\u8fdb\u884c\u8bc4\u5206\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5bf9\u5355\u4e00\u8def\u5f84\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u4f46\u5bf9\u8def\u5f84\u96c6\u5408\u7684\u5206\u6790\u7f3a\u4e4f\u540c\u7b49\u6df1\u5ea6\u3002\u8def\u5f84\u7279\u6027\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u6240\u8003\u8651\u7684\u6027\u8d28\uff0c\u8fd9\u5728\u591a\u8def\u5f84\u5206\u6790\u4e2d\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u65b9\u6cd5\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u529f\u80fd\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5f88\u597d\u5730\u8868\u5f81\u4e00\u822c\u5f62\u5f0f\u4e0b\u7684\u8def\u5f84\u96c6\u5408\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u5c06\u7279\u5b9a\u5c5e\u6027\u7f6e\u4e8e\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u529f\u80fd\u6a21\u578b\u5982\u4f55\u5c06\u7279\u5b9a\u5c5e\u6027\u7f6e\u4e8e\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u8868\u5f81\uff0c\u4e3a\u8def\u5f84\u96c6\u5408\u5206\u6790\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8def\u5f84\u96c6\u5408\u7684\u7279\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5728\u591a\u8def\u5f84\u573a\u666f\u4e2d\u57fa\u4e8e\u5c5e\u6027\u8fdb\u884c\u8def\u5f84\u8868\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.05205", "pdf": "https://arxiv.org/pdf/2511.05205", "abs": "https://arxiv.org/abs/2511.05205", "authors": ["Huimin Hu", "Michael Pradel"], "title": "CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits", "categories": ["cs.SE"], "comment": null, "summary": "During software evolution, developers commonly face the problem of mapping a\nspecific code region from one commit to another. For example, they may want to\ndetermine how the condition of an if-statement, a specific line in a\nconfiguration file, or the definition of a function changes. We call this the\ncode mapping problem. Existing techniques, such as git diff, address this\nproblem only insufficiently because they show all changes made to a file\ninstead of focusing on a code region of the developer's choice. Other\ntechniques focus on specific code elements and programming languages (e.g.,\nmethods in Java), limiting their applicability. This paper introduces\nCodeMapper, an approach to address the code mapping problem in a way that is\nindependent of specific program elements and programming languages. Given a\ncode region in one commit, CodeMapper finds the corresponding region in another\ncommit. The approach consists of two phases: (i) computing candidate regions by\nanalyzing diffs, detecting code movements, and searching for specific code\nfragments, and (ii) selecting the most likely target region by calculating\nsimilarities. Our evaluation applies CodeMapper to four datasets, including two\nnew hand-annotated datasets containing code region pairs in ten popular\nprogramming languages. CodeMapper correctly identifies the expected target\nregion in 71.0%--94.5% of all cases, improving over the best available\nbaselines by 1.5--58.8 absolute percent points.", "AI": {"tldr": "CodeMapper\u662f\u4e00\u4e2a\u89e3\u51b3\u4ee3\u7801\u6620\u5c04\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u63d0\u4ea4\u4e4b\u95f4\u627e\u5230\u5bf9\u5e94\u4ee3\u7801\u533a\u57df\uff0c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u6216\u7a0b\u5e8f\u5143\u7d20\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5982git diff\u65e0\u6cd5\u4e13\u6ce8\u4e8e\u5f00\u53d1\u8005\u9009\u62e9\u7684\u7279\u5b9a\u4ee3\u7801\u533a\u57df\uff0c\u800c\u5176\u4ed6\u6280\u672f\u53c8\u5c40\u9650\u4e8e\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u548c\u4ee3\u7801\u5143\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u4ee3\u7801\u6620\u5c04\u65b9\u6cd5\u3002", "method": "CodeMapper\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u5206\u6790\u5dee\u5f02\u3001\u68c0\u6d4b\u4ee3\u7801\u79fb\u52a8\u548c\u641c\u7d22\u7279\u5b9a\u4ee3\u7801\u7247\u6bb5\u6765\u8ba1\u7b97\u5019\u9009\u533a\u57df\uff1b2) \u901a\u8fc7\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u9009\u62e9\u6700\u53ef\u80fd\u7684\u76ee\u6807\u533a\u57df\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCodeMapper\u572871.0%-94.5%\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u8bc6\u522b\u9884\u671f\u76ee\u6807\u533a\u57df\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e861.5-58.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CodeMapper\u63d0\u4f9b\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u7f16\u7a0b\u8bed\u8a00\u548c\u7a0b\u5e8f\u5143\u7d20\u7684\u4ee3\u7801\u6620\u5c04\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.04751", "pdf": "https://arxiv.org/pdf/2511.04751", "abs": "https://arxiv.org/abs/2511.04751", "authors": ["Matteo Cercola", "Michele Lomuscio", "Dario Piga", "Simone Formentin"], "title": "Regularized GLISp for sensor-guided human-in-the-loop optimization", "categories": ["cs.LG"], "comment": null, "summary": "Human-in-the-loop calibration is often addressed via preference-based\noptimization, where algorithms learn from pairwise comparisons rather than\nexplicit cost evaluations. While effective, methods such as Preferential\nBayesian Optimization or Global optimization based on active preference\nlearning with radial basis functions (GLISp) treat the system as a black box\nand ignore informative sensor measurements. In this work, we introduce a\nsensor-guided regularized extension of GLISp that integrates measurable\ndescriptors into the preference-learning loop through a physics-informed\nhypothesis function and a least-squares regularization term. This injects\ngrey-box structure, combining subjective feedback with quantitative sensor\ninformation while preserving the flexibility of preference-based search.\nNumerical evaluations on an analytical benchmark and on a human-in-the-loop\nvehicle suspension tuning task show faster convergence and superior final\nsolutions compared to baseline GLISp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u611f\u5668\u5f15\u5bfc\u7684GLISp\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u6d4b\u91cf\u7684\u4f20\u611f\u5668\u6570\u636e\u5230\u504f\u597d\u5b66\u4e60\u5faa\u73af\u4e2d\uff0c\u7ed3\u5408\u4e3b\u89c2\u53cd\u9988\u4e0e\u5b9a\u91cf\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u6821\u51c6\u6548\u7387\u548c\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u504f\u597d\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u5982GLISp\uff09\u5c06\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u7565\u4e86\u4fe1\u606f\u4e30\u5bcc\u7684\u4f20\u611f\u5668\u6d4b\u91cf\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u6821\u51c6\u6548\u7387\u3002", "method": "\u5f15\u5165\u4f20\u611f\u5668\u5f15\u5bfc\u7684\u6b63\u5219\u5316GLISp\u6269\u5c55\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u5047\u8bbe\u51fd\u6570\u548c\u6700\u5c0f\u4e8c\u4e58\u6b63\u5219\u5316\u9879\u5c06\u53ef\u6d4b\u91cf\u63cf\u8ff0\u7b26\u6574\u5408\u5230\u504f\u597d\u5b66\u4e60\u5faa\u73af\u4e2d\u3002", "result": "\u5728\u5206\u6790\u57fa\u51c6\u548c\u4eba\u5728\u73af\u8f66\u8f86\u60ac\u67b6\u8c03\u8c10\u4efb\u52a1\u4e0a\u7684\u6570\u503c\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebfGLISp\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f18\u7684\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f20\u611f\u5668\u5f15\u5bfc\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u7070\u76d2\u7ed3\u6784\u6ce8\u5165\u504f\u597d\u5b66\u4e60\uff0c\u7ed3\u5408\u4e86\u4e3b\u89c2\u53cd\u9988\u4e0e\u5b9a\u91cf\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u641c\u7d22\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2511.05434", "pdf": "https://arxiv.org/pdf/2511.05434", "abs": "https://arxiv.org/abs/2511.05434", "authors": ["Han Huang", "Pakawut Jiradilok", "Elchanan Mossel"], "title": "Reconstructing Riemannian Metrics From Random Geometric Graphs", "categories": ["math.PR", "cs.CG", "cs.DS"], "comment": null, "summary": "Random geometric graphs are random graph models defined on metric measure\nspaces. A random geometric graph is generated by first sampling points from a\nmetric space and then connecting each pair of sampled points independently with\na probability that depends on their distance.\n  In recent work of Huang, Jiradilok, and Mossel~\\cite{HJM24}, the authors\nstudy the problem of reconstructing an embedded manifold form a random\ngeometric graph sampled from the manifold, where edge probabilities depend\nmonotonically on the Euclidean distance between the embedded points. They show\nthat, under mild regularity assumptions on the manifold, the sampling measure,\nand the connection probability function, it is possible to recover the pairwise\nEuclidean distances of the embedded sampled points up to a vanishing error as\nthe number of vertices grows.\n  In this work we consider a similar and arguably more natural problem where\nthe metric is the Riemannian metric on the manifold. Again points are sampled\nfrom the manifold and a random graph is generated where the connection\nprobability is monotone in the Riemannian distance. Perhaps surprisingly we\nobtain stronger results in this setup.\n  Unlike the previous work that only considered dense graph we provide\nreconstruction algorithms from sparse graphs with average degree $n^{1/2}{\\rm\npolylog}(n)$, where $n$ denotes the number of vertices. Our algorithm is also a\nmore efficient algorithm for distance reconstruction with improved error\nbounds. The running times of the algorithm is\n  $O(n^2\\,{\\rm polylog}(n))$ which up to polylog factor matches the size of the\ninput graph.\n  Our distance error also nearly matches the volumetric lower bounds for\ndistance estimation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u9ece\u66fc\u6d41\u5f62\u4e0a\u91c7\u6837\u7684\u7a00\u758f\u968f\u673a\u51e0\u4f55\u56fe\u4e2d\u91cd\u5efa\u70b9\u95f4\u9ece\u66fc\u8ddd\u79bb\u7684\u95ee\u9898\uff0c\u76f8\u6bd4\u4e4b\u524d\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u7684\u5de5\u4f5c\uff0c\u5728\u7a00\u758f\u56fe\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u66f4\u5f3a\u7684\u7ed3\u679c\u3002", "motivation": "\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u7814\u7a76\u4ece\u5d4c\u5165\u6d41\u5f62\u7684\u968f\u673a\u51e0\u4f55\u56fe\u4e2d\u6062\u590d\u6b27\u6c0f\u8ddd\u79bb\uff0c\u800c\u672c\u6587\u8003\u8651\u66f4\u81ea\u7136\u7684\u9ece\u66fc\u5ea6\u91cf\u8bbe\u7f6e\uff0c\u65e8\u5728\u4ece\u7a00\u758f\u56fe\u4e2d\u9ad8\u6548\u91cd\u5efa\u9ece\u66fc\u8ddd\u79bb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u4ece\u5e73\u5747\u5ea6\u4e3an^{1/2}polylog(n)\u7684\u7a00\u758f\u968f\u673a\u51e0\u4f55\u56fe\u4e2d\u91cd\u5efa\u9ece\u66fc\u8ddd\u79bb\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3aO(n^2 polylog(n))\u3002", "result": "\u7b97\u6cd5\u5728\u7a00\u758f\u56fe\u8bbe\u7f6e\u4e0b\u6210\u529f\u91cd\u5efa\u9ece\u66fc\u8ddd\u79bb\uff0c\u8bef\u5dee\u63a5\u8fd1\u4f53\u79ef\u4f30\u8ba1\u7684\u4e0b\u754c\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u4e0e\u8f93\u5165\u56fe\u5927\u5c0f\u5339\u914d\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\u3002", "conclusion": "\u5728\u9ece\u66fc\u5ea6\u91cf\u8bbe\u7f6e\u4e0b\uff0c\u53ef\u4ee5\u4ece\u7a00\u758f\u968f\u673a\u51e0\u4f55\u56fe\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u91cd\u5efa\u70b9\u95f4\u8ddd\u79bb\uff0c\u7ed3\u679c\u4f18\u4e8e\u4e4b\u524d\u7684\u6b27\u6c0f\u8ddd\u79bb\u65b9\u6cd5\u3002"}}
{"id": "2511.05362", "pdf": "https://arxiv.org/pdf/2511.05362", "abs": "https://arxiv.org/abs/2511.05362", "authors": ["Lucian Trestioreanu", "Flaviene Scheidt", "Wazen Shbair", "Jerome Francois", "Damien Magoni", "Radu State"], "title": "To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger", "categories": ["cs.NI"], "comment": "7 pages", "summary": "With the large increase in the adoption of blockchain technologies, their\nunderlying peer-to-peer networks must also scale with the demand. In this\ncontext, previous works highlighted the importance of ensuring efficient and\nresilient communication for the underlying consensus and replication\nmechanisms. However, they were mainly focused on mainstream,\nProof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.\n  In this paper, the problem is investigated in the context of\nconsensus-validation based blockchains, like the XRP Ledger. The latter relies\non a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to\nhave a good scalability in regards to transaction throughput. However, it is\nknown that significant increases in the size of the XRP Ledger network would be\nchallenging to achieve. The main reason is the flooding mechanism used to\ndisseminate the messages related to the consensus protocol, which creates many\nduplicates in the network. Squelching is a recent solution proposed for\nlimiting this duplication, however, it was never evaluated quantitatively in\nreal-life scenarios involving the XRPL production network. In this paper, our\naim is to assess this mechanism using a real-life controllable testbed and the\nXRPL production network, to assess its benefit and compare it to alternative\nsolutions relying on Named Data Networking and on a gossip-based approach.", "AI": {"tldr": "\u8bc4\u4f30XRP\u8d26\u672c\u7f51\u7edc\u4e2dSquelching\u673a\u5236\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u6548\u679c\uff0c\u5e76\u4e0e\u57fa\u4e8e\u547d\u540d\u6570\u636e\u7f51\u7edc\u548cgossip\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u6280\u672f\u91c7\u7528\u7387\u5927\u5e45\u589e\u957f\uff0c\u5e95\u5c42P2P\u7f51\u7edc\u9700\u8981\u76f8\u5e94\u6269\u5c55\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6bd4\u7279\u5e01\u3001\u4ee5\u592a\u574a\u7b49\u4e3b\u6d41PoW\u533a\u5757\u94fe\uff0c\u800c\u5171\u8bc6\u9a8c\u8bc1\u578b\u533a\u5757\u94fe\uff08\u5982XRP\u8d26\u672c\uff09\u7684\u901a\u4fe1\u6548\u7387\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76", "method": "\u4f7f\u7528\u771f\u5b9e\u53ef\u63a7\u6d4b\u8bd5\u5e8a\u548cXRPL\u751f\u4ea7\u7f51\u7edc\uff0c\u5b9a\u91cf\u8bc4\u4f30Squelching\u673a\u5236\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u57fa\u4e8e\u547d\u540d\u6570\u636e\u7f51\u7edc\u548cgossip\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u8fdb\u884c\u5bf9\u6bd4", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u6d4b\u8bd5\u8bc4\u4f30Squelching\u673a\u5236\u7684\u6709\u6548\u6027\uff0c\u4e3aXRP\u8d26\u672c\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e"}}
{"id": "2511.05297", "pdf": "https://arxiv.org/pdf/2511.05297", "abs": "https://arxiv.org/abs/2511.05297", "authors": ["Mohammed Hilel", "Yannis Karmim", "Jean De Bodinat", "Reda Sarehane", "Antoine Gillon"], "title": "Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6846\u67b6\uff0c\u81ea\u52a8\u5c06\u4f01\u4e1aWeb\u5e94\u7528\u8f6c\u6362\u4e3a\u72b6\u6001-\u52a8\u4f5c\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7fLLM\u80fd\u591f\u751f\u6210\u57fa\u4e8e\u5b9e\u9645\u8f6f\u4ef6\u754c\u9762\u7684\u53ef\u9760\u6307\u5bfc", "motivation": "\u4f20\u7edf\u6570\u5b57\u91c7\u7528\u5e73\u53f0\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6784\u5efa\u7ef4\u62a4\u6307\u5357\uff0c\u800c\u76f4\u63a5\u4f7f\u7528LLM\u4f5c\u4e3a\u865a\u62df\u52a9\u624b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u4e14\u4e0d\u53ef\u9760\uff0c\u540c\u65f6\u751f\u4ea7\u7ea7LLM\u65e0\u6cd5\u5fae\u8c03", "method": "\u5f00\u53d1\u56fe\u57fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u81ea\u52a8\u63d0\u53d6\u548c\u7ed3\u6784\u5316\u8f6f\u4ef6\u754c\u9762\u4e3a\u72b6\u6001-\u52a8\u4f5c\u77e5\u8bc6\u56fe\u8c31\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u6d41\u7a0b", "result": "\u6846\u67b6\u4e0eRAKAM\u548cLemon Learning\u5408\u4f5c\u5f00\u53d1\uff0c\u5df2\u96c6\u6210\u5230\u751f\u4ea7DAP\u5de5\u4f5c\u6d41\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u5e7b\u89c9\u95ee\u9898", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u4f01\u4e1a\u8f6f\u4ef6\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u9645\u754c\u9762\u7684\u53ef\u9760\u6307\u5bfc\uff0c\u8ba8\u8bba\u4e86\u5de5\u4e1a\u7528\u4f8b\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u90e8\u7f72\u7ecf\u9a8c"}}
{"id": "2511.04760", "pdf": "https://arxiv.org/pdf/2511.04760", "abs": "https://arxiv.org/abs/2511.04760", "authors": ["Vaibhav Singh", "Eugene Belilovsky", "Rahaf Aljundi"], "title": "When Data Falls Short: Grokking Below the Critical Threshold", "categories": ["cs.LG"], "comment": "6 pages", "summary": "In this paper, we investigate the phenomenon of grokking, where models\nexhibit delayed generalization following overfitting on training data. We focus\non data-scarce regimes where the number of training samples falls below the\ncritical threshold, making grokking unobservable, and on practical scenarios\ninvolving distribution shift. We first show that Knowledge Distillation (KD)\nfrom a model that has already grokked on a distribution (p1) can induce and\naccelerate grokking on a different distribution (p2), even when the available\ndata lies below the critical threshold. This highlights the value of KD for\ndeployed models that must adapt to new distributions under limited data. We\nthen study training on the joint distribution (p1, p2) and demonstrate that\nwhile standard supervised training fails when either distribution has\ninsufficient data, distilling from models grokked on the individual\ndistributions enables generalization. Finally, we examine a continual\npretraining setup, where a grokked model transitions from p1 to p2, and find\nthat KD both accelerates generalization and mitigates catastrophic forgetting,\nachieving strong performance even with only 10% of the data. Together, our\nresults provide new insights into the mechanics of grokking under knowledge\ntransfer and underscore the central role of KD in enabling generalization in\nlow-data and evolving distribution settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u77e5\u8bc6\u84b8\u998f\u5982\u4f55\u8bf1\u5bfc\u548c\u52a0\u901f\u6a21\u578b\u7684grokking\u73b0\u8c61\uff0c\u4ee5\u53ca\u5728\u8054\u5408\u5206\u5e03\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u6570\u636e\u7a00\u7f3a\uff08\u8bad\u7ec3\u6837\u672c\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\uff09\u548c\u5206\u5e03\u504f\u79fb\u7684\u5b9e\u9645\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6765\u8bf1\u5bfc\u548c\u52a0\u901f\u6a21\u578b\u7684grokking\u73b0\u8c61\uff0c\u89e3\u51b3\u90e8\u7f72\u6a21\u578b\u5728\u65b0\u5206\u5e03\u4e0b\u6570\u636e\u6709\u9650\u65f6\u7684\u9002\u5e94\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u4ece\u5df2\u5728\u5206\u5e03p1\u4e0agrokked\u7684\u6a21\u578b\u84b8\u998f\u5230\u65b0\u5206\u5e03p2\uff1b\u7814\u7a76\u5728\u8054\u5408\u5206\u5e03(p1,p2)\u4e0a\u7684\u8bad\u7ec3\uff1b\u4ee5\u53ca\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u4ecep1\u5230p2\u7684\u8fc7\u6e21\u3002", "result": "\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u5728\u6570\u636e\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\u8bf1\u5bfc\u548c\u52a0\u901fgrokking\uff1b\u5728\u8054\u5408\u5206\u5e03\u8bad\u7ec3\u4e2d\uff0c\u4ece\u4e2a\u4f53\u5206\u5e03grokked\u6a21\u578b\u84b8\u998f\u80fd\u591f\u5b9e\u73b0\u6cdb\u5316\uff1b\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0c\u77e5\u8bc6\u84b8\u998f\u52a0\u901f\u6cdb\u5316\u5e76\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4ec5\u970010%\u6570\u636e\u5373\u53ef\u83b7\u5f97\u5f3a\u6027\u80fd\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u5728\u4f4e\u6570\u636e\u548c\u5206\u5e03\u6f14\u5316\u8bbe\u7f6e\u4e2d\u5bf9\u4e8e\u5b9e\u73b0\u6cdb\u5316\u5177\u6709\u6838\u5fc3\u4f5c\u7528\uff0c\u4e3a\u77e5\u8bc6\u8f6c\u79fb\u4e0b\u7684grokking\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2511.05423", "pdf": "https://arxiv.org/pdf/2511.05423", "abs": "https://arxiv.org/abs/2511.05423", "authors": ["Maynard Koch", "Raphael Hiesgen", "Marcin Nawrocki", "Thomas C. Schmidt", "Matthias W\u00e4hlisch"], "title": "Scanning the IPv6 Internet Using Subnet-Router Anycast Probing", "categories": ["cs.NI"], "comment": null, "summary": "Identifying active IPv6 addresses is challenging. Various methods emerged to\nmaster the measurement challenge in this huge address space, including\nhitlists, new probing techniques, and AI-generated target lists. In this paper,\nwe apply active Subnet-Router anycast (SRA) probing, a commonly unused method\nto explore the IPv6 address space. We compare our results with lists of active\nIPv6 nodes obtained from prior methods and with random probing. Our findings\nindicate that probing an SRA address reveals on average 10% more router IP\naddresses than random probing and is far less affected by ICMP rate limiting.\nCompared to targeting router addresses directly, SRA probing discovers 80% more\naddresses. We conclude that SRA probing is an important addition to the IPv6\nmeasurement toolbox and may improve the stability of results significantly. We\nalso find evidence that some active scans can cause harmful conditions in\ncurrent IPv6 deployments, which we started to fix in collaboration with network\noperators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u4e3b\u52a8\u5b50\u7f51\u8def\u7531\u5668\u4efb\u64ad\uff08SRA\uff09\u63a2\u6d4b\u6280\u672f\u6765\u63a2\u7d22IPv6\u5730\u5740\u7a7a\u95f4\uff0c\u76f8\u6bd4\u968f\u673a\u63a2\u6d4b\u80fd\u53d1\u73b0\u66f4\u591a\u8def\u7531\u5668IP\u5730\u5740\uff0c\u4e14\u53d7ICMP\u901f\u7387\u9650\u5236\u5f71\u54cd\u66f4\u5c0f\u3002", "motivation": "IPv6\u5730\u5740\u7a7a\u95f4\u5de8\u5927\uff0c\u8bc6\u522b\u6d3b\u8dc3IPv6\u5730\u5740\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5305\u62ec\u547d\u4e2d\u5217\u8868\u3001\u65b0\u63a2\u6d4b\u6280\u672f\u548cAI\u751f\u6210\u76ee\u6807\u5217\u8868\uff0c\u4f46SRA\u63a2\u6d4b\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u5e94\u7528\u4e3b\u52a8\u5b50\u7f51\u8def\u7531\u5668\u4efb\u64ad\uff08SRA\uff09\u63a2\u6d4b\u6280\u672f\uff0c\u5e76\u4e0e\u5148\u524d\u65b9\u6cd5\u83b7\u5f97\u7684\u6d3b\u8dc3IPv6\u8282\u70b9\u5217\u8868\u4ee5\u53ca\u968f\u673a\u63a2\u6d4b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "SRA\u63a2\u6d4b\u5e73\u5747\u6bd4\u968f\u673a\u63a2\u6d4b\u591a\u53d1\u73b010%\u7684\u8def\u7531\u5668IP\u5730\u5740\uff0c\u4e14\u53d7ICMP\u901f\u7387\u9650\u5236\u5f71\u54cd\u66f4\u5c0f\u3002\u76f8\u6bd4\u76f4\u63a5\u5b9a\u4f4d\u8def\u7531\u5668\u5730\u5740\uff0cSRA\u63a2\u6d4b\u53d1\u73b080%\u66f4\u591a\u7684\u5730\u5740\u3002", "conclusion": "SRA\u63a2\u6d4b\u662fIPv6\u6d4b\u91cf\u5de5\u5177\u7bb1\u7684\u91cd\u8981\u8865\u5145\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u7ed3\u679c\u7a33\u5b9a\u6027\u3002\u540c\u65f6\u53d1\u73b0\u67d0\u4e9b\u4e3b\u52a8\u626b\u63cf\u53ef\u80fd\u5bf9\u5f53\u524dIPv6\u90e8\u7f72\u9020\u6210\u6709\u5bb3\u6761\u4ef6\uff0c\u5df2\u5f00\u59cb\u4e0e\u7f51\u7edc\u8fd0\u8425\u5546\u5408\u4f5c\u4fee\u590d\u3002"}}
{"id": "2511.05302", "pdf": "https://arxiv.org/pdf/2511.05302", "abs": "https://arxiv.org/abs/2511.05302", "authors": ["Qianru Meng", "Xiao Zhang", "Zhaochen Ren", "Joost Visser"], "title": "Code Review Automation using Retrieval Augmented Generation", "categories": ["cs.SE"], "comment": null, "summary": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86RARe\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u6280\u672f\uff0c\u5229\u7528RAG\u6846\u67b6\u5c06\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u878d\u5165\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\u7684\u8d28\u91cf\u3002", "motivation": "\u4ee3\u7801\u5ba1\u67e5\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u4f46\u52b3\u52a8\u5bc6\u96c6\uff0c\u73b0\u6709\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u8bc4\u8bba\u504f\u79bb\u91cd\u70b9\u6216\u8fc7\u4e8e\u7b3c\u7edf\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u5bc6\u96c6\u68c0\u7d22\u5668\u4ece\u4ee3\u7801\u5e93\u4e2d\u9009\u62e9\u6700\u76f8\u5173\u7684\u8bc4\u8bba\uff0c\u7136\u540e\u5229\u7528\u795e\u7ecf\u751f\u6210\u5668\u7ed3\u5408LLMs\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u751f\u6210\u6700\u7ec8\u5ba1\u67e5\u610f\u89c1\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5206\u522b\u8fbe\u523012.32\u548c12.96\u7684BLEU-4\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u548c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RARe\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u4ee3\u7801\u5ba1\u67e5\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u5c55\u793a\u4e86RAG\u6846\u67b6\u5728\u4ee3\u7801\u5ba1\u67e5\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2511.04768", "pdf": "https://arxiv.org/pdf/2511.04768", "abs": "https://arxiv.org/abs/2511.04768", "authors": ["Rubens Lacouture", "Nathan Zhang", "Ritvik Sharma", "Marco Siracusa", "Fredrik Kjolstad", "Kunle Olukotun", "Olivia Hsu"], "title": "FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow", "categories": ["cs.LG", "cs.AR", "cs.PL"], "comment": null, "summary": "As deep learning models scale, sparse computation and specialized dataflow\nhardware have emerged as powerful solutions to address efficiency. We propose\nFuseFlow, a compiler that converts sparse machine learning models written in\nPyTorch to fused sparse dataflow graphs for reconfigurable dataflow\narchitectures (RDAs). FuseFlow is the first compiler to support general\ncross-expression fusion of sparse operations. In addition to fusion across\nkernels (expressions), FuseFlow also supports optimizations like\nparallelization, dataflow ordering, and sparsity blocking. It targets a\ncycle-accurate dataflow simulator for microarchitectural analysis of fusion\nstrategies. We use FuseFlow for design-space exploration across four real-world\nmachine learning applications with sparsity, showing that full fusion (entire\ncross-expression fusion across all computation in an end-to-end model) is not\nalways optimal for sparse models-fusion granularity depends on the model\nitself. FuseFlow also provides a heuristic to identify and prune suboptimal\nconfigurations. Using Fuseflow, we achieve performance improvements, including\na ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse\nattention.", "AI": {"tldr": "FuseFlow\u662f\u4e00\u4e2a\u5c06PyTorch\u7a00\u758f\u6a21\u578b\u8f6c\u6362\u4e3a\u878d\u5408\u7a00\u758f\u6570\u636e\u6d41\u56fe\u7684\u7f16\u8bd1\u5668\uff0c\u652f\u6301\u8de8\u8868\u8fbe\u5f0f\u878d\u5408\u3001\u5e76\u884c\u5316\u7b49\u4f18\u5316\uff0c\u9488\u5bf9\u53ef\u91cd\u6784\u6570\u636e\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u53d1\u73b0\u5168\u878d\u5408\u5e76\u975e\u603b\u662f\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u7a00\u758f\u8ba1\u7b97\u548c\u4e13\u7528\u6570\u636e\u6d41\u786c\u4ef6\u6210\u4e3a\u63d0\u9ad8\u6548\u7387\u7684\u91cd\u8981\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u652f\u6301\u901a\u7528\u8de8\u8868\u8fbe\u5f0f\u7a00\u758f\u64cd\u4f5c\u878d\u5408\u7684\u7f16\u8bd1\u5668\u3002", "method": "\u5f00\u53d1FuseFlow\u7f16\u8bd1\u5668\uff0c\u652f\u6301\u8de8\u5185\u6838\u878d\u5408\u3001\u5e76\u884c\u5316\u3001\u6570\u636e\u6d41\u6392\u5e8f\u548c\u7a00\u758f\u5206\u5757\u7b49\u4f18\u5316\uff0c\u4f7f\u7528\u5468\u671f\u7cbe\u786e\u7684\u6570\u636e\u6d41\u6a21\u62df\u5668\u8fdb\u884c\u5fae\u67b6\u6784\u5206\u6790\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u53d1\u73b0\u5168\u878d\u5408\u5e76\u975e\u603b\u662f\u6700\u4f18\uff0c\u878d\u5408\u7c92\u5ea6\u53d6\u51b3\u4e8e\u6a21\u578b\u672c\u8eab\uff1b\u5728GPT-3 BigBird\u5757\u7a00\u758f\u6ce8\u610f\u529b\u4e0a\u5b9e\u73b0\u7ea62.7\u500d\u52a0\u901f\u3002", "conclusion": "FuseFlow\u4e3a\u7a00\u758f\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u878d\u5408\u7b56\u7565\u63a2\u7d22\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u878d\u5408\u7c92\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc6\u522b\u6b21\u4f18\u914d\u7f6e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2511.04882", "pdf": "https://arxiv.org/pdf/2511.04882", "abs": "https://arxiv.org/abs/2511.04882", "authors": ["Joon Kim", "Chengwei Duan", "Sandip Ray"], "title": "Bit-Flipping Attack Exploration and Countermeasure in 5G Network", "categories": ["cs.CR", "cs.NI"], "comment": "Presented at the IEEE MASS 2025 REUNS Workshop", "summary": "5G communication technology has become a vital component in a wide range of\napplications due to its unique advantages such as high data rate and low\nlatency. While much of the existing research has focused on optimizing its\nefficiency and performance, security considerations have not received\ncomparable attention, potentially leaving critical vulnerabilities unexplored.\nIn this work, we investigate the vulnerability of 5G systems to bit-flipping\nattacks, which is an integrity attack where an adversary intercepts 5G network\ntraffic and modifies specific fields of an encrypted message without\ndecryption, thus mutating the message while remaining valid to the receiver.\nNotably, these attacks do not require the attacker to know the plaintext, and\nonly the semantic meaning or position of certain fields would be enough to\neffect targeted modifications. We conduct our analysis on OpenAirInterface\n(OAI), an open-source 5G platform that follows the 3GPP Technical\nSpecifications, to rigorously test the real-world feasibility and impact of\nbit-flipping attacks under current 5G encryption mechanisms. Finally, we\npropose a keystream-based shuffling defense mechanism to mitigate the effect of\nsuch attacks by raising the difficulty of manipulating specific encrypted\nfields, while introducing no additional communication overhead compared to the\nNAS Integrity Algorithm (NIA) in 5G. Our findings reveal that enhancements to\n5G security are needed to better protect against attacks that alter data during\ntransmission at the network level.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865G\u7cfb\u7edf\u5bf9\u4f4d\u7ffb\u8f6c\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u5b8c\u6574\u6027\u653b\u51fb\uff0c\u653b\u51fb\u8005\u65e0\u9700\u89e3\u5bc6\u5373\u53ef\u4fee\u6539\u52a0\u5bc6\u6d88\u606f\u7684\u7279\u5b9a\u5b57\u6bb5\u3002\u7814\u7a76\u5728OpenAirInterface\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u73b0\u5b9e\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u94a5\u6d41\u7684\u6d17\u724c\u9632\u5fa1\u673a\u5236\u6765\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u3002", "motivation": "5G\u901a\u4fe1\u6280\u672f\u56e0\u5176\u9ad8\u6570\u636e\u901f\u7387\u548c\u4f4e\u5ef6\u8fdf\u7b49\u72ec\u7279\u4f18\u52bf\u800c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u5e94\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b89\u5168\u8003\u8651\u672a\u5f97\u5230\u540c\u7b49\u91cd\u89c6\uff0c\u53ef\u80fd\u7559\u4e0b\u5173\u952e\u6f0f\u6d1e\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5728\u9075\u5faa3GPP\u6280\u672f\u89c4\u8303\u7684\u5f00\u6e905G\u5e73\u53f0OpenAirInterface\u4e0a\uff0c\u5bf9\u4f4d\u7ffb\u8f6c\u653b\u51fb\u7684\u73b0\u5b9e\u53ef\u884c\u6027\u548c\u5f71\u54cd\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\u3002\u653b\u51fb\u8005\u62e6\u622a5G\u7f51\u7edc\u6d41\u91cf\uff0c\u65e0\u9700\u89e3\u5bc6\u5373\u53ef\u4fee\u6539\u52a0\u5bc6\u6d88\u606f\u7684\u7279\u5b9a\u5b57\u6bb5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d5G\u52a0\u5bc6\u673a\u5236\u4e0b\u4f4d\u7ffb\u8f6c\u653b\u51fb\u662f\u53ef\u884c\u7684\uff0c\u653b\u51fb\u8005\u65e0\u9700\u77e5\u9053\u660e\u6587\uff0c\u4ec5\u9700\u4e86\u89e3\u67d0\u4e9b\u5b57\u6bb5\u7684\u8bed\u4e49\u542b\u4e49\u6216\u4f4d\u7f6e\u5373\u53ef\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\u3002", "conclusion": "5G\u5b89\u5168\u9700\u8981\u589e\u5f3a\u4ee5\u66f4\u597d\u5730\u4fdd\u62a4\u7f51\u7edc\u5c42\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u4e0d\u88ab\u7be1\u6539\u3002\u63d0\u51fa\u7684\u57fa\u4e8e\u5bc6\u94a5\u6d41\u7684\u6d17\u724c\u9632\u5fa1\u673a\u5236\u5728\u4e0d\u589e\u52a0\u901a\u4fe1\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u64cd\u7eb5\u7279\u5b9a\u52a0\u5bc6\u5b57\u6bb5\u7684\u96be\u5ea6\u3002"}}
{"id": "2511.05459", "pdf": "https://arxiv.org/pdf/2511.05459", "abs": "https://arxiv.org/abs/2511.05459", "authors": ["Jingxuan Xu", "Ken Deng", "Weihao Li", "Songwei Yu", "Huaixi Tang", "Haoyang Huang", "Zhiyi Lai", "Zizheng Zhan", "Yanan Wu", "Chenchen Zhang", "Kepeng Lei", "Yifan Yao", "Xinping Lei", "Wenqiang Zhu", "Zongxian Feng", "Han Li", "Junqi Xiong", "Dailin Li", "Zuchen Gao", "Kun Wu", "Wen Xiang", "Ziqi Zhan", "Yuanxing Zhang", "Wuxuan Gong", "Ziyuan Gao", "Guanxiang Wang", "Yirong Xue", "Xiaojiang Zhang", "Jinghui Wang", "Huiming Wang", "Wenhao Zhuang", "Zhaoxiang Zhang", "Yuqun Zhang", "Haotian Zhang", "Bin Chen", "Jiaheng Liu"], "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.", "AI": {"tldr": "SWE-Compass\u662f\u4e00\u4e2a\u5168\u9762\u7684\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d68\u79cd\u4efb\u52a1\u7c7b\u578b\u30018\u79cd\u7f16\u7a0b\u573a\u666f\u548c10\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5305\u542b2000\u4e2a\u6765\u81ea\u771f\u5b9eGitHub\u62c9\u53d6\u8bf7\u6c42\u7684\u9ad8\u8d28\u91cf\u5b9e\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u7a84\u3001\u8bed\u8a00\u504f\u89c1\u4e25\u91cd\u3001\u4e0e\u771f\u5b9e\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u591f\u5339\u914d\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u95ee\u9898\u6216Python\u9519\u8bef\u4fee\u590d\uff0c\u5ffd\u7565\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u6784\u5efaSWE-Compass\u57fa\u51c6\uff0c\u7edf\u4e00\u5f02\u6784\u4ee3\u7801\u76f8\u5173\u8bc4\u4f30\u5230\u7ed3\u6784\u5316\u4e14\u4e0e\u751f\u4ea7\u73af\u5883\u5bf9\u9f50\u7684\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u7cfb\u7edf\u7b5b\u9009\u548c\u9a8c\u8bc1\u4ece\u771f\u5b9eGitHub\u62c9\u53d6\u8bf7\u6c42\u4e2d\u7cbe\u90092000\u4e2a\u9ad8\u8d28\u91cf\u5b9e\u4f8b\uff0c\u5728\u4e24\u79cd\u4ee3\u7406\u6846\u67b6(SWE-Agent\u548cClaude Code)\u4e0b\u8bc4\u4f3010\u4e2a\u6700\u5148\u8fdb\u7684LLM\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u3001\u8bed\u8a00\u548c\u573a\u666f\u4e4b\u95f4\u7684\u96be\u5ea6\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u8bca\u65ad\u548c\u6539\u8fdbLLM\u5728\u4ee3\u7406\u7f16\u7801\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u4e25\u683c\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002", "conclusion": "SWE-Compass\u901a\u8fc7\u5c06\u8bc4\u4f30\u4e0e\u771f\u5b9e\u5f00\u53d1\u8005\u5b9e\u8df5\u5bf9\u9f50\uff0c\u4e3aLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u66f4\u8d34\u8fd1\u5b9e\u9645\u7684\u57fa\u51c6\u6846\u67b6\u3002"}}
{"id": "2511.04774", "pdf": "https://arxiv.org/pdf/2511.04774", "abs": "https://arxiv.org/abs/2511.04774", "authors": ["Liu Jiang", "Zerui Bao", "Shiqi Sheng", "Di Zhu"], "title": "SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Large-scale networked services rely on deep soft-ware stacks and microservice\norchestration, which increase instruction footprints and create frontend stalls\nthat inflate tail latency and energy. We revisit instruction prefetching for\nthese cloud workloads and present a design that aligns with SLO driven and self\noptimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we\nintroduce a Compressed Entry that captures up to eight destinations around a\nbase using 36 bits by exploiting spatial clustering, and a Hierarchical\nMetadata Storage scheme that keeps only L1 resident and frequently queried\nentries on chip while virtualizing bulk metadata into lower levels. We further\nadd a lightweight Online ML Controller that scores prefetch profitability using\ncontext features and a bandit adjusted threshold. On data center applications,\nour approach preserves EIP like speedups with smaller on chip state and\nimproves efficiency for networked services in the ML era.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6307\u4ee4\u9884\u53d6\u8bbe\u8ba1\uff0c\u901a\u8fc7\u538b\u7f29\u6761\u76ee\u3001\u5206\u5c42\u5143\u6570\u636e\u5b58\u50a8\u548c\u5728\u7ebfML\u63a7\u5236\u5668\u6765\u51cf\u5c11\u7247\u4e0a\u72b6\u6001\u5e76\u63d0\u9ad8\u6548\u7387", "motivation": "\u5927\u89c4\u6a21\u7f51\u7edc\u670d\u52a1\u4f9d\u8d56\u6df1\u5ea6\u8f6f\u4ef6\u6808\u548c\u5fae\u670d\u52a1\u7f16\u6392\uff0c\u8fd9\u4f1a\u589e\u52a0\u6307\u4ee4\u5360\u7528\u7a7a\u95f4\u5e76\u9020\u6210\u524d\u7aef\u505c\u987f\uff0c\u4ece\u800c\u63a8\u9ad8\u5c3e\u5ef6\u8fdf\u548c\u80fd\u8017", "method": "\u57fa\u4e8e\u7ea0\u7f20\u6307\u4ee4\u9884\u53d6\u5668(EIP)\uff0c\u5f15\u5165\u538b\u7f29\u6761\u76ee(36\u4f4d\u6355\u83b78\u4e2a\u76ee\u6807)\u3001\u5206\u5c42\u5143\u6570\u636e\u5b58\u50a8(\u7247\u4e0a\u4ec5\u4fdd\u7559L1\u5e38\u9a7b\u548c\u9891\u7e41\u67e5\u8be2\u6761\u76ee)\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u5728\u7ebfML\u63a7\u5236\u5668(\u4f7f\u7528\u4e0a\u4e0b\u6587\u7279\u5f81\u548cbandit\u8c03\u6574\u9608\u503c\u8bc4\u4f30\u9884\u53d6\u6536\u76ca)", "result": "\u5728\u6570\u636e\u4e2d\u5fc3\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301EIP\u7c7b\u4f3c\u52a0\u901f\u6548\u679c\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u7247\u4e0a\u72b6\u6001\uff0c\u63d0\u9ad8\u4e86\u7f51\u7edc\u670d\u52a1\u5728ML\u65f6\u4ee3\u7684\u6548\u7387", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6307\u4ee4\u9884\u53d6\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408SLO\u9a71\u52a8\u548c\u81ea\u4f18\u5316\u7cfb\u7edf"}}
{"id": "2511.05133", "pdf": "https://arxiv.org/pdf/2511.05133", "abs": "https://arxiv.org/abs/2511.05133", "authors": ["Urslla Uchechi Izuazu", "Mounir Bensalem", "Admela Jukan"], "title": "A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection", "categories": ["cs.CR", "cs.NI"], "comment": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "summary": "While Intent-Based Networking (IBN) promises operational efficiency through\nautonomous and abstraction-driven network management, a critical unaddressed\nissue lies in IBN's implicit trust in the integrity of intent ingested by the\nnetwork. This inherent assumption of data reliability creates a blind spot\nexploitable by Man-in-the-Middle (MitM) attacks, where an adversary intercepts\nand alters intent before it is enacted, compelling the network to orchestrate\nmalicious configurations. This study proposes a secured IBN (sIBN) system with\ndata driven intrusion detection method designed to secure legitimate user\nintent from adversarial tampering. The proposed intent intrusion detection\nsystem uses a ML model applied for network behavioral anomaly detection to\nreveal temporal patterns of intent tampering. This is achieved by leveraging a\nset of original behavioral metrics and newly engineered time-aware features,\nwith the model's hyperparameters fine-tuned through the randomized search\ncross-validation (RSCV) technique. Numerical results based on real-world data\nsets, show the effectiveness of sIBN, achieving the best performance across\nstandard evaluation metrics, in both binary and multi classification tasks,\nwhile maintaining low error rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u7684\u57fa\u4e8e\u610f\u56fe\u7684\u7f51\u7edc(sIBN)\u7cfb\u7edf\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u884c\u4e3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ee5\u4fdd\u62a4\u7528\u6237\u610f\u56fe\u514d\u53d7\u4e2d\u95f4\u4eba\u653b\u51fb\u7be1\u6539\u3002", "motivation": "\u57fa\u4e8e\u610f\u56fe\u7684\u7f51\u7edc(IBN)\u9690\u542b\u4fe1\u4efb\u610f\u56fe\u6570\u636e\u7684\u5b8c\u6574\u6027\uff0c\u8fd9\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u4e2d\u95f4\u4eba\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u5728\u610f\u56fe\u6267\u884c\u524d\u62e6\u622a\u5e76\u7be1\u6539\uff0c\u5bfc\u81f4\u7f51\u7edc\u914d\u7f6e\u6076\u610f\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u610f\u56fe\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u884c\u4e3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408\u539f\u59cb\u884c\u4e3a\u6307\u6807\u548c\u65b0\u8bbe\u8ba1\u7684\u65f6\u95f4\u611f\u77e5\u7279\u5f81\uff0c\u901a\u8fc7\u968f\u673a\u641c\u7d22\u4ea4\u53c9\u9a8c\u8bc1\u6280\u672f\u4f18\u5316\u6a21\u578b\u8d85\u53c2\u6570\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u7684\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0csIBN\u5728\u4e8c\u5143\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684sIBN\u7cfb\u7edf\u80fd\u6709\u6548\u68c0\u6d4b\u610f\u56fe\u7be1\u6539\uff0c\u89e3\u51b3\u4e86IBN\u7cfb\u7edf\u4e2d\u6570\u636e\u5b8c\u6574\u6027\u5047\u8bbe\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u3002"}}
{"id": "2511.05476", "pdf": "https://arxiv.org/pdf/2511.05476", "abs": "https://arxiv.org/abs/2511.05476", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?", "categories": ["cs.SE", "cs.LG"], "comment": "The paper is currently under review at a peer-reviewed journal", "summary": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMetaCompress\u6846\u67b6\uff0c\u901a\u8fc7\u8715\u53d8\u6d4b\u8bd5\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u884c\u4e3a\u4fdd\u771f\u5ea6\uff0c\u53d1\u73b0\u4f20\u7edf\u7cbe\u5ea6\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u5e08\u751f\u6a21\u578b\u95f4\u7684\u6df1\u5c42\u884c\u4e3a\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u7cbe\u5ea6\u8bc4\u4f30\uff0c\u4f46\u65e0\u6cd5\u6df1\u5165\u8bc4\u4f30\u5b66\u751f\u6a21\u578b\u662f\u5426\u771f\u6b63\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u7684\u884c\u4e3a\u548c\u5185\u90e8\u8868\u793a\uff0c\u5b58\u5728\u884c\u4e3a\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMetaCompress\u8715\u53d8\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u7ec4\u884c\u4e3a\u4fdd\u6301\u7684\u8715\u53d8\u5173\u7cfb\u7cfb\u7edf\u6bd4\u8f83\u5e08\u751f\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u8bc4\u4f30\u884c\u4e3a\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5b66\u751f\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe285%\uff0cMetaCompress\u80fd\u8bc6\u522b\u9ad8\u8fbe62%\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u8fdc\u8d85\u4f20\u7edf\u7cbe\u5ea6\u8bc4\u4f30\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u4e2d\u9700\u8981\u884c\u4e3a\u4fdd\u771f\u5ea6\u8bc4\u4f30\uff0cMetaCompress\u4e3a\u6d4b\u8bd5\u538b\u7f29\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2511.04789", "pdf": "https://arxiv.org/pdf/2511.04789", "abs": "https://arxiv.org/abs/2511.04789", "authors": ["Xiaoda Wang", "Yuji Zhao", "Kaiqiao Han", "Xiao Luo", "Sanne van Rooij", "Jennifer Stevens", "Lifang He", "Liang Zhan", "Yizhou Sun", "Wei Wang", "Carl Yang"], "title": "Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting", "categories": ["cs.LG"], "comment": "Accepted to IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM) 2025", "summary": "Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry\npatterns. Modeling these longitudinal trajectories enables mechanistic insight,\ntreatment development, and individualized 'digital-twin' forecasting. However,\nexisting methods usually adopt recurrent neural networks and transformer\narchitectures, which rely on discrete, regularly sampled data while struggling\nto handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.\nMoreover, these methods have difficulty capturing individual heterogeneity\nincluding variations in disease onset, progression rate, and symptom severity,\nwhich is a hallmark of PD. To address these challenges, we propose CNODE\n(Conditional Neural ODE), a novel framework for continuous, individualized PD\nprogression forecasting. The core of CNODE is to model morphological brain\nchanges as continuous temporal processes using a neural ODE model. In addition,\nwe jointly learn patient-specific initial time and progress speed to align\nindividual trajectories into a shared progression trajectory. We validate CNODE\non the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental\nresults show that our method outperforms state-of-the-art baselines in\nforecasting longitudinal PD progression.", "AI": {"tldr": "\u63d0\u51faCNODE\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecfODE\u5efa\u6a21\u5e15\u91d1\u68ee\u75c5\u8111\u5f62\u6001\u53d8\u5316\u7684\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u901a\u8fc7\u60a3\u8005\u7279\u5f02\u6027\u521d\u59cb\u65f6\u95f4\u548c\u8fdb\u5c55\u901f\u5ea6\u5b9e\u73b0\u4e2a\u4f53\u5316\u8f68\u8ff9\u5bf9\u9f50\uff0c\u5728PPMI\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u5177\u6709\u5f02\u8d28\u6027\u548c\u6f14\u53d8\u7684\u8111\u5f62\u6001\u6a21\u5f0f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u7a00\u758f\u7684MRI\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u4e2a\u4f53\u5f02\u8d28\u6027\uff08\u5982\u53d1\u75c5\u65f6\u95f4\u3001\u8fdb\u5c55\u901f\u5ea6\u548c\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u7684\u5dee\u5f02\uff09\u3002", "method": "CNODE\u6846\u67b6\u4f7f\u7528\u795e\u7ecfODE\u6a21\u578b\u5c06\u8111\u5f62\u6001\u53d8\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u8054\u5408\u5b66\u4e60\u60a3\u8005\u7279\u5f02\u6027\u521d\u59cb\u65f6\u95f4\u548c\u8fdb\u5c55\u901f\u5ea6\uff0c\u5c06\u4e2a\u4f53\u8f68\u8ff9\u5bf9\u9f50\u5230\u5171\u4eab\u8fdb\u5c55\u8f68\u8ff9\u4e2d\u3002", "result": "\u5728PPMI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7eb5\u5411\u5e15\u91d1\u68ee\u75c5\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CNODE\u80fd\u591f\u6709\u6548\u5efa\u6a21\u5e15\u91d1\u68ee\u75c5\u7684\u8fde\u7eed\u4e2a\u4f53\u5316\u8fdb\u5c55\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u89c4\u5219\u6570\u636e\u548c\u4e2a\u4f53\u5f02\u8d28\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.05156", "pdf": "https://arxiv.org/pdf/2511.05156", "abs": "https://arxiv.org/abs/2511.05156", "authors": ["Azhar Hussain Mozumder", "M. John Basha", "Chayapathi A. R"], "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI", "C.2.3"], "comment": "20 pages, 12 figures", "summary": "With more and more existing networks being transformed to Software-Defined\nNetworking (SDN), they need to be more secure and demand smarter ways of\ntraffic control. This work, SmartSecChain-SDN, is a platform that combines\nmachine learning based intrusion detection, blockchain-based storage of logs,\nand application-awareness-based priority in SDN networks. To detect network\nintrusions in a real-time, precision and low-false positives setup, the\nframework utilizes the application of advanced machine learning algorithms,\nnamely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is\nbased on the Hyperledger Fabric, which is a permissioned blockchain technology,\nto provide secure, scalable, and privacy-preserving storage and, thus,\nguarantee that the Intrusion Detection System (IDS) records cannot be altered\nand can be analyzed comprehensively. The system also has Quality of Service\n(QoS) rules and traffic shaping based on applications, which enables\nprioritization of critical services, such as VoIP, video conferencing, and\nbusiness applications, as well as de-prioritization of non-essential traffic,\nsuch as downloads and updates. Mininet can simulate real-time SDN scenarios\nbecause it is used to prototype whole architectures. It is also compatible with\ncontrollers OpenDaylight and Ryu. It has tested the framework using the InSDN\ndataset and proved that it can identify different kinds of cyberattacks and\nhandle bandwidth allocation efficiently under circumstances of resource\nconstraints. SmartSecChain-SDN comprehensively addresses SDN system protection,\nsecuring and enhancing. The proposed study offers an innovative, extensible way\nto improve cybersecurity, regulatory compliance, and the administration of\nnext-generation programmable networks.", "AI": {"tldr": "SmartSecChain-SDN\u662f\u4e00\u4e2a\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u5165\u4fb5\u68c0\u6d4b\u3001\u533a\u5757\u94fe\u65e5\u5fd7\u5b58\u50a8\u548c\u5e94\u7528\u611f\u77e5\u4f18\u5148\u7ea7\u7684SDN\u5b89\u5168\u5e73\u53f0\uff0c\u901a\u8fc7\u591a\u79cdML\u7b97\u6cd5\u5b9e\u65f6\u68c0\u6d4b\u7f51\u7edc\u5165\u4fb5\uff0c\u4f7f\u7528Hyperledger Fabric\u786e\u4fdd\u65e5\u5fd7\u4e0d\u53ef\u7be1\u6539\uff0c\u5e76\u57fa\u4e8e\u5e94\u7528\u7c7b\u578b\u5b9e\u73b0QoS\u6d41\u91cf\u63a7\u5236\u3002", "motivation": "\u968f\u7740\u4f20\u7edf\u7f51\u7edc\u5411SDN\u8f6c\u578b\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u7f51\u7edc\u73af\u5883\u548c\u667a\u80fd\u6d41\u91cf\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u548c\u591a\u6837\u5316\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001CatBoost\u3001CNN-BiLSTM\uff09\u8fdb\u884c\u5b9e\u65f6\u5165\u4fb5\u68c0\u6d4b\uff0c\u4f7f\u7528Hyperledger Fabric\u533a\u5757\u94fe\u5b58\u50a8IDS\u65e5\u5fd7\uff0c\u57fa\u4e8e\u5e94\u7528\u7c7b\u578b\u5b9e\u73b0QoS\u4f18\u5148\u7ea7\u63a7\u5236\uff0c\u901a\u8fc7Mininet\u6a21\u62dfSDN\u73af\u5883\u8fdb\u884c\u539f\u578b\u9a8c\u8bc1\u3002", "result": "\u4f7f\u7528InSDN\u6570\u636e\u96c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u5404\u7c7b\u7f51\u7edc\u653b\u51fb\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u5e26\u5bbd\u5206\u914d\uff0c\u8bc1\u660e\u4e86\u5176\u68c0\u6d4b\u7cbe\u5ea6\u9ad8\u3001\u8bef\u62a5\u7387\u4f4e\u7684\u7279\u6027\u3002", "conclusion": "SmartSecChain-SDN\u4e3aSDN\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b89\u5168\u4fdd\u62a4\u548c\u6027\u80fd\u589e\u5f3a\u65b9\u6848\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u7f16\u7a0b\u7f51\u7edc\u7684\u7f51\u7edc\u5b89\u5168\u3001\u5408\u89c4\u6027\u548c\u7ba1\u7406\u63d0\u4f9b\u4e86\u521b\u65b0\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04827", "pdf": "https://arxiv.org/pdf/2511.04827", "abs": "https://arxiv.org/abs/2511.04827", "authors": ["Tobias Fischer", "Wolf Vollprecht", "Bas Zalmstra", "Ruben Arts", "Tim de Jager", "Alejandro Fontan", "Adam D Hines", "Michael Milford", "Silvio Traversaro", "Daniel Claes", "Scarlett Raine"], "title": "Pixi: Unified Software Development and Distribution for Robotics and AI", "categories": ["cs.RO", "cs.SE"], "comment": "20 pages, 3 figures, 11 code snippets", "summary": "The reproducibility crisis in scientific computing constrains robotics\nresearch. Existing studies reveal that up to 70% of robotics algorithms cannot\nbe reproduced by independent teams, while many others fail to reach deployment\nbecause creating shareable software environments remains prohibitively complex.\nThese challenges stem from fragmented, multi-language, and hardware-software\ntoolchains that lead to dependency hell. We present Pixi, a unified\npackage-management framework that addresses these issues by capturing exact\ndependency states in project-level lockfiles, ensuring bit-for-bit\nreproducibility across platforms. Its high-performance SAT solver achieves up\nto 10x faster dependency resolution than comparable tools, while integration of\nthe conda-forge and PyPI ecosystems removes the need for multiple managers.\nAdopted in over 5,300 projects since 2023, Pixi reduces setup times from hours\nto minutes and lowers technical barriers for researchers worldwide. By enabling\nscalable, reproducible, collaborative research infrastructure, Pixi accelerates\nprogress in robotics and AI.", "AI": {"tldr": "Pixi\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u6355\u83b7\u7cbe\u786e\u4f9d\u8d56\u72b6\u6001\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u6bd4\u7279\u7ea7\u53ef\u91cd\u73b0\u6027\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5b66\u7814\u7a76\u4e2d\u7684\u53ef\u91cd\u73b0\u6027\u5371\u673a\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u7814\u7a76\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u91cd\u73b0\u6027\u5371\u673a\uff0c70%\u7684\u7b97\u6cd5\u65e0\u6cd5\u88ab\u72ec\u7acb\u56e2\u961f\u91cd\u73b0\uff0c\u591a\u8bed\u8a00\u3001\u786c\u4ef6-\u8f6f\u4ef6\u5de5\u5177\u94fe\u788e\u7247\u5316\u5bfc\u81f4\u4f9d\u8d56\u5730\u72f1\u95ee\u9898\uff0c\u963b\u788d\u4e86\u7814\u7a76\u90e8\u7f72\u548c\u534f\u4f5c\u3002", "method": "\u5f00\u53d1Pixi\u7edf\u4e00\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u6027\u80fdSAT\u6c42\u89e3\u5668\u8fdb\u884c\u4f9d\u8d56\u89e3\u6790\uff0c\u96c6\u6210conda-forge\u548cPyPI\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u786e\u4fdd\u4f9d\u8d56\u72b6\u6001\u7cbe\u786e\u6027\u3002", "result": "Pixi\u4f9d\u8d56\u89e3\u6790\u901f\u5ea6\u6bd4\u540c\u7c7b\u5de5\u5177\u5feb10\u500d\uff0c\u81ea2023\u5e74\u4ee5\u6765\u5df2\u88ab5300\u591a\u4e2a\u9879\u76ee\u91c7\u7528\uff0c\u5c06\u8bbe\u7f6e\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u5206\u949f\uff0c\u964d\u4f4e\u4e86\u5168\u7403\u7814\u7a76\u8005\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "Pixi\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u73b0\u7684\u534f\u4f5c\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0c\u52a0\u901f\u4e86\u673a\u5668\u4eba\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.04790", "pdf": "https://arxiv.org/pdf/2511.04790", "abs": "https://arxiv.org/abs/2511.04790", "authors": ["Caroline Uhler", "Jiaqi Zhang"], "title": "Causal Structure and Representation Learning with Biomedical Applications", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "This article has successfully completed peer review and will appear\n  in the Proceedings of the International Congress of Mathematicians 2026. Both\n  authors contributed equally to this work", "summary": "Massive data collection holds the promise of a better understanding of\ncomplex phenomena and, ultimately, better decisions. Representation learning\nhas become a key driver of deep learning applications, as it allows learning\nlatent spaces that capture important properties of the data without requiring\nany supervised annotations. Although representation learning has been hugely\nsuccessful in predictive tasks, it can fail miserably in causal tasks including\npredicting the effect of a perturbation/intervention. This calls for a marriage\nbetween representation learning and causal inference. An exciting opportunity\nin this regard stems from the growing availability of multi-modal data\n(observational and perturbational, imaging-based and sequencing-based, at the\nsingle-cell level, tissue-level, and organism-level). We outline a statistical\nand computational framework for causal structure and representation learning\nmotivated by fundamental biomedical questions: how to effectively use\nobservational and perturbational data to perform causal discovery on observed\ncausal variables; how to use multi-modal views of the system to learn causal\nvariables; and how to design optimal perturbations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u56e0\u679c\u63a8\u65ad\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff08\u89c2\u5bdf\u6027\u548c\u6270\u52a8\u6027\u6570\u636e\uff09\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u548c\u8868\u793a\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u751f\u7269\u533b\u5b66\u4e2d\u7684\u57fa\u672c\u95ee\u9898\u3002", "motivation": "\u8868\u793a\u5b66\u4e60\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u5f88\u6210\u529f\uff0c\u4f46\u5728\u56e0\u679c\u4efb\u52a1\uff08\u5982\u9884\u6d4b\u5e72\u9884\u6548\u679c\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u56e0\u679c\u63a8\u65ad\u76f8\u7ed3\u5408\u3002\u591a\u6a21\u6001\u6570\u636e\u7684\u53ef\u7528\u6027\u4e3a\u8fd9\u4e00\u7ed3\u5408\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u548c\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u56e0\u679c\u7ed3\u6784\u548c\u8868\u793a\u5b66\u4e60\uff0c\u5305\u62ec\u5982\u4f55\u5229\u7528\u89c2\u5bdf\u6027\u548c\u6270\u52a8\u6027\u6570\u636e\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u3001\u5982\u4f55\u4f7f\u7528\u591a\u6a21\u6001\u89c6\u56fe\u5b66\u4e60\u56e0\u679c\u53d8\u91cf\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u6700\u4f18\u6270\u52a8\u3002", "result": "\u8bba\u6587\u6982\u8ff0\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8868\u793a\u5b66\u4e60\u4e0e\u56e0\u679c\u63a8\u65ad\u7684\u7ed3\u5408\u662f\u89e3\u51b3\u751f\u7269\u533b\u5b66\u4e2d\u56e0\u679c\u95ee\u9898\u7684\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u591a\u6a21\u6001\u6570\u636e\u4e3a\u8fd9\u4e00\u7ed3\u5408\u63d0\u4f9b\u4e86\u91cd\u8981\u673a\u4f1a\u3002"}}
{"id": "2511.05179", "pdf": "https://arxiv.org/pdf/2511.05179", "abs": "https://arxiv.org/abs/2511.05179", "authors": ["Ragini Gupta", "Naman Raina", "Bo Chen", "Li Chen", "Claudiu Danilov", "Josh Eckhardt", "Keyshla Bernard", "Klara Nahrstedt"], "title": "No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "Modern IoT deployments for environmental sensing produce high volume\nspatiotemporal data to support downstream tasks such as forecasting, typically\npowered by machine learning models. While existing filtering and strategic\ndeployment techniques optimize collected data volume at the edge, they overlook\nhow variations in sampling frequencies and spatial coverage affect downstream\nmodel performance. In many forecasting models, incorporating data from\nadditional sensors denoise predictions by providing broader spatial contexts.\nThis interplay between sampling frequency, spatial coverage and different\nforecasting model architectures remain underexplored. This work presents a\nsystematic study of forecasting models - classical models (VAR), neural\nnetworks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),\nand time series foundation models (TSFMs: Chronos Moirai, TimesFM) under\nvarying spatial sensor nodes density and sampling intervals using real-world\ntemperature data in a wireless sensor network. Our results show that STGNNs are\neffective when sensor deployments are sparse and sampling rate is moderate,\nleveraging spatial correlations via encoded graph structure to compensate for\nlimited coverage. In contrast, TSFMs perform competitively at high frequencies\nbut degrade when spatial coverage from neighboring sensors is reduced.\nCrucially, the multivariate TSFM Moirai outperforms all models by natively\nlearning cross-sensor dependencies. These findings offer actionable insights\nfor building efficient forecasting pipelines in spatio-temporal systems. All\ncode for model configurations, training, dataset, and logs are open-sourced for\nreproducibility:\nhttps://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u5728\u65f6\u7a7a\u6570\u636e\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0STGNN\u5728\u7a00\u758f\u4f20\u611f\u5668\u90e8\u7f72\u548c\u4e2d\u7b49\u91c7\u6837\u7387\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800cTSFM\u5728\u9ad8\u9891\u91c7\u6837\u65f6\u8868\u73b0\u4f18\u5f02\u4f46\u53d7\u9650\u4e8e\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u6570\u636e\u8fc7\u6ee4\u548c\u90e8\u7f72\u6280\u672f\u5ffd\u7565\u4e86\u91c7\u6837\u9891\u7387\u548c\u7a7a\u95f4\u8986\u76d6\u53d8\u5316\u5bf9\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u65f6\u7a7a\u6570\u636e\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u6e29\u5ea6\u6570\u636e\uff0c\u7cfb\u7edf\u7814\u7a76\u7ecf\u5178\u6a21\u578b(VAR)\u3001\u795e\u7ecf\u7f51\u7edc(GRU\u3001Transformer)\u3001\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc(STGNNs)\u548c\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u5728\u4e0d\u540c\u7a7a\u95f4\u4f20\u611f\u5668\u8282\u70b9\u5bc6\u5ea6\u548c\u91c7\u6837\u95f4\u9694\u4e0b\u7684\u8868\u73b0\u3002", "result": "STGNN\u5728\u4f20\u611f\u5668\u90e8\u7f72\u7a00\u758f\u4e14\u91c7\u6837\u7387\u4e2d\u7b49\u65f6\u6700\u6709\u6548\uff0c\u901a\u8fc7\u7f16\u7801\u56fe\u7ed3\u6784\u5229\u7528\u7a7a\u95f4\u76f8\u5173\u6027\u8865\u507f\u6709\u9650\u8986\u76d6\uff1bTSFM\u5728\u9ad8\u9891\u65f6\u8868\u73b0\u6709\u7ade\u4e89\u529b\u4f46\u5728\u7a7a\u95f4\u8986\u76d6\u51cf\u5c11\u65f6\u6027\u80fd\u4e0b\u964d\uff1b\u591a\u53d8\u91cfTSFM Moirai\u901a\u8fc7\u5b66\u4e60\u8de8\u4f20\u611f\u5668\u4f9d\u8d56\u5173\u7cfb\u4f18\u4e8e\u6240\u6709\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9ad8\u6548\u7684\u65f6\u7a7a\u7cfb\u7edf\u9884\u6d4b\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2511.05040", "pdf": "https://arxiv.org/pdf/2511.05040", "abs": "https://arxiv.org/abs/2511.05040", "authors": ["Mykyta Syromiatnikov", "Victoria Ruvinskaya"], "title": "UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "8 pages, 5 figures. XI International conference \"Informatics.\n  Culture. Technique.\" (2025)", "summary": "Evaluating the real capabilities of large language models in low-resource\nlanguages still represents a challenge, as many existing benchmarks focus on\nwidespread tasks translated from English or evaluate only simple language\nunderstanding. This paper introduces UA-Code-Bench, a new open-source benchmark\nestablished for a thorough evaluation of language models' code generation and\ncompetitive programming problem-solving abilities in Ukrainian. The benchmark\ncomprises 500 problems from the Eolymp platform, evenly distributed across five\ncomplexity levels from very easy to very hard. A diverse set of 13 leading\nproprietary and open-source models, generating Python solutions based on a\none-shot prompt, was evaluated via the dedicated Eolymp environment against\nhidden tests, ensuring code correctness. The obtained results reveal that even\ntop-performing models, such as OpenAI o3 and GPT-5, solve only half of the\nproblems, highlighting the challenge of code generation in low-resource natural\nlanguage. Furthermore, this research presents a comprehensive analysis of\nperformance across various difficulty levels, as well as an assessment of\nsolution uniqueness and computational efficiency, measured by both elapsed time\nand memory consumption of the generated solutions. In conclusion, this work\ndemonstrates the value of competitive programming benchmarks in evaluating\nlarge language models, especially in underrepresented languages. It also paves\nthe way for future research on multilingual code generation and\nreasoning-enhanced models. The benchmark, data parsing, preparation, code\ngeneration, and evaluation scripts are available at\nhttps://huggingface.co/datasets/NLPForUA/ua-code-bench.", "AI": {"tldr": "UA-Code-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u4e4c\u514b\u5170\u8bed\u4ee3\u7801\u751f\u6210\u548c\u7ade\u4e89\u6027\u7f16\u7a0b\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b500\u4e2a\u96be\u5ea6\u4e0d\u540c\u7684\u95ee\u9898\uff0c\u6d4b\u8bd5\u663e\u793a\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\u4e5f\u53ea\u80fd\u89e3\u51b3\u4e00\u534a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5927\u591a\u5173\u6ce8\u4ece\u82f1\u8bed\u7ffb\u8bd1\u7684\u4efb\u52a1\u6216\u4ec5\u8bc4\u4f30\u7b80\u5355\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Eolymp\u5e73\u53f0\u7684500\u4e2a\u95ee\u9898\uff0c\u5206\u5e03\u57285\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u901a\u8fc713\u4e2a\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u751f\u6210Python\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528\u4e00\u6b21\u6027\u63d0\u793a\uff0c\u5728\u4e13\u7528\u73af\u5883\u4e2d\u8fdb\u884c\u9690\u85cf\u6d4b\u8bd5\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u3002", "result": "\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff08\u5982OpenAI o3\u548cGPT-5\uff09\u4e5f\u53ea\u80fd\u89e3\u51b3\u4e00\u534a\u7684\u95ee\u9898\uff0c\u7a81\u663e\u4e86\u5728\u4f4e\u8d44\u6e90\u81ea\u7136\u8bed\u8a00\u4e2d\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u7684\u6311\u6218\u3002", "conclusion": "\u7ade\u4e89\u6027\u7f16\u7a0b\u57fa\u51c6\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u4e2d\uff0c\u4e3a\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u7684\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.04791", "pdf": "https://arxiv.org/pdf/2511.04791", "abs": "https://arxiv.org/abs/2511.04791", "authors": ["Lei Gao", "Chaoyi Jiang", "Hossein Entezari Zarch", "Daniel Wong", "Murali Annavaram"], "title": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing", "categories": ["cs.LG"], "comment": null, "summary": "Modern LLM serving systems must sustain high throughput while meeting strict\nlatency SLOs across two distinct inference phases: compute-intensive prefill\nand memory-bound decode phases. Existing approaches either (1) aggregate both\nphases on shared GPUs, leading to interference between prefill and decode\nphases, which degrades time-between-tokens (TBT); or (2) disaggregate the two\nphases across GPUs, improving latency but wasting resources through duplicated\nmodels and KV cache transfers. We present DuetServe, a unified LLM serving\nframework that achieves disaggregation-level isolation within a single GPU.\nDuetServe operates in aggregated mode by default and dynamically activates\nSM-level GPU spatial multiplexing when TBT degradation is predicted. Its key\nidea is to decouple prefill and decode execution only when needed through\nfine-grained, adaptive SM partitioning that provides phase isolation only when\ncontention threatens latency service level objectives (SLOs). DuetServe\nintegrates (1) an attention-aware roofline model to forecast iteration latency,\n(2) a partitioning optimizer that selects the optimal SM split to maximize\nthroughput under TBT constraints, and (3) an interruption-free execution engine\nthat eliminates CPU-GPU synchronization overhead. Evaluations show that\nDuetServe improves total throughput by up to 1.3x while maintaining low\ngeneration latency compared to state-of-the-art frameworks.", "AI": {"tldr": "DuetServe\u662f\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5355\u4e2aGPU\u5185\u5b9e\u73b0\u89e3\u8026\u7ea7\u522b\u7684\u9694\u79bb\uff0c\u52a8\u6001\u6fc0\u6d3bSM\u7ea7GPU\u7a7a\u95f4\u590d\u7528\uff0c\u5728\u9884\u6d4b\u5230TBT\u964d\u7ea7\u65f6\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u6267\u884c\uff0c\u4ece\u800c\u63d0\u9ad8\u541e\u5410\u91cf\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u8981\u4e48\u5728\u5171\u4eabGPU\u4e0a\u805a\u5408\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5bfc\u81f4\u5e72\u6270\u548cTBT\u964d\u7ea7\uff0c\u8981\u4e48\u5728GPU\u95f4\u89e3\u8026\u4e24\u4e2a\u9636\u6bb5\u4f46\u9020\u6210\u8d44\u6e90\u6d6a\u8d39\u548cKV\u7f13\u5b58\u4f20\u8f93\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u5355\u4e2aGPU\u5185\u63d0\u4f9b\u89e3\u8026\u7ea7\u522b\u9694\u79bb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DuetServe\u9ed8\u8ba4\u5728\u805a\u5408\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u5f53\u9884\u6d4b\u5230TBT\u964d\u7ea7\u65f6\u52a8\u6001\u6fc0\u6d3bSM\u7ea7GPU\u7a7a\u95f4\u590d\u7528\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u81ea\u9002\u5e94SM\u5206\u533a\u4ec5\u5728\u9700\u8981\u65f6\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u6267\u884c\u3002\u5305\u542b\u6ce8\u610f\u529b\u611f\u77e5\u5c4b\u9876\u6a21\u578b\u9884\u6d4b\u8fed\u4ee3\u5ef6\u8fdf\u3001\u5206\u533a\u4f18\u5316\u5668\u9009\u62e9\u6700\u4f18SM\u5206\u5272\u3001\u65e0\u4e2d\u65ad\u6267\u884c\u5f15\u64ce\u6d88\u9664CPU-GPU\u540c\u6b65\u5f00\u9500\u3002", "result": "\u8bc4\u4f30\u663e\u793aDuetServe\u76f8\u6bd4\u6700\u5148\u8fdb\u6846\u67b6\u5c06\u603b\u541e\u5410\u91cf\u63d0\u9ad8\u4e861.3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u751f\u6210\u5ef6\u8fdf\u3002", "conclusion": "DuetServe\u901a\u8fc7\u52a8\u6001SM\u5206\u533a\u5728\u5355\u4e2aGPU\u5185\u5b9e\u73b0\u4e86\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u89e3\u8026\u7ea7\u522b\u9694\u79bb\uff0c\u5728\u63d0\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u6ee1\u8db3\u4e86\u5ef6\u8fdfSLO\u8981\u6c42\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5e72\u6270\u548c\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\u3002"}}
{"id": "2511.05097", "pdf": "https://arxiv.org/pdf/2511.05097", "abs": "https://arxiv.org/abs/2511.05097", "authors": ["Romain Lefeuvre", "Charly Reux", "Stefano Zacchiroli", "Olivier Barais", "Benoit Combemale"], "title": "Chasing One-day Vulnerabilities Across Open Source Forks", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Tracking vulnerabilities inherited from third-party open-source components is\na well-known challenge, often addressed by tracing the threads of dependency\ninformation. However, vulnerabilities can also propagate through forking: a\nrepository forked after the introduction of a vulnerability, but before it is\npatched, may remain vulnerable in the fork well after being fixed in the\noriginal project. Current approaches for vulnerability analysis lack the\ncommit-level granularity needed to track vulnerability introductions and fixes\nacross forks, potentially leaving one-day vulnerabilities undetected. This\npaper presents a novel approach to help developers identify one-day\nvulnerabilities in forked repositories. Leveraging the global graph of public\ncode, as captured by the Software Heritage archive, the approach propagates\nvulnerability information at the commit level and performs automated impact\nanalysis. This enables automatic detection of forked projects that have not\nincorporated fixes, leaving them potentially vulnerable. Starting from 7162\nrepositories that, according to OSV, include vulnerable commits in their\ndevelopment histories, we identify 2.2 M forks, containing at least one\nvulnerable commit. Then we perform a strict filtering, allowing us to find 356\n___vulnerability, fork___ pairs impacting active and popular GitHub forks, we\nmanually evaluate 65 pairs, finding 3 high-severity vulnerabilities,\ndemonstrating the impact and applicability of this approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u5206\u53c9\u4ed3\u5e93\u4e2d\u4e00\u65e5\u6f0f\u6d1e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u4ea4\u7ea7\u522b\u7684\u6f0f\u6d1e\u4fe1\u606f\u4f20\u64ad\u548c\u81ea\u52a8\u5316\u5f71\u54cd\u5206\u6790\uff0c\u8bc6\u522b\u672a\u5408\u5e76\u4fee\u590d\u7684\u5206\u53c9\u9879\u76ee\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u63d0\u4ea4\u7ea7\u522b\u7684\u7c92\u5ea6\uff0c\u65e0\u6cd5\u8ddf\u8e2a\u8de8\u5206\u53c9\u7684\u6f0f\u6d1e\u5f15\u5165\u548c\u4fee\u590d\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e00\u65e5\u6f0f\u6d1e\u672a\u88ab\u68c0\u6d4b\u3002", "method": "\u5229\u7528Software Heritage\u6863\u6848\u4e2d\u7684\u516c\u5171\u4ee3\u7801\u5168\u5c40\u56fe\uff0c\u5728\u63d0\u4ea4\u7ea7\u522b\u4f20\u64ad\u6f0f\u6d1e\u4fe1\u606f\u5e76\u6267\u884c\u81ea\u52a8\u5316\u5f71\u54cd\u5206\u6790\u3002", "result": "\u4ece7162\u4e2a\u5305\u542b\u6f0f\u6d1e\u63d0\u4ea4\u7684\u4ed3\u5e93\u4e2d\u8bc6\u522b\u51fa220\u4e07\u4e2a\u5206\u53c9\uff0c\u7ecf\u8fc7\u4e25\u683c\u8fc7\u6ee4\u53d1\u73b0356\u4e2a\u6f0f\u6d1e-\u5206\u53c9\u5bf9\uff0c\u624b\u52a8\u8bc4\u4f3065\u5bf9\u53d1\u73b03\u4e2a\u9ad8\u4e25\u91cd\u6027\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5206\u53c9\u9879\u76ee\u4e2d\u672a\u4fee\u590d\u7684\u6f0f\u6d1e\uff0c\u8bc1\u660e\u4e86\u5176\u5f71\u54cd\u529b\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2511.04804", "pdf": "https://arxiv.org/pdf/2511.04804", "abs": "https://arxiv.org/abs/2511.04804", "authors": ["Chaymae Yahyati", "Ismail Lamaakal", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator", "categories": ["cs.LG"], "comment": null, "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial\npredictor that represents f: R^d -> R^k as a globally C^r finite-element field\non a learned simplicial mesh in an optionally warped input space. Each query\nactivates exactly one simplex and at most d+1 basis functions via barycentric\ncoordinates, yielding explicit locality, controllable smoothness, and\ncache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with\na light invertible warp and trains end-to-end with shape regularization,\nsemi-discrete OT coverage, and differentiable edge flips. Under standard\nshape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic\nFEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic\napproximation tasks, tabular regression/classification, and as a drop-in head\non compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter\nbudgets, improves calibration (lower ECE/Brier), and reduces inference latency\ndue to geometric locality. These properties make SiFEN a compact,\ninterpretable, and theoretically grounded alternative to dense MLPs and\nedge-spline networks.", "AI": {"tldr": "SiFEN\u662f\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u5143\u65b9\u6cd5\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u5206\u7247\u591a\u9879\u5f0f\u5728\u5b66\u4e60\u7684\u5355\u7eaf\u5f62\u7f51\u683c\u4e0a\u8868\u793a\u51fd\u6570\uff0c\u5177\u6709\u663e\u5f0f\u5c40\u90e8\u6027\u3001\u53ef\u63a7\u5e73\u6ed1\u5ea6\u548c\u7f13\u5b58\u53cb\u597d\u7684\u7a00\u758f\u6027\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u7406\u8bba\u57fa\u7840\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u66ff\u4ee3\u5bc6\u96c6MLP\u548c\u8fb9\u7f18\u6837\u6761\u7f51\u7edc\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u6821\u51c6\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u4f7f\u7528\u5ea6\u6570\u4e3am\u7684Bernstein-Bezier\u591a\u9879\u5f0f\u4e0e\u8f7b\u91cf\u53ef\u9006\u626d\u66f2\u914d\u5bf9\uff0c\u901a\u8fc7\u5f62\u72b6\u6b63\u5219\u5316\u3001\u534a\u79bb\u6563OT\u8986\u76d6\u548c\u53ef\u5fae\u5206\u8fb9\u7ffb\u8f6c\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u5408\u6210\u903c\u8fd1\u4efb\u52a1\u3001\u8868\u683c\u56de\u5f52/\u5206\u7c7b\u4ee5\u53ca\u4f5c\u4e3a\u7d27\u51d1CNN\u7684\u5934\u90e8\u65f6\uff0cSiFEN\u5728\u5339\u914d\u53c2\u6570\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8eMLP\u548cKAN\uff0c\u6539\u5584\u4e86\u6821\u51c6\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "SiFEN\u662f\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u7406\u8bba\u57fa\u7840\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u5c40\u90e8\u6027\u548c\u53ef\u63a7\u5e73\u6ed1\u5ea6\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.05311", "pdf": "https://arxiv.org/pdf/2511.05311", "abs": "https://arxiv.org/abs/2511.05311", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "Rapha\u00ebl Frank"], "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "comment": null, "summary": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u6c7d\u8f66\u884c\u4e1a\u9884\u6d4b\u6027\u7ef4\u62a4\uff08PdM\uff09\u6570\u636e\u6e05\u6d17\u7ba1\u9053\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ef4\u62a4\u65e5\u5fd7\u4e2d\u7684\u516d\u79cd\u5e38\u89c1\u566a\u58f0\u7c7b\u578b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6c7d\u8f66\u884c\u4e1a\u9884\u6d4b\u6027\u7ef4\u62a4\u9762\u4e34\u7ecf\u6d4e\u7ea6\u675f\u3001\u6570\u636e\u96c6\u53ef\u7528\u6027\u6709\u9650\u548c\u4e13\u4e1a\u4eba\u624d\u77ed\u7f3a\u7b49\u6311\u6218\uff0c\u800cLLM\u7684\u8fdb\u5c55\u4e3a\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u52a0\u901fPdM\u4ece\u7814\u7a76\u5411\u5de5\u4e1a\u5b9e\u8df5\u7684\u8fc7\u6e21\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u667a\u80fd\u4f53\u5728\u7ef4\u62a4\u65e5\u5fd7\u6570\u636e\u6e05\u6d17\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u516d\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u566a\u58f0\u5904\u7406\uff0c\u5305\u62ec\u62fc\u5199\u9519\u8bef\u3001\u7f3a\u5931\u5b57\u6bb5\u3001\u8fd1\u4f3c\u91cd\u590d\u6761\u76ee\u548c\u9519\u8bef\u65e5\u671f\u7b49\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u5904\u7406\u901a\u7528\u6e05\u6d17\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u4e3a\u672a\u6765\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002\u5c3d\u7ba1\u9886\u57df\u7279\u5b9a\u9519\u8bef\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u663e\u793a\u4e86\u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u548c\u589e\u5f3a\u667a\u80fd\u4f53\u80fd\u529b\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u6570\u636e\u6e05\u6d17\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u901a\u7528\u566a\u58f0\u7c7b\u578b\u65f6\u8868\u73b0\u826f\u597d\u3002\u867d\u7136\u9886\u57df\u7279\u5b9a\u95ee\u9898\u4ecd\u9700\u89e3\u51b3\uff0c\u4f46\u901a\u8fc7\u4e13\u95e8\u5316\u8bad\u7ec3\u548c\u80fd\u529b\u589e\u5f3a\uff0c\u6709\u671b\u5b9e\u73b0\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2511.04805", "pdf": "https://arxiv.org/pdf/2511.04805", "abs": "https://arxiv.org/abs/2511.04805", "authors": ["Yushu Zhao", "Zheng Wang", "Minjia Zhang"], "title": "PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models have shown strong potential in scaling\nlanguage models efficiently by activating only a small subset of experts per\ninput. However, their widespread deployment remains limited due to the high\nmemory overhead associated with storing all expert parameters, particularly as\nthe number of experts increases. To address this challenge, prior works have\nexplored expert dropping and merging strategies, yet they often suffer from\nperformance drop at high compression ratios. In this paper, we introduce\nPuzzleMoE, a training-free MoE compression method that achieves both high\naccuracy and efficient inference through two key innovations: First, PuzzleMoE\nperforms sparse expert merging by identifying element-wise weight redundancy\nand specialization. It uses a dual-mask to capture both shared and\nexpert-specific parameters. Second, to avoid the overhead of storing binary\nmasks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses\nunderutilized exponent bits, enabling efficient MoE inference on GPUs.\nExtensive experiments demonstrate that PuzzleMoE can compress MoE models by up\nto 50% while maintaining accuracy across various tasks. Specifically, it\noutperforms prior MoE compression methods by up to 16.7% on MMLU at 50%\ncompression ratio, and achieves up to 1.28\\times inference speedup.", "AI": {"tldr": "PuzzleMoE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684MoE\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u5408\u5e76\u548c\u4f4d\u538b\u7f29\u7f16\u7801\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u572850%\u538b\u7f29\u7387\u4e0b\u4fdd\u6301\u51c6\u786e\u5ea6\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u7531\u4e8e\u5b58\u50a8\u6240\u6709\u4e13\u5bb6\u53c2\u6570\u5bfc\u81f4\u7684\u9ad8\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e13\u5bb6\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u7a00\u758f\u4e13\u5bb6\u5408\u5e76\uff1a\u8bc6\u522b\u6743\u91cd\u5197\u4f59\u548c\u4e13\u4e1a\u5316\uff0c\u4f7f\u7528\u53cc\u63a9\u7801\u6355\u83b7\u5171\u4eab\u548c\u4e13\u5bb6\u7279\u5b9a\u53c2\u6570\n2. \u4f4d\u538b\u7f29\u7f16\u7801\uff1a\u91cd\u7528\u672a\u5145\u5206\u5229\u7528\u7684\u6307\u6570\u4f4d\uff0c\u907f\u514d\u5b58\u50a8\u4e8c\u8fdb\u5236\u63a9\u7801\u548c\u7b26\u53f7\u7684\u5f00\u9500", "result": "\u538b\u7f29MoE\u6a21\u578b\u8fbe50%\u540c\u65f6\u4fdd\u6301\u5404\u4efb\u52a1\u51c6\u786e\u5ea6\uff0c\u5728MMLU\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534716.7%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.28\u500d", "conclusion": "PuzzleMoE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684MoE\u538b\u7f29\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u9ad8\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u538b\u7f29\u4e0e\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.05410", "pdf": "https://arxiv.org/pdf/2511.05410", "abs": "https://arxiv.org/abs/2511.05410", "authors": ["Justin D. Weisz", "Michael Muller", "Kush R. Varshney"], "title": "Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering", "categories": ["cs.HC", "cs.SE"], "comment": "8 pages. Appeared in the 2025 Workshop on The End of Programming (as\n  we know it): Envisioning Radical Re-Conceptualizations of Co-Coding with AI,\n  held in conjunction with the Aarhus 2025 Decennial Conference, August 18-22,\n  2025", "summary": "What better way to understand the impact of AI on software engineering than\nto ask AI itself? We constructed Story Arena, a multi-agent \"writer's room\" in\nwhich multiple AI agents, independently imbued with a position statement on the\nfuture of software engineering, converse with each other to develop a shared\nvision. They then use this shared vision to collaboratively construct a design\nfiction that depicts this vision in narrative form. We present \"The Code of\nTrust,\" a short fiction that investigates themes of human comprehension, trust,\ncontent ownership, augmentation vs. replacement, and uncertain futures in\nhuman-AI co-creation.", "AI": {"tldr": "\u4f7f\u7528\u591aAI\u4ee3\u7406\u7cfb\u7edf\u6784\u5efaStory Arena\uff0c\u8ba9\u6301\u6709\u4e0d\u540c\u89c2\u70b9\u7684AI\u4ee3\u7406\u8ba8\u8bba\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\uff0c\u5e76\u5408\u4f5c\u521b\u4f5c\u8bbe\u8ba1\u5c0f\u8bf4\u300a\u4fe1\u4efb\u4e4b\u7801\u300b\uff0c\u63a2\u8ba8\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4fe1\u4efb\u3001\u7406\u89e3\u7b49\u4e3b\u9898\u3002", "motivation": "\u901a\u8fc7\u8ba9AI\u81ea\u8eab\u8ba8\u8bbaAI\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\uff0c\u4ee5\u65b0\u9896\u7684\u65b9\u5f0f\u7406\u89e3\u8fd9\u4e00\u4e3b\u9898\uff0c\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u7684\u672a\u6765\u53ef\u80fd\u6027\u3002", "method": "\u6784\u5efa\u591a\u4ee3\u7406\"\u7f16\u5267\u5ba4\"Story Arena\uff0c\u8ba9\u6301\u6709\u4e0d\u540c\u7acb\u573a\u89c2\u70b9\u7684AI\u4ee3\u7406\u8fdb\u884c\u5bf9\u8bdd\uff0c\u5f62\u6210\u5171\u540c\u613f\u666f\uff0c\u5e76\u57fa\u4e8e\u6b64\u534f\u4f5c\u521b\u4f5c\u8bbe\u8ba1\u5c0f\u8bf4\u3002", "result": "\u521b\u4f5c\u51fa\u77ed\u7bc7\u5c0f\u8bf4\u300a\u4fe1\u4efb\u4e4b\u7801\u300b\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u4eba\u7c7b\u7406\u89e3\u3001\u4fe1\u4efb\u3001\u5185\u5bb9\u6240\u6709\u6743\u3001\u589e\u5f3avs\u66ff\u4ee3\u3001\u4e0d\u786e\u5b9a\u672a\u6765\u7b49\u4eba\u673a\u5171\u521b\u4e3b\u9898\u3002", "conclusion": "\u901a\u8fc7AI\u4ee3\u7406\u95f4\u7684\u5bf9\u8bdd\u548c\u534f\u4f5c\u521b\u4f5c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u89e3AI\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5f71\u54cd\u7684\u521b\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u53d9\u4e8b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.04807", "pdf": "https://arxiv.org/pdf/2511.04807", "abs": "https://arxiv.org/abs/2511.04807", "authors": ["Matthew D. Kvalheim", "Eduardo D. Sontag"], "title": "Autoencoding Dynamics: Topological Limitations and Capabilities", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "Given a \"data manifold\" $M\\subset \\mathbb{R}^n$ and \"latent space\"\n$\\mathbb{R}^\\ell$, an autoencoder is a pair of continuous maps consisting of an\n\"encoder\" $E\\colon \\mathbb{R}^n\\to \\mathbb{R}^\\ell$ and \"decoder\" $D\\colon\n\\mathbb{R}^\\ell\\to \\mathbb{R}^n$ such that the \"round trip\" map $D\\circ E$ is\nas close as possible to the identity map $\\mbox{id}_M$ on $M$. We present\nvarious topological limitations and capabilites inherent to the search for an\nautoencoder, and describe capabilities for autoencoding dynamical systems\nhaving $M$ as an invariant manifold.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u81ea\u7f16\u7801\u5668\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u62d3\u6251\u9650\u5236\u548c\u80fd\u529b\uff0c\u5e76\u63cf\u8ff0\u4e86\u5728\u52a8\u529b\u7cfb\u7edf\u4e2d\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u81ea\u7f16\u7801\u5668\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u62d3\u6251\u7279\u6027\uff0c\u63a2\u7d22\u5176\u5728\u8868\u793a\u5b66\u4e60\u548c\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u62d3\u6251\u5b66\u65b9\u6cd5\u5206\u6790\u81ea\u7f16\u7801\u5668\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5bf9\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u884c\u4e3a\uff0c\u7279\u522b\u5173\u6ce8round-trip\u6620\u5c04\u4e0e\u6052\u7b49\u6620\u5c04\u7684\u63a5\u8fd1\u7a0b\u5ea6\u3002", "result": "\u63ed\u793a\u4e86\u81ea\u7f16\u7801\u5668\u5728\u6570\u636e\u6d41\u5f62\u8868\u793a\u4e2d\u7684\u56fa\u6709\u62d3\u6251\u9650\u5236\u548c\u53ef\u80fd\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u52a8\u529b\u7cfb\u7edf\u4e0d\u53d8\u6d41\u5f62\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "\u81ea\u7f16\u7801\u5668\u7684\u62d3\u6251\u7279\u6027\u5bf9\u5176\u8868\u793a\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u7406\u89e3\u8fd9\u4e9b\u7279\u6027\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7279\u522b\u662f\u5728\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u4e2d\u3002"}}
{"id": "2511.04808", "pdf": "https://arxiv.org/pdf/2511.04808", "abs": "https://arxiv.org/abs/2511.04808", "authors": ["Raymond Fan", "Bryce Sandlund", "Lin Myat Ko"], "title": "Sharp Minima Can Generalize: A Loss Landscape Perspective On Data", "categories": ["cs.LG"], "comment": null, "summary": "The volume hypothesis suggests deep learning is effective because it is\nlikely to find flat minima due to their large volumes, and flat minima\ngeneralize well. This picture does not explain the role of large datasets in\ngeneralization. Measuring minima volumes under varying amounts of training data\nreveals sharp minima which generalize well exist, but are unlikely to be found\ndue to their small volumes. Increasing data changes the loss landscape, such\nthat previously small generalizing minima become (relatively) large.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4f53\u79ef\u5047\u8bbe\uff0c\u53d1\u73b0\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u4f1a\u6539\u53d8\u635f\u5931\u666f\u89c2\uff0c\u4f7f\u539f\u672c\u4f53\u79ef\u5c0f\u4f46\u6cdb\u5316\u597d\u7684\u6781\u5c0f\u503c\u53d8\u5f97\u76f8\u5bf9\u66f4\u5927\uff0c\u4ece\u800c\u66f4\u5bb9\u6613\u88ab\u627e\u5230\u3002", "motivation": "\u4f53\u79ef\u5047\u8bbe\u8ba4\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6709\u6548\u662f\u56e0\u4e3a\u5e73\u5766\u6781\u5c0f\u503c\u4f53\u79ef\u5927\u6613\u88ab\u627e\u5230\uff0c\u4f46\u8fd9\u65e0\u6cd5\u89e3\u91ca\u5927\u6570\u636e\u96c6\u5728\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u6570\u636e\u91cf\u5982\u4f55\u5f71\u54cd\u6781\u5c0f\u503c\u4f53\u79ef\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u91cf\u4e0b\u6d4b\u91cf\u6781\u5c0f\u503c\u7684\u4f53\u79ef\uff0c\u5206\u6790\u635f\u5931\u666f\u89c2\u7684\u53d8\u5316\uff0c\u7814\u7a76\u6781\u5c0f\u503c\u4f53\u79ef\u4e0e\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4f53\u79ef\u5c0f\u4f46\u6cdb\u5316\u597d\u7684\u5c16\u9510\u6781\u5c0f\u503c\uff0c\u4f46\u7531\u4e8e\u4f53\u79ef\u5c0f\u96be\u4ee5\u88ab\u627e\u5230\u3002\u589e\u52a0\u6570\u636e\u4f1a\u4f7f\u8fd9\u4e9b\u6cdb\u5316\u597d\u7684\u6781\u5c0f\u503c\u76f8\u5bf9\u4f53\u79ef\u53d8\u5927\u3002", "conclusion": "\u5927\u6570\u636e\u96c6\u7684\u4f5c\u7528\u4e0d\u4ec5\u662f\u63d0\u4f9b\u66f4\u591a\u8bad\u7ec3\u6837\u672c\uff0c\u66f4\u91cd\u8981\u7684\u662f\u6539\u53d8\u635f\u5931\u666f\u89c2\uff0c\u4f7f\u6cdb\u5316\u597d\u7684\u6781\u5c0f\u503c\u53d8\u5f97\u66f4\u5bb9\u6613\u88ab\u4f18\u5316\u7b97\u6cd5\u627e\u5230\u3002"}}
{"id": "2511.04814", "pdf": "https://arxiv.org/pdf/2511.04814", "abs": "https://arxiv.org/abs/2511.04814", "authors": ["Sebastian Ojeda", "Rafael Velasquez", "Nicol\u00e1s Aparicio", "Juanita Puentes", "Paula C\u00e1rdenas", "Nicol\u00e1s Andrade", "Gabriel Gonz\u00e1lez", "Sergio Rinc\u00f3n", "Carolina Mu\u00f1oz-Camargo", "Pablo Arbel\u00e1ez"], "title": "A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "68T07, 62H30, 62P10", "I.2.6; I.2.1; I.5.1; I.5.2"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025). Camera-ready version. Code: https://github.com/BCV-Uniandes/ESCAPE.\n  Dataset DOI: https://doi.org/10.7910/DVN/C69MCD", "summary": "Antimicrobial peptides have emerged as promising molecules to combat\nantimicrobial resistance. However, fragmented datasets, inconsistent\nannotations, and the lack of standardized benchmarks hinder computational\napproaches and slow down the discovery of new candidates. To address these\nchallenges, we present the Expanded Standardized Collection for Antimicrobial\nPeptide Evaluation (ESCAPE), an experimental framework integrating over 80.000\npeptides from 27 validated repositories. Our dataset separates antimicrobial\npeptides from negative sequences and incorporates their functional annotations\ninto a biologically coherent multilabel hierarchy, capturing activities across\nantibacterial, antifungal, antiviral, and antiparasitic classes. Building on\nESCAPE, we propose a transformer-based model that leverages sequence and\nstructural information to predict multiple functional activities of peptides.\nOur method achieves up to a 2.56% relative average improvement in mean Average\nPrecision over the second-best method adapted for this task, establishing a new\nstate-of-the-art multilabel peptide classification. ESCAPE provides a\ncomprehensive and reproducible evaluation framework to advance AI-driven\nantimicrobial peptide research.", "AI": {"tldr": "ESCAPE\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u6297\u83cc\u80bd\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6765\u81ea27\u4e2a\u9a8c\u8bc1\u5e93\u768480,000\u591a\u4e2a\u80bd\u5e8f\u5217\uff0c\u63d0\u4f9b\u4e86\u591a\u6807\u7b7e\u529f\u80fd\u5206\u7c7b\u548c\u65b0\u7684\u6700\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u6297\u83cc\u80bd\u7814\u7a76\u4e2d\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u6ce8\u91ca\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u8ba1\u7b97\u65b9\u6cd5\u548c\u65b0\u5019\u9009\u80bd\u7684\u53d1\u73b0\u3002", "method": "\u6784\u5efaESCAPE\u6570\u636e\u96c6\uff0c\u6574\u5408\u591a\u4e2a\u6765\u6e90\u7684\u80bd\u5e8f\u5217\uff0c\u5efa\u7acb\u751f\u7269\u4e00\u81f4\u7684\u591a\u6807\u7b7e\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8etransformer\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\u9884\u6d4b\u80bd\u7684\u591a\u529f\u80fd\u6d3b\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\u4e0a\u76f8\u5bf9\u4e8e\u7b2c\u4e8c\u4f73\u65b9\u6cd5\u5b9e\u73b0\u4e862.56%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u591a\u6807\u7b7e\u80bd\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "ESCAPE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u6297\u83cc\u80bd\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2511.04825", "pdf": "https://arxiv.org/pdf/2511.04825", "abs": "https://arxiv.org/abs/2511.04825", "authors": ["Luigi Caputi", "Nicholas Meadows", "Henri Riihim\u00e4ki"], "title": "Persistent reachability homology in machine learning applications", "categories": ["cs.LG", "math.AT", "q-bio.QM"], "comment": "19 pages; any comments welcome", "summary": "We explore the recently introduced persistent reachability homology (PRH) of\ndigraph data, i.e. data in the form of directed graphs. In particular, we study\nthe effectiveness of PRH in network classification task in a key neuroscience\nproblem: epilepsy detection. PRH is a variation of the persistent homology of\ndigraphs, more traditionally based on the directed flag complex (DPH). A main\nadvantage of PRH is that it considers the condensations of the digraphs\nappearing in the persistent filtration and thus is computed from smaller\ndigraphs. We compare the effectiveness of PRH to that of DPH and we show that\nPRH outperforms DPH in the classification task. We use the Betti curves and\ntheir integrals as topological features and implement our pipeline on support\nvector machine.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6301\u4e45\u53ef\u8fbe\u6027\u540c\u8c03\uff08PRH\uff09\u5728\u6709\u5411\u56fe\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u766b\u75eb\u68c0\u6d4b\u8fd9\u4e00\u795e\u7ecf\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u3002PRH\u76f8\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u6709\u5411\u65d7\u590d\u5f62\uff08DPH\uff09\u7684\u6301\u4e45\u540c\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6709\u5411\u56fe\u7684\u51dd\u805a\u800c\u4f7f\u7528\u66f4\u5c0f\u7684\u56fe\u8fdb\u884c\u8ba1\u7b97\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u6301\u4e45\u53ef\u8fbe\u6027\u540c\u8c03\uff08PRH\uff09\u5728\u6709\u5411\u56fe\u6570\u636e\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u766b\u75eb\u68c0\u6d4b\u8fd9\u4e00\u91cd\u8981\u795e\u7ecf\u79d1\u5b66\u95ee\u9898\u4e2d\uff0c\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u6709\u5411\u65d7\u590d\u5f62\uff08DPH\uff09\u7684\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6301\u4e45\u53ef\u8fbe\u6027\u540c\u8c03\uff08PRH\uff09\u5206\u6790\u6709\u5411\u56fe\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u6709\u5411\u56fe\u5728\u6301\u4e45\u8fc7\u6ee4\u4e2d\u7684\u51dd\u805a\uff0c\u4ece\u800c\u4ece\u66f4\u5c0f\u7684\u6709\u5411\u56fe\u8fdb\u884c\u8ba1\u7b97\u3002\u4f7f\u7528Betti\u66f2\u7ebf\u53ca\u5176\u79ef\u5206\u4f5c\u4e3a\u62d3\u6251\u7279\u5f81\uff0c\u5e76\u5728\u652f\u6301\u5411\u91cf\u673a\u4e0a\u5b9e\u73b0\u5206\u7c7b\u7ba1\u9053\u3002", "result": "PRH\u5728\u766b\u75eb\u68c0\u6d4b\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684DPH\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6301\u4e45\u53ef\u8fbe\u6027\u540c\u8c03\uff08PRH\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u6709\u5411\u56fe\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u5728\u766b\u75eb\u68c0\u6d4b\u7b49\u795e\u7ecf\u79d1\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2511.04834", "pdf": "https://arxiv.org/pdf/2511.04834", "abs": "https://arxiv.org/abs/2511.04834", "authors": ["Jiwoo Shin", "Byeonghu Na", "Mina Kang", "Wonhyeok Choi", "Il-chul Moon"], "title": "Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at NeurIPS 2025 Workshop on Generative and Protective AI for\n  Content Creation", "summary": "Recent advances in text-to-image generative models have raised concerns about\ntheir potential to produce harmful content when provided with malicious input\ntext prompts. To address this issue, two main approaches have emerged: (1)\nfine-tuning the model to unlearn harmful concepts and (2) training-free\nguidance methods that leverage negative prompts. However, we observe that\ncombining these two orthogonal approaches often leads to marginal or even\ndegraded defense performance. This observation indicates a critical\nincompatibility between two paradigms, which hinders their combined\neffectiveness. In this work, we address this issue by proposing a conceptually\nsimple yet experimentally robust method: replacing the negative prompts used in\ntraining-free methods with implicit negative embeddings obtained through\nconcept inversion. Our method requires no modification to either approach and\ncan be easily integrated into existing pipelines. We experimentally validate\nits effectiveness on nudity and violence benchmarks, demonstrating consistent\nimprovements in defense success rate while preserving the core semantics of\ninput prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\u9632\u5fa1\u95ee\u9898\uff0c\u901a\u8fc7\u7528\u6982\u5ff5\u53cd\u8f6c\u83b7\u5f97\u7684\u9690\u5f0f\u8d1f\u5d4c\u5165\u66ff\u6362\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\u4e2d\u7684\u8d1f\u63d0\u793a\uff0c\u5b9e\u73b0\u4e86\u4e24\u79cd\u9632\u5fa1\u65b9\u6cd5\u7684\u517c\u5bb9\u6027\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9632\u5fa1\u65b9\u6cd5\u5305\u62ec\u5fae\u8c03\u6a21\u578b\u548c\u8bad\u7ec3\u81ea\u7531\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4f46\u5c06\u8fd9\u4e24\u79cd\u6b63\u4ea4\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u5f80\u5f80\u5bfc\u81f4\u9632\u5fa1\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u4e24\u79cd\u8303\u5f0f\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u7684\u4e0d\u517c\u5bb9\u6027\u3002", "method": "\u63d0\u51fa\u7528\u6982\u5ff5\u53cd\u8f6c\u83b7\u5f97\u7684\u9690\u5f0f\u8d1f\u5d4c\u5165\u66ff\u6362\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\u4e2d\u7684\u8d1f\u63d0\u793a\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728\u88f8\u9732\u548c\u66b4\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u9632\u5fa1\u6210\u529f\u7387\u6301\u7eed\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f93\u5165\u63d0\u793a\u7684\u6838\u5fc3\u8bed\u4e49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4e24\u79cd\u9632\u5fa1\u8303\u5f0f\u4e4b\u95f4\u7684\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6709\u5bb3\u5185\u5bb9\u9632\u5fa1\u6548\u679c\u3002"}}
{"id": "2511.04838", "pdf": "https://arxiv.org/pdf/2511.04838", "abs": "https://arxiv.org/abs/2511.04838", "authors": ["Brenda Nogueira", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression", "categories": ["cs.LG", "math.SP", "q-bio.MN"], "comment": null, "summary": "In molecular property prediction, the most valuable compounds (e.g., high\npotency) often occupy sparse regions of the target space. Standard Graph Neural\nNetworks (GNNs) commonly optimize for the average error, underperforming on\nthese uncommon but critical cases, with existing oversampling methods often\ndistorting molecular topology. In this paper, we introduce SPECTRA, a Spectral\nTarget-Aware graph augmentation framework that generates realistic molecular\ngraphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute\nmolecular graphs from SMILES; (ii) aligns molecule pairs via (Fused)\nGromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates\nLaplacian eigenvalues, eigenvectors and node features in a stable share-basis;\nand (iv) reconstructs edges to synthesize physically plausible intermediates\nwith interpolated targets. A rarity-aware budgeting scheme, derived from a\nkernel density estimation of labels, concentrates augmentation where data are\nscarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions,\nSPECTRA densifies underrepresented regions without degrading global accuracy.\nOn benchmarks, SPECTRA consistently improves error in relevant target ranges\nwhile maintaining competitive overall MAE, and yields interpretable synthetic\nmolecules whose structure reflects the underlying spectral geometry. Our\nresults demonstrate that spectral, geometry-aware augmentation is an effective\nand efficient strategy for imbalanced molecular property regression.", "AI": {"tldr": "SPECTRA\u662f\u4e00\u4e2a\u5149\u8c31\u76ee\u6807\u611f\u77e5\u7684\u56fe\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5149\u8c31\u57df\u751f\u6210\u771f\u5b9e\u7684\u5206\u5b50\u56fe\u6765\u89e3\u51b3\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\u7a00\u6709\u5316\u5408\u7269\u9884\u6d4b\u6027\u80fd\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\uff0c\u6700\u6709\u4ef7\u503c\u7684\u5316\u5408\u7269\uff08\u5982\u9ad8\u6548\u529b\uff09\u901a\u5e38\u5360\u636e\u76ee\u6807\u7a7a\u95f4\u7684\u7a00\u758f\u533a\u57df\u3002\u6807\u51c6\u56fe\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5e73\u5747\u8bef\u5dee\uff0c\u5728\u8fd9\u4e9b\u4e0d\u5e38\u89c1\u4f46\u5173\u952e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\u7ecf\u5e38\u626d\u66f2\u5206\u5b50\u62d3\u6251\u7ed3\u6784\u3002", "method": "SPECTRA\u6846\u67b6\uff1a(i)\u4eceSMILES\u91cd\u5efa\u591a\u5c5e\u6027\u5206\u5b50\u56fe\uff1b(ii)\u901a\u8fc7(Fused)Gromov-Wasserstein\u8026\u5408\u5bf9\u9f50\u5206\u5b50\u5bf9\u4ee5\u83b7\u5f97\u8282\u70b9\u5bf9\u5e94\u5173\u7cfb\uff1b(iii)\u5728\u7a33\u5b9a\u5171\u4eab\u57fa\u4e2d\u63d2\u503c\u62c9\u666e\u62c9\u65af\u7279\u5f81\u503c\u3001\u7279\u5f81\u5411\u91cf\u548c\u8282\u70b9\u7279\u5f81\uff1b(iv)\u91cd\u5efa\u8fb9\u4ee5\u5408\u6210\u5177\u6709\u63d2\u503c\u76ee\u6807\u7684\u7269\u7406\u5408\u7406\u4e2d\u95f4\u4f53\u3002\u91c7\u7528\u57fa\u4e8e\u6807\u7b7e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u7a00\u6709\u611f\u77e5\u9884\u7b97\u65b9\u6848\uff0c\u5c06\u589e\u5f3a\u96c6\u4e2d\u5728\u6570\u636e\u7a00\u7f3a\u533a\u57df\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPECTRA\u6301\u7eed\u6539\u5584\u76f8\u5173\u76ee\u6807\u8303\u56f4\u5185\u7684\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6574\u4f53MAE\uff0c\u5e76\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5408\u6210\u5206\u5b50\uff0c\u5176\u7ed3\u6784\u53cd\u6620\u4e86\u5e95\u5c42\u7684\u5149\u8c31\u51e0\u4f55\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5149\u8c31\u3001\u51e0\u4f55\u611f\u77e5\u7684\u589e\u5f3a\u662f\u89e3\u51b3\u4e0d\u5e73\u8861\u5206\u5b50\u5c5e\u6027\u56de\u5f52\u95ee\u9898\u7684\u6709\u6548\u4e14\u9ad8\u6548\u7b56\u7565\u3002"}}
{"id": "2511.04844", "pdf": "https://arxiv.org/pdf/2511.04844", "abs": "https://arxiv.org/abs/2511.04844", "authors": ["Matthew S. Zhang", "Stephen Huan", "Jerry Huang", "Nicholas M. Boffi", "Sitan Chen", "Sinho Chewi"], "title": "Sublinear iterations can suffice even for DDPMs", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "SDE-based methods such as denoising diffusion probabilistic models (DDPMs)\nhave shown remarkable success in real-world sample generation tasks. Prior\nanalyses of DDPMs have been focused on the exponential Euler discretization,\nshowing guarantees that generally depend at least linearly on the dimension or\ninitial Fisher information. Inspired by works in log-concave sampling (Shen and\nLee, 2019), we analyze an integrator -- the denoising diffusion randomized\nmidpoint method (DDRaM) -- that leverages an additional randomized midpoint to\nbetter approximate the SDE. Using a recently-developed analytic framework\ncalled the \"shifted composition rule\", we show that this algorithm enjoys\nfavorable discretization properties under appropriate smoothness assumptions,\nwith sublinear $\\widetilde{O}(\\sqrt{d})$ score evaluations needed to ensure\nconvergence. This is the first sublinear complexity bound for pure DDPM\nsampling -- prior works which obtained such bounds worked instead with\nODE-based sampling and had to make modifications to the sampler which deviate\nfrom how they are used in practice. We also provide experimental validation of\nthe advantages of our method, showing that it performs well in practice with\npre-trained image synthesis models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDDRaM\u7684SDE\u79ef\u5206\u5668\uff0c\u901a\u8fc7\u4f7f\u7528\u968f\u673a\u4e2d\u70b9\u6765\u66f4\u597d\u5730\u8fd1\u4f3cSDE\uff0c\u5b9e\u73b0\u4e86DDPM\u91c7\u6837\u7684\u9996\u4e2a\u6b21\u7ebf\u6027\u590d\u6742\u5ea6\u754c\u9650\uff0c\u4ec5\u9700O(\u221ad)\u6b21\u8bc4\u5206\u8bc4\u4f30\u5373\u53ef\u786e\u4fdd\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u7684DDPM\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u6307\u6570\u6b27\u62c9\u79bb\u6563\u5316\uff0c\u5176\u6536\u655b\u4fdd\u8bc1\u901a\u5e38\u81f3\u5c11\u7ebf\u6027\u4f9d\u8d56\u4e8e\u7ef4\u5ea6\u6216\u521d\u59cbFisher\u4fe1\u606f\u3002\u672c\u6587\u53d7\u5bf9\u6570\u51f9\u91c7\u6837\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684SDE\u79ef\u5206\u5668\u3002", "method": "\u63d0\u51fa\u4e86\u53bb\u566a\u6269\u6563\u968f\u673a\u4e2d\u70b9\u65b9\u6cd5(DDRaM)\uff0c\u5229\u7528\u968f\u673a\u4e2d\u70b9\u6765\u6539\u8fdbSDE\u8fd1\u4f3c\u3002\u4f7f\u7528\u65b0\u5f00\u53d1\u7684\"\u79fb\u4f4d\u7ec4\u5408\u89c4\u5219\"\u5206\u6790\u6846\u67b6\uff0c\u5728\u9002\u5f53\u7684\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\u5206\u6790\u8be5\u7b97\u6cd5\u7684\u79bb\u6563\u5316\u7279\u6027\u3002", "result": "DDRaM\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u590d\u6742\u5ea6\u754c\u9650\uff0c\u4ec5\u9700O(\u221ad)\u6b21\u8bc4\u5206\u8bc4\u4f30\u5373\u53ef\u786e\u4fdd\u6536\u655b\uff0c\u8fd9\u662f\u7eafDDPM\u91c7\u6837\u7684\u9996\u4e2a\u6b21\u7ebf\u6027\u590d\u6742\u5ea6\u754c\u9650\u3002\u5148\u524d\u83b7\u5f97\u6b64\u7c7b\u754c\u9650\u7684\u5de5\u4f5c\u9700\u8981\u4fee\u6539\u91c7\u6837\u5668\uff0c\u504f\u79bb\u5b9e\u9645\u4f7f\u7528\u65b9\u5f0f\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660eDDRaM\u5728\u5b9e\u9645\u9884\u8bad\u7ec3\u56fe\u50cf\u5408\u6210\u6a21\u578b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3aDDPM\u91c7\u6837\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u9009\u62e9\u3002"}}
{"id": "2511.04845", "pdf": "https://arxiv.org/pdf/2511.04845", "abs": "https://arxiv.org/abs/2511.04845", "authors": ["Jingchen Bi", "Rodrigo Mesa-Arango"], "title": "Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches", "categories": ["cs.LG"], "comment": null, "summary": "This paper utilizes a machine learning model to estimate the consumer's\nbehavior for food products with innovative transportation certificates in the\nU.S. Building on previous research that examined demand for food products with\nsupply chain traceability using stated preference analysis, transportation\nfactors were identified as significant in consumer food purchasing choices.\nConsequently, a second experiment was conducted to pinpoint the specific\ntransportation attributes valued by consumers. A machine learning model was\napplied, and five innovative certificates related to transportation were\nproposed: Transportation Mode, Internet of Things (IoT), Safety measures,\nEnergy Source, and Must Arrive By Dates (MABDs). The preference experiment also\nincorporated product-specific and decision-maker factors for control purposes.\nThe findings reveal a notable inclination toward safety and energy certificates\nwithin the transportation domain of the U.S. food supply chain. Additionally,\nthe study examined the influence of price, product type, certificates, and\ndecision-maker factors on purchasing choices. Ultimately, the study offers\ndata-driven recommendations for improving food supply chain systems.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u7f8e\u56fd\u6d88\u8d39\u8005\u5bf9\u5177\u6709\u521b\u65b0\u8fd0\u8f93\u8bc1\u4e66\u98df\u54c1\u7684\u504f\u597d\uff0c\u53d1\u73b0\u5b89\u5168\u6027\u548c\u80fd\u6e90\u8bc1\u4e66\u6700\u53d7\u91cd\u89c6", "motivation": "\u57fa\u4e8e\u5148\u524d\u7814\u7a76\u53d1\u73b0\u8fd0\u8f93\u56e0\u7d20\u5728\u6d88\u8d39\u8005\u98df\u54c1\u8d2d\u4e70\u9009\u62e9\u4e2d\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u5177\u4f53\u8bc6\u522b\u6d88\u8d39\u8005\u91cd\u89c6\u7684\u8fd0\u8f93\u5c5e\u6027", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u51fa\u4e94\u79cd\u521b\u65b0\u8fd0\u8f93\u8bc1\u4e66\uff08\u8fd0\u8f93\u6a21\u5f0f\u3001\u7269\u8054\u7f51\u3001\u5b89\u5168\u63aa\u65bd\u3001\u80fd\u6e90\u6765\u6e90\u3001\u5fc5\u987b\u5230\u8fbe\u65e5\u671f\uff09\uff0c\u5e76\u8fdb\u884c\u504f\u597d\u5b9e\u9a8c\u63a7\u5236\u4ea7\u54c1\u548c\u51b3\u7b56\u8005\u56e0\u7d20", "result": "\u7814\u7a76\u53d1\u73b0\u6d88\u8d39\u8005\u660e\u663e\u503e\u5411\u4e8e\u8fd0\u8f93\u9886\u57df\u7684\u5b89\u5168\u548c\u80fd\u6e90\u8bc1\u4e66\uff0c\u540c\u65f6\u5206\u6790\u4e86\u4ef7\u683c\u3001\u4ea7\u54c1\u7c7b\u578b\u3001\u8bc1\u4e66\u548c\u51b3\u7b56\u8005\u56e0\u7d20\u5bf9\u8d2d\u4e70\u9009\u62e9\u7684\u5f71\u54cd", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u98df\u54c1\u4f9b\u5e94\u94fe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u5efa\u8bae"}}
{"id": "2511.04847", "pdf": "https://arxiv.org/pdf/2511.04847", "abs": "https://arxiv.org/abs/2511.04847", "authors": ["Arthur Chen", "Zuxin Liu", "Jianguo Zhang", "Akshara Prabhakar", "Zhiwei Liu", "Shelby Heinecke", "Silvio Savarese", "Victor Zhong", "Caiming Xiong"], "title": "Grounded Test-Time Adaptation for LLM Agents", "categories": ["cs.LG"], "comment": "Preprint. Under review", "summary": "Large language model (LLM)-based agents struggle to generalize to novel and\ncomplex environments, such as unseen websites or new sets of functions, due to\na fundamental mismatch between their pre-training and test-time conditions.\nThis challenge stems from two distinct failure modes: a syntactic\nmisunderstanding of environment-specific components like observation formats,\nand a semantic misunderstanding of state-transition dynamics, which are only\nrevealed at test time. To address these issues, we propose two distinct and\ncomplementary strategies for adapting LLM agents by leveraging\nenvironment-specific information available during deployment. First, an online\ndistributional adaptation method parameterizes environmental nuances by\nlearning a lightweight adaptation vector that biases the model's output\ndistribution, enabling rapid alignment with an environment response format.\nSecond, a deployment-time dynamics grounding method employs a persona-driven\nexploration phase to systematically probe and learn the environment's causal\ndynamics before task execution, equipping the agent with a nonparametric world\nmodel. We evaluate these strategies across diverse agentic benchmarks,\nincluding function calling and web navigation. Our empirical results show the\neffectiveness of both strategies across all benchmarks with minimal\ncomputational cost. We find that dynamics grounding is particularly effective\nin complex environments where unpredictable dynamics pose a major obstacle,\ndemonstrating a robust path toward more generalizable and capable LLM-based\nagents. For example, on the WebArena multi-site split, this method increases\nthe agent's success rate from 2% to 23%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u6765\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u5728\u7ebf\u5206\u5e03\u9002\u5e94\u548c\u90e8\u7f72\u65f6\u52a8\u6001\u57fa\u7840\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u672a\u89c1\u8fc7\u7684\u590d\u6742\u73af\u5883\uff08\u5982\u65b0\u7f51\u7ad9\u6216\u51fd\u6570\u96c6\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u9884\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6761\u4ef6\u4e0d\u5339\u914d\uff0c\u5b58\u5728\u8bed\u6cd5\u8bef\u89e3\u548c\u8bed\u4e49\u8bef\u89e3\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\u3002", "method": "1. \u5728\u7ebf\u5206\u5e03\u9002\u5e94\uff1a\u5b66\u4e60\u8f7b\u91cf\u7ea7\u9002\u5e94\u5411\u91cf\u6765\u504f\u7f6e\u6a21\u578b\u8f93\u51fa\u5206\u5e03\uff0c\u5feb\u901f\u5bf9\u9f50\u73af\u5883\u54cd\u5e94\u683c\u5f0f\uff1b2. \u90e8\u7f72\u65f6\u52a8\u6001\u57fa\u7840\u5316\uff1a\u901a\u8fc7\u89d2\u8272\u9a71\u52a8\u63a2\u7d22\u9636\u6bb5\u7cfb\u7edf\u6027\u5730\u63a2\u6d4b\u548c\u5b66\u4e60\u73af\u5883\u7684\u56e0\u679c\u52a8\u6001\uff0c\u6784\u5efa\u975e\u53c2\u6570\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5728\u51fd\u6570\u8c03\u7528\u548c\u7f51\u9875\u5bfc\u822a\u7b49\u591a\u6837\u5316\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u7b56\u7565\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u6700\u5c0f\u3002\u5728WebArena\u591a\u7ad9\u70b9\u5206\u5272\u4e0a\uff0c\u52a8\u6001\u57fa\u7840\u5316\u65b9\u6cd5\u5c06\u6210\u529f\u7387\u4ece2%\u63d0\u5347\u523023%\u3002", "conclusion": "\u52a8\u6001\u57fa\u7840\u5316\u5728\u590d\u6742\u73af\u5883\u4e2d\u7279\u522b\u6709\u6548\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u548c\u5f3a\u5927\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\u3002"}}
{"id": "2511.04854", "pdf": "https://arxiv.org/pdf/2511.04854", "abs": "https://arxiv.org/abs/2511.04854", "authors": ["Alvaro Prat", "Leo Zhang", "Charlotte M. Deane", "Yee Whye Teh", "Garrett M. Morris"], "title": "SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion", "categories": ["cs.LG", "q-bio.QM"], "comment": "Preprint", "summary": "Determining the binding pose of a ligand to a protein, known as molecular\ndocking, is a fundamental task in drug discovery. Generative approaches promise\nfaster, improved, and more diverse pose sampling than physics-based methods,\nbut are often hindered by chemically implausible outputs, poor\ngeneralisability, and high computational cost. To address these challenges, we\nintroduce a novel fragmentation scheme, leveraging inductive biases from\nstructural chemistry, to decompose ligands into rigid-body fragments. Building\non this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion\nmodel that generates poses by learning to reassemble these rigid bodies within\nthe binding pocket. By operating at the level of fragments in SE(3), SigmaDock\nexploits well-established geometric priors while avoiding overly complex\ndiffusion processes and unstable training dynamics. Experimentally, we show\nSigmaDock achieves state-of-the-art performance, reaching Top-1 success rates\n(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%\nreported by recent deep learning approaches, whilst demonstrating consistent\ngeneralisation to unseen proteins. SigmaDock is the first deep learning\napproach to surpass classical physics-based docking under the PB train-test\nsplit, marking a significant leap forward in the reliability and feasibility of\ndeep learning for molecular modelling.", "AI": {"tldr": "SigmaDock\u662f\u4e00\u79cd\u57fa\u4e8eSE(3)\u9ece\u66fc\u6269\u6563\u6a21\u578b\u7684\u5206\u5b50\u5bf9\u63a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u914d\u4f53\u5206\u89e3\u4e3a\u521a\u6027\u7247\u6bb5\u5e76\u5728\u7ed3\u5408\u53e3\u888b\u4e2d\u91cd\u65b0\u7ec4\u88c5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5bf9\u63a5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0f\u5206\u5b50\u5bf9\u63a5\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5316\u5b66\u4e0d\u5408\u7406\u8f93\u51fa\u3001\u6cdb\u5316\u6027\u5dee\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u7247\u6bb5\u5316\u65b9\u6848\uff0c\u5c06\u914d\u4f53\u5206\u89e3\u4e3a\u521a\u6027\u7247\u6bb5\uff0c\u7136\u540e\u4f7f\u7528SE(3)\u9ece\u66fc\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5728\u7ed3\u5408\u53e3\u888b\u4e2d\u91cd\u65b0\u7ec4\u88c5\u8fd9\u4e9b\u7247\u6bb5\u3002", "result": "\u5728PoseBusters\u6570\u636e\u96c6\u4e0a\u8fbe\u523079.9%\u7684Top-1\u6210\u529f\u7387\uff0c\u8fdc\u8d85\u8fd1\u671f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5(12.7-30.8%)\uff0c\u5e76\u4e14\u9996\u6b21\u8d85\u8d8a\u7ecf\u5178\u7269\u7406\u5bf9\u63a5\u65b9\u6cd5\u3002", "conclusion": "SigmaDock\u5728\u5206\u5b50\u5efa\u6a21\u7684\u53ef\u9760\u6027\u548c\u53ef\u884c\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u662f\u7b2c\u4e00\u4e2a\u5728PB\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\u4e0b\u8d85\u8d8a\u7ecf\u5178\u7269\u7406\u5bf9\u63a5\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2511.04856", "pdf": "https://arxiv.org/pdf/2511.04856", "abs": "https://arxiv.org/abs/2511.04856", "authors": ["Thore Gerlach", "Michael Schenk", "Verena Kain"], "title": "Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "We introduce theoretically grounded Continuous Semi-Quantum Boltzmann\nMachines (CSQBMs) that supports continuous-action reinforcement learning. By\ncombining exponential-family priors over visible units with quantum Boltzmann\ndistributions over hidden units, CSQBMs yield a hybrid quantum-classical model\nthat reduces qubit requirements while retaining strong expressiveness.\nCrucially, gradients with respect to continuous variables can be computed\nanalytically, enabling direct integration into Actor-Critic algorithms.\nBuilding on this, we propose a continuous Q-learning framework that replaces\nglobal maximization by efficient sampling from the CSQBM distribution, thereby\novercoming instability issues in continuous control.", "AI": {"tldr": "\u63d0\u51fa\u7406\u8bba\u57fa\u7840\u7684\u8fde\u7eed\u534a\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a(CSQBMs)\uff0c\u652f\u6301\u8fde\u7eed\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u89c1\u5355\u5143\u7684\u6307\u6570\u65cf\u5148\u9a8c\u548c\u9690\u85cf\u5355\u5143\u7684\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u6784\u5efa\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u63a7\u5236\u4e2d\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5f00\u53d1\u80fd\u591f\u652f\u6301\u8fde\u7eed\u52a8\u4f5c\u7684\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u964d\u4f4e\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u6307\u6570\u65cf\u5148\u9a8c\u548c\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u6784\u5efa\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u5229\u7528\u89e3\u6790\u68af\u5ea6\u8ba1\u7b97\u5b9e\u73b0\u4e0eActor-Critic\u7b97\u6cd5\u7684\u76f4\u63a5\u96c6\u6210\uff0c\u63d0\u51fa\u7528CSQBM\u5206\u5e03\u91c7\u6837\u66ff\u4ee3\u5168\u5c40\u6700\u5927\u5316\u7684\u8fde\u7eedQ\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u51fa\u7406\u8bba\u57fa\u7840\u7684CSQBMs\u6a21\u578b\uff0c\u80fd\u591f\u8ba1\u7b97\u8fde\u7eed\u53d8\u91cf\u7684\u89e3\u6790\u68af\u5ea6\uff0c\u6784\u5efa\u4e86\u7a33\u5b9a\u7684\u8fde\u7eed\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "conclusion": "CSQBMs\u4e3a\u8fde\u7eed\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51cf\u5c11\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2511.04865", "pdf": "https://arxiv.org/pdf/2511.04865", "abs": "https://arxiv.org/abs/2511.04865", "authors": ["Esha Sharma", "Lauren Davis", "Julie Ivy", "Min Chi"], "title": "FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Food banks are crucial for alleviating food insecurity, but their\neffectiveness hinges on accurately forecasting highly volatile in-kind\ndonations to ensure equitable and efficient resource distribution. Traditional\nforecasting models often fail to maintain consistent accuracy due to\nunpredictable fluctuations and concept drift driven by seasonal variations and\nnatural disasters such as hurricanes in the Southeastern U.S. and wildfires in\nthe West Coast. To address these challenges, we propose FoodRL, a novel\nreinforcement learning (RL) based metalearning framework that clusters and\ndynamically weights diverse forecasting models based on recent performance and\ncontextual information. Evaluated on multi-year data from two structurally\ndistinct U.S. food banks-one large regional West Coast food bank affected by\nwildfires and another state-level East Coast food bank consistently impacted by\nhurricanes, FoodRL consistently outperforms baseline methods, particularly\nduring periods of disruption or decline. By delivering more reliable and\nadaptive forecasts, FoodRL can facilitate the redistribution of food equivalent\nto 1.7 million additional meals annually, demonstrating its significant\npotential for social impact as well as adaptive ensemble learning for\nhumanitarian supply chains.", "AI": {"tldr": "FoodRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u98df\u54c1\u94f6\u884c\u7684\u5b9e\u7269\u6350\u8d60\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u6765\u5e94\u5bf9\u6ce2\u52a8\u6027\u548c\u6982\u5ff5\u6f02\u79fb\uff0c\u5728\u707e\u5bb3\u671f\u95f4\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u98df\u54c1\u94f6\u884c\u5bf9\u7f13\u89e3\u7cae\u98df\u4e0d\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u9ad8\u5ea6\u6ce2\u52a8\u7684\u5b9e\u7269\u6350\u8d60\uff0c\u7279\u522b\u662f\u5728\u5b63\u8282\u6027\u53d8\u5316\u548c\u81ea\u7136\u707e\u5bb3\uff08\u5982\u98d3\u98ce\u3001\u91ce\u706b\uff09\u5bfc\u81f4\u7684\u4e0d\u53ef\u9884\u6d4b\u6ce2\u52a8\u548c\u6982\u5ff5\u6f02\u79fb\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faFoodRL\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5143\u5b66\u4e60\uff0c\u57fa\u4e8e\u8fd1\u671f\u6027\u80fd\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e0d\u540c\u7684\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u805a\u7c7b\u548c\u52a8\u6001\u52a0\u6743\u3002", "result": "\u5728\u4e24\u4e2a\u7ed3\u6784\u4e0d\u540c\u7684\u7f8e\u56fd\u98df\u54c1\u94f6\u884c\uff08\u53d7\u91ce\u706b\u5f71\u54cd\u7684\u897f\u6d77\u5cb8\u5927\u578b\u533a\u57df\u98df\u54c1\u94f6\u884c\u548c\u53d7\u98d3\u98ce\u5f71\u54cd\u7684\u4e1c\u6d77\u5cb8\u5dde\u7ea7\u98df\u54c1\u94f6\u884c\uff09\u7684\u591a\u5e74\u5ea6\u6570\u636e\u8bc4\u4f30\u4e2d\uff0cFoodRL\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4e2d\u65ad\u6216\u4e0b\u964d\u671f\u95f4\u3002", "conclusion": "FoodRL\u901a\u8fc7\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u9884\u6d4b\uff0c\u6bcf\u5e74\u53ef\u4fc3\u8fdb\u76f8\u5f53\u4e8e\u989d\u5916170\u4e07\u9910\u7684\u98df\u54c1\u91cd\u65b0\u5206\u914d\uff0c\u5c55\u793a\u4e86\u5176\u5728\u793e\u4f1a\u5f71\u54cd\u548c\u4eba\u9053\u4e3b\u4e49\u4f9b\u5e94\u94fe\u81ea\u9002\u5e94\u96c6\u6210\u5b66\u4e60\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.04883", "pdf": "https://arxiv.org/pdf/2511.04883", "abs": "https://arxiv.org/abs/2511.04883", "authors": ["Di Chen", "Jia Li", "Michael Zhang"], "title": "Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Autonomous vehicles (AVs) are expected to be commercially available in the\nnear future, leading to mixed autonomy traffic consisting of both AVs and\nhuman-driven vehicles (HVs). Although numerous studies have shown that AVs can\nbe deployed to benefit the overall traffic system performance by incorporating\nsystem-level goals into their decision making, it is not clear whether the\nbenefits still exist when agents act out of self-interest -- a trait common to\nall driving agents, both human and autonomous. This study aims to understand\nwhether self-interested AVs can bring benefits to all driving agents in mixed\nautonomy traffic systems. The research is centered on the concept of collective\nrationality (CR). This concept, originating from game theory and behavioral\neconomics, means that driving agents may cooperate collectively even when\npursuing individual interests. Our recent research has proven the existence of\nCR in an analytical game-theoretical model and empirically in mixed\nhuman-driven traffic. In this paper, we demonstrate that CR can be attained\namong driving agents trained using deep reinforcement learning (DRL) with a\nsimple reward design. We examine the extent to which self-interested traffic\nagents can achieve CR without directly incorporating system-level objectives.\nResults show that CR consistently emerges in various scenarios, which indicates\nthe robustness of this property. We also postulate a mechanism to explain the\nemergence of CR in the microscopic and dynamic environment and verify it based\non simulation evidence. This research suggests the possibility of leveraging\nadvanced learning methods (such as federated learning) to achieve collective\ncooperation among self-interested driving agents in mixed-autonomy systems.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u9a7e\u9a76\u4ee3\u7406\uff08\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff09\u51fa\u4e8e\u81ea\u8eab\u5229\u76ca\u884c\u52a8\uff0c\u4e5f\u80fd\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u96c6\u4f53\u7406\u6027\uff0c\u4ece\u800c\u4e3a\u6240\u6709\u53c2\u4e0e\u8005\u5e26\u6765\u76ca\u5904\u3002", "motivation": "\u63a2\u8ba8\u5728\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u4e2d\uff0c\u5f53\u6240\u6709\u9a7e\u9a76\u4ee3\u7406\uff08\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff09\u90fd\u51fa\u4e8e\u81ea\u8eab\u5229\u76ca\u884c\u52a8\u65f6\uff0c\u662f\u5426\u4ecd\u80fd\u4e3a\u6574\u4e2a\u4ea4\u901a\u7cfb\u7edf\u5e26\u6765\u76ca\u5904\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9a7e\u9a76\u4ee3\u7406\uff0c\u91c7\u7528\u7b80\u5355\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u4e0d\u76f4\u63a5\u5305\u542b\u7cfb\u7edf\u7ea7\u76ee\u6807\uff0c\u7814\u7a76\u96c6\u4f53\u7406\u6027\u662f\u5426\u80fd\u5728\u5404\u79cd\u573a\u666f\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002", "result": "\u96c6\u4f53\u7406\u6027\u5728\u5404\u79cd\u573a\u666f\u4e2d\u6301\u7eed\u6d8c\u73b0\uff0c\u8868\u660e\u8fd9\u4e00\u7279\u6027\u5177\u6709\u9c81\u68d2\u6027\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u89e3\u91ca\u96c6\u4f53\u7406\u6027\u5728\u5fae\u89c2\u52a8\u6001\u73af\u5883\u4e2d\u6d8c\u73b0\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u8bc1\u636e\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u5229\u7528\u5148\u8fdb\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u8054\u90a6\u5b66\u4e60\uff09\u5728\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u81ea\u5229\u9a7e\u9a76\u4ee3\u7406\u4e4b\u95f4\u7684\u96c6\u4f53\u5408\u4f5c\uff0c\u4e3a\u4ea4\u901a\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.04902", "pdf": "https://arxiv.org/pdf/2511.04902", "abs": "https://arxiv.org/abs/2511.04902", "authors": ["Shuvendu Roy", "Hossein Hajimirsadeghi", "Mengyao Zhai", "Golnoosh Samei"], "title": "You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models", "categories": ["cs.LG", "cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: MATH-AI", "summary": "Recent advances in large language models have demonstrated the promise of\nunsupervised reinforcement learning (RL) methods for enhancing reasoning\ncapabilities without external supervision. However, the generalizability of\nthese label-free RL approaches to smaller base models with limited reasoning\ncapabilities remains unexplored. In this work, we systematically investigate\nthe performance of label-free RL methods across different model sizes and\nreasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals\ncritical limitations: label-free RL is highly dependent on the base model's\npre-existing reasoning capability, with performance often degrading below\nbaseline levels for weaker models. We find that smaller models fail to generate\nsufficiently long or diverse chain-of-thought reasoning to enable effective\nself-reflection, and that training data difficulty plays a crucial role in\ndetermining success. To address these challenges, we propose a simple yet\neffective method for label-free RL that utilizes curriculum learning to\nprogressively introduce harder problems during training and mask no-majority\nrollouts during training. Additionally, we introduce a data curation pipeline\nto generate samples with predefined difficulty. Our approach demonstrates\nconsistent improvements across all model sizes and reasoning capabilities,\nproviding a path toward more robust unsupervised RL that can bootstrap\nreasoning abilities in resource-constrained models. We make our code available\nat https://github.com/BorealisAI/CuMa", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65e0\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff080.5B-7B\u53c2\u6570\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u8f83\u5f31\u6a21\u578b\u6548\u679c\u4e0d\u4f73\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5f15\u5165\u96be\u9898\u548c\u5c4f\u853d\u65e0\u591a\u6570\u5171\u8bc6\u7684rollout\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u65e0\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63a8\u7406\u80fd\u529b\u6709\u9650\u7684\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4f9d\u8d56\u8fc7\u5f3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u65e0\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6e10\u8fdb\u5f0f\u5f15\u5165\u66f4\u56f0\u96be\u7684\u95ee\u9898\uff1b2\uff09\u5c4f\u853d\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u65e0\u591a\u6570\u5171\u8bc6\u7684rollout\uff1b3\uff09\u5efa\u7acb\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\u751f\u6210\u9884\u5b9a\u4e49\u96be\u5ea6\u7684\u6837\u672c\u3002", "result": "\u6539\u8fdb\u65b9\u6cd5\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u90fd\u8868\u73b0\u51fa\u6301\u7eed\u63d0\u5347\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u81ea\u4e3e\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\u3002", "conclusion": "\u65e0\u6807\u7b7e\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u6570\u636e\u7b5b\u9009\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u89c4\u6a21\u6a21\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.04904", "pdf": "https://arxiv.org/pdf/2511.04904", "abs": "https://arxiv.org/abs/2511.04904", "authors": ["Bassel Al Omari", "Michael Matthews", "Alexander Rutherford", "Jakob Nicolaus Foerster"], "title": "Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Progress in multi-agent reinforcement learning (MARL) requires challenging\nbenchmarks that assess the limits of current methods. However, existing\nbenchmarks often target narrow short-horizon challenges that do not adequately\nstress the long-term dependencies and generalization capabilities inherent in\nmany multi-agent systems. To address this, we first present\n\\textit{Craftax-MA}: an extension of the popular open-ended RL environment,\nCraftax, that supports multiple agents and evaluates a wide range of general\nabilities within a single environment. Written in JAX, \\textit{Craftax-MA} is\nexceptionally fast with a training run using 250 million environment\ninteractions completing in under an hour. To provide a more compelling\nchallenge for MARL, we also present \\textit{Craftax-Coop}, an extension\nintroducing heterogeneous agents, trading and more mechanics that require\ncomplex cooperation among agents for success. We provide analysis demonstrating\nthat existing algorithms struggle with key challenges in this benchmark,\nincluding long-horizon credit assignment, exploration and cooperation, and\nargue for its potential to drive long-term research in MARL.", "AI": {"tldr": "\u63d0\u51fa\u4e86Craftax-MA\u548cCraftax-Coop\u4e24\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u73af\u5883\uff0c\u524d\u8005\u6269\u5c55\u4e86Craftax\u652f\u6301\u591a\u667a\u80fd\u4f53\uff0c\u540e\u8005\u5f15\u5165\u5f02\u8d28\u667a\u80fd\u4f53\u3001\u4ea4\u6613\u7b49\u673a\u5236\uff0c\u9700\u8981\u590d\u6742\u5408\u4f5c\u3002\u57fa\u4e8eJAX\u5b9e\u73b0\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb\uff0c\u80fd\u8bc4\u4f30\u957f\u671f\u4f9d\u8d56\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MARL\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u77ed\u671f\u6311\u6218\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u957f\u671f\u4f9d\u8d56\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u63a8\u52a8MARL\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u6269\u5c55Craftax\u73af\u5883\u652f\u6301\u591a\u667a\u80fd\u4f53\uff08Craftax-MA\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165\u5f02\u8d28\u667a\u80fd\u4f53\u3001\u4ea4\u6613\u7b49\u5408\u4f5c\u673a\u5236\uff08Craftax-Coop\uff09\uff0c\u57fa\u4e8eJAX\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u73b0\u6709\u7b97\u6cd5\u5728\u8be5\u57fa\u51c6\u4e2d\u9762\u4e34\u957f\u671f\u4fe1\u7528\u5206\u914d\u3001\u63a2\u7d22\u548c\u5408\u4f5c\u7b49\u5173\u952e\u6311\u6218\uff0c\u8bad\u7ec3250M\u73af\u5883\u4ea4\u4e92\u53ef\u57281\u5c0f\u65f6\u5185\u5b8c\u6210\u3002", "conclusion": "Craftax\u57fa\u51c6\u73af\u5883\u4e3aMARL\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u6f5c\u529b\u63a8\u52a8\u957f\u671f\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2511.04907", "pdf": "https://arxiv.org/pdf/2511.04907", "abs": "https://arxiv.org/abs/2511.04907", "authors": ["Lunjia Hu", "Haipeng Luo", "Spandan Senapati", "Vatsal Sharan"], "title": "Efficient Swap Multicalibration of Elicitable Properties", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Multicalibration [HJKRR18] is an algorithmic fairness perspective that\ndemands that the predictions of a predictor are correct conditional on\nthemselves and membership in a collection of potentially overlapping subgroups\nof a population. The work of [NR23] established a surprising connection between\nmulticalibration for an arbitrary property $\\Gamma$ (e.g., mean or median) and\nproperty elicitation: a property $\\Gamma$ can be multicalibrated if and only if\nit is elicitable, where elicitability is the notion that the true property\nvalue of a distribution can be obtained by solving a regression problem over\nthe distribution. In the online setting, [NR23] proposed an inefficient\nalgorithm that achieves $\\sqrt T$ $\\ell_2$-multicalibration error for a\nhypothesis class of group membership functions and an elicitable property\n$\\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.\n  In this paper, we generalize multicalibration for an elicitable property\n$\\Gamma$ from group membership functions to arbitrary bounded hypothesis\nclasses and introduce a stronger notion -- swap multicalibration, following\n[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when\ngiven access to an online agnostic learner, achieves $T^{1/(r+1)}$\n$\\ell_r$-swap multicalibration error with high probability (for $r\\ge2$) for a\nhypothesis class with bounded sequential Rademacher complexity and an\nelicitable property $\\Gamma$. For the special case of $r=2$, this implies an\noracle-efficient algorithm that achieves $T^{1/3}$ $\\ell_2$-swap\nmulticalibration error, which significantly improves on the previously\nestablished bounds for the problem [NR23, GMS25, LSS25a], and completely\nresolves an open question raised in [GJRR24] on the possibility of an\noracle-efficient algorithm that achieves $\\sqrt{T}$ $\\ell_2$-mean\nmulticalibration error by answering it in a strongly affirmative sense.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u53ef\u5f15\u51fa\u5c5e\u6027\u7684\u591a\u6821\u51c6\u6cdb\u5316\uff0c\u4ece\u7fa4\u7ec4\u6210\u5458\u51fd\u6570\u6269\u5c55\u5230\u4efb\u610f\u6709\u754c\u5047\u8bbe\u7c7b\uff0c\u5e76\u5f15\u5165\u66f4\u5f3a\u7684\u4ea4\u6362\u591a\u6821\u51c6\u6982\u5ff5\u3002\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cdoracle\u9ad8\u6548\u7b97\u6cd5\uff0c\u5728\u7ed9\u5b9a\u5728\u7ebf\u4e0d\u53ef\u77e5\u5b66\u4e60\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u9ad8\u6982\u7387\u5b9e\u73b0T^{1/(r+1)}\u7684\u2113_r-\u4ea4\u6362\u591a\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c[NR23]\u5efa\u7acb\u4e86\u591a\u6821\u51c6\u4e0e\u5c5e\u6027\u5f15\u51fa\u6027\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\uff0c\u4f46\u5176\u5728\u7ebf\u7b97\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u4e3a\u4efb\u610f\u6709\u754c\u5047\u8bbe\u7c7b\u548c\u53ef\u5f15\u51fa\u5c5e\u6027\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684oracle\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u8bef\u5dee\u754c\u9650\u3002", "method": "\u5c06\u591a\u6821\u51c6\u4ece\u7fa4\u7ec4\u6210\u5458\u51fd\u6570\u6cdb\u5316\u5230\u4efb\u610f\u6709\u754c\u5047\u8bbe\u7c7b\uff0c\u5f15\u5165\u4ea4\u6362\u591a\u6821\u51c6\u6982\u5ff5\u3002\u63d0\u51fa\u57fa\u4e8e\u5728\u7ebf\u4e0d\u53ef\u77e5\u5b66\u4e60\u5668\u7684oracle\u9ad8\u6548\u7b97\u6cd5\uff0c\u5229\u7528\u5047\u8bbe\u7c7b\u7684\u6709\u754c\u987a\u5e8fRademacher\u590d\u6742\u5ea6\u3002", "result": "\u5bf9\u4e8er\u22652\uff0c\u7b97\u6cd5\u4ee5\u9ad8\u6982\u7387\u5b9e\u73b0T^{1/(r+1)}\u7684\u2113_r-\u4ea4\u6362\u591a\u6821\u51c6\u8bef\u5dee\u3002\u7279\u522b\u5730\uff0c\u5f53r=2\u65f6\uff0c\u5b9e\u73b0\u4e86T^{1/3}\u7684\u2113_2-\u4ea4\u6362\u591a\u6821\u51c6\u8bef\u5dee\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u73b0\u6709\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u5b8c\u5168\u89e3\u51b3\u4e86[GJRR24]\u4e2d\u5173\u4e8eoracle\u9ad8\u6548\u7b97\u6cd5\u80fd\u5426\u5b9e\u73b0\u221aT \u2113_2-\u5747\u503c\u591a\u6821\u51c6\u8bef\u5dee\u7684\u5f00\u653e\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u5f3a\u70c8\u80af\u5b9a\u7684\u7b54\u6848\uff0c\u5e76\u4e3a\u591a\u6821\u51c6\u7406\u8bba\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7b97\u6cd5\u4fdd\u8bc1\u3002"}}
{"id": "2511.04909", "pdf": "https://arxiv.org/pdf/2511.04909", "abs": "https://arxiv.org/abs/2511.04909", "authors": ["Paula Rodriguez-Diaz", "Kirk Bansak Elisabeth Paulson"], "title": "A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many real-world decisions are made under uncertainty by solving optimization\nproblems using predicted quantities. This predict-then-optimize paradigm has\nmotivated decision-focused learning, which trains models with awareness of how\nthe optimizer uses predictions, improving the performance of downstream\ndecisions. Despite its promise, scaling is challenging: state-of-the-art\nmethods either differentiate through a solver or rely on task-specific\nsurrogates, both of which require frequent and expensive calls to an optimizer,\noften a combinatorial one. In this paper, we leverage dual variables from the\ndownstream problem to shape learning and introduce Dual-Guided Loss (DGL), a\nsimple, scalable objective that preserves decision alignment while reducing\nsolver dependence. We construct DGL specifically for combinatorial selection\nproblems with natural one-of-many constraints, such as matching, knapsack, and\nshortest path. Our approach (a) decouples optimization from gradient updates by\nsolving the downstream problem only periodically; (b) between refreshes, trains\non dual-adjusted targets using simple differentiable surrogate losses; and (c)\nas refreshes become less frequent, drives training cost toward standard\nsupervised learning while retaining strong decision alignment. We prove that\nDGL has asymptotically diminishing decision regret, analyze runtime complexity,\nand show on two problem classes that DGL matches or exceeds state-of-the-art\nDFL methods while using far fewer solver calls and substantially less training\ntime. Code is available at https://github.com/paularodr/Dual-Guided-Learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dual-Guided Loss (DGL)\uff0c\u4e00\u79cd\u7b80\u5355\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u5bfc\u5411\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e0b\u6e38\u95ee\u9898\u7684\u5bf9\u5076\u53d8\u91cf\u6765\u51cf\u5c11\u5bf9\u6c42\u89e3\u5668\u7684\u4f9d\u8d56\uff0c\u5728\u7ec4\u5408\u9009\u62e9\u95ee\u9898\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u51b3\u7b56\u5bfc\u5411\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u9891\u7e41\u8c03\u7528\u6c42\u89e3\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u51b3\u7b56\u5bf9\u9f50\u53c8\u80fd\u51cf\u5c11\u6c42\u89e3\u5668\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4e0b\u6e38\u7ec4\u5408\u9009\u62e9\u95ee\u9898\u7684\u5bf9\u5076\u53d8\u91cf\u6784\u5efaDGL\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6c42\u89e3\u4e0b\u6e38\u95ee\u9898\u6765\u89e3\u8026\u4f18\u5316\u548c\u68af\u5ea6\u66f4\u65b0\uff0c\u5728\u5237\u65b0\u95f4\u9694\u671f\u95f4\u4f7f\u7528\u5bf9\u5076\u8c03\u6574\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "DGL\u5728\u4e24\u79cd\u95ee\u9898\u7c7b\u522b\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684DFL\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6c42\u89e3\u5668\u8c03\u7528\u6b21\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u8bad\u7ec3\u6210\u672c\u63a5\u8fd1\u6807\u51c6\u76d1\u7763\u5b66\u4e60\u3002", "conclusion": "DGL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u5bfc\u5411\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u5f3a\u51b3\u7b56\u5bf9\u9f50\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5339\u914d\u3001\u80cc\u5305\u3001\u6700\u77ed\u8def\u5f84\u7b49\u7ec4\u5408\u9009\u62e9\u95ee\u9898\u3002"}}
{"id": "2511.04918", "pdf": "https://arxiv.org/pdf/2511.04918", "abs": "https://arxiv.org/abs/2511.04918", "authors": ["A. Ganapathi Rao", "Sathish Krishna Anumula", "Aditya Kumar Singh", "Renukhadevi M", "Y. Jeevan Nagendra Kumar", "Tammineni Rama Tulasi"], "title": "Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application", "categories": ["cs.LG"], "comment": "9 Pages, 4 Figures", "summary": "It involves the completely novel ways of integrating ML algorithms with\ntraditional statistical modelling that has changed the way we analyze data, do\npredictive analytics or make decisions in the fields of the data. In this\npaper, we study some ML and statistical model connections to understand ways in\nwhich some modern ML algorithms help 'enrich' conventional models; we\ndemonstrate how new algorithms improve performance, scale, flexibility and\nrobustness of the traditional models. It shows that the hybrid models are of\ngreat improvement in predictive accuracy, robustness, and interpretability", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e0e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u4e0e\u4f20\u7edf\u7edf\u8ba1\u5efa\u6a21\u7684\u65b0\u9896\u96c6\u6210\u65b9\u5f0f\uff0c\u6539\u53d8\u6570\u636e\u5206\u6790\u3001\u9884\u6d4b\u5206\u6790\u548c\u51b3\u7b56\u5236\u5b9a\u7684\u65b9\u6cd5", "method": "\u7814\u7a76\u673a\u5668\u5b66\u4e60\u4e0e\u7edf\u8ba1\u6a21\u578b\u7684\u8fde\u63a5\uff0c\u5c55\u793a\u73b0\u4ee3ML\u7b97\u6cd5\u5982\u4f55\u4e30\u5bcc\u4f20\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u63d0\u5347\u6027\u80fd", "result": "\u6df7\u5408\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u91cd\u5927\u6539\u8fdb", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e0e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u7684\u96c6\u6210\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3001\u6269\u5c55\u6027\u3001\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2511.04934", "pdf": "https://arxiv.org/pdf/2511.04934", "abs": "https://arxiv.org/abs/2511.04934", "authors": ["Hadi Reisizadeh", "Jiajun Ruan", "Yiwei Chen", "Soumyadeep Pal", "Sijia Liu", "Mingyi Hong"], "title": "Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding", "categories": ["cs.LG"], "comment": null, "summary": "Unlearning in large language models (LLMs) is critical for regulatory\ncompliance and for building ethical generative AI systems that avoid producing\nprivate, toxic, illegal, or copyrighted content. Despite rapid progress, in\nthis work we show that \\textit{almost all} existing unlearning methods fail to\nachieve true forgetting in practice. Specifically, while evaluations of these\n`unlearned' models under deterministic (greedy) decoding often suggest\nsuccessful knowledge removal using standard benchmarks (as has been done in the\nliterature), we show that sensitive information reliably resurfaces when models\nare sampled with standard probabilistic decoding. To rigorously capture this\nvulnerability, we introduce \\texttt{leak@$k$}, a new meta-evaluation metric\nthat quantifies the likelihood of forgotten knowledge reappearing when\ngenerating $k$ samples from the model under realistic decoding strategies.\nUsing three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the\nfirst large-scale, systematic study of unlearning reliability using our newly\ndefined \\texttt{leak@$k$} metric. Our findings demonstrate that knowledge\nleakage persists across methods and tasks, underscoring that current\nstate-of-the-art unlearning techniques provide only limited forgetting and\nhighlighting the urgent need for more robust approaches to LLM unlearning.", "AI": {"tldr": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u5728\u6982\u7387\u89e3\u7801\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u77e5\u8bc6\u9057\u5fd8\uff0c\u654f\u611f\u4fe1\u606f\u4f1a\u53ef\u9760\u5730\u91cd\u65b0\u51fa\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9057\u5fd8\u5bf9\u4e8e\u6cd5\u89c4\u5408\u89c4\u548c\u6784\u5efa\u9053\u5fb7\u751f\u6210AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u907f\u514d\u4ea7\u751f\u79c1\u4eba\u3001\u6709\u6bd2\u3001\u975e\u6cd5\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\u3002", "method": "\u5f15\u5165leak@k\u5143\u8bc4\u4f30\u6307\u6807\uff0c\u91cf\u5316\u5728k\u4e2a\u6837\u672c\u751f\u6210\u4e2d\u88ab\u9057\u5fd8\u77e5\u8bc6\u91cd\u65b0\u51fa\u73b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5728TOFU\u3001MUSE\u548cWMDP\u4e09\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u7cfb\u7edf\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u77e5\u8bc6\u6cc4\u9732\u5728\u65b9\u6cd5\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6280\u672f\u4ec5\u63d0\u4f9b\u6709\u9650\u7684\u9057\u5fd8\u6548\u679c\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u77e5\u8bc6\u79fb\u9664\uff0c\u8feb\u5207\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9057\u5fd8\u65b9\u6cd5\u3002"}}
{"id": "2511.04937", "pdf": "https://arxiv.org/pdf/2511.04937", "abs": "https://arxiv.org/abs/2511.04937", "authors": ["Zhankun Luo", "Abolfazl Hashemi"], "title": "Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression", "categories": ["cs.LG"], "comment": "Preprint of the paper submitted to IEEE Transactions on Information\n  Theory", "summary": "This work investigates the structural properties, cycloid trajectories, and\nnon-asymptotic convergence guarantees of the Expectation-Maximization (EM)\nalgorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing\nweights and regression parameters. Recent studies have established global\nconvergence for 2MLR with known balanced weights and super-linear convergence\nin noiseless and high signal-to-noise ratio (SNR) regimes. However, the\ntheoretical behavior of EM in the fully unknown setting remains unclear, with\nits trajectory and convergence order not yet fully characterized. We derive\nexplicit EM update expressions for 2MLR with unknown mixing weights and\nregression parameters across all SNR regimes and analyze their structural\nproperties and cycloid trajectories. In the noiseless case, we prove that the\ntrajectory of the regression parameters in EM iterations traces a cycloid by\nestablishing a recurrence relation for the sub-optimality angle, while in high\nSNR regimes we quantify its discrepancy from the cycloid trajectory. The\ntrajectory-based analysis reveals the order of convergence: linear when the EM\nestimate is nearly orthogonal to the ground truth, and quadratic when the angle\nbetween the estimate and ground truth is small at the population level. Our\nanalysis establishes non-asymptotic guarantees by sharpening bounds on\nstatistical errors between finite-sample and population EM updates, relating\nEM's statistical accuracy to the sub-optimality angle, and proving convergence\nwith arbitrary initialization at the finite-sample level. This work provides a\nnovel trajectory-based framework for analyzing EM in Mixed Linear Regression.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86EM\u7b97\u6cd5\u5728\u4e24\u5206\u91cf\u6df7\u5408\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u7ed3\u6784\u7279\u6027\u3001\u6446\u7ebf\u8f68\u8ff9\u548c\u975e\u6e10\u8fd1\u6536\u655b\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u6df7\u5408\u6743\u91cd\u548c\u56de\u5f52\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u5728\u5df2\u77e5\u5e73\u8861\u6743\u91cd\u548c\u9ad8\u4fe1\u566a\u6bd4\u60c5\u51b5\u4e0b\u5efa\u7acb\u4e86EM\u7b97\u6cd5\u7684\u5168\u5c40\u6536\u655b\u6027\uff0c\u4f46\u5728\u5b8c\u5168\u672a\u77e5\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u5176\u8f68\u8ff9\u548c\u6536\u655b\u9636\u6570\u5c1a\u672a\u5b8c\u5168\u8868\u5f81\u3002", "method": "\u63a8\u5bfc\u4e86\u5728\u6240\u6709\u4fe1\u566a\u6bd4\u60c5\u51b5\u4e0b\u672a\u77e5\u6df7\u5408\u6743\u91cd\u548c\u56de\u5f52\u53c2\u6570\u76842MLR\u7684\u663e\u5f0fEM\u66f4\u65b0\u8868\u8fbe\u5f0f\uff0c\u5206\u6790\u4e86\u5176\u7ed3\u6784\u7279\u6027\u548c\u6446\u7ebf\u8f68\u8ff9\u3002\u5728\u65e0\u566a\u58f0\u60c5\u51b5\u4e0b\u8bc1\u660e\u4e86\u56de\u5f52\u53c2\u6570\u8f68\u8ff9\u5f62\u6210\u6446\u7ebf\uff0c\u5728\u9ad8\u4fe1\u566a\u6bd4\u60c5\u51b5\u4e0b\u91cf\u5316\u4e86\u4e0e\u6446\u7ebf\u8f68\u8ff9\u7684\u504f\u5dee\u3002", "result": "\u8f68\u8ff9\u5206\u6790\u63ed\u793a\u4e86\u6536\u655b\u9636\u6570\uff1a\u5f53EM\u4f30\u8ba1\u4e0e\u771f\u5b9e\u503c\u51e0\u4e4e\u6b63\u4ea4\u65f6\u5448\u7ebf\u6027\u6536\u655b\uff0c\u5f53\u4f30\u8ba1\u4e0e\u771f\u5b9e\u503c\u5939\u89d2\u8f83\u5c0f\u65f6\u5448\u4e8c\u6b21\u6536\u655b\u3002\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u6c34\u5e73\u4e0a\u4efb\u610f\u521d\u59cb\u5316\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5206\u6790\u6df7\u5408\u7ebf\u6027\u56de\u5f52\u4e2d\u7684EM\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u8f68\u8ff9\u7684\u6846\u67b6\u3002"}}
{"id": "2511.04971", "pdf": "https://arxiv.org/pdf/2511.04971", "abs": "https://arxiv.org/abs/2511.04971", "authors": ["Esha Chowdhury"], "title": "Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques", "categories": ["cs.LG"], "comment": "24 pages with 6 table and 8 figures", "summary": "Accurate prediction of cardiovascular disease (CVD) risk is crucial for\nhealthcare institutions. This study addresses the growing prevalence of\ndiabetes and its strong link to heart disease by proposing an efficient CVD\nrisk prediction model for diabetic patients using machine learning (ML) and\nhybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by\nremoving duplicates, handling missing values, identifying categorical and\nnumerical features, and applying Principal Component Analysis (PCA) for feature\nextraction. Several ML models, including Decision Trees (DT), Random Forest\n(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and\nXGBoost, were implemented, with XGBoost achieving the highest accuracy of\n0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep\nNeural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural\nNetworks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and\nGated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,\nBiLSTM, and GRU, were also explored. Some of these models achieved perfect\nrecall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.\nOur research highlights the effectiveness of ML and DL models in predicting CVD\nrisk among diabetic patients, automating and enhancing clinical\ndecision-making. High accuracy and F1 scores demonstrate these models'\npotential to improve personalized risk management and preventive strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u5fc3\u8840\u7ba1\u75be\u75c5\u98ce\u9669\uff0c\u5176\u4e2dXGBoost\u548cLSTM\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u9ad8\u76840.9050\u51c6\u786e\u7387\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u4e0e\u5fc3\u8840\u7ba1\u75be\u75c5\u4e4b\u95f4\u5b58\u5728\u5f3a\u5173\u8054\uff0c\u4e14\u7cd6\u5c3f\u75c5\u60a3\u75c5\u7387\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684CVD\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u6765\u6539\u5584\u533b\u7597\u51b3\u7b56\u3002", "method": "\u4f7f\u7528BRFSS\u6570\u636e\u96c6\uff0c\u7ecf\u8fc7\u6570\u636e\u9884\u5904\u7406\u548cPCA\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u65bd\u4e86\u591a\u79cdML\u6a21\u578b\uff08DT\u3001RF\u3001KNN\u3001SVM\u3001AdaBoost\u3001XGBoost\uff09\u548cDL\u6a21\u578b\uff08ANN\u3001DNN\u3001RNN\u3001CNN\u3001LSTM\u3001BiLSTM\u3001GRU\uff09\u4ee5\u53caCNN\u4e0eLSTM/BiLSTM/GRU\u7684\u6df7\u5408\u6a21\u578b\u3002", "result": "XGBoost\u548cLSTM\u6a21\u578b\u5747\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u73870.9050\uff0c\u90e8\u5206\u6a21\u578b\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\uff081.00\uff09\uff0c\u9ad8\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7cd6\u5c3f\u75c5\u60a3\u8005CVD\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5e76\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\uff0c\u6709\u6f5c\u529b\u6539\u5584\u4e2a\u6027\u5316\u98ce\u9669\u7ba1\u7406\u548c\u9884\u9632\u7b56\u7565\u3002"}}
{"id": "2511.04973", "pdf": "https://arxiv.org/pdf/2511.04973", "abs": "https://arxiv.org/abs/2511.04973", "authors": ["Siyuan Li", "Yifan Sun", "Lei Cheng", "Lewen Wang", "Yang Liu", "Weiqing Liu", "Jianlong Li", "Jiang Bian", "Shikai Fang"], "title": "Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces", "categories": ["cs.LG"], "comment": null, "summary": "Generative models for multivariate time series are essential for data\naugmentation, simulation, and privacy preservation, yet current\nstate-of-the-art diffusion-based approaches are slow and limited to\nfixed-length windows. We propose FAR-TS, a simple yet effective framework that\ncombines disentangled factorization with an autoregressive Transformer over a\ndiscrete, quantized latent space to generate time series. Each time series is\ndecomposed into a data-adaptive basis that captures static cross-channel\ncorrelations and temporal coefficients that are vector-quantized into discrete\ntokens. A LLaMA-style autoregressive Transformer then models these token\nsequences, enabling fast and controllable generation of sequences with\narbitrary length. Owing to its streamlined design, FAR-TS achieves\norders-of-magnitude faster generation than Diffusion-TS while preserving\ncross-channel correlations and an interpretable latent space, enabling\nhigh-quality and flexible time series synthesis.", "AI": {"tldr": "FAR-TS\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u56e0\u5b50\u5316\u548c\u81ea\u56de\u5f52Transformer\u5728\u79bb\u6563\u91cf\u5316\u6f5c\u7a7a\u95f4\u4e0a\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u63a7\u7684\u4efb\u610f\u957f\u5ea6\u5e8f\u5217\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u4ec5\u9650\u4e8e\u56fa\u5b9a\u957f\u5ea6\u7a97\u53e3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7075\u6d3b\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u6355\u83b7\u9759\u6001\u8de8\u901a\u9053\u76f8\u5173\u6027\u7684\u6570\u636e\u81ea\u9002\u5e94\u57fa\u548c\u65f6\u5e8f\u7cfb\u6570\uff0c\u540e\u8005\u88ab\u5411\u91cf\u91cf\u5316\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u7136\u540e\u4f7f\u7528LLaMA\u98ce\u683c\u7684\u81ea\u56de\u5f52Transformer\u5efa\u6a21\u8fd9\u4e9b\u6807\u8bb0\u5e8f\u5217\u3002", "result": "FAR-TS\u6bd4Diffusion-TS\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8de8\u901a\u9053\u76f8\u5173\u6027\u548c\u53ef\u89e3\u91ca\u7684\u6f5c\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7075\u6d3b\u7684\u65f6\u95f4\u5e8f\u5217\u5408\u6210\u3002", "conclusion": "FAR-TS\u901a\u8fc7\u7b80\u5316\u7684\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u53ef\u63a7\u7684\u4efb\u610f\u957f\u5ea6\u65f6\u95f4\u5e8f\u5217\u751f\u6210\uff0c\u5728\u4fdd\u6301\u6570\u636e\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2511.04979", "pdf": "https://arxiv.org/pdf/2511.04979", "abs": "https://arxiv.org/abs/2511.04979", "authors": ["Gimun Bae", "Seung Jun Shin"], "title": "Scaling Up ROC-Optimizing Support Vector Machines", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "15 pages, Submitted to Stat", "summary": "The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the\narea under the ROC curve (AUC) and has become an attractive alternative of the\nconventional binary classification under the presence of class imbalance.\nHowever, its practical use is limited by high computational cost, as training\ninvolves evaluating all $O(n^2)$. To overcome this limitation, we develop a\nscalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby\nsubstantially reducing computational complexity. We further extend the\nframework to nonlinear classification through a low-rank kernel approximation,\nenabling efficient training in reproducing kernel Hilbert spaces. Theoretical\nanalysis establishes an error bound that justifies the proposed approximation,\nand empirical results on both synthetic and real datasets demonstrate that the\nproposed method achieves comparable AUC performance to the original ROC-SVM\nwith drastically reduced training time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684ROC-SVM\u53d8\u4f53\uff0c\u901a\u8fc7\u4e0d\u5b8c\u5168U\u7edf\u8ba1\u91cf\u548c\u4f4e\u79e9\u6838\u8fd1\u4f3c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u65b9\u6cd5\u76f8\u5f53\u7684AUC\u6027\u80fd\u3002", "motivation": "\u539f\u59cbROC-SVM\u867d\u7136\u80fd\u76f4\u63a5\u6700\u5927\u5316AUC\u4e14\u5728\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176O(n\u00b2)\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u4e0d\u5b8c\u5168U\u7edf\u8ba1\u91cf\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u6838\u8fd1\u4f3c\u6269\u5c55\u5230\u975e\u7ebf\u6027\u5206\u7c7b\uff0c\u5b9e\u73b0\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u8bef\u5dee\u754c\u8bc1\u660e\u8fd1\u4f3c\u5408\u7406\u6027\uff0c\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u80fd\u8fbe\u5230\u4e0e\u539fROC-SVM\u76f8\u5f53\u7684AUC\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u6269\u5c55ROC-SVM\u53d8\u4f53\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2511.04980", "pdf": "https://arxiv.org/pdf/2511.04980", "abs": "https://arxiv.org/abs/2511.04980", "authors": ["Rongbin Ye", "Jiaqi Chen"], "title": "Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk", "categories": ["cs.LG"], "comment": null, "summary": "The financial industry faces a significant challenge modeling and risk\nportfolios: balancing the predictability of advanced machine learning models,\nneural network models, and explainability required by regulatory entities (such\nas Office of the Comptroller of the Currency, Consumer Financial Protection\nBureau). This paper intends to fill the gap in the application between these\n\"black box\" models and explainability frameworks, such as LIME and SHAP.\nAuthors elaborate on the application of these frameworks on different models\nand demonstrates the more complex models with better prediction powers could be\napplied and reach the same level of the explainability, using SHAP and LIME.\nBeyond the comparison and discussion of performances, this paper proposes a\nnovel five dimensional framework evaluating Inherent Interpretability, Global\nExplanations, Local Explanations, Consistency, and Complexity to offer a\nnuanced method for assessing and comparing model explainability beyond simple\naccuracy metrics. This research demonstrates the feasibility of employing\nsophisticated, high performing ML models in regulated financial environments by\nutilizing modern explainability techniques and provides a structured approach\nto evaluate the crucial trade offs between model performance and\ninterpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u4e94\u7ef4\u6846\u67b6\u6765\u8bc4\u4f30\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u91d1\u878d\u98ce\u9669\u5efa\u6a21\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u901a\u8fc7SHAP\u548cLIME\u7b49\u89e3\u91ca\u6280\u672f\u53ef\u4ee5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u884c\u4e1a\u5728\u98ce\u9669\u5efa\u6a21\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff1a\u5e73\u8861\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u4e0e\u76d1\u7ba1\u673a\u6784\u8981\u6c42\u7684\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u5e94\u7528LIME\u548cSHAP\u89e3\u91ca\u6846\u67b6\u4e8e\u4e0d\u540c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u5305\u542b\u56fa\u6709\u53ef\u89e3\u91ca\u6027\u3001\u5168\u5c40\u89e3\u91ca\u3001\u5c40\u90e8\u89e3\u91ca\u3001\u4e00\u81f4\u6027\u548c\u590d\u6742\u5ea6\u7684\u4e94\u7ef4\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5177\u6709\u66f4\u597d\u9884\u6d4b\u80fd\u529b\u7684\u590d\u6742\u6a21\u578b\u901a\u8fc7SHAP\u548cLIME\u53ef\u4ee5\u8fbe\u5230\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u540c\u7684\u53ef\u89e3\u91ca\u6027\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u73b0\u4ee3\u89e3\u91ca\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u53d7\u76d1\u7ba1\u7684\u91d1\u878d\u73af\u5883\u4e2d\u5e94\u7528\u590d\u6742\u7684\u9ad8\u6027\u80fdML\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2511.04981", "pdf": "https://arxiv.org/pdf/2511.04981", "abs": "https://arxiv.org/abs/2511.04981", "authors": ["Zhiqi Bu"], "title": "Deep Progressive Training: scaling up depth capacity of zero/one-layer models", "categories": ["cs.LG"], "comment": null, "summary": "Model depth is a double-edged sword in deep learning: deeper models achieve\nhigher accuracy but require higher computational cost. To efficiently train\nmodels at scale, an effective strategy is the progressive training, which\nscales up model capacity during training, hence significantly reducing\ncomputation with little to none performance degradation. In this work, we study\nthe depth expansion of large models through the lens of optimization theory and\nfeature learning, offering insights on the initialization of new layers,\nhyperparameter transfer, learning rate schedule, and timing of model expansion.\nSpecifically, we propose zero/one-layer progressive training for the optimal\ntradeoff between computation and loss. For example, zero/one-layer progressive\ntraining on GPT2 can save $\\approx 80\\%$ compute, or equivalently accelerate\n$\\approx 5\\times$ while achieving almost the same loss, compared to to a fully\ntrained 60-layer model with 7B parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u96f6/\u4e00\u5c42\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728GPT2\u4e0a\u53ef\u8282\u7701\u7ea680%\u8ba1\u7b97\u91cf\u6216\u52a0\u901f5\u500d\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e60\u5c42\u6a21\u578b\u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd", "motivation": "\u6a21\u578b\u6df1\u5ea6\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u662f\u4e00\u628a\u53cc\u5203\u5251\uff1a\u66f4\u6df1\u7684\u6a21\u578b\u80fd\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u4f46\u9700\u8981\u66f4\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u6e10\u8fdb\u8bad\u7ec3\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\u6765\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "method": "\u4ece\u4f18\u5316\u7406\u8bba\u548c\u7279\u5f81\u5b66\u4e60\u7684\u89d2\u5ea6\u7814\u7a76\u6df1\u5ea6\u6269\u5c55\uff0c\u63d0\u51fa\u96f6/\u4e00\u5c42\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5173\u6ce8\u65b0\u5c42\u521d\u59cb\u5316\u3001\u8d85\u53c2\u6570\u8fc1\u79fb\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u6a21\u578b\u6269\u5c55\u65f6\u673a", "result": "\u5728GPT2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u8282\u7701\u7ea680%\u8ba1\u7b97\u91cf\u6216\u5b9e\u73b05\u500d\u52a0\u901f\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e60\u5c427B\u53c2\u6570\u6a21\u578b\u51e0\u4e4e\u76f8\u540c\u7684\u635f\u5931", "conclusion": "\u6e10\u8fdb\u8bad\u7ec3\u662f\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u5e73\u8861\u7684\u6709\u6548\u7b56\u7565\uff0c\u96f6/\u4e00\u5c42\u6e10\u8fdb\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6700\u4f18\u7684\u8ba1\u7b97\u4e0e\u635f\u5931\u6743\u8861"}}
{"id": "2511.04984", "pdf": "https://arxiv.org/pdf/2511.04984", "abs": "https://arxiv.org/abs/2511.04984", "authors": ["Xinheng He", "Yijia Zhang", "Haowei Lin", "Xingang Peng", "Xiangzhe Kong", "Mingyu Li", "Jianzhu Ma"], "title": "Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding", "categories": ["cs.LG"], "comment": "Abstract 1 page, main text 9 pages, references 2 pages, 4 figures.\n  Submitted to RECOMB 2026", "summary": "Structure-based drug design has seen significant advancements with the\nintegration of artificial intelligence (AI), particularly in the generation of\nhit and lead compounds. However, most AI-driven approaches neglect the\nimportance of endogenous protein interactions with peptides, which may result\nin suboptimal molecule designs. In this work, we present Peptide2Mol, an\nE(3)-equivariant graph neural network diffusion model that generates small\nmolecules by referencing both the original peptide binders and their\nsurrounding protein pocket environments. Trained on large datasets and\nleveraging sophisticated modeling techniques, Peptide2Mol not only achieves\nstate-of-the-art performance in non-autoregressive generative tasks, but also\nproduces molecules with similarity to the original peptide binder.\nAdditionally, the model allows for molecule optimization and peptidomimetic\ndesign through a partial diffusion process. Our results highlight Peptide2Mol\nas an effective deep generative model for generating and optimizing bioactive\nsmall molecules from protein binding pockets.", "AI": {"tldr": "Peptide2Mol\u662f\u4e00\u4e2aE(3)\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u8003\u539f\u59cb\u80bd\u7ed3\u5408\u5242\u53ca\u5176\u5468\u56f4\u86cb\u767d\u53e3\u888b\u73af\u5883\u6765\u751f\u6210\u5c0f\u5206\u5b50\uff0c\u5728\u975e\u81ea\u56de\u5f52\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570AI\u9a71\u52a8\u7684\u836f\u7269\u8bbe\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u5185\u6e90\u6027\u86cb\u767d\u4e0e\u80bd\u76f8\u4e92\u4f5c\u7528\u7684\u91cd\u8981\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u5206\u5b50\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528E(3)-\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u539f\u59cb\u80bd\u7ed3\u5408\u5242\u548c\u86cb\u767d\u53e3\u888b\u73af\u5883\u751f\u6210\u5c0f\u5206\u5b50\uff0c\u652f\u6301\u90e8\u5206\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u5206\u5b50\u4f18\u5316\u548c\u80bd\u6a21\u62df\u8bbe\u8ba1\u3002", "result": "\u6a21\u578b\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u975e\u81ea\u56de\u5f52\u751f\u6210\u6027\u80fd\uff0c\u8fd8\u751f\u6210\u4e86\u4e0e\u539f\u59cb\u80bd\u7ed3\u5408\u5242\u76f8\u4f3c\u7684\u5206\u5b50\u3002", "conclusion": "Peptide2Mol\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u53ef\u7528\u4e8e\u4ece\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u751f\u6210\u548c\u4f18\u5316\u751f\u7269\u6d3b\u6027\u5c0f\u5206\u5b50\u3002"}}
{"id": "2511.04988", "pdf": "https://arxiv.org/pdf/2511.04988", "abs": "https://arxiv.org/abs/2511.04988", "authors": ["Runsheng Ren", "Jing Li", "Yanxiu Li", "Shixun Huang", "Jun Shen", "Wanqing Li", "John Le", "Sheng Wang"], "title": "Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models", "categories": ["cs.LG"], "comment": null, "summary": "Accurately forecasting carbon prices is essential for informed energy market\ndecision-making, guiding sustainable energy planning, and supporting effective\ndecarbonization strategies. However, it remains challenging due to structural\nbreaks and high-frequency noise caused by frequent policy interventions and\nmarket shocks. Existing studies, including the most recent baseline approaches,\nhave attempted to incorporate breakpoints but often treat denoising and\nmodeling as separate processes and lack systematic evaluation across advanced\ndeep learning architectures, limiting the robustness and the generalization\ncapability. To address these gaps, this paper proposes a comprehensive hybrid\nframework that integrates structural break detection (Bai-Perron, ICSS, and\nPELT algorithms), wavelet signal denoising, and three state-of-the-art deep\nlearning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot\nprices from 2007 to 2024 and exogenous features such as energy prices and\npolicy indicators, the framework constructs univariate and multivariate\ndatasets for comparative evaluation. Experimental results demonstrate that our\nproposed PELT-WT-TCN achieves the highest prediction accuracy, reducing\nforecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the\nstate-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by\n70.55% in RMSE and 74.42% in MAE compared to the original LSTM without\ndecomposition from the same baseline study. These findings underscore the value\nof integrating structural awareness and multiscale decomposition into deep\nlearning architectures to enhance accuracy and interpretability in carbon price\nforecasting and other nonstationary financial time series.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u65ad\u70b9\u68c0\u6d4b\u3001\u5c0f\u6ce2\u4fe1\u53f7\u53bb\u566a\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u78b3\u4ef7\u683c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u78b3\u4ef7\u683c\u9884\u6d4b\u5bf9\u80fd\u6e90\u5e02\u573a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7ed3\u6784\u65ad\u70b9\u548c\u9ad8\u9891\u566a\u58f0\u7684\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u96c6\u6210Bai-Perron\u3001ICSS\u548cPELT\u7b97\u6cd5\u8fdb\u884c\u7ed3\u6784\u65ad\u70b9\u68c0\u6d4b\uff0c\u4f7f\u7528\u5c0f\u6ce2\u4fe1\u53f7\u53bb\u566a\uff0c\u7ed3\u5408LSTM\u3001GRU\u548cTCN\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6784\u5efa\u6df7\u5408\u6846\u67b6\u3002", "result": "PELT-WT-TCN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0cRMSE\u964d\u4f4e22.35%\uff0cMAE\u964d\u4f4e18.63%\uff1b\u76f8\u6bd4\u539f\u59cbLSTM\uff0cRMSE\u964d\u4f4e70.55%\uff0cMAE\u964d\u4f4e74.42%\u3002", "conclusion": "\u5c06\u7ed3\u6784\u610f\u8bc6\u548c\u591a\u5c3a\u5ea6\u5206\u89e3\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\uff0c\u53ef\u663e\u8457\u63d0\u5347\u78b3\u4ef7\u683c\u9884\u6d4b\u53ca\u5176\u4ed6\u975e\u5e73\u7a33\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.04998", "pdf": "https://arxiv.org/pdf/2511.04998", "abs": "https://arxiv.org/abs/2511.04998", "authors": ["Daniel S. Lee", "Mayra S. Haedo-Cruz", "Chen Jiang", "Oshin Miranda", "LiRong Wang"], "title": "BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "20 pages, 2 figures, 6 tables, 2 supplementary figures, 4\n  supplementary tables, submitted to Journal of Biomedical Informatics on 6\n  Nov, 2025", "summary": "Transformer-based deep learning models have shown promise for disease risk\nprediction using electronic health records(EHRs), but modeling temporal\ndependencies remains a key challenge due to irregular visit intervals and lack\nof uniform structure. We propose a Bi-Positional Embedding Transformer Encoder\nor BiPETE for single-disease prediction, which integrates rotary positional\nembeddings to encode relative visit timing and sinusoidal embeddings to\npreserve visit order. Without relying on large-scale pretraining, BiPETE is\ntrained on EHR data from two mental health cohorts-depressive disorder and\npost-traumatic stress disorder (PTSD)-to predict the risk of alcohol and\nsubstance use disorders (ASUD). BiPETE outperforms baseline models, improving\nthe area under the precision-recall curve (AUPRC) by 34% and 50% in the\ndepression and PTSD cohorts, respectively. An ablation study further confirms\nthe effectiveness of the dual positional encoding strategy. We apply the\nIntegrated Gradients method to interpret model predictions, identifying key\nclinical features associated with ASUD risk and protection, such as abnormal\ninflammatory, hematologic, and metabolic markers, as well as specific\nmedications and comorbidities. Overall, these key clinical features identified\nby the attribution methods contribute to a deeper understanding of the risk\nassessment process and offer valuable clues for mitigating potential risks. In\nsummary, our study presents a practical and interpretable framework for disease\nrisk prediction using EHR data, which can achieve strong performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86BiPETE\u6a21\u578b\uff0c\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u548c\u6b63\u5f26\u5d4c\u5165\u6765\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u6291\u90c1\u548cPTSD\u961f\u5217\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9152\u7cbe\u548c\u7269\u8d28\u4f7f\u7528\u969c\u788d\u98ce\u9669\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5c31\u8bca\u95f4\u9694\u4e0d\u89c4\u5219\u548c\u7f3a\u4e4f\u7edf\u4e00\u7ed3\u6784\uff0c\u5efa\u6a21\u65f6\u5e8f\u4f9d\u8d56\u4ecd\u5177\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86Bi-Positional Embedding Transformer Encoder (BiPETE)\uff0c\u6574\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u7f16\u7801\u76f8\u5bf9\u5c31\u8bca\u65f6\u95f4\uff0c\u6b63\u5f26\u5d4c\u5165\u4fdd\u7559\u5c31\u8bca\u987a\u5e8f\uff0c\u5728\u4e24\u4e2a\u5fc3\u7406\u5065\u5eb7\u961f\u5217\u4e0a\u8bad\u7ec3\u9884\u6d4bASUD\u98ce\u9669\u3002", "result": "BiPETE\u5728\u6291\u90c1\u548cPTSD\u961f\u5217\u4e2d\u5206\u522b\u5c06AUPRC\u63d0\u5347\u4e8634%\u548c50%\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u53cc\u91cd\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u75be\u75c5\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5f52\u56e0\u65b9\u6cd5\u8bc6\u522b\u4e86\u4e0eASUD\u98ce\u9669\u76f8\u5173\u7684\u5173\u952e\u4e34\u5e8a\u7279\u5f81\u3002"}}
{"id": "2511.05005", "pdf": "https://arxiv.org/pdf/2511.05005", "abs": "https://arxiv.org/abs/2511.05005", "authors": ["Dongsu Lee", "Daehee Lee", "Amy Zhang"], "title": "Multi-agent Coordination via Flow Matching", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "This work presents MAC-Flow, a simple yet expressive framework for\nmulti-agent coordination. We argue that requirements of effective coordination\nare twofold: (i) a rich representation of the diverse joint behaviors present\nin offline data and (ii) the ability to act efficiently in real time. However,\nprior approaches often sacrifice one for the other, i.e., denoising\ndiffusion-based solutions capture complex coordination but are computationally\nslow, while Gaussian policy-based solutions are fast but brittle in handling\nmulti-agent interaction. MAC-Flow addresses this trade-off by first learning a\nflow-based representation of joint behaviors, and then distilling it into\ndecentralized one-step policies that preserve coordination while enabling fast\nexecution. Across four different benchmarks, including $12$ environments and\n$34$ datasets, MAC-Flow alleviates the trade-off between performance and\ncomputational cost, specifically achieving about $\\boldsymbol{\\times14.5}$\nfaster inference compared to diffusion-based MARL methods, while maintaining\ngood performance. At the same time, its inference speed is similar to that of\nprior Gaussian policy-based offline multi-agent reinforcement learning (MARL)\nmethods.", "AI": {"tldr": "MAC-Flow\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u8868\u793a\u5b66\u4e60\u8054\u5408\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u84b8\u998f\u4e3a\u5206\u6563\u5f0f\u5355\u6b65\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u534f\u8c03\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u534f\u8c03\u7684\u4e24\u4e2a\u8981\u6c42\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u6269\u6563\u65b9\u6cd5\u80fd\u6355\u6349\u590d\u6742\u534f\u8c03\u4f46\u8ba1\u7b97\u6162\uff0c\u9ad8\u65af\u7b56\u7565\u65b9\u6cd5\u63a8\u7406\u5feb\u4f46\u5904\u7406\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8106\u5f31\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\u3002", "method": "\u9996\u5148\u5b66\u4e60\u57fa\u4e8e\u6d41\u7684\u8054\u5408\u884c\u4e3a\u8868\u793a\uff0c\u7136\u540e\u5c06\u5176\u84b8\u998f\u4e3a\u5206\u6563\u5f0f\u5355\u6b65\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5728\u4fdd\u6301\u534f\u8c03\u7684\u540c\u65f6\u652f\u6301\u5feb\u901f\u6267\u884c\u3002", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0812\u4e2a\u73af\u5883\u548c34\u4e2a\u6570\u636e\u96c6\uff09\u4e0a\uff0cMAC-Flow\u7f13\u89e3\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u4e8e\u6269\u6563\u7684MARL\u65b9\u6cd5\u5feb\u7ea614.5\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u4e0e\u5148\u524d\u57fa\u4e8e\u9ad8\u65af\u7b56\u7565\u7684\u79bb\u7ebfMARL\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "MAC-Flow\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u8868\u8fbe\u80fd\u529b\u5f3a\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.05028", "pdf": "https://arxiv.org/pdf/2511.05028", "abs": "https://arxiv.org/abs/2511.05028", "authors": ["Dongjin Park", "Hasung Yeo", "Joon-Woo Lee"], "title": "OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated fine-tuning (FFT) adapts foundation models to decentralized data\nbut remains fragile under heterogeneous client distributions due to local\ndrift, i.e., client-level update divergences that induce systematic bias and\namplified variance in the global model. Existing aggregation and\npersonalization methods largely correct drift post hoc, which proves brittle\nunder extreme non-IID conditions. We introduce OvA-LP, a minimalist framework\nthat is, to our knowledge, the first explicitly designed to suppress drift at\nits source within the PEFT-based FFT paradigm. OvA-LP combines linear probing\non a frozen encoder with a one-vs-all head and a simple two-stage procedure,\npreserving pretrained feature geometry and decoupling logits to prevent the\nmechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over\nshard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of\nits IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%\n(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains\nresilience under both symmetric and asymmetric label noise. In addition,\nprecomputing encoder features makes per-round cost nearly independent of\nencoder size. Together, these results demonstrate that OvA-LP provides a\nprincipled and efficient basis for robust FFT under heterogeneity.", "AI": {"tldr": "OvA-LP\u662f\u4e00\u4e2a\u5728\u8054\u90a6\u5fae\u8c03\u4e2d\u4ece\u6e90\u5934\u6291\u5236\u5c40\u90e8\u6f02\u79fb\u7684\u7b80\u7ea6\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u548c\u4e00\u5bf9\u591a\u5934\u8bbe\u8ba1\uff0c\u5728\u975eIID\u6570\u636e\u4e0b\u4fdd\u630195.9%\u7684IID\u51c6\u786e\u7387\uff0c\u8fdc\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u5728\u5f02\u6784\u5ba2\u6237\u7aef\u5206\u5e03\u4e0b\u56e0\u5c40\u90e8\u6f02\u79fb\u5bfc\u81f4\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u65b9\u5dee\u653e\u5927\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u975eIID\u6761\u4ef6\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u7ed3\u5408\u51bb\u7ed3\u7f16\u7801\u5668\u4e0a\u7684\u7ebf\u6027\u63a2\u6d4b\u4e0e\u4e00\u5bf9\u591a\u5934\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7a0b\u5e8f\uff0c\u4fdd\u6301\u9884\u8bad\u7ec3\u7279\u5f81\u51e0\u4f55\u5e76\u89e3\u8026\u903b\u8f91\u4ee5\u9632\u6b62\u6f02\u79fb\u653e\u5927\u673a\u5236\u3002", "result": "\u5728CIFAR-100\u7684100\u4e2a\u5ba2\u6237\u7aef\u4e0a\uff0cOvA-LP\u5728\u975eIID\u6761\u4ef6\u4e0b\u4fdd\u630195.9%\u7684IID\u51c6\u786e\u7387\uff0c\u800cPFPT\u548cFFT-MoE\u5206\u522b\u53ea\u670910.1%\u548c34.5%\u3002\u5728\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u6807\u7b7e\u566a\u58f0\u4e0b\u4e5f\u4fdd\u6301\u97e7\u6027\u3002", "conclusion": "OvA-LP\u4e3a\u5f02\u6784\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u8054\u90a6\u5fae\u8c03\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u9ad8\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2511.05114", "pdf": "https://arxiv.org/pdf/2511.05114", "abs": "https://arxiv.org/abs/2511.05114", "authors": ["\u00c1lvaro Guglielmin Becker", "Lana Bertoldo Rossato", "Anderson Rocha Tavares"], "title": "Usando LLMs para Programar Jogos de Tabuleiro e Varia\u00e7\u00f5es", "categories": ["cs.LG"], "comment": "Accepted for presentation at the I Escola Regional de Aprendizado de\n  M\\'aquina e Intelig\\^encia Artificial da Regi\\~ao Sul, 2025, in Portuguese\n  language", "summary": "Creating programs to represent board games can be a time-consuming task.\nLarge Language Models (LLMs) arise as appealing tools to expedite this process,\ngiven their capacity to efficiently generate code from simple contextual\ninformation. In this work, we propose a method to test how capable three LLMs\n(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as\nnew variants of existing games.", "AI": {"tldr": "\u6d4b\u8bd5\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Claude\u3001DeepSeek\u548cChatGPT\uff09\u521b\u5efa\u68cb\u76d8\u6e38\u620f\u4ee3\u7801\u53ca\u53d8\u4f53\u7684\u80fd\u529b", "motivation": "\u5229\u7528LLMs\u9ad8\u6548\u751f\u6210\u4ee3\u7801\u7684\u80fd\u529b\u6765\u52a0\u901f\u68cb\u76d8\u6e38\u620f\u7a0b\u5e8f\u7684\u5f00\u53d1\u8fc7\u7a0b", "method": "\u63d0\u51fa\u65b9\u6cd5\u6d4b\u8bd5\u4e09\u79cdLLMs\u521b\u5efa\u68cb\u76d8\u6e38\u620f\u4ee3\u7801\u548c\u73b0\u6709\u6e38\u620f\u53d8\u4f53\u7684\u80fd\u529b", "result": "", "conclusion": ""}}
{"id": "2511.05124", "pdf": "https://arxiv.org/pdf/2511.05124", "abs": "https://arxiv.org/abs/2511.05124", "authors": ["Felix Divo", "Maurice Kraus", "Anh Q. Nguyen", "Hao Xue", "Imran Razzak", "Flora D. Salim", "Kristian Kersting", "Devendra Singh Dhami"], "title": "QuAnTS: Question Answering on Time Series", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "Text offers intuitive access to information. This can, in particular,\ncomplement the density of numerical time series, thereby allowing improved\ninteractions with time series models to enhance accessibility and\ndecision-making. While the creation of question-answering datasets and models\nhas recently seen remarkable growth, most research focuses on question\nanswering (QA) on vision and text, with time series receiving minute attention.\nTo bridge this gap, we propose a challenging novel time series QA (TSQA)\ndataset, QuAnTS, for Question Answering on Time Series data. Specifically, we\npose a wide variety of questions and answers about human motion in the form of\ntracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is\nwell-formed and comprehensive through extensive experiments. Thoroughly\nevaluating existing and newly proposed baselines then lays the groundwork for a\ndeeper exploration of TSQA using QuAnTS. Additionally, we provide human\nperformances as a key reference for gauging the practical usability of such\nmodels. We hope to encourage future research on interacting with time series\nmodels through text, enabling better decision-making and more transparent\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6570\u636e\u96c6QuAnTS\uff0c\u4e13\u6ce8\u4e8e\u4eba\u4f53\u8fd0\u52a8\u7684\u9aa8\u67b6\u8f68\u8ff9\u6570\u636e\uff0c\u586b\u8865\u4e86\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u6587\u672c\u4fe1\u606f\u53ef\u4ee5\u8865\u5145\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u7684\u5bc6\u5ea6\uff0c\u6539\u5584\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u4ea4\u4e92\uff0c\u4f46\u5f53\u524d\u95ee\u7b54\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u548c\u6587\u672c\uff0c\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7f3a\u4e4f\u5173\u6ce8\u3002", "method": "\u521b\u5efa\u4e86QuAnTS\u6570\u636e\u96c6\uff0c\u5305\u542b\u5173\u4e8e\u4eba\u4f53\u8fd0\u52a8\u9aa8\u67b6\u8f68\u8ff9\u7684\u5404\u79cd\u95ee\u7b54\u5bf9\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u3002", "result": "\u8bc4\u4f30\u4e86\u73b0\u6709\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u7684\u6df1\u5165\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u4eba\u7c7b\u8868\u73b0\u4f5c\u4e3a\u5b9e\u7528\u6027\u7684\u53c2\u8003\u6807\u51c6\u3002", "conclusion": "\u5e0c\u671b\u4fc3\u8fdb\u901a\u8fc7\u6587\u672c\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u4ea4\u4e92\u7684\u7814\u7a76\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u51b3\u7b56\u652f\u6301\u548c\u66f4\u900f\u660e\u7684\u7cfb\u7edf\u3002"}}
{"id": "2511.05131", "pdf": "https://arxiv.org/pdf/2511.05131", "abs": "https://arxiv.org/abs/2511.05131", "authors": ["Fernando Berzal"], "title": "DL101 Neural Network Outputs and Loss Functions", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "The loss function used to train a neural network is strongly connected to its\noutput layer from a statistical point of view. This technical report analyzes\ncommon activation functions for a neural network output layer, like linear,\nsigmoid, ReLU, and softmax, detailing their mathematical properties and their\nappropriate use cases. A strong statistical justification exists for the\nselection of the suitable loss function for training a deep learning model.\nThis report connects common loss functions such as Mean Squared Error (MSE),\nMean Absolute Error (MAE), and various Cross-Entropy losses to the statistical\nprinciple of Maximum Likelihood Estimation (MLE). Choosing a specific loss\nfunction is equivalent to assuming a specific probability distribution for the\nmodel output, highlighting the link between these functions and the Generalized\nLinear Models (GLMs) that underlie network output layers. Additional scenarios\nof practical interest are also considered, such as alternative output\nencodings, constrained outputs, and distributions with heavy tails.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u5206\u6790\u4e86\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5c42\u6fc0\u6d3b\u51fd\u6570\uff08\u7ebf\u6027\u3001sigmoid\u3001ReLU\u3001softmax\uff09\u4e0e\u635f\u5931\u51fd\u6570\uff08MSE\u3001MAE\u3001\u4ea4\u53c9\u71b5\uff09\u4e4b\u95f4\u7684\u7edf\u8ba1\u8054\u7cfb\uff0c\u63ed\u793a\u4e86\u635f\u5931\u51fd\u6570\u9009\u62e9\u7b49\u4ef7\u4e8e\u5047\u8bbe\u7279\u5b9a\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4e0e\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u76f8\u5173\u8054\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5c42\u6fc0\u6d3b\u51fd\u6570\u4e0e\u635f\u5931\u51fd\u6570\u4e4b\u95f4\u7684\u7edf\u8ba1\u8054\u7cfb\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u7684\u6df1\u5c42\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\u5e38\u89c1\u6fc0\u6d3b\u51fd\u6570\uff08\u7ebf\u6027\u3001sigmoid\u3001ReLU\u3001softmax\uff09\u548c\u635f\u5931\u51fd\u6570\uff08MSE\u3001MAE\u3001\u4ea4\u53c9\u71b5\uff09\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5efa\u7acb\u5b83\u4eec\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u7684\u8054\u7cfb\u3002", "result": "\u53d1\u73b0\u635f\u5931\u51fd\u6570\u9009\u62e9\u7b49\u4ef7\u4e8e\u5047\u8bbe\u7279\u5b9a\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff0c\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u5bf9\u5e94\u4e0d\u540c\u7684\u7edf\u8ba1\u5206\u5e03\u5047\u8bbe\uff0c\u4e3a\u635f\u5931\u51fd\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u7edf\u8ba1\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5c42\u6fc0\u6d3b\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u5177\u6709\u6df1\u523b\u7684\u7edf\u8ba1\u610f\u4e49\uff0c\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u7d27\u5bc6\u76f8\u5173\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7edf\u8ba1\u89e3\u91ca\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2511.05163", "pdf": "https://arxiv.org/pdf/2511.05163", "abs": "https://arxiv.org/abs/2511.05163", "authors": ["Aras Erarslan", "Carlos Sevilla Salcedo", "Ville Tanskanen", "Anni Nisov", "Eero P\u00e4iv\u00e4kumpu", "Heikki Aisala", "Kaisu Honkap\u00e4\u00e4", "Arto Klami", "Petrus Mikkola"], "title": "Consecutive Preferential Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Preferential Bayesian optimization allows optimization of objectives that are\neither expensive or difficult to measure directly, by relying on a minimal\nnumber of comparative evaluations done by a human expert. Generating candidate\nsolutions for evaluation is also often expensive, but this cost is ignored by\nexisting methods. We generalize preference-based optimization to explicitly\naccount for production and evaluation costs with Consecutive Preferential\nBayesian Optimization, reducing production cost by constraining comparisons to\ninvolve previously generated candidates. We also account for the perceptual\nambiguity of the oracle providing the feedback by incorporating a\nJust-Noticeable Difference threshold into a probabilistic preference model to\ncapture indifference to small utility differences. We adapt an\ninformation-theoretic acquisition strategy to this setting, selecting new\nconfigurations that are most informative about the unknown optimum under a\npreference model accounting for the perceptual ambiguity. We empirically\ndemonstrate a notable increase in accuracy in setups with high production costs\nor with indifference feedback.", "AI": {"tldr": "\u63d0\u51faConsecutive Preferential Bayesian Optimization\u65b9\u6cd5\uff0c\u5728\u504f\u597d\u4f18\u5316\u4e2d\u8003\u8651\u751f\u4ea7\u548c\u8bc4\u4f30\u6210\u672c\uff0c\u901a\u8fc7\u7ea6\u675f\u6bd4\u8f83\u4f7f\u7528\u5148\u524d\u751f\u6210\u7684\u5019\u9009\u6765\u964d\u4f4e\u751f\u4ea7\u6210\u672c\uff0c\u5e76\u5f15\u5165Just-Noticeable Difference\u9608\u503c\u5904\u7406\u611f\u77e5\u6a21\u7cca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5ffd\u7565\u4e86\u751f\u6210\u5019\u9009\u89e3\u7684\u751f\u4ea7\u6210\u672c\uff0c\u4e14\u672a\u8003\u8651\u8bc4\u4f30\u8005\u611f\u77e5\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u5728\u6210\u672c\u9ad8\u6602\u6216\u5b58\u5728\u611f\u77e5\u6a21\u7cca\u7684\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6269\u5c55\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u663e\u5f0f\u8003\u8651\u751f\u4ea7\u548c\u8bc4\u4f30\u6210\u672c\uff1b\u7ea6\u675f\u6bd4\u8f83\u4f7f\u7528\u5386\u53f2\u5019\u9009\u89e3\uff1b\u5f15\u5165JND\u9608\u503c\u6784\u5efa\u6982\u7387\u504f\u597d\u6a21\u578b\u5904\u7406\u611f\u77e5\u6a21\u7cca\u6027\uff1b\u91c7\u7528\u4fe1\u606f\u8bba\u83b7\u53d6\u7b56\u7565\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u65b0\u914d\u7f6e\u3002", "result": "\u5728\u9ad8\u751f\u4ea7\u6210\u672c\u6216\u5b58\u5728\u611f\u77e5\u6a21\u7cca\u53cd\u9988\u7684\u8bbe\u7f6e\u4e2d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u504f\u597d\u4f18\u5316\u7684\u751f\u4ea7\u548c\u8bc4\u4f30\u6210\u672c\uff0c\u5e76\u5728\u5b58\u5728\u611f\u77e5\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.05169", "pdf": "https://arxiv.org/pdf/2511.05169", "abs": "https://arxiv.org/abs/2511.05169", "authors": ["Simon Baur", "Tristan Ruhwedel", "Ekin B\u00f6ke", "Zuzanna Kobus", "Gergana Lishkova", "Christoph Wetz", "Holger Amthauer", "Christoph Roderburg", "Frank Tacke", "Julian M. Rogasch", "Wojciech Samek", "Henning Jann", "Jackie Ma", "Johannes Eschrich"], "title": "Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy", "categories": ["cs.LG"], "comment": null, "summary": "Peptide receptor radionuclide therapy (PRRT) is an established treatment for\nmetastatic neuroendocrine tumors (NETs), yet long-term disease control occurs\nonly in a subset of patients. Predicting progression-free survival (PFS) could\nsupport individualized treatment planning. This study evaluates laboratory,\nimaging, and multimodal deep learning models for PFS prediction in PRRT-treated\npatients. In this retrospective, single-center study 116 patients with\nmetastatic NETs undergoing 177Lu-DOTATOC were included. Clinical\ncharacteristics, laboratory values, and pretherapeutic somatostatin receptor\npositron emission tomography/computed tomographies (SR-PET/CT) were collected.\nSeven models were trained to classify low- vs. high-PFS groups, including\nunimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches.\nExplainability was evaluated by feature importance analysis and gradient maps.\nForty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1\nyear). Groups were similar in most characteristics, except for higher baseline\nchromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT\ncycles (p < 0.001) in short-PFS patients. The Random Forest model trained only\non laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal\nthree-dimensional convolutional neural networks using SR-PET or CT performed\nworse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion\nmodel laboratory values, SR-PET, and CT -augmented with a pretrained CT branch\n- achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01).\nMultimodal deep learning combining SR-PET, CT, and laboratory biomarkers\noutperformed unimodal approaches for PFS prediction after PRRT. Upon external\nvalidation, such models may support risk-adapted follow-up strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5b9e\u9a8c\u5ba4\u6307\u6807\u3001\u5f71\u50cf\u5b66\u548c\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\uff09\u7528\u4e8e\u9884\u6d4bPRRT\u6cbb\u7597\u60a3\u8005\u7684\u65e0\u8fdb\u5c55\u751f\u5b58\u671f\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "PRRT\u6cbb\u7597\u8f6c\u79fb\u6027\u795e\u7ecf\u5185\u5206\u6ccc\u80bf\u7624\u65f6\uff0c\u53ea\u6709\u90e8\u5206\u60a3\u8005\u80fd\u83b7\u5f97\u957f\u671f\u75be\u75c5\u63a7\u5236\uff0c\u9884\u6d4b\u65e0\u8fdb\u5c55\u751f\u5b58\u671f\u6709\u52a9\u4e8e\u4e2a\u4f53\u5316\u6cbb\u7597\u89c4\u5212\u3002", "method": "\u56de\u987e\u6027\u5355\u4e2d\u5fc3\u7814\u7a76\uff0c\u7eb3\u5165116\u4f8b\u63a5\u53d7177Lu-DOTATOC\u6cbb\u7597\u7684\u8f6c\u79fb\u6027NET\u60a3\u8005\uff0c\u6536\u96c6\u4e34\u5e8a\u7279\u5f81\u3001\u5b9e\u9a8c\u5ba4\u503c\u548c\u6cbb\u7597\u524dSR-PET/CT\u5f71\u50cf\uff0c\u8bad\u7ec37\u79cd\u6a21\u578b\u5206\u7c7b\u4f4e/\u9ad8PFS\u7ec4\u3002", "result": "\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\uff08\u5b9e\u9a8c\u5ba4\u503c+SR-PET+CT+\u9884\u8bad\u7ec3CT\u5206\u652f\uff09\u8868\u73b0\u6700\u4f73\uff08AUROC 0.72\u00b10.01\uff09\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408SR-PET\u3001CT\u548c\u5b9e\u9a8c\u5ba4\u751f\u7269\u6807\u5fd7\u7269\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5728PRRT\u540ePFS\u9884\u6d4b\u4e2d\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u7ecf\u5916\u90e8\u9a8c\u8bc1\u540e\u53ef\u80fd\u652f\u6301\u98ce\u9669\u9002\u5e94\u968f\u8bbf\u7b56\u7565\u3002"}}
