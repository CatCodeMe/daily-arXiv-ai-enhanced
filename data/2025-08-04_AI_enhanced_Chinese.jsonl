{"id": "2508.00217", "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "categories": ["cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8868\u683c\u5728LLMs\u548cMLLMs\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u8868\u683c\u8f93\u5165\u8868\u793a\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "motivation": "\u8868\u683c\u56e0\u5176\u590d\u6742\u548c\u7075\u6d3b\u7684\u7ed3\u6784\u5728LLMs\u548cMLLMs\u4e2d\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u65b9\u6cd5\uff0c\u7814\u7a76\u9886\u57df\u5b58\u5728\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6cd5\u603b\u7ed3\u8868\u683c\u8f93\u5165\u8868\u793a\uff0c\u5e76\u4ecb\u7ecd\u8868\u683c\u7406\u89e3\u4efb\u52a1\u3002", "result": "\u6307\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u7814\u7a76\u7f3a\u53e3\uff1a\u4efb\u52a1\u504f\u91cd\u68c0\u7d22\u3001\u6a21\u578b\u5904\u7406\u590d\u6742\u8868\u683c\u80fd\u529b\u4e0d\u8db3\u3001\u6a21\u578b\u6cdb\u5316\u6027\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u8868\u683c\u7406\u89e3\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.00776", "pdf": "https://arxiv.org/pdf/2508.00776", "abs": "https://arxiv.org/abs/2508.00776", "authors": ["Dieter van Melkebeek"], "title": "From Dynamic Programs to Greedy Algorithms", "categories": ["cs.DS"], "comment": "14 pages, 2 figures", "summary": "We show for several computational problems how classical greedy algorithms\nfor special cases can be derived in a simple way from dynamic programs for the\ngeneral case: interval scheduling (restricted to unit weights), knapsack\n(restricted to unit values), and shortest paths (restricted to nonnegative edge\nlengths). Conceptually, we repeatedly expand the Bellman equations underlying\nthe dynamic program and use straightforward monotonicity properties to figure\nout which terms yield the optimal value under the respective restrictions. The\napproach offers an alternative for developing these greedy algorithms in\nundergraduate algorithms courses and/or for arguing their correctness. In the\nsetting of interval scheduling, it elucidates the change in order from earliest\nstart time first for the memoized dynamic program to earliest finish time first\nfor the greedy algorithm.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u52a8\u6001\u89c4\u5212\u7684\u4e00\u822c\u60c5\u51b5\u7b80\u5355\u63a8\u5bfc\u51fa\u7ecf\u5178\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u533a\u95f4\u8c03\u5ea6\u3001\u80cc\u5305\u95ee\u9898\u548c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u3002", "motivation": "\u4e3a\u672c\u79d1\u751f\u7b97\u6cd5\u8bfe\u7a0b\u63d0\u4f9b\u4e00\u79cd\u5f00\u53d1\u8d2a\u5fc3\u7b97\u6cd5\u548c\u8bba\u8bc1\u5176\u6b63\u786e\u6027\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u52a8\u6001\u89c4\u5212\u7684Bellman\u65b9\u7a0b\uff0c\u5229\u7528\u5355\u8c03\u6027\u6027\u8d28\u786e\u5b9a\u6700\u4f18\u89e3\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51fa\u533a\u95f4\u8c03\u5ea6\u3001\u80cc\u5305\u95ee\u9898\u548c\u6700\u77ed\u8def\u5f84\u95ee\u9898\u7684\u8d2a\u5fc3\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u8d2a\u5fc3\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c24\u5176\u5728\u533a\u95f4\u8c03\u5ea6\u4e2d\u89e3\u91ca\u4e86\u4ece\u6700\u65e9\u5f00\u59cb\u65f6\u95f4\u5230\u6700\u65e9\u7ed3\u675f\u65f6\u95f4\u7684\u987a\u5e8f\u53d8\u5316\u3002"}}
{"id": "2508.00031", "pdf": "https://arxiv.org/pdf/2508.00031", "abs": "https://arxiv.org/abs/2508.00031", "authors": ["Junde Wu"], "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "categories": ["cs.SE"], "comment": "in updating", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGit-Context-Controller (GCC)\uff0c\u4e00\u79cd\u57fa\u4e8eGit\u7248\u672c\u63a7\u5236\u601d\u60f3\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\uff08\u5982\u5927\u578b\u8f6f\u4ef6\u5f00\u53d1\uff09\u4e2d\u7684\u90e8\u7f72\uff0c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002", "method": "GCC\u5c06\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u8bbe\u8ba1\u4e3a\u7c7b\u4f3cGit\u7684\u7248\u672c\u5316\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u652f\u6301COMMIT\u3001BRANCH\u3001MERGE\u548cCONTEXT\u7b49\u64cd\u4f5c\uff0c\u5b9e\u73b0\u91cc\u7a0b\u7891\u68c0\u67e5\u70b9\u3001\u66ff\u4ee3\u8ba1\u5212\u63a2\u7d22\u548c\u7ed3\u6784\u5316\u53cd\u601d\u3002", "result": "\u5728SWE-Bench-Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGCC\u4ee3\u7406\u89e3\u51b3\u4e8648.00%\u7684\u8f6f\u4ef6\u9519\u8bef\uff0c\u4f18\u4e8e26\u4e2a\u7ade\u4e89\u7cfb\u7edf\uff1b\u5728\u81ea\u590d\u5236\u6848\u4f8b\u4e2d\uff0cGCC\u4ee3\u7406\u7684\u4efb\u52a1\u89e3\u51b3\u7387\u8fbe\u523040.7%\uff0c\u800c\u672a\u4f7f\u7528GCC\u7684\u4ec5\u4e3a11.7%\u3002", "conclusion": "GCC\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00007", "pdf": "https://arxiv.org/pdf/2508.00007", "abs": "https://arxiv.org/abs/2508.00007", "authors": ["Gaowei Chang", "Eidan Lin", "Chengxuan Yuan", "Rizhao Cai", "Binbin Chen", "Xuan Xie", "Yin Zhang"], "title": "Agent Network Protocol Technical White Paper", "categories": ["cs.NI", "cs.AI"], "comment": "This white paper is a reformatted version of the open-source\n  community edition previously released by the ANP Open Source Technology\n  Community(https://github.com/agent-network-protocol)", "summary": "With the development of large models and autonomous decision-making AI,\nagents are rapidly becoming the new entities of the internet, following mobile\napps. However, existing internet infrastructure is primarily designed for human\ninteraction, creating data silos, unfriendly interfaces, and high collaboration\ncosts among agents, making it difficult to support the needs for large-scale\nagent interconnection and collaboration. The internet is undergoing a profound\ntransformation, showing four core trends: agents replacing traditional\nsoftware, universal agent interconnection, native protocol-based connections,\nand autonomous agent organization and collaboration. To align with these\ntrends, Agent Network Protocol (ANP) proposes a new generation of communication\nprotocols for the Agentic Web. ANP adheres to AI-native design, maintains\ncompatibility with existing internet protocols, adopts a modular composable\narchitecture, follows minimalist yet extensible principles, and enables rapid\ndeployment based on existing infrastructure. Through a three-layer protocol\nsystem--identity and encrypted communication layer, meta-protocol negotiation\nlayer, and application protocol layer--ANP. systematically solves the problems\nof agent identity authentication, dynamic negotiation, and capability discovery\ninteroperability.", "AI": {"tldr": "ANP\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411Agentic Web\u7684\u65b0\u4e00\u4ee3\u901a\u4fe1\u534f\u8bae\uff0c\u89e3\u51b3\u73b0\u6709\u4e92\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\u5bf9\u5927\u89c4\u6a21Agent\u4e92\u8054\u4e0e\u534f\u4f5c\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4e92\u8054\u7f51\u57fa\u7840\u8bbe\u65bd\u4e3b\u8981\u4e3a\u4eba\u7c7b\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u5bfc\u81f4Agent\u95f4\u6570\u636e\u5b64\u5c9b\u3001\u63a5\u53e3\u4e0d\u53cb\u597d\u548c\u9ad8\u534f\u4f5c\u6210\u672c\uff0c\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21Agent\u4e92\u8054\u4e0e\u534f\u4f5c\u7684\u9700\u6c42\u3002", "method": "ANP\u91c7\u7528AI\u539f\u751f\u8bbe\u8ba1\uff0c\u517c\u5bb9\u73b0\u6709\u4e92\u8054\u7f51\u534f\u8bae\uff0c\u6a21\u5757\u5316\u53ef\u7ec4\u5408\u67b6\u6784\uff0c\u9075\u5faa\u6781\u7b80\u4f46\u53ef\u6269\u5c55\u539f\u5219\uff0c\u901a\u8fc7\u4e09\u5c42\u534f\u8bae\u7cfb\u7edf\uff08\u8eab\u4efd\u4e0e\u52a0\u5bc6\u901a\u4fe1\u5c42\u3001\u5143\u534f\u8bae\u534f\u5546\u5c42\u3001\u5e94\u7528\u534f\u8bae\u5c42\uff09\u89e3\u51b3\u95ee\u9898\u3002", "result": "ANP\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86Agent\u8eab\u4efd\u8ba4\u8bc1\u3001\u52a8\u6001\u534f\u5546\u548c\u80fd\u529b\u53d1\u73b0\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "conclusion": "ANP\u4e3aAgentic Web\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u517c\u5bb9\u4e14\u53ef\u6269\u5c55\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u652f\u6301\u672a\u6765Agent\u5927\u89c4\u6a21\u4e92\u8054\u4e0e\u534f\u4f5c\u3002"}}
{"id": "2508.00341", "pdf": "https://arxiv.org/pdf/2508.00341", "abs": "https://arxiv.org/abs/2508.00341", "authors": ["Shengheng Liu", "Ningning Fu", "Zhonghao Zhang", "Yongming Huang", "Tony Q. S. Quek"], "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT", "categories": ["cs.DC"], "comment": "To appear in ACM TOIT. 24 pages, 8 figures", "summary": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a7a\u4e2d\u8ba1\u7b97\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u8c03\u5ea6\u548c\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u901a\u4fe1\u6548\u7387\u548c\u63a8\u7406\u7cbe\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u7269\u8054\u7f51(IoT)\u7684\u666e\u53ca\u5e26\u6765\u4e86\u9690\u79c1\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60(FL)\u4f5c\u4e3a\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9700\u8981\u89e3\u51b3\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7528\u6237\u53c2\u4e0e\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7a7a\u4e2d\u8ba1\u7b97\u5230FL\u6846\u67b6\uff0c\u63d0\u51fa\u7528\u6237\u8c03\u5ea6\u548c\u6ce2\u675f\u6210\u5f62\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u51f8\u6280\u672f\u5206\u89e3\u975e\u51f8\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4f4e\u590d\u6742\u5ea6\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u805a\u5408\u8bef\u5dee\u548c\u5b66\u4e60\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86FL\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.00037", "pdf": "https://arxiv.org/pdf/2508.00037", "abs": "https://arxiv.org/abs/2508.00037", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IEEE Transactions on Industrial Informatics", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u53ef\u6269\u5c55\u65f6\u7a7aTransformer\uff08ScaleSTF\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u7684\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u7f51\u7edc\u7cfb\u7edf\u6d89\u53ca\u590d\u6742\u4e14\u89c4\u5219\u672a\u77e5\u7684\u8fc7\u7a0b\uff0c\u73b0\u6709\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u9884\u6d4b\u6548\u679c\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7ed3\u6784\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u6269\u6563\u65b9\u6848\uff0c\u5176\u6ce8\u610f\u529b\u5c42\u7531\u4f4e\u7ef4\u5d4c\u5165\u8bf1\u5bfc\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "\u5728\u4ea4\u901a\u6d41\u91cf\u3001\u592a\u9633\u80fd\u53d1\u7535\u548c\u667a\u80fd\u7535\u8868\u7b49\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86ScaleSTF\u7684\u5148\u8fdb\u6027\u80fd\u548c\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ScaleSTF\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u52a8\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u540c\u65f6\u517c\u987e\u4e86\u6548\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2508.00055", "pdf": "https://arxiv.org/pdf/2508.00055", "abs": "https://arxiv.org/abs/2508.00055", "authors": ["Ewin Tang", "John Wright"], "title": "Are controlled unitaries helpful?", "categories": ["quant-ph", "cs.CC", "cs.DS"], "comment": "18 pages", "summary": "Many quantum algorithms, to compute some property of a unitary $U$, require\naccess not just to $U$, but to $cU$, the unitary with a control qubit. We show\nthat having access to $cU$ does not help for a large class of quantum problems.\nFor a quantum circuit which uses $cU$ and $cU^\\dagger$ and outputs\n$|\\psi(U)\\rangle$, we show how to ``decontrol'' the circuit into one which uses\nonly $U$ and $U^\\dagger$ and outputs $|\\psi(\\varphi U)\\rangle$ for a uniformly\nrandom phase $\\varphi$, with a small amount of time and space overhead. When we\nonly care about the output state up to a global phase on $U$, then the\ndecontrolled circuit suffices. Stated differently, $cU$ is only helpful because\nit contains global phase information about $U$.\n  A version of our procedure is described in an appendix of Sheridan, Maslov,\nand Mosca [SMM09]. Our goal with this work is to popularize this result by\ngeneralizing it and investigating its implications, in order to counter\nnegative results in the literature which might lead one to believe that\ndecontrolling is not possible. As an application, we give a simple proof for\nthe existence of unitary ensembles which are pseudorandom under access to $U$,\n$U^\\dagger$, $cU$, and $cU^\\dagger$.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\uff0c\u63a7\u5236\u95e8\uff08cU\uff09\u5bf9\u8bb8\u591a\u95ee\u9898\u5e76\u65e0\u5e2e\u52a9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u53bb\u63a7\u5236\u201d\u65b9\u6cd5\uff0c\u5c06\u4f7f\u7528cU\u7684\u7535\u8def\u8f6c\u6362\u4e3a\u4ec5\u4f7f\u7528U\u7684\u7535\u8def\u3002", "motivation": "\u9488\u5bf9\u91cf\u5b50\u7b97\u6cd5\u4e2d\u63a7\u5236\u95e8\uff08cU\uff09\u7684\u4f5c\u7528\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u53cd\u9a73\u4e86\u6587\u732e\u4e2d\u8ba4\u4e3a\u53bb\u63a7\u5236\u4e0d\u53ef\u884c\u7684\u8d1f\u9762\u7ed3\u8bba\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u53bb\u63a7\u5236\u201d\u6280\u672f\uff0c\u5c06\u4f7f\u7528cU\u548ccU\u2020\u7684\u7535\u8def\u8f6c\u6362\u4e3a\u4ec5\u4f7f\u7528U\u548cU\u2020\u7684\u7535\u8def\uff0c\u8f93\u51fa\u5e26\u6709\u968f\u673a\u76f8\u4f4d\u7684\u7ed3\u679c\u3002", "result": "\u8bc1\u660e\u4e86cU\u4ec5\u5728\u63d0\u4f9bU\u7684\u5168\u5c40\u76f8\u4f4d\u4fe1\u606f\u65f6\u6709\u7528\uff0c\u4e14\u53bb\u63a7\u5236\u540e\u7684\u7535\u8def\u5728\u5ffd\u7565\u5168\u5c40\u76f8\u4f4d\u65f6\u8db3\u591f\u3002", "conclusion": "\u53bb\u63a7\u5236\u6280\u672f\u662f\u53ef\u884c\u7684\uff0c\u5e76\u53ef\u7528\u4e8e\u8bc1\u660e\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b58\u5728\u4f2a\u968f\u673a\u9149\u77e9\u9635\u96c6\u5408\u3002"}}
{"id": "2508.00033", "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u590d\u6742Python\u4ee3\u7801\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4ec5\u6709\u5c11\u6570\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u80fd\u7a33\u5b9a\u751f\u6210\u6b63\u786e\u4ee3\u7801\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7b2c\u4e09\u65b9\u5e93\u7684\u6587\u6863\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u79d1\u5b66\u7814\u7a76\u7684\u4ee3\u7801\u751f\u6210\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u4e0d\u719f\u6089Python API\u7684\u89e3\u6790\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\uff0c\u6d4b\u8bd5LLMs\u5728\u4e24\u79cd\u590d\u6742\u4efb\u52a1\uff08\u6570\u636e\u5206\u6790\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4ee3\u7801\u529f\u80fd\u6027\u548c\u9519\u8bef\u3002", "result": "\u4ec5\u5c11\u6570\u6a21\u578b\u80fd\u7a33\u5b9a\u751f\u6210\u6b63\u786e\u4ee3\u7801\uff0cGPT-4.1\u8868\u73b0\u6700\u4f73\uff1b\u540c\u65f6\u53d1\u73b0\u7b2c\u4e09\u65b9\u5e93\u6587\u6863\u548c\u5b9e\u73b0\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u3001\u5b8c\u5584\u6587\u6863\u5e76\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2508.00009", "pdf": "https://arxiv.org/pdf/2508.00009", "abs": "https://arxiv.org/abs/2508.00009", "authors": ["Sourav Mondal", "Elaine Wong"], "title": "Enabling Immersive XR Collaborations over FTTR Networks (Invited)", "categories": ["cs.NI", "cs.AI"], "comment": "This invited paper was presented in Optica Advanced Photonic Congress\n  2025", "summary": "Fiber-To-The-Room is a potential solution to achieve in-premise extended\nreality collaborations. This paper explores predictive bandwidth allocation and\nseamless handover schemes over FTTR, showing high-quality immersive experience\nfor in-premise collaborations can be achieved. \\c{opyright} 2025 The Author(s).", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86FTTR\u4e2d\u7684\u9884\u6d4b\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6c89\u6d78\u5f0f\u534f\u4f5c\u4f53\u9a8c\u3002", "motivation": "\u7814\u7a76FTTR\u4f5c\u4e3a\u5b9e\u73b0\u5ba4\u5185\u6269\u5c55\u73b0\u5b9e\u534f\u4f5c\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\u3002", "result": "\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u7684\u6c89\u6d78\u5f0f\u534f\u4f5c\u4f53\u9a8c\u3002", "conclusion": "FTTR\u7ed3\u5408\u9884\u6d4b\u5e26\u5bbd\u5206\u914d\u548c\u65e0\u7f1d\u5207\u6362\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5ba4\u5185\u6269\u5c55\u73b0\u5b9e\u534f\u4f5c\u3002"}}
{"id": "2508.00426", "pdf": "https://arxiv.org/pdf/2508.00426", "abs": "https://arxiv.org/abs/2508.00426", "authors": ["Rohan Gandhi", "Ankur Mallick", "Ken Sueda", "Rui Liang"], "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services", "categories": ["cs.DC"], "comment": null, "summary": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.", "AI": {"tldr": "Tetris\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\u548c\u5468\u671f\u6027\u8fc1\u79fb\u547c\u53eb\uff0c\u51cf\u5c11\u9ad8\u8d1f\u8f7d\u5a92\u4f53\u5904\u7406\u5668\uff08MP\uff09\u7684\u4f7f\u7528\uff0c\u63d0\u5347\u4f1a\u8bae\u670d\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\uff08\u5982Teams\u4f7f\u7528\u7684\uff09\u5bb9\u6613\u5bfc\u81f4MP\u8fc7\u8f7d\uff0c\u5f71\u54cd\u6027\u80fd\u5e76\u589e\u52a0\u6210\u672c\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u7565\u4e86\u547c\u53ebCPU\u4f7f\u7528\u7684\u53d8\u5f02\u6027\u3002", "method": "Tetris\u6846\u67b6\u7ed3\u5408\u5386\u53f2\u6570\u636e\u4f18\u5316\u521d\u59cb\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u4f18\u5316\u5468\u671f\u6027\u8fc1\u79fb\u547c\u53eb\u4ee5\u51cf\u5c11\u70edMP\u7684\u4f7f\u7528\u3002", "result": "\u5728\u5305\u542b1000\u4e07\u6b21\u547c\u53eb\u768424\u5c0f\u65f6\u8ddf\u8e2a\u4e2d\uff0cTetris\u5c06\u70edMP\u4e0a\u7684\u53c2\u4e0e\u8005\u6570\u91cf\u51cf\u5c11\u4e86\u81f3\u5c112.5\u500d\u3002", "conclusion": "Tetris\u6709\u6548\u89e3\u51b3\u4e86MP\u8fc7\u8f7d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2508.00039", "pdf": "https://arxiv.org/pdf/2508.00039", "abs": "https://arxiv.org/abs/2508.00039", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6d4b\u91cf\u516c\u8def\u94c1\u8def\u5e73\u4ea4\u9053\u53e3\uff08HRGC\uff09\u7684\u5256\u9762\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u7684\u95ee\u9898\u3002", "motivation": "HRGC\u7684\u9ad8\u5256\u9762\u53ef\u80fd\u5bfc\u81f4\u8f66\u8f86\u5361\u4f4f\uff0c\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Transformer-LSTM\u3001LSTM-Transformer\u5e8f\u5217\u548c\u5e76\u884c\u6a21\u578b\uff09\uff0c\u5229\u7528IMU\u548cGPS\u4f20\u611f\u5668\u6570\u636e\u4e0e\u5730\u9762\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b2\u548c3\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u5feb\u901f\u751f\u62102D/3D HRGC\u5256\u9762\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u8bc4\u4f30\u6548\u7387\u3002", "conclusion": "\u8be5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e3aHRGC\u5256\u9762\u6d4b\u91cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u516c\u8def\u548c\u94c1\u8def\u5b89\u5168\u6027\u3002"}}
{"id": "2508.00276", "pdf": "https://arxiv.org/pdf/2508.00276", "abs": "https://arxiv.org/abs/2508.00276", "authors": ["Shuichi Hirahara", "Naoto Ohsaka"], "title": "Asymptotically Optimal Inapproximability of E$k$-SAT Reconfiguration", "categories": ["cs.CC", "cs.DM", "cs.DS"], "comment": "To appear in Proceedings of the 66th IEEE Symposium on Foundations of\n  Computer Science (FOCS 2025)", "summary": "In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable\n$k$-CNF formula $\\varphi$ where each clause contains exactly $k$ literals,\nalong with a pair of its satisfying assignments. The objective is transform one\nsatisfying assignment into the other by repeatedly flipping the value of a\nsingle variable, while maximizing the minimum fraction of satisfied clauses of\n$\\varphi$ throughout the transformation. In this paper, we demonstrate that the\noptimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 -\n\\Theta\\left(\\frac{1}{k}\\right)$. On the algorithmic side, we develop a\ndeterministic $\\left(1-\\frac{1}{k-1}-\\frac{1}{k}\\right)$-factor approximation\nalgorithm for every $k \\geq 3$. On the hardness side, we show that it is\n$\\mathsf{PSPACE}$-hard to approximate this problem within a factor of\n$1-\\frac{1}{10k}$ for every sufficiently large $k$. Note that an\n``$\\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT,\nwhose approximation threshold is $1-\\frac{1}{2^k}$ shown by H\\r{a}stad (JACM\n2001). To the best of our knowledge, this is the first reconfiguration problem\nwhose approximation threshold is (asymptotically) worse than that of its\n$\\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new\n``non-monotone'' test, which is specially tailored to reconfiguration problems,\ndespite not being helpful in the PCP regime.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Maxmin E$k$-SAT Reconfiguration\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8fd1\u4f3c\u7b97\u6cd5\u5e76\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u56e0\u5b50\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u5728\u6ee1\u8db3$k$-CNF\u516c\u5f0f\u7684\u8d4b\u503c\u8f6c\u6362\u8fc7\u7a0b\u4e2d\uff0c\u5982\u4f55\u6700\u5927\u5316\u6700\u5c0f\u6ee1\u8db3\u5b50\u53e5\u6bd4\u4f8b\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u786e\u5b9a\u6027\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u95ee\u9898\u7684PSPACE-hard\u8fd1\u4f3c\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u8fd1\u4f3c\u56e0\u5b50\u4e3a$1 - \\Theta\\left(\\frac{1}{k}\\right)$\uff0c\u5e76\u63d0\u51fa\u4e86$(1-\\frac{1}{k-1}-\\frac{1}{k})$-\u56e0\u5b50\u7b97\u6cd5\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8fd1\u4f3c\u9608\u503c\u6bd4\u5176NP\u7c7b\u4f3c\u95ee\u9898\u66f4\u5dee\u7684\u91cd\u914d\u7f6e\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u975e\u5355\u8c03\u6d4b\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2508.00045", "pdf": "https://arxiv.org/pdf/2508.00045", "abs": "https://arxiv.org/abs/2508.00045", "authors": ["Samah Kansab"], "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "categories": ["cs.SE"], "comment": null, "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6700\u4f73\u5b9e\u8df5\u3001\u6311\u6218\u548c\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u7a33\u5065\u7684ML\u6d41\u7a0b\u5bf9\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u548c\u6548\u7387\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u5bfc\u81f4\u8c03\u8bd5\u65f6\u95f4\u957f\u3001\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002ML\u4e3a\u81ea\u52a8\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u6d41\u7a0b\u7684\u7a33\u5065\u6027\u3002", "method": "\u901a\u8fc7SLR\u5206\u6790\u6700\u65b0\u7684ML\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u7b97\u6cd5\u9009\u62e9\u548c\u9a8c\u8bc1\u6280\u672f\uff0c\u5982SMOTE\u3001SZZ\u7b97\u6cd5\u3001\u96c6\u6210\u65b9\u6cd5\u548c\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7a33\u5065\u7684\u9884\u5904\u7406\u548c\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\uff09\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6734\u7d20\u8d1d\u53f6\u65af\u7b49\u7b80\u5355\u6a21\u578b\u5728\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4ecd\u6709\u4ef7\u503c\u3002\u65b0\u6307\u6807\uff08\u5982BAM\uff09\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u5d2d\u9732\u5934\u89d2\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u826f\u597d\u7684ML\u6d41\u7a0b\u5bf9\u89e3\u51b3SE\u6311\u6218\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4f18\u5316\u8f6f\u4ef6\u8d28\u91cf\u548c\u6548\u7387\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u4e3a\u672a\u6765\u521b\u65b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.00010", "pdf": "https://arxiv.org/pdf/2508.00010", "abs": "https://arxiv.org/abs/2508.00010", "authors": ["Ruibo Wang", "Baha Eddine Youcef Belmekki", "Howard H. Yang", "Mohamed Slim Alouini"], "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?", "categories": ["cs.NI"], "comment": null, "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u76f8\u5bf9\u8bef\u5dee\u91cf\u5316\u5e73\u9762\u4e0e\u7403\u9762\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u70b9\u8fc7\u7a0b\u751f\u6210\u7b97\u6cd5\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u63a8\u5bfc\u6700\u4f18\u5e73\u9762\u9ad8\u5ea6\u8868\u8fbe\u5f0f\uff0c\u4e3aNTN\u5efa\u6a21\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u968f\u7740\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7f51\u7edc\u6027\u80fd\u5206\u6790\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u6025\u5267\u589e\u52a0\uff0c\u800c\u5e73\u9762\u6a21\u578b\u56e0\u5ffd\u7565\u5730\u7403\u66f2\u7387\u5728\u9ad8\u7a7aNTN\u5206\u6790\u4e2d\u5b58\u5728\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u70b9\u8fc7\u7a0b\u751f\u6210\u7b97\u6cd5\uff0c\u751f\u6210\u5e73\u9762\u548c\u7403\u9762\u70b9\u8fc7\u7a0b\u5bf9\uff1b\u5f15\u5165\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5f00\u53d1\u76f8\u5bf9\u8bef\u5dee\u4f30\u8ba1\u7b97\u6cd5\uff1b\u63a8\u5bfc\u6700\u4f18\u5e73\u9762\u9ad8\u5ea6\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u7814\u7a76\u4e86\u90e8\u7f72\u9ad8\u5ea6\u548c\u533a\u57df\u5bf9NTN\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7HAP\u548cLEO\u536b\u661f\u661f\u5ea7\u6848\u4f8b\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e73\u9762\u6a21\u578b\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u6807\u51c6\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u652f\u6301NTN\u7684\u9ad8\u6548\u5206\u6790\u3002"}}
{"id": "2508.00622", "pdf": "https://arxiv.org/pdf/2508.00622", "abs": "https://arxiv.org/abs/2508.00622", "authors": ["Kapel Dev", "Yash Madhwal", "Sofia Shevelo", "Pavel Osinenko", "Yury Yanovich"], "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments", "categories": ["cs.DC"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.", "AI": {"tldr": "SwarnRaft\u662f\u4e00\u4e2a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u5b9a\u4f4d\u548c\u5171\u8bc6\u6846\u67b6\uff0c\u7528\u4e8e\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u7ef4\u6301\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u8c03\u548c\u6570\u636e\u5b8c\u6574\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u5173\u952e\u5e94\u7528\u4e2d\u4f9d\u8d56GNSS\u4fe1\u53f7\uff0c\u4f46\u4fe1\u53f7\u53ef\u80fd\u56e0\u5e72\u6270\u3001\u73af\u5883\u6216\u653b\u51fb\u800c\u4e2d\u65ad\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u5229\u7528Raft\u5171\u8bc6\u7b97\u6cd5\uff0c\u4f7f\u65e0\u4eba\u673a\u8282\u70b9\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u4ecd\u80fd\u8fbe\u6210\u72b6\u6001\u66f4\u65b0\u5171\u8bc6\uff0c\u5e76\u901a\u8fc7\u672c\u5730\u611f\u77e5\u548cWiFi\u901a\u4fe1\u5b9e\u73b0\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u4fe1\u53f7\u4e22\u5931\u65f6\u901a\u8fc7\u5171\u8bc6\u91cd\u5efa\u6216\u9a8c\u8bc1\u8282\u70b9\u4f4d\u7f6e\u7684\u9c81\u68d2\u6027\uff0c\u4fdd\u6301\u4e86\u7fa4\u7684\u4e00\u81f4\u6027\u548c\u5bb9\u9519\u6027\u3002", "conclusion": "SwarnRaft\u4e3a\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u65e0\u4eba\u673a\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u57fa\u7840\u3002"}}
{"id": "2508.00040", "pdf": "https://arxiv.org/pdf/2508.00040", "abs": "https://arxiv.org/abs/2508.00040", "authors": ["Abhinav Das", "Stephan Schl\u00fcter"], "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "comment": null, "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "AI": {"tldr": "\u7ed3\u5408\u8d1d\u53f6\u65af\u673a\u5236\u68c0\u6d4b\u4e0e\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\uff0c\u63d0\u51faR-NP\u6a21\u578b\u7528\u4e8e\u5fb7\u56fd\u7535\u529b\u5e02\u573a24\u5c0f\u65f6\u7535\u4ef7\u9884\u6d4b\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u5747\u8861\u4e14\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7535\u4ef7\u9884\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u5e02\u573a\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u7ed3\u5408\u673a\u5236\u68c0\u6d4b\u4e0e\u5c40\u90e8\u5efa\u6a21\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528DS-HDP-HMM\u68c0\u6d4b\u7535\u4ef7\u673a\u5236\uff0c\u6bcf\u4e2a\u673a\u5236\u7531\u72ec\u7acb\u7684CNP\u5efa\u6a21\uff0c\u6700\u7ec8\u9884\u6d4b\u4e3a\u673a\u5236\u52a0\u6743\u7684CNP\u8f93\u51fa\u6df7\u5408\u3002", "result": "R-NP\u5728TOPSIS\u591a\u6807\u51c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u57282021-2023\u5e74\u6210\u4e3a\u6700\u5747\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "R-NP\u6a21\u578b\u5728\u7535\u4ef7\u9884\u6d4b\u4e2d\u517c\u5177\u51c6\u786e\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u4f18\u4e8eDNN\u548cLEAR\u6a21\u578b\u3002"}}
{"id": "2508.00083", "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff0c\u6db5\u76d6\u5176\u81ea\u4e3b\u6027\u3001\u4efb\u52a1\u8303\u56f4\u6269\u5c55\u548c\u5de5\u7a0b\u5b9e\u7528\u6027\u589e\u5f3a\u4e09\u5927\u7279\u5f81\uff0c\u5e76\u63a2\u8ba8\u4e86\u6280\u672f\u53d1\u5c55\u3001\u5e94\u7528\u573a\u666f\u3001\u8bc4\u4f30\u5de5\u5177\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u81ea\u4e3b\u6027\u3001\u4efb\u52a1\u8303\u56f4\u6269\u5c55\u548c\u5de5\u7a0b\u5b9e\u7528\u6027\u589e\u5f3a\uff0c\u9769\u65b0\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8c03\u67e5\uff0c\u8ffd\u6eaf\u6280\u672f\u53d1\u5c55\u8f68\u8ff9\uff0c\u5206\u7c7b\u6838\u5fc3\u6280\u672f\uff08\u5355\u4ee3\u7406\u4e0e\u591a\u4ee3\u7406\u67b6\u6784\uff09\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u5176\u5728SDLC\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u4e3b\u6d41\u8bc4\u4f30\u57fa\u51c6\u4e0e\u5de5\u5177\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524d\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u57fa\u7840\u6027\u3001\u957f\u671f\u6027\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.00011", "pdf": "https://arxiv.org/pdf/2508.00011", "abs": "https://arxiv.org/abs/2508.00011", "authors": ["Ahmet Melih Ince", "Ayse Elif Canbilen", "Halim Yanikomeroglu"], "title": "AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": "6 pages, 3 figures, to appear in IEEE conference proceedings", "summary": "Sixth-generation (6G) networks are designed to meet the hyper-reliable and\nlow-latency communication (HRLLC) requirements of safety-critical applications\nsuch as autonomous driving. Integrating non-terrestrial networks (NTN) into the\n6G infrastructure brings redundancy to the network, ensuring continuity of\ncommunications even under extreme conditions. In particular, high-altitude\nplatform stations (HAPS) stand out for their wide coverage and low latency\nadvantages, supporting communication reliability and enhancing information\nfreshness, especially in rural areas and regions with infrastructure\nconstraints. In this paper, we present reinforcement learning-based approaches\nusing deep deterministic policy gradient (DDPG) to dynamically optimize the\nage-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.\nThe proposed method improves information freshness and overall network\nreliability by enabling independent learning without centralized coordination.\nThe findings reveal the potential of HAPS-supported solutions, combined with\nDDPG-based learning, for efficient AoI-aware resource allocation in\nplatoon-based autonomous vehicle systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316HAPS\u652f\u6301\u7684V2X\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\uff0c\u63d0\u5347\u7f51\u7edc\u53ef\u9760\u6027\u3002", "motivation": "6G\u7f51\u7edc\u9700\u6ee1\u8db3\u9ad8\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08HRLLC\uff09\u9700\u6c42\uff0c\u5c24\u5176\u662f\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u5982HAPS\u56e0\u5176\u5e7f\u8986\u76d6\u548c\u4f4e\u5ef6\u8fdf\u4f18\u52bf\uff0c\u53ef\u589e\u5f3a\u901a\u4fe1\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528DDPG\u7b97\u6cd5\u52a8\u6001\u4f18\u5316HAPS\u652f\u6301\u7684V2X\u7f51\u7edc\u4e2d\u7684AoI\uff0c\u5b9e\u73b0\u65e0\u9700\u96c6\u4e2d\u534f\u8c03\u7684\u72ec\u7acb\u5b66\u4e60\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u65b0\u9c9c\u5ea6\u548c\u7f51\u7edc\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8f66\u961f\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "conclusion": "HAPS\u4e0eDDPG\u7ed3\u5408\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\u5728AoI\u4f18\u5316\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u53d7\u9650\u533a\u57df\u3002"}}
{"id": "2508.00041", "pdf": "https://arxiv.org/pdf/2508.00041", "abs": "https://arxiv.org/abs/2508.00041", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "AI": {"tldr": "DevFT\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u6784\u5efaLLM\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u9690\u79c1\u3002", "method": "\u5206\u9636\u6bb5\u5fae\u8c03\uff0c\u9010\u6b65\u589e\u52a0\u5b50\u6a21\u578b\u53c2\u6570\u5bb9\u91cf\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u548c\u4f18\u5316\u521d\u59cb\u5316\u53c2\u6570\u52a0\u901f\u8bad\u7ec3\u3002", "result": "DevFT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u53474.59\u500d\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1110.67\u500d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u53479.07%\u3002", "conclusion": "DevFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2508.00128", "pdf": "https://arxiv.org/pdf/2508.00128", "abs": "https://arxiv.org/abs/2508.00128", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "categories": ["cs.SE"], "comment": null, "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "AI": {"tldr": "\u91cf\u5316\u6280\u672f\u663e\u8457\u964d\u4f4eLLMs4Code\u7684\u9690\u79c1\u98ce\u9669\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4efb\u52a1\u6027\u80fd\u4e0e\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u91cf\u5316\u6280\u672f\u5bf9LLMs4Code\u4efb\u52a1\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u4ee5\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u5bf9Pythia\u3001CodeGen\u548cGPTNeo\u4e09\u79cd\u6a21\u578b\u5bb6\u65cf\u5e94\u7528\u9759\u6001\u548c\u52a8\u6001\u91cf\u5316\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u4efb\u52a1\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\u3002", "result": "\u91cf\u5316\u663e\u8457\u964d\u4f4e\u9690\u79c1\u98ce\u9669\uff0c\u4e14\u4efb\u52a1\u6027\u80fd\u4e0e\u9690\u79c1\u98ce\u9669\u6b63\u76f8\u5173\uff1b\u91cf\u5316\u5927\u6a21\u578b\u53ef\u80fd\u6bd4\u5c0f\u6a21\u578b\u66f4\u4f18\u3002", "conclusion": "\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u548c\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2508.00020", "pdf": "https://arxiv.org/pdf/2508.00020", "abs": "https://arxiv.org/abs/2508.00020", "authors": ["Ferdaous Tarhouni", "Ruibo Wang", "Mohamed-Slim Alouini"], "title": "Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach", "categories": ["cs.NI"], "comment": null, "summary": "In recent years, the satellite-aerial-ground integrated network (SAGIN) has\nbecome essential in meeting the increasing demands for global wireless\ncommunications. In SAGIN, high-altitude platforms (HAPs) can serve as\ncommunication hubs and act as relays to enhance communication performance. In\nthis paper, we evaluate network performance and analyze the role of HAPs in\nSAGIN from the relay perspective. Based on this unique perspective, we\nintroduce three metrics to evaluate the performance, named the average access\ndata rate, the average backhaul data rate, and the backhaul rate exceedance\nprobability (BREP). Considering the need for dynamic topology and interference\nanalysis, we choose spherical stochastic geometry (SSG) as a tool and derive\nanalytical expressions for the above metrics to achieve low-complexity\nperformance evaluation. Specifically, we provide a closed-form expression for\nthe end-to-end performance metric BREP. Given that there is no existing\nliterature in the SSG field studying networks from a relay perspective, we\nspecifically investigate the impact of satellite network topology on\nperformance in our numerical results to further highlight the advantages of the\nSSG framework. Additionally, we analyze the minimum HAP transmission power\nrequired to maintain both short-term and long-term data rate demands.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u536b\u661f-\u7a7a\u4e2d-\u5730\u9762\u4e00\u4f53\u5316\u7f51\u7edc\uff08SAGIN\uff09\u4e2d\u9ad8\u7a7a\u5e73\u53f0\uff08HAPs\uff09\u4f5c\u4e3a\u4e2d\u7ee7\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5229\u7528\u7403\u5f62\u968f\u673a\u51e0\u4f55\uff08SSG\uff09\u8fdb\u884c\u4f4e\u590d\u6742\u5ea6\u6027\u80fd\u5206\u6790\u3002", "motivation": "\u968f\u7740\u5168\u7403\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u7684\u589e\u957f\uff0cSAGIN\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0cHAPs\u4f5c\u4e3a\u4e2d\u7ee7\u53ef\u63d0\u5347\u901a\u4fe1\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u4ece\u4e2d\u7ee7\u89d2\u5ea6\u8bc4\u4f30\u5176\u6027\u80fd\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u7403\u5f62\u968f\u673a\u51e0\u4f55\uff08SSG\uff09\u5de5\u5177\uff0c\u63a8\u5bfc\u4e86\u4e09\u79cd\u6027\u80fd\u6307\u6807\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5305\u62ec\u7aef\u5230\u7aef\u6027\u80fd\u6307\u6807BREP\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u5c55\u793a\u4e86\u536b\u661f\u7f51\u7edc\u62d3\u6251\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u6ee1\u8db3\u77ed\u671f\u548c\u957f\u671f\u6570\u636e\u7387\u9700\u6c42\u7684\u6700\u5c0fHAP\u4f20\u8f93\u529f\u7387\u3002", "conclusion": "SSG\u6846\u67b6\u5728SAGIN\u4e2d\u5177\u6709\u4f18\u52bf\uff0cHAPs\u4f5c\u4e3a\u4e2d\u7ee7\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u7814\u7a76\u4e3a\u52a8\u6001\u62d3\u6251\u548c\u5e72\u6270\u5206\u6790\u63d0\u4f9b\u4e86\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00234", "pdf": "https://arxiv.org/pdf/2508.00234", "abs": "https://arxiv.org/abs/2508.00234", "authors": ["Jin Yang", "Qiong Wu", "Zhiying Feng", "Zhi Zhou", "Deke Guo", "Xu Chen"], "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.MA"], "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nleading to a significant increase in user demand for LLM services. However,\ncloud-based LLM services often suffer from high latency, unstable\nresponsiveness, and privacy concerns. Therefore, multiple LLMs are usually\ndeployed at the network edge to boost real-time responsiveness and protect data\nprivacy, particularly for many emerging smart mobile and IoT applications.\nGiven the varying response quality and latency of LLM services, a critical\nissue is how to route user requests from mobile and IoT devices to an\nappropriate LLM service (i.e., edge LLM expert) to ensure acceptable\nquality-of-service (QoS). Existing routing algorithms fail to simultaneously\naddress the heterogeneity of LLM services, the interference among requests, and\nthe dynamic workloads necessary for maintaining long-term stable QoS. To meet\nthese challenges, in this paper we propose a novel deep reinforcement learning\n(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM\nservices. Due to the dynamic nature of the global state, we propose a dynamic\nstate abstraction technique to compactly represent global state features with a\nheterogeneous graph attention network (HAN). Additionally, we introduce an\naction impact estimator and a tailored reward function to guide the DRL agent\nin maximizing QoS and preventing latency violations. Extensive experiments on\nboth Poisson and real-world workloads demonstrate that our proposed algorithm\nsignificantly improves average QoS and computing resource efficiency compared\nto existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684QoS\u611f\u77e5LLM\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u8fb9\u7f18LLM\u670d\u52a1\u7684\u54cd\u5e94\u8d28\u91cf\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4e91LLM\u670d\u52a1\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u9690\u79c1\u95ee\u9898\uff0c\u8fb9\u7f18\u90e8\u7f72LLM\u53ef\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u9700\u89e3\u51b3\u8def\u7531\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u72b6\u6001\u62bd\u8c61\u6280\u672f\u548c\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08HAN\uff09\u8868\u793a\u5168\u5c40\u72b6\u6001\uff0c\u7ed3\u5408\u52a8\u4f5c\u5f71\u54cd\u4f30\u8ba1\u5668\u548c\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u4f18\u5316DRL\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747QoS\u548c\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684DRL\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8fb9\u7f18LLM\u670d\u52a1\u7684\u52a8\u6001\u8def\u7531\u95ee\u9898\uff0c\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2508.00043", "pdf": "https://arxiv.org/pdf/2508.00043", "abs": "https://arxiv.org/abs/2508.00043", "authors": ["Nhut Truong", "Uri Hasson"], "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "categories": ["cs.LG"], "comment": null, "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u7a7a\u95f4\u7ea6\u675f\uff08\u6743\u91cd\u76f8\u4f3c\u6027\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff09\u5bf9\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6743\u91cd\u76f8\u4f3c\u6027\u5728\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u5730\u5f62\u7ea6\u675f\u7684\u4e0d\u540c\u5b9e\u73b0\u65b9\u5f0f\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8868\u793a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6743\u91cd\u76f8\u4f3c\u6027\uff08WS\uff09\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff08AS\uff09\u4e24\u79cd\u7ea6\u675f\u8bad\u7ec3\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u8bc4\u4f30\u5206\u7c7b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8868\u793a\u7684\u7a7a\u95f4\u7ec4\u7ec7\u3002", "result": "WS\u5728\u566a\u58f0\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u4f18\u4e8eAS\u548c\u6807\u51c6CNN\uff0c\u5e76\u5f71\u54cd\u4e86\u7f51\u7edc\u7684\u8868\u793a\u51e0\u4f55\u3002", "conclusion": "\u6743\u91cd\u76f8\u4f3c\u6027\u7ea6\u675f\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u4e2d\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u5e76\u5bf9\u7279\u5f81\u5b66\u4e60\u548c\u529f\u80fd\u7ec4\u7ec7\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2508.00198", "pdf": "https://arxiv.org/pdf/2508.00198", "abs": "https://arxiv.org/abs/2508.00198", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "categories": ["cs.SE"], "comment": null, "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u771f\u5b9e\u5e94\u7528\u5f00\u53d1\u4e2d\u5982\u4f55\u6d4b\u8bd5\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u53d1\u73b0\u6d4b\u8bd5\u7b56\u7565\u7ed3\u5408\u4e86\u624b\u52a8\u4e0e\u81ea\u52a8\u65b9\u6cd5\uff0c\u5e76\u9762\u4e34\u6a21\u578b\u884c\u4e3a\u4e0d\u786e\u5b9a\u7b49\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u5176\u96c6\u6210\u7cfb\u7edf\u7684\u6d4b\u8bd5\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u679099\u4efd\u5b66\u751f\u62a5\u544a\uff0c\u91c7\u7528\u4e3b\u9898\u5206\u6790\u548c\u7ed3\u6784\u5316\u7f16\u7801\u8fdb\u884c\u63a2\u7d22\u6027\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6d4b\u8bd5\u7b56\u7565\u5305\u62ec\u63a2\u7d22\u6027\u6d4b\u8bd5\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u63d0\u793a\u8fed\u4ee3\uff0c\u6311\u6218\u5305\u62ec\u96c6\u6210\u5931\u8d25\u3001\u8f93\u51fa\u4e0d\u53ef\u9884\u6d4b\u548c\u63d0\u793a\u654f\u611f\u6027\u3002", "conclusion": "\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u9700\u7ed3\u5408\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u548c\u884c\u4e3a\u611f\u77e5\u8bc4\u4f30\uff0c\u4e3a\u751f\u6210\u7ec4\u4ef6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u8df5\u4f9d\u636e\u3002"}}
{"id": "2508.00028", "pdf": "https://arxiv.org/pdf/2508.00028", "abs": "https://arxiv.org/abs/2508.00028", "authors": ["Abir Ray"], "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": "12 pages", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u548cITU-R\u4f20\u64ad\u6a21\u578b\u7684\u53ef\u6269\u5c55\u9891\u8c31\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u9891\u8c31\u63a5\u5165\u3002", "motivation": "\u9891\u8c31\u8d44\u6e90\u5728\u65f6\u7a7a\u4e0a\u5e38\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u52a8\u6001\u9891\u8c31\u63a5\u5165\u7b56\u7565\u4ee5\u5229\u7528\u7a7a\u95f2\u9891\u6bb5\u3002", "method": "\u7ed3\u5408\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\uff08\u6355\u6349\u65f6\u95f4\u5360\u7528\u6a21\u5f0f\uff09\u548cITU-R\u4f20\u64ad\u6a21\u578b\uff08\u8003\u8651\u8def\u5f84\u635f\u8017\u548c\u6742\u6ce2\u6548\u5e94\uff09\uff0c\u9884\u6d4b\u9891\u8c31\u53ef\u7528\u6027\u3002", "result": "\u65b9\u6cd5\u80fd\u9ad8\u6548\u9884\u6d4b\u65f6\u7a7a\u9891\u8c31\u673a\u4f1a\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u9891\u8c31\u7ba1\u7406\u3002", "conclusion": "\u6846\u67b6\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u9891\u6bb5\u548c\u573a\u666f\uff0c\u4e3a\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2508.00596", "pdf": "https://arxiv.org/pdf/2508.00596", "abs": "https://arxiv.org/abs/2508.00596", "authors": ["Xiang Zhang", "Zhou Li", "Shuangyang Li", "Kai Wan", "Derrick Wing Kwan Ng", "Giuseppe Caire"], "title": "Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience", "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.LG", "math.IT"], "comment": "Submitted to IEEE for potential journal publication", "summary": "In decentralized federated learning (FL), multiple clients collaboratively\nlearn a shared machine learning (ML) model by leveraging their privately held\ndatasets distributed across the network, through interactive exchange of the\nintermediate model updates. To ensure data security, cryptographic techniques\nare commonly employed to protect model updates during aggregation. Despite\ngrowing interest in secure aggregation, existing works predominantly focus on\nprotocol design and computational guarantees, with limited understanding of the\nfundamental information-theoretic limits of such systems. Moreover, optimal\nbounds on communication and key usage remain unknown in decentralized settings,\nwhere no central aggregator is available. Motivated by these gaps, we study the\nproblem of decentralized secure aggregation (DSA) from an information-theoretic\nperspective. Specifically, we consider a network of $K$ fully-connected users,\neach holding a private input -- an abstraction of local training data -- who\naim to securely compute the sum of all inputs. The security constraint requires\nthat no user learns anything beyond the input sum, even when colluding with up\nto $T$ other users. We characterize the optimal rate region, which specifies\nthe minimum achievable communication and secret key rates for DSA. In\nparticular, we show that to securely compute one symbol of the desired input\nsum, each user must (i) transmit at least one symbol to others, (ii) hold at\nleast one symbol of secret key, and (iii) all users must collectively hold no\nfewer than $K - 1$ independent key symbols. Our results establish the\nfundamental performance limits of DSA, providing insights for the design of\nprovably secure and communication-efficient protocols in distributed learning\nsystems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u805a\u5408\u95ee\u9898\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5206\u6790\u4e86\u901a\u4fe1\u548c\u5bc6\u94a5\u4f7f\u7528\u7684\u6700\u4f18\u754c\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u534f\u8bae\u8bbe\u8ba1\u548c\u8ba1\u7b97\u4fdd\u8bc1\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u4fe1\u606f\u8bba\u6781\u9650\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e2d\u3002", "method": "\u8003\u8651\u4e86K\u4e2a\u5b8c\u5168\u8fde\u63a5\u7684\u7528\u6237\u7f51\u7edc\uff0c\u6bcf\u4e2a\u7528\u6237\u6301\u6709\u79c1\u6709\u8f93\u5165\uff0c\u76ee\u6807\u662f\u5b89\u5168\u8ba1\u7b97\u6240\u6709\u8f93\u5165\u7684\u548c\u3002", "result": "\u786e\u5b9a\u4e86\u6700\u4f18\u901f\u7387\u533a\u57df\uff0c\u8868\u660e\u6bcf\u4e2a\u7528\u6237\u5fc5\u987b\u4f20\u8f93\u81f3\u5c11\u4e00\u4e2a\u7b26\u53f7\u3001\u6301\u6709\u81f3\u5c11\u4e00\u4e2a\u5bc6\u94a5\u7b26\u53f7\uff0c\u4e14\u6240\u6709\u7528\u6237\u9700\u6301\u6709\u4e0d\u5c11\u4e8eK-1\u4e2a\u72ec\u7acb\u5bc6\u94a5\u7b26\u53f7\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u5206\u5e03\u5f0f\u5b66\u4e60\u7cfb\u7edf\u4e2d\u8bbe\u8ba1\u5b89\u5168\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u534f\u8bae\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.00046", "pdf": "https://arxiv.org/pdf/2508.00046", "abs": "https://arxiv.org/abs/2508.00046", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "categories": ["cs.LG", "cs.AI"], "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u6027\u80fd\u7684\u57fa\u51c6\u6846\u67b6POBAX\uff0c\u5f3a\u8c03\u57fa\u51c6\u9700\u8986\u76d6\u591a\u79cd\u90e8\u5206\u53ef\u89c2\u6d4b\u5f62\u5f0f\u5e76\u4f53\u73b0\u8bb0\u5fc6\u6539\u8fdb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u8bc4\u4f30\u7b80\u5355\u7684\u72b6\u6001\u6df7\u6dc6\u5f62\u5f0f\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u590d\u6742\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\uff0c\u5982\u89c6\u89c9\u906e\u6321\u6216\u672a\u77e5\u5bf9\u624b\u610f\u56fe\u3002", "method": "\u63d0\u51faPOBAX\u5f00\u6e90\u5e93\uff0c\u5305\u542b\u591a\u79cd\u4ee3\u8868\u6027\u73af\u5883\uff08\u5982\u5b9a\u4f4d\u3001\u89c6\u89c9\u63a7\u5236\u3001\u6e38\u620f\u7b49\uff09\uff0c\u5e76\u9a8c\u8bc1\u8fd9\u4e9b\u4efb\u52a1\u5177\u6709\u8bb0\u5fc6\u6539\u8fdb\u80fd\u529b\u3002", "result": "POBAX\u63d0\u4f9b\u4e86\u5feb\u901f\u8bc4\u4f30\u7684\u5de5\u5177\u548cGPU\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u73af\u5883\uff0c\u652f\u6301\u7b97\u6cd5\u5728\u590d\u6742\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u6027\u80fd\u6d4b\u8bd5\u3002", "conclusion": "POBAX\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7b97\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.00244", "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "AI": {"tldr": "\u6bd4\u8f83\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\uff08OOP\uff09\u548c\u51fd\u6570\u5f0f\u7f16\u7a0b\uff08FP\uff09\u5bf9\u8f6f\u4ef6\u7cfb\u7edf\u67b6\u6784\u7279\u6027\u7684\u5f71\u54cd\uff0c\u901a\u8fc7Kotlin\u548cScala\u5b9e\u73b0\u6570\u5b57\u94b1\u5305\u7cfb\u7edf\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "motivation": "\u968f\u7740\u51fd\u6570\u5f0f\u7f16\u7a0b\u5728\u8f6f\u4ef6\u884c\u4e1a\u4e2d\u7684\u5173\u6ce8\u5ea6\u589e\u52a0\uff0c\u7814\u7a76OOP\u548cFP\u5bf9\u7cfb\u7edf\u67b6\u6784\u7279\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7ec4\u7ec7\u63d0\u4f9b\u9009\u62e9\u7f16\u7a0b\u8303\u5f0f\u7684\u4f9d\u636e\u3002", "method": "\u901a\u8fc7Kotlin\uff08OOP\uff09\u548cScala\uff08FP\uff09\u5b9e\u73b0\u6570\u5b57\u94b1\u5305\u7cfb\u7edf\uff0c\u8fdb\u884c\u5b9a\u6027\u7684\u81ea\u6211\u6c11\u65cf\u5fd7\u5206\u6790\u548c\u5b9a\u91cf\u7684\u5f00\u53d1\u8005\u8c03\u67e5\u5206\u6790\u3002", "result": "\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u7f16\u5199\u4ee3\u7801\u7684\u89c6\u89d2\uff0c\u5b9a\u91cf\u5206\u6790\u5c55\u793a\u4e86\u5f00\u53d1\u8005\u5bf9\u4ee3\u7801\u7684\u9605\u8bfb\u4f53\u9a8c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u8005\u548c\u7ec4\u7ec7\u5728\u9009\u62e9\u7f16\u7a0b\u8303\u5f0f\u65f6\u63d0\u4f9b\u4e86\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2508.00042", "pdf": "https://arxiv.org/pdf/2508.00042", "abs": "https://arxiv.org/abs/2508.00042", "authors": ["Athanasios Tziouvaras", "Carolina Fortuna", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Marko Grobelnik", "Bla\u017e Bertalani\u010d"], "title": "Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network", "categories": ["cs.NI", "cs.LG"], "comment": "10 pages, 12 figures", "summary": "AI-native 6G networks promise unprecedented automation and performance by\nembedding machine-learning models throughout the radio access and core segments\nof the network. However, the non-stationary nature of wireless environments due\nto infrastructure changes, user mobility, and emerging traffic patterns,\ninduces concept drifts that can quickly degrade these model accuracies.\nExisting methods in general are very domain specific, or struggle with certain\ntype of concept drift. In this paper, we introduce two unsupervised,\nmodel-agnostic, batch concept drift detectors. Both methods compute an\nexpected-utility score to decide when concept drift occurred and if model\nretraining is warranted, without requiring ground-truth labels after\ndeployment. We validate our framework on two real-world wireless use cases in\noutdoor fingerprinting for localization and for link-anomaly detection, and\ndemonstrate that both methods are outperforming classical detectors such as\nADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an\nF1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus\nreducing the false alarm rate by up to 20 percentage points compared to the\nbest classical detectors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65e0\u76d1\u7763\u3001\u6a21\u578b\u65e0\u5173\u7684\u6279\u91cf\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8eAI\u539f\u751f6G\u7f51\u7edc\u4e2d\u6a21\u578b\u6027\u80fd\u7684\u7ef4\u62a4\u3002", "motivation": "\u65e0\u7ebf\u73af\u5883\u7684\u975e\u9759\u6001\u6027\uff08\u5982\u57fa\u7840\u8bbe\u65bd\u53d8\u5316\u3001\u7528\u6237\u79fb\u52a8\u6027\u7b49\uff09\u4f1a\u5bfc\u81f4\u6982\u5ff5\u6f02\u79fb\uff0c\u964d\u4f4e\u6a21\u578b\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u9886\u57df\u4e13\u7528\u6216\u5bf9\u67d0\u4e9b\u6f02\u79fb\u7c7b\u578b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u57fa\u4e8e\u9884\u671f\u6548\u7528\u5206\u6570\u7684\u65e0\u76d1\u7763\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u90e8\u7f72\u540e\u7684\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u5224\u65ad\u662f\u5426\u9700\u8981\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "result": "\u5728\u5ba4\u5916\u6307\u7eb9\u5b9a\u4f4d\u548c\u94fe\u8def\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u9645\u7528\u4f8b\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u6bd4\u7ecf\u5178\u68c0\u6d4b\u5668\uff08\u5982ADWIN\u3001DDM\u3001CUSUM\uff09\u6027\u80fd\u63d0\u534720-40\u4e2a\u767e\u5206\u70b9\uff0cF1\u5206\u6570\u8fbe0.94\u548c1.00\uff0c\u8bef\u62a5\u7387\u964d\u4f4e20\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u6a21\u578b\u7ef4\u62a4\u3002"}}
{"id": "2508.00636", "pdf": "https://arxiv.org/pdf/2508.00636", "abs": "https://arxiv.org/abs/2508.00636", "authors": ["Haocheng Jiang", "Hua Shen", "Jixin Zhang", "Willy Susilo", "Mingwu Zhang"], "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.", "AI": {"tldr": "FedGuard\u662f\u4e00\u79cd\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u5229\u7528\u6210\u5458\u63a8\u65ad\u5bf9\u6a21\u578b\u504f\u5dee\u7684\u9ad8\u654f\u611f\u6027\uff0c\u6709\u6548\u8bc6\u522b\u5e76\u6392\u9664\u4e2d\u6bd2\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u6613\u53d7\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u5c24\u5176\u662f\u5f53\u6076\u610f\u5ba2\u6237\u7aef\u8d85\u8fc750%\u6216\u6570\u636e\u9ad8\u5ea6\u975e\u72ec\u7acb\u540c\u5206\u5e03\u65f6\u3002\u73b0\u6709\u9632\u5fa1\u673a\u5236\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u653b\u51fb\u7c7b\u578b\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "FedGuard\u8981\u6c42\u5ba2\u6237\u7aef\u5728\u8bad\u7ec3\u4e2d\u5305\u542b\u670d\u52a1\u5668\u6307\u5b9a\u7684\u989d\u5916\u5c0f\u6279\u91cf\u6570\u636e\uff0c\u901a\u8fc7\u89c2\u5bdf\u5176\u7f6e\u4fe1\u5ea6\u4e0b\u964d\u6765\u8bc6\u522b\u4e2d\u6bd2\u6a21\u578b\u3002", "result": "\u572890%\u5ba2\u6237\u7aef\u4e3a\u62dc\u5360\u5ead\u4e14\u6570\u636e\u9ad8\u5ea6\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0cFedGuard\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u80fd\u62b5\u5fa1\u591a\u79cd\u653b\u51fb\u7c7b\u578b\u3002", "conclusion": "FedGuard\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u62dc\u5360\u5ead\u653b\u51fb\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.00047", "pdf": "https://arxiv.org/pdf/2508.00047", "abs": "https://arxiv.org/abs/2508.00047", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6TriP-LLM\uff0c\u901a\u8fc7\u4e09\u5206\u652f\u8bbe\u8ba1\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u548c\u667a\u80fd\u5236\u9020\u7684\u666e\u53ca\uff0c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u9ad8\u5f02\u8d28\u6027\u548c\u590d\u6742\u6027\u3002", "method": "TriP-LLM\u91c7\u7528\u4e09\u5206\u652f\u8bbe\u8ba1\uff08\u5206\u5757\u3001\u9009\u62e9\u548c\u5168\u5c40\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u4e3a\u5206\u5757\u6807\u8bb0\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u91cd\u6784\u8f93\u5165\u4ee5\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTriP-LLM\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "TriP-LLM\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8eGPU\u5185\u5b58\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2508.00253", "pdf": "https://arxiv.org/pdf/2508.00253", "abs": "https://arxiv.org/abs/2508.00253", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "categories": ["cs.SE"], "comment": null, "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "AI": {"tldr": "GenLoc\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684bug\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u63a2\u7d22\u529f\u80fd\u8fed\u4ee3\u5206\u6790\u4ee3\u7801\u5e93\uff0c\u89e3\u51b3bug\u62a5\u544a\u4e0e\u6e90\u4ee3\u7801\u4e4b\u95f4\u7684\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709bug\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\u6548\u679c\u6709\u9650\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528LLM\u548c\u4ee3\u7801\u63a2\u7d22\u529f\u80fd\u8fed\u4ee3\u5206\u6790\u4ee3\u7801\u5e93\uff0c\u53ef\u9009\u5411\u91cf\u5d4c\u5165\u68c0\u7d22\u76f8\u5173\u6587\u4ef6\u3002", "result": "\u57286\u4e2a\u5927\u578bJava\u9879\u76ee\u76849,000\u591a\u4e2abug\u62a5\u544a\u4e0a\u6d4b\u8bd5\uff0cGenLoc\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e5\u79cd\u5148\u8fdb\u6280\u672f\uff0cAccuracy@1\u5e73\u5747\u63d0\u534760%\u4ee5\u4e0a\u3002", "conclusion": "GenLoc\u901a\u8fc7LLM\u548c\u4ee3\u7801\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86bug\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00228", "pdf": "https://arxiv.org/pdf/2508.00228", "abs": "https://arxiv.org/abs/2508.00228", "authors": ["Aashay Arora", "Diego Davila", "Frank W\u00fcrthwein", "John Graham", "Dima Mishin", "Justas Balcas", "Tom Lehman", "Xi Yang", "Chin Guok", "Harvey Newman"], "title": "Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies", "categories": ["cs.NI"], "comment": "Submitted to CHEP 24", "summary": "In anticipation of the High Luminosity-LHC era, there is a critical need to\noversee software readiness for upcoming growth in network traffic for\nproduction and user data analysis access. This paper looks into software and\nhardware required improvements in US-CMS Tier-2 sites to be able to sustain and\nmeet the projected 400 Gbps bandwidth demands while tackling the challenge\nposed by varying latencies between sites. Specifically, our study focuses on\nidentifying the performance of XRootD HTTP third-party copies across multiple\n400 Gbps links and exploring different host and transfer configurations. Our\napproach involves systematic testing with variations in the number of origins\nper cluster and CPU allocations for each origin. By replicating real network\nconditions and creating network \"loops\" that traverse multiple switches across\nthe wide area network, we are able to replicate authentic network conditions", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86US-CMS Tier-2\u7ad9\u70b9\u4e3a\u5e94\u5bf9\u9ad8\u4eae\u5ea6LHC\u65f6\u4ee3400 Gbps\u5e26\u5bbd\u9700\u6c42\u6240\u9700\u7684\u8f6f\u786c\u4ef6\u6539\u8fdb\uff0c\u91cd\u70b9\u6d4b\u8bd5\u4e86XRootD HTTP\u7b2c\u4e09\u65b9\u590d\u5236\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u672a\u6765\u7f51\u7edc\u6d41\u91cf\u589e\u957f\u548c\u7528\u6237\u6570\u636e\u5206\u6790\u9700\u6c42\uff0c\u786e\u4fdd\u8f6f\u4ef6\u548c\u786c\u4ef6\u80fd\u591f\u652f\u6301400 Gbps\u5e26\u5bbd\u9700\u6c42\uff0c\u5e76\u89e3\u51b3\u7ad9\u70b9\u95f4\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\uff0c\u63a2\u7d22\u4e0d\u540c\u4e3b\u673a\u548c\u4f20\u8f93\u914d\u7f6e\uff0c\u5305\u62ec\u6bcf\u96c6\u7fa4\u7684\u6e90\u6570\u91cf\u548cCPU\u5206\u914d\u3002", "result": "\u901a\u8fc7\u521b\u5efa\u8de8\u5e7f\u57df\u7f51\u7684\u7f51\u7edc\u201c\u5faa\u73af\u201d\uff0c\u6210\u529f\u6a21\u62df\u4e86\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\uff0c\u5e76\u6d4b\u8bd5\u4e86XRootD\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3aUS-CMS Tier-2\u7ad9\u70b9\u5728\u9ad8\u5e26\u5bbd\u9700\u6c42\u4e0b\u7684\u8f6f\u786c\u4ef6\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2508.00806", "pdf": "https://arxiv.org/pdf/2508.00806", "abs": "https://arxiv.org/abs/2508.00806", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "categories": ["cs.LG", "cs.DC"], "comment": "8 pages", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "AI": {"tldr": "Adacc\u662f\u4e00\u4e2a\u7ed3\u5408\u81ea\u9002\u5e94\u538b\u7f29\u548c\u6fc0\u6d3b\u68c0\u67e5\u70b9\u7684\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u91cd\u8ba1\u7b97\u4f1a\u5e26\u6765\u9ad8\u8fbe30%\u7684\u5f00\u9500\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u4f18\u5316\u548c\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "Adacc\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a(1) \u9488\u5bf9LLM\u5f20\u91cf\u7684\u5f02\u5e38\u503c\u8bbe\u8ba1\u5c42\u7279\u5b9a\u538b\u7f29\u7b97\u6cd5\uff1b(2) \u4f7f\u7528MILP\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff1b(3) \u5f15\u5165\u81ea\u9002\u5e94\u7b56\u7565\u6f14\u5316\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdacc\u6bd4\u73b0\u6709\u6846\u67b6\u52a0\u901f1.01x\u81f31.37x\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "Adacc\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2508.00078", "pdf": "https://arxiv.org/pdf/2508.00078", "abs": "https://arxiv.org/abs/2508.00078", "authors": ["Imen Mahmoud", "Andrei Velichko"], "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "comment": "22 pages, 5 figures", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LightGBM\u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30COVID-19\u76f8\u5173\u6307\u6807\u5bf9\u6bd4\u7279\u5e01\u56de\u62a5\u9884\u6d4b\u7684\u8d21\u732e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u75ab\u60c5\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u6807\u4e0d\u4ec5\u662f\u9884\u6d4b\u6bd4\u7279\u5e01\u56de\u62a5\uff0c\u800c\u662f\u786e\u5b9a\u75ab\u60c5\u76f8\u5173\u7684\u5065\u5eb7\u6570\u636e\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u6bcf\u65e5\u6bd4\u7279\u5e01\u56de\u62a5\u548cCOVID-19\u6307\u6807\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528GA\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982Mann-Whitney U\u68c0\u9a8c\uff09\u6bd4\u8f83\u6027\u80fd\u3002", "result": "COVID-19\u6307\u6807\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff08R2\u589e\u52a040%\uff0cRMSE\u964d\u4f4e2%\uff09\uff0c\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u662f\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u516c\u5171\u536b\u751f\u4fe1\u53f7\u6269\u5c55\u4e86\u91d1\u878d\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5728\u7cfb\u7edf\u6027\u5371\u673a\u4e2d\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5e02\u573a\u6307\u6807\u3002"}}
{"id": "2508.00255", "pdf": "https://arxiv.org/pdf/2508.00255", "abs": "https://arxiv.org/abs/2508.00255", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62bd\u8c61-\u5177\u4f53\u5316\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u591a\u4e2a\u5019\u9009\u8f93\u51fa\uff0c\u901a\u8fc7\u805a\u5408\u548c\u4f18\u5316\u751f\u6210\u66f4\u4e00\u81f4\u3001\u51c6\u786e\u7684\u56fe\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u56fe\u6a21\u578b\u65f6\u7684\u8bed\u6cd5\u8fdd\u89c4\u3001\u7ea6\u675f\u4e0d\u4e00\u81f4\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u6784\u5efa\u6982\u7387\u90e8\u5206\u6a21\u578b\u805a\u5408\u5019\u9009\u8f93\u51fa\uff0c\u518d\u4f18\u5316\u4e3a\u6ee1\u8db3\u7ea6\u675f\u7684\u5177\u4f53\u6a21\u578b\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u751f\u6210\u56fe\u6a21\u578b\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00098", "pdf": "https://arxiv.org/pdf/2508.00098", "abs": "https://arxiv.org/abs/2508.00098", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "title": "Stress-Aware Resilient Neural Training", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStress-Aware Learning\u7684\u5f39\u6027\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f18\u5316\u884c\u4e3a\u6765\u63d0\u5347\u6a21\u578b\u5728\u7a33\u5b9a\u6216\u4e0d\u786e\u5b9a\u8bad\u7ec3\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u7ed3\u6784\u75b2\u52b3\u6982\u5ff5\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u4f18\u5316\u505c\u6ede\u95ee\u9898\u3002", "method": "\u63d0\u51faPlastic Deformation Optimizer\uff0c\u901a\u8fc7\u5185\u90e8\u5e94\u529b\u4fe1\u53f7\u68c0\u6d4b\u4f18\u5316\u56f0\u96be\uff0c\u5e76\u6ce8\u5165\u81ea\u9002\u5e94\u566a\u58f0\u4ee5\u9003\u79bb\u5c16\u9510\u6781\u5c0f\u503c\u3002", "result": "\u5728\u516d\u79cd\u67b6\u6784\u3001\u56db\u79cd\u4f18\u5316\u5668\u548c\u4e03\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "Stress-Aware Learning\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f18\u5316\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00408", "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "categories": ["cs.SE", "cs.CL"], "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ULT\uff08UnLeakedTestbench\uff09\u57fa\u51c6\uff0c\u7528\u4e8e\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6c61\u67d3\u548c\u7ed3\u6784\u7b80\u5355\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u751f\u6210\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u7ed3\u6784\u7b80\u5355\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u79d1\u5b66\u7ed3\u8bba\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u8fc7\u7a0b\u6784\u5efaULT\uff0c\u5305\u542b3,909\u4e2a\u9ad8\u590d\u6742\u5ea6\u7684Python\u51fd\u6570\u4efb\u52a1\uff0c\u5e76\u5f15\u5165PLT\u4f5c\u4e3a\u5bf9\u7167\u57fa\u51c6\u3002", "result": "ULT\u4e0a\u7684\u6d4b\u8bd5\u751f\u6210\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u57fa\u51c6\uff08\u5982TestEval\u548cPLT\uff09\uff0c\u8868\u660e\u5176\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "ULT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8861\u91cfLLM\u7684\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2508.00256", "pdf": "https://arxiv.org/pdf/2508.00256", "abs": "https://arxiv.org/abs/2508.00256", "authors": ["Chuang Zhang", "Geng Sun", "Jiacheng Wang", "Yijing Lin", "Weijie Yuan", "Sinem Coleri", "Dusit Niyato", "Tony Q. S. Quek"], "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study", "categories": ["cs.NI", "cs.AI"], "comment": "This paper has been submitted to IEEE Communications Magazine for\n  consideration", "summary": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u7684\u5b89\u5168\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAMs\uff09\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u72b6\u6001\u7279\u5f81\u548c\u8bbe\u8ba1\u5185\u5728\u5956\u52b1\u6765\u63d0\u5347\u5b89\u5168\u901a\u4fe1\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "LAWNs\u56e0\u5176\u4f4e\u7a7a\u64cd\u4f5c\u3001\u9891\u7e41\u79fb\u52a8\u548c\u4f9d\u8d56\u975e\u6388\u6743\u9891\u8c31\u7b49\u7279\u70b9\uff0c\u9762\u4e34\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u4f20\u7edfAI\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLAM\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u589e\u5f3a\u72b6\u6001\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u5185\u5728\u5956\u52b1\uff0c\u4ee5\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u5176\u5728\u63d0\u5347\u5b89\u5168\u901a\u4fe1\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "LAMs\u5728LAWNs\u5b89\u5168\u901a\u4fe1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2508.00117", "pdf": "https://arxiv.org/pdf/2508.00117", "abs": "https://arxiv.org/abs/2508.00117", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStackLiverNet\uff0c\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u809d\u810f\u75be\u75c5\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u7c7b\u9519\u8bef\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u809d\u810f\u75be\u75c5\u8bca\u65ad\u9700\u8981\u7cbe\u786e\u4e14\u53ca\u65f6\u7684\u65b9\u6cd5\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u9ad8\u8bef\u5206\u7c7b\u7387\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u9ad8\u7ea7\u6570\u636e\u9884\u5904\u7406\u548c\u7279\u5f81\u9009\u62e9\u6280\u672f\uff0c\u7ed3\u5408\u968f\u673a\u6b20\u91c7\u6837\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u6784\u5efa\u57fa\u4e8eLightGBM\u5143\u6a21\u578b\u7684\u5806\u53e0\u96c6\u6210\u6a21\u578b\u3002", "result": "\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe99.89%\uff0cCohen Kappa\u4e3a0.9974\uff0cAUC\u4e3a0.9993\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "StackLiverNet\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u3002"}}
{"id": "2508.00462", "pdf": "https://arxiv.org/pdf/2508.00462", "abs": "https://arxiv.org/abs/2508.00462", "authors": ["Linus Ververs", "Lutz Prechelt"], "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "categories": ["cs.SE"], "comment": null, "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5de5\u4e1a\u73af\u5883\u4e2d\u7ed3\u5bf9\u7f16\u7a0b\u4e2d\u7684\u6743\u529b\u5dee\u8ddd\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u907f\u514d\u6743\u529b\u5dee\u8ddd\u7684\u5efa\u8bae\u3002", "motivation": "\u7406\u89e3\u7ed3\u5bf9\u7f16\u7a0b\u4e2d\u6743\u529b\u76f8\u5173\u73b0\u8c61\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u5206\u6790\u4e8622\u4e2a\u5de5\u4e1a\u7ed3\u5bf9\u7f16\u7a0b\u4f1a\u8bdd\uff0c\u4f7f\u7528\u624e\u6839\u7406\u8bba\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7292\u540d\u53c2\u4e0e\u8005\u7684\u8c03\u67e5\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u63d0\u51fa\u4e86\u6743\u529b\u5dee\u8ddd\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u5176\u8d1f\u9762\u5f71\u54cd\uff0c\u8c03\u67e5\u8bc1\u5b9e\u7406\u8bba\u6982\u5ff5\u5728\u5b9e\u8df5\u4e2d\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u907f\u514d\u6743\u529b\u5dee\u8ddd\u662f\u7ed3\u5bf9\u7f16\u7a0b\u7684\u91cd\u8981\u6280\u80fd\uff0c\u9700\u51cf\u5c11\u7b49\u7ea7\u884c\u4e3a\uff0c\u589e\u52a0\u5e73\u7b49\u884c\u4e3a\u3002"}}
{"id": "2508.00261", "pdf": "https://arxiv.org/pdf/2508.00261", "abs": "https://arxiv.org/abs/2508.00261", "authors": ["Saichao Liu", "Geng Sun", "Chuang Zhang", "Xuejie Liu", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "title": "Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning", "categories": ["cs.NI", "eess.SP"], "comment": "This paper has been accepted by IEEE GLOBECOM 2025", "summary": "Mobile edge computing (MEC) is a promising technique to improve the\ncomputational capacity of smart devices (SDs) in Internet of Things (IoT).\nHowever, the performance of MEC is restricted due to its fixed location and\nlimited service scope. Hence, we investigate an unmanned aerial vehicle\n(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can\nsimultaneously provide computing service for multiple SDs. To improve the\nperformance of system, we formulated a UAV-based trajectory control and\nresource allocation multi-objective optimization problem (TCRAMOP) to\nsimultaneously maximize the offloading number of UAVs and minimize total\noffloading delay and total energy consumption of UAVs by optimizing the flight\npaths of UAVs as well as the computing resource allocated to served SDs. Then,\nconsider that the solution of TCRAMOP requires continuous decision-making and\nthe system is dynamic, we propose an enhanced deep reinforcement learning (DRL)\nalgorithm, namely, distributed proximal policy optimization with imitation\nlearning (DPPOIL). This algorithm incorporates the generative adversarial\nimitation learning technique to improve the policy performance. Simulation\nresults demonstrate the effectiveness of our proposed DPPOIL and prove that the\nlearned strategy of DPPOIL is better compared with other baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u8f85\u52a9\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002\u91c7\u7528\u589e\u5f3a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5DPPOIL\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u56e0\u56fa\u5b9a\u4f4d\u7f6e\u548c\u670d\u52a1\u8303\u56f4\u53d7\u9650\uff0c\u6027\u80fd\u53d7\u9650\u3002\u65e0\u4eba\u673a\u8f85\u52a9MEC\u7cfb\u7edf\u53ef\u6269\u5c55\u670d\u52a1\u8303\u56f4\uff0c\u63d0\u5347\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u65e0\u4eba\u673a\u8f68\u8ff9\u63a7\u5236\u548c\u8d44\u6e90\u5206\u914d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff08TCRAMOP\uff09\uff0c\u5e76\u8bbe\u8ba1\u589e\u5f3a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5DPPOIL\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u6280\u672f\u4f18\u5316\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cDPPOIL\u7b97\u6cd5\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6700\u5927\u5316\u5378\u8f7d\u6570\u91cf\u5e76\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "conclusion": "\u65e0\u4eba\u673a\u8f85\u52a9MEC\u7cfb\u7edf\u7ed3\u5408DPPOIL\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00127", "pdf": "https://arxiv.org/pdf/2508.00127", "abs": "https://arxiv.org/abs/2508.00127", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Structured Transformations for Stable and Interpretable Neural Computation", "categories": ["cs.LG"], "comment": null, "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u53d8\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u6821\u6b63\u7ec4\u4ef6\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u4ee3\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u7ed3\u6784\u4fdd\u969c\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u5c06\u5c42\u53d8\u6362\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u6821\u6b63\u7ec4\u4ef6\uff0c\u4f18\u5316\u4fe1\u53f7\u4f20\u64ad\u548c\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6539\u5584\u4e86\u68af\u5ea6\u6761\u4ef6\u3001\u964d\u4f4e\u6270\u52a8\u654f\u611f\u6027\uff0c\u5e76\u589e\u5f3a\u5c42\u95f4\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u66f4\u7a33\u5b9a\u3001\u900f\u660e\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2508.00508", "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "AI": {"tldr": "Desyan\u662f\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u65e0\u7f1d\u6574\u5408\u4e86\u503c\u6d41\u5206\u6790\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u7a0b\u5e8f\u5206\u6790\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u503c\u6d41\u5206\u6790\u548c\u7b26\u53f7\u5206\u6790\u6280\u672f\u5404\u81ea\u72ec\u7acb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5e73\u53f0\u5b9e\u73b0\u9ad8\u6548\u96c6\u6210\u3002", "method": "Desyan\u6269\u5c55\u4e86Datalog\u5f15\u64ce\uff08Souffl\u00e9\uff09\uff0c\u96c6\u6210SMT\u6c42\u89e3\u5668\uff0c\u652f\u6301\u81ea\u52a8\u5904\u7406\u5178\u578b\u7a0b\u5e8f\u5206\u6790\u6a21\u5f0f\u3002", "result": "Desyan\u5728\u503c\u6d41\u5206\u6790\u4e2d\u6027\u80fd\u9886\u5148\uff08\u901f\u5ea6\u63d0\u534720\u500d\uff09\uff0c\u5728\u7b26\u53f7\u63a8\u7406\u4e2d\u652f\u6301\u591a\u79cd\u6c42\u89e3\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\uff08\u901f\u5ea6\u63d0\u53472\u500d\uff09\u3002", "conclusion": "Desyan\u6210\u529f\u586b\u8865\u4e86\u503c\u6d41\u4e0e\u7b26\u53f7\u5206\u6790\u95f4\u7684\u6280\u672f\u9e3f\u6c9f\uff0c\u4e3a\u7a0b\u5e8f\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00403", "pdf": "https://arxiv.org/pdf/2508.00403", "abs": "https://arxiv.org/abs/2508.00403", "authors": ["Rongsheng Zhang", "Ruichen Zhang", "Yang Lu", "Wei Chen", "Bo Ai", "Dusit Niyato"], "title": "Mamba for Wireless Communications and Networking: Principles and Opportunities", "categories": ["cs.NI"], "comment": null, "summary": "Mamba has emerged as a powerful model for efficiently addressing tasks\ninvolving temporal and spatial data. Regarding the escalating heterogeneity and\ndynamics in wireless networks, Mamba holds the potential to revolutionize\nwireless communication and networking designs by balancing the trade-off\nbetween computational efficiency and effectiveness. This article presents a\ncomprehensive overview of Mamba' applications in wireless systems.\nSpecifically, we first analyze the potentials of Mamba for wireless signal\nprocessing tasks from the perspectives of long-range dependency modeling and\nspatial feature extraction. Then we propose two application frameworks for\nMamba in wireless communications, i.e., replacement of traditional algorithms,\nand enabler of novel paradigms. Guided by the two frameworks, we conduct case\nstudies on intelligent resource allocation and joint source and channel\ndecoding to demonstrate Mamba's improvements in both feature enhancement and\ncomputational efficiency. Finally, we highlight critical challenges and outline\npotential research directions for Mamba in wireless communications and\nnetworking.", "AI": {"tldr": "Mamba\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u901a\u8fc7\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6548\u679c\uff0c\u9769\u65b0\u65e0\u7ebf\u7f51\u7edc\u8bbe\u8ba1\u3002\u6587\u7ae0\u7efc\u8ff0\u4e86Mamba\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e24\u4e2a\u6846\u67b6\u5e76\u9a8c\u8bc1\u5176\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6307\u51fa\u672a\u6765\u6311\u6218\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u65e5\u76ca\u589e\u52a0\uff0cMamba\u6a21\u578b\u6709\u671b\u901a\u8fc7\u9ad8\u6548\u5904\u7406\u65f6\u7a7a\u6570\u636e\uff0c\u4f18\u5316\u65e0\u7ebf\u901a\u4fe1\u8bbe\u8ba1\u3002", "method": "\u5206\u6790Mamba\u5728\u65e0\u7ebf\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e24\u4e2a\u5e94\u7528\u6846\u67b6\uff08\u66ff\u4ee3\u4f20\u7edf\u7b97\u6cd5\u548c\u542f\u7528\u65b0\u8303\u5f0f\uff09\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "Mamba\u5728\u7279\u5f81\u589e\u5f3a\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "Mamba\u4e3a\u65e0\u7ebf\u901a\u4fe1\u5e26\u6765\u65b0\u673a\u9047\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5173\u952e\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u503c\u5f97\u63a2\u7d22\u3002"}}
{"id": "2508.00131", "pdf": "https://arxiv.org/pdf/2508.00131", "abs": "https://arxiv.org/abs/2508.00131", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "categories": ["cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7PCA\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7b80\u5316\u5fc3\u7535\u56fe\uff08ECG\uff09\u6570\u636e\uff0c\u63d0\u51fa\u4e09\u79cd\u65b0\u578bVAE\u53d8\u4f53\uff0c\u5728\u4fe1\u53f7\u91cd\u5efa\u548c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "ECG\u4fe1\u53f7\u590d\u6742\u5ea6\u9ad8\u4e14\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6548\u679c\u6709\u9650\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528PCA\u548c\u81ea\u7f16\u7801\u5668\uff08\u5305\u62ec\u4e09\u79cd\u65b0\u578bVAE\u53d8\u4f53\uff1aSAE\u3001A beta-VAE\u548cC beta-VAE\uff09\u7b80\u5316ECG\u6570\u636e\uff0c\u5e76\u7ed3\u5408LGBM\u5206\u7c7b\u5668\u8fdb\u884c\u9884\u6d4b\u4efb\u52a1\u3002", "result": "A beta-VAE\u5728\u4fe1\u53f7\u91cd\u5efa\u4e2d\u8868\u73b0\u6700\u4f73\uff08MAE\u4e3a15.7\u00b13.2 \u03bcV\uff09\uff0cSAE\u7f16\u7801\u7ed3\u5408\u4f20\u7edf\u7279\u5f81\u63d0\u5347\u4e86LVEF\u9884\u6d4b\uff08AUROC\u4e3a0.901\uff09\uff0c\u63a5\u8fd1CNN\u6a21\u578b\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u65b0\u578bVAE\u7f16\u7801\u4e0d\u4ec5\u7b80\u5316\u4e86ECG\u6570\u636e\uff0c\u8fd8\u4e3a\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00546", "pdf": "https://arxiv.org/pdf/2508.00546", "abs": "https://arxiv.org/abs/2508.00546", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPENCER\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u4ee5\u63d0\u9ad8\u4ee3\u7801\u68c0\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u6280\u672f\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u4ee3\u7801\u68c0\u7d22\u4efb\u52a1\u4e2d\u56e0\u7f3a\u4e4f\u5e95\u5c42\u4ea4\u4e92\u800c\u6027\u80fd\u53d7\u9650\uff0c\u9700\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6548\u679c\u3002", "method": "SPENCER\u6846\u67b6\u5148\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u518d\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u63d0\u5347\u51c6\u786e\u6027\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u6a21\u578b\u84b8\u998f\u6280\u672f\u548c\u52a9\u6559\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53cc\u7f16\u7801\u5668\u4e0e\u4ea4\u53c9\u7f16\u7801\u5668\u7ed3\u5408\u4f18\u4e8e\u5355\u4e00\u53cc\u7f16\u7801\u5668\u6a21\u578b\uff1b\u6a21\u578b\u84b8\u998f\u6280\u672f\u51cf\u5c1170%\u63a8\u7406\u65f6\u95f4\uff0c\u6027\u80fd\u4fdd\u755998%\u4ee5\u4e0a\u3002", "conclusion": "SPENCER\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u68c0\u7d22\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u6a21\u578b\u84b8\u998f\u6280\u672f\u663e\u8457\u4f18\u5316\u4e86\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2508.00583", "pdf": "https://arxiv.org/pdf/2508.00583", "abs": "https://arxiv.org/abs/2508.00583", "authors": ["Yunting Xu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Deepu Rajan", "Liang Yu", "Haibo Zhou", "Abbas Jamalipour", "Xianbin Wang"], "title": "Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications", "categories": ["cs.NI"], "comment": "7 pages, 6 figures", "summary": "Large vision models (LVMs) have emerged as a foundational paradigm in visual\nintelligence, achieving state-of-the-art performance across diverse visual\ntasks. Recent advances in LVMs have facilitated their integration into Internet\nof Things (IoT) scenarios, offering superior generalization and adaptability\nfor vision-assisted network optimization. In this paper, we first investigate\nthe functionalities and core architectures of LVMs, highlighting their\ncapabilities across classification, segmentation, generation, and multimodal\nvisual processing. We then explore a variety of LVM applications in wireless\ncommunications, covering representative tasks across the physical layer,\nnetwork layer, and application layer. Furthermore, given the substantial model\nsize of LVMs and the challenges of model retraining in wireless domains, we\npropose a progressive fine-tuning framework that incrementally adapts\npretrained LVMs for joint optimization of multiple IoT tasks. A case study in\nlow-altitude economy networks (LAENets) demonstrates the effectiveness of the\nproposed framework over conventional CNNs in joint beamforming and positioning\ntasks for Internet of drones, underscoring a promising direction for\nintegrating LVMs into intelligent wireless systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u89c6\u89c9\u6a21\u578b\uff08LVMs\uff09\u5728\u89c6\u89c9\u667a\u80fd\u4e2d\u7684\u57fa\u7840\u4f5c\u7528\u53ca\u5176\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6846\u67b6\uff0c\u5e76\u5728\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "LVMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6a21\u578b\u5927\u5c0f\u548c\u91cd\u65b0\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9002\u5e94\u591a\u4efb\u52a1\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6846\u67b6\uff0c\u9010\u6b65\u8c03\u6574\u9884\u8bad\u7ec3\u7684LVMs\u4ee5\u9002\u5e94\u591a\u4efb\u52a1\u4f18\u5316\uff0c\u5e76\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\uff08LAENets\uff09\u4e2d\u8fdb\u884c\u4e86\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5728\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u8054\u5408\u6ce2\u675f\u6210\u5f62\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u6846\u67b6\u4f18\u4e8e\u4f20\u7edfCNNs\uff0c\u5c55\u793a\u4e86LVMs\u5728\u667a\u80fd\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "LVMs\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6846\u67b6\u4e3a\u591a\u4efb\u52a1\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00141", "pdf": "https://arxiv.org/pdf/2508.00141", "abs": "https://arxiv.org/abs/2508.00141", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "AI": {"tldr": "INSPIRE-GNN\u662f\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u81ea\u884c\u8f66\u6d41\u91cf\u4f20\u611f\u5668\u7684\u5e03\u5c40\u5e76\u63d0\u9ad8\u6570\u636e\u7a00\u758f\u73af\u5883\u4e0b\u7684\u6d41\u91cf\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u57ce\u5e02\u81ea\u884c\u8f66\u6d41\u91cf\u6570\u636e\u7a00\u758f\uff0c\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\uff0c\u5bfc\u81f4\u6d41\u91cf\u4f30\u8ba1\u56f0\u96be\u3002", "method": "\u7ed3\u5408GCN\u3001GAT\u548cDQN\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u6570\u636e\u9a71\u52a8\u5730\u9009\u62e9\u4f20\u611f\u5668\u4f4d\u7f6e\u3002", "result": "\u5728\u58a8\u5c14\u672c\u81ea\u884c\u8f66\u7f51\u7edc\u4e2d\uff0cINSPIRE-GNN\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86MSE\u3001RMSE\u548cMAE\u7b49\u6307\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u4f18\u5316\u4f20\u611f\u5668\u5e03\u5c40\u548c\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.00593", "pdf": "https://arxiv.org/pdf/2508.00593", "abs": "https://arxiv.org/abs/2508.00593", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "categories": ["cs.SE"], "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u53cd\u9988\u4e2d\u5927\u91cf\u4fe1\u606f\u4e0e\u7cfb\u7edf\u95ee\u9898\u65e0\u5173\uff0c\u9700\u8fc7\u6ee4\uff1b\u4e25\u91cd\u95ee\u9898\u96be\u4ee5\u4ec5\u901a\u8fc7\u53cd\u9988\u7279\u5f81\u68c0\u6d4b\uff1b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9002\u7528\u4e8e\u5206\u6790\u7528\u6237\u53cd\u9988\u3002", "motivation": "\u7406\u89e3\u7528\u6237\u53cd\u9988\u5728\u5927\u578b\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u7279\u6027\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u53cd\u9988\u7684\u95ee\u9898\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5bf9\u6765\u81ea6\u4e2a\u771f\u5b9e\u670d\u52a1\u768450,378,766\u6761\u7528\u6237\u53cd\u9988\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u5185\u5bb9\u3001\u7279\u5f81\u53ca\u673a\u5668\u5b66\u4e60\u9002\u7528\u6027\u3002", "result": "\u5927\u90e8\u5206\u53cd\u9988\u4e0e\u95ee\u9898\u65e0\u5173\uff1b\u4e25\u91cd\u95ee\u9898\u96be\u4ee5\u901a\u8fc7\u53cd\u9988\u7279\u5f81\u68c0\u6d4b\uff1b\u53cd\u9988\u4e3b\u9898\u5206\u5e03\u7a33\u5b9a\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u884c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5927\u89c4\u6a21\u670d\u52a1\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u53cd\u9988\u7684\u95ee\u9898\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u6307\u5bfc\u5b9e\u7528\u68c0\u6d4b\u65b9\u6cd5\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u3002"}}
{"id": "2508.00616", "pdf": "https://arxiv.org/pdf/2508.00616", "abs": "https://arxiv.org/abs/2508.00616", "authors": ["Mingzhe Fan", "Geng Sun", "Hongyang Pan", "Jiacheng Wang", "Jiancheng An", "Hongyang Du", "Chau Yuen"], "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications", "categories": ["cs.NI"], "comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u642d\u8f7d\u667a\u80fd\u8d85\u8868\u9762\uff08UAV-SIMs\uff09\u7684\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7528\u6237\u5173\u8054\u3001\u65e0\u4eba\u673a\u4f4d\u7f6e\u548c\u8d85\u8868\u9762\u76f8\u4f4d\uff0c\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\u3002", "motivation": "\u56fa\u5b9a\u8d85\u8868\u9762\u9650\u5236\u4e86\u901a\u4fe1\u6027\u80fd\uff0c\u800c\u79fb\u52a8\u8d85\u8868\u9762\uff08\u5982\u65e0\u4eba\u673a\u642d\u8f7d\uff09\u53ef\u4ee5\u7075\u6d3b\u90e8\u7f72\uff0c\u4ece\u800c\u589e\u5f3a\u6027\u80fd\u3002", "method": "\u5c06\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\uff08\u7528\u6237\u5173\u8054\u3001\u65e0\u4eba\u673a\u4f4d\u7f6e\u3001\u8d85\u8868\u9762\u76f8\u4f4d\uff09\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65e0\u4eba\u673a\u642d\u8f7d\u8d85\u8868\u9762\u53ca\u5176\u4f18\u5316\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.00161", "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u76d1\u63a7\u5fae\u8c03\u540e\u7684LLM\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u8f93\u5165\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u7684\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u548c\u9632\u5fa1\u65b0\u578b\u5a01\u80c1\uff08\u5982\u540e\u95e8\uff09\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u6743\u91cd\u5dee\u5f02\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\uff0c\u8bc6\u522b\u65b0\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u76d1\u63a7\u8fd9\u4e9b\u884c\u4e3a\u3002", "result": "\u5728\u540e\u95e8\u6a21\u578b\u4e2d\u963b\u6b62100%\u653b\u51fb\uff08\u5047\u9633\u6027\u7387<1.2%\uff09\uff0c\u5728\u9057\u5fd8\u6a21\u578b\u4e2d\u68c0\u6d4b\u9057\u5fd8\u4e3b\u9898\u51c6\u786e\u7387\u8fbe95.42%\uff0c\u5e76\u80fd\u6062\u590d\u201c\u9057\u5fd8\u201d\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u76d1\u63a7\u548c\u9884\u90e8\u7f72\u5ba1\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5546\u4e1a\u6a21\u578b\u5206\u6790\u3002"}}
{"id": "2508.00630", "pdf": "https://arxiv.org/pdf/2508.00630", "abs": "https://arxiv.org/abs/2508.00630", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "categories": ["cs.SE"], "comment": "MODELS 2025", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "AI": {"tldr": "MCeT\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u884c\u4e3a\u6a21\u578b\uff08\u5982\u5e8f\u5217\u56fe\uff09\u7684\u6b63\u786e\u6027\uff0c\u5e76\u751f\u6210\u95ee\u9898\u5217\u8868\u3002\u5b83\u5229\u7528LLM\u8fdb\u884c\u591a\u89d2\u5ea6\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u9898\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6570\u91cf\u3002", "motivation": "\u968f\u7740LLM\u5728AI\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u8bc4\u4f30\u6a21\u578b\u6b63\u786e\u6027\uff0c\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u53cd\u9988\u5e76\u5e2e\u52a9AI\u81ea\u6211\u6539\u8fdb\u3002", "method": "MCeT\u5c06\u56fe\u548c\u9700\u6c42\u5206\u89e3\u4e3a\u539f\u5b50\u5355\u5143\uff0c\u8fdb\u884c\u591a\u89d2\u5ea6\u6bd4\u8f83\uff0c\u5e76\u7ed3\u5408\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\u4ee5\u51cf\u5c11LLM\u5e7b\u89c9\u95ee\u9898\u3002", "result": "MCeT\u5c06\u76f4\u63a5\u65b9\u6cd5\u7684\u7cbe\u5ea6\u4ece0.58\u63d0\u5347\u81f30.81\uff0c\u68c0\u6d4b\u5230\u7684\u95ee\u9898\u6570\u91cf\u589e\u52a0\u4e8690%\uff0c\u5e73\u5747\u6bcf\u4e2a\u56fe\u53d1\u73b06\u4e2a\u65b0\u95ee\u9898\u3002", "conclusion": "MCeT\u901a\u8fc7\u591a\u89d2\u5ea6\u5206\u6790\u548c\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u6a21\u578b\u6b63\u786e\u6027\u8bc4\u4f30\u7684\u6548\u679c\u3002"}}
{"id": "2508.00629", "pdf": "https://arxiv.org/pdf/2508.00629", "abs": "https://arxiv.org/abs/2508.00629", "authors": ["Francisco Crespo", "Javier Villegas", "Carlos Baena", "Eduardo Baena", "Sergio Fortes", "Raquel Barco"], "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach", "categories": ["cs.NI", "cs.OS", "cs.PF"], "comment": null, "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u7f16\u7a0b\u7684\u5206\u5e03\u5f0f\u5e94\u7528\uff08dApp\uff09\uff0c\u7528\u4e8e\u5728O-RAN\u67b6\u6784\u4e0b\u52a8\u6001\u7ba1\u7406CPU\u8d44\u6e90\uff0c\u63d0\u5347\u80fd\u6548\u548c\u5229\u7528\u7387\uff0c\u65e0\u9700\u4fee\u6539\u5185\u6838\u6216\u4f9d\u8d56\u4e13\u6709\u8f6f\u4ef6\u3002", "motivation": "O-RAN\u7684\u8f6f\u786c\u4ef6\u89e3\u8026\u548c\u865a\u62df\u5316\u5e26\u6765\u4e86CPU\u8d44\u6e90\u7ba1\u7406\u7684\u6311\u6218\uff0c\u73b0\u6709\u8c03\u5ea6\u5668\u5728\u5b9e\u65f6\u6027\u8981\u6c42\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u80fd\u6548\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u90e8\u7f72\u5728DU\u5c42\u7684dApp\uff0c\u901a\u8fc7\u7ebf\u7a0b\u7ea7\u9065\u6d4b\u6570\u636e\uff08\u5982\u4e0a\u4e0b\u6587\u5207\u6362\u3001IPC\u3001\u7f13\u5b58\u6307\u6807\uff09\u5b9e\u65f6\u8c03\u6574CPU\u7ebf\u7a0b\u4eb2\u548c\u6027\u3001\u6838\u5fc3\u9694\u79bb\u548c\u9891\u7387\u7f29\u653e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u5546\u7528srsRAN\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8282\u80fd\u6548\u679c\uff0c\u4e14\u4e0d\u5f71\u54cd\u5b9e\u65f6\u5904\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5dApp\u5c55\u793a\u4e86\u5728\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u5e94\u7528\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8d44\u6e90\u63a7\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00172", "pdf": "https://arxiv.org/pdf/2508.00172", "abs": "https://arxiv.org/abs/2508.00172", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "categories": ["cs.LG", "eess.IV"], "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6DiSC-Med\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u4f20\u8f93\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u548c\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u8fdc\u7a0b\u533b\u7597\u7684\u9700\u6c42\uff0c\u4f46\u533b\u5b66\u6570\u636e\u5728\u6709\u9650\u5e26\u5bbd\u548c\u566a\u58f0\u4fe1\u9053\u4e2d\u7684\u9ad8\u6548\u4f20\u8f93\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u533b\u5b66\u589e\u5f3a\u7684\u538b\u7f29\u548c\u53bb\u566a\u6a21\u5757\uff0c\u901a\u8fc7\u6355\u6349\u5173\u952e\u8bed\u4e49\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u5e26\u5bbd\u5229\u7528\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiSC-Med\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u5b9e\u73b0\u4e86\u8d85\u9ad8\u5e26\u5bbd\u6548\u7387\u548c\u4f18\u5f02\u7684\u56fe\u50cf\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "DiSC-Med\u6846\u67b6\u4e3a\u8fdc\u7a0b\u533b\u7597\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00700", "pdf": "https://arxiv.org/pdf/2508.00700", "abs": "https://arxiv.org/abs/2508.00700", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "categories": ["cs.SE"], "comment": "Accepted ESEM2025", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86LLM\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u7684\u5185\u90e8\u8d28\u91cf\u5c5e\u6027\uff0c\u53d1\u73b0LLM\u4ee3\u7801\u6574\u4f53\u7f3a\u9677\u66f4\u5c11\u4e14\u4fee\u590d\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u53ef\u80fd\u5f15\u5165\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u8f6f\u4ef6\u8d28\u91cf\u8868\u73b0\u53ca\u5176\u4e0e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u7684\u5bf9\u6bd4\u3002", "method": "\u7ed3\u5408\u6570\u636e\u96c6\u3001\u4e09\u79cdLLM\u914d\u7f6e\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u5fae\u8c03\uff09\u548cSonarQube\u5de5\u5177\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\uff0c\u5206\u6790\u7ef4\u62a4\u6027\u3001\u53ef\u9760\u6027\u7b49\u6307\u6807\u3002", "result": "LLM\u751f\u6210\u4ee3\u7801\u7f3a\u9677\u8f83\u5c11\u4e14\u4fee\u590d\u6210\u672c\u4f4e\uff0c\u5fae\u8c03\u6a21\u578b\u51cf\u5c11\u9ad8\u4e25\u91cd\u6027\u95ee\u9898\u4f46\u964d\u4f4e\u6027\u80fd\uff1b\u590d\u6742\u95ee\u9898\u4e2d\u53ef\u80fd\u5f15\u5165\u7ed3\u6784\u6027\u95ee\u9898\u3002", "conclusion": "LLM\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u6709\u4f18\u52bf\uff0c\u4f46\u590d\u6742\u573a\u666f\u9700\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7814\u7a76\u52a0\u6df1\u4e86\u5bf9LLM\u4ee3\u7801\u751f\u6210\u4f18\u7f3a\u70b9\u7684\u7406\u89e3\u3002"}}
{"id": "2508.00688", "pdf": "https://arxiv.org/pdf/2508.00688", "abs": "https://arxiv.org/abs/2508.00688", "authors": ["Ruiyang Huang", "Haocheng Wang", "Yixuan Shen", "Ning Gao", "Qiang Ni", "Shi Jin", "Yifan Wu"], "title": "Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience", "categories": ["cs.NI", "eess.SP"], "comment": "Submit to INFOCOM 2026", "summary": "Heterogeneous marine-aerial swarm networks encounter substantial difficulties\ndue to targeted communication disruptions and structural weaknesses in\nadversarial environments. This paper proposes a two-step framework to\nstrengthen the network's resilience. Specifically, our framework combines the\nnode prioritization based on criticality with multi-objective topology\noptimization. First, we design a three-layer architecture to represent\nstructural, communication, and task dependencies of the swarm networks. Then,\nwe introduce the SurBi-Ranking method, which utilizes graph convolutional\nnetworks, to dynamically evaluate and rank the criticality of nodes and edges\nin real time. Next, we apply the NSGA-III algorithm to optimize the network\ntopology, aiming to balance communication efficiency, global connectivity, and\nmission success rate. Experiments demonstrate that compared to traditional\nmethods like K-Shell, our SurBi-Ranking method identifies critical nodes and\nedges with greater accuracy, as deliberate attacks on these components cause\nmore significant connectivity degradation. Furthermore, our optimization\napproach, when prioritizing SurBi-Ranked critical components under attack,\nreduces the natural connectivity degradation by around 30%, achieves higher\nmission success rates, and incurs lower communication reconfiguration costs,\nensuring sustained connectivity and mission effectiveness across multi-phase\noperations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u591a\u76ee\u6807\u62d3\u6251\u4f18\u5316\u589e\u5f3a\u5f02\u6784\u6d77\u7a7a\u7fa4\u7f51\u7edc\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u5f02\u6784\u6d77\u7a7a\u7fa4\u7f51\u7edc\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u6613\u53d7\u901a\u4fe1\u4e2d\u65ad\u548c\u7ed3\u6784\u5f31\u70b9\u5f71\u54cd\uff0c\u9700\u63d0\u5347\u5176\u97e7\u6027\u3002", "method": "\u8bbe\u8ba1\u4e09\u5c42\u67b6\u6784\u8868\u793a\u7f51\u7edc\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u51faSurBi-Ranking\u65b9\u6cd5\u52a8\u6001\u8bc4\u4f30\u8282\u70b9\u5173\u952e\u6027\uff0c\u5e76\u5e94\u7528NSGA-III\u7b97\u6cd5\u4f18\u5316\u62d3\u6251\u3002", "result": "SurBi-Ranking\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u8bc6\u522b\u5173\u952e\u8282\u70b9\uff0c\u4f18\u5316\u540e\u7f51\u7edc\u5728\u653b\u51fb\u4e0b\u8fde\u63a5\u6027\u4e0b\u964d\u51cf\u5c1130%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u6301\u7eed\u8fde\u63a5\u6027\u548c\u4efb\u52a1\u6709\u6548\u6027\u3002"}}
{"id": "2508.00174", "pdf": "https://arxiv.org/pdf/2508.00174", "abs": "https://arxiv.org/abs/2508.00174", "authors": ["Yongchao Huang"], "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "categories": ["cs.LG"], "comment": "7 pages", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u56de\u5f52\u95ee\u9898\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\u548cRL\u7b97\u6cd5\u5b9e\u73b0\u51fd\u6570\u903c\u8fd1\uff0c\u5c55\u793a\u4e86\u5728\u566a\u58f0\u6b63\u5f26\u6ce2\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff08\u5982\u5747\u65b9\u8bef\u5dee\uff09\uff0c\u96be\u4ee5\u5904\u7406\u975e\u5bf9\u79f0\u6210\u672c\u6216\u590d\u6742\u3001\u4e0d\u53ef\u5fae\u76ee\u6807\u3002", "method": "\u5c06\u6a21\u578b\u9884\u6d4b\u89c6\u4e3a\u52a8\u4f5c\uff0c\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u5b9a\u4e49\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u5229\u7528Actor-Critic\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u4f18\u5148\u7ea7\u7ecf\u9a8c\u56de\u653e\u3001\u589e\u52a0\u7f51\u7edc\u5bb9\u91cf\u548c\u4f4d\u7f6e\u7f16\u7801\u7b49\u6280\u672f\u3002", "result": "RL\u6846\u67b6\u4e0d\u4ec5\u6210\u529f\u89e3\u51b3\u4e86\u56de\u5f52\u95ee\u9898\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5b9a\u4e49\u76ee\u6807\u548c\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u7684\u66f4\u5927\u7075\u6d3b\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u8303\u5f0f\u3002"}}
{"id": "2508.00738", "pdf": "https://arxiv.org/pdf/2508.00738", "abs": "https://arxiv.org/abs/2508.00738", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber", "Valdes Voufo"], "title": "Tool-Assisted Conformance Checking to Reference Process Models", "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "comment": null, "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u4f9d\u8d56\u5206\u6790\u7684\u81ea\u52a8\u5316\u4e00\u81f4\u6027\u68c0\u67e5\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5177\u4f53\u6d41\u7a0b\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u81f4\u6027\u68c0\u67e5\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u6a21\u578b\u6bd4\u8f83\u7684\u8868\u8fbe\u80fd\u529b\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u56e0\u679c\u4f9d\u8d56\u5206\u6790\u4efb\u52a1\u548c\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u5230\u8bed\u4e49\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\u8fdb\u884c\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u6d41\u7a0b\u6a21\u578b\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5de5\u5177\u8f85\u52a9\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u6d41\u7a0b\u6a21\u578b\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u80fd\u529b\u3002"}}
{"id": "2508.00715", "pdf": "https://arxiv.org/pdf/2508.00715", "abs": "https://arxiv.org/abs/2508.00715", "authors": ["Olga Kondrateva", "Grace Li Zhang", "Julian Zobel", "Bj\u00f6rn Scheuermann", "Stefan Dietzel"], "title": "Deep Joint Source-Channel Coding for Small Satellite Applications", "categories": ["cs.NI"], "comment": null, "summary": "Small satellites used for Earth observation generate vast amounts of\nhigh-dimensional data, but their operation in low Earth orbit creates a\nsignificant communication bottleneck due to limited contact times and harsh,\nvarying channel conditions. While deep joint source-channel coding (DJSCC) has\nemerged as a promising technique, its practical application to the complex\nsatellite environment remains an open question. This paper presents a\ncomprehensive DJSCC framework tailored for satellite communications. We first\nestablish a basic system, DJSCC-SAT, and integrate a realistic, multi-state\nstatistical channel model to guide its training and evaluation. To overcome the\nimpracticality of using separate models for every channel condition, we then\nintroduce an adaptable architecture, ADJSCC-SAT, which leverages attention\nmodules to allow a single neural network to adjust to a wide range of channel\nstates with minimal overhead. Through extensive evaluation on Sentinel-2\nmulti-spectral data, we demonstrate that our adaptable approach achieves\nperformance comparable to using multiple specialized networks while\nsignificantly reducing model storage requirements. Furthermore, the adaptable\nmodel shows enhanced robustness to channel estimation errors, outperforming the\nnon-adaptable baseline. The proposed framework is a practical and efficient\nstep toward deploying robust, adaptive DJSCC systems for real-world satellite\nmissions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u536b\u661f\u901a\u4fe1\u7684\u6df1\u5ea6\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\uff08DJSCC\uff09\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7840\u7cfb\u7edfDJSCC-SAT\u548c\u81ea\u9002\u5e94\u67b6\u6784ADJSCC-SAT\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5b58\u50a8\u9700\u6c42\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5c0f\u536b\u661f\u5728\u4f4e\u5730\u7403\u8f68\u9053\u4e2d\u56e0\u901a\u4fe1\u74f6\u9888\u5bfc\u81f4\u7684\u9ad8\u7ef4\u6570\u636e\u4f20\u8f93\u95ee\u9898\uff0c\u63a2\u7d22DJSCC\u5728\u590d\u6742\u536b\u661f\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5efa\u7acbDJSCC-SAT\u7cfb\u7edf\uff0c\u5f15\u5165\u591a\u72b6\u6001\u7edf\u8ba1\u4fe1\u9053\u6a21\u578b\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u67b6\u6784ADJSCC-SAT\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5757\u9002\u5e94\u591a\u79cd\u4fe1\u9053\u72b6\u6001\u3002", "result": "\u5728Sentinel-2\u591a\u5149\u8c31\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u6027\u80fd\u63a5\u8fd1\u591a\u4e13\u7528\u7f51\u7edc\uff0c\u4e14\u5b58\u50a8\u9700\u6c42\u66f4\u4f4e\uff0c\u5bf9\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u9645\u536b\u661f\u4efb\u52a1\u4e2d\u90e8\u7f72\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684DJSCC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00180", "pdf": "https://arxiv.org/pdf/2508.00180", "abs": "https://arxiv.org/abs/2508.00180", "authors": ["Adam Block", "Cyril Zhang"], "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBEMA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63EMA\u4e2d\u7684\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u968f\u673a\u6027\uff08\u901a\u5e38\u7531\u5c0f\u6279\u91cf\u8bad\u7ec3\u5f15\u8d77\uff09\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u800c\u4f20\u7edf\u7684EMA\u65b9\u6cd5\u867d\u80fd\u51cf\u5c11\u968f\u673a\u6027\uff0c\u4f46\u4f1a\u5f15\u5165\u504f\u5dee\u3002", "method": "\u63d0\u51faBEMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63EMA\u4e2d\u7684\u504f\u5dee\uff0c\u4fdd\u7559\u65b9\u5dee\u51cf\u5c11\u7684\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBEMA\u5728\u591a\u79cd\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "BEMA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u9ad8\u5fae\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.00749", "pdf": "https://arxiv.org/pdf/2508.00749", "abs": "https://arxiv.org/abs/2508.00749", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber"], "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "comment": null, "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u6001\u7b26\u53f7\u6267\u884c\uff08DSE\uff09\u5728\u7ec4\u4ef6-\u8fde\u63a5\u5668\u67b6\u6784\u8bed\u4e49\u5dee\u5f02\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u589e\u5f3aMontiArc-to-Java\u751f\u6210\u5668\u6536\u96c6\u8fd0\u884c\u65f6\u6570\u636e\uff0c\u8bc4\u4f30\u4e86DSE\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u5176\u6f5c\u529b\u4f46\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "motivation": "\u5728\u6a21\u578b\u9a71\u52a8\u5f00\u53d1\u4e2d\uff0c\u786e\u4fdd\u6a21\u578b\u6b63\u786e\u6027\u548c\u4e00\u81f4\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u6790\u8bed\u4e49\u5dee\u5f02\u3002", "method": "\u589e\u5f3aMontiArc-to-Java\u751f\u6210\u5668\uff0c\u6536\u96c6\u7b26\u53f7\u548c\u5177\u4f53\u6267\u884c\u6570\u636e\uff0c\u8bc4\u4f30\u4e0d\u540c\u6267\u884c\u7b56\u7565\u7684\u6548\u7387\u3001\u6700\u5c0f\u6027\u548c\u5b8c\u6574\u6027\u3002", "result": "DSE\u5728\u7ec4\u4ef6-\u8fde\u63a5\u5668\u67b6\u6784\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u53ef\u6269\u5c55\u6027\u662f\u5176\u4e3b\u8981\u9650\u5236\u3002", "conclusion": "DSE\u9002\u7528\u4e8e\u8bed\u4e49\u5dee\u5f02\u5206\u6790\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u5728\u5927\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00735", "pdf": "https://arxiv.org/pdf/2508.00735", "abs": "https://arxiv.org/abs/2508.00735", "authors": ["Lucas Aubard", "Johan Mazel", "Gilles Guette", "Pierre Chifflier"], "title": "Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE", "categories": ["cs.NI"], "comment": null, "summary": "IP fragmentation and TCP segmentation allow for splitting large data packets\ninto smaller ones, e.g., for transmission across network links of limited\ncapacity. These mechanisms permit complete or partial overlaps with different\ndata on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,\ni.e., the data chunk preferences that depend on the overlap types, differ\nacross protocol implementations. This leads to vulnerabilities, as NIDSes may\ninterpret the packet differently from the monitored host OSes. Some NIDSes,\nsuch as Suricata or Snort, can be configured so that their policies are\nconsistent with the monitored OSes. The first contribution of the paper is\nPYROLYSE, an audit tool that exhaustively tests and describes the reassembly\npolicies of various IP and TCP implementation types. This tool ensures that\nimplementations reassemble overlapping chunk sequences without errors. The\nsecond contribution is the analysis of PYROLYSE artifacts. We first show that\nthe reassembly policies are much more diverse than previously thought. Indeed,\nby testing all the overlap possibilities for n <= 3 test case chunks and\ndifferent testing scenarios, we observe from 14 to 20 different behaviors out\nof 23 tested implementations depending on the protocol. Second, we report eight\nerrors impacting one OS, two NIDSes, and two embedded stacks, which can lead to\nsecurity issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was\nassigned to a NIDS error. Finally, we show that implemented IP and TCP policies\nobtained through chunk pair testing are usually inconsistent with the observed\ntriplet reassemblies. Therefore, contrarily to what they currently do, NIDSes\nor other network traffic analysis tools should not apply n = 2 pair policies\nwhen the number of overlapping chunks exceeds two.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PYROLYSE\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u8bd5IP\u548cTCP\u91cd\u7ec4\u7b56\u7565\u7684\u591a\u6837\u6027\uff0c\u5e76\u53d1\u73b0\u5176\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u590d\u6742\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u591a\u4e2a\u5b9e\u73b0\u4e2d\u7684\u9519\u8bef\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3IP\u548cTCP\u91cd\u7ec4\u7b56\u7565\u5728\u4e0d\u540c\u5b9e\u73b0\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u7531\u6b64\u5bfc\u81f4\u7684NIDS\u4e0e\u4e3b\u673aOS\u89e3\u91ca\u5dee\u5f02\u5f15\u53d1\u7684\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528PYROLYSE\u5de5\u5177\u5bf923\u79cd\u5b9e\u73b0\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u91cd\u7ec4\u7b56\u7565\u7684\u591a\u6837\u6027\uff0c\u5e76\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002", "result": "\u53d1\u73b014\u81f320\u79cd\u4e0d\u540c\u7684\u91cd\u7ec4\u884c\u4e3a\uff0c\u62a5\u544a\u4e868\u4e2a\u9519\u8bef\uff0c\u5305\u62ec\u5b89\u5168\u6f0f\u6d1e\uff08\u5982NIDS\u7ed5\u8fc7\u6216DoS\u653b\u51fb\uff09\u3002", "conclusion": "\u5efa\u8baeNIDS\u7b49\u5de5\u5177\u4e0d\u5e94\u4ec5\u57fa\u4e8e\u53cc\u5757\u6d4b\u8bd5\u7b56\u7565\u5904\u7406\u591a\u5757\u91cd\u53e0\u60c5\u51b5\uff0c\u4ee5\u907f\u514d\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2508.00201", "pdf": "https://arxiv.org/pdf/2508.00201", "abs": "https://arxiv.org/abs/2508.00201", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "categories": ["cs.LG"], "comment": null, "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "AI": {"tldr": "RecoMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f1a\u8bdd\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u96be\u4ee5\u4f18\u5316\u957f\u671f\u76ee\u6807\uff08\u5982\u4f1a\u8bdd\u53c2\u4e0e\u5ea6\uff09\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u9762\u4e34\u52a8\u4f5c\u7a7a\u95f4\u5927\u548c\u5de5\u7a0b\u590d\u6742\u6027\u7684\u6311\u6218\u3002", "method": "RecoMind\u5229\u7528\u73b0\u6709\u63a8\u8350\u6a21\u578b\u6784\u5efa\u6a21\u62df\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u63a2\u7d22\u7b56\u7565\u9ad8\u6548\u63a2\u7d22\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7b80\u5316RL\u7b56\u7565\u7684\u8bad\u7ec3\u4e0e\u90e8\u7f72\u3002", "result": "\u79bb\u7ebf\u6a21\u62df\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cRecoMind\u8bad\u7ec3\u7684RL\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f1a\u8bdd\u6df1\u5ea6\u548c\u89c2\u770b\u65f6\u957f\u5206\u522b\u63d0\u53474.71%\u548c15.81%\u3002", "conclusion": "RecoMind\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684RL\u5d4c\u5165\u65b9\u6cd5\uff0c\u6709\u6548\u4f18\u5316\u4f1a\u8bdd\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2508.00772", "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790Codeforces\u7528\u6237\u7684\u7f16\u7a0b\u7ade\u8d5b\u8868\u73b0\uff0c\u9884\u6d4b\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5c31\u4e1a\u6f5c\u529b\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5c06\u7528\u6237\u5206\u4e3a\u56db\u4e2a\u5c31\u4e1a\u80fd\u529b\u7b49\u7ea7\u3002", "motivation": "\u5feb\u901f\u53d1\u5c55\u7684\u79d1\u6280\u884c\u4e1a\u9700\u8981\u8bc4\u4f30\u7a0b\u5e8f\u5458\u5c31\u4e1a\u6f5c\u529b\u7684\u5de5\u5177\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7f16\u7a0b\u7ade\u8d5b\u8868\u73b0\u4e0e\u5c31\u4e1a\u673a\u4f1a\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "\u901a\u8fc7Codeforces API\u6536\u96c6\u7528\u6237\u6570\u636e\uff0c\u5904\u7406\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u7cfb\u7edf\u901a\u8fc7Flask\u90e8\u7f72\u5728Render\u4e0a\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\uff0c\u57fa\u4e8e\u7f16\u7a0b\u719f\u7ec3\u5ea6\u548c\u53c2\u4e0e\u5ea6\u5c06\u7528\u6237\u5206\u7c7b\u4e3a\u56db\u4e2a\u5c31\u4e1a\u80fd\u529b\u7b49\u7ea7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u5b66\u4e60\u5728\u804c\u4e1a\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u6280\u672f\u9886\u57df\u7684\u5c31\u4e1a\u80fd\u529b\u9884\u6d4b\u3002"}}
{"id": "2508.00792", "pdf": "https://arxiv.org/pdf/2508.00792", "abs": "https://arxiv.org/abs/2508.00792", "authors": ["Aashay Arora", "Diego Davila", "Jonathan Guiang", "Frank W\u00fcrthwein", "Harvey Newman", "Justas Balcas", "Tom Lehman", "Xi Yang"], "title": "Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype", "categories": ["cs.NI"], "comment": "Submitted to CHEP 24", "summary": "The Data Movement Manager (DMM) is a prototype interface that connects CERN's\ndata management software, Rucio, with the Sofware-Defined Networking (SDN)\nservice SENSE by ESNet. It enables SDN-enabled high-energy physics data flows\nusing the existing worldwide LHC computing grid infrastructure. A key feature\nof DMM is transfer priority-based bandwidth allocation, optimizing network\nusage. Additionally, it provides fine-grained monitoring of underperforming\nflows by leveraging end-to-end data flow monitoring. This is achieved through\naccess to host-level (network interface) throughput metrics and transfer-tool\n(FTS) data transfer job-level metrics. This paper details the design and\nimplementation of DMM.", "AI": {"tldr": "DMM\u662f\u4e00\u4e2a\u539f\u578b\u63a5\u53e3\uff0c\u8fde\u63a5CERN\u7684\u6570\u636e\u7ba1\u7406\u8f6f\u4ef6Rucio\u4e0eSDN\u670d\u52a1SENSE\uff0c\u4f18\u5316\u9ad8\u80fd\u7269\u7406\u6570\u636e\u4f20\u8f93\u7684\u5e26\u5bbd\u5206\u914d\u548c\u76d1\u63a7\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408Rucio\u548cSENSE\uff0c\u4f18\u5316\u73b0\u6709\u5168\u7403LHC\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u7f51\u7edc\u4f7f\u7528\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0DMM\uff0c\u652f\u6301\u57fa\u4e8e\u4f20\u8f93\u4f18\u5148\u7ea7\u7684\u5e26\u5bbd\u5206\u914d\u548c\u7ec6\u7c92\u5ea6\u76d1\u63a7\u3002", "result": "DMM\u6210\u529f\u5b9e\u73b0\u4e86\u7f51\u7edc\u4f18\u5316\u548c\u7aef\u5230\u7aef\u6570\u636e\u6d41\u76d1\u63a7\u3002", "conclusion": "DMM\u4e3a\u9ad8\u80fd\u7269\u7406\u6570\u636e\u4f20\u8f93\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7f51\u7edc\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00202", "pdf": "https://arxiv.org/pdf/2508.00202", "abs": "https://arxiv.org/abs/2508.00202", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u5206\u7c7b\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u5fae\u8c03\u9700\u6c42\uff0c\u4ee5\u53ca\u73b0\u6709kNN\u65b9\u6cd5\u5229\u7528\u5c40\u90e8\u51e0\u4f55\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u53ef\u9760\u6027\u52a0\u6743\u63a8\u65ad\uff0c\u7ed3\u5408\u975e\u8d1f\u6838\uff08NNK\uff09\u90bb\u57df\u6784\u5efa\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728CIFAR-10\u548cDermaMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u5404\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u6807\u51c6kNN\u548c\u81ea\u9002\u5e94\u90bb\u57df\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u4fe1\u606f\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u5206\u7c7b\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00178", "pdf": "https://arxiv.org/pdf/2508.00178", "abs": "https://arxiv.org/abs/2508.00178", "authors": ["Brian Houck", "Travis Lowdermilk", "Cody Beyer", "Steven Clarke", "Ben Hanrahan"], "title": "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "As artificial intelligence (AI) tools become increasingly embedded in\nsoftware development workflows, questions persist about their true impact on\ndeveloper productivity and experience. This paper presents findings from a\nmixed-methods study examining how developers perceive AI's influence across the\ndimensions of the SPACE framework: Satisfaction, Performance, Activity,\nCollaboration and Efficiency. Drawing on survey responses from over 500\ndevelopers and qualitative insights from interviews and observational studies,\nwe find that AI is broadly adopted and widely seen as enhancing productivity,\nparticularly for routine tasks. However, the benefits vary, depending on task\ncomplexity, individual usage patterns, and team-level adoption. Developers\nreport increased efficiency and satisfaction, with less evidence of impact on\ncollaboration. Organizational support and peer learning play key roles in\nmaximizing AI's value. These findings suggest that AI is augmenting developers\nrather than replacing them, and that effective integration depends as much on\nteam culture and support structures as on the tools themselves. We conclude\nwith practical recommendations for teams, organizations and researchers seeking\nto harness AI's potential in software engineering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u91c7\u7528\uff0c\u63d0\u5347\u751f\u4ea7\u529b\uff0c\u4f46\u5bf9\u534f\u4f5c\u5f71\u54cd\u6709\u9650\uff0c\u56e2\u961f\u6587\u5316\u548c\u652f\u6301\u7ed3\u6784\u662f\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8AI\u5de5\u5177\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u548c\u4f53\u9a8c\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5305\u62ec500\u591a\u540d\u5f00\u53d1\u8005\u7684\u8c03\u67e5\u53ca\u8bbf\u8c08\u548c\u89c2\u5bdf\u3002", "result": "AI\u63d0\u5347\u6548\u7387\u548c\u6ee1\u610f\u5ea6\uff0c\u4f46\u5bf9\u534f\u4f5c\u5f71\u54cd\u8f83\u5c0f\uff0c\u56e2\u961f\u652f\u6301\u662f\u5173\u952e\u3002", "conclusion": "AI\u662f\u5f00\u53d1\u8005\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u56e2\u961f\u6587\u5316\u548c\u652f\u6301\u7ed3\u6784\u5bf9\u6709\u6548\u6574\u5408\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.00230", "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5KRAdapter\uff0c\u901a\u8fc7Khatri-Rao\u4e58\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u8fd1\u4f3c\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u6a21\u6001\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "LoRA\u5728\u9002\u5e94\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u3002", "method": "\u5f15\u5165KRAdapter\u7b97\u6cd5\uff0c\u5229\u7528Khatri-Rao\u4e58\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u3002", "result": "KRAdapter\u57281B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c8B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u4e8eLoRA\uff0c\u5c24\u5176\u5728\u672a\u89c1\u8fc7\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "conclusion": "KRAdapter\u5728\u4fdd\u6301LoRA\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2508.00500", "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u7684\u4e3b\u52a8\u8fd0\u884c\u65f6\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u98ce\u9669\u5e76\u63d0\u524d\u5e72\u9884\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c4\u5219\u7cfb\u7edf\u7f3a\u4e4f\u524d\u77bb\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u81ea\u4e3b\u80fd\u529b\u5f3a\u5927\u7684\u540c\u65f6\uff0c\u5176\u968f\u673a\u884c\u4e3a\u5e26\u6765\u4e86\u96be\u4ee5\u9884\u6d4b\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u89c4\u5219\u7cfb\u7edf\u53cd\u5e94\u6ede\u540e\u4e14\u65e0\u6cd5\u5904\u7406\u957f\u671f\u4f9d\u8d56\u548c\u5206\u5e03\u53d8\u5316\u3002", "method": "Pro2Guard\u5c06\u4ee3\u7406\u884c\u4e3a\u62bd\u8c61\u4e3a\u7b26\u53f7\u72b6\u6001\uff0c\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u8fd0\u884c\u65f6\u901a\u8fc7\u4f30\u8ba1\u5230\u8fbe\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\u9884\u6d4b\u98ce\u9669\uff0c\u5e76\u5728\u98ce\u9669\u8d85\u8fc7\u9608\u503c\u65f6\u89e6\u53d1\u5e72\u9884\u3002", "result": "\u5728\u5bb6\u5ead\u4ee3\u7406\u548c\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cPro2Guard\u5206\u522b\u5b9e\u73b0\u4e8693.6%\u7684\u65e9\u671f\u5b89\u5168\u5e72\u9884\u548c100%\u7684\u4ea4\u901a\u8fdd\u89c4\u53ca\u78b0\u649e\u9884\u6d4b\uff0c\u98ce\u9669\u9884\u6d4b\u65f6\u95f4\u6700\u957f\u53ef\u8fbe38.66\u79d2\u3002", "conclusion": "Pro2Guard\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u548c\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u884c\u4e3a\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u3002"}}
{"id": "2508.00264", "pdf": "https://arxiv.org/pdf/2508.00264", "abs": "https://arxiv.org/abs/2508.00264", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u4f1a\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u9000\u5316\uff0c\u6807\u7b7e\u5e73\u6ed1\u662f\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u5927\u8bcd\u6c47\u91cfLLM\u4e2d\u6548\u679c\u6709\u9650\u3002", "motivation": "\u63a2\u8ba8\u6307\u4ee4\u5fae\u8c03\u5bf9LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u5e76\u5bfb\u627e\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u5f00\u6e90LLM\u7684\u6821\u51c6\u9000\u5316\u73b0\u8c61\uff0c\u63d0\u51fa\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u5b9a\u5236\u5185\u6838\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u6807\u7b7e\u5e73\u6ed1\u80fd\u6709\u6548\u7ef4\u6301\u6821\u51c6\uff0c\u4f46\u5728\u5927\u8bcd\u6c47\u91cfLLM\u4e2d\u6548\u679c\u53d7\u9650\uff0c\u7406\u8bba\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e0e\u6a21\u578b\u89c4\u6a21\u7684\u5173\u8054\u3002", "conclusion": "\u6807\u7b7e\u5e73\u6ed1\u662f\u6307\u4ee4\u5fae\u8c03\u4e2d\u4fdd\u6301\u6821\u51c6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e94\u5bf9\u5927\u8bcd\u6c47\u91cfLLM\u7684\u6311\u6218\u3002"}}
{"id": "2508.00654", "pdf": "https://arxiv.org/pdf/2508.00654", "abs": "https://arxiv.org/abs/2508.00654", "authors": ["Rodrigo Escobar D\u00edaz Guerrero", "Jamile Mohammad Jafari", "Tobias Meyer-Zedler", "Michael Schmitt", "Juergen Popp", "Thomas Bocklitz"], "title": "LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources", "categories": ["cs.CE", "cs.SE"], "comment": null, "summary": "In the interdisciplinary field of microscopy research, managing and\nintegrating large volumes of data stored across disparate platforms remains a\nmajor challenge. Data types such as bioimages, experimental records, and\nspectral information are often maintained in separate repositories, each\nfollowing different management standards. However, linking these data sources\nacross the research lifecycle is essential to align with the FAIR principles of\ndata management: Findability, Accessibility, Interoperability, and Reusability.\nDespite this need, there is a notable lack of tools capable of effectively\nintegrating and linking data from heterogeneous sources. To address this gap,\nwe present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based\nplatform designed to create and manage links between distributed data systems.\nLEO was initially developed to link objects between Electronic Lab Notebooks\n(ELNs) and OMERO, but its functionality has since been extended through a\nplugin-based architecture, allowing the integration of additional data sources.\nThis extensibility makes LEO a scalable and flexible solution for a wide range\nof microscopy research workflows.", "AI": {"tldr": "LEO\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u5e73\u53f0\uff0c\u65e8\u5728\u8fde\u63a5\u548c\u7ba1\u7406\u5206\u5e03\u5f0f\u6570\u636e\u7cfb\u7edf\u4e4b\u95f4\u7684\u94fe\u63a5\uff0c\u89e3\u51b3\u663e\u5fae\u955c\u7814\u7a76\u4e2d\u6570\u636e\u6574\u5408\u7684\u6311\u6218\u3002", "motivation": "\u663e\u5fae\u955c\u7814\u7a76\u4e2d\uff0c\u8de8\u5e73\u53f0\u7ba1\u7406\u5927\u91cf\u5f02\u6784\u6570\u636e\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u4ee5\u6ee1\u8db3FAIR\u539f\u5219\u3002", "method": "\u5f00\u53d1LEO\u5e73\u53f0\uff0c\u901a\u8fc7\u63d2\u4ef6\u67b6\u6784\u94fe\u63a5\u7535\u5b50\u5b9e\u9a8c\u7b14\u8bb0\u672c\uff08ELN\uff09\u548cOMERO\uff0c\u5e76\u6269\u5c55\u652f\u6301\u5176\u4ed6\u6570\u636e\u6e90\u3002", "result": "LEO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u663e\u5fae\u955c\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "LEO\u586b\u8865\u4e86\u5f02\u6784\u6570\u636e\u6e90\u6574\u5408\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u652f\u6301FAIR\u539f\u5219\uff0c\u63d0\u5347\u4e86\u6570\u636e\u7ba1\u7406\u7684\u6548\u7387\u548c\u53ef\u91cd\u7528\u6027\u3002"}}
{"id": "2508.00270", "pdf": "https://arxiv.org/pdf/2508.00270", "abs": "https://arxiv.org/abs/2508.00270", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "categories": ["cs.LG"], "comment": null, "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff0c\u4f18\u5316\u5b66\u751f\u53cd\u9988\u7b56\u7565\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b66\u751f\u6570\u636e\u4f18\u5316\u53cd\u9988\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5b66\u751f\u7684\u5373\u65f6\u548c\u6574\u4f53\u5b66\u4e60\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u6846\u67b6\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff0c\u5206\u679043,000\u79cd\u8f85\u52a9\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u9009\u62e9\u6700\u4f73\u7b56\u7565\u3002\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0a\u4e0b\u6587\u8001\u864e\u673a\uff08CB\uff09\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "result": "\u5728166,000\u6b21\u7ec3\u4e60\u4f1a\u8bdd\u4e2d\u9a8c\u8bc1\u4e86MAB\u7b56\u7565\u663e\u8457\u63d0\u5347\u5b66\u751f\u8868\u73b0\uff0c\u4f46CB\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u53cd\u9988\u7cfb\u7edf\u5df2\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u4e2a\u6027\u5316\u7b56\u7565\u3002"}}
{"id": "2508.00682", "pdf": "https://arxiv.org/pdf/2508.00682", "abs": "https://arxiv.org/abs/2508.00682", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "title": "Unveiling Dynamic Binary Instrumentation Techniques", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u52a8\u6001\u4e8c\u8fdb\u5236\u63d2\u6869\uff08DBI\uff09\u6280\u672f\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\uff0c\u53d1\u73b0\u6ca1\u6709\u4e00\u79cd\u6280\u672f\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6700\u4f18\u3002", "motivation": "DBI\u6280\u672f\u5728\u5b89\u5168\u548c\u591a\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5404\u6709\u5c40\u9650\uff0c\u9700\u7cfb\u7edf\u6bd4\u8f83\u5176\u6027\u80fd\u548c\u9002\u7528\u6027\u3002", "method": "\u7efc\u5408\u5206\u6790\u4e86\u8fdb\u7a0b\u7ea7\u548c\u5168\u7cfb\u7edf\u7ea7\u7684DBI\u65b9\u6cd5\uff0c\u6bd4\u8f83\u5176\u63d2\u6869\u6280\u672f\u548c\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u6280\u672f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\uff0c\u6ca1\u6709\u7edd\u5bf9\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "conclusion": "DBI\u6280\u672f\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u4f18\u5316\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.00286", "pdf": "https://arxiv.org/pdf/2508.00286", "abs": "https://arxiv.org/abs/2508.00286", "authors": ["Mohsen Zaker Esteghamati"], "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "categories": ["cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57fa\u4e8e\u6027\u80fd\u7684\u6297\u9707\u8bbe\u8ba1\u89c6\u4e3a\u9006\u5de5\u7a0b\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6027\u80fd\u7684\u6297\u9707\u8bbe\u8ba1\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u4f18\u5316\u8bbe\u8ba1\u53c2\u6570\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u8bc4\u4f30\u51fd\u6570\u96c6\u6210\u5230\u9057\u4f20\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u89e3\u51b3\u9006\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u5efa\u7b51\u7c7b\u578b\u3001\u51e0\u4f55\u5f62\u72b6\u548c\u5730\u9707\u8bbe\u8ba1\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff08R2>90%\uff09\uff0c\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u8bc6\u522b\u7b26\u5408\u5de5\u7a0b\u539f\u7406\u7684\u6700\u4f18\u6784\u4ef6\u5c5e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4e0e\u4f18\u5316\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6297\u9707\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5efa\u7b51\u548c\u5730\u9707\u6761\u4ef6\u3002"}}
{"id": "2508.00304", "pdf": "https://arxiv.org/pdf/2508.00304", "abs": "https://arxiv.org/abs/2508.00304", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "categories": ["cs.LG"], "comment": null, "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "AI": {"tldr": "GOODFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u4e0d\u53d8\u5b66\u4e60\u7684Transformer\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u4e0d\u53d8\u4e0e\u53ef\u53d8\u5b50\u56fe\u5e76\u4f18\u5316\u7f16\u7801\u6a21\u5757\uff0c\u5b9e\u73b0\u5bf9\u65b0\u56fe\u7684\u6cdb\u5316\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u56feTransformer\u5728\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56fe\u4e0d\u53d8\u5b66\u4e60\u53ef\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5982\u4f55\u8bbe\u8ba1\u57fa\u4e8e\u4e0d\u53d8\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u7f16\u7801\u65b9\u5f0f\u4ecd\u5177\u6311\u6218\u3002", "method": "GOODFormer\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u8054\u5408\u4f18\u5316\uff1a1) \u71b5\u5f15\u5bfc\u7684\u4e0d\u53d8\u5b50\u56fe\u5206\u79bb\u5668\uff1b2) \u52a8\u6001\u5b50\u56fe\u7f16\u7801\u5668\uff1b3) \u4e0d\u53d8\u5b66\u4e60\u6a21\u5757\uff0c\u7ed3\u5408\u7406\u8bba\u652f\u6301\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGOODFormer\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GOODFormer\u6709\u6548\u89e3\u51b3\u4e86\u56feTransformer\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u56fe\u4e0d\u53d8\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00325", "pdf": "https://arxiv.org/pdf/2508.00325", "abs": "https://arxiv.org/abs/2508.00325", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "AI": {"tldr": "PnP-DA\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u540c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u68af\u5ea6\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\uff0c\u51cf\u5c11\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u4f18\u4e8e\u4f20\u7edf\u53d8\u5206\u65b9\u6cd5\u3002", "motivation": "\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\u8bef\u5dee\u7d2f\u79ef\u548c\u4f20\u7edf\u53d8\u5206\u65b9\u6cd5\u5bf9\u9ad8\u65af\u8bef\u5dee\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002", "method": "PnP-DA\u4ea4\u66ff\u4f7f\u7528\u68af\u5ea6\u5206\u6790\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u751f\u6210\u5148\u9a8c\uff0c\u907f\u514d\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u6807\u51c6\u6df7\u6c8c\u6d4b\u8bd5\u4e2d\uff0cPnP-DA\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "PnP-DA\u901a\u8fc7\u653e\u677e\u7edf\u8ba1\u5047\u8bbe\u548c\u5229\u7528\u5386\u53f2\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u540c\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00331", "pdf": "https://arxiv.org/pdf/2508.00331", "abs": "https://arxiv.org/abs/2508.00331", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "title": "Embryology of a Language Model", "categories": ["cs.LG"], "comment": null, "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUMAP\u548c\u654f\u611f\u6027\u5206\u6790\u7684\u80da\u80ce\u5b66\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u7f51\u7edc\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5f62\u6210\u662f\u6df1\u5ea6\u5b66\u4e60\u79d1\u5b66\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u800c\u654f\u611f\u6027\u5206\u6790\u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\u5c1a\u672a\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "method": "\u5e94\u7528UMAP\u5bf9\u654f\u611f\u6027\u77e9\u9635\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5206\u6790\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\u3002", "result": "\u53ef\u89c6\u5316\u7ed3\u679c\u5c55\u793a\u4e86\u6e05\u6670\u7684\u201c\u8eab\u4f53\u8ba1\u5212\u201d\uff0c\u5305\u62ec\u5df2\u77e5\u7279\u5f81\uff08\u5982\u611f\u5e94\u7535\u8def\uff09\u548c\u65b0\u53d1\u73b0\u7684\u7ed3\u6784\uff08\u5982\u7528\u4e8e\u8ba1\u6570\u7a7a\u683c\u6807\u8bb0\u7684\u201c\u95f4\u8ddd\u9ccd\u201d\uff09\u3002", "conclusion": "\u654f\u611f\u6027\u5206\u6790\u4e0d\u4ec5\u80fd\u9a8c\u8bc1\u6a21\u578b\uff0c\u8fd8\u80fd\u63ed\u793a\u65b0\u673a\u5236\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u80b2\u539f\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\u3002"}}
{"id": "2508.00350", "pdf": "https://arxiv.org/pdf/2508.00350", "abs": "https://arxiv.org/abs/2508.00350", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "categories": ["cs.LG"], "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOOD\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684OOD\u7279\u5f81\u548c\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u63d0\u53d6\u6709\u6548\u7684OOD\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210OOD\u6570\u636e\u3002", "method": "BOOD\u901a\u8fc7\u5b66\u4e60\u6587\u672c\u6761\u4ef6\u7684\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\uff0c\u9009\u62e9\u9760\u8fd1\u51b3\u7b56\u8fb9\u754c\u7684ID\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6270\u52a8\u751f\u6210OOD\u7279\u5f81\uff0c\u518d\u7528\u6269\u6563\u6a21\u578b\u89e3\u7801\u4e3a\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBOOD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cFPR95\u964d\u4f4e29.64%\uff0cAUROC\u63d0\u53477.27%\u3002", "conclusion": "BOOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684OOD\u7279\u5f81\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.00357", "pdf": "https://arxiv.org/pdf/2508.00357", "abs": "https://arxiv.org/abs/2508.00357", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "AI": {"tldr": "SGPC\uff08Sheaf GNNs with PAC-Bayes Calibration\uff09\u662f\u4e00\u79cd\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u63d0\u5347\u3001\u65b9\u5dee\u51cf\u5c11\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86GNN\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u5f02\u8d28\u56fe\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u73b0\u6709\u9798\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6216\u9ad8\u53c2\u6570\u5316\u7ed3\u6784\uff0c\u7f3a\u4e4f\u6cdb\u5316\u6027\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "method": "SGPC\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u3001\u6700\u4f18\u4f20\u8f93\u63d0\u5347\u3001\u65b9\u5dee\u51cf\u5c11\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u3002", "result": "\u57289\u4e2a\u540c\u8d28\u548c\u5f02\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSGPC\u4f18\u4e8e\u73b0\u6709\u5149\u8c31\u548c\u9798\u57faGNN\uff0c\u5e76\u63d0\u4f9b\u672a\u89c1\u8282\u70b9\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "SGPC\u901a\u8fc7\u7406\u8bba\u6027\u80fd\u8fb9\u754c\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2508.00364", "pdf": "https://arxiv.org/pdf/2508.00364", "abs": "https://arxiv.org/abs/2508.00364", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "categories": ["cs.LG"], "comment": null, "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "AI": {"tldr": "OID-PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5ba4\u5185\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u5b9a\u4e49\u7684\u529f\u80fd\u548c\u89c6\u89c9\u51c6\u5219\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f4f\u5b85\u5ba4\u5185\u8bbe\u8ba1\u5bf9\u5c45\u4f4f\u8005\u6ee1\u610f\u5ea6\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u7a00\u7f3a\u6216\u8bbe\u8ba1\u539f\u5219\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faOID-PPO\u6846\u67b6\uff0c\u5229\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u548c\u8fde\u7eed\u5bb6\u5177\u653e\u7f6e\u7b56\u7565\uff0c\u6574\u5408\u8bbe\u8ba1\u51c6\u5219\u5230\u5956\u52b1\u51fd\u6570\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOID-PPO\u5728\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u51c6\u5219\u7684\u91cd\u8981\u6027\u3002", "conclusion": "OID-PPO\u4e3a\u5ba4\u5185\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bbe\u8ba1\u51c6\u5219\u7684\u5177\u4f53\u8d21\u732e\u3002"}}
{"id": "2508.00392", "pdf": "https://arxiv.org/pdf/2508.00392", "abs": "https://arxiv.org/abs/2508.00392", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u91cd\u9002\u5e94\u6027\u7684\u901a\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u4e13\u5bb6\u6846\u67b6\u52a8\u6001\u521b\u5efa\u548c\u805a\u5408\u4e13\u5bb6\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u51fd\u6570\u7c7b\u578b\u548c\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u53ea\u80fd\u5904\u7406\u5355\u4e00\u51f8\u51fd\u6570\u7c7b\u578b\u4e14\u9700\u8981\u5148\u9a8c\u53c2\u6570\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5143\u4e13\u5bb6\u6846\u67b6\uff0c\u52a8\u6001\u521b\u5efa\u591a\u4e2a\u4e13\u5bb6\u5e76\u901a\u8fc7\u5143\u7b97\u6cd5\u805a\u5408\uff0c\u7ed3\u5408\u7761\u7720\u4e13\u5bb6\u6280\u672f\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "result": "\u7b97\u6cd5\u80fd\u540c\u65f6\u6700\u5c0f\u5316\u591a\u79cd\u51f8\u51fd\u6570\u7684\u81ea\u9002\u5e94\u9057\u61be\uff0c\u5e76\u5141\u8bb8\u51fd\u6570\u7c7b\u578b\u5728\u8f6e\u6b21\u95f4\u5207\u6362\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u81f3\u5728\u7ebf\u590d\u5408\u4f18\u5316\uff0c\u5f00\u53d1\u4e86\u901a\u7528\u7b97\u6cd5\u4ee5\u6700\u5c0f\u5316\u590d\u5408\u51fd\u6570\u7684\u81ea\u9002\u5e94\u9057\u61be\u3002"}}
{"id": "2508.00394", "pdf": "https://arxiv.org/pdf/2508.00394", "abs": "https://arxiv.org/abs/2508.00394", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "AI": {"tldr": "ExeKGLib\u662f\u4e00\u4e2aPython\u5e93\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u548c\u77e5\u8bc6\u56fe\u8c31\u5e2e\u52a9\u975eML\u4e13\u5bb6\u6784\u5efaML\u6d41\u7a0b\u3002", "motivation": "\u89e3\u51b3\u975eML\u4e13\u5bb6\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u6784\u5efa\u9ad8\u8d28\u91cfML\u6d41\u7a0b\u7684\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u5f62\u754c\u9762\uff0c\u7b80\u5316ML\u6d41\u7a0b\u7684\u6784\u5efa\u3002", "result": "ExeKGLib\u63d0\u9ad8\u4e86\u6d41\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u786e\u4fdd\u5176\u53ef\u6267\u884c\u3002", "conclusion": "ExeKGLib\u5728\u771f\u5b9e\u7528\u4f8b\u4e2d\u5c55\u793a\u4e86\u5176\u53ef\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00410", "pdf": "https://arxiv.org/pdf/2508.00410", "abs": "https://arxiv.org/abs/2508.00410", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "categories": ["cs.LG"], "comment": null, "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Reward\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bed\u4e49\u76f8\u4f3c\u95ee\u9898\u7684\u7b54\u6848\u4e00\u81f4\u6027\u4f5c\u4e3a\u5956\u52b1\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u81ea\u5956\u52b1\u4fe1\u53f7\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u5e26\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6807\u7b7e\u7684\u95ee\u9898\u9650\u5236\u4e86\u6269\u5c55\u6027\u3002\u73b0\u6709\u81ea\u5956\u52b1\u65b9\u6cd5\u867d\u80fd\u6fc0\u53d1LLM\u63a8\u7406\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faCo-Reward\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u6784\u9020\u76f8\u4f3c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6295\u7968\u751f\u6210\u4ee3\u7406\u6807\u7b7e\uff0c\u5229\u7528\u5bf9\u6bd4\u4e00\u81f4\u6027\u6784\u5efa\u5956\u52b1\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548cLLM\u7cfb\u5217\u4e2d\uff0cCo-Reward\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u5956\u52b1\uff0c\u5982\u5728MATH500\u4e0a\u63d0\u53476.8%\u3002", "conclusion": "Co-Reward\u901a\u8fc7\u81ea\u76d1\u7763\u5956\u52b1\u673a\u5236\u6709\u6548\u907f\u514d\u4e86\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u8f93\u5165\u6837\u672c\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2508.00415", "pdf": "https://arxiv.org/pdf/2508.00415", "abs": "https://arxiv.org/abs/2508.00415", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "categories": ["cs.LG"], "comment": null, "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdResE-BiLSTM\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u548c\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u5bf9\u4fe1\u7528\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528ResE-BiLSTM\u6a21\u578b\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\uff0c\u5e76\u5728Freddie Mac\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0eLSTM\u3001BiLSTM\u3001GRU\u3001CNN\u548cRNN\u7b49\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "ResE-BiLSTM\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u548cAUC\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ResE-BiLSTM\u5728\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00472", "pdf": "https://arxiv.org/pdf/2508.00472", "abs": "https://arxiv.org/abs/2508.00472", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "categories": ["cs.LG"], "comment": null, "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3actdGAN\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "motivation": "\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u6027\u80fd\uff0c\u73b0\u6709GAN\u65b9\u6cd5\u672a\u8003\u8651\u8f93\u5165\u6837\u672c\u7684\u5411\u91cf\u5b50\u7a7a\u95f4\uff0c\u5bfc\u81f4\u751f\u6210\u6570\u636e\u4f4d\u7f6e\u968f\u610f\u4e14\u6761\u4ef6\u91c7\u6837\u6548\u679c\u4e0d\u4f73\u3002", "method": "ctdGAN\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u6b65\u9aa4\u4e3a\u8f93\u5165\u6837\u672c\u5206\u914d\u805a\u7c7b\u6807\u7b7e\uff0c\u5229\u7528\u65b0\u6982\u7387\u91c7\u6837\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u751f\u6210\u6837\u672c\uff0c\u786e\u4fdd\u751f\u6210\u6570\u636e\u5206\u5e03\u63a5\u8fd1\u539f\u59cb\u6570\u636e\u3002", "result": "\u572814\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cctdGAN\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u5e76\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "ctdGAN\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u751f\u6210\u6837\u672c\u8d28\u91cf\u9ad8\u4e14\u5206\u7c7b\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2508.00507", "pdf": "https://arxiv.org/pdf/2508.00507", "abs": "https://arxiv.org/abs/2508.00507", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "categories": ["cs.LG"], "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u65b0\u6846\u67b6CoLL\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\uff08TAGs\uff09\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u56fe\u57df\u5185\u7684\u4f18\u5316\u76ee\u6807\uff0c\u5ffd\u89c6\u4e86\u6587\u672c\u6a21\u6001\u7684\u4e92\u8865\u4ef7\u503c\uff0c\u4e14\u6587\u672c\u7279\u5f81\u5e38\u901a\u8fc7\u6d45\u5c42\u5d4c\u5165\u6280\u672f\u7f16\u7801\uff0c\u53ef\u80fd\u9057\u6f0f\u5f02\u5e38\u76f8\u5173\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faCoLL\u6846\u67b6\uff0c\u901a\u8fc7\u591aLLM\u534f\u4f5c\u8fdb\u884c\u8bc1\u636e\u589e\u5f3a\u751f\u6210\uff0c\u6355\u6349\u5f02\u5e38\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u5e76\u7ed3\u5408\u5e26\u95e8\u63a7\u673a\u5236\u7684GNN\u81ea\u9002\u5e94\u878d\u5408\u6587\u672c\u7279\u5f81\u4e0e\u62d3\u6251\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoLL\u5728AP\u4e0a\u5e73\u5747\u63d0\u534713.37%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CoLL\u4e3a\u7ed3\u5408LLMs\u63a8\u8fdb\u56fe\u5f02\u5e38\u68c0\u6d4b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.00513", "pdf": "https://arxiv.org/pdf/2508.00513", "abs": "https://arxiv.org/abs/2508.00513", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "categories": ["cs.LG"], "comment": "Accepted by ECAI 2025", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMUCL\u7684\u7aef\u5230\u7aef\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u548c\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u56fe\u6570\u636e\u5e38\u5e26\u6709\u6587\u672c\u63cf\u8ff0\uff08TAGs\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u7f16\u7801\u548c\u5f02\u5e38\u68c0\u6d4b\u76ee\u6807\u4e4b\u95f4\u5b58\u5728\u5206\u79bb\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u80fd\u529b\u3002\u5982\u4f55\u6574\u5408\u6587\u672c\u548c\u56fe\u62d3\u6251\u4ee5\u91ca\u653e\u8de8\u6a21\u6001\u6570\u636e\u7684\u6f5c\u529b\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faCMUCL\u65b9\u6cd5\uff0c\u540c\u65f6\u5efa\u6a21\u6587\u672c\u548c\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u548c\u5355\u6a21\u6001\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u8054\u5408\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u4e0d\u4e00\u81f4\u6027\u6316\u6398\u7684\u5f02\u5e38\u8bc4\u5206\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCMUCL\u5728\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u51c6\u786e\u7387\uff08AP\uff09\u6bd4\u6b21\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e8611.13%\u3002", "conclusion": "CMUCL\u901a\u8fc7\u6574\u5408\u6587\u672c\u548c\u56fe\u62d3\u6251\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e868\u4e2a\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.00523", "pdf": "https://arxiv.org/pdf/2508.00523", "abs": "https://arxiv.org/abs/2508.00523", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "categories": ["cs.LG"], "comment": null, "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u975e\u5b50\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff08DBGD-NF\u548c\u5176\u6269\u5c55\u7248\uff09\u4ee5\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u5bf9\u5ef6\u8fdf\u548c\u53cd\u9988\u7684\u654f\u611f\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u5ef6\u8fdf\u654f\u611f\u4e14\u672a\u89e3\u8026\u5ef6\u8fdf\u4e0e\u53cd\u9988\u7684\u8054\u5408\u6548\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86DBGD-NF\u7b97\u6cd5\u53ca\u5176\u6269\u5c55\u7248\uff0c\u5206\u522b\u5229\u7528\u5355\u70b9\u68af\u5ea6\u4f30\u8ba1\u548c\u5757\u66f4\u65b0\u673a\u5236\u3002", "result": "DBGD-NF\u5b9e\u73b0\u4e86\u4e0e\u5e73\u5747\u5ef6\u8fdf\u76f8\u5173\u7684\u9057\u61be\u754c\uff0c\u6269\u5c55\u7248\u8fdb\u4e00\u6b65\u89e3\u8026\u4e86\u5ef6\u8fdf\u4e0e\u53cd\u9988\u7684\u6548\u5e94\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u5ef6\u8fdf\u548c\u53cd\u9988\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u7a00\u758f\u5b66\u4e60\u7b49\u573a\u666f\u3002"}}
{"id": "2508.00539", "pdf": "https://arxiv.org/pdf/2508.00539", "abs": "https://arxiv.org/abs/2508.00539", "authors": ["Judy X Yang"], "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "categories": ["cs.LG", "Cs"], "comment": "8 pages, 6 figures", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u566a\u6bd4\u9608\u503c\u548c\u6ee4\u6ce2\u4f18\u5316\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u7ed3\u5408\u805a\u7c7b\u548cNNLS\u89e3\u6df7\uff0c\u63d0\u5347\u77ff\u7269\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5149\u8c31\u6210\u50cf\u4e2d\u5f31\u77ff\u7269\u4fe1\u53f7\u88ab\u566a\u58f0\u548c\u5197\u4f59\u6ce2\u6bb5\u63a9\u76d6\u7684\u95ee\u9898\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u4fe1\u566a\u6bd4\u9608\u503c\u548cSavitzky-Golay\u6ee4\u6ce2\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff1aKMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5f31\u77ff\u7269\u533a\u57df\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u89e3\u6df7\u51c6\u786e\u6027\u3002", "conclusion": "\u4e24\u9636\u6bb5\u7b56\u7565\u4e3a\u5730\u8d28\u9ad8\u5149\u8c31\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00545", "pdf": "https://arxiv.org/pdf/2508.00545", "abs": "https://arxiv.org/abs/2508.00545", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "title": "Foundations of Interpretable Models", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5b9a\u4e49\u4e0d\u5177\u64cd\u4f5c\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u4e00\u822c\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\u64cd\u4f5c\u6027\uff0c\u65e0\u6cd5\u6307\u5bfc\u7528\u6237\u8bbe\u8ba1\u901a\u7528\u3001\u53ef\u9760\u4e14\u7a33\u5065\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u5bfc\u81f4\u7814\u7a76\u95ee\u9898\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u65b0\u5b9a\u4e49\uff0c\u6db5\u76d6\u73b0\u6709\u975e\u6b63\u5f0f\u6982\u5ff5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u901a\u7528\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "result": "\u65b0\u5b9a\u4e49\u5177\u6709\u64cd\u4f5c\u6027\uff0c\u63ed\u793a\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u6240\u9700\u7684\u57fa\u7840\u5c5e\u6027\u3001\u5047\u8bbe\u3001\u539f\u5219\u3001\u6570\u636e\u7ed3\u6784\u548c\u67b6\u6784\u7279\u5f81\u3002", "conclusion": "\u8bba\u6587\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u65b9\u5411\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00578", "pdf": "https://arxiv.org/pdf/2508.00578", "abs": "https://arxiv.org/abs/2508.00578", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gr\u00e4ter", "Pascal Friederich"], "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5927\u91cf\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08SchNet\u3001Allegro\u548cMACE\uff09\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u53cd\u5e94\u52bf\u80fd\u9762\u548c\u53cd\u5e94\u80fd\u5792\u3002MACE\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u80f6\u539f\u86cb\u767d\u6a21\u62df\u3002", "motivation": "\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u5728\u751f\u7269\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u7406\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\uff08\u5982\u7ecf\u5178\u529b\u573a\u6216DFT\u5206\u5b50\u52a8\u529b\u5b66\uff09\u96be\u4ee5\u6ee1\u8db3\u91cf\u5b50\u5316\u5b66\u7cbe\u5ea6\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u534a\u7ecf\u9a8c\u65b9\u6cd5\u548cDFT\u751f\u6210HAT\u53cd\u5e94\u6570\u636e\u96c6\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08SchNet\u3001Allegro\u548cMACE\uff09\u7684\u6027\u80fd\u3002MACE\u7528\u4e8e\u9884\u6d4b\u53cd\u5e94\u52bf\u80fd\u9762\u548c\u80fd\u5792\u3002", "result": "MACE\u5728\u80fd\u91cf\u3001\u529b\u548c\u80fd\u5792\u9884\u6d4b\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.13 kcal/mol\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u80f6\u539f\u86cb\u767d\u6a21\u62df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u751f\u7269\u5206\u5b50\u7cfb\u7edf\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5316\u5b66\u53cd\u5e94\u63d0\u4f9b\u91cf\u5b50\u7ea7\u7cbe\u5ea6\u7684\u6a21\u62df\u3002"}}
{"id": "2508.00586", "pdf": "https://arxiv.org/pdf/2508.00586", "abs": "https://arxiv.org/abs/2508.00586", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "title": "The Role of Active Learning in Modern Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6548\u7387\u6700\u4f4e\uff0c\u4ec5\u63d0\u53471-4%\uff0c\u800c\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7ed3\u5408\u968f\u673a\u91c7\u6837\u53ef\u63d0\u534760%\u3002\u4f46AL\u4e0eDA\u548cSSL\u7ed3\u5408\u540e\u4ecd\u80fd\u63d0\u4f9b\u989d\u5916\u63d0\u5347\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u6027\u80fd\u4f18\u5316\u7684\u6700\u540e\u4e00\u6b65\u3002", "motivation": "\u63a2\u8ba8\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8f83\u5c11\u4f7f\u7528\u7684\u539f\u56e0\uff0c\u5e76\u7814\u7a76\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u6570\u636e\u589e\u5f3a\uff08DA\uff09\u3001\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u7ec4\u5408\u6548\u679c\u3002", "result": "AL\u5355\u72ec\u4f7f\u7528\u65f6\u6548\u7387\u6700\u4f4e\uff0c\u4f46\u4e0eDA\u548cSSL\u7ed3\u5408\u540e\u4ecd\u80fd\u63d0\u4f9b\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5efa\u8bae\u5c06AL\u4f5c\u4e3a\u6570\u636e\u4f18\u5316\u7684\u6700\u540e\u4e00\u6b65\uff0c\u800c\u975e\u89e3\u51b3\u6807\u7b7e\u7f3a\u5931\u7684\u4e3b\u8981\u65b9\u6cd5\u3002"}}
{"id": "2508.00615", "pdf": "https://arxiv.org/pdf/2508.00615", "abs": "https://arxiv.org/abs/2508.00615", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u81ea\u5efa\u56fe\u6a21\u578b\uff08SBSCGM\uff09\u548c\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HybridGraphMedGNN\uff09\uff0c\u7528\u4e8e\u9884\u6d4bICU\u60a3\u8005\u7684\u6b7b\u4ea1\u98ce\u9669\u548c\u8fde\u7eed\u5173\u952e\u6027\u8bc4\u5206\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u800c\u51c6\u786e\u9884\u6d4bICU\u60a3\u8005\u7684\u5173\u952e\u6027\u5bf9\u65e9\u671f\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "SBSCGM\u52a8\u6001\u6784\u5efa\u60a3\u8005\u76f8\u4f3c\u56fe\uff0cHybridGraphMedGNN\u7ed3\u5408GCN\u3001GraphSAGE\u548cGAT\u5c42\u5b66\u4e60\u60a3\u8005\u8868\u793a\u3002", "result": "\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578bAUC-ROC\u8fbe0.94\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u5355\u4e00GNN\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u91cd\u75c7\u76d1\u62a4\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00627", "pdf": "https://arxiv.org/pdf/2508.00627", "abs": "https://arxiv.org/abs/2508.00627", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime R\u00e9jou M\u00e9chain"], "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "categories": ["cs.LG", "I.4.9; I.4.6"], "comment": "11 pages, 5 figures", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "AI": {"tldr": "IAMAP\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684QGIS\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u5927\u6570\u636e\u9700\u6c42\u3001\u9ad8\u8ba1\u7b97\u8d44\u6e90\u548c\u5f3a\u7f16\u7801\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5927\u6570\u636e\u9700\u6c42\u3001\u9ad8\u8ba1\u7b97\u8d44\u6e90\u548c\u5f3a\u7f16\u7801\u80fd\u529b\uff0cIAMAP\u65e8\u5728\u4e3a\u975eAI\u4e13\u5bb6\u63d0\u4f9b\u4fbf\u6377\u7684\u5de5\u5177\u3002", "method": "IAMAP\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u901a\u7528\u6a21\u578b\uff0c\u652f\u6301\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u3001\u805a\u7c7b\u3001\u76f8\u4f3c\u6027\u6620\u5c04\u548c\u6a21\u578b\u9a8c\u8bc1\u7b49\u529f\u80fd\u3002", "result": "IAMAP\u4f7f\u975e\u4e13\u5bb6\u80fd\u591f\u9ad8\u6548\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\uff0c\u65e0\u9700GPU\u6216\u5927\u91cf\u6570\u636e\u3002", "conclusion": "IAMAP\u63a8\u52a8\u4e86\u9ad8\u6548\u3001\u8282\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u666e\u53ca\u3002"}}
{"id": "2508.00628", "pdf": "https://arxiv.org/pdf/2508.00628", "abs": "https://arxiv.org/abs/2508.00628", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "categories": ["cs.LG"], "comment": null, "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "AI": {"tldr": "SV-SNN\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u53d8\u91cf\u548c\u81ea\u9002\u5e94\u8c31\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edfPINN\u5728\u9ad8\u9891\u632f\u8361PDE\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfPINN\u5728\u9ad8\u9891\u632f\u8361PDE\u4e2d\u5b58\u5728\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9ad8\u9891\u89e3\u5206\u91cf\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "SV-SNN\u901a\u8fc7\u5206\u89e3\u591a\u53d8\u91cf\u51fd\u6570\u4e3a\u5355\u53d8\u91cf\u4e58\u79ef\u3001\u81ea\u9002\u5e94\u5085\u91cc\u53f6\u8c31\u7279\u5f81\u548c\u5b66\u4e60\u9891\u7387\u53c2\u6570\uff0c\u7ed3\u5408SVD\u7406\u8bba\u6846\u67b6\u91cf\u5316\u8c31\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u95ee\u9898\u4e2d\uff0cSV-SNN\u7cbe\u5ea6\u63d0\u53471-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u53c2\u6570\u51cf\u5c1190%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed60%\u3002", "conclusion": "SV-SNN\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecfPDE\u6c42\u89e3\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.00635", "pdf": "https://arxiv.org/pdf/2508.00635", "abs": "https://arxiv.org/abs/2508.00635", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKAN\u7684\u81ea\u9002\u5e94\u9891\u7387\u9009\u62e9\u5b66\u4e60\u67b6\u6784\uff08KFS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u591a\u5c3a\u5ea6\u566a\u58f0\u5e72\u6270\u548c\u9891\u7387\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u5b58\u5728\u566a\u58f0\u5e72\u6270\uff0c\u4e14\u9891\u7387\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\uff0c\u5bfc\u81f4\u591a\u5c3a\u5ea6\u8868\u793a\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408Kolmogorov-Arnold Networks\uff08KAN\uff09\u548cParseval\u5b9a\u7406\uff0c\u8bbe\u8ba1KFS\u6846\u67b6\uff0c\u901a\u8fc7FreK\u6a21\u5757\u8fdb\u884c\u9891\u57df\u80fd\u91cf\u5206\u5e03\u4e3b\u5bfc\u9891\u7387\u9009\u62e9\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u6233\u5d4c\u5165\u5bf9\u9f50\u591a\u5c3a\u5ea6\u65f6\u95f4\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKFS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "KFS\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u67b6\u6784\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00641", "pdf": "https://arxiv.org/pdf/2508.00641", "abs": "https://arxiv.org/abs/2508.00641", "authors": ["Alessandro Palmas"], "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "categories": ["cs.LG"], "comment": "11 pages, 10 figures", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5e94\u5bf9\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u7fa4\u5a01\u80c1\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9632\u5fa1\u7cfb\u7edf\u4e2d\u7684\u6218\u7565\u4f18\u52bf\u3002", "motivation": "\u4f4e\u6210\u672c\u81ea\u6740\u5f0f\u65e0\u4eba\u673a\u7fa4\u7684\u5a01\u80c1\u5bf9\u73b0\u4ee3\u9632\u5fa1\u7cfb\u7edf\u63d0\u51fa\u4e86\u5feb\u901f\u6218\u7565\u51b3\u7b56\u7684\u9700\u6c42\uff0c\u9700\u4f18\u5316\u62e6\u622a\u4f18\u5148\u7ea7\u3002", "method": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\uff0c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u534f\u8c03\u591a\u6548\u5e94\u5668\uff0c\u4f18\u5316\u62e6\u622a\u4f18\u5148\u7ea7\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6a21\u62df\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\uff0c\u5e73\u5747\u635f\u5bb3\u66f4\u4f4e\uff0c\u9632\u5fa1\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4f5c\u4e3a\u9632\u5fa1\u67b6\u6784\u7684\u6218\u7565\u5c42\uff0c\u63d0\u5347\u97e7\u6027\u800c\u4e0d\u53d6\u4ee3\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\u3002"}}
{"id": "2508.00643", "pdf": "https://arxiv.org/pdf/2508.00643", "abs": "https://arxiv.org/abs/2508.00643", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "categories": ["cs.LG"], "comment": null, "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "AI": {"tldr": "DINOZAUR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86FNOs\u7684\u8fc7\u53c2\u6570\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "FNOs\u5b58\u5728\u8fc7\u53c2\u6570\u5316\u548c\u7f3a\u4e4f\u539f\u751f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "DINOZAUR\u91c7\u7528\u6269\u6563\u4e58\u6cd5\u5668\u66ff\u4ee3FNOs\u4e2d\u7684\u5bc6\u96c6\u5f20\u91cf\u4e58\u6cd5\u5668\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u5b9a\u4e49\u65f6\u95f4\u53c2\u6570\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDINOZAUR\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "DINOZAUR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u79d1\u5b66\u548c\u5de5\u7a0b\u95ee\u9898\u3002"}}
{"id": "2508.00657", "pdf": "https://arxiv.org/pdf/2508.00657", "abs": "https://arxiv.org/abs/2508.00657", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "categories": ["cs.LG"], "comment": "Accepted by MLHC 2025", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "AI": {"tldr": "TrajSurv\u662f\u4e00\u4e2a\u57fa\u4e8e\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u751f\u5b58\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u63d0\u53d6\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u89e3\u91ca\u65b9\u6cd5\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u53ef\u9760\u7684\u751f\u5b58\u9884\u6d4b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u4e34\u5e8a\u6570\u636e\u5e76\u900f\u660e\u5730\u5173\u8054\u751f\u5b58\u7ed3\u679c\u3002", "method": "TrajSurv\u4f7f\u7528NCDE\u63d0\u53d6\u6f5c\u5728\u8f68\u8ff9\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u6f5c\u5728\u72b6\u6001\u4e0e\u60a3\u8005\u72b6\u6001\uff0c\u5e76\u91c7\u7528\u4e24\u6b65\u89e3\u91ca\u65b9\u6cd5\u5173\u8054\u4e34\u5e8a\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u3002", "result": "\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\uff0cTrajSurv\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u51c6\u786e\u6027\u548c\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "TrajSurv\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u900f\u660e\u7684\u751f\u5b58\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2508.00664", "pdf": "https://arxiv.org/pdf/2508.00664", "abs": "https://arxiv.org/abs/2508.00664", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "categories": ["cs.LG"], "comment": null, "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u539f\u578b\uff08DP\uff09\u7684DGAD\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u6f14\u53d8\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u5185\u5b58\u7f13\u51b2\u533a\u548c\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u8bb0\u5b9e\u73b0\u8de8\u57df\u68c0\u6d4b\u3002", "motivation": "\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\uff08DGAD\uff09\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u91d1\u878d\u3001\u4ea4\u901a\u3001\u793e\u4ea4\u7f51\u7edc\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u96be\u4ee5\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u7684\u5f02\u5e38\u6f14\u53d8\uff0c\u4e14\u65b0\u9886\u57df\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u3002", "method": "DP-DGAD\u901a\u8fc7\u63d0\u53d6\u52a8\u6001\u539f\u578b\uff08\u6b63\u5e38\u548c\u5f02\u5e38\u6a21\u5f0f\u7684\u6f14\u53d8\u8868\u793a\uff09\u5e76\u5b58\u50a8\u5728\u5185\u5b58\u7f13\u51b2\u533a\u4e2d\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u7f13\u51b2\u533a\u4ee5\u4fdd\u7559\u901a\u7528\u6a21\u5f0f\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u9886\u57df\u7279\u5b9a\u6a21\u5f0f\uff0c\u6700\u540e\u901a\u8fc7\u5f02\u5e38\u8bc4\u5206\u5668\u548c\u4f2a\u6807\u8bb0\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u5341\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDP-DGAD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DP-DGAD\u901a\u8fc7\u52a8\u6001\u539f\u578b\u548c\u4f2a\u6807\u8bb0\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2508.00692", "pdf": "https://arxiv.org/pdf/2508.00692", "abs": "https://arxiv.org/abs/2508.00692", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GDFM\u548cGAN\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u5408\u6210\u5206\u5e03\u5f0f\u98ce\u7535\u573a\u7684\u957f\u671f\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528GDFM\u6216GAN\u7684\u65b9\u6cd5\u3002", "motivation": "\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u9700\u8981\u5408\u6210\u591a\u98ce\u7535\u573a\u957f\u671f\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982GDFM\u548cGAN\uff09\u5404\u6709\u5c40\u9650\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u6ce2\u5f62\u6a21\u4eff\u9700\u6c42\u3002", "method": "\u7ed3\u5408GDFM\u548cGAN\uff0c\u5229\u7528GAN\u63d0\u53d6\u52a8\u6001\u56e0\u5b50\u5e76\u4f5c\u4e3aGDFM\u7684\u6ee4\u6ce2\u5668\uff0c\u4ee5\u540c\u65f6\u6355\u6349\u65f6\u7a7a\u548c\u9891\u7387\u76f8\u5173\u6027\u3002", "result": "\u5728\u6fb3\u5927\u5229\u4e9a\u98ce\u7535\u6570\u636e\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u98ce\u7535\u529f\u7387\u573a\u666f\u65f6\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\uff0c\u66f4\u63a5\u8fd1\u5b9e\u9645\u98ce\u7535\u7684\u7edf\u8ba1\u7279\u6027\u3002", "conclusion": "GDFM\u4e0eGAN\u7684\u7ed3\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u5408\u6210\u5177\u6709\u5b9e\u9645\u7edf\u8ba1\u7279\u6027\u7684\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u3002"}}
{"id": "2508.00695", "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "\u6bd4\u8f83\u591a\u79cdAI\u6a21\u578b\u5bf9\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u800c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u7814\u7a76AI\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u4e2d\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u6bd4\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08\u5982\u968f\u673a\u68ee\u6797\u3001SVM\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982DistilBERT\u3001SciBERT\uff09\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u8fc7\u91c7\u6837\u7b56\u7565\uff08\u5982SMOTE\uff09\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u6548\u679c\u3002", "result": "\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u7387\uff08\u6700\u9ad896%\uff09\uff0c\u8fc7\u91c7\u6837\u65b9\u6cd5\u4ec5\u5bf9BERT\u6a21\u578b\u6709\u8f7b\u5fae\u6b63\u9762\u5f71\u54cd\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7AI\u8bca\u65ad\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.00706", "pdf": "https://arxiv.org/pdf/2508.00706", "abs": "https://arxiv.org/abs/2508.00706", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "title": "Learning Network Dismantling without Handcrafted Inputs", "categories": ["cs.LG"], "comment": null, "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u624b\u5de5\u7279\u5f81\u7684\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6MIND\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u7edc\u62c6\u89e3\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7edc\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u5f15\u5165\u504f\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7eaf\u7cb9\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u6d88\u606f\u8fed\u4ee3\u914d\u7f6e\u6587\u4ef6\uff0c\u5229\u7528\u591a\u6837\u5316\u5408\u6210\u7f51\u7edc\u8bad\u7ec3\u96c6\u6784\u5efa\u9ad8\u6548\u6846\u67b6\u3002", "result": "MIND\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7edc\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MIND\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u7f51\u7edc\u62c6\u89e3\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u590d\u6742\u7f51\u7edc\u95ee\u9898\u3002"}}
{"id": "2508.00707", "pdf": "https://arxiv.org/pdf/2508.00707", "abs": "https://arxiv.org/abs/2508.00707", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "title": "Efficient Solution and Learning of Robust Factored MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u548c\u5b66\u4e60\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08r-MDPs\uff09\uff0c\u901a\u8fc7\u5229\u7528\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u72ec\u7acb\u6027\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "r-MDPs\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8f6c\u79fb\u52a8\u6001\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6269\u5c55MDPs\uff0c\u4f46\u5b66\u4e60r-MDPs\u9700\u8981\u5927\u91cf\u6837\u672c\u4ea4\u4e92\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u51cf\u5c11\u6837\u672c\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u76f4\u63a5\u5b66\u4e60\u5206\u89e3\u6a21\u578b\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u5206\u89e3\u7ed3\u6784\u53ef\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u9c81\u68d2\u7b56\u7565\u548c\u66f4\u4e25\u683c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u662f\u89e3\u51b3\u548c\u5b66\u4e60r-MDPs\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002"}}
{"id": "2508.00712", "pdf": "https://arxiv.org/pdf/2508.00712", "abs": "https://arxiv.org/abs/2508.00712", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "title": "JSON-Bag: A generic game trajectory representation", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "AI": {"tldr": "JSON-Bag\u6a21\u578b\u901a\u8fc7JSON\u63cf\u8ff0\u7684\u6e38\u620f\u8f68\u8ff9\u8fdb\u884c\u6807\u8bb0\u5316\uff0c\u5e76\u4f7f\u7528Jensen-Shannon\u8ddd\u79bb\uff08JSD\uff09\u4f5c\u4e3a\u5ea6\u91cf\u6807\u51c6\uff0c\u7ed3\u5408\u539f\u578b\u6700\u8fd1\u90bb\u641c\u7d22\uff08P-NNS\uff09\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u516d\u6b3e\u684c\u6e38\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u6837\u672c\u6548\u7387\u9ad8\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u7528\u5730\u8868\u793a\u6e38\u620f\u8f68\u8ff9\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528JSON-Bag\u6a21\u578b\u6807\u8bb0\u5316\u6e38\u620f\u8f68\u8ff9\u7684JSON\u63cf\u8ff0\uff0c\u7ed3\u5408JSD\u548cP-NNS\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4e14\u80fd\u901a\u8fc7\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "JSON-Bag\u6a21\u578b\u80fd\u6709\u6548\u8868\u793a\u6e38\u620f\u8f68\u8ff9\uff0cJSD\u4e0e\u7b56\u7565\u8ddd\u79bb\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.00716", "pdf": "https://arxiv.org/pdf/2508.00716", "abs": "https://arxiv.org/abs/2508.00716", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "AI": {"tldr": "NeGPR\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u9884\u8bad\u7ec3\u548c\u5d4c\u5957\u4f2a\u6807\u7b7e\u7ec6\u5316\u673a\u5236\u63d0\u5347\u8de8\u57df\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u56fe\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5047\u8bbe\u6e90\u6807\u7b7e\u5e72\u51c0\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u6807\u7b7e\u566a\u58f0\u666e\u904d\u5b58\u5728\uff0c\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u3002", "method": "NeGPR\u91c7\u7528\u53cc\u5206\u652f\u9884\u8bad\u7ec3\uff08\u8bed\u4e49\u548c\u62d3\u6251\u5206\u652f\uff09\u548c\u5d4c\u5957\u7ec6\u5316\u673a\u5236\uff0c\u7ed3\u5408\u566a\u58f0\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNeGPR\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12.7%\u3002", "conclusion": "NeGPR\u901a\u8fc7\u566a\u58f0\u9c81\u68d2\u6027\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u57df\u81ea\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2508.00718", "pdf": "https://arxiv.org/pdf/2508.00718", "abs": "https://arxiv.org/abs/2508.00718", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "AI": {"tldr": "MOSTLY AI SDK\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u9690\u79c1\u548c\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4f46\u9690\u79c1\u3001\u4e13\u6709\u5229\u76ca\u548c\u4f26\u7406\u95ee\u9898\u9650\u5236\u4e86\u6570\u636e\u8bbf\u95ee\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eTabularARGN\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u96c6\u6210\u5dee\u5206\u9690\u79c1\u3001\u516c\u5e73\u6027\u751f\u6210\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\uff0c\u652f\u6301\u591a\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u96c6\u3002", "result": "SDK\u5728\u901f\u5ea6\u548c\u53ef\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5df2\u5feb\u901f\u90e8\u7f72\u4e3a\u4e91\u670d\u52a1\u548c\u672c\u5730\u8f6f\u4ef6\u3002", "conclusion": "MOSTLY AI SDK\u5b9e\u7528\u6027\u5f3a\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u74f6\u9888\uff0c\u63a8\u52a8\u6570\u636e\u6c11\u4e3b\u5316\u3002"}}
{"id": "2508.00734", "pdf": "https://arxiv.org/pdf/2508.00734", "abs": "https://arxiv.org/abs/2508.00734", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u4fdd\u771f\u5ea6\u5206\u5c42\u91c7\u6837\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\uff0c\u9ad8\u6548\u4f30\u8ba1\u5c0f\u5931\u6548\u6982\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u5728\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u4e2d\u4ecd\u9700\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u5728\u590d\u6742\u975e\u7ebf\u6027\u6709\u9650\u5143\u6a21\u578b\u4e2d\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u91c7\u6837\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5143\u6a21\u578b\uff0c\u4f5c\u4e3a\u4f4e\u4fdd\u771f\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4fdd\u771f\u8499\u7279\u5361\u7f57\u6846\u67b6\u4f30\u8ba1\u5931\u6548\u6982\u7387\u3002", "result": "\u5e94\u7528\u4e8e\u9ad8\u5c42\u94a2\u7ed3\u6784\u5efa\u7b51\uff0c\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u8d85\u8d8a\u6982\u7387\u66f2\u7ebf\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u3002"}}
{"id": "2508.00754", "pdf": "https://arxiv.org/pdf/2508.00754", "abs": "https://arxiv.org/abs/2508.00754", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\u7684\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u5e03\u504f\u79fb\u548cOOD\u68c0\u6d4b\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u4fe1\u606f\u52bf\u573a\u8fd1\u4f3c\u8bad\u7ec3\u96c6\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\uff0c\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u6837\u672c\u7279\u5f81\u5224\u65ad\u5206\u5e03\u504f\u79fb\u3002", "result": "\u57282D\u5408\u6210\u6570\u636e\u96c6\u548cOOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u504f\u79fb\u548cOOD\u68c0\u6d4b\u3002"}}
{"id": "2508.00758", "pdf": "https://arxiv.org/pdf/2508.00758", "abs": "https://arxiv.org/abs/2508.00758", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "categories": ["cs.LG"], "comment": "22 pages, 16 figures, 7 tables, preprint version", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u53bb\u566a\u81ea\u7f16\u7801\u5668\u7684\u65b0\u6846\u67b6DDAE\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8868\u683c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u7279\u5f81\u4ea4\u4e92\u590d\u6742\u548c\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u9002\u5e94\u6027\u548c\u91cd\u5efa\u6620\u5c04\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDDAE\u6846\u67b6\uff0c\u6574\u5408\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u5bf9\u6bd4\u5b66\u4e60\u5230\u7f16\u7801\u8fc7\u7a0b\uff0c\u4f18\u5316\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728ADBench\u768457\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDDAE\u5728\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0cPR-AUC\u548cROC-AUC\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u566a\u58f0\u7b56\u7565\u5728\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0cDDAE\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00768", "pdf": "https://arxiv.org/pdf/2508.00768", "abs": "https://arxiv.org/abs/2508.00768", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "categories": ["cs.LG"], "comment": null, "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u632f\u5e45\u7f16\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\u5728\u4e0d\u540c\u65cb\u8f6c\u95e8\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u65b9\u5f0f\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\uff08QML\uff09\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u91cf\u5b50\u7535\u8def\u7684\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u4e0d\u540c\u7f16\u7801\u65b9\u5f0f\u548c\u65cb\u8f6c\u95e8\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u632f\u5e45\u7f16\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u7684VQC\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u65cb\u8f6c\u95e8\u914d\u7f6e\u4e0b\u8bad\u7ec3\uff0c\u5e76\u5728Wine\u548cDiabetes\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u76f8\u540c\u62d3\u6251\u7ed3\u6784\u4e0b\uff0c\u6700\u4f73\u548c\u6700\u5dee\u6a21\u578b\u7684\u51c6\u786e\u7387\u5dee\u5f02\u4e3a10%\u81f330%\uff0c\u6700\u9ad8\u8fbe41%\uff0c\u7f16\u7801\u65b9\u5f0f\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u7f16\u7801\u65b9\u5f0f\u662fVQC\u6a21\u578b\u7684\u91cd\u8981\u8d85\u53c2\u6570\uff0c\u65cb\u8f6c\u95e8\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2508.00785", "pdf": "https://arxiv.org/pdf/2508.00785", "abs": "https://arxiv.org/abs/2508.00785", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "categories": ["cs.LG"], "comment": null, "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f71\u54cd\u5b66\u751fCGPA\u7684\u591a\u53d8\u91cf\u56e0\u7d20\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u95ee\u5377\u8c03\u67e5\u548c\u56e0\u679c\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u9884\u6d4b\u6a21\u578b\u548c\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eWeb\u7684\u5e94\u7528\u5e2e\u52a9\u5b66\u751f\u4f18\u5316\u5b66\u672f\u8868\u73b0\u3002", "motivation": "\u5b66\u672f\u8868\u73b0\u53d7\u591a\u79cd\u793e\u4f1a\u3001\u5b66\u672f\u548c\u8d22\u52a1\u56e0\u7d20\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u8fd9\u4e9b\u56e0\u7d20\u5e76\u5f00\u53d1\u7b56\u7565\u4ee5\u4f18\u5316\u5b66\u751fCGPA\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6784\u5efa\u56e0\u679c\u56fe\uff0c\u8fdb\u884c\u5728\u7ebf\u95ee\u5377\u8c03\u67e5\uff081050\u540d\u5b66\u751f\uff09\uff0c\u6570\u636e\u9884\u5904\u7406\u540e\u4f7f\u7528\u56de\u5f52\u548c\u5206\u7c7b\u6a21\u578b\uff08\u5982\u5cad\u56de\u5f52\u548c\u968f\u673a\u68ee\u6797\uff09\u5206\u6790\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\uff08SHAP\u3001LIME\uff09\u3002", "result": "\u5cad\u56de\u5f52\u9884\u6d4bCGPA\u7684MAE\u4e3a0.12\uff0cMSE\u4e3a0.023\uff1b\u968f\u673a\u68ee\u6797\u5206\u7c7b\u51c6\u786e\u7387\u8fbe98.68%\u3002\u5173\u952e\u56e0\u7d20\u5305\u62ec\u5b66\u4e60\u65f6\u95f4\u3001\u5956\u5b66\u91d1\u3001\u7236\u6bcd\u6559\u80b2\u80cc\u666f\u548c\u5148\u524d\u5b66\u672f\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2aWeb\u5e94\u7528\uff0c\u4e3a\u5b66\u751f\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\uff0c\u5e2e\u52a9\u4ed6\u4eec\u9884\u6d4b\u548c\u6539\u8fdb\u5b66\u672f\u8868\u73b0\u3002"}}
{"id": "2502.18148", "pdf": "https://arxiv.org/pdf/2502.18148", "abs": "https://arxiv.org/abs/2502.18148", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "AI": {"tldr": "NusaAksara\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5c3c\u8bed\u8a00\u53ca\u5176\u539f\u751f\u811a\u672c\u7684\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff0c\u5305\u542b\u591a\u79cd\u4efb\u52a1\uff0c\u5982OCR\u548c\u7ffb\u8bd1\u3002\u6570\u636e\u7531\u4e13\u5bb6\u6784\u5efa\uff0c\u8986\u76d68\u79cd\u811a\u672c\u548c7\u79cd\u8bed\u8a00\uff0c\u5305\u62ec\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u6d4b\u8bd5\u663e\u793a\u73b0\u6709NLP\u6280\u672f\u5bf9\u672c\u5730\u811a\u672c\u5904\u7406\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u5370\u5c3c\u8bed\u8a00\u548c\u811a\u672c\u4e30\u5bcc\uff0c\u4f46NLP\u7814\u7a76\u591a\u57fa\u4e8e\u7f57\u9a6c\u5316\u6587\u672c\u3002NusaAksara\u65e8\u5728\u586b\u8865\u539f\u751f\u811a\u672c\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b8\u79cd\u811a\u672c\u548c7\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982GPT-4o\u3001Llama 3.2\uff09\u7684\u6027\u80fd\u3002", "result": "\u73b0\u6709NLP\u6280\u672f\u5bf9\u5370\u5c3c\u672c\u5730\u811a\u672c\u5904\u7406\u80fd\u529b\u6781\u4f4e\uff0c\u8bb8\u591a\u4efb\u52a1\u8868\u73b0\u63a5\u8fd1\u96f6\u3002", "conclusion": "NusaAksara\u4e3a\u5370\u5c3c\u539f\u751f\u811a\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u51f8\u663e\u4e86\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.00024", "pdf": "https://arxiv.org/pdf/2508.00024", "abs": "https://arxiv.org/abs/2508.00024", "authors": ["Sebasti\u00e1n Andr\u00e9s Cajas Ord\u00f3\u00f1ez", "Luis Fernando Torres Torres", "Mario Bifulco", "Carlos Andr\u00e9s Dur\u00e1n", "Cristian Bosch", "Ricardo Sim\u00f3n Carbajo"], "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7c7b\u5e73\u8861k-means\u84b8\u998f\u548c\u9884\u8bad\u7ec3Vision Transformer\u5d4c\u5165\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728Fashion-MNIST\u548cMNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u56e0\u9ad8\u7ef4\u91cf\u5b50\u6001\u548c\u786c\u4ef6\u9650\u5236\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5d4c\u5165\u611f\u77e5\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u6d41\u7a0b\uff0c\u7ed3\u5408\u7c7b\u5e73\u8861k-means\u84b8\u998f\u548c\u9884\u8bad\u7ec3Vision Transformer\u5d4c\u5165\u3002", "result": "\u5728Fashion-MNIST\u548cMNIST\u6570\u636e\u96c6\u4e0a\uff0cViT\u5d4c\u5165\u5b9e\u73b0\u4e868.02%\u548c4.42%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u800cCNN\u7279\u5f81\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u91cf\u5b50\u6838\u4f18\u52bf\u9ad8\u5ea6\u4f9d\u8d56\u5d4c\u5165\u9009\u62e9\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2508.00027", "pdf": "https://arxiv.org/pdf/2508.00027", "abs": "https://arxiv.org/abs/2508.00027", "authors": ["Azadeh Alavi", "Fatemeh Kouchmeshki", "Abdolrahman Alavi", "Yongli Ren", "Jiayang Niu"], "title": "Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems", "categories": ["quant-ph", "cs.LG"], "comment": "Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer\n  review", "summary": "Modern recommenders describe each item with hundreds of sparse semantic tags,\nyet most quantum pipelines still map one qubit per tag, demanding well beyond\none hundred qubits, far out of reach for current noisy-intermediate-scale\nquantum (NISQ) devices and prone to deep, error-amplifying circuits. We close\nthis gap with a three-stage hybrid machine learning algorithm that compresses\ntag profiles, optimizes feature selection under a fixed qubit budget via QAOA,\nand scores recommendations with a Quantum semi-Random Forest (QsRF) built on\njust five qubits, while performing similarly to the state-of-the-art methods.\nLeveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \\%\nvariance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A\n100-tree QsRF trained on these codes matches full-feature baselines on\nICM-150/500.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u6df7\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u6807\u7b7e\u914d\u7f6e\u6587\u4ef6\u3001\u5728\u56fa\u5b9a\u91cf\u5b50\u6bd4\u7279\u9884\u7b97\u4e0b\u4f18\u5316\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u4f7f\u7528\u4ec5\u97005\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u534a\u968f\u673a\u68ee\u6797\uff08QsRF\uff09\u8fdb\u884c\u63a8\u8350\u8bc4\u5206\uff0c\u6027\u80fd\u63a5\u8fd1\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u4f7f\u7528\u5927\u91cf\u7a00\u758f\u8bed\u4e49\u6807\u7b7e\u63cf\u8ff0\u7269\u54c1\uff0c\u4f46\u4f20\u7edf\u91cf\u5b50\u65b9\u6cd5\u9700\u8981\u8fc7\u591a\u91cf\u5b50\u6bd4\u7279\uff0c\u8d85\u51fa\u5f53\u524dNISQ\u8bbe\u5907\u7684\u5904\u7406\u80fd\u529b\u4e14\u6613\u53d7\u8bef\u5dee\u5f71\u54cd\u3002", "method": "1. \u4f7f\u7528SVD\u548ck-means\u538b\u7f29\u6807\u7b7e\u914d\u7f6e\u6587\u4ef6\uff1b2. \u901a\u8fc7\u6df1\u5ea63\u7684QAOA\u4f18\u5316\u7279\u5f81\u9009\u62e9\uff1b3. \u8bad\u7ec3100\u68f5\u6811\u7684QsRF\u8fdb\u884c\u63a8\u8350\u8bc4\u5206\u3002", "result": "\u5728ICM-150/500\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u5168\u7279\u5f81\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u97005\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u8fd1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5f53\u524dNISQ\u8bbe\u5907\u3002"}}
{"id": "2508.00029", "pdf": "https://arxiv.org/pdf/2508.00029", "abs": "https://arxiv.org/abs/2508.00029", "authors": ["Azadeh Alavi", "Sanduni Jayasinghe", "Mojtaba Mahmoodian", "Sam Mazaheri", "John Thangarajah", "Sujeeva Setunge"], "title": "Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins", "categories": ["quant-ph", "cs.LG"], "comment": "Submitted to Scientific Report", "summary": "Large-scale civil structures, such as bridges, pipelines, and offshore\nplatforms, are vital to modern infrastructure, where unexpected failures can\ncause significant economic and safety repercussions. Although finite element\n(FE) modeling is widely used for real-time structural health monitoring (SHM),\nits high computational cost and the complexity of inverse FE analysis, where\nlow dimensional sensor data must map onto high-dimensional displacement or\nstress fields pose ongoing challenges. Here, we propose a hybrid quantum\nclassical multilayer perceptron (QMLP) framework to tackle these issues and\nfacilitate swift updates to digital twins across a range of structural\napplications.\n  Our approach embeds sensor data using symmetric positive definite (SPD)\nmatrices and polynomial features, yielding a representation well suited to\nquantum processing. A parameterized quantum circuit (PQC) transforms these\nfeatures, and the resultant quantum outputs feed into a classical neural\nnetwork for final inference. By fusing quantum capabilities with classical\nmodeling, the QMLP handles large scale inverse FE mapping while preserving\ncomputational viability.\n  Through extensive experiments on a bridge, we demonstrate that the QMLP\nachieves a mean squared error (MSE) of 0.0000000000316, outperforming purely\nclassical baselines with a large margin. These findings confirm the potential\nof quantum-enhanced methods for real time SHM, establishing a pathway toward\nmore efficient, scalable digital twins that can robustly monitor and diagnose\nstructural integrity in near real time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u591a\u5c42\u611f\u77e5\u5668\uff08QMLP\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6c11\u7528\u7ed3\u6784\uff08\u5982\u6865\u6881\u3001\u7ba1\u9053\u7b49\uff09\u7684\u610f\u5916\u6545\u969c\u53ef\u80fd\u5e26\u6765\u91cd\u5927\u7ecf\u6d4e\u548c\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u6709\u9650\u5143\u5efa\u6a21\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u548c\u591a\u9879\u5f0f\u7279\u5f81\u5d4c\u5165\u4f20\u611f\u5668\u6570\u636e\uff0c\u7ed3\u5408\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u548c\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9006\u6709\u9650\u5143\u6620\u5c04\u3002", "result": "\u5728\u6865\u6881\u5b9e\u9a8c\u4e2d\uff0cQMLP\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u4e3a0.0000000000316\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u7ecf\u5178\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00048", "pdf": "https://arxiv.org/pdf/2508.00048", "abs": "https://arxiv.org/abs/2508.00048", "authors": ["Ammar Daskin"], "title": "Dimension reduction with structure-aware quantum circuits for hybrid machine learning", "categories": ["quant-ph", "cs.LG"], "comment": "Any comments are welcome! The simulation code is provided at\n  https://github.com/adaskin/structure-aware-circuits", "summary": "Schmidt decomposition of a vector can be understood as writing the singular\nvalue decomposition (SVD) in vector form. A vector can be written as a linear\ncombination of tensor product of two dimensional vectors by recursively\napplying Schmidt decompositions via SVD to all subsystems. Given a vector\nexpressed as a linear combination of tensor products, using only the $k$\nprincipal terms yields a $k$-rank approximation of the vector. Therefore,\nwriting a vector in this reduced form allows to retain most important parts of\nthe vector while removing small noises from it, analogous to SVD-based\ndenoising.\n  In this paper, we show that quantum circuits designed based on a value $k$\n(determined from the tensor network decomposition of the mean vector of the\ntraining sample) can approximate the reduced-form representations of entire\ndatasets. We then employ this circuit ansatz with a classical neural network\nhead to construct a hybrid machine learning model. Since the output of the\nquantum circuit for an $2^n$ dimensional vector is an $n$ dimensional\nprobability vector, this provides an exponential compression of the input and\npotentially can reduce the number of learnable parameters for training\nlarge-scale models. We use datasets provided in the Python scikit-learn module\nfor the experiments. The results confirm the quantum circuit is able to\ncompress data successfully to provide effective $k$-rank approximations to the\nclassical processing component.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchmidt\u5206\u89e3\u548c\u91cf\u5b50\u7535\u8def\u7684\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u6570\u636e\u538b\u7f29\u548c\u964d\u566a\u3002", "motivation": "\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u6570\u636e\u7684\u9ad8\u6548\u538b\u7f29\uff0c\u51cf\u5c11\u5927\u89c4\u6a21\u6a21\u578b\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "method": "\u7ed3\u5408Schmidt\u5206\u89e3\u548c\u91cf\u5b50\u7535\u8def\uff0c\u751f\u6210k\u79e9\u8fd1\u4f3c\u8868\u793a\uff0c\u5e76\u4e0e\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cf\u5b50\u7535\u8def\u80fd\u6709\u6548\u538b\u7f29\u6570\u636e\u5e76\u63d0\u4f9bk\u79e9\u8fd1\u4f3c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u538b\u7f29\u548c\u964d\u566a\u65b9\u6848\u3002"}}
{"id": "2508.00091", "pdf": "https://arxiv.org/pdf/2508.00091", "abs": "https://arxiv.org/abs/2508.00091", "authors": ["Chandler Smith", "HanQin Cai", "Abiy Tasissa"], "title": "Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence", "categories": ["math.OC", "cs.CG", "cs.LG"], "comment": "54 pages, 6 figures", "summary": "The problem of recovering a configuration of points from partial pairwise\ndistances, referred to as the Euclidean Distance Geometry (EDG) problem, arises\nin a broad range of applications, including sensor network localization,\nmolecular conformation, and manifold learning. In this paper, we propose a\nRiemannian optimization framework for solving the EDG problem by formulating it\nas a low-rank matrix completion task over the space of positive semi-definite\nGram matrices. The available distance measurements are encoded as expansion\ncoefficients in a non-orthogonal basis, and optimization over the Gram matrix\nimplicitly enforces geometric consistency through the triangle inequality, a\nstructure inherited from classical multidimensional scaling. Under a Bernoulli\nsampling model for observed distances, we prove that Riemannian gradient\ndescent on the manifold of rank-$r$ matrices locally converges linearly with\nhigh probability when the sampling probability satisfies $p \\geq\n\\mathcal{O}(\\nu^2 r^2 \\log(n)/n)$, where $\\nu$ is an EDG-specific incoherence\nparameter. Furthermore, we provide an initialization candidate using a one-step\nhard thresholding procedure that yields convergence, provided the sampling\nprobability satisfies $p \\geq \\mathcal{O}(\\nu r^{3/2} \\log^{3/4}(n)/n^{1/4})$.\nA key technical contribution of this work is the analysis of a symmetric linear\noperator arising from a dual basis expansion in the non-orthogonal basis, which\nrequires a novel application of the Hanson--Wright inequality to establish an\noptimal restricted isometry property in the presence of coupled terms.\nEmpirical evaluations on synthetic data demonstrate that our algorithm achieves\ncompetitive performance relative to state-of-the-art methods. Moreover, we\npropose a novel notion of matrix incoherence tailored to the EDG setting and\nprovide robustness guarantees for our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u51e0\u4f55\u95ee\u9898\u8f6c\u5316\u4e3a\u534a\u6b63\u5b9aGram\u77e9\u9635\u7684\u4f4e\u79e9\u77e9\u9635\u8865\u5168\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u6210\u5bf9\u8ddd\u79bb\u6062\u590d\u7684\u95ee\u9898\u3002", "motivation": "\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u51e0\u4f55\u95ee\u9898\u5728\u4f20\u611f\u5668\u7f51\u7edc\u5b9a\u4f4d\u3001\u5206\u5b50\u6784\u8c61\u548c\u6d41\u5f62\u5b66\u4e60\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u9ece\u66fc\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3aGram\u77e9\u9635\u7684\u4f4e\u79e9\u8865\u5168\u4efb\u52a1\uff0c\u5229\u7528\u975e\u6b63\u4ea4\u57fa\u7684\u5c55\u5f00\u7cfb\u6570\u7f16\u7801\u8ddd\u79bb\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u9ece\u66fc\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u5c40\u90e8\u7ebf\u6027\u6536\u655b\u3002", "result": "\u5728\u4f2f\u52aa\u5229\u91c7\u6837\u6a21\u578b\u4e0b\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u7279\u5b9a\u91c7\u6837\u6982\u7387\u4e0b\u80fd\u5b9e\u73b0\u5c40\u90e8\u7ebf\u6027\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u51e0\u4f55\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00101", "pdf": "https://arxiv.org/pdf/2508.00101", "abs": "https://arxiv.org/abs/2508.00101", "authors": ["Alena Kopani\u010d\u00e1kov\u00e1", "Youngkyu Lee", "George Em Karniadakis"], "title": "Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method", "categories": ["math.NA", "cs.LG", "cs.NA", "math.OC", "65M55, 68T05, 49K20"], "comment": "31 pages", "summary": "We propose a new deflation strategy to accelerate the convergence of the\npreconditioned conjugate gradient(PCG) method for solving parametric\nlarge-scale linear systems of equations. Unlike traditional deflation\ntechniques that rely on eigenvector approximations or recycled Krylov\nsubspaces, we generate the deflation subspaces using operator learning,\nspecifically the Deep Operator Network~(DeepONet). To this aim, we introduce\ntwo complementary approaches for assembling the deflation operators. The first\napproach approximates near-null space vectors of the discrete PDE operator\nusing the basis functions learned by the DeepONet. The second approach directly\nleverages solutions predicted by the DeepONet. To further enhance convergence,\nwe also propose several strategies for prescribing the sparsity pattern of the\ndeflation operator. A comprehensive set of numerical experiments encompassing\nsteady-state, time-dependent, scalar, and vector-valued problems posed on both\nstructured and unstructured geometries is presented and demonstrates the\neffectiveness of the proposed DeepONet-based deflated PCG method, as well as\nits generalization across a wide range of model parameters and problem\nresolutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eDeepONet\u7684\u653e\u6c14\u7b56\u7565\uff0c\u52a0\u901fPCG\u65b9\u6cd5\u6c42\u89e3\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u7684\u6536\u655b\u6027\u3002", "motivation": "\u4f20\u7edf\u653e\u6c14\u6280\u672f\u4f9d\u8d56\u7279\u5f81\u5411\u91cf\u8fd1\u4f3c\u6216Krylov\u5b50\u7a7a\u95f4\uff0c\u800c\u65b0\u65b9\u6cd5\u5229\u7528\u7b97\u5b50\u5b66\u4e60\u751f\u6210\u653e\u6c14\u5b50\u7a7a\u95f4\uff0c\u63d0\u5347\u6548\u7387\u3002", "method": "\u901a\u8fc7DeepONet\u5b66\u4e60\u57fa\u51fd\u6570\u6216\u76f4\u63a5\u9884\u6d4b\u89e3\uff0c\u6784\u5efa\u653e\u6c14\u7b97\u5b50\uff0c\u5e76\u63d0\u51fa\u7a00\u758f\u6a21\u5f0f\u7b56\u7565\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u95ee\u9898\u548c\u53c2\u6570\u4e0b\u6709\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u3002", "conclusion": "DeepONet\u653e\u6c14PCG\u65b9\u6cd5\u5728\u52a0\u901f\u6536\u655b\u548c\u6cdb\u5316\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00106", "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08SecRL\uff09\uff0c\u7528\u4e8e\u5728\u6ee1\u8db3\u5b89\u5168\u6027\u548c\u4e0d\u900f\u660e\u6027\u7ea6\u675f\u7684\u6761\u4ef6\u4e0b\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u63a2\u7d22\u57fa\u4e8e\u8d85\u5c5e\u6027\u7684\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001Boltzmann softmax RL\u548cHyperTWTL\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u4ee5Markov Decision Process\uff08MDP\uff09\u5efa\u6a21\u4ee3\u7406\u52a8\u6001\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00110", "pdf": "https://arxiv.org/pdf/2508.00110", "abs": "https://arxiv.org/abs/2508.00110", "authors": ["Katharine M. Clark", "Paul D. McNicholas"], "title": "funOCLUST: Clustering Functional Data with Outliers", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Functional data present unique challenges for clustering due to their\ninfinite-dimensional nature and potential sensitivity to outliers. An extension\nof the OCLUST algorithm to the functional setting is proposed to address these\nissues. The approach leverages the OCLUST framework, creating a robust method\nto cluster curves and trim outliers. The methodology is evaluated on both\nsimulated and real-world functional datasets, demonstrating strong performance\nin clustering and outlier identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOCLUST\u7b97\u6cd5\u7684\u529f\u80fd\u6570\u636e\u805a\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u548c\u5f02\u5e38\u503c\u95ee\u9898\u3002", "motivation": "\u529f\u80fd\u6570\u636e\u7684\u9ad8\u7ef4\u6027\u548c\u5bf9\u5f02\u5e38\u503c\u7684\u654f\u611f\u6027\u7ed9\u805a\u7c7b\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u6269\u5c55OCLUST\u7b97\u6cd5\uff0c\u521b\u5efa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u529f\u80fd\u6570\u636e\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u4fee\u526a\u65b9\u6cd5\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u529f\u80fd\u6570\u636e\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u5904\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.00135", "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u5229\u7528\u773c\u5468\u533a\u57df\u7684\u989c\u8272\u56fe\u50cf\u8fdb\u884c\u6027\u522b\u5206\u7c7b\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0899%\u548c96%\uff09\u3002", "motivation": "\u6027\u522b\u5206\u7c7b\u5728\u5b89\u5168\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5316\u5986\u54c1\u548c\u4f2a\u88c5\u7b49\u56e0\u7d20\u53ef\u80fd\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e13\u6ce8\u4e8e\u773c\u5468\u533a\u57df\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528CNN\u6a21\u578b\u5206\u6790\u773c\u5468\u533a\u57df\u7684\u989c\u8272\u56fe\u50cf\uff0c\u63d0\u53d6\u5173\u952e\u7279\u5f81\u8fdb\u884c\u6027\u522b\u5206\u7c7b\uff0c\u5e76\u5728CVBL\u548c\uff08Female and Male\uff09\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728CVBL\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u7684\u51c6\u786e\u7387\uff0c\u5728\uff08Female and Male\uff09\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u53c2\u6570\u91cf\u8f83\u5c11\uff087,235,089\uff09\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6027\u522b\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5728\u5b89\u5168\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00154", "pdf": "https://arxiv.org/pdf/2508.00154", "abs": "https://arxiv.org/abs/2508.00154", "authors": ["Babak Esmaeili", "Hamidreza Modares", "Stefano Di Cairano"], "title": "Data-Driven Motion Planning for Uncertain Nonlinear Systems", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper proposes a data-driven motion-planning framework for nonlinear\nsystems that constructs a sequence of overlapping invariant polytopes. Around\neach randomly sampled waypoint, the algorithm identifies a convex admissible\nregion and solves data-driven linear-matrix-inequality problems to learn\nseveral ellipsoidal invariant sets together with their local state-feedback\ngains. The convex hull of these ellipsoids, still invariant under a\npiece-wise-affine controller obtained by interpolating the gains, is then\napproximated by a polytope. Safe transitions between nodes are ensured by\nverifying the intersection of consecutive convex-hull polytopes and introducing\nan intermediate node for a smooth transition. Control gains are interpolated in\nreal time via simplex-based interpolation, keeping the state inside the\ninvariant polytopes throughout the motion. Unlike traditional approaches that\nrely on system dynamics models, our method requires only data to compute safe\nregions and design state-feedback controllers. The approach is validated\nthrough simulations, demonstrating the effectiveness of the proposed method in\nachieving safe, dynamically feasible paths for complex nonlinear systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u91cd\u53e0\u4e0d\u53d8\u591a\u9762\u4f53\u5e8f\u5217\u5b9e\u73b0\u5b89\u5168\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u4ec5\u9700\u6570\u636e\u5373\u53ef\u8ba1\u7b97\u5b89\u5168\u533a\u57df\u5e76\u8bbe\u8ba1\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u3002", "method": "\u56f4\u7ed5\u968f\u673a\u91c7\u6837\u7684\u8def\u5f84\u70b9\uff0c\u7b97\u6cd5\u8bc6\u522b\u51f8\u53ef\u5bb9\u8bb8\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u95ee\u9898\u5b66\u4e60\u591a\u4e2a\u692d\u7403\u4e0d\u53d8\u96c6\u53ca\u5176\u5c40\u90e8\u72b6\u6001\u53cd\u9988\u589e\u76ca\u3002\u8fd9\u4e9b\u692d\u7403\u7684\u51f8\u5305\u5728\u591a\u6bb5\u4eff\u5c04\u63a7\u5236\u5668\u4e0b\u4ecd\u4fdd\u6301\u4e0d\u53d8\uff0c\u5e76\u8fd1\u4f3c\u4e3a\u591a\u9762\u4f53\u3002\u901a\u8fc7\u9a8c\u8bc1\u8fde\u7eed\u51f8\u5305\u591a\u9762\u4f53\u7684\u4ea4\u96c6\u5e76\u5f15\u5165\u4e2d\u95f4\u8282\u70b9\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u52a8\u6001\u53ef\u884c\u8def\u5f84\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u65e0\u9700\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4ec5\u4f9d\u8d56\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2508.00155", "pdf": "https://arxiv.org/pdf/2508.00155", "abs": "https://arxiv.org/abs/2508.00155", "authors": ["Tomasz Szczepa\u0144ski", "Szymon P\u0142otka", "Michal K. Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemys\u0142aw Korzeniowski", "Tomasz Trzci\u0144ski", "Arkadiusz Sitek"], "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "summary": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains\nchallenging, especially for fine structures like root apices, which is critical\nfor assessing root resorption in orthodontics. We introduce GEPAR3D, a novel\napproach that unifies instance detection and multi-class segmentation into a\nsingle step tailored to improve root segmentation. Our method integrates a\nStatistical Shape Model of dentition as a geometric prior, capturing anatomical\ncontext and morphological consistency without enforcing restrictive adjacency\nconstraints. We leverage a deep watershed method, modeling each tooth as a\ncontinuous 3D energy basin encoding voxel distances to boundaries. This\ninstance-aware representation ensures accurate segmentation of narrow, complex\nroot apices. Trained on publicly available CBCT scans from a single center, our\nmethod is evaluated on external test sets from two in-house and two public\nmedical centers. GEPAR3D achieves the highest overall segmentation performance,\naveraging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the\nsecond-best method) and increasing recall to 95.2% (+9.5%) across all test\nsets. Qualitative analyses demonstrated substantial improvements in root\nsegmentation quality, indicating significant potential for more accurate root\nresorption assessment and enhanced clinical decision-making in orthodontics. We\nprovide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.", "AI": {"tldr": "GEPAR3D\u662f\u4e00\u79cd\u7ed3\u5408\u5b9e\u4f8b\u68c0\u6d4b\u548c\u591a\u7c7b\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CBCT\u4e2d\u7259\u9f7f\u5206\u5272\u7684\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u6839\u5c16\u5206\u5272\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "CBCT\u4e2d\u7684\u7259\u9f7f\u5206\u5272\uff0c\u5c24\u5176\u662f\u6839\u5c16\u7b49\u7cbe\u7ec6\u7ed3\u6784\u7684\u5206\u5272\uff0c\u5bf9\u6b63\u7578\u4e2d\u7684\u6839\u5438\u6536\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6311\u6218\u3002", "method": "GEPAR3D\u5c06\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u7ed3\u5408\u6df1\u5ea6\u5206\u6c34\u5cad\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u7259\u9f7f\u5efa\u6a21\u4e3a3D\u80fd\u91cf\u76c6\u5730\uff0c\u5b9e\u73b0\u5b9e\u4f8b\u611f\u77e5\u5206\u5272\u3002", "result": "\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\uff0cGEPAR3D\u7684\u5e73\u5747Dice\u76f8\u4f3c\u7cfb\u6570\u8fbe95.0%\uff0c\u53ec\u56de\u7387\u63d0\u53479.5%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "GEPAR3D\u5728\u6839\u5206\u5272\u8d28\u91cf\u4e0a\u5927\u5e45\u63d0\u5347\uff0c\u6709\u671b\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u6839\u5438\u6536\u8bc4\u4f30\uff0c\u5e76\u5df2\u5f00\u6e90\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.00159", "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u901a\u8fc7\u660e\u786e\u8981\u6c42AI\u8d4b\u80fd\u4eba\u7c7b\u5e76\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u6743\u529b\u5e73\u8861\uff0c\u4ee5\u4fc3\u8fdb\u5b89\u5168\u4e0e\u798f\u7949\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u3001\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u8ba1\u7b97\u6216\u8fd1\u4f3c\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76AI\u5b89\u5168\u4e2d\u7684\u6743\u529b\u6982\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u8d4b\u80fd\u4eba\u7c7b\u548c\u7ba1\u7406\u6743\u529b\u5e73\u8861\uff0c\u540c\u65f6\u63d0\u5347\u5b89\u5168\u4e0e\u798f\u7949\u3002", "method": "\u91c7\u7528\u90e8\u5206\u516c\u7406\u5316\u65b9\u6cd5\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\uff0c\u8003\u8651\u4eba\u7c7b\u6709\u9650\u7406\u6027\u4e0e\u793e\u4f1a\u89c4\u8303\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u3002", "result": "\u5728\u591a\u79cd\u60c5\u5883\u4e0b\u9a8c\u8bc1\u76ee\u6807\u51fd\u6570\u7684\u6548\u679c\uff0c\u8868\u660e\u5176\u53ef\u80fd\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002", "conclusion": "\u9002\u5ea6\u6700\u5927\u5316\u4eba\u7c7b\u6743\u529b\u7684\u805a\u5408\u6307\u6807\u53ef\u80fd\u662fAI\u7cfb\u7edf\u7684\u6709\u76ca\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u6548\u7528\u76ee\u6807\u66f4\u5b89\u5168\u3002"}}
{"id": "2508.00197", "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u56fe\u201c\u8c31\u7cfb\u201d\u7684\u5b9a\u4e49\uff0c\u7528\u4e8e\u6784\u5efa\u5c42\u6b21\u5316\u589e\u957f\u7684\u56fe\u6a21\u578b\uff0c\u652f\u6301\u9ad8\u6548\u7684\u4ee3\u6570\u64cd\u4f5c\u548c\u7c7b\u578b\u6784\u9020\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u5c42\u6b21\u5316\u67b6\u6784\u8bbe\u8ba1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e3a\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u79d1\u5b66\u4e2d\u7684\u5c42\u6b21\u5316\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e00\u79cd\u6570\u5b66\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u6548\u7684\u56fe\u64cd\u4f5c\u548c\u7c7b\u578b\u6784\u9020\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5b9a\u4e49\u5c42\u6b21\u5316\u589e\u957f\u7684\u56fe\u8c31\u7cfb\u3001\u5f15\u5165\u53cc\u5206\u56fe\u8fde\u63a5\u3001\u5ef6\u957f\u6620\u5c04\u3001\u4ee5\u53ca\u63a8\u5bfc\u4f4e\u6210\u672c\u7684\u201c\u9aa8\u67b6\u201d\u4ee3\u6570\u64cd\u4f5c\u548c\u7c7b\u578b\u6784\u9020\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u9aa8\u67b6\u64cd\u4f5c\u5177\u6709\u7c7b\u4f3c\u6807\u51c6\u64cd\u4f5c\u7684\u4ee3\u6570\u6027\u8d28\uff0c\u5e76\u80fd\u903c\u8fd1\u8fde\u7eed\u6781\u9650\u5bf9\u8c61\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u7f51\u683c\u6570\u503c\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u4e3a\u5c42\u6b21\u5316\u6a21\u578b\u67b6\u6784\u548c\u5c40\u90e8\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b66\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00213", "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "SAM-PTx\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668Parallel-Text\uff0c\u5c06CLIP\u6587\u672c\u5d4c\u5165\u5f15\u5165SAM\uff0c\u5b9e\u73b0\u8bed\u4e49\u5f15\u5bfc\u7684\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7a7a\u95f4\u63d0\u793a\u3002", "motivation": "\u63a2\u7d22\u8bed\u4e49\u6587\u672c\u63d0\u793a\u5728SAM\u4e2d\u7684\u6f5c\u529b\uff0c\u5f25\u8865\u4f20\u7edf\u7a7a\u95f4\u63d0\u793a\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u9002\u914d\u5668Parallel-Text\uff0c\u5c06\u6587\u672c\u5d4c\u5165\u6ce8\u5165SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4ec5\u4fee\u6539MLP\u5e76\u884c\u5206\u652f\u3002", "result": "\u5728COD10K\u3001COCO\u548cADE20K\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u7eaf\u7a7a\u95f4\u63d0\u793a\u57fa\u7ebf\u3002", "conclusion": "\u8bed\u4e49\u6761\u4ef6\u96c6\u6210\u5230SAM\u67b6\u6784\u4e2d\uff0c\u4e3a\u9ad8\u6548\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2508.00218", "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u5229\u7528\u5bf9\u8c61\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7Segment Anything Model\u6216\u65e0\u76d1\u7763\u524d\u666f\u63d0\u53d6\u65b9\u6cd5\u5b9e\u73b0\u6539\u8fdb\u3002", "motivation": "\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u56fe\u50cf\u6a21\u7cca\u6027\uff08\u5982\u591a\u5bf9\u8c61\u6216\u590d\u6742\u80cc\u666f\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5bf9\u8c61\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u6765\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "method": "\u901a\u8fc7Segment Anything Model\uff08\u4ec5\u9700\u6807\u8bb0\u5bf9\u8c61\u7684\u4e00\u4e2a\u50cf\u7d20\uff09\u6216\u65e0\u76d1\u7763\u524d\u666f\u5bf9\u8c61\u63d0\u53d6\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u8c61\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u5229\u7528\u5bf9\u8c61\u7684\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f\u662f\u63d0\u5347\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e14\u53ef\u901a\u8fc7\u7b80\u5355\u6807\u8bb0\u6216\u65e0\u76d1\u7763\u65b9\u6cd5\u5b9e\u73b0\u3002"}}
{"id": "2508.00222", "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u63a2\u7d22\u548c\u5916\u90e8\u6570\u636e\uff0c\u89e3\u51b3\u4e86RLVR\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "RLVR\u65b9\u6cd5\u56e0\u56fa\u6709\u7684\u7b56\u7565\u9650\u5236\u548c\u7a00\u758f\u5956\u52b1\uff0c\u96be\u4ee5\u7a81\u7834\u57fa\u7840LLM\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u751a\u81f3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002RL-PLUS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RL-PLUS\u7ed3\u5408\u4e86\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u4ee5\u5229\u7528\u5916\u90e8\u6570\u636e\u5e76\u5f15\u5bfc\u6a21\u578b\u63a2\u7d22\u9ad8\u4ef7\u503c\u8def\u5f84\u3002", "result": "RL-PLUS\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e76\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u534721.1%\u81f369.2%\u3002", "conclusion": "RL-PLUS\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.00247", "pdf": "https://arxiv.org/pdf/2508.00247", "abs": "https://arxiv.org/abs/2508.00247", "authors": ["Sergei Gleyzer", "Hanh Nguyen", "Dinesh P. Ramakrishnan", "Eric A. F. Reinhardt"], "title": "Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": "15 pages, 3 figures", "summary": "The Kolmogorov-Arnold representation theorem states that any continuous\nmultivariable function can be exactly represented as a finite superposition of\ncontinuous single variable functions. Subsequent simplifications of this\nrepresentation involve expressing these functions as parameterized sums of a\nsmaller number of unique monotonic functions. These developments led to the\nproof of the universal approximation capabilities of multilayer perceptron\nnetworks with sigmoidal activations, forming the alternative theoretical\ndirection of most modern neural networks.\n  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an\nalternative to multilayer perceptrons. KANs feature learnable nonlinear\nactivations applied directly to input values, modeled as weighted sums of basis\nspline functions. This approach replaces the linear transformations and\nsigmoidal post-activations used in traditional perceptrons. Subsequent works\nhave explored alternatives to spline-based activations. In this work, we\npropose a novel KAN variant by replacing both the inner and outer functions in\nthe Kolmogorov-Arnold representation with weighted sinusoidal functions of\nlearnable frequencies. Inspired by simplifications introduced by Lorentz and\nSprecher, we fix the phases of the sinusoidal activations to linearly spaced\nconstant values and provide a proof of its theoretical validity. We also\nconduct numerical experiments to evaluate its performance on a range of\nmultivariable functions, comparing it with fixed-frequency Fourier transform\nmethods and multilayer perceptrons (MLPs). We show that it outperforms the\nfixed-frequency Fourier transform and achieves comparable performance to MLPs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u6b63\u5f26\u51fd\u6570\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u53d8\u4f53\uff0c\u66ff\u4ee3\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u56fa\u5b9a\u9891\u7387\u5085\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\uff0c\u4e0eMLP\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u63a2\u7d22KAN\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u7b80\u5316Kolmogorov-Arnold\u8868\u793a\u4e2d\u7684\u5185\u5916\u51fd\u6570\u4e3a\u52a0\u6743\u6b63\u5f26\u51fd\u6570\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u6b63\u5f26\u51fd\u6570\u66ff\u6362KAN\u4e2d\u7684\u5185\u5916\u51fd\u6570\uff0c\u56fa\u5b9a\u76f8\u4f4d\u4e3a\u7ebf\u6027\u95f4\u9694\u5e38\u6570\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u9a8c\u8bc1\u548c\u6570\u503c\u5b9e\u9a8c\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u56fa\u5b9a\u9891\u7387\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4e0eMLP\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684KAN\u53d8\u4f53\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00250", "pdf": "https://arxiv.org/pdf/2508.00250", "abs": "https://arxiv.org/abs/2508.00250", "authors": ["Victor D. Martinez", "Vidya Manian", "Sudhir Malik"], "title": "Jet Image Generation in High Energy Physics Using Diffusion Models", "categories": ["hep-ph", "cs.AI", "cs.CV", "cs.LG"], "comment": "The paper is under review at IEEE Transactions in Nuclear Science", "summary": "This article presents, for the first time, the application of diffusion\nmodels for generating jet images corresponding to proton-proton collision\nevents at the Large Hadron Collider (LHC). The kinematic variables of quark,\ngluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset\nare mapped to two-dimensional image representations. Diffusion models are\ntrained on these images to learn the spatial distribution of jet constituents.\nWe compare the performance of score-based diffusion models and consistency\nmodels in accurately generating class-conditional jet images. Unlike approaches\nbased on latent distributions, our method operates directly in image space. The\nfidelity of the generated images is evaluated using several metrics, including\nthe Fr\\'echet Inception Distance (FID), which demonstrates that consistency\nmodels achieve higher fidelity and generation stability compared to score-based\ndiffusion models. These advancements offer significant improvements in\ncomputational efficiency and generation accuracy, providing valuable tools for\nHigh Energy Physics (HEP) research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u751f\u6210LHC\u8d28\u5b50-\u8d28\u5b50\u78b0\u649e\u4e8b\u4ef6\u7684\u55b7\u6ce8\u56fe\u50cf\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u548c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u540e\u8005\u5728\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u55b7\u6ce8\u56fe\u50cf\uff0c\u4ee5\u6539\u8fdb\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u51c6\u786e\u6027\u3002", "method": "\u5c06\u55b7\u6ce8\u7684\u52a8\u529b\u5b66\u53d8\u91cf\u6620\u5c04\u4e3a\u4e8c\u7ef4\u56fe\u50cf\uff0c\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b66\u4e60\u55b7\u6ce8\u6210\u5206\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u5e76\u6bd4\u8f83\u5206\u6570\u6269\u6563\u6a21\u578b\u548c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u4e00\u81f4\u6027\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u5206\u6570\u6269\u6563\u6a21\u578b\uff0cFID\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u80fd\u7269\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5de5\u5177\uff0c\u4e00\u81f4\u6027\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.00267", "pdf": "https://arxiv.org/pdf/2508.00267", "abs": "https://arxiv.org/abs/2508.00267", "authors": ["Molly Noel", "Gabriel Mancino-Ball", "Yangyang Xu"], "title": "Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks", "categories": ["math.OC", "cs.LG"], "comment": "32 pages", "summary": "Graph convolutional networks (GCNs) are a powerful tool for graph\nrepresentation learning. Due to the recursive neighborhood aggregations\nemployed by GCNs, efficient training methods suffer from a lack of theoretical\nguarantees or are missing important practical elements from modern deep\nlearning algorithms, such as adaptivity and momentum. In this paper, we present\nseveral neighbor-sampling (NS) based Adam-type stochastic methods for solving a\nnonconvex GCN training problem. We utilize the control variate technique\nproposed by [1] to reduce the stochastic error caused by neighbor sampling.\nUnder standard assumptions for Adam-type methods, we show that our methods\nenjoy the optimal convergence rate. In addition, we conduct extensive numerical\nexperiments on node classification tasks with several benchmark datasets. The\nresults demonstrate superior performance of our methods over classic NS-based\nSGD that also uses the control-variate technique, especially for large-scale\ngraph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90bb\u5c45\u91c7\u6837\uff08NS\uff09\u7684Adam\u578b\u968f\u673a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u51f8GCN\u8bad\u7ec3\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6280\u672f\u51cf\u5c11\u91c7\u6837\u8bef\u5dee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "GCN\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u6216\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u81ea\u9002\u5e94\u6027\u548c\u52a8\u91cf\uff09\u7684\u5173\u952e\u5b9e\u8df5\u5143\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u90bb\u5c45\u91c7\u6837\uff08NS\uff09\u7ed3\u5408Adam\u578b\u968f\u673a\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u63a7\u5236\u53d8\u91cf\u6280\u672f\u51cf\u5c11\u91c7\u6837\u8bef\u5dee\u3002", "result": "\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\uff0c\u65b9\u6cd5\u5177\u6709\u6700\u4f18\u6536\u655b\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edfNS-SGD\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21GCN\u8bad\u7ec3\u3002"}}
{"id": "2508.00300", "pdf": "https://arxiv.org/pdf/2508.00300", "abs": "https://arxiv.org/abs/2508.00300", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.", "AI": {"tldr": "MetaExplainer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u751f\u6210\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u89e3\u91ca\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u4e0e\u7528\u6237\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff1a1) \u7528LLM\u5206\u89e3\u7528\u6237\u95ee\u9898\uff1b2) \u7528\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u751f\u6210\u7cfb\u7edf\u63a8\u8350\uff1b3) \u5408\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u4f7f\u7528Explanation Ontology\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u548c\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u5728\u95ee\u9898\u91cd\u6784\u3001\u6a21\u578b\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u548c\u81ea\u7136\u8bed\u8a00\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff08F1-score 59.06%\uff0c\u5fe0\u5b9e\u5ea670%\uff0c\u4e0a\u4e0b\u6587\u5229\u7528\u738767%\uff09\u3002\u7528\u6237\u7814\u7a76\u652f\u6301\u5176\u521b\u9020\u6027\u548c\u5168\u9762\u6027\u3002", "conclusion": "MetaExplainer\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u53ef\u8ffd\u6eaf\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u91ca\u7c7b\u578b\u548c\u9886\u57df\uff0c\u6709\u671b\u63d0\u5347AI\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.00305", "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "categories": ["cs.CL", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u4f18\u5316\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u548ctoken\u4e22\u5f03\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7ec4\u5408\u53ef\u80fd\u5bf9\u5927\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86F1\u5206\u6570\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u8d44\u6e90\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u65b9\u9762\u7684\u6311\u6218\uff0c\u63a2\u7d22\u4f18\u5316\u6280\u672f\u7684\u5b9e\u9645\u6548\u679c\u53ca\u5176\u7ec4\u5408\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u5bf9\u4e24\u79cd\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7684LLM\u67b6\u6784\u8fdb\u884c\u4f18\u5316\u65b9\u6cd5\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u7ec4\u5408\u6280\u672f\u7684\u6027\u80fd\uff0c\u968f\u540e\u572870B\u53c2\u6570\u6a21\u578b\u4e0a\u7814\u7a76\u6269\u5c55\u6027\u3002", "result": "\u53d1\u73b0\u4f18\u5316\u65b9\u6cd5\u7684\u7ec4\u5408\u53ef\u80fd\u5bf9\u5927\u6a21\u578b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0cF1\u5206\u6570\u63a9\u76d6\u4e86\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u5206\u6790\u548c\u4efb\u52a1\u7279\u5b9a\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5728\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u6269\u5c55\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2508.00319", "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u5f31\u6a21\u578b\u7684\u6743\u91cd\u63d2\u503c\uff0c\u5e73\u8861\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u4e0e\u6587\u672c\u7f16\u8f91\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982CFG\u548cAG\uff09\u5728\u5c11\u91cf\u56fe\u50cf\u5fae\u8c03\u65f6\u65e0\u6cd5\u6709\u6548\u5e73\u8861\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u4e0e\u6a21\u578b\u539f\u59cb\u77e5\u8bc6\u4fdd\u7559\u3002", "method": "\u5229\u7528\u672a\u5b66\u4e60\u7684\u5f31\u6a21\u578b\u548c\u7a7a\u6587\u672c\u63d0\u793a\uff0c\u52a8\u6001\u63a7\u5236\u6743\u91cd\u63d2\u503c\u4ee5\u5e73\u8861\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6587\u672c\u5bf9\u9f50\u548c\u76ee\u6807\u5206\u5e03\u4fdd\u771f\u5ea6\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u4e2a\u6027\u5316\u5f15\u5bfc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u6027\u80fd\u3002"}}
{"id": "2508.00368", "pdf": "https://arxiv.org/pdf/2508.00368", "abs": "https://arxiv.org/abs/2508.00368", "authors": ["Alessandro Gaudenzi", "Lorenzo Nodari", "Lance Kaplan", "Alessandra Russo", "Murat Sensoy", "Federico Cerutti"], "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification", "categories": ["cs.CR", "cs.LG"], "comment": "Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy,\n  co-located with ECAI2025", "summary": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\uff08EDL\uff09\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u65adAPT\u653b\u51fb\u9636\u6bb5\uff0c\u5e76\u5904\u7406\u5206\u5e03\u5916\u8f93\u5165\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u653b\u51fb\u9636\u6bb5\uff0c\u800c\u51c6\u786e\u63a8\u65ad\u653b\u51fb\u9636\u6bb5\u5bf9\u5b9a\u5236\u54cd\u5e94\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528EDL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u51faDirichlet\u5206\u5e03\u53c2\u6570\u6765\u5efa\u6a21\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63a8\u65ad\u653b\u51fb\u9636\u6bb5\u5e76\u68c0\u6d4bOOD\u8f93\u5165\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u63a8\u65ad\u653b\u51fb\u9636\u6bb5\u5e76\u6709\u6548\u68c0\u6d4bOOD\u8f93\u5165\uff0c\u652f\u6301\u5728\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u5728APT\u653b\u51fb\u9636\u6bb5\u63a8\u65ad\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5bf9\u6297\u73af\u5883\u3002"}}
{"id": "2508.00370", "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "EdgeInfinite-Instruct\u901a\u8fc7\u5206\u6bb5\u76d1\u7763\u5fae\u8c03\uff08S-SFT\uff09\u548c\u7ec6\u7c92\u5ea6\u91cf\u5316\u4f18\u5316\uff0c\u89e3\u51b3\u4e86Transformer\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u95ee\u9898\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u548cKV\u7f13\u5b58\u9700\u6c42\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u517c\u987eTTFT\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faEdgeInfinite-Instruct\uff0c\u91c7\u7528S-SFT\u7b56\u7565\u9488\u5bf9\u957f\u5e8f\u5217\u4efb\u52a1\uff08\u5982\u6458\u8981\u548c\u95ee\u7b54\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u7ec6\u7c92\u5ea6PTQ\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\u4f18\u5316\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u79fb\u52a8\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fb9\u7f18NPU\u8bbe\u5907\u7684\u6548\u7387\u3002", "conclusion": "EdgeInfinite-Instruct\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2508.00381", "pdf": "https://arxiv.org/pdf/2508.00381", "abs": "https://arxiv.org/abs/2508.00381", "authors": ["Kamal Basha S", "Athira Nambiar"], "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAdapt-WeldNet\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u67b6\u6784\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\u4f18\u5316\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165DDIA\u6846\u67b6\u63d0\u5347\u7cfb\u7edf\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65e0\u635f\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u7ec6\u5fae\u6216\u5185\u90e8\u7f3a\u9677\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "Adapt-WeldNet\u7cfb\u7edf\u8bc4\u4f30\u9884\u8bad\u7ec3\u67b6\u6784\u3001\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff1bDDIA\u6846\u67b6\u7ed3\u5408XAI\u6280\u672f\u548c\u4e13\u5bb6\u9a8c\u8bc1\u63d0\u5347\u900f\u660e\u5ea6\u3002", "result": "\u4f18\u5316\u4e86\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7DDIA\u6846\u67b6\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u710a\u63a5\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u900f\u660e\u5ea6\uff0c\u589e\u5f3a\u4e86\u5728\u6d77\u6d0b\u548c\u79bb\u5cb8\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.00383", "pdf": "https://arxiv.org/pdf/2508.00383", "abs": "https://arxiv.org/abs/2508.00383", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784$MV_{Hybrid}$\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u548cViT\uff0c\u7528\u4e8e\u9884\u6d4b\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u7a7a\u95f4\u57fa\u56e0\u8868\u8fbe\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709ViT\u6a21\u578b\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\uff0c\u800c\u73b0\u6709\u57fa\u4e8eViT\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165$MV_{Hybrid}$\uff0c\u7ed3\u5408SSMs\u548cViT\uff0c\u5229\u7528\u8d1f\u5b9e\u7279\u5f81\u503c\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6355\u6349\u4f4e\u9891\u5f62\u6001\u6a21\u5f0f\u3002", "result": "\u5728LOSO\u8bc4\u4f30\u4e2d\uff0c$MV_{Hybrid}$\u6bd4\u6700\u4f73ViT\u6a21\u578b\u76f8\u5173\u6027\u9ad857%\uff0c\u6027\u80fd\u4e0b\u964d\u51cf\u5c1143%\u3002", "conclusion": "$MV_{Hybrid}$\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u548c\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u75c5\u7406\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2508.00385", "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u6f14\u793a\u65e0\u6548\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u6d41\u7684\u6f14\u793a\u9009\u62e9\u65b9\u6cd5GradS\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbeICL\u4e2d\u7684\u6f14\u793a\u5747\u6709\u6548\uff0c\u4f46\u5b9e\u9645\u5e76\u975e\u5982\u6b64\uff0c\u56e0\u6b64\u63a2\u7a76\u6f14\u793a\u65e0\u6548\u7684\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u6d41\u548c\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u5206\u6790\uff0c\u63d0\u51faGradS\u65b9\u6cd5\uff0c\u5229\u7528\u68af\u5ea6\u6d41\u5e45\u5ea6\u9009\u62e9\u6709\u6548\u6f14\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5c42\u6570\u589e\u52a0\u4f1a\u653e\u5927\u6f14\u793a\u6709\u6548\u6027\u5dee\u5f02\uff0cGradS\u5e73\u5747\u63d0\u53476.8%\u6027\u80fd\u3002", "conclusion": "GradS\u901a\u8fc7\u68af\u5ea6\u6d41\u9009\u62e9\u6f14\u793a\uff0c\u663e\u8457\u63d0\u5347ICL\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6f14\u793a\u65e0\u6548\u539f\u56e0\u7684\u7406\u8bba\u63a8\u5bfc\u3002"}}
{"id": "2508.00419", "pdf": "https://arxiv.org/pdf/2508.00419", "abs": "https://arxiv.org/abs/2508.00419", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers", "categories": ["cs.LO", "cs.LG", "cs.PL"], "comment": "Under Review", "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7ed3\u5408Z3\u6c42\u89e3\u5668\u81ea\u52a8\u5408\u6210\u5faa\u73af\u4e0d\u53d8\u5f0f\u7684\u65b9\u6cd5\uff0c\u5728Code2Inv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u5faa\u73af\u4e0d\u53d8\u5f0f\u5bf9\u7a0b\u5e8f\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5408\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u90e8\u5206\u57fa\u51c6\u6d4b\u8bd5\uff0c\u56e0\u6b64\u7814\u7a76LLM\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06OpenAI\u7684LLM\uff08O1\u3001O1-mini\u3001O3-mini\uff09\u4e0eZ3\u6c42\u89e3\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u751f\u6210-\u68c0\u67e5\u8fed\u4ee3\u4f18\u5316\u4e0d\u53d8\u5f0f\u3002", "result": "\u5728133\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0100%\u8986\u76d6\u7387\uff0c\u4ec5\u97001-2\u6b21\u6a21\u578b\u63d0\u8bae\u548c14-55\u79d2\u65f6\u95f4\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f73\u7ed3\u679c\uff08107/133\uff09\u3002", "conclusion": "LLM\u5177\u6709\u6f5c\u5728\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u547d\u4ee4\u5f0f\u8bed\u8a00\uff0c\u4e3a\u5faa\u73af\u4e0d\u53d8\u5f0f\u5408\u6210\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.00422", "pdf": "https://arxiv.org/pdf/2508.00422", "abs": "https://arxiv.org/abs/2508.00422", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Automated Type Annotation in Python Using Large Language Models", "categories": ["cs.PL", "cs.LG"], "comment": "Under Review", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528LLMs\uff08\u5982GPT 4oMini\u3001GPT 4.1mini\u7b49\uff09\u81ea\u52a8\u751f\u6210Python\u7c7b\u578b\u6ce8\u91ca\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210-\u68c0\u67e5-\u4fee\u590d\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "Python\u7c7b\u578b\u6ce8\u91ca\u80fd\u63d0\u5347\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u548c\u9519\u8bef\u68c0\u6d4b\uff0c\u4f46\u624b\u52a8\u751f\u6210\u6613\u51fa\u9519\u4e14\u8017\u65f6\u3002\u4f20\u7edf\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u8bcd\u6c47\u9650\u5236\u3001\u884c\u4e3a\u8fc7\u5ea6\u8fd1\u4f3c\u548c\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u751f\u6210-\u68c0\u67e5-\u4fee\u590d\u6d41\u7a0b\uff1aLLM\u57fa\u4e8e\u8bed\u6cd5\u6811\u751f\u6210\u6ce8\u91ca\uff0c\u9759\u6001\u7c7b\u578b\u68c0\u67e5\u5668\uff08Mypy\uff09\u9a8c\u8bc1\uff0c\u9519\u8bef\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u3002\u8bc4\u4f30\u4e86\u56db\u79cdLLM\u53d8\u4f53\u57286000\u4e2a\u4ee3\u7801\u7247\u6bb5\u4e0a\u7684\u8868\u73b0\u3002", "result": "GPT 4.1mini\u548cO3Mini\u8868\u73b0\u6700\u4f73\uff0c\u4e00\u81f4\u6027\u8fbe88.6%\uff0c\u7cbe\u786e\u5339\u914d\u548c\u57fa\u7840\u7c7b\u578b\u51c6\u786e\u7387\u5206\u522b\u4e3a70.5%\u548c79.1%\uff0c\u5e73\u5747\u4fee\u590d\u8fed\u4ee3\u6b21\u6570\u4f4e\u4e8e1\u6b21\u3002", "conclusion": "\u901a\u7528\u548c\u63a8\u7406\u4f18\u5316\u7684LLMs\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u751f\u6210\u4e00\u81f4\u7684\u7c7b\u578b\u6ce8\u91ca\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u53ef\u9009\u7c7b\u578b\u8bed\u8a00\u3002"}}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u81ea\u4e3b\u51b3\u7b56\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u89e3\u51b3GNN\u4e2d\u8282\u70b9\u4fe1\u606f\u4e0d\u5e73\u8861\u548c\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfGNN\u7684\u56fa\u5b9a\u805a\u5408\u673a\u5236\u65e0\u6cd5\u5904\u7406\u8282\u70b9\u4fe1\u606f\u4e0d\u5e73\u8861\uff0c\u4e14\u5ffd\u7565\u4e86\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "ReaGAN\u4e3a\u6bcf\u4e2a\u8282\u70b9\u8d4b\u4e88\u4ee3\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u57fa\u4e8e\u5185\u90e8\u8bb0\u5fc6\u81ea\u4e3b\u89c4\u5212\u884c\u52a8\uff0c\u5e76\u901a\u8fc7RAG\u8bbf\u95ee\u5168\u5c40\u8bed\u4e49\u76f8\u5173\u5185\u5bb9\u3002", "result": "ReaGAN\u5728\u5c11\u6837\u672c\u60c5\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5229\u7528\u51bb\u7ed3\u7684LLM\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "ReaGAN\u5c55\u793a\u4e86\u4ee3\u7406\u89c4\u5212\u548c\u5c40\u90e8-\u5168\u5c40\u68c0\u7d22\u5728\u56fe\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00447", "pdf": "https://arxiv.org/pdf/2508.00447", "abs": "https://arxiv.org/abs/2508.00447", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 8 figures", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "AI": {"tldr": "CLIPTime\u662f\u4e00\u4e2a\u57fa\u4e8eCLIP\u67b6\u6784\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u771f\u83cc\u751f\u957f\u7684\u53d1\u80b2\u9636\u6bb5\u548c\u65f6\u95f4\u6233\uff0c\u65e0\u9700\u663e\u5f0f\u65f6\u95f4\u8f93\u5165\u3002", "motivation": "\u7406\u89e3\u751f\u7269\u751f\u957f\u7684\u65f6\u5e8f\u52a8\u6001\u5728\u5fae\u751f\u7269\u5b66\u3001\u519c\u4e1a\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u65f6\u5e8f\u8fdb\u5c55\u65b9\u9762\u6709\u9650\u3002", "method": "\u57fa\u4e8eCLIP\u67b6\u6784\uff0c\u5b66\u4e60\u89c6\u89c9-\u6587\u672c\u8054\u5408\u5d4c\u5165\uff0c\u901a\u8fc7\u5206\u7c7b\u548c\u56de\u5f52\u9884\u6d4b\u79bb\u6563\u751f\u957f\u9636\u6bb5\u548c\u8fde\u7eed\u65f6\u95f4\u6233\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCLIPTime\u80fd\u6709\u6548\u5efa\u6a21\u751f\u7269\u8fdb\u5c55\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u65f6\u5e8f\u8f93\u51fa\u3002", "conclusion": "CLIPTime\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u76d1\u6d4b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00544", "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPaPaformer\u7684\u5e76\u884c\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u8def\u5f84\u8bad\u7ec3\u548c\u7ec4\u5408\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u53c2\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8017\u65f6\u957f\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u5373\u4f7f\u662f\u5c0f\u578b\u6a21\u578b\u4e5f\u9700\u8981\u591aGPU\u548c\u6570\u5929\u65f6\u95f4\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5feb\u901f\u8bad\u7ec3\u548c\u8bc4\u4f30\u89e3\u7801\u5668\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPaPaformer\uff0c\u4e00\u79cd\u5e76\u884c\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5206\u8def\u5f84\u8bad\u7ec3\u540e\u7ec4\u5408\u6210\u5b8c\u6574\u6a21\u578b\uff0c\u51cf\u5c11\u53c2\u6570\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u53c2\u6570\u91cf\uff0c\u540c\u65f6\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u5e76\u652f\u6301\u8def\u5f84\u5b9a\u5236\u4ee5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u3002", "conclusion": "PaPaformer\u4e3a\u9ad8\u6548\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u548c\u7075\u6d3b\u5b9a\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00558", "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4e0e\u90e8\u5206\u70b9\u4e91\u5bf9\u9f50\u4e14\u7269\u7406\u5408\u7406\u7684\u94f0\u63a5\u7269\u4f53\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\u662f\u65e5\u5e38\u73af\u5883\u4e2d\u91cd\u8981\u7684\u53ef\u4ea4\u4e92\u5bf9\u8c61\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u5408\u7406\u6027\u548c\u70b9\u4e91\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528SDF\u8868\u793a\u90e8\u4ef6\u5f62\u72b6\uff0c\u901a\u8fc7\u70b9\u4e91\u5bf9\u9f50\u635f\u5931\u548c\u975e\u7a7f\u900f\u3001\u79fb\u52a8\u6027\u7ea6\u675f\u5f15\u5bfc\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u7c7b\u522b\u4fe1\u606f\u4f18\u5316\u5bf9\u9f50\u3002", "result": "\u5728PartNet-Mobility\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPhysNAP\u5728\u7ea6\u675f\u4e00\u81f4\u6027\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u4e8e\u65e0\u5f15\u5bfc\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PhysNAP\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u548c\u7c7b\u522b\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u94f0\u63a5\u7269\u4f53\u751f\u6210\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2508.00600", "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "CRUX\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u6307\u6807\u6765\u6539\u8fdbLLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u54cd\u5e94\u4e0e\u4e0a\u4e0b\u6587\u7684\u76f8\u5173\u6027\uff0c\u800cCRUX\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faCRUX\u6846\u67b6\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u71b5\u51cf\u548c\u7edf\u4e00\u4e00\u81f4\u6027\u68c0\u67e5\u4e24\u79cd\u65b0\u6307\u6807\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793aCRUX\u7684AUROC\u6700\u9ad8\u3002", "conclusion": "CRUX\u6709\u6548\u63d0\u5347\u4e86LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00602", "pdf": "https://arxiv.org/pdf/2508.00602", "abs": "https://arxiv.org/abs/2508.00602", "authors": ["Francesco Panebianco", "Stefano Bonfanti", "Francesco Trov\u00f2", "Michele Carminati"], "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "22 pages, preprint", "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790LLM\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u7684\u65b9\u6cd5\u548cLeakSealer\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u9632\u5fa1jailbreaking\u653b\u51fb\u4e0e\u6570\u636e\u6cc4\u9732\u3002", "motivation": "LLM\u7684\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u4e86\u5b89\u5168\u5a01\u80c1\uff0c\u5982jailbreaking\u548c\u6570\u636e\u6cc4\u9732\uff0c\u9700\u8981\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "1. \u5206\u6790LLM\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u751f\u6210\u4f7f\u7528\u5730\u56fe\uff1b2. \u63d0\u51faLeakSealer\u6846\u67b6\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u9632\u5fa1\u3002", "result": "LeakSealer\u5728ToxicChat\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0cPII\u6cc4\u9732\u68c0\u6d4bAUPRC\u8fbe0.97\u3002", "conclusion": "LeakSealer\u80fd\u6709\u6548\u8bc6\u522b\u548c\u9632\u5fa1LLM\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2508.00617", "pdf": "https://arxiv.org/pdf/2508.00617", "abs": "https://arxiv.org/abs/2508.00617", "authors": ["Natha\u00ebl Da Costa", "Marvin Pf\u00f6rtner", "Jon Cockayne"], "title": "Constructive Disintegration and Conditional Modes", "categories": ["math.ST", "cs.LG", "math.PR", "stat.ML", "stat.TH"], "comment": null, "summary": "Conditioning, the central operation in Bayesian statistics, is formalised by\nthe notion of disintegration of measures. However, due to the implicit nature\nof their definition, constructing disintegrations is often difficult. A\nfolklore result in machine learning conflates the construction of a\ndisintegration with the restriction of probability density functions onto the\nsubset of events that are consistent with a given observation. We provide a\ncomprehensive set of mathematical tools which can be used to construct\ndisintegrations and apply these to find densities of disintegrations on\ndifferentiable manifolds. Using our results, we provide a disturbingly simple\nexample in which the restricted density and the disintegration density\ndrastically disagree. Motivated by applications in approximate Bayesian\ninference and Bayesian inverse problems, we further study the modes of\ndisintegrations. We show that the recently introduced notion of a \"conditional\nmode\" does not coincide in general with the modes of the conditional measure\nobtained through disintegration, but rather the modes of the restricted\nmeasure. We also discuss the implications of the discrepancy between the two\nmeasures in practice, advocating for the utility of both approaches depending\non the modelling context.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8d1d\u53f6\u65af\u7edf\u8ba1\u4e2d\u7684\u6761\u4ef6\u5316\u64cd\u4f5c\uff0c\u901a\u8fc7\u6d4b\u5ea6\u5206\u89e3\u7406\u8bba\u5f62\u5f0f\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u6784\u5efa\u5206\u89e3\u7684\u6570\u5b66\u5de5\u5177\u3002\u7814\u7a76\u53d1\u73b0\u9650\u5236\u5bc6\u5ea6\u4e0e\u5206\u89e3\u5bc6\u5ea6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u8ba8\u8bba\u4e86\u6761\u4ef6\u6a21\u5f0f\u4e0e\u5206\u89e3\u6d4b\u5ea6\u6a21\u5f0f\u7684\u4e0d\u4e00\u81f4\u6027\u53ca\u5176\u5b9e\u9645\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u6d4b\u5ea6\u5206\u89e3\u7684\u6784\u5efa\u56f0\u96be\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u4f9b\u6570\u5b66\u5de5\u5177\u6784\u5efa\u6d4b\u5ea6\u5206\u89e3\uff0c\u5e76\u5728\u53ef\u5fae\u6d41\u5f62\u4e0a\u5e94\u7528\u8fd9\u4e9b\u5de5\u5177\u627e\u5230\u5206\u89e3\u5bc6\u5ea6\u3002", "result": "\u7ed3\u679c\u663e\u793a\u9650\u5236\u5bc6\u5ea6\u4e0e\u5206\u89e3\u5bc6\u5ea6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u6761\u4ef6\u6a21\u5f0f\u4e0e\u5206\u89e3\u6d4b\u5ea6\u6a21\u5f0f\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e24\u79cd\u65b9\u6cd5\uff08\u9650\u5236\u5bc6\u5ea6\u4e0e\u5206\u89e3\u5bc6\u5ea6\uff09\u5728\u4e0d\u540c\u5efa\u6a21\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u547c\u5401\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u3002"}}
{"id": "2508.00619", "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86DACTYL\u6570\u636e\u96c6\u548c\u4e24\u79cd\u5206\u7c7b\u5668\u4f18\u5316\u65b9\u6cd5\uff0c\u53d1\u73b0DXO\u5206\u7c7b\u5668\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5728\u5185\u90e8\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u5c11\u6837\u672c\u548c\u9886\u57df\u7279\u5b9a\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51faDACTYL\u6570\u636e\u96c6\uff0c\u5305\u542b\u5c11\u6837\u672c\u548c\u9886\u57df\u7279\u5b9a\u751f\u6210\u6587\u672c\uff1b\u6bd4\u8f83\u4e86\u6807\u51c6BCE\u4f18\u5316\u548cDXO\u4f18\u5316\u4e24\u79cd\u5206\u7c7b\u5668\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "DXO\u5206\u7c7b\u5668\u5728OOD\u6587\u672c\u4e0a\u8868\u73b0\u4f18\u4e8eBCE\u5206\u7c7b\u5668\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u7684\u6539\u8fdb\u7a7a\u95f4\uff0cDXO\u4f18\u5316\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.00620", "pdf": "https://arxiv.org/pdf/2508.00620", "abs": "https://arxiv.org/abs/2508.00620", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "title": "Backdoor Attacks on Deep Learning Face Detection", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u7684\u751f\u6210\u653b\u51fb\uff08Face Generation Attacks\uff09\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5e72\u6270\u5750\u6807\u56de\u5f52\u4efb\u52a1\u7684Landmark Shift Attack\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e0b\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u9762\u4e34\u5149\u7167\u3001\u59ff\u6001\u7b49\u6311\u6218\uff0c\u9700\u8981\u4f9d\u8d56\u4eba\u8138\u68c0\u6d4b\u6a21\u5757\u8fdb\u884c\u5bf9\u9f50\u3002\u7814\u7a76\u8fd9\u4e9b\u6a21\u5757\u7684\u8106\u5f31\u6027\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff08Landmark Shift Attack\uff09\uff0c\u5e72\u6270\u4eba\u8138\u68c0\u6d4b\u5668\u7684\u5750\u6807\u56de\u5f52\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86Face Generation Attacks\u548cLandmark Shift Attack\u5bf9\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u7684\u7834\u574f\u6027\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.00658", "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u9891\u5e26\u53d8\u6ede\u540e\u683c\u5170\u6770\u56e0\u679c\u5173\u7cfb\uff08MB-VLGC\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9891\u7387\u4f9d\u8d56\u6027\u56e0\u679c\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9886\u57df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u683c\u5170\u6770\u56e0\u679c\u5173\u7cfb\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7684\u6ede\u540e\u65f6\u95f4\uff0c\u800c\u53d8\u6ede\u540e\u65b9\u6cd5\uff08VLGC\uff09\u867d\u5141\u8bb8\u6ede\u540e\u65f6\u95f4\u53d8\u5316\uff0c\u4f46\u5ffd\u7565\u4e86\u9891\u7387\u4f9d\u8d56\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMB-VLGC\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9891\u7387\u4f9d\u8d56\u6027\u56e0\u679c\u5ef6\u8fdf\uff0c\u6269\u5c55\u4e86VLGC\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u63a8\u65ad\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMB-VLGC\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MB-VLGC\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002"}}
{"id": "2508.00665", "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edfXAI\u6280\u672f\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u591a\u6a21\u6001\u3001\u4e2a\u6027\u5316\u7684\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3a\u6559\u80b2\u4e2dAI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4e14XAI\u6280\u672f\u591a\u5173\u6ce8\u6280\u672f\u8f93\u51fa\u800c\u5ffd\u89c6\u7528\u6237\u89d2\u8272\u548c\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4f20\u7edfXAI\u6280\u672f\u4e0e\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u7ed3\u5408\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u751f\u6210\u52a8\u6001\u3001\u591a\u6a21\u6001\u7684\u89e3\u91ca\u3002", "result": "\u6846\u67b6\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u6559\u80b2\u4e2d\u7684\u5c40\u9650\u6027\u53ca\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u76ee\u6807\u662f\u63a8\u52a8\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u5e76\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.00669", "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u7684\u589e\u5f3a\u6280\u672f\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u4e0e\u6311\u6218\u3002", "motivation": "\u533b\u5b66\u5b9e\u8df5\u4e2d\u9700\u8981\u7cfb\u7edf\u3001\u900f\u660e\u4e14\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709LLMs\u5728\u6b64\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u63a8\u52a8\u4e86\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u63a8\u7406\u7684LLMs\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u679060\u9879\u7814\u7a76\uff082022-2025\uff09\uff0c\u63d0\u51fa\u8bad\u7ec3\u65f6\uff08\u5982\u76d1\u7763\u5fae\u8c03\uff09\u548c\u6d4b\u8bd5\u65f6\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\u7684\u6280\u672f\u5206\u7c7b\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u591a\u6a21\u6001\u6570\u636e\u53ca\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bc4\u4f30\u6807\u51c6\u4ece\u7b80\u5355\u51c6\u786e\u6027\u8f6c\u5411\u63a8\u7406\u8d28\u91cf\u548c\u53ef\u89c6\u5316\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u5fe0\u5b9e\u6027\u4e0e\u5408\u7406\u6027\u5dee\u8ddd\u7b49\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u53d1\u5c55\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u793e\u4f1a\u6280\u672f\u8d23\u4efb\u5f3a\u7684\u533b\u5b66AI\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u7b49\u6311\u6218\u3002"}}
{"id": "2508.00674", "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5206\u6bb5\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u89e3\u91ca\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u65b9\u6cd5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u5bf9AI\u63a8\u8350\u7684\u7406\u89e3\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u7684AI\u63a8\u8350\u7f3a\u4e4f\u9488\u5bf9\u7528\u6237\u7279\u5b9a\u9700\u6c42\u7684\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u7528\u6237\u4e0d\u7406\u89e3\u63a8\u8350\u539f\u56e0\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u63a8\u8350\u7684\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u7ed3\u5408\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\uff0c\u63d0\u4f9b\u4e0d\u540c\u5f62\u5f0f\u7684\u89e3\u91ca\uff08\u5982\u6280\u672f\u8be6\u7ec6\u7248\u548c\u7b80\u5316\u7248\uff09\u3002", "result": "\u7cfb\u7edf\u9996\u6b21\u5728\u5355\u4e00\u6d41\u7a0b\u4e2d\u540c\u65f6\u8c03\u6574\u89e3\u91ca\u98ce\u683c\uff08\u89c6\u89c9vs.\u6570\u5b57\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6vs.\u666e\u901a\u7528\u6237\uff09\u3002", "conclusion": "\u901a\u8fc730\u540dX\u7528\u6237\u7684\u516c\u5f00\u8bd5\u70b9\u9a8c\u8bc1\u7cfb\u7edf\u5bf9\u51b3\u7b56\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.00679", "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever\u662f\u4e00\u79cd\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u4fee\u8f9e\u663e\u8457\u7247\u6bb5\u800c\u975e\u5b8c\u6574\u6587\u6863\uff0c\u7ed3\u5408BM25\u3001\u5411\u91cf\u6570\u636e\u5e93\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u6a21\u578b\uff0c\u89e3\u51b3\u6cd5\u5f8b\u6587\u6863\u590d\u6742\u6027\u548c\u6570\u91cf\u589e\u957f\u7684\u95ee\u9898\u3002", "motivation": "\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\u5728\u666e\u901a\u6cd5\u4f53\u7cfb\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6cd5\u5f8b\u6587\u6863\u6570\u91cf\u548c\u5185\u5bb9\u3002", "method": "\u7ed3\u5408BM25\u3001\u5411\u91cf\u6570\u636e\u5e93\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u5c42\u6b21\u5316BiLSTM CRF\u5206\u7c7b\u5668\u751f\u6210\u4fee\u8f9e\u6807\u6ce8\uff0c\u5e76\u91c7\u7528\u4e92\u60e0\u6392\u540d\u878d\u5408\u8fdb\u884c\u7ed3\u679c\u6574\u5408\u3002", "result": "\u5728IL-PCR\u548cCOLIEE 2025\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cTraceRetriever\u80fd\u6709\u6548\u5e94\u5bf9\u6587\u6863\u6570\u91cf\u589e\u957f\uff0c\u5e76\u5728\u90e8\u5206\u6848\u4f8b\u77e5\u8bc6\u53ef\u7528\u65f6\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "conclusion": "TraceRetriever\u4e3a\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u3002"}}
{"id": "2508.00709", "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "NyayaRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\uff0c\u901a\u8fc7\u7ed3\u5408\u6848\u4ef6\u4e8b\u5b9e\u3001\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5370\u5ea6\u6cd5\u5f8b\u80cc\u666f\u4e0b\u5ffd\u89c6\u4e86\u666e\u901a\u6cd5\u7cfb\u7684\u6838\u5fc3\u5143\u7d20\uff08\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\uff09\uff0cNyayaRAG\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faNyayaRAG\u6846\u67b6\uff0c\u6a21\u62df\u771f\u5b9e\u6cd5\u5ead\u573a\u666f\uff0c\u7ed3\u5408\u6848\u4ef6\u63cf\u8ff0\u3001\u6cd5\u5f8b\u6761\u6587\u548c\u8bed\u4e49\u68c0\u7d22\u7684\u5148\u4f8b\uff0c\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7ba1\u9053\u8bc4\u4f30\u8f93\u5165\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6cd5\u5f8b\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "NyayaRAG\u4e3a\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u666e\u901a\u6cd5\u7cfb\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.00721", "pdf": "https://arxiv.org/pdf/2508.00721", "abs": "https://arxiv.org/abs/2508.00721", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.", "AI": {"tldr": "FMPlug\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u57fa\u7840\u6d41\u5339\u914d\uff08FM\uff09\u5148\u9a8c\u6765\u89e3\u51b3\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u6216\u65e0\u8bad\u7ec3\u5148\u9a8c\uff0cFMPlug\u5229\u7528\u89c2\u5bdf\u4e0e\u76ee\u6807\u5bf9\u8c61\u7684\u76f8\u4f3c\u6027\u53ca\u751f\u6210\u6d41\u7684\u9ad8\u65af\u6027\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u81ea\u9002\u5e94\u9884\u70ed\u7b56\u7565\u548c\u9510\u5229\u9ad8\u65af\u6027\u6b63\u5219\u5316\uff0c\u5145\u5206\u53d1\u6325\u9886\u57df\u65e0\u5173\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002", "result": "\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u548c\u9ad8\u65af\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FMPlug\u901a\u8fc7\u7b80\u5355\u800c\u5f3a\u5927\u7684\u6d1e\u5bdf\u529b\uff0c\u4e3a\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00742", "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u91cd\u73b0HEXACO\u4eba\u683c\u6d4b\u8bd5\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4ee3\u7406\u80fd\u90e8\u5206\u5bf9\u9f50HEXACO\u6846\u67b6\uff0c\u4f46\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\u504f\u5dee\u3002", "motivation": "\u9a8c\u8bc1\u57fa\u4e8e\u89d2\u8272\u7684\u751f\u6210\u4ee3\u7406\u662f\u5426\u80fd\u6709\u6548\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91cd\u73b0HEXACO\u4eba\u683c\u6d4b\u8bd5\u5b9e\u9a8c\uff0c\u8c03\u67e5310\u4e2aGPT-4\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u8fdb\u884c\u56e0\u5b50\u5206\u6790\u5e76\u4e0e\u539f\u59cb\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "1\uff09\u4ee3\u7406\u54cd\u5e94\u4e2d\u53ef\u6062\u590d\u51fa\u90e8\u5206\u5bf9\u9f50HEXACO\u7684\u4eba\u683c\u7ed3\u6784\uff1b2\uff09GPT-4\u751f\u6210\u7684\u4eba\u683c\u7ef4\u5ea6\u4e00\u81f4\u4e14\u53ef\u9760\uff1b3\uff09\u8de8\u6a21\u578b\u5206\u6790\u663e\u793a\u4eba\u683c\u5206\u6790\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\u504f\u5dee\u3002", "conclusion": "\u751f\u6210\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u578b\u7279\u5f02\u6027\u548c\u8bbe\u8ba1\u4e00\u81f4\u6027\u4ee5\u6700\u5927\u5316\u4eba\u7c7b\u4eba\u683c\u7279\u5f81\u7684\u8986\u76d6\u548c\u4ee3\u8868\u6027\u3002"}}
{"id": "2508.00743", "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald K\u00f6stler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u653e\u5c04\u5b66\u95ee\u7b54\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u6b65\u68c0\u7d22\u7684RAG\u7cfb\u7edf\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4ee3\u7406RAG\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u81ea\u4e3b\u5206\u89e3\u95ee\u9898\u3001\u8fed\u4ee3\u68c0\u7d22\u4e34\u5e8a\u8bc1\u636e\u5e76\u52a8\u6001\u5408\u6210\u56de\u7b54\uff0c\u8bc4\u4f30\u4e8624\u79cd\u4e0d\u540c\u67b6\u6784\u548c\u89c4\u6a21\u7684LLM\u3002", "result": "\u4ee3\u7406\u68c0\u7d22\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\uff0873% vs. 64%\uff09\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\uff089.4%\uff09\uff0c\u5e76\u572846%\u7684\u6848\u4f8b\u4e2d\u68c0\u7d22\u5230\u76f8\u5173\u4e34\u5e8a\u80cc\u666f\u3002", "conclusion": "\u4ee3\u7406\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u653e\u5c04\u5b66\u95ee\u7b54\u7684\u4e8b\u5b9e\u6027\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5bf9\u4e2d\u578bLLM\u6548\u679c\u663e\u8457\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u4e34\u5e8a\u9a8c\u8bc1\u3002"}}
{"id": "2508.00750", "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "SU-ESRGAN\u662f\u4e00\u79cd\u9488\u5bf9\u536b\u661f\u56fe\u50cf\u7684SR\u6846\u67b6\uff0c\u7ed3\u5408ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u536b\u661f\u7cfb\u7edf\u3002", "motivation": "GANs\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u50cf\u7d20\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u9065\u611f\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faSU-ESRGAN\uff0c\u96c6\u6210ESRGAN\u3001DeepLabv3\u5206\u5272\u635f\u5931\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u56fe\u3002", "result": "\u6027\u80fd\u4e0e\u57fa\u7ebfESRGAN\u76f8\u5f53\uff0c\u5728\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u5728\u6210\u50cf\u7279\u5f81\u5339\u914d\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "SU-ESRGAN\u9002\u7528\u4e8e\u536b\u661f\u548c\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u5f3a\u8c03\u9886\u57df\u611f\u77e5\u8bad\u7ec3\u5728SR\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00775", "pdf": "https://arxiv.org/pdf/2508.00775", "abs": "https://arxiv.org/abs/2508.00775", "authors": ["Andrea Martin", "Ian R. Manchester", "Luca Furieri"], "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6700\u574f\u60c5\u51b5\u6536\u655b\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u7b97\u6cd5\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u4f18\u5316\u7b97\u6cd5\u9700\u5177\u5907\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u4f46\u901a\u5e38\u727a\u7272\u4e86\u5b9e\u9645\u5e38\u89c1\u95ee\u9898\u7684\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4ece\u57fa\u7ebf\u7ebf\u6027\u6536\u655b\u7b97\u6cd5\u51fa\u53d1\uff0c\u63a8\u5bfc\u51fa\u6240\u6709\u4e14\u4ec5\u80fd\u4fdd\u6301\u5176\u6536\u655b\u6027\u7684\u4fee\u6539\u89c4\u5219\u3002", "result": "\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u4e0b\u964d\u3001Nesterov\u52a0\u901f\u65b9\u6cd5\u7b49\uff09\uff0c\u5e76\u5728\u7ebf\u6027\u65b9\u7a0b\u7ec4\u548cMPC\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u7559\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u7279\u5b9a\u95ee\u9898\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00804", "pdf": "https://arxiv.org/pdf/2508.00804", "abs": "https://arxiv.org/abs/2508.00804", "authors": ["Julian Lemmel", "Manuel Kranzl", "Adam Lamine", "Philipp Neubauer", "Radu Grosu", "Sophie Neubauer"], "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models", "categories": ["cs.CE", "cs.LG", "cs.SY", "eess.SY"], "comment": "6 pages", "summary": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60\u7684\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u3002", "motivation": "SSM\u901a\u5e38\u79bb\u7ebf\u8bad\u7ec3\u4e14\u90e8\u7f72\u540e\u9759\u6001\u8fd0\u884c\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0SSM\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b9e\u65f6\u5faa\u73af\u5b66\u4e60\u6280\u672f\uff0c\u5728\u7ebf\u66f4\u65b0\u7ebf\u6027\u5faa\u73af\u5355\u5143SSM\u7684\u53c2\u6570\u3002", "result": "\u5728\u5d4c\u5165\u5f0f\u6c7d\u8f66\u786c\u4ef6\u91c7\u96c6\u7684\u5c0f\u578b\u78b3\u6392\u653e\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u65f6\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u52a8\u6001\u4e14\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\uff0c\u5c55\u793a\u4e86SSM\u5728\u7ebf\u9002\u5e94\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00816", "pdf": "https://arxiv.org/pdf/2508.00816", "abs": "https://arxiv.org/abs/2508.00816", "authors": ["Youssef Ait El Mahjoub", "Jean-Michel Fourneau", "Salma Alouah"], "title": "Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process", "categories": ["math.OC", "cs.LG", "cs.PF"], "comment": "Preprint article submitted to ValueTools2025", "summary": "Solving Markov Decision Processes (MDPs) remains a central challenge in\nsequential decision-making, especially when dealing with large state spaces and\nlong-term optimization criteria. A key step in Bellman dynamic programming\nalgorithms is the policy evaluation, which becomes computationally demanding in\ninfinite-horizon settings such as average-reward or discounted-reward\nformulations. In the context of Markov chains, aggregation and disaggregation\ntechniques have for a long time been used to reduce complexity by exploiting\nstructural decompositions. In this work, we extend these principles to a\nstructured class of MDPs. We define the Single-Input Superstate Decomposable\nMarkov Decision Process (SISDMDP), which combines Chiu's single-input\ndecomposition with Robertazzi's single-cycle recurrence property. When a policy\ninduces this structure, the resulting transition graph can be decomposed into\ninteracting components with centralized recurrence. We develop an exact and\nefficient policy evaluation method based on this structure. This yields a\nscalable solution applicable to both average and discounted reward MDPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u8f93\u5165\u8d85\u72b6\u6001\u53ef\u5206\u89e3MDP\uff08SISDMDP\uff09\u7684\u9ad8\u6548\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u548c\u957f\u671f\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21MDP\u4e2d\u7b56\u7565\u8bc4\u4f30\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u9650\u65f6\u57df\u8bbe\u7f6e\u4e0b\uff08\u5982\u5e73\u5747\u5956\u52b1\u6216\u6298\u6263\u5956\u52b1\uff09\u3002", "method": "\u7ed3\u5408Chiu\u7684\u5355\u8f93\u5165\u5206\u89e3\u548cRobertazzi\u7684\u5355\u5faa\u73af\u9012\u5f52\u7279\u6027\uff0c\u5b9a\u4e49SISDMDP\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6b64\u7ed3\u6784\u7684\u7cbe\u786e\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u8bc4\u4f30\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5e73\u5747\u548c\u6298\u6263\u5956\u52b1MDP\u3002", "conclusion": "SISDMDP\u53ca\u5176\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21MDP\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
