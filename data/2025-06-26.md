<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 74]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.IT](#cs.IT) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Near Data Processing in Taurus Database](https://arxiv.org/abs/2506.20010)
*Shu Lin,Arunprasad P. Marathe,Per-Ȧke Larson,Chong Chen,Calvin Sun,Paul Lee,Weidong Yu*

Main category: cs.DB

TL;DR: 华为的GaussDB for MySQL（Taurus）通过近数据处理（NDP）将数据操作推送到存储层，显著减少网络数据传输、释放计算层CPU资源，并降低查询时间。实验显示，TPCH基准测试中18/22查询受益，数据传输减少63%，CPU时间减少50%。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统中，数据操作通常在计算层完成，导致大量数据传输和CPU资源占用。通过NDP将操作下推到存储层，可以优化性能。

Method: 在Taurus中设计并实现了NDP，将选择、投影和聚合等操作推送到存储服务器执行。

Result: TPCH基准测试（100GB）显示，18/22查询受益，数据传输减少63%，CPU时间减少50%；Q15表现更佳，数据传输减少98%，CPU时间减少91%，运行时间减少80%。

Conclusion: NDP在Taurus中显著提升了系统性能，减少了数据传输和计算资源占用，适用于大规模数据处理场景。

Abstract: Huawei's cloud-native database system GaussDB for MySQL (also known as
Taurus) stores data in a separate storage layer consisting of a pool of storage
servers. Each server has considerable compute power making it possible to push
data reduction operations (selection, projection, and aggregation) close to
storage. This paper describes the design and implementation of near data
processing (NDP) in Taurus. NDP has several benefits: it reduces the amount of
data shipped over the network; frees up CPU capacity in the compute layer; and
reduces query run time, thereby enabling higher system throughput. Experiments
with the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited
from NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.
On Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU
time by 91 percent; and run time by 80 percent.

</details>


### [2] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: MAIZX框架通过动态资源排名优化云操作，显著减少碳排放，测试中CO2排放减少85.68%。


<details>
  <summary>Details</summary>
Motivation: 云计算的高能耗和碳排放问题日益突出，亟需高效透明的解决方案，尤其是针对占87%组织使用的私有云基础设施。

Method: MAIZX框架利用实时和预测的碳强度、PUE及能耗数据，通过灵活排名算法动态优化资源。

Result: 测试显示，MAIZX比基准超管理器操作减少85.68%的CO2排放，并展示出可扩展性和有效性。

Conclusion: MAIZX为提升气候性能潜力同时保持运营效率提供了强大工具。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [4] [On the $h$-majority dynamics with many opinions](https://arxiv.org/abs/2506.20218)
*Francesco d'Amore,Niccolò D'Archivio,George Giakkoupis,Emanuele Natale*

Main category: cs.DC

TL;DR: 论文首次给出了在同步设置下，$h$-majority动态过程在非恒定$h$和$k$值下的收敛时间上界。


<details>
  <summary>Details</summary>
Motivation: 研究在初始偏置条件下，$h$-majority动态过程如何快速收敛到共识，填补了非恒定参数下的理论空白。

Method: 假设初始存在对某个多数意见的加性偏置，证明在特定条件下，过程以高概率在$O(\log n)$轮内收敛。

Result: 证明了在偏置为$\omega(\sqrt{x})$且初始多数意见支持节点数为$\omega(\log n)$时，过程快速收敛。

Conclusion: 论文的上界表明现有下界无法进一步优化，且所需偏置比之前研究更小。

Abstract: We present the first upper bound on the convergence time to consensus of the
well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,
for $h$ and $k$ that are both non-constant values.
  We suppose that, at the beginning of the process, there is some initial
additive bias towards some plurality opinion, that is, there is an opinion that
is supported by $x$ nodes while any other opinion is supported by strictly
fewer nodes.
  We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and
the initial plurality opinion is supported by at least $x = \omega(\log n)$
nodes, then the process converges to plurality consensus in $O(\log n)$ rounds
whenever $h = \omega(n \log n / x)$.
  A main corollary is the following: if $k = o(n / \log n)$ and the process
starts from an almost-balanced configuration with an initial bias of magnitude
$\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function
$h = \omega(k \log n)$ suffices to guarantee convergence to consensus in
$O(\log n)$ rounds, with high probability.
  Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to
reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than
$\widetilde{\Omega}(k / h)$.
  Moreover, the bias we require is asymptotically smaller than the
$\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the
$3$-majority dynamics: in our case, the required bias is at most any
(arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.

</details>


### [5] [PAT: a new algorithm for all-gather and reduce-scatter operations at scale](https://arxiv.org/abs/2506.20252)
*Sylvain Jeaugey*

Main category: cs.DC

TL;DR: PAT算法是一种用于实现all-gather和reduce-scatter操作的并行聚合树算法，适用于任意数量的节点，优化了NCCL库的性能。


<details>
  <summary>Details</summary>
Motivation: 解决环形算法在小规模或大规模操作中因线性延迟导致的性能不佳问题。

Method: 采用并行聚合树结构，减少网络传输次数和长距离通信，同时降低内部缓冲区需求。

Result: 算法在小规模操作中具有对数级网络传输效率，且不受总操作规模影响。

Conclusion: PAT算法显著提升了NCCL库在环形算法低效场景下的性能。

Abstract: This paper describes a new algorithm called PAT, for Parallel Aggregated
Trees, and which can be used to implement all-gather and reduce-scatter
operations. This algorithm works on any number of ranks, has a logarithmic
number of network transfers for small size operations, minimizes long-distance
communication, and requires a logarithmic amount of internal buffers,
independently from the total operation size. It is aimed at improving the
performance of the NCCL library in cases where the ring algorithm would be
inefficient, as its linear latency would show poor performance for small sizes
and/or at scale.

</details>


### [6] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: WattsOnAI是一个用于测量、分析和可视化AI工作负载的能源使用、功耗、硬件性能和碳排放的工具包，旨在解决现有工具的碎片化和缺乏系统性指标整合的问题。


<details>
  <summary>Details</summary>
Motivation: AI（尤其是大语言模型）的快速发展引发了对其能源使用和碳排放的担忧，但现有工具缺乏系统性指标整合和相关分析支持。

Method: WattsOnAI通过集成现有AI框架，提供标准化报告和细粒度时间序列数据，支持基准测试和可重复性，并深入分析硬件指标与模型性能的关联。

Result: 该工具包成功解决了现有工具的局限性，支持瓶颈识别和性能优化，推动研究社区在AI工作负载中考虑环境影响。

Conclusion: WattsOnAI促进了向更可持续的“绿色AI”实践的转变，代码已开源。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


### [7] [SuperSONIC: Cloud-Native Infrastructure for ML Inferencing](https://arxiv.org/abs/2506.20657)
*Dmitry Kondratyev,Benedikt Riedel,Yuan-Tang Chou,Miles Cochran-Branson,Noah Paladino,David Schultz,Mia Liu,Javier Duarte,Philip Harris,Shih-Chieh Hsu*

Main category: cs.DC

TL;DR: SONIC通过将ML推理卸载到协处理器来优化资源利用，SuperSONIC是其可扩展服务器基础设施，成功应用于多个科学实验和测试平台。


<details>
  <summary>Details</summary>
Motivation: 大规模科学实验中数据率和复杂ML算法的计算需求增长，推动了SONIC方法的采用。

Method: 开发SuperSONIC项目，利用Kubernetes集群和NVIDIA Triton Inference Server，标准化通信并优化性能。

Result: SuperSONIC已成功部署于CERN LHC、IceCube、LIGO等实验，并在多个测试平台上验证。

Conclusion: SuperSONIC为云原生时代提供可重用、可配置的框架，提升了跨领域加速器推理部署的效率。

Abstract: The increasing computational demand from growing data rates and complex
machine learning (ML) algorithms in large-scale scientific experiments has
driven the adoption of the Services for Optimized Network Inference on
Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it
to local or remote coprocessors to optimize resource utilization. Leveraging
its portability to different types of coprocessors, SONIC enhances data
processing and model deployment efficiency for cutting-edge research in high
energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the
SuperSONIC project, a scalable server infrastructure for SONIC, enabling the
deployment of computationally intensive tasks to Kubernetes clusters equipped
with graphics processing units (GPUs). Using NVIDIA Triton Inference Server,
SuperSONIC decouples client workflows from server infrastructure, standardizing
communication, optimizing throughput, load balancing, and monitoring.
SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at
the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory
(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)
and tested on Kubernetes clusters at Purdue University, the National Research
Platform (NRP), and the University of Chicago. SuperSONIC addresses the
challenges of the Cloud-native era by providing a reusable, configurable
framework that enhances the efficiency of accelerator-based inference
deployment across diverse scientific domains and industries.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [All-Pairs Shortest Paths with Few Weights per Node](https://arxiv.org/abs/2506.20017)
*Amir Abboud,Nick Fischer,Ce Jin,Virginia Vassilevska Williams,Zoe Xi*

Main category: cs.DS

TL;DR: 论文研究了在节点出边最多有d种不同权重限制下的全对最短路径问题（APSP），提出了新算法，改进了节点加权APSP的时间复杂度，并推广了对APSP假设的理解。


<details>
  <summary>Details</summary>
Motivation: 探索APSP问题在节点出边权重限制下的复杂性，改进现有算法，验证节点加权APSP是否为中间复杂度问题。

Method: 提出新算法，利用组合数学方法，针对不同权重限制d优化时间复杂度。

Result: 节点加权APSP的时间复杂度改进为O(n^2.686)，对d≤n^(3−ω−ϵ)的情况实现了次立方时间。

Conclusion: 研究深化了对APSP问题的理解，表明最困难实例需消耗线性权重数，技术为图算法中罕见的组合数学应用。

Abstract: We study the central All-Pairs Shortest Paths (APSP) problem under the
restriction that there are at most $d$ distinct weights on the outgoing edges
from every node. For $d=n$ this is the classical (unrestricted) APSP problem
that is hypothesized to require cubic time $n^{3-o(1)}$, and at the other
extreme, for $d=1$, it is equivalent to the Node-Weighted APSP problem. We
present new algorithms that achieve the following results:
  1. Node-Weighted APSP can be solved in time $\tilde{O}(n^{(3+\omega)/2}) =
\tilde{O}(n^{2.686})$, improving on the 15-year-old subcubic bounds
$\tilde{O}(n^{(9+\omega)/4}) = \tilde{O}(n^{2.843})$ [Chan; STOC '07] and
$\tilde{O}(n^{2.830})$ [Yuster; SODA '09]. This positively resolves the
question of whether Node-Weighted APSP is an ``intermediate'' problem in the
sense of having complexity $n^{2.5+o(1)}$ if $\omega=2$, in which case it also
matches an $n^{2.5-o(1)}$ conditional lower bound.
  2. For up to $d \leq n^{3-\omega-\epsilon}$ distinct weights per node (where
$\epsilon > 0$), the problem can be solved in subcubic time
$O(n^{3-f(\epsilon)})$ (where $f(\epsilon) > 0$). In particular, assuming that
$\omega = 2$, we can tolerate any sublinear number of distinct weights per node
$d \leq n^{1-\epsilon}$, whereas previous work [Yuster; SODA '09] could only
handle $d \leq n^{1/2-\epsilon}$ in subcubic time. This promotes our
understanding of the APSP hypothesis showing that the hardest instances must
exhaust a linear number of weights per node. Our result also applies to the
All-Pairs Exact Triangle problem, thus generalizing a result of Chan and
Lewenstein on "Clustered 3SUM" from arrays to matrices. Notably, our technique
constitutes a rare application of additive combinatorics in graph algorithms.

</details>


### [9] [LZSE: an LZ-style compressor supporting $O(\log n)$-time random access](https://arxiv.org/abs/2506.20107)
*Hiroki Shibata,Yuto Nakashima,Yutaro Yamaguchi,Shunsuke Inenaga*

Main category: cs.DS

TL;DR: 论文提出了一种新的LZ-like因子分解方法LZ-Start-End (LZSE)，支持高效随机访问，并在压缩效率上优于基于语法的压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LZ-like因子分解方法在支持高效随机访问时牺牲了压缩效率，而基于语法的方法虽然支持高效访问但无法直接应用于LZ-like因子分解。

Method: 提出LZSE因子分解方法，其中每个复制因子引用前面连续的因子序列，并研究了贪婪LZSE因子分解的线性时间计算。

Result: LZSE在压缩效率上优于基于语法的方法，且支持线性空间内的O(log n)时间随机访问。

Conclusion: LZSE在重复性度量方面优于基于语法的压缩方法，并提供了高效的随机访问支持。

Abstract: An LZ-like factorization of a string is a factorization in which each factor
is either a single character or a copy of a substring that occurs earlier in
the string. While grammar-based compression schemes support efficient random
access with linear space in the size of the compressed representation, such
methods are not known for general LZ-like factorizations. This has led to the
development of restricted LZ-like schemes such as LZ-End [Kreft and Navarro,
2013] and height-bounded (LZHB) [Bannai et al., 2024], which trade off some
compression efficiency for faster access. We introduce LZ-Start-End (LZSE), a
new variant of LZ-like factorizations in which each copy factor refers to a
contiguous sequence of preceding factors. By its nature, any context-free
grammar can easily be converted into an LZSE factorization of equal size.
Further, we study the greedy LZSE factorization, in which each copy factor is
taken as long as possible. We show how the greedy LZSE factorization can be
computed in linear time with respect to the input string length, and that there
exists a family of strings for which the size of the greedy LZSE factorization
is of strictly lower order than that of the smallest grammar. These imply that
our LZSE scheme is stronger than grammar-based compressions in the context of
repetitiveness measures. To support fast queries, we propose a data structure
for LZSE-compressed strings that permits $O(\log n)$-time random access within
space linear in the compressed size, where $n$ is the length of the input
string.

</details>


### [10] [Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data](https://arxiv.org/abs/2506.20141)
*Xiaoyu Li,Zhao Song,Jiahao Zhang*

Main category: cs.DS

TL;DR: 论文提出了一种基于线性规划松弛和舍入方案的算法，以减少会议投稿限制政策下的不必要拒稿，并在ICLR数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI会议投稿量激增导致严格的投稿限制政策可能误拒有价值论文，作者希望在不违反限制的前提下减少不必要拒稿。

Method: 将当前拒稿政策形式化为优化问题，开发基于线性规划松弛和舍入方案的算法。

Result: 在ICLR数据上，该方法保留了19.23%的论文且未违反作者限制，计算效率高（最多53.64秒）。

Conclusion: 该算法简单实用，显著减少不必要拒稿，有望改进当前计算机科学会议投稿政策。

Abstract: The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

</details>


### [11] [Cut-Query Algorithms with Few Rounds](https://arxiv.org/abs/2506.20412)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: 论文研究了在割查询模型中通过常数轮次解决经典图问题的算法，包括最小割、最小(s,t)-割和近似最大割，并提供了不同轮次和查询复杂度的权衡。


<details>
  <summary>Details</summary>
Motivation: 探索割查询模型中算法的轮复杂度，解决传统优化问题如全局最小割，并研究如何在有限轮次内高效完成。

Method: 提出多种算法，针对不同图类型（加权/非加权）和参数（如最小度），在常数轮次内完成最小割等任务，并分析查询复杂度。

Result: 在非加权图中，两轮内完成最小割需O~(n^{4/3})查询；加权图中，4r+3轮内完成需O~(rn^{1+(1+log_n W)/r})查询。

Conclusion: 割查询模型下，经典图问题可在常数轮次内高效解决，为算法设计提供了新的权衡思路。

Abstract: In the cut-query model, the algorithm can access the input graph $G=(V,E)$
only via cut queries that report, given a set $S\subseteq V$, the total weight
of edges crossing the cut between $S$ and $V\setminus S$. This model was
introduced by Rubinstein, Schramm and Weinberg [ITCS'18] and its investigation
has so far focused on the number of queries needed to solve optimization
problems, such as global minimum cut. We turn attention to the round complexity
of cut-query algorithms, and show that several classical problems can be solved
in this model with only a constant number of rounds.
  Our main results are algorithms for finding a minimum cut in a graph, that
offer different tradeoffs between round complexity and query complexity, where
$n=|V|$ and $\delta(G)$ denotes the minimum degree of $G$: (i)
$\tilde{O}(n^{4/3})$ cut queries in two rounds in unweighted graphs; (ii)
$\tilde{O}(rn^{1+1/r}/\delta(G)^{1/r})$ queries in $2r+1$ rounds for any
integer $r\ge 1$ again in unweighted graphs; and (iii)
$\tilde{O}(rn^{1+(1+\log_n W)/r})$ queries in $4r+3$ rounds for any $r\ge1$ in
weighted graphs. We also provide algorithms that find a minimum $(s,t)$-cut and
approximate the maximum cut in a few rounds.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLM）在政府遗留代码现代化中的应用，重点解决了输入限制问题，并评估了代码分块方法对文档生成质量的影响。


<details>
  <summary>Details</summary>
Motivation: 政府企业软件常使用遗留语言（如MUMPS或ALC），现有LLM未充分解决其独特挑战，如上下文窗口限制和遗留语言理解能力不足。

Method: 研究了多种代码分块方法，优化遗留代码文件的模块注释生成，并评估了不同LLM（如GPT-4o、Claude 3 Sonnet等）的表现。

Result: LLM选择的分区点与人类专家分区高度一致，且分块方法对文档生成任务有显著影响，LLM生成的分区注释比人类分区更准确和有用。

Conclusion: LLM可作为人类分区的替代方案，适用于大型代码库的现代化改造。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [13] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: 研究探讨了跨学科软件开发（CDSD）中的协作动态，揭示了领域专家（DEs）和软件开发人员（SDEs）之间的期望差异及其导致的摩擦。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发团队的多样化和跨学科化，DEs和SDEs之间的协作摩擦日益显著，研究旨在理解这些摩擦的来源和表现。

Method: 采用活动理论（AT）作为分析框架，结合24次访谈和293人的大规模验证调查。

Result: 识别了SDEs的8种期望和DEs的6种期望，并通过AT框架揭示了21种摩擦及其产生机制。

Conclusion: 研究为理解CDSD动态提供了理论框架，并为未来研究和实践提供了实用建议。

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [14] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: 研讨会探讨了AI与敏捷开发的整合挑战，提出了研究路线图以推动行业与学术合作。


<details>
  <summary>Details</summary>
Motivation: 解决AI与敏捷开发整合中的实际挑战，如工具、治理、数据质量和技能缺口。

Method: 通过互动会议识别挑战，系统分析并制定研究路线图。

Result: 提出了包括短期解决方案和长期目标的研究议程。

Conclusion: 研讨会成果为行业与学术合作提供了明确方向，推动AI在敏捷开发中的成功实施。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [15] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: 本文提出了十条简单规则，旨在提高研究软件工程（RSEng）的可及性，并为研究团队领导者提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程（RSEng）对高质量研究软件至关重要，但许多研究者对其不了解或难以应用。

Method: 提出了十条简单规则，帮助研究团队领导者整合RSEng。

Result: 通过遵循这些规则，可以提高研究软件的质量、可重复性和可信度。

Conclusion: 这些规则最终能带来更好、更可重复且更可信的研究成果。

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [16] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: 本文系统回顾了数字孪生（DT）在复杂系统中的组合与验证方法，分析了21项研究，发现组合机制讨论多但形式化不足，验证方法多样但形式验证较少，需标准化框架。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在复杂系统（如CPS和SoS）中的应用日益广泛，但组合与验证方法缺乏系统研究，亟需标准化和形式化支持。

Method: 通过系统文献综述，分析了2022-2024年的21项研究，探讨了组合机制、SoS特性及验证方法的范围与挑战。

Result: 组合机制形式化不足，验证方法以半形式和仿真为主，形式验证较少；技术挑战包括模型不确定性和集成复杂性。

Conclusion: 需开发标准化、可扩展的验证框架和严格的组合方法，以支持复杂数字孪生的实现。

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [17] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: 论文提出了一种基于数据集映射的新方法，通过识别和缓解难以学习的异常样本（“坏种子”）来提高漏洞检测模型的训练效率。该方法在主动学习框架中整合样本学习难度信息，显著提升了F1分数和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞检测中机器学习模型的性能受限于低质量训练数据（噪声、错误标注或样本不平衡），因此需要一种方法优化样本选择以提高模型效果。

Method: 提出数据集映射方法，分类训练样本的学习难度，并将其整合到主动学习框架中，优先过滤有害样本并强调信息丰富的样本。

Result: 实验显示，该方法在Big-Vul数据集上比随机选择提升F1分数45.36%（DeepGini）和45.91%（K-Means），优于标准主动学习方法61.46%（DeepGini）和32.65%（K-Means）。

Conclusion: 该方法通过优化样本选择和过滤坏种子，显著提升了漏洞检测的效率和鲁棒性，并为未来数据集构建提供了改进方向。

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [18] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型（LLM）的半自动化BIM合规检查方法，显著减少时间和错误。


<details>
  <summary>Details</summary>
Motivation: 解决BIM中手动合规检查耗时且易出错的问题。

Method: 集成LLM（如GPT、Claude等）与Revit软件，生成Python脚本进行半自动化检查。

Result: 案例研究表明，系统减少了合规检查的时间和精力，提高了准确性。

Conclusion: 该方法为BIM合规检查提供了全面、适应性强且经济的解决方案，具有广泛应用潜力。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [19] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出CCIBench数据集和CCISolver框架，解决代码注释不一致问题，显著提升检测和修复性能。


<details>
  <summary>Details</summary>
Motivation: 代码注释不一致（CCI）对软件开发、测试和维护有负面影响，现有研究因数据集不准确和解决方案不足而效果有限。

Method: 通过定量分析现有数据集，提出高质量数据集CCIBench，并开发基于LLM的端到端框架CCISolver。

Result: CCISolver在检测任务中F1-score达89.54%，修复任务GLEU分数相对提升18.84%，推理速度比基线快36%。

Conclusion: CCISolver在性能和实用性上显著优于现有方法，具备实际应用潜力。

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [20] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML扩展了Lean Inception，通过结构化活动整合数据和ML技术约束，提升ML产品构思的清晰度和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统构思方法缺乏对ML特有挑战的支持，可能导致产品愿景不明确或不切实际。

Method: 基于技术转移模型开发Define-ML，通过静态和动态验证评估其效用和易用性。

Result: Define-ML有效澄清数据问题、对齐业务目标，并促进跨职能协作，但需专家支持降低学习曲线。

Conclusion: Define-ML为ML产品构思提供了开放且验证的方法，增强数据和技术可行性的意识。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: MiLAAP是一种基于学习的注意力框架，用于预测无线网络中信道占用状态，无需节点间状态共享，显著提高了动态网络中的预测准确性和吞吐效率。


<details>
  <summary>Details</summary>
Motivation: 在动态无线网络中，传统的信道调度方法因状态共享导致通信开销大，影响吞吐效率。MiLAAP旨在通过本地学习和预测减少开销。

Method: 提出MiLAAP框架，利用自注意力机制和多头自注意力机制，分别预测信道占用状态和节点移动轨迹，仅依赖本地被动观测数据。

Result: MiLAAP在动态网络中实现了约100%的信道状态预测准确性，且具有零样本泛化能力。

Conclusion: MiLAAP通过本地学习和预测显著提升了动态网络的性能，避免了通信开销，适用于不同移动模式和信道序列。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


### [22] [Notes on Degeneracy and Robustness](https://arxiv.org/abs/2506.19974)
*Indrakshi Dey,Nicola Marchetti*

Main category: cs.NI

TL;DR: 论文定义了衡量网络资源可替代性的新指标，以解决静态、移动和动态网络的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 研究退化性（不同结构元素在约束下实现相同功能的能力）对网络鲁棒性的影响，提出新指标以量化资源可替代性。

Method: 定义并公式化了几种新的资源可替代性指标，用于评估静态、移动和动态网络的鲁棒性。

Result: 提出的指标能够量化退化性对网络鲁棒性的贡献，支持更快速和自适应的恢复能力。

Conclusion: 退化性通过资源可替代性增强网络鲁棒性，新指标为网络设计提供了实用工具。

Abstract: Degeneracy is the ability of structurally different elements to perform the
same function or yield the same output under certain constraints. In contrast
to redundancy, which implies identical backups, degeneracy allows diverse
components to step in and perform the same or similar role. Mathematically, it
is about mapping multiple distinct elements into the same function. In a
degenerate system, failure in one part can be compensated by others not
structurally linked. System functions are distributed within the system itself
or the entire network. This renders faster and more adaptive recovery. In this
work, we define and formulate several novel metrics for resource fungibility to
address robustness in networks (static/mobile/dynamic).

</details>


### [23] [A clusterability test for directed graphs](https://arxiv.org/abs/2506.20111)
*Mario R. Guarracino,Pierre Miasnikof,Alexander Y. Shestopaloff,Houyem Demni,Cristián Bravo,Yuri Lawryshyn*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this article, we extend a statistical test of graph clusterability, the
$\delta$ test, to directed graphs with no self loops. The $\delta$ test,
originally designed for undirected graphs, is based on the premise that graphs
with a clustered structure display a mean local density that is statistically
higher than the graph's global density. We posit that graphs that do not meet
this necessary (but not sufficient) condition for clusterability can be
considered unsuited to clustering. In such cases, vertex clusters do not offer
a meaningful summary of the broader graph. Additionally in this study, we aim
to determine the optimal sample size (number of neighborhoods). Our test,
designed for the analysis of large networks, is based on sampling subsets of
neighborhoods/nodes. It is designed for cases where computing the density of
every node's neighborhood is infeasible. Our results show that the $\delta$
test performs very well, even with very small samples of neighborhoods ($1\%$).
It accurately detects unclusterable graphs and is also shown to be robust to
departures from the underlying assumptions of the $t$ test.

</details>


### [24] [A Detailed Measurement View on IPv6 Scanners and Their Adaption to BGP Signals](https://arxiv.org/abs/2506.20383)
*Isabell Egloff,Raphael Hiesgen,Maynard Koch,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 分析IPv6扫描行为及其在不同网络条件下的表现，提供部署网络望远镜的操作指南。


<details>
  <summary>Details</summary>
Motivation: IPv6扫描的成功率较低，但越来越多的扫描者试图掌握这一技术，因此需要研究其行为模式和网络条件的影响。

Method: 通过四个网络望远镜（其中一个定期重新配置BGP公告）观察扫描者行为，分析其时间行为、目标选择、工具和指纹等。

Result: 发现较大的前缀中的静默子网仍不可见，而BGP前缀公告能迅速吸引扫描者。

Conclusion: 基于研究结果，提供了部署网络望远镜以增加IPv6扫描可见性的操作指南。

Abstract: Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes
successfully is still a challenge, which an increasing crowd of actors tries to
master. In this paper, we analyze current IPv6 scanning under various network
conditions. We observe scanner behavior during eleven months in four network
telescopes, one of which is periodically reconfigured by changing BGP
announcements. We analyze and classify the observed scanners w.r.t. their
temporal behavior, their target, and network selection strategy, as well as
their individual tools, fingerprints, and correlations across categories. We
find that silent subnets of larger prefixes remain invisible, whereas BGP
prefix announcements quickly attract attention by scanners. Based on our
findings, we derive operational guidance on how to deploy network telescopes to
increase visibility of IPv6 scanners.

</details>


### [25] [Semantic Caching for Improving Web Affordability](https://arxiv.org/abs/2506.20420)
*Hafsa Akbar,Danish Athar,Muhammad Ayain Fida Rana,Chaudhary Hammad Javed,Zartash Afzal Uzmi,Ihsan Ayyub Qazi,Zafar Ayyub Qazi*

Main category: cs.NI

TL;DR: 利用大型语言模型（LLM）进行语义缓存以减少网页数据量，提升发展中国家网络可负担性。


<details>
  <summary>Details</summary>
Motivation: 网页内容快速增长导致数据成本高昂，尤其在发展中国家。通过语义缓存重用相似图像，降低数据传输量。

Method: 分析50个新闻和媒体网站的4264张图像及40000对图像，评估语义替换潜力。提出概念验证架构，比较商业和开源多模态LLM的性能。

Result: 部分网站类别37%的图像可替换，用户可节省约10%字节。GPT-4o表现最佳，开源LLaMA 3.1性能接近。

Conclusion: 语义缓存显著减少数据传输，对用户和运营商均有益。需解决伦理和实际问题，如语义保留、隐私和运营商阻力。

Abstract: The rapid growth of web content has led to increasingly large webpages,
posing significant challenges for Internet affordability, especially in
developing countries where data costs remain prohibitively high. We propose
semantic caching using Large Language Models (LLMs) to improve web
affordability by enabling reuse of semantically similar images within webpages.
Analyzing 50 leading news and media websites, encompassing 4,264 images and
over 40,000 image pairs, we demonstrate potential for significant data transfer
reduction, with some website categories showing up to 37% of images as
replaceable. Our proof-of-concept architecture shows users can achieve
approximately 10% greater byte savings compared to exact caching. We evaluate
both commercial and open-source multi-modal LLMs for assessing semantic
replaceability. GPT-4o performs best with a low Normalized Root Mean Square
Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA
3.1 model shows comparable performance, highlighting its viability for
large-scale applications. This approach offers benefits for both users and
website operators, substantially reducing data transmission. We discuss ethical
concerns and practical challenges, including semantic preservation, user-driven
cache configuration, privacy concerns, and potential resistance from website
operators

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM是一个预处理框架，通过模式聚类和自适应掩码策略，提升时间序列插补模型在真实缺失数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列插补模型使用人工掩码模拟缺失数据，无法应对真实场景中复杂、异构的缺失模式。

Method: 结合模式聚类和自适应掩码策略，并提供理论学习保证，处理多样化的真实缺失模式。

Result: 在加州水务、电力数据集及基准测试中，DIM-SUM表现优于传统方法，达到相似精度但处理时间和训练数据需求更低。

Conclusion: DIM-SUM显著提升了时间序列插补模型在真实缺失数据上的鲁棒性和效率。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [27] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: ML会议应设立专门的“反驳与批评”（R&C）轨道，以纠正研究中的错误并促进自我修正。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域快速发展，但存在错误或误导性研究被接受的问题，缺乏系统性纠正机制。

Method: 提出在ML会议中建立R&C轨道，讨论其设计、评审原则及潜在问题，并以ICLR 2025为例。

Result: R&C轨道可为挑战先前研究提供高声誉平台，促进研究生态的自我修正。

Conclusion: ML会议应建立官方机制，支持研究的自我修正。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [28] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为STIMULUS的新算法，用于解决多目标优化问题，通过递归框架更新梯度估计，提高了收敛性能并降低了样本复杂度。还提出了增强版本STIMULUS-M和自适应批处理版本STIMULUS+/STIMULUS-M+。


<details>
  <summary>Details</summary>
Motivation: 多目标优化（MOO）在机器学习等领域应用广泛，但现有方法收敛速度和样本复杂度表现不佳。

Method: 提出STIMULUS算法，采用递归框架更新梯度估计；引入动量项的STIMULUS-M；进一步提出自适应批处理版本STIMULUS+/STIMULUS-M+。

Result: 在非凸和强凸设置下分别达到$O(1/T)$和$O(\exp{-\mu T})$的收敛速度，样本复杂度达到当前最优水平。

Conclusion: STIMULUS系列算法在多目标优化中表现出色，显著提升了收敛性能和样本效率。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [29] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: 论文提出FlightKooba框架，结合HIPPO方法、Koopman理论和状态空间方程，显著提升飞行轨迹预测的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Koopman理论的飞行轨迹预测模型效率低、可解释性差且计算量大，亟需改进。

Method: 结合HIPPO方法、Koopman理论和状态空间方程，直接从数据构建Koopman算子，减少可训练参数。

Result: FlightKooba在时间和内存消耗上表现优异，训练时间接近Mamba模块，内存减少50%以上，参数减少十倍。

Conclusion: FlightKooba为Koopman算子的快速计算提供了新方法，为时间序列预测与控制的结合开辟了新途径。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [30] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: 论文提出了一种结合自适应关键帧提取和因果感知强化学习的智能框架，以优化多用户VR交互中的QoE。通过新QoE度量和PS-CDDPG算法，显著降低了延迟并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多用户VR交互中忽视了带宽、CPU频率与用户感知之间的因果关系，限制了QoE的提升。

Method: 提出PS-CDDPG算法，结合DDPG和因果推理，优化关键帧比例、带宽和计算资源。

Result: 实验表明，该框架显著降低延迟、提升QoE并保持公平性。

Conclusion: 该框架在多用户VR交互中实现了优于基准方法的性能。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [31] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 提出了一种基于正交卷积核正则化的类感知软剪枝框架，实现快速精确的机器遗忘，具有毫秒级响应时间。


<details>
  <summary>Details</summary>
Motivation: 满足GDPR等隐私法规要求，选择性移除预训练神经网络中的特定类别知识，解决现有方法在遗忘速度和预测准确性之间的权衡问题。

Method: 通过正交约束训练解耦卷积滤波器和特征表示，利用激活差异分析高效识别类别特定通道。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上验证，实现稳定剪枝、目标类完全遗忘、保留数据精度损失最小，显著降低成员推理攻击风险。

Conclusion: 该框架为MLaaS场景提供了高效、实时的机器遗忘解决方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [32] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 论文提出DeKA-g算法，通过知识蒸馏和自适应传输优化生成语义通信系统，显著提升边缘与云端生成内容的一致性及传输效率。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）的快速增长导致网络流量激增，生成语义通信（GSC）虽能通过传输紧凑信息解决此问题，但知识对齐仍具挑战性。

Method: 提出DeKA-g算法，包含元词辅助知识蒸馏（MAKD）和可变率分组SNR自适应（VGSA）两种方法，优化知识对齐与传输效率。

Result: DeKA-g将边缘与云端生成图像的对齐度提升44%，压缩率适应效率提高116%，低SNR条件下性能提升28%。

Conclusion: DeKA-g有效解决了GSC系统中的知识对齐问题，显著提升了生成内容的一致性和传输效率。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [33] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN是一种新颖的自监督图学习框架，通过谱引导技术避免负采样需求，利用拉普拉斯信号捕获结构表示，无需对比目标或手工增强。


<details>
  <summary>Details</summary>
Motivation: 解决传统自监督图学习方法依赖负采样或手工增强的问题，提出更高效、简单的替代方案。

Method: 结合拉普拉斯信号，通过最大-最小中心性优化预计算谱增强，并采用对抗性引导训练方案增强特征学习。

Result: 在不同基准数据集上表现优于现有自监督图学习方法。

Conclusion: LaplaceGNN为高效学习表达性图表示提供了有前景的方向。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [34] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: 论文使用深度神经网络（DNN）预测电价，并结合可解释AI方法（如SHAP和Gradient）分析电价驱动因素，以增强对电力市场的理解。


<details>
  <summary>Details</summary>
Motivation: 电力市场高度复杂，传统计量经济学方法（白盒模型）不如DNN强大，但DNN缺乏可解释性。本文旨在通过XAI方法揭示电价动态的驱动因素。

Method: 使用DNN预测电价，并应用SHAP、Gradient等可解释方法及热图（显著性图）分析五个电力市场中各特征的行为和贡献。提出SSHAP值和SSHAP线的新概念。

Result: 通过XAI方法成功揭示了电价动态的驱动因素，并增强了高维表格模型的复杂表示能力。

Conclusion: 结合DNN和XAI方法能有效提升对电力市场运作的理解，SSHAP值和SSHAP线为高维模型的可解释性提供了新工具。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [35] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 提出了一种新的后处理框架，通过检索训练案例测量神经网络决策的不确定性，并引入两种新指标：决策变化和层不确定性，提升不确定性估计效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络在高风险领域（如医疗诊断或自动驾驶）中可能返回错误决策，需要有效方法来检测和缓解这些错误。

Method: 基于检索与查询激活向量相似的训练案例，提出决策变化和层不确定性两种新指标，评估分类模型在CIFAR-10和MNIST数据集上的表现。

Result: 新指标在不确定性估计上优于基于softmax的置信度方法，尤其在具有挑战性的分类任务中表现突出。

Conclusion: 提出的框架和指标能有效提升神经网络决策的不确定性估计，适用于高风险领域。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [36] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 通过硬件使用优化改进联邦学习中的本地训练过程，使用贪婪随机搜索优化本地批量大小，提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，参与者共享硬件但无信息交换，可能导致训练配置不当，影响训练效率。

Method: 利用联邦学习的并行处理特性，采用贪婪随机搜索优化本地批量大小。

Result: 与默认参数设置相比，该方法提高了收敛速度，且接近本地参数优化的效果。

Conclusion: 优化本地批量大小可显著提升联邦学习的训练效率。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [37] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 该研究探讨了强化学习（特别是DQN）在轴承故障诊断中的可行性，结果显示其在适应性上优于传统方法，但计算需求较高。


<details>
  <summary>Details</summary>
Motivation: 轴承故障可能导致严重的运行中断和维护成本，现有方法依赖大量标记数据且适应性不足。

Method: 采用深度Q网络（DQN）进行轴承故障分类任务，优化奖励结构以提升适应性。

Result: RL模型在受控条件下与传统方法性能相当，但在适应性上表现更优，计算需求较高。

Conclusion: 强化学习有潜力补充传统方法，为自适应诊断框架铺平道路。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [38] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 论文分析了联邦学习中恶意梯度泄漏攻击的局限性，并提出了一种轻量级的客户端检测机制。


<details>
  <summary>Details</summary>
Motivation: 研究恶意服务器通过操纵全局模型获取客户端敏感信息的风险，并从防御者角度分析其实际可行性。

Method: 通过理论分析和实验验证，评估攻击的有效性和隐蔽性，并提出客户端检测机制。

Result: 发现攻击在现实中难以同时高效且隐蔽，且可通过简单监控检测。

Conclusion: 恶意梯度泄漏攻击在实践中受限，轻量级防御机制可有效保护隐私。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


### [39] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 研究比较了自回归（AR）和掩码扩散模型（MDM）在解码器框架下的表现，发现解码器MDM在生成速度和困惑度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 比较AR和MDM时，架构差异导致不公平对比，需在相同架构下评估核心范式差异。

Method: 在解码器框架下评估MDM（称为Any-Order AR），比较其与标准AR的性能。

Result: 解码器MDM在生成速度上快25倍，困惑度与标准AR相当。

Conclusion: 解码器MDM在生成效率上有优势，为未来模型设计提供参考。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [40] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: 论文提出了一种高效的方法，用于评估特征组在广义可加模型（GAMs）中的重要性，无需重新训练模型，支持事后定义组和重叠组，并在高维数据中保持有效性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释机器学习中常忽视特征组的联合信号，而实际上许多重要预测因子是特征组的综合效应，尤其是在多模态数据集中。

Method: 提出了一种新方法，用于确定特征组的重要性，适用于GAMs，具有高效性、无需重新训练、支持事后定义组和重叠组等特点。

Result: 通过合成实验和两个案例研究（抑郁症症状识别和髋关节置换术后健康社会决定因素）验证了方法的有效性，显示特征组分析比单特征分析更全面。

Conclusion: 分析特征组的重要性提供了更准确、全面的视角，尤其在医学问题中，弥补了单特征分析的不足。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [41] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES是一种新型分层k-means聚类算法，支持多模态数据，并利用LLM生成语义丰富的聚类标题和描述，提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂数据集的快速增长需要既能有效分组又能提供人类可理解洞察的工具。

Method: HERCULES通过递归应用k-means聚类构建层次结构，并利用LLM生成聚类标题和描述。支持两种表示模式：直接模式和描述模式。

Result: HERCULES能够从复杂数据集中提取有意义的分层知识，并通过交互式可视化工具增强分析。

Conclusion: HERCULES为多模态数据提供了高效且可解释的分层聚类解决方案。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [42] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: TRACED方法通过结合过渡预测误差和共同学习能力，改进了UED框架中的任务生成，提升了零样本泛化能力并减少了环境交互需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习代理在未见环境中的泛化问题，改进现有UED方法中仅依赖值函数损失衡量学习潜力的局限性。

Method: 提出过渡预测误差作为后悔近似的新项，并引入共同学习能力度量任务间影响，形成TRACED方法。

Result: TRACED在多个基准测试中提升了零样本泛化能力，且环境交互需求减少至基线方法的50%。

Conclusion: 通过优化后悔近似和显式建模任务关系，TRACED实现了样本高效的课程设计。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [43] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 论文提出了一种基于共振放电神经元（RF）的无线分割计算架构，用于高效处理时域信号，避免昂贵的频谱预处理，显著降低计算和传输能耗。


<details>
  <summary>Details</summary>
Motivation: 传统漏电积分放电神经元（LIF）无法有效捕捉边缘应用中具有丰富频谱特征的流信号，如无线传感和音频识别。

Method: 采用共振放电神经元（RF）直接处理时域信号，通过可调谐频率提取时域局部频谱特征，并利用OFDM模拟无线接口传输脉冲信号。

Result: 实验表明，RF-SNN架构在音频分类和调制分类任务中与传统LIF-SNN和ANN精度相当，同时显著降低脉冲率和总能耗。

Conclusion: RF-SNN架构为边缘应用提供了一种高效节能的时域信号处理解决方案。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [44] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种基于深度展开的量子联邦学习新方法，通过动态调整超参数解决客户端异构性问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 客户端异构性对量子联邦学习性能构成挑战，传统聚合方法在高度异构环境中表现不佳。

Method: 利用深度展开技术，客户端自主优化学习率和正则化因子等超参数，动态适应训练行为。

Result: 在IBM量子硬件和Qiskit Aer模拟器上实现约90%的准确率，显著优于传统方法的55%。

Conclusion: 该方法通过自适应性优化解决了传统QFL的核心限制，适用于医疗和基因组研究等复杂挑战。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [45] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: ERDM框架将滚动预测结构与高性能扩散模型（EDM）结合，解决了高维混沌系统中复杂时间依赖性和不确定性增长的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在高维混沌系统中难以建模复杂时间依赖性和不确定性增长，需要一种更有效的框架。

Method: ERDM通过调整EDM的核心组件（噪声计划、网络预处理、Heun采样器）并引入新的损失加权方案、初始化策略和混合序列架构，实现了滚动预测。

Result: 在2D Navier-Stokes模拟和ERA5全球天气预报中，ERDM表现优于其他扩散基线模型。

Conclusion: ERDM为扩散模型在序列生成问题中建模不确定性提供了一种灵活且强大的框架。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [46] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: 论文探讨了在模型参数化适中的情况下，损失加权在最后一层重新训练（LLR）中的有效性，并指出权重需考虑模型的相对过参数化。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在判别任务中表现提升，其克服训练数据偏差的能力受到关注。研究旨在探索损失加权在参数化适中情况下的作用。

Method: 通过理论和实践分析，研究了最后一层重新训练（LLR）中损失加权的有效性，并强调权重需考虑模型的相对过参数化。

Result: 研究表明，损失加权在LLR中仍然有效，但权重需根据模型的过参数化程度调整。

Conclusion: 在参数化适中的情况下，损失加权是有效的，但需结合模型的过参数化特性调整权重。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [47] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种新的理论和计算框架，用于生成多样化的行动方案（COA）池，以支持多智能体任务分配和适应性任务排序。


<details>
  <summary>Details</summary>
Motivation: 在灾害响应、搜救和军事任务中，多智能体操作需要自动化的行动方案规划，同时考虑环境变化和智能体能力差异。

Method: 采用图抽象和遗传算法生成多样化的COA池，并结合图神经网络进行任务排序。

Result: 模拟测试显示，该方法在性能上显著优于随机基线，任务排序接近最优，且能在50分钟内规划20个COA。

Conclusion: 该框架为多智能体任务分配提供了高效且适应性强的解决方案。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [48] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明（zk-SNARKs）的验证框架，用于在边缘设备上确认数据删除操作，同时保护隐私并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在个性化模型上执行数据删除时可能面临的版权、偏见或监管问题，确保删除操作的可验证性。

Method: 利用zk-SNARKs设计算法，支持高效的数据删除操作验证，同时减少计算和内存开销，适应边缘设备的资源限制。

Result: 验证框架在保持个性化性能的同时，实现了可验证的数据删除，且计算开销低。

Conclusion: 该方法为边缘设备提供了一种隐私保护、高效且可验证的机器学习数据删除解决方案。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [49] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: 论文提出了一种名为CLVQVAE的框架，通过向量量化技术跨层映射神经网络表示，以压缩和解释重复的特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单层神经表示，忽略了跨层叠加和冗余问题，导致特征演化难以理解。

Method: 结合top-k温度采样和EMA码本更新的向量量化方法，以及基于方向相似性的k-means++初始化码本。

Result: 框架能够将重复特征压缩为紧凑、可解释的概念向量。

Conclusion: CLVQVAE为跨层特征演化提供了新的解释方法。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [50] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: 提出了一种结合LSH-RHP和DynED框架的新方法，用于处理多类不平衡数据流分类问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多类不平衡数据流分类问题研究较少，动态不平衡比率的处理是关键挑战。

Method: 通过LSH-RHP对多数类进行欠采样，生成平衡训练集，提升集成模型的预测性能。

Result: 在23个真实和10个半合成数据集上，LSH-DynED在Kappa和mG-Mean指标上优于15种现有方法。

Conclusion: LSH-DynED在大规模、高维数据集上表现优异，具有适应性和鲁棒性，为未来研究提供了指导。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [51] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: 提出一种基于知识蒸馏的新方法，高效且高精度地量化图神经网络的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中，量化GNN的预测不确定性对可信度至关重要，但现有方法（如贝叶斯和集成方法）计算成本高且无法充分捕捉模型多样性。

Method: 采用自蒸馏技术，同一网络同时作为教师和学生模型，避免独立训练多个网络，并开发新的不确定性度量标准以捕捉多样性。

Result: 在MIMIC-IV和Enzymes数据集上的实验表明，该方法能有效捕捉预测不确定性，性能与MC Dropout和集成方法相当。

Conclusion: 所提方法在高效性和精度上优于传统不确定性量化方法，适用于临床环境。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [52] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: 论文研究了使用随机生成的数据预训练模型，从算法复杂性角度理论支持，并实证验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索随机生成数据在模型预训练中的潜力，补充现有理论，并验证其在实际数据中的表现。

Method: 基于算法复杂性理论，提出随机数据预训练方法，并通过实验验证其零样本学习和泛化能力。

Result: 模型在预训练后表现出零样本学习能力，且随规模提升性能增强；微调后收敛更快、泛化更好。

Conclusion: 随机数据预训练是一种有效方法，能提升模型性能，尤其在零样本学习和泛化方面。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [53] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 本文提出了一种利用大型语言模型（LLMs）自动生成开放式指令的方法，通过重新标注失败轨迹来增强强化学习中的指令跟随策略，减少对人类标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中指令跟随策略对大量人工标注数据的依赖以及稀疏奖励学习困难的问题。

Method: 利用LLMs从已收集的代理轨迹中自动生成开放式指令，重新标注失败轨迹以识别隐含完成的子任务，从而丰富训练数据。

Result: 在Craftax环境中验证，样本效率、指令覆盖率和策略性能均优于现有基线。

Conclusion: LLM引导的开放式指令重新标注能有效提升指令跟随强化学习的性能。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [54] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的监督耦合矩阵-张量分解（SCMTF）方法，用于整合患者报告结果（PROs）和实验室数据，预测溃疡性结肠炎（UC）患者的药物持续性。该方法首次将张量分解应用于UC领域和PRO数据，并展示了其在高缺失数据下的有效性。


<details>
  <summary>Details</summary>
Motivation: 患者报告的症状（PROs）通常噪声大、主观性强且数据稀疏，因此在表型分析中常被忽略。本文旨在探索如何利用PROs进行表型分析，以预测UC患者的药物持续性。

Method: 提出了一种监督耦合矩阵-张量分解（SCMTF）方法，整合时间序列的PROs和实验室数据以及静态特征，使用深度学习框架提高模型的灵活性和易训练性。

Result: 最佳模型在测试集上预测8个月和20个月后药物变化的AUC分别为0.853和0.803，并提取了可解释的表型。

Conclusion: 低秩矩阵和张量分解方法可成功应用于UC领域和高缺失PRO数据，证明PROs包含通常被忽略的相关信息。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [55] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: 本文综述了预测性维护（PdM）中回归与分类方法的比较，强调了各自优势、挑战及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 预测性维护对工业可靠性至关重要，但缺乏回归与分类方法的独立比较研究。

Method: 通过文献综述，分析回归和分类方法在预测设备故障和剩余寿命中的应用。

Result: 回归方法提供剩余寿命估计，分类方法预测故障概率；挑战包括数据不平衡和高维特征。

Conclusion: 未来研究应关注混合方法、公共数据集和开源工具，以推动PdM发展。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [56] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: 论文提出了一种名为MEL的多级集成学习框架，用于在边缘计算环境中实现高容错性的AI推理服务，同时保持低延迟和高准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘计算环境资源有限且易发生故障，传统的容错方法（如云故障转移或压缩备份）往往牺牲延迟或准确性，无法满足关键边缘推理服务的需求。

Method: MEL通过同时训练多个轻量级备份模型，这些模型可以协作运行以相互优化，或在故障时独立运行。方法被表述为一个多目标优化问题，鼓励模型多样性并确保各自性能。

Result: 实验表明，MEL在视觉、语言和音频数据集上表现接近原始架构，同时提供容错性和部署灵活性。集成模型大小仅为原模型的40%，在故障情况下仍能保持95.6%的准确性。

Conclusion: MEL是一种有效的边缘推理容错框架，能够在资源受限的环境中实现高性能和高可靠性。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [57] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: 利用预训练的多模态地球观测模型生成大范围、高精度的Live Fuel Moisture Content (LFMC)地图，显著降低了RMSE，并提供了自动化管道。


<details>
  <summary>Details</summary>
Motivation: 野火风险监测需要高分辨率、低延迟的LFMC数据，但传统地面采样成本高且更新慢。

Method: 采用预训练的多模态地球观测模型，生成空间完整的LFMC地图。

Result: 相比随机初始化模型，RMSE降低了20%，并在美国两个野火影响区域验证了有效性。

Conclusion: 该方法为野火研究和应急响应提供了高效、低成本的LFMC监测解决方案。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [58] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: 论文提出了一种名为PoV的方法，用于在LTI-DAE系统中进行因果发现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（DIPCA）在纯动态系统中有效，但在反馈控制或守恒定律耦合的系统中表现不佳。

Method: PoV方法通过分区变量和约束矩阵的条件数来识别因果驱动变量。

Result: PoV方法能有效识别因果驱动变量，适用于更广泛的系统类型。

Conclusion: PoV方法在LTI-DAE系统中表现优越，扩展了因果发现的应用范围。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [59] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络和反事实扰动的因果结构发现框架，用于偏微分方程（PDEs），优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法如残差最小化或稀疏回归无法量化算子级必要性，因此需要一种新方法来评估候选微分算子的影响。

Method: 通过功能干预和引入因果敏感性指数与结构偏差度量，在神经代理模型中评估算子影响。

Result: 在理论和实验上验证了方法的有效性，能够准确恢复因果算子支持，并在噪声、冗余和数据稀缺情况下表现优异。

Conclusion: 该框架将因果PDE发现定位为基于结构因果模型和变分残差分析的可解释推理任务。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [60] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT结合权重剪枝和激活稀疏性，提出双稀疏框架，优化GPU执行，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）部署成本高，现有剪枝方法忽略运行时激活稀疏性。

Method: 将激活稀疏性重新解释为动态结构化权重稀疏性，结合非结构化权重剪枝，扩展OBC框架并引入输出残差。

Result: 在LLaMA-2和LLaMA-3上，DuoGPT在同等加速下准确率提升9.17%。

Conclusion: DuoGPT通过双稀疏设计有效降低部署成本，同时保持高精度。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [61] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: 论文提出了一种名为Anubis的零样本归因工具，通过假设检验和分布测试方法，利用LLM的样本和密度估计，高效区分代码来源。


<details>
  <summary>Details</summary>
Motivation: 随着大量代码由大型语言模型生成，如何准确归因代码来源成为重要问题。传统方法因维度灾难难以实现，需要新的解决方案。

Method: 提出Anubis工具，将归因问题转化为分布测试问题，利用LLM的样本和密度估计进行零样本归因。

Result: 实验表明，Anubis在区分DeepSeek-Coder、CodeGemma和Stable-Code等LLM时，仅需约2000样本即可达到高AUROC分数（≥0.9）。

Conclusion: Anubis通过分布测试方法，有效解决了代码归因问题，为LLM生成的代码来源识别提供了实用工具。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [62] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: 提出了一种名为Affective Priming Score (APS)的数据驱动方法，用于检测情感计算中受启动效应影响的数据点，显著降低了模型的误分类率。


<details>
  <summary>Details</summary>
Motivation: 情感计算中启动效应的模糊性挑战，尤其是其对生理信号数据的影响尚未充分研究，可能导致模型误分类。

Method: 开发了APS方法，为每个数据点分配分数以量化启动效应的影响，并在SEED和SEED-VII数据集上验证。

Result: 使用无启动效应的数据序列显著降低了模型的误分类率。

Conclusion: APS方法有效识别和缓解数据层面的启动效应，增强了模型鲁棒性，为情感计算数据集的设计和收集提供了重要参考。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [63] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: 提出了一种融合特征嵌入与社区信息的图神经网络框架，用于提升有向链接预测性能，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决有向图中链接预测问题，现有深度学习方法通常仅通过对比学习和图卷积聚合邻域信息，未能充分利用社区信息。

Method: 提出新型GNN框架，融合特征嵌入与社区信息，并将输入图转换为有向线图以提升信息聚合效率。

Result: 在多个基准数据集上，当训练数据为30%至60%的连接链接时，性能优于现有最优方法。

Conclusion: 融合社区信息与特征嵌入的GNN框架能显著提升有向链接预测性能。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [64] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedBKD的无数据蒸馏框架，通过生成对抗网络（GAN）合成数据，实现全局和局部模型之间的双向知识蒸馏，解决了联邦学习中非独立同分布（non-IID）数据的挑战，同时提升了全局和局部模型的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）面临非独立同分布数据的挑战，现有解决方案难以同时兼顾全局模型的泛化能力和局部模型的性能，且引入公共数据集可能增加数据泄露风险。

Method: 提出FedBKD框架，利用GAN生成合成数据，局部模型作为判别器且参数冻结，通过双向蒸馏实现全局与局部模型的知识交互。

Result: 在4个基准测试中，FedBKD在不同非IID设置下均达到最优性能。

Conclusion: FedBKD有效解决了非IID数据问题，同时提升了全局和局部模型的性能，且避免了数据泄露风险。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [65] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: 本文研究了量化大型语言模型（LLMs）对安全性的影响，并提出了一种量化感知的安全修复框架Q-resafe，以恢复量化模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 量化LLMs在资源受限环境中部署具有重要意义，但现有研究表明量化可能损害模型的安全性，因此需要系统性评估和修复策略。

Method: 通过主流量化技术和多样化校准数据集进行安全性评估，并提出Q-resafe框架以修复量化模型的安全漏洞。

Result: 实验表明，Q-resafe能有效恢复量化LLMs的安全性，使其与量化前的模型表现一致。

Conclusion: Q-resafe为解决量化LLMs的安全性问题提供了有效方案，同时最小化对模型实用性的影响。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [66] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: 论文比较了四种数据驱动方法（WGAN、DDPM、HMM、MABF）用于生成长时间尺度电力消费的合成时间序列数据，评估其性能并分析适用性。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注短期电力消费预测，而忽略了个体消费者的长期预测需求。高保真合成数据对电网规划等应用至关重要。

Method: 采用混合WGAN、DDPM、HMM和MABF方法，评估其在复制电力消费时间动态、长程依赖性和概率转移方面的性能。

Result: 研究比较了各方法的优缺点，为电力系统状态估计等任务提供了方法选择依据。

Conclusion: 该框架提升了合成数据的准确性和隐私保护性，适用于电网规划和消费预测等应用。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [67] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种名为'recourse-aware ensembling (RAE)'的方法，用于在模型多重性（MM）下提供鲁棒的反事实解释（CEs），通过计算论证解决模型间的冲突。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，模型多重性（MM）导致不同模型对同一输入的预测结果不同，使得反事实解释（CEs）的鲁棒性成为问题。

Method: 提出了一种基于计算论证的集成方法，显式表示模型和反事实之间的冲突，并通过论证语义解决冲突。

Result: 理论分析表明该方法在四种论证语义下表现良好，实证研究验证了其满足六种理想性质。

Conclusion: RAE方法有效解决了MM下CEs的鲁棒性问题，并支持对模型的偏好定制。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [68] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习框架，通过从多个集群中提取通用专家模型，解决了非独立同分布数据的问题，并有效整合了跨集群的共享知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了集群间的共享信息，而这些信息对联邦学习系统的所有参与者具有普遍价值。

Method: 框架分为三步：本地模型训练、集群特定模型聚合和通用专家提取。

Result: 实验结果表明，该方法在多种场景下表现优异，能够更有效地平衡个性化和共享知识。

Conclusion: 该方法通过灵活处理模型异构性和减少集群专家冲突，推动了聚类联邦学习的进展。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [69] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 论文研究了基于学习的QR码解码，发现Transformer模型能突破理论纠错限制，通过学习嵌入文本结构实现解码。


<details>
  <summary>Details</summary>
Motivation: 探索中等输入敏感性的学习函数，特别是QR码解码任务，以填补学习输入敏感任务的研究空白。

Method: 使用Transformer模型进行QR码解码实验，分析其学习机制和泛化能力。

Result: Transformer能成功解码QR码，甚至超越理论纠错限制，并能泛化到其他语言和随机字符串。

Conclusion: Transformer的解码机制与传统QR码阅读器不同，更关注数据位而非纠错位。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [70] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了一种新的模仿学习算法ILDE，通过双重探索（乐观策略优化和好奇心驱动探索）提高样本效率并实现超越专家性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在有限演示下难以准确学习专家策略，且需要探索环境以实现超越专家性能。

Method: ILDE算法结合乐观策略优化（奖励高不确定性状态-动作对）和好奇心驱动探索（偏离演示轨迹的状态）。

Result: ILDE在Atari和MuJoCo任务中样本效率更高，且用更少演示实现超越专家性能。

Conclusion: ILDE通过双重探索和理论支持，成为高效的模仿学习算法。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [71] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: 开发了一种基于AI的作物病害检测系统，比较了多种深度学习模型在迁移学习中的效果，验证准确率达95.76%。


<details>
  <summary>Details</summary>
Motivation: 帮助资源有限的农村农民通过AI技术改善作物健康管理和可持续农业。

Method: 比较了EfficientNet、ResNet101、MobileNetV2和自定义CNN模型，并利用迁移学习进行分类。

Result: 系统验证准确率达95.76%，证明了迁移学习在农业中的潜力。

Conclusion: AI驱动的病害检测系统可有效支持农村农业实践，提升作物管理效率。

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [72] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: 论文提出了一种基于置换等变性的神经图控制微分方程（Permutation Equivariant Neural Graph CDEs），通过减少参数数量提升训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动态图因节点特征和网络结构的动态变化而复杂，现有方法（如Graph Neural CDEs）需进一步优化参数效率。

Method: 将Graph Neural CDEs投影到置换等变函数空间，减少参数数量但保持表达能力。

Result: 实验表明，该方法在模拟动态系统和真实任务中表现优异，插值和外推能力均提升。

Conclusion: 置换等变设计显著提升了模型的效率和泛化性能，适用于复杂动态图任务。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [73] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: 论文研究了序列化捆绑推荐中的公平性问题，提出了生产者公平性概念，并通过多种方法在保证推荐质量的同时实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中需要确保不同商品组在推荐会话中获得公平的曝光机会，同时保持推荐质量。

Method: 提出了精确解决方案和两种启发式方法（质量优先和公平优先），以及一种自适应变体。

Result: 在三个真实数据集上的实验验证了方法的有效性，能够在保证推荐质量的同时实现公平性。

Conclusion: 研究展示了在捆绑推荐中平衡公平性和质量的可行方法，并分析了不同方法的优缺点。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [74] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Granger因果关系（GC）新范式，通过联合建模时间序列并比较模型不确定性或残差分布来揭示GC结构，无需显式稀疏回归。


<details>
  <summary>Details</summary>
Motivation: 传统线性VAR模型在GC分析中因假设限制而应用有限，现有DNN方法将GC视为变量选择问题，未能充分利用其预测能力。

Method: 利用深度神经网络联合建模时间序列，通过比较完整模型与剔除特定时间序列后的模型不确定性或残差分布来推断GC结构，并研究输入层dropout的影响。

Result: 研究表明，经过良好正则化的模型能够直接从数据中学习真实的GC结构，无需在损失函数中添加稀疏性约束。

Conclusion: 论文提出的方法为GC分析提供了更灵活且数据驱动的解决方案，突破了传统线性模型的限制。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [75] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种双级重要性保护机制（DipSVD），通过局部和全局重要性保护提升SVD压缩方法的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的计算需求和部署成本不断增加，现有SVD压缩方法忽视矩阵关键组件的保护，导致压缩模型性能下降。

Method: 提出双级重要性保护机制：局部保护通过通道加权数据白化保留关键奇异向量；全局保护通过启发式或优化方法让次要层承担更多压缩负担。

Result: 实验表明，DipSVD在多个基准测试中优于现有SVD压缩方法，尤其在高压缩比下表现更佳。

Conclusion: DipSVD通过保护关键组件显著提升了SVD压缩的性能，为LLMs的高效压缩提供了新思路。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [76] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出了一种新型自注意力机制MVPA，用于处理多变量时间序列数据中的异构通道配置问题，并构建了MVPFormer模型，在iEEG数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在多变量时间序列（如iEEG）中因通道配置差异导致的泛化挑战。

Method: 提出MVPA机制，分离内容、时间和空间注意力，构建MVPFormer模型。

Result: 在多个数据集上表现优异，达到专家级癫痫检测水平，并超越现有Transformer模型。

Conclusion: MVPA是一种通用的注意力机制，MVPFormer是首个开源、开放权重的iEEG基础模型，具有临床领先性能。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [77] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: 论文提出了一种基于分类学的特征选择方法，用于解决轨迹分析中的高维特征爆炸问题，提高了模型效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 轨迹分析中高维特征导致效率和可解释性降低，影响机器学习模型准确性。

Method: 采用分类学方法将特征分为几何和运动学特征，进一步细分为曲率、凹陷、速度和加速度。

Result: 分类学方法在预测性能上表现优异，显著减少特征选择时间，并增强数据集的敏感性分析。

Conclusion: 分类学特征选择方法降低了维度和计算复杂度，为轨迹数据集提供了方法论框架，推动了可解释人工智能的发展。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [78] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA是一种新型遥感基础模型，通过自监督学习从卫星时间序列数据生成全球10米尺度的稳健表示，结合光学和SAR数据，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 卫星遥感在地球观测应用中具有广泛用途，但现有模型在性能和分辨率上存在局限，TESSERA旨在提供高性能、高分辨率的全球表示。

Method: TESSERA使用两个并行的Transformer编码器分别处理Sentinel-1 SAR和Sentinel-2 MSI数据，通过多层感知机融合生成全球表示。

Result: TESSERA在五项任务中表现优于传统遥感模型和其他地理空间基础模型，设立了新的性能基准。

Conclusion: TESSERA通过开源方法提供了高性能的全球遥感表示，推动了地球观测应用的民主化。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [79] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: P4是一种个性化、隐私保护的P2P方法，用于资源受限的IoT设备，通过轻量级去中心化算法实现高效知识共享，同时确保差分隐私和抗毒化攻击。


<details>
  <summary>Details</summary>
Motivation: AI在IoT中的广泛应用需要高效且隐私保护的个性化学习方法，但去中心化环境中的知识共享、隐私保护和抗攻击能力面临挑战。

Method: P4采用轻量级去中心化算法检测客户端相似性并形成协作组，组内通过差分隐私知识蒸馏共同训练模型。

Result: P4在基准测试中比现有方法准确率提升5%-30%，并能抵御30%的恶意客户端攻击，实际部署中仅增加约7秒开销。

Conclusion: P4为资源受限的IoT设备提供了一种高效、隐私保护且抗攻击的个性化学习解决方案。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [80] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出了一种名为OPFV的新估计器，用于在非平稳环境中准确估计和优化未来策略价值，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中（如电子商务推荐系统），现有方法因假设平稳性或依赖限制性奖励建模假设而产生显著偏差，无法准确估计未来策略价值。

Method: 提出OPFV估计器，利用时间序列数据中的有用结构（如季节性、周效应等），通过新型重要性加权实现有效的未来策略评估和学习。

Result: 理论分析表明OPFV在特定条件下偏差较低，实验结果显示其在非平稳环境中显著优于现有方法。

Conclusion: OPFV为未来策略评估和学习提供了有效工具，特别适用于非平稳环境。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [81] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: 提出了一种针对联邦学习中客户参与不均问题的知识蒸馏策略（KDIA），通过加权聚合和辅助训练提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户标签倾斜、数据量倾斜等问题，尤其是在大规模客户中仅少数参与训练的挑战性场景。

Method: 采用知识蒸馏策略，学生模型为参与客户的平均聚合，教师模型为基于参与频率、数据量等的加权聚合，并结合生成器辅助训练。

Result: 在CIFAR-10/100/CINIC-10数据集上，KDIA在较少训练轮次下实现更高准确率，尤其在严重异构环境下表现更优。

Conclusion: KDIA能有效利用所有客户知识，显著提升联邦学习在异构环境下的性能。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [82] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: 提出了一种基于Hessian矩阵的新积分方法，用于指导PINNs训练中配点的选择。


<details>
  <summary>Details</summary>
Motivation: 改进PINNs中配点选择的效率，通过自适应采样提升求解PDE的精度。

Method: 利用Hessian矩阵近似定积分，并以此指导配点选择。

Result: 新方法在PINNs训练中优化了配点分布，提升了求解精度。

Conclusion: 基于Hessian的积分方法为PINNs的配点选择提供了有效指导。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [83] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图理论的算法，自动选择适合的演示数量，用于表格数据分类的上下文学习。


<details>
  <summary>Details</summary>
Motivation: 解决在表格数据分类中确定上下文学习演示数量的挑战。

Method: 结合数据分布、用户提示模板和LLM，利用谱图理论构建相似性图，通过拉普拉斯矩阵特征值分析确定最小演示数量。

Result: 实验验证了该方法在多种数据集和LLM上优于随机选择算法。

Conclusion: 提出的算法能有效确定演示数量，提升上下文学习在表格数据分类中的表现。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [84] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 研究探讨了机器学习模型记忆训练数据的机制，指出仅依赖自影响（self-influence）会低估记忆风险，提出通过全影响分布（full influence distribution）更全面评估记忆现象。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型记忆训练数据可能引发隐私和泛化问题，现有研究多关注自影响，但忽略了其他样本（如近重复样本）的影响。

Method: 通过计算小语言模型中训练样本间的全影响分布，分析其特性，并在图像分类（CIFAR-10）中验证。

Result: 发现仅关注自影响会低估记忆风险，近重复样本的存在显著降低自影响但仍可被提取；全影响分布能揭示近重复样本的存在。

Conclusion: 记忆现象源于训练数据间的复杂交互，全影响分布比自影响更能全面捕捉记忆机制。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [85] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习通过结合图像、文本和音频等多源信息，帮助AI系统构建更丰富的内部表示，提升解释、推理和决策能力。核心技术包括表示学习、对齐方法和融合策略，但仍面临数据格式差异、输入缺失和对抗攻击等挑战。未来研究方向包括无监督学习、AutoML工具和更好的评估指标。


<details>
  <summary>Details</summary>
Motivation: 多模态学习旨在通过整合不同模态的信息，提升AI系统对复杂事物的理解和处理能力，使其更接近人类的理解方式。

Method: 采用表示学习、对齐方法和融合策略等核心技术，结合深度学习模型，整合多源数据。

Result: 多模态学习在计算机视觉、自然语言处理、语音识别和医疗等领域展现出潜力，但仍需解决数据格式差异和对抗攻击等问题。

Conclusion: 多模态学习有望推动AI系统向更灵活、上下文感知的方向发展，未来可能实现更接近人类的理解能力。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [86] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: 提出了一种基于去中心化金融（DeFi）和自动化做市商（AMMs）的联邦学习激励机制框架，旨在解决现有奖励分配方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在数据隐私和模型性能方面具有优势，但在盈利性FL中，激励机制的分配框架研究不足。

Method: 引入客户端特定代币作为投资工具，利用DeFi平台和AMMs构建灵活、可扩展的奖励分配系统。

Result: 提出了一个新颖的框架，允许第三方投资于联邦学习过程，并优化奖励分配。

Conclusion: 该框架为联邦学习提供了更灵活和可扩展的激励机制，同时为参与者创造了新的投资机会。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [87] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 研究了介于离策略强化学习和监督微调之间的简单离策略REINFORCE算法，分析了其理论性能并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 离策略方法在实现简单性和数据效率上优于同策略方法，但性能常不理想，因此探索中间范围的算法。

Method: 采用简单的离策略REINFORCE算法，定义优势函数A=r-V，调整基线V以优化样本利用。

Result: 理论分析表明，当基线V低于期望奖励时，算法具有策略改进保证；实验验证了其有效性。

Conclusion: 离策略更新更应关注正向奖励，算法在理论和实验中均表现出色。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [88] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: 论文提出了一种合成工业数据集（SIDED）和数据增强方法（AMDA），以解决工业非侵入式负载监测（NILM）中的数据稀缺和隐私问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业NILM面临高质量数据集稀缺和工业能耗模式复杂多变的挑战，需要解决数据稀缺和隐私问题。

Method: 使用数字孪生模拟生成开源数据集SIDED，并提出AMDA方法，通过智能调整电器功率贡献来增强模型泛化能力。

Result: 实验表明，使用AMDA增强数据训练的NILM模型在复杂工业电器能耗分解中表现优异，归一化分解误差为0.093，优于未使用数据增强（0.451）和随机数据增强（0.290）的模型。

Conclusion: SIDED和AMDA有效解决了工业NILM中的数据问题，并显著提升了模型性能，为实际应用提供了有力支持。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [89] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: 提出了一种名为FEA-PINN的高效建模框架，结合物理信息神经网络（PINN）和有限元分析（FEA），用于加速激光粉末床熔融（LPBF）过程中的热场预测，同时保持FEA的精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法（如FEA）在LPBF过程中计算成本高，亟需高效模拟方法。

Method: 开发了动态材料更新策略，结合温度依赖的材料特性和相变行为，通过FEA-PINN框架整合纠正性FEA模拟以减少误差漂移。

Result: FEA-PINN在保持与FEA相同精度的同时，显著降低了计算成本，并通过单轨扫描实验验证。

Conclusion: FEA-PINN框架为LPBF过程提供了一种高效且准确的模拟方法，解决了传统方法的高计算成本问题。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [90] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: 本文研究了基于技能的排队系统（如数据中心、云计算网络和服务系统）的最优控制，通过真实数据集验证了一种强化学习算法在客户路由中的高效性和适应性，并引入新启发式规则以减少延迟。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于优化复杂排队系统中的客户路由，提升系统效率并适应动态环境。

Method: 采用强化学习算法进行最优客户路由，引入新启发式规则，并通过参数调优平衡多目标（如收益最大化、服务器负载公平性和客户等待时间减少）。

Result: 实验表明算法能高效学习并适应变化环境，优于静态基准策略，且能优化多目标。

Conclusion: 该算法具有实际应用潜力，但需注意参数调优和估计误差对性能的影响。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [91] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: 本文探讨了基于Transformer的多变量时间序列异常检测方法，重点研究了iTransformer架构，并分析了关键参数、异常标签提取、训练数据中异常的影响以及与其他Transformer模型的比较。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常检测在多个领域至关重要，但由于异常未知性和维度间复杂依赖关系，准确检测具有挑战性。

Method: 研究iTransformer在异常检测中的应用，分析窗口大小、步长等参数影响，探讨异常标签提取方法，评估训练数据中异常的影响，并比较多种Transformer模型。

Result: 提出了iTransformer在异常检测中的性能分析，探讨了异常标签提取和评估指标，研究了训练数据异常的影响，并提供了多模型比较结果。

Conclusion: iTransformer在多变量时间序列异常检测中表现良好，关键参数和异常数据处理方法对性能有显著影响，为未来研究提供了参考。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [92] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: 论文探讨了图神经网络在分布外（OOD）泛化中的表现，重点比较了图变换器（GT）和混合架构与传统消息传递神经网络（MPNNs）的性能。结果表明GT和混合架构在OOD场景下表现更优，并提出了一种新的后训练分析方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练和测试数据分布通常不一致，而现有图神经网络方法大多假设分布相同。GT在分布内（ID）表现优异，但其在OOD泛化中的效果尚未充分研究。

Method: 系统评估GT和混合架构在OOD设置下的性能，并比较MPNNs。将多种领域泛化（DG）算法适配到GT，并在设计好的基准测试上评估。提出了一种新的后训练分析方法，分析ID和OOD数据的聚类结构。

Result: GT和混合GT-MPNN架构在OOD泛化中表现优于MPNNs，即使未使用专门的DG算法。后训练分析方法提供了对模型泛化能力的深入见解。

Conclusion: GT在现实图学习中展现出更强的鲁棒性，为OOD泛化研究提供了新方向。后训练分析方法具有广泛适用性，可超越传统精度指标评估模型。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [93] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的新型图索引SVG，适用于度量和非度量向量空间，并引入SVG-L0以解决传统方法的启发式问题。


<details>
  <summary>Details</summary>
Motivation: 传统图索引仅适用于欧几里得空间，无法满足度量和非度量向量空间的需求。

Method: 利用核方法构建图连接性，提出SVG索引，并进一步引入SVG-L0以限制出度。

Result: SVG和SVG-L0在度量和非度量空间中均具有理论可导航性，且SVG-L0避免了启发式方法的不足。

Conclusion: SVG为图索引提供了新视角，SVG-L0解决了传统方法的实践问题，具有自调优特性。

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [94] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: H-FEX是一种符号学习方法，用于从哈密顿系统的观测数据中学习复杂的哈密顿函数，并保持能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法难以准确捕捉复杂哈密顿函数并保持能量守恒，因此需要一种更有效的方法。

Method: 提出H-FEX方法，通过新颖的交互节点捕捉复杂相互作用项。

Result: 实验表明，H-FEX能准确恢复复杂系统的哈密顿函数，并长期保持能量守恒。

Conclusion: H-FEX是发现复杂动力系统闭式表达的有力框架。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [95] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: 研究了闭环学习中指数族模型的参数动态，发现最大似然估计会导致吸收状态放大初始偏差，但可通过数据污染、最大后验估计或正则化避免。


<details>
  <summary>Details</summary>
Motivation: 探讨大型神经网络未来可能主要依赖自身生成数据训练时，闭环学习的动态行为及其潜在问题。

Method: 针对指数族模型，推导参数动态方程，分析最大似然估计的性质及其收敛行为。

Result: 最大似然估计使充分统计量具有鞅性质，导致收敛到放大初始偏差的吸收状态；但可通过数据污染、最大后验估计或正则化避免。

Conclusion: 闭环学习的动态行为受参数化影响，需谨慎选择方法以避免偏差放大。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [96] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP（Precise LoRA Placement）是一种轻量级方法，通过理论分析自动确定LoRA适配器的最佳放置位置，显著优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA适配器放置策略缺乏统一结论，不同研究提出不同模块（如注意力或MLP模块）作为放置位置，但效果不一致。

Method: 通过直观的理论分析，PLoP自动识别给定预训练模型和微调任务中LoRA适配器的最佳模块类型。

Result: PLoP在监督微调和强化学习任务中表现优于或至少与常用放置策略相当。

Conclusion: PLoP提供了一种高效且自动化的LoRA适配器放置方法，显著提升了模型微调效果。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [97] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: 提出了一种名为FedEDS的新联邦学习方案，通过加密数据共享解决边缘设备上的网络拓扑、物理距离和数据异构性问题，提升模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视了网络拓扑、物理距离和数据异构性对边缘设备的影响，导致延迟增加和模型性能下降。

Method: FedEDS利用客户端模型和随机层训练数据加密器，生成加密数据并与其他客户端共享，结合本地私有数据和加密共享数据训练模型。

Result: 实验证明FedEDS能有效提升模型性能并加速收敛。

Conclusion: FedEDS适用于需要快速收敛的边缘设备应用服务。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [98] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出了一种新的代理损失函数和高效算法，用于解决多专家学习中的延迟分配问题，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决在多专家系统中平衡准确性和计算成本的延迟分配问题，尤其是在自然语言生成、图像处理和医学诊断等领域。

Method: 引入新的代理损失函数和算法，针对单阶段和两阶段学习场景，分别实现可实现的H一致性和贝叶斯一致性。

Result: 实验结果表明，所提出的代理损失函数在性能上优于现有基线方法。

Conclusion: 本文提出的方法在多专家延迟分配问题上具有理论保证和实际效果，为相关领域提供了新的解决方案。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [99] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 本文提出了一种基于时空点过程的概率模型，用于更全面地模拟阅读行为，包括注视点的位置、时间和持续时间，以及眼跳的动态特性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于聚合的眼动追踪数据和强假设模型，忽略了阅读中的时空动态特性。本文旨在通过更通用的模型捕捉这些细节。

Method: 使用标记的时空点过程模型，眼跳采用霍克斯过程模拟，注视持续时间通过时间卷积的预测变量建模。

Result: 霍克斯过程模型在模拟人类眼跳方面优于基线模型，但上下文惊讶度对注视持续时间的预测改进有限。

Conclusion: 惊讶度理论难以解释精细的眼动行为，表明需要更复杂的模型来捕捉阅读动态。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [100] [Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization](https://arxiv.org/abs/2506.20056)
*Yuheng Chen,Alexander Montes McNeil,Taehyuk Park,Blake A. Wilson,Vaishnavi Iyer,Michael Bezick,Jae-Ik Choi,Rohan Ojha,Pravin Mahendran,Daksh Kumar Singh,Geetika Chitturi,Peigang Chen,Trang Do,Alexander V. Kildishev,Vladimir M. Shalaev,Michael Moebius,Wenshan Cai,Yongmin Liu,Alexandra Boltasseva*

Main category: physics.optics

TL;DR: 机器学习辅助光子器件开发（ML-PDD）通过数据驱动方法解决了传统PDD中的计算成本高、优化复杂等问题，提供了高效的设计优化、快速模拟和强化学习制造等新策略。


<details>
  <summary>Details</summary>
Motivation: 传统光子器件开发（PDD）面临计算成本高、优化复杂、制造不确定性等挑战，机器学习为这些问题提供了新的解决方案。

Method: 采用机器学习方法，包括替代估计器加速计算、生成建模处理噪声测量、强化学习优化制造过程，以及主动学习用于实验发现。

Result: ML-PDD显著提高了光子器件设计的效率和性能，特别是在复杂优化和噪声环境下。

Conclusion: 机器学习为光子器件开发带来了革命性的改进，促进了跨学科合作，加速了复杂光子系统的研发。

Abstract: Photonic device development (PDD) has achieved remarkable success in
designing and implementing new devices for controlling light across various
wavelengths, scales, and applications, including telecommunications, imaging,
sensing, and quantum information processing. PDD is an iterative, five-step
process that consists of: i) deriving device behavior from design parameters,
ii) simulating device performance, iii) finding the optimal candidate designs
from simulations, iv) fabricating the optimal device, and v) measuring device
performance. Classically, all these steps involve Bayesian optimization,
material science, control theory, and direct physics-driven numerical methods.
However, many of these techniques are computationally intractable, monetarily
costly, or difficult to implement at scale. In addition, PDD suffers from large
optimization landscapes, uncertainties in structural or optical
characterization, and difficulties in implementing robust fabrication
processes. However, the advent of machine learning over the past decade has
provided novel, data-driven strategies for tackling these challenges, including
surrogate estimators for speeding up computations, generative modeling for
noisy measurement modeling and data augmentation, reinforcement learning for
fabrication, and active learning for experimental physical discovery. In this
review, we present a comprehensive perspective on these methods to enable
machine-learning-assisted PDD (ML-PDD) for efficient design optimization with
powerful generative models, fast simulation and characterization modeling under
noisy measurements, and reinforcement learning for fabrication. This review
will provide researchers from diverse backgrounds with valuable insights into
this emerging topic, fostering interdisciplinary efforts to accelerate the
development of complex photonic devices and systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent是一种新型AI代理，结合大型语言模型（LLMs）和形式化证明助手Lean，实现自动定理证明，成功率达86.1%。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLMs和形式化证明工具，提高自动定理证明的效率和成功率。

Method: 整合非正式推理LLM、形式化证明模型和Lean的反馈，生成辅助引理以支持整体证明策略。

Result: 在MiniF2F基准测试中达到86.1%的成功率，优于其他使用小型语言模型（SLMs）的方法。

Conclusion: Prover Agent通过生成辅助引理和高效协作，显著提升了自动定理证明的性能。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [102] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live框架评估ML代理在模拟Kaggle社区中的交流能力，CoMind代理在社区环境中表现优异，超越79.2%的人类竞争者。


<details>
  <summary>Details</summary>
Motivation: 现有ML代理通常孤立运行，缺乏与社区互动，而人类研究者通过共享知识获益。

Method: 提出MLE-Live框架评估代理的社区交流能力，并开发CoMind代理以在社区中交换见解和开发新方案。

Result: CoMind在MLE-Live中表现最佳，并在四个Kaggle竞赛中平均超越79.2%的人类竞争者。

Conclusion: CoMind展示了在社区环境中协作的潜力，为ML代理的未来发展提供了新方向。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [103] [DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules](https://arxiv.org/abs/2506.19862)
*Junjie Xu,Jiahao Zhang,Mangal Prakash,Xiang Zhang,Suhang Wang*

Main category: q-bio.BM

TL;DR: DualEquiNet是一种双空间层次等变网络，结合欧几里得和球谐空间表示，解决了现有几何GNN在大生物分子建模中的可扩展性和表达能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN难以同时捕捉大生物分子（如RNA和蛋白质）的细粒度原子相互作用和长程依赖关系，需要多尺度建模方法。

Method: DualEquiNet通过双向跨空间消息传递和跨空间交互池化机制，在欧几里得和球谐空间中构建互补表示，实现层次化特征聚合。

Result: DualEquiNet在RNA和蛋白质建模的多个基准测试中达到最先进性能，并在新引入的3D结构基准上优于现有方法。

Conclusion: DualEquiNet为大型生物分子系统提供了一种高效且表达能力强的多尺度建模方法。

Abstract: Geometric graph neural networks (GNNs) that respect E(3) symmetries have
achieved strong performance on small molecule modeling, but they face
scalability and expressiveness challenges when applied to large biomolecules
such as RNA and proteins. These systems require models that can simultaneously
capture fine-grained atomic interactions, long-range dependencies across
spatially distant components, and biologically relevant hierarchical structure,
such as atoms forming residues, which in turn form higher-order domains.
Existing geometric GNNs, which typically operate exclusively in either
Euclidean or Spherical Harmonics space, are limited in their ability to capture
both the fine-scale atomic details and the long-range, symmetry-aware
dependencies required for modeling the multi-scale structure of large
biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant
Network that constructs complementary representations in both Euclidean and
Spherical Harmonics spaces to capture local geometry and global symmetry-aware
features. DualEquiNet employs bidirectional cross-space message passing and a
novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate
atomic features into biologically meaningful units, such as residues, enabling
efficient and expressive multi-scale modeling for large biomolecular systems.
DualEquiNet achieves state-of-the-art performance on multiple existing
benchmarks for RNA property prediction and protein modeling, and outperforms
prior methods on two newly introduced 3D structural benchmarks demonstrating
its broad effectiveness across a range of large biomolecule modeling tasks.

</details>


### [104] [Scalable and Cost-Efficient de Novo Template-Based Molecular Generation](https://arxiv.org/abs/2506.19865)
*Piotr Gaiński,Oussama Boussif,Andrei Rekesh,Dmytro Shevchuk,Ali Parviz,Mike Tyers,Robert A. Batey,Michał Koziarski*

Main category: q-bio.BM

TL;DR: 论文提出了一种基于模板的分子生成方法，通过递归成本指导和动态库机制解决合成成本、大规模库扩展和小片段集利用问题。


<details>
  <summary>Details</summary>
Motivation: 解决模板生成分子中的合成成本高、库规模扩展困难以及小片段集利用效率低的问题。

Method: 采用递归成本指导框架和动态库机制，结合机器学习模型优化合成路径和库利用率。

Result: 显著提高了成本效率、分子多样性和质量，并在模板分子生成中取得了领先成果。

Conclusion: 该方法为药物设计提供了一种高效且可扩展的分子生成解决方案。

Abstract: Template-based molecular generation offers a promising avenue for drug design
by ensuring generated compounds are synthetically accessible through predefined
reaction templates and building blocks. In this work, we tackle three core
challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)
scaling to large building block libraries, and (3) effectively utilizing small
fragment sets. We propose \textbf{Recursive Cost Guidance}, a backward policy
framework that employs auxiliary machine learning models to approximate
synthesis cost and viability. This guidance steers generation toward low-cost
synthesis pathways, significantly enhancing cost-efficiency, molecular
diversity, and quality, especially when paired with an \textbf{Exploitation
Penalty} that balances the trade-off between exploration and exploitation. To
enhance performance in smaller building block libraries, we develop a
\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states
to construct full synthesis trees. Our approach establishes state-of-the-art
results in template-based molecular generation.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [105] [Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation](https://arxiv.org/abs/2506.19855)
*Ashish Masarkar,Rakesh Gupta,Naga Neehar Dingari,Beena Rai*

Main category: physics.med-ph

TL;DR: 提出了一种基于神经网络的预测方法，用于减少皮肤粘合剂剥离行为分析的计算成本和时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如实验测试和有限元法）资源密集且耗时，限制了材料参数空间的广泛分析。

Method: 利用有限元模拟生成的数据集训练神经网络，预测最小剥离力，并通过5折交叉验证验证准确性。

Result: 模型在测试集上表现出色，MSE为3.66*10^-7，R^2得分为0.94。

Conclusion: 该方法为皮肤粘合剂系统的设计和优化提供了高效、可靠的框架，适用于未来计算皮肤力学和生物粘合剂材料研究。

Abstract: Studying the peeling behaviour of adhesives on skin is vital for advancing
biomedical applications such as medical adhesives and transdermal patches.
Traditional methods like experimental testing and finite element method (FEM),
though considered gold standards, are resource-intensive, computationally
expensive and time-consuming, particularly when analysing a wide material
parameter space. In this study, we present a neural network-based approach to
predict the minimum peel force (F_min) required for adhesive detachment from
skin tissue, limiting the need for repeated FEM simulations and significantly
reducing the computational cost. Leveraging a dataset generated from FEM
simulations of 90 degree peel test with varying adhesive and fracture mechanics
parameters, our neural network model achieved high accuracy, validated through
rigorous 5-fold cross-validation. The final architecture was able to predict a
wide variety of skin-adhesive peeling behaviour, exhibiting a mean squared
error (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating
robust performance. This work introduces a reliable, computationally efficient
method for predicting adhesive behaviour, significantly reducing simulation
time while maintaining accuracy. This integration of machine learning with
high-fidelity biomechanical simulations enables efficient design and
optimization of skin-adhesive systems, providing a scalable framework for
future research in computational dermato-mechanics and bio-adhesive material
design.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [106] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.OS

TL;DR: 论文提出了一种名为MNN-AECS的系统，通过动态选择低功耗CPU核心来优化设备上大型语言模型（LLM）解码阶段的能源效率，显著降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 随着设备上LLM推理需求的增长，能源效率成为主要问题，尤其是电池有限的移动设备。现有工作多关注预填充阶段的加速，而忽视了解码阶段的能源消耗。

Method: 引入自适应能源核心选择（AECS）并将其集成到MNN中，创建了MNN-AECS系统，动态选择低功耗CPU核心以减少解码能耗。

Result: 在5款Android和2款iOS设备上测试5种不同规模的LLM，MNN-AECS平均降低23%的能耗且无速度损失；与其他引擎相比，节能39%至78%，速度提升12%至363%。

Conclusion: MNN-AECS是首个无需root或操作系统修改的引擎级解决方案，显著提升了LLM解码的能源效率。

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [107] [Polynomial-Time Approximation Schemes via Utility Alignment: Unit-Demand Pricing and More](https://arxiv.org/abs/2506.20030)
*Robin Bowers,Marius Garbea,Emmanouil Pountourakis,Samuel Taggart*

Main category: cs.GT

TL;DR: 本文提出了多项式时间近似方案，解决了几个NP难随机优化问题，包括定价、分类优化和委托选择，并引入了“效用对齐”这一关键技术概念。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机制设计和运筹学中的NP难随机优化问题，如定价、分类优化和委托选择，以提升现有算法的性能。

Method: 方法包括提出多项式时间近似方案（PTAS），并引入“效用对齐”概念，通过优化代理效用高的实现来简化问题。

Result: 结果包括改进了离散分布下的定价问题、分类优化的常数近似，以及委托选择的近似比。

Conclusion: 结论表明“效用对齐”是一个广泛适用的关键技术，能够简化算法设计并提升性能。

Abstract: This paper derives polynomial-time approximation schemes for several NP-hard
stochastic optimization problems from the algorithmic mechanism design and
operations research literatures. The problems we consider involve a principal
or seller optimizing with respect to a subsequent choice by an agent or buyer.
These include posted pricing for a unit-demand buyer with independent values
(Chawla et al., 2007, Cai and Daskalakis, 2011), assortment optimization with
independent utilities (Talluri and van Ryzin, 2004), and delegated choice
(Khodabakhsh et al., 2024). Our results advance the state of the art for each
of these problems. For unit-demand pricing with discrete distributions, our
multiplicative PTAS improves on the additive PTAS of Cai and Daskalakis, and we
additionally give a PTAS for the unbounded regular case, improving on the
latter paper's QPTAS. For assortment optimization, no constant approximation
was previously known. For delegated choice, we improve on both the
$3$-approximation for the case with no outside option and the
super-constant-approximation with an outside option.
  A key technical insight driving our results is an economically meaningful
property we term utility alignment. Informally, a problem is utility aligned
if, at optimality, the principal derives most of their utility from
realizations where the agent's utility is also high. Utility alignment allows
the algorithm designer to focus on maximizing performance on realizations with
high agent utility, which is often an algorithmically simpler task. We prove
utility alignment results for all the problems mentioned above, including
strong results for unit-demand pricing and delegation, as well as a weaker but
very broad guarantee that holds for many other problems under very mild
conditions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [108] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: 论文提出了一种可解释的表面视觉Transformer（X-SiT），用于基于可解释的皮层特征进行人类可理解的预测，并在阿尔茨海默病和额颞叶痴呆检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 3D体积数据难以可视化复杂结构（如大脑皮层），而皮层表面渲染提供了更易理解的3D表示。基于此优势及表面数据在神经疾病研究中的广泛应用，开发了X-SiT。

Method: 提出了X-SiT，一种可解释的神经网络，结合原型表面补丁解码器，通过空间对应的皮层原型进行分类。

Result: 在阿尔茨海默病和额颞叶痴呆检测中达到最先进性能，并提供与已知疾病模式一致的原型。

Conclusion: X-SiT不仅性能优越，还能提供可解释的预测，有助于临床决策。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [109] [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](https://arxiv.org/abs/2506.20554)
*Andrew Mole,Max Weissenbacher,Georgios Rigas,Sylvain Laizet*

Main category: physics.flu-dyn

TL;DR: 传统风电场控制独立运行每台风机以最大化单机功率输出，但协调尾流转向可显著提升全场发电量。本文首次将强化学习控制器与高保真大涡模拟结合，实现动态控制，发电量提升4.30%。


<details>
  <summary>Details</summary>
Motivation: 传统风电场控制依赖静态低精度模拟器，忽略了湍流动态特性，无法实现动态优化。

Method: 结合强化学习（RL）与高保真大涡模拟（LES），开发动态闭环控制器，实时响应大气湍流。

Result: RL控制器使风电场发电量提升4.30%，优于静态最优偏航控制的2.19%增益。

Conclusion: 动态流响应控制是风电场优化的革命性方法，对加速可再生能源部署具有重要意义。

Abstract: Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [110] [DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy](https://arxiv.org/abs/2506.20668)
*Sungjae Park,Homanga Bharadhwaj,Shubham Tulsiani*

Main category: cs.RO

TL;DR: DemoDiffusion通过模仿单次人类演示，结合运动学重定向和扩散策略，实现机器人在自然环境中完成任务，无需在线强化学习或配对数据。


<details>
  <summary>Details</summary>
Motivation: 解决机器人通过单次人类演示完成任务的问题，避免复杂的数据收集和训练过程。

Method: 结合运动学重定向生成粗略轨迹，利用预训练的扩散策略优化轨迹，使其符合机器人动作分布。

Result: 在仿真和真实环境中表现优于基准策略和重定向轨迹，能完成预训练策略无法完成的任务。

Conclusion: DemoDiffusion是一种简单且可扩展的方法，能高效适应新任务和场景。

Abstract: We propose DemoDiffusion, a simple and scalable method for enabling robots to
perform manipulation tasks in natural environments by imitating a single human
demonstration. Our approach is based on two key insights. First, the hand
motion in a human demonstration provides a useful prior for the robot's
end-effector trajectory, which we can convert into a rough open-loop robot
motion trajectory via kinematic retargeting. Second, while this retargeted
motion captures the overall structure of the task, it may not align well with
plausible robot actions in-context. To address this, we leverage a pre-trained
generalist diffusion policy to modify the trajectory, ensuring it both follows
the human motion and remains within the distribution of plausible robot
actions. Our approach avoids the need for online reinforcement learning or
paired human-robot data, enabling robust adaptation to new tasks and scenes
with minimal manual effort. Experiments in both simulation and real-world
settings show that DemoDiffusion outperforms both the base policy and the
retargeted trajectory, enabling the robot to succeed even on tasks where the
pre-trained generalist policy fails entirely. Project page:
https://demodiffusion.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [111] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode框架通过强化学习帮助大语言模型适应动态API更新，显著提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在适应外部库API频繁更新时表现不佳，影响动态环境中的可靠代码生成。

Method: 构建数据集训练模型进行版本迁移，引入改进的字符串相似度指标作为强化学习的奖励。

Result: ReCode显著提升LLMs在动态API场景下的性能，且对通用代码生成能力影响较小。

Conclusion: ReCode在多种LLMs和强化学习算法上均表现一致改进，Qwen2.5-Coder-7B表现优于更大模型。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [112] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason结合大型语言模型和时空模型，通过上下文学习分解复杂查询为模块化程序，生成解决方案和详细解释，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有时空数据挖掘模型局限于单一任务，缺乏多任务推理和复杂长形式推理能力，限制了实际应用。

Method: STReason框架结合LLMs的推理能力和时空模型的分析能力，无需任务特定微调，通过上下文学习生成模块化程序。

Result: 实验显示STReason在复杂时空推理场景中显著优于先进LLM基线，并通过人类评估验证其实用性。

Conclusion: STReason为开发更通用和强大的时空推理系统提供了有前景的方向。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [113] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 提出一种利用自动评分技术填补缺失分数的新方法，以提高IRT能力估计的准确性，同时显著减少人工评分工作量。


<details>
  <summary>Details</summary>
Motivation: 评估学习者的高阶能力（如表达技能和逻辑思维）需求增加，但传统构建反应测试需要大量人工评分，成本高。IRT虽能从不完整数据估计能力，但缺失分数比例高时准确性下降。

Method: 利用自动评分技术填补缺失分数，结合IRT进行能力估计。

Result: 新方法在能力估计中实现高准确性，并显著减少人工评分工作量。

Conclusion: 该方法为高效、准确地评估学习者能力提供了可行解决方案。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [114] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 提出CCRS，一种基于预训练LLM的零样本评估框架，用于全面评估RAG系统的多维度质量，包括上下文连贯性、问题相关性等五个指标。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估方法存在不足，如依赖简单词汇重叠或复杂多阶段流程，难以高效捕捉多维度质量。

Method: CCRS利用单一预训练LLM作为零样本端到端评估器，包含五个指标：上下文连贯性、问题相关性、信息密度、答案正确性和信息召回。

Result: 在BioASQ数据集上验证，CCRS能有效区分不同RAG系统性能，如Mistral-7B优于Llama变体，且计算效率优于RAGChecker。

Conclusion: CCRS为RAG系统提供了实用、全面且高效的评估框架，支持迭代优化。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [115] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN是一个不确定性保护选择框架，通过统计校准阈值在用户指定的FDR约束下筛选单个生成答案，显著提高样本保留率。


<details>
  <summary>Details</summary>
Motivation: 解决传统不确定性量化方法在选择性预测中缺乏形式化保证的问题，特别是针对基础模型生成文本中的幻觉。

Method: 采用统计置信区间方法（如Clopper-Pearson）校准阈值，确保测试数据上的FDR控制。

Result: COIN在风险控制、测试时保留合格答案的能力以及有限校准数据下的预测效率方面表现优异。

Conclusion: COIN具有扩展性和适应性，适用于多种应用场景，并能通过替代上限构建和UQ策略进一步提升性能。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [116] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched是一个通过两阶段标注过程从PubMed构建的生物医学文本数据集，包含临床案例等高质量段落，用于生物医学NLP研究。


<details>
  <summary>Details</summary>
Motivation: 临床文本通常难以获取，Biomed-Enriched提供了一个公开的大规模临床案例数据集，填补了这一空白。

Method: 通过大语言模型标注400K段落，再用小模型扩展到PMC-OA语料库，提取高质量子集并进行质量过滤和领域上采样。

Result: 实验显示，临床上采样提升MMLU ProfMed性能5%，教育质量过滤提升MedQA和MedMCQA性能1%，组合方法加速收敛。

Conclusion: Biomed-Enriched为生物医学预训练提供了高效策略，具有潜在的研究和应用价值。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [117] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文研究了不同基础语言模型家族（如Llama和Qwen）在强化学习（RL）后训练中的表现差异，提出了两阶段中训练策略Stable-then-Decay，并开发了OctoThinker模型家族，显著提升了RL兼容性。


<details>
  <summary>Details</summary>
Motivation: 探讨基础语言模型在强化学习中的适用性，为下一代可扩展的基础模型开发提供指导。

Method: 研究中训练策略对RL动态的影响，聚焦Qwen和Llama模型家族，提出两阶段中训练策略Stable-then-Decay。

Result: 发现高质量数学语料和QA风格数据对RL性能的提升作用，开发了OctoThinker模型家族，缩小了与RL友好模型的性能差距。

Conclusion: 研究为RL时代的基础模型预训练策略提供了重要参考，并开源了模型和语料库。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [118] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Main category: cs.CR

TL;DR: RepuNet是一种去中心化声誉系统，用于检测和缓解去中心化联邦学习（DFL）中的恶意行为，通过动态评估节点行为并调整其影响力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: DFL中节点自主选择聚合伙伴可能导致恶意行为（如模型投毒、延迟攻击等），现有解决方案依赖固定配置或额外基础设施，存在计算开销和可扩展性问题。

Method: 提出RepuNet，通过模型相似性、参数变化、消息延迟和通信量等指标动态评估节点行为，并基于声誉分数调整节点影响力。

Result: 在MNIST和CIFAR-10数据集上测试，RepuNet的F1分数分别超过95%和约76%，有效检测和缓解恶意行为。

Conclusion: RepuNet具有适应性、鲁棒性和实用性，能有效缓解DFL中的威胁。

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [119] [Quantum-Resistant Domain Name System: A Comprehensive System-Level Study](https://arxiv.org/abs/2506.19943)
*Juyoul Lee,Sanzida Hoque,Abdullah Aydeger,Engin Zeydan*

Main category: cs.CR

TL;DR: 本文研究了量子计算对DNS安全的影响，提出了PQC-DNS框架，评估了后量子密码学在DNSSEC、DoT和DoH中的性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机的威胁日益临近，确保DNS在后量子时代的保密性、真实性和完整性变得至关重要。

Method: 提出了PQC-DNS框架，结合Open Quantum Safe库，将基于格和哈希的密码学原语集成到BIND9和TLS 1.3中，并分析了性能和威胁模型。

Result: 实验表明，基于格的密码学（如MLKEM和Falcon）在延迟和资源消耗上表现良好，而哈希方案（如SPHINCS+）则增加了消息大小和处理开销。

Conclusion: 研究为部署量子安全的DNS提供了实用指导，并为后量子时代核心互联网协议的安全性做出了贡献。

Abstract: The Domain Name System (DNS) plays a foundational role in Internet
infrastructure, yet its core protocols remain vulnerable to compromise by
quantum adversaries. As cryptographically relevant quantum computers become a
realistic threat, ensuring DNS confidentiality, authenticity, and integrity in
the post-quantum era is imperative. In this paper, we present a comprehensive
system-level study of post-quantum DNS security across three widely deployed
mechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose
Post-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS
security under legacy, post-quantum, and hybrid cryptographic configurations.
Our implementation leverages the Open Quantum Safe (OQS) libraries and
integrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We
formalize performance and threat models and analyze the impact of post-quantum
key encapsulation and digital signatures on end-to-end DNS resolution.
Experimental results on a containerized testbed reveal that lattice-based
primitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and
Falcon offer practical latency and resource profiles, while hash-based schemes
like SPHINCS+ significantly increase message sizes and processing overhead. We
also examine security implications including downgrade risks, fragmentation
vulnerabilities, and susceptibility to denial-of-service amplification. Our
findings inform practical guidance for deploying quantum-resilient DNS and
contribute to the broader effort of securing core Internet protocols for the
post-quantum future.

</details>


### [120] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: Guardian-FC是一个新颖的两层框架，用于隐私保护的联邦计算，统一了多种隐私保护机制的安全执行。


<details>
  <summary>Details</summary>
Motivation: 解决联邦计算中不同隐私保护机制（如FHE、MPC和DP）的安全执行问题，提供统一的风险管理和可审计性。

Method: 采用后端中立的领域特定语言（DSL）编写插件，通过执行提供者（EPs）实现不同隐私后端操作，并由Agentic-AI控制平面强制执行安全循环。

Result: 提出了一个支持快速失败任务准入和可扩展性的设计，并展示了后端无关的安全性和形式化验证基础。

Conclusion: Guardian-FC为联邦计算提供了统一的隐私保护框架，并提出了未来研究方向，如自适应护栏调整和多后端组合。

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [121] [Generative AI for Vulnerability Detection in 6G Wireless Networks: Advances, Case Study, and Future Directions](https://arxiv.org/abs/2506.20488)
*Shuo Yang,Xinran Zheng,Jinfeng Xu,Jinze Li,Danyang Song,Zheyu Chen,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: 本文探讨了在6G无线网络中集成生成式AI（GAI）进行漏洞检测的方法，提出了一种三层框架，并通过案例研究展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着6G、物联网和边缘计算的快速发展，传统安全方法难以应对动态网络环境中的新型威胁，需要更智能的漏洞检测机制。

Method: 提出了一种三层框架（技术层、能力层和应用层），利用VAEs、GANs、LLMs和GDMs等GAI技术，重点研究代码审计、协议安全、云-边缘防御和硬件保护。

Result: 案例研究表明，LLM驱动的代码漏洞检测在性能和有效性上表现良好，但也面临挑战。

Conclusion: 本文为研究人员和实践者提供了利用GAI构建6G网络安全解决方案的路线图，并指出了未来研究方向。

Abstract: The rapid advancement of 6G wireless networks, IoT, and edge computing has
significantly expanded the cyberattack surface, necessitating more intelligent
and adaptive vulnerability detection mechanisms. Traditional security methods,
while foundational, struggle with zero-day exploits, adversarial threats, and
context-dependent vulnerabilities in highly dynamic network environments.
Generative AI (GAI) emerges as a transformative solution, leveraging synthetic
data generation, multimodal reasoning, and adaptive learning to enhance
security frameworks. This paper explores the integration of GAI-powered
vulnerability detection in 6G wireless networks, focusing on code auditing,
protocol security, cloud-edge defenses, and hardware protection. We introduce a
three-layer framework comprising the Technology Layer, Capability Layer, and
Application Layer to systematically analyze the role of VAEs, GANs, LLMs, and
GDMs in securing next-generation wireless ecosystems. To demonstrate practical
implementation, we present a case study on LLM-driven code vulnerability
detection, highlighting its effectiveness, performance, and challenges.
Finally, we outline future research directions, including lightweight models,
high-authenticity data generation, external knowledge integration, and
privacy-preserving technologies. By synthesizing current advancements and open
challenges, this work provides a roadmap for researchers and practitioners to
harness GAI for building resilient and adaptive security solutions in 6G
networks.

</details>


### [122] [Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability](https://arxiv.org/abs/2506.19870)
*Md Asif Ul Hoq Khan,MD Zahedul Islam,Istiaq Ahmed,Md Masud Karim Rabbi,Farhana Rahman Anonna,MD Abdul Fahim Zeeshan,Mehedi Hasan Ridoy,Bivash Ranjan Chowdhury,Md Nazmul Shakir Rabbi,GM Alamin Sadnan*

Main category: cs.CR

TL;DR: 研究开发了一个结合区块链和人工智能的安全、智能、高效的能源交易系统，用于解决去中心化能源市场中的安全和欺诈问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化能源市场的发展带来了安全和交易真实性的新挑战，需要一种创新的解决方案。

Method: 结合区块链和人工智能技术，利用包含120万条匿名交易记录的数据集，构建双层系统架构。

Result: 提出了一个高效的系统，能够提升交易安全性和市场可靠性，并检测欺诈行为。

Conclusion: 区块链与人工智能的结合为去中心化能源市场提供了可行的解决方案，解决了安全和欺诈问题。

Abstract: Peer-to-peer trading and the move to decentralized grids have reshaped the
energy markets in the United States. Notwithstanding, such developments lead to
new challenges, mainly regarding the safety and authenticity of energy trade.
This study aimed to develop and build a secure, intelligent, and efficient
energy transaction system for the decentralized US energy market. This research
interlinks the technological prowess of blockchain and artificial intelligence
(AI) in a novel way to solve long-standing challenges in the distributed energy
market, specifically those of security, fraudulent behavior detection, and
market reliability. The dataset for this research is comprised of more than 1.2
million anonymized energy transaction records from a simulated peer-to-peer
(P2P) energy exchange network emulating real-life blockchain-based American
microgrids, including those tested by LO3 Energy and Grid+ Labs. Each record
contains detailed fields of transaction identifier, timestamp, energy volume
(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier
(hashed for privacy), smart meter readings, geolocation regions, and settlement
confirmation status. The dataset also includes system-calculated behavior
metrics of transaction rate, variability of energy production, and historical
pricing patterns. The system architecture proposed involves the integration of
two layers, namely a blockchain layer and artificial intelligence (AI) layer,
each playing a unique but complementary function in energy transaction securing
and market intelligence improvement. The machine learning models used in this
research were specifically chosen for their established high performance in
classification tasks, specifically in the identification of energy transaction
fraud in decentralized markets.

</details>


### [123] [Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017](https://arxiv.org/abs/2506.19877)
*Zhaoyang Xu,Yunbo Liu*

Main category: cs.CR

TL;DR: 比较四种机器学习模型在入侵检测中的表现，发现监督模型在已知攻击上表现优异，但在未知攻击上表现不佳，而无监督模型在未知攻击上表现较好。


<details>
  <summary>Details</summary>
Motivation: 为动态网络环境中的入侵检测系统（IDS）选择适合的机器学习模型提供实践指导。

Method: 在CICIDS2017数据集上比较了四种模型（MLP、CNN、OCSVM和LOF）在已知和未知攻击检测中的表现。

Result: 监督模型（MLP和CNN）在已知攻击上准确率高，但在未知攻击上召回率低；无监督模型（LOF和OCSVM）在未知攻击上表现较好，其中OCSVM在精度和召回率上表现最佳。

Conclusion: OCSVM在动态网络环境中表现出稳健的检测能力，适合用于入侵检测系统。

Abstract: Identifying suitable machine learning paradigms for intrusion detection
remains critical for building effective and generalizable security solutions.
In this study, we present a controlled comparison of four representative models
- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),
One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on
the CICIDS2017 dataset under two scenarios: detecting known attack types and
generalizing to previously unseen threats. Our results show that supervised MLP
and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic
recall drops on novel attacks. Unsupervised LOF attains moderate overall
accuracy and high recall on unknown threats at the cost of elevated false
alarms, while boundary-based OCSVM balances precision and recall best,
demonstrating robust detection across both scenarios. These findings offer
practical guidance for selecting IDS models in dynamic network environments.

</details>


### [124] [Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models](https://arxiv.org/abs/2506.19881)
*Aloni Cohen*

Main category: cs.CR

TL;DR: 本文探讨生成模型输出是否侵犯训练数据版权的问题，提出新的版权保护框架，并证明差分隐私在特定条件下可实现版权保护。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型输出可能侵犯训练数据版权的问题，为版权保护提供更坚实的技术和法律基础。

Method: 分析近访问自由（NAF）的局限性，提出无责版权保护框架，并实例化为干净室版权保护。

Result: 证明NAF无法防止侵权，提出干净室版权保护框架，并证明差分隐私在特定条件下可实现版权保护。

Conclusion: 干净室版权保护为版权保护提供了更可靠的保证，差分隐私在满足特定条件时也能提供版权保护。

Abstract: Are there any conditions under which a generative model's outputs are
guaranteed not to infringe the copyrights of its training data? This is the
question of "provable copyright protection" first posed by Vyas, Kakade, and
Barak (ICML 2023). They define near access-freeness (NAF) and propose it as
sufficient for protection. This paper revisits the question and establishes new
foundations for provable copyright protection -- foundations that are firmer
both technically and legally. First, we show that NAF alone does not prevent
infringement. In fact, NAF models can enable verbatim copying, a blatant
failure of copy protection that we dub being tainted. Then, we introduce our
blameless copy protection framework for defining meaningful guarantees, and
instantiate it with clean-room copy protection. Clean-room copy protection
allows a user to control their risk of copying by behaving in a way that is
unlikely to copy in a counterfactual clean-room setting. Finally, we formalize
a common intuition about differential privacy and copyright by proving that DP
implies clean-room copy protection when the dataset is golden, a copyright
deduplication requirement.

</details>


### [125] [Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack](https://arxiv.org/abs/2506.19886)
*Xuesong Wang,Mo Li,Xingyan Shi,Zhaoqian Liu,Shenghao Yang*

Main category: cs.CR

TL;DR: DiffSem是一种基于扩散机制的语义通信框架，通过自引用标签嵌入优化语义信息重建，显著提升任务性能，并在动态信道中保持稳定表现。


<details>
  <summary>Details</summary>
Motivation: 解决任务导向语义通信中隐私保护与任务准确性之间的挑战，同时应对模型反转攻击的威胁。

Method: 提出DiffSem框架，利用扩散机制和自引用标签嵌入优化语义信息重建，并采用语义信息失真增强系统鲁棒性。

Result: 在MNIST数据集上，DiffSem分类准确率提升10.03%，且在动态信道中表现稳定。

Conclusion: DiffSem有效平衡隐私保护与任务性能，同时揭示传统图像质量指标与任务相关语义信息泄露之间的显著偏差。

Abstract: Semantic communication has emerged as a promising neural network-based system
design for 6G networks. Task-oriented semantic communication is a novel
paradigm whose core goal is to efficiently complete specific tasks by
transmitting semantic information, optimizing communication efficiency and task
performance. The key challenge lies in preserving privacy while maintaining
task accuracy, as this scenario is susceptible to model inversion attacks. In
such attacks, adversaries can restore or even reconstruct input data by
analyzing and processing model outputs, owing to the neural network-based
nature of the systems. In addition, traditional systems use image quality
indicators (such as PSNR or SSIM) to assess attack severity, which may be
inadequate for task-oriented semantic communication, since visual differences
do not necessarily ensure semantic divergence. In this paper, we propose a
diffusion-based semantic communication framework, named DiffSem, that optimizes
semantic information reconstruction through a diffusion mechanism with
self-referential label embedding to significantly improve task performance. Our
model also compensates channel noise and adopt semantic information distortion
to ensure the robustness of the system in various signal-to-noise ratio
environments. To evaluate the attacker's effectiveness, we propose a new metric
that better quantifies the semantic fidelity of estimations from the adversary.
Experimental results based on this criterion show that on the MNIST dataset,
DiffSem improves the classification accuracy by 10.03%, and maintain stable
performance under dynamic channels. Our results further demonstrate that
significant deviation exists between traditional image quality indicators and
the leakage of task-relevant semantic information.

</details>


### [126] [Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks](https://arxiv.org/abs/2506.20082)
*Yali Yuan,Weiyi Zou,Guang Cheng*

Main category: cs.CR

TL;DR: 该论文提出了一种名为ADWPF的注意力驱动的细粒度网页指纹攻击方法，用于解决大规模环境中用户访问多个子页面和多标签浏览带来的分类挑战。


<details>
  <summary>Details</summary>
Motivation: 传统网站指纹攻击（WF）在实验环境中有效，但在实际应用中仅能识别主页，无法处理用户快速访问多个子页面或多标签浏览的情况。

Method: 提出ADWPF方法，通过注意力驱动的数据增强（如注意力裁剪和掩码）提取低维特征，并利用自注意力模块捕捉全局上下文模式，同时使用残差注意力处理多标签场景。

Result: 实验表明，ADWPF在不同规模的数据集上均优于现有基线方法。

Conclusion: ADWPF通过注意力机制和细粒度特征提取，显著提升了大规模环境中网页指纹攻击的性能。

Abstract: Website Fingerprinting (WF) attacks aim to infer which websites a user is
visiting by analyzing traffic patterns, thereby compromising user anonymity.
Although this technique has been demonstrated to be effective in controlled
experimental environments, it remains largely limited to small-scale scenarios,
typically restricted to recognizing website homepages. In practical settings,
however, users frequently access multiple subpages in rapid succession, often
before previous content fully loads. WebPage Fingerprinting (WPF) generalizes
the WF framework to large-scale environments by modeling subpages of the same
site as distinct classes. These pages often share similar page elements,
resulting in lower inter-class variance in traffic features. Furthermore, we
consider multi-tab browsing scenarios, in which a single trace encompasses
multiple categories of webpages. This leads to overlapping traffic segments,
and similar features may appear in different positions within the traffic,
thereby increasing the difficulty of classification. To address these
challenges, we propose an attention-driven fine-grained WPF attack, named
ADWPF. Specifically, during the training phase, we apply targeted augmentation
to salient regions of the traffic based on attention maps, including attention
cropping and attention masking. ADWPF then extracts low-dimensional features
from both the original and augmented traffic and applies self-attention modules
to capture the global contextual patterns of the trace. Finally, to handle the
multi-tab scenario, we employ the residual attention to generate class-specific
representations of webpages occurring at different temporal positions.
Extensive experiments demonstrate that the proposed method consistently
surpasses state-of-the-art baselines across datasets of different scales.

</details>


### [127] [Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox](https://arxiv.org/abs/2506.20102)
*Malikussaid,Sutiyo*

Main category: cs.CR

TL;DR: 论文提出ARC框架，通过自主闭环强化过程实现分析韧性，利用红蓝代理的协同进化动态提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: IT与OT的融合导致关键基础设施面临新型智能攻击，传统静态防御失效，需新的安全范式。

Method: 引入ARC框架，包含红代理（攻击者）和蓝代理（防御者）的协同进化，通过对抗训练提升韧性。

Result: 在TEP和SWaT测试平台上验证了框架的优越性能，协同进化显著提升了对新型攻击的检测能力。

Conclusion: ARC框架是动态自改进安全范式的必要转变，适用于未来关键基础设施的保护。

Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing
critical infrastructure to a new class of adaptive, intelligent adversaries
that render static defenses obsolete. Existing security paradigms often fail to
address a foundational "Trinity of Trust," comprising the fidelity of the
system model, the integrity of synchronizing data, and the resilience of the
analytical engine against sophisticated evasion. This paper introduces the ARC
framework, a method for achieving analytical resilience through an autonomous,
closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms
race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red
Agent," is formalized and incentivized to autonomously discover stealthy,
physically-plausible attack paths that maximize process disruption while
evading detection. Concurrently, an ensemble-based "Blue Agent" defender is
continuously hardened via adversarial training against the evolving threats
discovered by its adversary. This co-evolutionary dynamic forces both agents to
become progressively more sophisticated, enabling the system to autonomously
probe and patch its own vulnerabilities. Experimental validation on both the
TEP and the SWaT testbeds demonstrates the framework's superior performance. A
comprehensive ablation study, supported by extensive visualizations including
ROC curves and SHAP plots, reveals that the co-evolutionary process itself is
responsible for a significant performance increase in detecting novel attacks.
By integrating XAI to ensure operator trust and proposing a scalable F-ARC
architecture, this work presents ARC not merely as an improvement, but as a
necessary paradigm shift toward dynamic, self-improving security for the future
of critical infrastructure.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [128] [Exploration-Exploitation Tradeoff in Universal Lossy Compression](https://arxiv.org/abs/2506.20261)
*Nir Weinberger,Ram Zamir*

Main category: cs.IT

TL;DR: 论文将通用压缩中的顺序模式重新建模为多臂老虎机问题，探讨了探索与利用的权衡，并分析了现有方法的局限性，提出了更鲁棒的成本导向算法。


<details>
  <summary>Details</summary>
Motivation: 研究通用压缩中顺序模式（反向适应）的探索与利用权衡问题，特别是在有损压缩场景下。

Method: 将顺序模式建模为多臂老虎机问题，分析现有“自然类型选择”方案的局限性，并提出鲁棒的成本导向算法。

Result: 证明了现有方法的局限性，并提出了适用于任意块长度的鲁棒算法。

Conclusion: 成本导向的多臂老虎机算法在顺序有损压缩中表现更优，适用于更广泛的场景。

Abstract: Universal compression can learn the source and adapt to it either in a batch
mode (forward adaptation), or in a sequential mode (backward adaptation). We
recast the sequential mode as a multi-armed bandit problem, a fundamental model
in reinforcement-learning, and study the trade-off between exploration and
exploitation in the lossy compression case. We show that a previously proposed
"natural type selection" scheme can be cast as a reconstruction-directed MAB
algorithm, for sequential lossy compression, and explain its limitations in
terms of robustness and short-block performance. We then derive and analyze
robust cost-directed MAB algorithms, which work at any block length.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [129] [Supervised Similarity for Firm Linkages](https://arxiv.org/abs/2506.19856)
*Ryan Samson,Adrian Banner,Luca Candelori,Sebastien Cottrell,Tiziana Di Matteo,Paul Duchnowski,Vahagn Kirakosyan,Jose Marques,Kharen Musaelian,Stefano Pasquali,Ryan Stever,Dario Villani*

Main category: q-fin.ST

TL;DR: 论文提出了一种新的企业关联代理方法（CVLs），通过欧几里得相似性和量子认知机器学习（QCML）估计关联性，并证明QCML方法在动量溢出交易策略中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究企业关联性的新方法，以改进动量溢出交易策略的构建。

Method: 使用欧几里得相似性和QCML方法估计企业关联性。

Result: 两种方法均能构建盈利的动量溢出交易策略，但QCML方法表现更优。

Conclusion: CVLs和QCML方法为企业关联性分析提供了有效工具，尤其在交易策略中QCML更具优势。

Abstract: We introduce a novel proxy for firm linkages, Characteristic Vector Linkages
(CVLs). We use this concept to estimate firm linkages, first through Euclidean
similarity, and then by applying Quantum Cognition Machine Learning (QCML) to
similarity learning. We demonstrate that both methods can be used to construct
profitable momentum spillover trading strategies, but QCML similarity
outperforms the simpler Euclidean similarity.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [130] [Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design](https://arxiv.org/abs/2506.20334)
*Daniele Ravasio,Marcello Farina,Alessio La Bella,Andrea Ballarino*

Main category: eess.SY

TL;DR: 本文研究了基于一类递归神经网络的输出反馈方案设计，提出了基于线性矩阵不等式的观测器和静态状态反馈控制器设计方法，并引入非线性模型预测控制器以增强性能。


<details>
  <summary>Details</summary>
Motivation: 针对递归神经网络系统的输出反馈问题，设计鲁棒的控制方案以应对干扰和状态估计不确定性。

Method: 采用线性矩阵不等式设计观测器和静态反馈控制器，并引入基于区域的增量输入-状态稳定性；进一步提出基于管道的非线性模型预测控制器（NMPC）。

Result: 理论分析表明，所提方法能保证收敛性和递归可行性，并通过数值仿真验证了其有效性。

Conclusion: 提出的方案在鲁棒性和性能上优于传统方法，适用于复杂系统控制。

Abstract: This paper investigates the design of output-feedback schemes for systems
described by a class of recurrent neural networks. We propose a procedure based
on linear matrix inequalities for designing an observer and a static
state-feedback controller. The algorithm leverages global and regional
incremental input-to-state stability (incremental ISS) and enables the tracking
of constant setpoints, ensuring robustness to disturbances and state estimation
uncertainty. To address the potential limitations of regional incremental ISS,
we introduce an alternative scheme in which the static law is replaced with a
tube-based nonlinear model predictive controller (NMPC) that exploits regional
incremental ISS properties. We show that these conditions enable the
formulation of a robust NMPC law with guarantees of convergence and recursive
feasibility, leading to an enlarged region of attraction. Theoretical results
are validated through numerical simulations on the pH-neutralisation process
benchmark, demonstrating the effectiveness of the proposed schemes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker,Agostino Capponi,J. Antonio Sidaoui*

Main category: stat.ML

TL;DR: 提出了一种数据驱动的动态因子框架，通过非线性流形学习技术揭示高维协变量与响应变量的联合动态，并利用线性扩散和卡尔曼滤波进行预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法对高维协变量与响应变量的联合动态建模存在局限性，需一种无需参数假设的数据驱动方法。

Method: 利用Anisotropic Diffusion Maps进行非线性流形学习，通过线性扩散和卡尔曼滤波预测动态。

Result: 在金融压力测试中，该方法比标准情景分析和主成分分析减少预测误差达55%和39%。

Conclusion: 该框架为高维动态建模提供了有效的数据驱动解决方案，显著提升了预测性能。

Abstract: We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [132] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong,Jiayi Wang,Zhengling Qi,Raymond Ka Wai Wong*

Main category: stat.ML

TL;DR: 该论文提出了一种称为拟合分布评估（FDE）的方法，用于在强化学习中扩展分布离策略评估（OPE），并提供了设计FDE方法的统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏统一框架，作者旨在填补这一空白，提供理论支持和新方法。

Method: 基于指导原则开发新的FDE方法，并进行收敛性分析，同时为现有方法提供理论依据。

Result: 实验表明，FDE方法在多种环境中（如线性二次调节器和Atari游戏）表现优异。

Conclusion: FDE方法为分布离策略评估提供了有效的解决方案，并具有理论和实验支持。

Abstract: In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [133] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu,Rahul Mazumder,Peter Radchenko*

Main category: stat.ML

TL;DR: 提出一种从树集成中提取紧凑决策规则集的估计器，提高可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 树集成模型预测能力强但难以解释，需提取可手动检查的规则以揭示数据关系。

Method: 开发精确和近似算法，控制规则数量和交互深度，优化正则化路径。

Result: 新估计器优于现有规则提取算法，预测性能接近理论最优。

Conclusion: 该方法在保持高预测性能的同时，显著提升了模型的可解释性。

Abstract: Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [134] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy,Liviu Aolaritei,Michael I. Jordan,Aymeric Dieuleveut*

Main category: stat.ML

TL;DR: 提出了一种基于稳定性的方法，确保在多个有效共形预测集中选择最优集时仍能保持覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 解决在多个有效共形预测集中选择最优集（如最小集）时可能破坏覆盖保证的问题。

Method: 提出稳定性方法，扩展至在线共形预测场景，并在有额外结构时提出改进。

Result: 实验验证了方法的有效性。

Conclusion: 稳定性方法能有效保持覆盖保证，适用于多种场景。

Abstract: Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [135] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang,Zhengling Qi,Yue Wu,Xiangyu Zhang,Yanxun Xu*

Main category: stat.ML

TL;DR: POLAR是一种新型的悲观模型策略学习算法，用于优化动态治疗策略（DTR），通过量化不确定性并提供理论保证，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖强假设且缺乏鲁棒性，而离线强化学习方法缺乏统计保证且计算复杂。POLAR旨在解决这些问题。

Method: POLAR从离线数据估计转移动态，量化不确定性，并在奖励函数中加入悲观惩罚以减少高不确定性行为。

Result: POLAR在合成数据和MIMIC-III数据集上表现优于现有方法，提供接近最优的历史感知治疗策略。

Conclusion: POLAR是首个兼具统计和计算保证的模型DTR方法，显著提升了动态治疗策略的优化效果。

Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [136] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson,Matt P. Wand,Joanna J. J. Wang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [137] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman,Kang Li,Tyler Maunu,Teng Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种动态平滑正则化的IRLS变体，证明了其在确定性条件下能线性收敛到潜在子空间，并扩展到仿射子空间估计，同时展示了其在低维神经网络训练中的实际优势。


<details>
  <summary>Details</summary>
Motivation: 稳健子空间估计是机器学习和数据分析的基础任务，但IRLS的理论性质尚未充分理解。本文旨在填补这一空白。

Method: 采用动态平滑正则化的IRLS变体，并在确定性条件下分析其收敛性。

Result: 证明了该IRLS变体能从任意初始点线性收敛到潜在子空间，并首次提供了仿射子空间估计的恢复理论。

Conclusion: 本文首次为IRLS在稳健子空间恢复和非凸IRLS在黎曼流形上的应用提供了全局收敛保证。

Abstract: Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [138] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev,Dimitar Iliev Dimitrov,Nikola Konstantinov*

Main category: stat.ML

TL;DR: 论文提出了一种学习者无关的鲁棒数据预过滤方法（LARP），旨在通过预过滤保护下游学习任务免受低质量数据的影响，尽管可能牺牲部分性能。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中常包含低质量或污染数据，影响学习效果，因此需要开发鲁棒的数据预过滤方法。

Method: 提出LARP框架，基于Huber估计器和Huber数据污染模型，分析多种预过滤方法，并通过实验验证其效果。

Result: 理论分析表明，LARP在异构学习者集上会导致性能损失，但实验证明其在大数据集上具有优势。

Conclusion: LARP在数据预过滤中提供了鲁棒性，尽管存在性能损失，但在大规模数据集中具有实用价值。

Abstract: The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [139] [The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape](https://arxiv.org/abs/2506.20104)
*Malikussaid,Sutiyo*

Main category: cs.CY

TL;DR: 俄罗斯入侵乌克兰改变了IT风险格局，尤其是云计算领域，推动了数据主权、网络安全和云基础设施策略的全球调整。


<details>
  <summary>Details</summary>
Motivation: 研究地缘政治冲突如何加速IT风险的重新评估，特别是针对国家支持的网络威胁和数据治理问题。

Method: 通过分析2022年至2025年初的网络行动、监管响应和组织适应，提出多层次的IT风险管理方法。

Result: 传统IT风险框架不足以应对地缘政治威胁，需结合弹性云架构、数据安全策略和地缘政治治理。

Conclusion: 多层次的综合方法能更有效应对地缘政治冲突带来的数字风险，强调技术和人员因素的结合。

Abstract: The Russian invasion of Ukraine has fundamentally altered the information
technology (IT) risk landscape, particularly in cloud computing environments.
This paper examines how this geopolitical conflict has accelerated data
sovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud
infrastructure strategies worldwide. Through an analysis of documented cyber
operations, regulatory responses, and organizational adaptations between 2022
and early 2025, this research demonstrates how the conflict has served as a
catalyst for a broader reassessment of IT risk. The research reveals that while
traditional IT risk frameworks offer foundational guidance, their standard
application may inadequately address the nuances of state-sponsored threats,
conflicting data governance regimes, and the weaponization of digital
dependencies without specific geopolitical augmentation. The contribution of
this paper lies in its focused synthesis and strategic adaptation of existing
best practices into a multi-layered approach. This approach uniquely synergizes
resilient cloud architectures (including sovereign and hybrid models), enhanced
data-centric security strategies (such as advanced encryption and
privacy-enhancing technologies), and geopolitically-informed governance to
build digital resilience. The interplay between these layers, emphasizing how
geopolitical insights directly shape architectural and security choices beyond
standard best practices-particularly by integrating the human element,
including personnel vulnerabilities and expertise, as a core consideration in
technical design and operational management-offers a more robust defense
against the specific, multifaceted risks arising from geopolitical conflict in
increasingly fractured digital territories.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [140] [OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning](https://arxiv.org/abs/2506.20297)
*Natalie Lang,Maya Simhi,Nir Shlezinger*

Main category: eess.SP

TL;DR: 论文提出了一种名为OLALa的联邦学习框架，通过动态调整量化器来优化通信效率和学习性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，固定的量化规则在异构和动态环境中表现不佳，因此需要一种能够在线调整的量化方案。

Method: 提出OLALa框架，允许客户端通过轻量级本地计算在线调整量化器，并设计了相应的在线学习算法。

Result: 实验表明，OLALa在各种量化率下均能提升学习性能，优于传统的固定码本和非自适应方案。

Conclusion: OLALa通过动态量化器调整，显著提高了联邦学习的效率和性能。

Abstract: Federated learning (FL) enables collaborative training across distributed
clients without sharing raw data, often at the cost of substantial
communication overhead induced by transmitting high-dimensional model updates.
This overhead can be alleviated by having the clients quantize their model
updates, with dithered lattice quantizers identified as an attractive scheme
due to its structural simplicity and convergence-preserving properties.
However, existing lattice-based FL schemes typically rely on a fixed
quantization rule, which is suboptimal in heterogeneous and dynamic
environments where the model updates distribution varies across users and
training rounds. In this work, we propose Online Learned Adaptive Lattices
(OLALa), a heterogeneous FL framework where each client can adjust its
quantizer online using lightweight local computations. We first derive
convergence guarantees for FL with non-fixed lattice quantizers and show that
proper lattice adaptation can tighten the convergence bound. Then, we design an
online learning algorithm that enables clients to tune their quantizers
throughout the FL process while exchanging only a compact set of quantization
parameters. Numerical experiments demonstrate that OLALa consistently improves
learning performance under various quantization rates, outperforming
conventional fixed-codebook and non-adaptive schemes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [141] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文评估了五种目标检测架构在三个历史文档数据集上的性能，发现Transformer和CNN-OBB模型各有优势，OBB对非笛卡尔布局至关重要。


<details>
  <summary>Details</summary>
Motivation: 历史文档布局复杂，需要鲁棒的文档布局分析（DLA）方法，以支持自动化处理和理解。

Method: 比较了五种目标检测架构（包括两种Transformer和三种YOLO变体）在三个数据集上的表现。

Result: Co-DETR在结构化数据集上表现最佳，而YOLOv11x-OBB在复杂数据集上显著优于其他模型。OBB是关键因素。

Conclusion: Transformer适合结构化布局，CNN-OBB适合复杂文档，OBB是准确建模历史文档的必要条件。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [142] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 提出了一种结合离线图像和在线笔画数据的端到端网络，通过早期融合提升手写识别性能。


<details>
  <summary>Details</summary>
Motivation: 手写识别通常仅利用单一模态（图像或笔画数据），而忽略了二者的互补性。

Method: 设计了一个共享潜在空间的网络，通过视觉标记和轻量级Transformer融合两种数据，并利用可学习查询增强上下文。

Result: 在IAMOn-DB和VNOn-DB数据集上实现了SOTA准确率，提升达1%。

Conclusion: 早期融合多模态数据能增强表示学习，提升识别性能和独立性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [143] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的纺织品相似性评估方法，用于艺术品中画布的鉴定和比较。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖线密度图匹配，无法适用于非连续位置的画布，需要更通用的解决方案。

Method: 设计了Siamese深度学习模型，通过图像对的特征表示比较画布相似性，并提出多样本预测聚合方法以提高鲁棒性。

Result: 在Museo Nacional del Prado的画布上验证了方法的可行性，证明即使线密度相似也能有效比较。

Conclusion: 该方法为艺术品分析提供了新途径，具有高准确性和实用性。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [144] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出了一种深度翻译动作识别框架，通过联合预测动作概念和辅助特征提升识别准确率，并引入两种新的领域特定描述符。


<details>
  <summary>Details</summary>
Motivation: 视频中的人类动作理解需要高级语义推理和多模态特征的有效整合，现有方法在特征表示和计算效率上存在不足。

Method: 结合对象检测特征（ODF）和显著性检测特征（SDF），整合多种辅助模态，并采用不确定性建模和鲁棒损失函数。

Result: 在多个基准测试（如Kinetics-400、Kinetics-600和Something-Something V2）上达到最先进性能。

Conclusion: 该框架能有效捕捉细粒度动作动态，且兼容多种先进架构。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [145] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 提出了一种基于失真不变特征学习的深度零水印框架，无需修改原图，通过特征空间优化学习参考签名。


<details>
  <summary>Details</summary>
Motivation: 解决传统水印方法对图像失真敏感的问题，同时保持图像的原始性。

Method: 采用噪声对抗学习训练特征提取器，结合对抗监督和重建约束，设计基于学习的多比特零水印方案。

Result: 在多种图像数据集和失真条件下，表现出卓越的特征稳定性和水印恢复能力。

Conclusion: 该框架在泛化性和鲁棒性上优于现有自监督和深度水印技术。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [146] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT和DyHiT是高效的视觉跟踪模型，通过轻量级Transformer和动态路由技术，在保持高性能的同时提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer跟踪器在资源受限设备上速度慢的问题。

Method: HiT采用Bridge Module和双图像位置编码；DyHiT通过动态路由分类场景并选择计算路径。

Result: HiT在NVIDIA Jetson AGX上达61 fps，AUC 64.6%；DyHiT达111 fps，AUC 62.4%。

Conclusion: HiT和DyHiT在速度和精度上取得平衡，动态路由方法可加速其他高性能跟踪器。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [147] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练、零样本的方法，通过两阶段流程和基于小波的细节增强模块，显著提升预训练扩散模型在超高分辨率图像合成中的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高分辨率图像合成中常出现伪影和空间不连贯问题，HiWave旨在解决这些问题，无需重新训练或修改模型架构。

Method: 采用两阶段流程：1）生成基础图像；2）通过DDIM反演和小波域细节增强模块，保留低频结构一致性并增强高频细节。

Result: 在Stable Diffusion XL上的评估表明，HiWave有效减少了伪影，用户研究中80%以上的人认为其优于现有方法。

Conclusion: HiWave为超高分辨率图像合成提供了一种高效且高质量的解决方案，无需额外训练或架构调整。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [148] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA是一种新型框架，通过处理压缩图像中的“块效应”和利用配对与非配对数据，显著提升了OSNs中深度伪造图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着GANs和DMs的发展，深度伪造图像在OSNs中广泛传播，现有检测方法忽视了压缩引入的“块效应”且依赖原始图像，难以应对实际场景。

Method: PLADA包含两个核心模块：B2E（双阶段注意力机制处理块效应）和ODA（处理配对与非配对数据）。

Result: 在26个数据集上的实验表明，PLADA在检测OSNs中的深度伪造图像时表现优异，尤其在有限配对数据和压缩情况下。

Conclusion: PLADA不仅解决了块效应问题，还为开放世界场景提供了鲁棒的深度伪造检测方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [149] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 论文提出了一种通过分组观测学习可识别因果表示的方法，用于胸部X光片的疾病分类，提高了模型的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，学习可识别的因果表示可以提升任务特定潜在特征的泛化性和鲁棒性。

Method: 采用端到端框架，通过分组观测学习可识别表示，并在实验中利用种族、性别和成像视角的分组来强制不变性。

Result: 实验表明，这种因果表示在多个分类任务中提高了泛化性和鲁棒性。

Conclusion: 分组观测学习可识别因果表示是一种有效的方法，能够提升医学影像分类任务的性能。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [150] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 本文提出了一种解耦表示学习（DRL）方法，用于提高显微镜图像分类模型的可解释性，并在三个不同领域的基准数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分析在诊断、合成工程和环境监测中至关重要，但深度学习模型的可解释性仍是一个挑战。

Method: 采用解耦表示学习（DRL）框架，通过从合成数据中学习表示，提升模型的可解释性。

Result: 在浮游生物、酵母液泡和人类细胞三个显微镜图像领域的基准数据集上，DRL框架在准确性和可解释性之间取得了良好平衡。

Conclusion: DRL方法为显微镜图像分类提供了一种兼具高准确性和可解释性的解决方案。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [151] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Main category: physics.geo-ph

TL;DR: 提出了一种基于CUDA和PyTorch的高性能双参数全波形反演框架，用于探地雷达数据，实现了高效且灵活的反演。


<details>
  <summary>Details</summary>
Motivation: 结合GPU计算效率和Python深度学习框架的灵活性，提升探地雷达数据的双参数反演性能。

Method: 通过将定制CUDA内核集成到PyTorch的自动微分机制中，实现介电常数和电导率的双参数反演。

Result: 在合成数据和实际波场数据上验证了高精度和高效性，支持多种正则化策略。

Conclusion: 该框架为快速地下成像提供了实用且可扩展的解决方案，适用于工程和环境监测等领域。

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [152] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.IR

TL;DR: 提出了一种名为CoVE的新系统，通过扩展词汇表并利用LLM的序列处理能力，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用LLM的序列信息处理能力，导致推荐任务性能不佳。

Method: 为每个项目分配唯一ID，扩展词汇表，并压缩嵌入层以适用于大规模应用。

Result: 在多个推荐数据集上验证了CoVE的有效性和性能，优于现有方法。

Conclusion: CoVE通过优化LLM的序列处理能力，显著提升了推荐系统的性能，并适用于工业级应用。

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [153] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: FemmIR是一种无需相似性标签的多媒体检索框架，利用预训练编码器和弱监督方法，避免标注开销，适用于数据标注稀缺的场景。


<details>
  <summary>Details</summary>
Motivation: 避免将检索作为监督分类任务带来的标注开销，并重用预训练编码器。

Method: 基于编辑距离的弱监督方法，通过多级交互评分衡量样本与查询的相关性。

Result: 在MuQNOL数据集上表现与现有检索系统相当，支持精确和近似相似性检索。

Conclusion: FemmIR在无需微调的情况下，有效实现了多模态检索任务。

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [154] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: 研究发现双塔模型在点击数据上训练可能导致排名性能下降，分析了混淆效应和模型可识别性问题，并提出样本加权技术以缓解问题。


<details>
  <summary>Details</summary>
Motivation: 解决双塔模型在点击数据训练中性能下降的问题，探究其根本原因并提出改进方法。

Method: 理论分析双塔模型的可识别性条件，研究日志策略对模型的影响，并提出样本加权技术。

Result: 发现文档位置交换或特征分布重叠是恢复模型参数的必要条件，日志策略在模型不完美时会放大偏差。

Conclusion: 样本加权技术可有效缓解偏差，为双塔模型的研究和实践提供了实用建议。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [155] [Finite-Time Information-Theoretic Bounds in Queueing Control](https://arxiv.org/abs/2506.18278)
*Yujie Liu,Vincent Y. F. Tan,Yunbei Xu*

Main category: math.OC

TL;DR: 论文提出了调度问题中总队列长度的有限时间信息论下界，并设计了新策略实现这些下界。研究发现MaxWeight在有限时间内存在性能局限，并提出了一种新的调度规则以匹配下界。


<details>
  <summary>Details</summary>
Motivation: 现有MaxWeight方法仅保证稳定性和渐近最优性，但在有限时间内性能不足，需探索更优的非渐近调度策略。

Method: 采用极小极大框架确定问题参数，提出信息论下界，分析MaxWeight的局限性，并设计基于Lyapunov漂移的新调度规则。

Result: 证明了MaxWeight在有限时间内性能不足，新调度规则在特定条件下能匹配下界。

Conclusion: 研究揭示了"仅漂移"方法的局限性，为非渐近最优队列控制提供了理论基础。

Abstract: We establish the first finite-time information-theoretic lower bounds-and
derive new policies that achieve them-for the total queue length in scheduling
problems over stochastic processing networks with both adversarial and
stochastic arrivals. Prior analyses of MaxWeight guarantee only stability and
asymptotic optimality in heavy traffic; we prove that, at finite horizons,
MaxWeight can incur strictly larger backlog by problem-dependent factors which
we identify. Our main innovations are 1) a minimax framework that pinpoints the
precise problem parameters governing any policy's finite-time performance; 2)
an information-theoretic lower bound on total queue length; 3) fundamental
limitation of MaxWeight that it is suboptimal in finite time; and 4) a new
scheduling rule that minimizes the full Lyapunov drift-including its
second-order term-thereby matching the lower bound under certain conditions, up
to universal constants. These findings reveal a fundamental limitation on
"drift-only" methods and points the way toward principled, non-asymptotic
optimality in queueing control.

</details>


### [156] [A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization](https://arxiv.org/abs/2506.20344)
*Po Chen,Rujun Jiang,Peng Wang*

Main category: math.OC

TL;DR: 该论文研究了深度矩阵分解（DMF）的优化基础，分析了其损失景观，并提供了临界点的分类条件，解释了梯度方法为何通常收敛到局部最小值。


<details>
  <summary>Details</summary>
Motivation: 尽管深度矩阵分解在多个领域有广泛应用，但其优化基础尚未完全明确，因此需要深入研究其损失景观和临界点性质。

Method: 通过提供临界点的闭式表达式，并建立其分类条件（如局部最小值、全局最小值、严格鞍点等），结合数值实验验证理论。

Result: 确定了临界点的分类条件，并解释了梯度方法收敛到局部最小值的原因。

Conclusion: 研究填补了深度矩阵分解优化基础的空白，为理解其损失景观和优化行为提供了理论支持。

Abstract: Despite its wide range of applications across various domains, the
optimization foundations of deep matrix factorization (DMF) remain largely
open. In this work, we aim to fill this gap by conducting a comprehensive study
of the loss landscape of the regularized DMF problem. Toward this goal, we
first provide a closed-form expression of all critical points. Building on
this, we establish precise conditions under which a critical point is a local
minimizer, a global minimizer, a strict saddle point, or a non-strict saddle
point. Leveraging these results, we derive a necessary and sufficient condition
under which each critical point is either a local minimizer or a strict saddle
point. This provides insights into why gradient-based methods almost always
converge to a local minimizer of the regularized DMF problem. Finally, we
conduct numerical experiments to visualize its loss landscape under different
settings to support our theory.

</details>


### [157] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Main category: math.OC

TL;DR: 论文提出了一种新的随机优化方法，确保约束几乎确定性地满足，同时期望最优性差距在容忍范围内。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅保证期望约束满足，但实际应用中需要确定性约束满足，以避免重大违反风险。

Method: 采用加速随机梯度（ASG）或改进的方差缩减ASG方案，应用于一系列二次惩罚子问题。

Result: 建立了计算确定性可行解的一阶Oracle复杂度界限。

Conclusion: 提出的方法不仅解决了确定性约束问题，还扩展了样本平均逼近方法的应用。

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [158] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Main category: cs.MA

TL;DR: 提出了一种动态多智能体系统中双边团队形成的框架，填补了现有研究在动态群体中双边分组选择影响的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单边分组、预定义团队或固定群体设置，动态群体中双边分组选择的影响尚未充分探索。

Method: 引入了一个学习双边团队形成的框架，研究其算法特性对策略性能和泛化能力的影响。

Result: 在广泛采用的多智能体场景中验证了方法的竞争力，多数场景下表现优异且泛化能力提升。

Conclusion: 双边团队形成框架为动态多智能体系统提供了新的研究方向，展示了算法特性对性能的重要影响。

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [159] [PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning](https://arxiv.org/abs/2506.20043)
*Ahmet Sarigun,Bora Uyar,Vedran Franke,Altuna Akalin*

Main category: q-bio.QM

TL;DR: PocketVina是一种快速、内存高效的分子对接框架，结合口袋预测和多口袋探索，在物理有效对接姿态采样上表现优异，适用于高通量虚拟筛选。


<details>
  <summary>Details</summary>
Motivation: 解决分子对接中物理有效配体结合姿态采样的挑战，特别是对于未见或结构多样的目标。

Method: 结合口袋预测与系统多口袋探索的搜索对接框架PocketVina。

Result: 在多个基准测试中表现优异，尤其在物理有效性和RMSD上达到先进水平，且适用于不同灵活性的配体。

Conclusion: PocketVina提供了一种无需任务特定训练、高效且可扩展的对接策略，适用于高通量虚拟筛选和药物发现。

Abstract: Sampling physically valid ligand-binding poses remains a major challenge in
molecular docking, particularly for unseen or structurally diverse targets. We
introduce PocketVina, a fast and memory-efficient, search-based docking
framework that combines pocket prediction with systematic multi-pocket
exploration. We evaluate PocketVina across four established
benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and
PoseBusters--and observe consistently strong performance in sampling physically
valid docking poses. PocketVina achieves state-of-the-art performance when
jointly considering ligand RMSD and physical validity (PB-valid), while
remaining competitive with deep learning-based approaches in terms of RMSD
alone, particularly on structurally diverse and previously unseen targets.
PocketVina also maintains state-of-the-art physically valid docking accuracy
across ligands with varying degrees of flexibility. We further introduce
TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000
protein-ligand pairs, and a partition of the dataset labeled with PubChem
activity annotations. On this large-scale dataset, PocketVina successfully
discriminates active from inactive targets, outperforming a deep learning
baseline while requiring significantly less GPU memory and runtime. PocketVina
offers a robust and scalable docking strategy that requires no task-specific
training and runs efficiently on standard GPUs, making it well-suited for
high-throughput virtual screening and structure-based drug discovery.

</details>
