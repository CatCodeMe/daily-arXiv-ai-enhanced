<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 77]
- [math.NA](#math.NA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.MS](#cs.MS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [math.ST](#math.ST) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 6]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications](https://arxiv.org/abs/2506.18951)
*Jinyang Li,Xiaolong Li,Ge Qu,Per Jacobsson,Bowen Qin,Binyuan Hui,Shuzheng Si,Nan Huo,Xiaohan Xu,Yue Zhang,Ziwei Tang,Yuanshuai Li,Florensia Widjaja,Xintong Zhu,Feige Zhou,Yongfeng Huang,Yannis Papakonstantinou,Fatma Ozcan,Chenhao Ma,Reynold Cheng*

Main category: cs.DB

TL;DR: 论文提出了BIRD-CRITIC基准和Six-Gym训练环境，用于评估和提升开源模型在SQL问题调试中的能力，并开发了Bird-Fixer代理，性能优于主流专有模型。


<details>
  <summary>Details</summary>
Motivation: 解决复杂SQL问题在实际数据库应用中的瓶颈，并填补当前大语言模型在SQL调试任务中缺乏严格评估的空白。

Method: 引入BIRD-CRITIC基准（包含PostgreSQL和多方言任务），开发Six-Gym训练环境（采用SQL-Rewind策略和f-Plan Boosting方法），并构建Bird-Fixer代理。

Result: Bird-Fixer在BIRD-CRITIC-PG上达到38.11%的成功率，在BIRD-CRITIC-Multi上达到29.65%，优于Claude-3.7-Sonnet和GPT-4.1。

Conclusion: 该研究为开源模型在SQL调试任务中的能力提升提供了重要工具，推动了SQL调试技术的民主化。

Abstract: Resolution of complex SQL issues persists as a significant bottleneck in
real-world database applications. Current Large Language Models (LLMs), while
adept at text-to-SQL translation, have not been rigorously evaluated on the
more challenging task of debugging SQL issues. To address this gap, we
introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530
PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks
(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within
new environments to facilitate rigorous evaluation. Baseline evaluations
underscore the task's complexity, with the leading reasoning model O3-Mini
achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on
BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks
is crucial for empowering local development while safeguarding data privacy.
Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for
elevating open-source model capabilities for SQL issue debugging. This
environment leverages SQL-Rewind strategy, which automatically generates
executable issue-solution datasets by reverse-engineering issues from verified
SQLs. However, popular trajectory-based fine-tuning methods do not explore
substantial supervisory signals. We further propose f-Plan Boosting, which
extracts high-level debugging plans from SQL solutions, enabling teacher LLMs
to produce 73.7% more successful trajectories for training. We integrate these
components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,
Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on
BIRD-CRITIC-Multi, surpassing leading proprietary models such as
Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing
sophisticated SQL-debugging capabilities. The leaderboard and source code are
available: https://bird-critic.github.io/

</details>


### [2] [Higher-Order Graph Databases](https://arxiv.org/abs/2506.19661)
*Maciej Besta,Shriram Chandran,Jakub Cudak,Patrick Iff,Marcin Copik,Robert Gerstenberger,Tomasz Szydlo,Jürgen Müller,Torsten Hoefler*

Main category: cs.DB

TL;DR: 论文提出了一种新型的高阶图数据库（HO-GDBs），通过提升和降低范式扩展传统图数据库，支持高阶交互，提升分析任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前图数据库系统无法支持高阶交互，限制了子图计数、多元建模等任务的需求。

Method: 引入提升和降低范式，设计轻量级、模块化且可并行化的HO-GDB原型，支持超图、节点元组等结构。

Result: 原型系统在大规模HO OLTP和OLAP工作负载中表现优异，提升图神经网络准确性44%。

Conclusion: HO-GDBs在低延迟、高吞吐和通用性方面表现突出，适用于ACID兼容和最终一致性系统。

Abstract: Recent advances in graph databases (GDBs) have been driving interest in
large-scale analytics, yet current systems fail to support higher-order (HO)
interactions beyond first-order (one-hop) relations, which are crucial for
tasks such as subgraph counting, polyadic modeling, and HO graph learning. We
address this by introducing a new class of systems, higher-order graph
databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly
extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and
OLAP queries, ensuring correctness, scalability, and ACID compliance. We
implement a lightweight, modular, and parallelizable HO-GDB prototype that
offers native support for hypergraphs, node-tuples, subgraphs, and other HO
structures under a unified API. The prototype scales to large HO OLTP & OLAP
workloads and shows how HO improves analytical tasks, for example enhancing
accuracy of graph neural networks within a GDB by 44%. Our work ensures low
latency and high query throughput, and generalizes both ACID-compliant and
eventually consistent systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Survey of HPC in US Research Institutions](https://arxiv.org/abs/2506.19019)
*Peng Shu,Junhao Chen,Zhengliang Liu,Huaqin Zhao,Xinliang Li,Tianming Liu*

Main category: cs.DC

TL;DR: 该论文调查了美国大学HPC系统的现状，发现其增长远低于国家和工业系统，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究大学HPC系统的资源不足问题，以支持学术研究和国家科研需求。

Method: 调查50多所研究机构，分析计算能力、架构设计、治理模式和能效。

Result: 大学HPC系统增长率（18%）显著低于国家（43%）和工业（78%）系统。

Conclusion: 建议采用联合计算、闲置GPU利用和成本分摊模式，以缩小差距并支持AI研究。

Abstract: The rapid growth of AI, data-intensive science, and digital twin technologies
has driven an unprecedented demand for high-performance computing (HPC) across
the research ecosystem. While national laboratories and industrial hyperscalers
have invested heavily in exascale and GPU-centric architectures,
university-operated HPC systems remain comparatively under-resourced. This
survey presents a comprehensive assessment of the HPC landscape across U.S.
universities, benchmarking their capabilities against Department of Energy
(DOE) leadership-class systems and industrial AI infrastructures. We examine
over 50 premier research institutions, analyzing compute capacity,
architectural design, governance models, and energy efficiency. Our findings
reveal that university clusters, though vital for academic research, exhibit
significantly lower growth trajectories (CAGR $\approx$ 18%) than their
national ($\approx$ 43%) and industrial ($\approx$ 78%) counterparts. The
increasing skew toward GPU-dense AI workloads has widened the capability gap,
highlighting the need for federated computing, idle-GPU harvesting, and
cost-sharing models. We also identify emerging paradigms, such as decentralized
reinforcement learning, as promising opportunities for democratizing AI
training within campus environments. Ultimately, this work provides actionable
insights for academic leaders, funding agencies, and technology partners to
ensure more equitable and sustainable HPC access in support of national
research priorities.

</details>


### [4] [Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms](https://arxiv.org/abs/2506.19197)
*Calum Buchanan,Puck Rombach,James Bagrow,Hamid R. Ossareh*

Main category: cs.DC

TL;DR: 本文提出了一种改进算法，用于优化单位球图的可靠性和区域覆盖，通过单顶点移动或添加来最大化可靠性，同时满足顶点分布均匀的约束。


<details>
  <summary>Details</summary>
Motivation: 在自主群体通信网络中，单位球图的顶点或边可能不可靠，影响网络连通性。设计高可靠性和均匀区域覆盖的群体形态是关键。

Method: 扩展了先前立方时间复杂度的算法，结合蒙特卡洛模拟，确定单顶点移动或添加的最佳位置，以最大化可靠性并满足顶点分布约束。

Result: 算法能有效生成高可靠性和均匀区域覆盖的单位球图，优于改进的Fruchterman-Reingold算法。

Conclusion: 该方法为设计高可靠性和均匀覆盖的群体网络提供了实用工具。

Abstract: A unit ball graph consists of a set of vertices, labeled by points in
Euclidean space, and edges joining all pairs of points within distance $1$.
These geometric graphs are used to model a variety of spatial networks,
including communication networks between agents in an autonomous swarm. In such
an application, vertices and/or edges of the graph may not be perfectly
reliable; an agent may experience failure or a communication link rendered
inoperable. With the goal of designing robust swarm formations, or unit ball
graphs with high reliability (probability of connectedness), in a preliminary
conference paper we provided an algorithm with cubic time complexity to
determine all possible changes to a unit ball graph by repositioning a single
vertex. Using this algorithm and Monte Carlo simulations, one obtains an
efficient method to modify a unit ball graph by moving a single vertex to a
location which maximizes the reliability. Another important consideration in
many swarm missions is area coverage, yet highly reliable ball graphs often
contain clusters of vertices. Here, we generalize our previous algorithm to
improve area coverage as well as reliability. Our algorithm determines a
location to add or move a vertex within a unit ball graph which maximizes the
reliability, under the constraint that no other vertices of the graph be within
some fixed distance. We compare this method of obtaining graphs with high
reliability and evenly distributed area coverage to another method which uses a
modified Fruchterman-Reingold algorithm for ball graphs.

</details>


### [5] [Shelby: Decentralized Storage Designed to Serve](https://arxiv.org/abs/2506.19233)
*Guy Goren,Andrew Hariri,Timothy D. R. Hartley,Ravi Kappiyoor,Alexander Spiegelman,David Zmick*

Main category: cs.DC

TL;DR: Shelby是一个高性能的去中心化存储协议，旨在满足高需求应用，提供快速、可靠的数据访问，同时保持去中心化特性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化存储协议在吞吐量、延迟、成本效益和可用性方面无法满足视频流、大规模数据分析或AI训练等高需求应用，导致Web3数据密集型应用仍依赖中心化基础设施。

Method: Shelby采用控制与数据平面分离、低复制开销的纠删码、最小修复带宽以及专用骨干网络连接RPC和存储节点。此外，通过付费读取激励性能，并引入新型审计协议提供强加密经济保证。

Result: Shelby实现了去中心化系统的高性能，为生产规模的Web3应用提供类似Web2的性能。

Conclusion: Shelby通过创新设计和优化，成功解决了现有去中心化存储协议的不足，为高需求Web3应用提供了可行的解决方案。

Abstract: Existing decentralized storage protocols fall short of the service required
by real-world applications. Their throughput, latency, cost-effectiveness, and
availability are insufficient for demanding workloads such as video streaming,
large-scale data analytics, or AI training. As a result, Web3 data-intensive
applications are predominantly dependent on centralized infrastructure.
  Shelby is a high-performance decentralized storage protocol designed to meet
demanding needs. It achieves fast, reliable access to large volumes of data
while preserving decentralization guarantees. The architecture reflects lessons
from Web2 systems: it separates control and data planes, uses erasure coding
with low replication overhead and minimal repair bandwidth, and operates over a
dedicated backbone connecting RPC and storage nodes. Reads are paid, which
incentivizes good performance. Shelby also introduces a novel auditing protocol
that provides strong cryptoeconomic guarantees without compromising
performance, a common limitation of other decentralized solutions. The result
is a decentralized system that brings Web2-grade performance to
production-scale, read-intensive Web3 applications.

</details>


### [6] [The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC](https://arxiv.org/abs/2506.19333)
*Craig Steven Wright*

Main category: cs.DC

TL;DR: 论文分析了闪电网络作为货币系统的特性，指出其与比特币基础层结算模型的差异，并揭示其可能形成流动性中心寡头垄断，具有影子银行特征。


<details>
  <summary>Details</summary>
Motivation: 研究闪电网络在交易需求增加时的表现，及其作为比特币扩展方案的潜在问题。

Method: 采用数学模型、博弈论证明和复杂性分析，研究闪电网络的运行机制和代理行为。

Result: 发现闪电网络的成本趋于有界，但可能形成不透明的金融中介，且存在系统性脆弱性。

Conclusion: 闪电网络并非简单扩展比特币，而是构成一种缺乏透明度和结算保障的合成金融系统。

Abstract: This paper presents a formal analysis of the Lightning Network as a monetary
system structurally diverging from Bitcoin's base-layer settlement model. We
demonstrate that under increasing transaction demand, BTC transaction fees rise
superlinearly due to throughput constraints, while Lightning Network routing
costs approach a bounded asymptote. Using mathematical modeling, game-theoretic
proofs, and complexity analysis, we show that Lightning enables indefinite
off-chain operation via the emergence of liquidity hub oligopolies. These hubs
exhibit properties of unregulated financial intermediaries, including rent
extraction, opacity, and systemic fragility. Strategic agent models show that
channel closure becomes economically infeasible, and routing problems approach
hardness limits in P-Space complexity. We conclude that Lightning does not
merely extend Bitcoin, but constitutes a synthetic financial system with
shadowbank characteristics, lacking reserve discipline, transparency, or
enforceable settlement guarantees.

</details>


### [7] [A Heuristic Algorithm for Shortest Path Search](https://arxiv.org/abs/2506.19349)
*Huashan Yu,Xiaolin Wang,Yingwei Luo*

Main category: cs.DC

TL;DR: 本文提出了一种新的最短路径搜索方法，通过动态步进和遍历优化启发式，显著提升了并行算法的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决单源最短路径（SSSP）问题中并行算法的速度、实用性和效率挑战。

Method: 引入动态步进启发式和遍历优化启发式，减少路径扩展和同步开销，优化负载均衡。

Result: 在73个真实和合成图上评估，算法比五种最先进实现快2.5倍至5.83倍。

Conclusion: 新方法显著提升了SSSP算法的效率和性能，适用于多种图结构。

Abstract: The Single-Source Shortest Path (SSSP) problem is well-known for the
challenges in developing fast, practical, and work-efficient parallel
algorithms. This work introduces a novel shortest path search method. It allows
paths with different lengths to be extended in parallel at the cost of almost
negligible repeated relaxations. A dynamic-stepping heuristic is proposed for
the method to efficiently reduce the extended paths and the synchronizations. A
traversal-optimization heuristic is proposed to improve the method by
efficiently reducing the created paths and alleviating the load imbalance.
Based on the method, the two heuristics are used to develop a practical SSSP
algorithm, which tactfully reduces workload and overhead. The heuristics and
the algorithm were evaluated on 73 real-world and synthetic graphs. The
algorithm was also compared with five state-of-the-art SSSP implementations. On
each GAP benchmark suite graph except Road, its speedup to the best achieved by
these five implementations is 2.5x to 5.83x.

</details>


### [8] [Computing Tree Structures in Anonymous Graphs via Mobile Agents](https://arxiv.org/abs/2506.19365)
*Prabhat Kumar Chand,Manish Kumar,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文研究了在基于代理的网络中构建最小生成树（MST）和广度优先搜索（BFS）树的问题，提出了一种高效算法，显著提高了内存和时间效率。


<details>
  <summary>Details</summary>
Motivation: 传统分布式计算中，MST和BFS树的构建通常在静态节点的消息传递模型下研究。本文探索了在移动代理网络中解决这些问题的新方法，以优化时间和内存使用。

Method: 采用同步模型，假设代理对图的参数（如节点数、边数、直径等）无先验知识。提出了一种确定性算法，解决了BFS树的构建、领导者选举和MST的构建。

Result: BFS树构建时间为O(min(DΔ, m log n) + n log n + Δ log² n)，内存为O(log n)位/代理；MST构建时间为O(n log n + Δ log² n)，内存为O(log n)位/代理。

Conclusion: 提出的算法在时间和内存效率上显著优于现有方法，实现了近乎线性的领导者选举和MST构建。

Abstract: Minimum Spanning Tree (MST) and Breadth-First Search (BFS) tree constructions
are classical problems in distributed computing, traditionally studied in the
message-passing model, where static nodes communicate via messages. This paper
investigates MST and BFS tree construction in an agent-based network, where
mobile agents explore a graph and compute. Each node hosts one agent, and
communication occurs when agents meet at a node. We consider $n$ agents
initially dispersed (one per node) in an anonymous, arbitrary $n$-node,
$m$-edge graph $G$. The goal is to construct the BFS and MST trees from this
configuration such that each tree edge is known to at least one of its
endpoints, while minimizing time and memory per agent. We work in a synchronous
model and assume agents have no prior knowledge of any graph parameters such as
$n$, $m$, $D$, $\Delta$ (graph diameter and maximum degree). Prior work solves
BFS in $O(D\Delta)$ rounds with $O(\log n)$ bits per agent, assuming the root
is known. We give a deterministic algorithm that constructs the BFS tree in
$O(\min(D\Delta, m\log n) + n\log n + \Delta \log^2 n)$ rounds using $O(\log
n)$ bits per agent without root knowledge. To determine the root, we solve
leader election and MST construction. We elect a leader and construct the MST
in $O(n\log n + \Delta \log^2 n)$ rounds, with $O(\log n)$ bits per agent.
Prior MST algorithms require $O(m + n\log n)$ rounds and $\max(\Delta, \log n)
\log n$ bits. Our results significantly improve memory efficiency and time,
achieving nearly linear-time leader election and MST. Agents are assumed to
know $\lambda$, the maximum identifier, bounded by a polynomial in $n$.

</details>


### [9] [Towards an Introspective Dynamic Model of Globally Distributed Computing Infrastructures](https://arxiv.org/abs/2506.19578)
*Ozgur O. Kilic,David K. Park,Yihui Ren,Tatiana Korchuganova,Sairam Sri Vatsavai,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Raees Khan,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Paul Nilsson,Verena Ingrid Martinez Outschoorn,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: 论文提出了一种基于真实数据的交互式系统，用于优化大规模科学合作中的计算和存储管理，通过分析PanDA工作流管理系统的执行记录，并开发生成式AI模型模拟负载。


<details>
  <summary>Details</summary>
Motivation: 大规模科学合作项目（如ATLAS、Belle II等）产生海量数据，现有数据管理和负载分配方法依赖启发式手段且缺乏动态评估模型，亟需更高效的解决方案。

Method: 通过分析PanDA系统的五个月执行记录，提取关键性能指标（如排队时间、错误率等），并开发生成式AI模型模拟负载时间序列，结合显性和隐性特征。

Result: 确定了影响性能的关键指标（如排队时间、远程数据访问），并构建了AI模型以模拟负载特征，为优化分配提供基础。

Conclusion: 提出的交互式系统和AI模型为大规模科学合作中的计算和存储管理提供了动态评估和优化工具，有望提升效率。

Abstract: Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and
others involve hundreds of research institutes and thousands of researchers
spread across the globe. These experiments generate petabytes of data, with
volumes soon expected to reach exabytes. Consequently, there is a growing need
for computation, including structured data processing from raw data to
consumer-ready derived data, extensive Monte Carlo simulation campaigns, and a
wide range of end-user analysis. To manage these computational and storage
demands, centralized workflow and data management systems are implemented.
However, decisions regarding data placement and payload allocation are often
made disjointly and via heuristic means. A significant obstacle in adopting
more effective heuristic or AI-driven solutions is the absence of a quick and
reliable introspective dynamic model to evaluate and refine alternative
approaches. In this study, we aim to develop such an interactive system using
real-world data. By examining job execution records from the PanDA workflow
management system, we have pinpointed key performance indicators such as
queuing time, error rate, and the extent of remote data access. The dataset
includes five months of activity. Additionally, we are creating a generative AI
model to simulate time series of payloads, which incorporate visible features
like category, event count, and submitting group, as well as hidden features
like the total computational load-derived from existing PanDA records and
computing site capabilities. These hidden features, which are not visible to
job allocators, whether heuristic or AI-driven, influence factors such as
queuing times and data movement.

</details>


### [10] [PS-WL: A Probability-Sensitive Wear Leveling scheme for SSD array scaling](https://arxiv.org/abs/2506.19660)
*Shuhang Xu,Yunfei Gu,Linhui Liu,Chentao Wu*

Main category: cs.DC

TL;DR: 论文提出了一种概率敏感的磨损均衡方案（PS-WL），通过直接平衡故障风险而非磨损，显著降低了SSD阵列的故障风险。


<details>
  <summary>Details</summary>
Motivation: 传统磨损均衡（WL）在SSD阵列扩展时忽略了磨损与故障概率的非线性关系，可能导致老化磁盘过早故障。

Method: PS-WL引入基于实际故障概率的“有效寿命”模型，并通过PID控制器进行磨损均衡操作，同时限制热数据迁移以减少性能开销。

Result: 仿真结果表明，PS-WL在降低性能开销的同时，显著降低了阵列的总体故障风险。

Conclusion: PS-WL通过直接优化可靠性，构建了更安全、高效且稳定的可扩展存储系统。

Abstract: As flash-based Solid State Drive (SSD) arrays become essential to modern data
centers, scaling these arrays to meet explosive data growth is a frequent and
critical operation. However, the conventional wear-leveling (WL) paradigm
applied during scaling suffers from a fundamental flaw: it ignores the
non-linear relationship between wear and failure probability, potentially
pushing the most vulnerable, aged disks towards premature failure. To address
this critical issue at its root, we propose the Probability-Sensitive Wear
Leveling (PS-WL) scheme, which shifts the optimization goal from balancing wear
to directly balancing failure risk. At its core, PS-WL introduces an "effective
lifetime" model derived from a realistic failure probability to more accurately
assess disk lifetime. This model guides a PID controller for wear leveling
operation, with a conservative zone minimizes performance overhead by
restricting warm data migration. Comprehensive simulations validate the
superiority of PS-WL over state-of-the-art methods. The results demonstrate
that our approach significantly reduces performance overhead while, most
critically, consistently and effectively lowering the aggregated array failure
risk across diverse system configurations and workloads. This proves that by
directly optimizing for reliability, PS-WL builds a scalable storage system
that is, by design, fundamentally safer, more efficient, and more stable.

</details>


### [11] [Formalization and security analysis of the Bridgeless protocol](https://arxiv.org/abs/2506.19730)
*Orestis Alpos,Oleg Fomenko,Dimitris Karakostas,Oleksandr Kurbatov,Andrey Sabelnikov*

Main category: cs.DC

TL;DR: 本文形式化并证明了Bridgeless协议的安全性，该协议支持跨链代币桥接。


<details>
  <summary>Details</summary>
Motivation: 研究跨链代币桥接的安全性问题，确保协议在多种链上的安全性和活性。

Method: 协议由一组验证者运行，负责验证源链上的存款交易并在目标链上生成相应的提款。协议设计为链无关，验证者通过链客户端与支持的链交互。

Result: 形式化了所有子协议，并描述了协议保持安全性和活性的条件。

Conclusion: Bridgeless协议在支持的链（如EVM兼容链、Zano和比特币链）上实现了安全且高效的跨链代币桥接。

Abstract: This paper formalizes the proves the security of the Bridgeless protocol, a
protocol able to bridge tokens between various chains. The Bridgeless protocol
is run by a set of validators, responsible for verifying deposit transactions
on the source chain and generating the corresponding withdrawals on the target
chain. The protocol is designed to be chain-agnostic and the validators
interact with each supported chain via a chain client. It currently supports
EVM-compatible chains, the Zano, and the Bitcoin chains. The paper formalizes
all involved subprotocols and describes the conditions under which the protocol
maintains safety and liveness.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [12] [Incremental Shortest Paths in Almost Linear Time via a Modified Interior Point Method](https://arxiv.org/abs/2506.19207)
*Yang P. Liu*

Main category: cs.DS

TL;DR: 提出了一种动态图算法，用于在有向图中维护近似最短路径距离，时间复杂度为$m^{1+o(1)}\log W$。


<details>
  <summary>Details</summary>
Motivation: 解决动态图中边插入时维护近似最短路径距离的高效计算问题。

Method: 使用非标准内点法检测距离变化，并结合确定性最小比率循环数据结构。

Result: 算法在$m^{1+o(1)}\log W$时间内实现了$(1+\epsilon)$-近似最短路径距离维护。

Conclusion: 该算法为动态图问题提供了一种高效的确定性解决方案。

Abstract: We give an algorithm that takes a directed graph $G$ undergoing $m$ edge
insertions with lengths in $[1, W]$, and maintains $(1+\epsilon)$-approximate
shortest path distances from a fixed source $s$ to all other vertices. The
algorithm is deterministic and runs in total time $m^{1+o(1)}\log W$, for any
$\epsilon > \exp(-(\log m)^{0.99})$. This is achieved by designing a
nonstandard interior point method to crudely detect when the distances from $s$
other vertices $v$ have decreased by a $(1+\epsilon)$ factor, and implementing
it using the deterministic min-ratio cycle data structure of
[Chen-Kyng-Liu-Meierhans-Probst, STOC 2024].

</details>


### [13] [Subcoloring of (Unit) Disk Graphs](https://arxiv.org/abs/2506.19452)
*Malory Marin,Rémi Watrigant*

Main category: cs.DS

TL;DR: 研究了单位圆盘图的子着色问题，提出了多项式时间内的3-近似算法，并证明了在特殊情况下2-子着色是NP难的。此外，解决了Broersma等人的开放问题，并给出了圆盘图的子着色上界和近似算法。


<details>
  <summary>Details</summary>
Motivation: 子着色是经典着色的自然推广，研究其在单位圆盘图上的性质，有助于理解密集图的着色问题。

Method: 通过定义分解和特殊的Δ-圆盘图，提出近似算法，并证明在特殊情况下问题的复杂性。

Result: 证明了单位圆盘图的子着色数可3-近似计算，2-子着色在特殊情况下是NP难的，并给出了圆盘图子着色的上界和近似算法。

Conclusion: 子着色在单位圆盘图中具有丰富的理论性质，未来可进一步优化近似算法或探索其他图类的子着色问题。

Abstract: A subcoloring of a graph is a partition of its vertex set into subsets
(called colors), each inducing a disjoint union of cliques. It is a natural
generalization of the classical proper coloring, in which each color must
instead induce an independent set. Similarly to proper coloring, we define the
subchromatic number of a graph as the minimum integer k such that it admits a
subcoloring with k colors, and the corresponding problem k-Subcoloring which
asks whether a graph has subchromatic number at most k. In this paper, we
initiate the study of the subcoloring of (unit) disk graphs. One motivation
stems from the fact that disk graphs can be seen as a dense generalization of
planar graphs where, intuitively, each vertex can be blown into a large
clique--much like subcoloring generalizes proper coloring. Interestingly, it
can be observed that every unit disk graph admits a subcoloring with at most 7
colors. We first prove that the subchromatic number can be 3-approximated in
polynomial-time in unit disk graphs. We then present several hardness results
for special cases of unit disk graphs which somehow prevents the use of
classical approaches for improving this result. We show in particular that
2-subcoloring remains NP-hard in triangle-free unit disk graphs, as well as in
unit disk graphs representable within a strip of bounded height. We also solve
an open question of Broersma, Fomin, Ne\v{s}et\v{r}il, and Woeginger (2002) by
proving that 3-Subcoloring remains NP-hard in co-comparability graphs. Finally,
we prove that every $n$-vertex disk graph admits a subcoloring with at most
$O(\log^3(n))$ colors and present a $O(\log^2(n))$-approximation algorithm for
computing the subchromatic number of such graphs. This is achieved by defining
a decomposition and a special type of co-comparability disk graph, called
$\Delta$-disk graphs, which might be of independent interest.

</details>


### [14] [Approximating Submodular Matroid-Constrained Partitioning](https://arxiv.org/abs/2506.19507)
*Kristóf Bérczi,Karthekeyan Chandrasekaran,Tamás Király,Daniel P. Szabo*

Main category: cs.DS

TL;DR: 研究提出了一种统一固定终端和全局设置的最小化子模拟阵约束划分问题，并探索了其近似性。


<details>
  <summary>Details</summary>
Motivation: 子模划分问题在近似性方面已有大量研究，但固定终端和全局设置的研究是分开的，本文旨在统一这两种设置。

Method: 提出最小化子模拟阵约束划分问题，并研究其近似性，包括对称子模函数、单调子模函数和一般子模函数的情况。

Result: 在对称子模函数的特殊情况下达到了最新技术水平，同时对单调和一般子模函数也提供了结果。

Conclusion: 该研究统一了子模划分问题的两种设置，并提供了新的近似性结果。

Abstract: The submodular partitioning problem asks to minimize, over all partitions P
of a ground set V, the sum of a given submodular function f over the parts of
P. The problem has seen considerable work in approximability, as it encompasses
multiterminal cuts on graphs, k-cuts on hypergraphs, and elementary linear
algebra problems such as matrix multiway partitioning. This research has been
divided between the fixed terminal setting, where we are given a set of
terminals that must be separated by P, and the global setting, where the only
constraint is the size of the partition. We investigate a generalization that
unifies these two settings: minimum submodular matroid-constrained partition.
In this problem, we are additionally given a matroid over the ground set and
seek to find a partition P in which there exists some basis that is separated
by P. We explore the approximability of this problem and its variants, reaching
the state of the art for the special case of symmetric submodular functions,
and provide results for monotone and general submodular functions as well.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](https://arxiv.org/abs/2506.19045)
*Ahmadreza Saboor Yaraghi,Golnaz Gharachorlu,Sakina Fatima,Lionel C. Briand,Ruiyuan Wan,Ruifeng Gao*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的静态方法，用于定位系统测试代码中的故障，无需执行测试用例，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统故障定位依赖于重复执行，成本高且不适用于非确定性故障；现有LLM方法主要关注被测系统而非测试代码。

Method: 利用失败执行日志估计执行轨迹，结合三种新算法修剪代码，再通过LLM对潜在故障位置进行排序。

Result: 修剪后的轨迹与实际轨迹匹配度达90%，推理时间减少34%，块级定位在Top-3命中率达81%。

Conclusion: 静态LLM方法在测试代码故障定位中高效且实用，尤其在块级定位中表现最佳。

Abstract: Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, pruning the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).

</details>


### [16] [Dataset of Yul Contracts to Support Solidity Compiler Research](https://arxiv.org/abs/2506.19153)
*Krzysztof Fonal*

Main category: cs.SE

TL;DR: YulCode数据集包含348,840个基于Yul的智能合约实例，约135,013个独特合约，填补了智能合约数据集中针对Yul语言的空白。


<details>
  <summary>Details</summary>
Motivation: 当前智能合约数据集缺乏针对Yul语言的资源，YulCode填补了这一空白，为低级别智能合约分析和生成研究提供基础。

Method: 通过编译已部署在以太坊主网上的Solidity源代码生成Yul合约实例。

Result: 数据集直接代表现实世界的去中心化应用，适用于机器学习、形式验证、优化分析等任务。

Conclusion: YulCode是首个公开的Yul语言数据集，为低级别智能合约研究开辟了新途径。

Abstract: The YulCode dataset presents a comprehensive collection of 348,840 Yul-based
smart contract instances, comprising approximately 135,013 unique contracts.
These contracts were generated through the compilation of Solidity source files
that have been deployed on the Ethereum mainnet, making the dataset directly
representative of real-world decentralized applications. YulCode provides a
rich foundation for a variety of research and development tasks, including but
not limited to machine learning applications, formal verification, optimization
analysis, and software engineering tool evaluation in the context of low-level
smart contract code. To the best of our knowledge at the time of writing,
YulCode is the first and only publicly available dataset that focuses
specifically on Yul, an intermediate language designed for the Ethereum Virtual
Machine (EVM). As such, it fills a critical gap in the current ecosystem of
smart contract datasets and opens new avenues for research and tooling aimed at
low-level contract analysis and generation.

</details>


### [17] [Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs](https://arxiv.org/abs/2506.19287)
*Yaoxuan Wu,Xiaojie Zhou,Ahmad Humayun,Muhammad Ali Gulzar,Miryung Kim*

Main category: cs.SE

TL;DR: PALM结合符号执行与LLM生成测试，通过静态路径枚举和程序变体生成，避免传统约束求解的限制，并提供可视化界面帮助用户理解路径覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统符号执行受限于约束建模和求解能力，而LLM生成的测试缺乏系统路径枚举能力，PALM旨在结合两者优势。

Method: PALM通过AST分析静态枚举路径，生成带断言的可执行变体，利用LLM生成测试，并提供可视化界面展示路径覆盖。

Result: 用户研究表明，PALM的可视化界面帮助用户更好地理解路径覆盖和测试效果。

Conclusion: PALM成功结合符号执行与LLM，解决了传统方法的局限性，并通过交互式界面提升了测试生成的可理解性。

Abstract: Symbolic execution is a widely used technique for test generation, offering
systematic exploration of program paths through constraint solving. However, it
is fundamentally constrained by the capability to model the target code
including library functions in terms of symbolic constraint and the capability
of underlying constraint solvers. As a result, many paths involving complex
features remain unanalyzed or insufficiently modeled. Recent advances in large
language models (LLMs) have shown promise in generating diverse and valid test
inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths
and often fail to cover subtle corner cases. We observe that directly prompting
an LLM with the full program leads to missed coverage of interesting paths. In
this paper, we present PALM, a test generation system that combines symbolic
path enumeration with LLM-assisted test generation. PALM statically enumerates
possible paths through AST-level analysis and transforms each into an
executable variant with embedded assertions that specify the target path. This
avoids the need to translate path constraints into SMT formulae, by instead
constructing program variants that LLM can interpret. Importantly, PALM is the
first to provide an interactive frontend that visualizes path coverage
alongside generated tests, assembling tests based on the specific paths they
exercise. A user study with 12 participants demonstrates that PALM's frontend
helps users better understand path coverage and identify which paths are
actually exercised by PALM-generated tests, through verification and
visualization of their path profiles.

</details>


### [18] [What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance](https://arxiv.org/abs/2506.19425)
*Ang Jia,He Jiang,Zhilei Ren,Xiaochen Li,Ming Fan,Ting Liu*

Main category: cs.SE

TL;DR: 本文研究了不同编译设置对函数调用图（FCG）的影响，发现FCG变化显著，现有二进制分解方法在跨编译器评估中面临挑战，并提出了一种识别最优分解的方法。


<details>
  <summary>Details</summary>
Motivation: 现有二进制分解方法依赖于函数调用关系的相似性，但不同编译设置（尤其是函数内联）会导致FCG显著变化，影响分解效果。

Method: 构建了一个由17种编译器、6种优化和4种架构编译的数据集，分析FCG的变化和映射关系，并评估现有方法在FCG变化下的表现。

Result: 发现FCG大小变化显著，但仍通过三种映射关系链接；现有方法在跨编译器评估中表现不佳。

Conclusion: 提出了一种识别最优分解的方法，发现现有方法在覆盖率和社区相似性稳定性方面存在问题。

Abstract: Binary decomposition, which decomposes binary files into modules, plays a
critical role in binary reuse detection. Existing binary decomposition works
either apply anchor-based methods by extending anchor functions to generate
modules, or apply clustering-based methods by using clustering algorithms to
group binary functions, which all rely on that reused code shares similar
function call relationships. However, we find that function call graphs (FCGs)
vary a lot when using different compilation settings, especially with diverse
function inlining decisions.
  In this work, we conduct the first systematic empirical study on the variance
of FCGs compiled by various compilation settings and explore its effect on
binary decomposition methods. We first construct a dataset compiled by 17
compilers, using 6 optimizations to 4 architectures and analyze the changes and
mappings of the FCGs. We find that the size of FCGs changes dramatically, while
the FCGs are still linked by three different kinds of mappings. Then we
evaluate the existing works under the FCG variance, and results show that
existing works are facing great challenges when conducting cross-compiler
evaluation with diverse optimization settings. Finally, we propose a method to
identify the optimal decomposition and compare the existing decomposition works
with the optimal decomposition. Existing works either suffer from low coverage
or cannot generate stable community similarities.

</details>


### [19] [LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code](https://arxiv.org/abs/2506.19481)
*Shahbaz Siddeeq,Muhammad Waseem,Zeeshan Rasheed,Md Mahade Hasan,Jussi Rasku,Mika Saari,Henri Terho,Kalle Makela,Kai-Kristian Kemell,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 提出了一种基于大型语言模型（LLM）的多智能体系统，用于自动化Haskell代码重构，实验结果显示代码复杂度降低11.03%，质量提升22.46%，性能效率提高13.27%。


<details>
  <summary>Details</summary>
Motivation: 代码重构是软件开发中的常见任务，但传统方法依赖人工分析，效率低且易引入缺陷。研究旨在利用LLM多智能体系统实现自动化重构。

Method: 设计了一个多智能体系统，包含代码分析、重构执行、验证和调试等角色，并在开源Haskell代码库上进行了实验评估。

Result: 实验表明，系统能显著降低代码复杂度（11.03%）、提升代码质量（22.46%）和性能效率（13.27%），并优化内存分配（14.57%）。

Conclusion: LLM多智能体系统在函数式编程语言重构中具有潜力，可提升代码可维护性并支持自动化开发流程。

Abstract: Refactoring is a constant activity in software development and maintenance.
Scale and maintain software systems are based on code refactoring. However,
this process is still labor intensive, as it requires programmers to analyze
the codebases in detail to avoid introducing new defects. In this research, we
put forward a large language model (LLM)-based multi-agent system to automate
the refactoring process on Haskell code. The objective of this research is to
evaluate the effect of LLM-based agents in performing structured and
semantically accurate refactoring on Haskell code. Our proposed multi-agent
system based on specialized agents with distinct roles, including code
analysis, refactoring execution, verification, and debugging. To test the
effectiveness and practical applicability of the multi-agent system, we
conducted evaluations using different open-source Haskell codebases. The
results of the experiments carried out showed that the proposed LLM-based
multi-agent system could average 11.03% decreased complexity in code, an
improvement of 22.46% in overall code quality, and increase performance
efficiency by an average of 13.27%. Furthermore, memory allocation was
optimized by up to 14.57%. These results highlight the ability of LLM-based
multi-agent in managing refactoring tasks targeted toward functional
programming paradigms. Our findings hint that LLM-based multi-agent systems
integration into the refactoring of functional programming languages can
enhance maintainability and support automated development workflows.

</details>


### [20] [Integrating Pair Programming as a Work Practice](https://arxiv.org/abs/2506.19511)
*Nina Haugland Andersen,Anastasiia Tkalich,Nils Brede Moe,Darja Smite,Asgaut Mjølne Söderbom,Ola Hast,Viktoria Stray*

Main category: cs.SE

TL;DR: 本研究探讨了影响团队成员采用和持续参与结对编程（PP）的因素，发现多种因素如感知贡献、努力程度、团队态度等影响PP的参与。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，知识共享和团队协作变得至关重要，但PP的采用仍不一致，因此研究其影响因素。

Method: 在挪威一家成熟的敏捷公司进行探索性单案例研究，通过两轮访谈和主题分析收集数据。

Result: 发现PP的参与受多种因素影响，包括对日常工作的贡献感知、努力程度、团队态度、资源、基础设施和任务特性。

Conclusion: 长期参与PP需要实践带来的预期效益得到验证，并根据团队特点调整实践。研究结果对希望将PP融入团队工作流程的从业者有益。

Abstract: Context: Pair programming (PP) is more relevant than ever. As modern systems
grow in complexity, knowledge sharing and collaboration across teams have
become essential. However, despite well-documented benefits of PP, its adoption
remains inconsistent across software teams. Objective: This study aims to
understand the factors that facilitate or hinder team members' adoption as well
as lasting engagement in PP. Method: We have conducted an exploratory
single-case study in a mature agile company in Norway. We collected data
through two rounds of interviews with team members in different roles and
performed a thematic analysis of the interviews. Results: Our key finding is
that multiple factors, related to the perceptions of how PP contributes to
daily work, efforts associated with engaging in PP sessions, company and team
attitudes, resources, infrastructure, and task characteristics, affect PP
engagement. Conclusion: Long-term engagement in PP requires expected benefits
with the practice being confirmed in firsthand experiences. Adapting the
practice to each unique team, with insights drawn from collective learning, is
also beneficial. Our findings will be beneficial for software practitioners
seeking to make PP an integrated part of their team's workflow.

</details>


### [21] [Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language](https://arxiv.org/abs/2506.19539)
*Julian Fragner,Christian Macho,Bernhard Dieber,Martin Pinzger*

Main category: cs.SE

TL;DR: Reptile是一个工具，结合规则和GPT-4，将正则表达式转换为DPL模式，提高迁移效率。


<details>
  <summary>Details</summary>
Motivation: 企业迁移到现代日志分析平台时，手动转换正则表达式到DPL模式成本高且易出错。

Method: 结合规则转换和GPT-4优化，部分转换采用尽力而为策略。

Result: 成功转换73.7%的正则表达式，优化后的F1分数和MCC超过0.91。

Conclusion: Reptile为迁移到现代日志分析平台提供了高效且可靠的解决方案。

Abstract: Log files provide valuable information for detecting and diagnosing problems
in enterprise software applications and data centers. Several log analytics
tools and platforms were developed to help filter and extract information from
logs, typically using regular expressions (RegExes). Recent commercial log
analytics platforms provide domain-specific languages specifically designed for
log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,
users who want to migrate to these platforms must manually convert their
RegExes into the new pattern language, which is costly and error-prone. In this
work, we present Reptile, which combines a rule-based approach for converting
RegExes into DPL patterns with a best-effort approach for cases where a full
conversion is impossible. Furthermore, it integrates GPT-4 to optimize the
obtained DPL patterns. The evaluation with 946 RegExes collected from a large
company shows that Reptile safely converted 73.7% of them. The evaluation of
Reptile's pattern optimization with 23 real-world RegExes showed an F1-score
and MCC above 0.91. These results are promising and have ample practical
implications for companies that migrate to a modern log analytics platform,
such as Dynatrace.

</details>


### [22] [Simulating the Waterfall Model: A Systematic Review](https://arxiv.org/abs/2506.19653)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了瀑布模型在计算模拟中的表现，发现尽管敏捷方法主导现代软件开发，瀑布模型仍在混合方法中存在。研究分析了68篇文献，揭示了模拟方法、工具、趋势及对Royce原始模型的忠实度。


<details>
  <summary>Details</summary>
Motivation: 尽管瀑布模型在混合开发方法中仍有应用，但其在学术研究中的模拟表现鲜少被关注，本研究旨在填补这一空白。

Method: 通过结构化检索2000-2024年的68篇文献，从模拟方法、工具、地理与时间趋势及对原始模型的忠实度四个维度进行分析。

Result: 离散事件模拟最常用，工具从专有平台转向开源Python工具；无研究完全遵循Royce原始模型，多为改编版本。

Conclusion: 瀑布模型模拟虽小众但存在，未来需开发易用工具并研究其与现代混合方法的结合。

Abstract: This systematic mapping study examines how the Waterfall Model has been
represented in computational simulations within peer-reviewed literature. While
Agile methodologies dominate contemporary software design practices, the
Waterfall Model persists, particularly, within hybrid approaches that fuse
structured, sequential workflows with the adaptability of agile practices.
Despite its continued presence, little attention has been given to how the
Waterfall Model is simulated in research contexts. A structured search of major
academic databases identified 68 peer-reviewed studies published between 2000
and 2024. After applying inclusion criteria, selected studies were analyzed
across four dimensions: (1) simulation methodologies (e.g., discrete-event
simulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,
SimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's
original seven-phase model. Discrete-event simulation was most commonly used,
reflecting the model's sequential nature. Early work relied on proprietary
platforms, while recent studies increasingly use open-source, Python-based
tools. No studies fully implemented Royce's original formulation, most employed
adaptations. These findings suggest that although niche, simulation of the
Waterfall Model is present in academic discourse. This work highlights the need
for accessible modeling tools and calls for future research that integrates the
waterfall software process model with modern hybrid practices.

</details>


### [23] [Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees](https://arxiv.org/abs/2506.19677)
*Shi Chang,Boyuan Chen,Kishanthan Thangarajah,Hanan Lutfiyya,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SABER是一种动态批处理策略，通过实时调整决策提高CodeLLM的服务性能，显著提升吞吐量并减少延迟波动。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的自托管环境中高效服务CodeLLM仍具挑战性，现有静态批处理配置无法适应请求波动和异构工作负载。

Method: 提出SABER，动态预测每请求的SLA可行性并实时调整批处理决策。

Result: SABER比最佳静态配置提升吞吐量26%，减少延迟波动45%，无需手动调整或服务重启。

Conclusion: SLA感知的自适应调度是实现高性能CodeLLM服务的关键。

Abstract: Code Large Language Models (CodeLLMs) are increasingly integrated into modern
software development workflows, yet efficiently serving them in
resource-constrained, self-hosted environments remains a significant challenge.
Existing LLM serving systems employs Continuous Batching for throughput
improvement. However, they rely on static batch size configurations that cannot
adapt to fluctuating request rates or heterogeneous workloads, leading to
frequent SLA (Service Level Agreement) violations and unstable performance. In
this study, We propose SABER, a dynamic batching strategy that predicts
per-request SLA feasibility and adjusts decisions in real time. SABER improves
goodput by up to 26% over the best static configurations and reduces latency
variability by up to 45%, all without manual tuning or service restarts. Our
results demonstrate that SLA-aware, adaptive scheduling is key to robust,
high-performance CodeLLM serving.

</details>


### [24] [Exploring Developer Experience Factors in Software Ecosystems](https://arxiv.org/abs/2506.19757)
*Rodrigo Oliveira Zacarias,Léo Carvalho Ramos Antunes,Márcio de Oliveira Barros,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: 该研究通过系统映射研究和德尔菲研究，识别了影响开发者体验（DX）的关键因素，并分析了这些因素对第三方开发者采纳和持续贡献软件生态系统（SECO）的影响。


<details>
  <summary>Details</summary>
Motivation: 开发者体验（DX）对开发者的表现和持续参与软件生态系统（SECO）至关重要，但目前缺乏对其关键因素的清晰路线图。

Method: 采用系统映射研究（SMS）分析29项研究，并通过德尔菲研究评估27个DX因素对21位第三方开发者的影响。

Result: 影响开发者采纳和持续贡献的关键因素包括：平台使用成本、开发所需技术资源、应用市场进入门槛低以及更多财务收益。

Conclusion: 开发者体验对SECO的成功和可持续性至关重要，研究结果为研究人员和实践者提供了有价值的见解。

Abstract: Context: Developer experience (DX) plays a key role in developers'
performance and their continued involvement in a software ecosystem (SECO)
platform. While researchers and practitioners have recognized several factors
affecting DX in SECO platforms, a clear roadmap of the most influential factors
is still missing. This is particularly important given the direct impact on
developers' interest in SECO and their ongoing engagement with the common
technological platform. Goal: This work aims to identify key DX factors and
understand how they influence third-party developers' decisions to adopt and
keep contributing to a SECO. Methods: We conducted a systematic mapping study
(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.
Additionally, we conducted a Delphi study to evaluate the influence of 27 DX
factors (identified in our SMS) from the perspective of 21 third-party
developers to adopt and keep contributing to a SECO. Results: The factors that
most strongly influence developers' adoption and ongoing contributions to a
SECO are: financial costs for using the platform, desired technical resources
for development, low barriers to entry into the applications market, and more
financial gains. Conclusion: DX is essential for the success and sustainability
of SECO. Our set of DX factors provides valuable insights and recommendations
for researchers and practitioners to address key DX concerns from the
perspective of third-party developers.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [WiLLM: An Open Wireless LLM Communication System](https://arxiv.org/abs/2506.19030)
*Boyi Liu,Yongguang Lu,Jianguo Zhao,Qiang Yang,Wen Wu,Lin Chen,Jagmohan Chauhan,Jun Zhang*

Main category: cs.NI

TL;DR: WiLLM是一个开源无线系统，专为移动LLM服务设计，通过分布式推理和创新的网络切片架构优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有无线基础设施难以应对快速发展的LLM需求，需要新的架构支持移动LLM服务。

Method: 在核心网络中部署LLM，提出“Tree-Branch-Fruit”网络切片扩展，并引入双层次切片架构和应用层隧道机制。

Result: WiLLM实现了更细粒度的资源分配和兼容性，发布了首个LLM无线通信数据集和基准测试。

Conclusion: WiLLM为跨层优化和AI-电信融合提供了开放平台，推动了移动LLM服务的实际应用。

Abstract: The rapid evolution of LLMs threatens to overwhelm existing wireless
infrastructure, necessitating architectural innovations for burgeoning mobile
LLM services. This paper introduces WiLLM, the first open-source wireless
system specifically designed for these services. First, we establish a new
paradigm by deploying LLMs in core networks (CNs) with abundant GPUs. This
enables distributed inference services, strategically positioning LLM inference
at the convergence of backbone bandwidth and the cellular network's edge.
Second, we propose an innovative "Tree-Branch-Fruit" extension to the
conventional network slicing architecture. This specialized design allows
telecom operators to monetize LLM services through slice subscriptions while
maintaining infrastructure ownership. Finally, to realize this vision, WiLLM
addresses critical limitations in current solutions with several novel
capabilities. It features enhanced slice orchestration through a dual-layer
slicing architecture, enabling coordinated multi-UE-multi-slice scheduling for
finer-grained resource allocation. To ensure universal compatibility, an
application-layer tunneling mechanism allows legacy devices without native
slicing to access LLM slice services without hardware upgrades. Furthermore,
its dual-mode scheduling and cross-layer APIs support flexible deployment from
CNs to servers. Built on OpenAirInterface, WiLLM extends this established
framework, lowering the adoption barrier for researchers. We also release the
first LLM wireless communication dataset with 1,649,996 records and
synchronized 58-dimensional metrics, alongside two benchmarks. A case study
with smart glasses demonstrates practical viability for resource-constrained
devices. WiLLM aims to foster an open platform for cross-layer optimization and
AI-telecom convergence. The code, datasets, and hardware details are available
at https://openwillm.github.io.

</details>


### [26] [Enhancing Evacuation Safety: Detecting Post-Nuclear Event Radiation Levels in an Urban Area](https://arxiv.org/abs/2506.19044)
*Ellis Duncalfe,Milena Radenkovic*

Main category: cs.NI

TL;DR: 该研究设计了一个模拟核爆炸后场景的动态网络环境，评估了两种延迟容忍网络（DTN）路由协议在辐射数据传输中的表现，证明了其在应急响应中的潜力。


<details>
  <summary>Details</summary>
Motivation: 核爆炸后通信基础设施可能瘫痪，及时传播辐射数据对减少伤亡至关重要。

Method: 使用ONE模拟器构建动态场景，整合应急响应者、无人机和民用设备的辐射数据，评估Epidemic和PRoPHET两种DTN路由协议的性能。

Result: 两种协议均实现高消息传递率，PRoPHET网络开销较低但延迟较高。

Conclusion: DTN解决方案能在基础设施严重损坏时确保关键辐射数据的传播，支持应急响应和疏散安全。

Abstract: The detonation of an improvised nuclear device (IND) in an urban area would
cause catastrophic damage, followed by hazardous radioactive fallout. Timely
dissemination of radiation data is crucial for evacuation and casualty
reduction. However, conventional communication infrastructure is likely to be
severely disrupted. This study designs and builds a pseudorealistic,
geospatially and temporally dynamic post-nuclear event (PNE) scenario using the
Opportunistic Network Environment (ONE) simulator. It integrates radiation
sensing by emergency responders, unmanned aerial vehicles (UAVs), and civilian
devices as dynamic nodes within Delay-Tolerant Networks (DTNs). The performance
of two DTN routing protocols, Epidemic and PRoPHET, was evaluated across
multiple PNE phases. Both protocols achieve high message delivery rates, with
PRoPHET exhibiting lower network overhead but higher latency. Findings
demonstrate the potential of DTN-based solutions to support emergency response
and evacuation safety by ensuring critical radiation data propagation despite
severe infrastructure damage.

</details>


### [27] [A Study on E2E Performance Improvement of Platooning Using Outdoor LiFi](https://arxiv.org/abs/2506.19304)
*Zhiyi Zhu,Eiji Takimoto,Patrick Finnerty,Chikara Ohta*

Main category: cs.NI

TL;DR: 论文提出了一种结合LiFi和C-V2X的混合通信架构，以减少自动驾驶车队中的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 随着车队长度增加，传统C-V2X架构因多跳通信导致延迟增加，影响效率。

Method: 提出混合架构，引入多辆配备LiFi通信节点的领头车，实现高速低延迟通信。

Result: 模拟显示该架构能有效降低端到端延迟。

Conclusion: 混合架构为解决车队通信延迟问题提供了可行方案。

Abstract: Platooning within autonomous vehicles has proven effective in addressing
driver shortages and reducing fuel consumption. However, as platooning lengths
increase, traditional C-V2X (cellular vehicle-to-everything) architectures are
susceptible to end-to-end (E2E) latency increases. This is due to the necessity
of relaying information through multiple hops from the leader vehicle to the
last vehicle. To address this problem, this paper proposes a hybrid
communication architecture based on a simulation that integrates light fidelity
(LiFi) and C-V2X. The proposed architecture introduces multiple-leader vehicles
equipped with outdoor LiFi communication nodes in platoons to achieve
high-speed and low-delay communication between leader vehicles, which reduces
E2E delay.

</details>


### [28] [Fractality of Wireless Mesh Networks: Dimensional Effects on Network Performance](https://arxiv.org/abs/2506.19366)
*Marat Zaidyn,Sayat Akhtanov,Dana Turlykozhayeva,Symbat Temesheva,Almat Akhmetali,Alisher Skabylov,Nurzhan Ussipov*

Main category: cs.NI

TL;DR: 提出了一种基于可调分形维度的无线网状网络拓扑构建算法，通过模拟验证了高分形维度拓扑在性能和弹性上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统模型假设节点均匀或随机分布，无法反映实际部署中的复杂空间模式，因此需要更精确的拓扑控制方法。

Method: 开发了一种算法，通过调整分形维度生成不同空间复杂度的网络布局，并通过NS-3模拟评估性能指标。

Result: 高分形维度拓扑在吞吐量、延迟、抖动和包传递率上表现优于传统随机、小世界和无标度网络模型。

Conclusion: 分形几何可作为设计高效、可扩展无线网状网络架构的新范式。

Abstract: Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,
which directly influences connectivity, routing efficiency, and overall network
performance. Conventional models typically assume uniform or random node
placement, which inadequately represent the complex, hierarchical spatial
patterns observed in practical deployments. In this study, we present a novel
algorithm that constructs WMN topologies with tunable fractal dimensions,
allowing precise control over spatial self-similarity. By systematically
varying the fractal dimension, the algorithm generates network layouts spanning
a continuum of spatial complexities, ranging from sparse fragmented clusters to
dense, cohesive structures. Through NS-3 simulations, Key performance metrics
including throughput, latency, jitter, and packet delivery ratio were evaluated
across a range of fractal dimensions. Comparative evaluations against classical
random, small-world, and scale-free network models reveal that high-dimensional
fractal topologies achieve enhanced resilience and throughput under equivalent
conditions. These findings demonstrate the potential of fractal geometry as a
design paradigm for scalable and efficient WMN architectures.

</details>


### [29] [CORMO-RAN: Lossless Migration of xApps in O-RAN](https://arxiv.org/abs/2506.19760)
*Antonio Calagna,Stefano Maxenti,Leonardo Bonati,Salvatore D'Oro,Tommaso Melodia,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: CORMO-RAN 是一种数据驱动的编排器，通过动态激活计算节点和迁移 xApps 来节省能源，同时确保服务可用性。


<details>
  <summary>Details</summary>
Motivation: 在低流量期间，计算资源可能未被充分利用，而现有方法无法高效管理 xApps 的迁移和能源消耗。

Method: CORMO-RAN 动态激活计算节点，支持无状态丢失的 xApp 迁移，并考虑 xApp 的多样性和时间约束。

Result: 实验表明，CORMO-RAN 能节省高达 64% 的能源消耗。

Conclusion: CORMO-RAN 在服务可用性、可扩展性和能源消耗之间取得了平衡，是高效的 RAN 编排解决方案。

Abstract: Open Radio Access Network (RAN) is a key paradigm to attain unprecedented
flexibility of the RAN via disaggregation and Artificial Intelligence
(AI)-based applications called xApps. In dense areas with many active RAN
nodes, compute resources are engineered to support potentially hundreds of
xApps monitoring and controlling the RAN to achieve operator's intents.
However, such resources might become underutilized during low-traffic periods,
where most cells are sleeping and, given the reduced RAN complexity, only a few
xApps are needed for its control. In this paper, we propose CORMO-RAN, a
data-driven orchestrator that dynamically activates compute nodes based on xApp
load to save energy, and performs lossless migration of xApps from nodes to be
turned off to active ones while ensuring xApp availability during migration.
CORMO-RAN tackles the trade-off among service availability, scalability, and
energy consumption while (i) preserving xApps' internal state to prevent RAN
performance degradation during migration; (ii) accounting for xApp diversity in
state size and timing constraints; and (iii) implementing several migration
strategies and providing guidelines on best strategies to use based on resource
availability and requirements. We prototype CORMO-RAN as an rApp, and
experimentally evaluate it on an O-RAN private 5G testbed hosted on a Red Hat
OpenShift cluster with commercial radio units. Results demonstrate that
CORMO-RAN is effective in minimizing energy consumption of the RAN Intelligent
Controller (RIC) cluster, yielding up to 64% energy saving when compared to
existing approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration](https://arxiv.org/abs/2506.18916)
*Ganesh Parab,Zishan Ahmad,Dagnachew Birru*

Main category: cs.LG

TL;DR: HI-SQL提出了一种基于历史查询日志的提示生成机制，用于改进Text-to-SQL生成，显著提高了复杂查询的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在处理多表连接、嵌套条件等复杂查询时存在高计算成本、延迟和错误传播问题，需要更高效的解决方案。

Method: HI-SQL通过分析历史查询日志生成上下文提示，指导SQL生成，避免多步流水线的高成本和错误。

Result: 实验表明，HI-SQL显著提高了LLM生成查询的准确性，同时减少了LLM调用和延迟。

Conclusion: HI-SQL为增强Text-to-SQL系统提供了一种高效且实用的解决方案。

Abstract: Text-to-SQL generation bridges the gap between natural language and
databases, enabling users to query data without requiring SQL expertise. While
large language models (LLMs) have significantly advanced the field, challenges
remain in handling complex queries that involve multi-table joins, nested
conditions, and intricate operations. Existing methods often rely on multi-step
pipelines that incur high computational costs, increase latency, and are prone
to error propagation. To address these limitations, we propose HI-SQL, a
pipeline that incorporates a novel hint generation mechanism utilizing
historical query logs to guide SQL generation. By analyzing prior queries, our
method generates contextual hints that focus on handling the complexities of
multi-table and nested operations. These hints are seamlessly integrated into
the SQL generation process, eliminating the need for costly multi-step
approaches and reducing reliance on human-crafted prompts. Experimental
evaluations on multiple benchmark datasets demonstrate that our approach
significantly improves query accuracy of LLM-generated queries while ensuring
efficiency in terms of LLM calls and latency, offering a robust and practical
solution for enhancing Text-to-SQL systems.

</details>


### [31] [From Tiny Machine Learning to Tiny Deep Learning: A Survey](https://arxiv.org/abs/2506.18927)
*Shriyank Somvanshi,Md Monzurul Islam,Gaurab Chhetri,Rohit Chakraborty,Mahmuda Sultana Mimi,Swagat Ahmed Shuvo,Kazi Sifatul Islam,Syed Aaqib Javed,Sharif Ahmed Rafat,Anandi Dutta,Subasish Das*

Main category: cs.LG

TL;DR: 本文综述了从TinyML到TinyDL的演变，重点介绍了架构创新、硬件平台、模型优化技术和软件工具链，并探讨了未来的发展方向。


<details>
  <summary>Details</summary>
Motivation: 边缘设备的快速增长推动了边缘人工智能的需求，促使TinyML和TinyDL的发展，本文旨在全面概述这一领域的进展。

Method: 通过分析量化、剪枝、神经架构搜索（NAS）等技术，以及硬件和软件工具链的现状，综述了TinyDL的实现方法。

Result: 总结了TinyDL在计算机视觉、音频识别、医疗和工业监测等领域的应用，并提出了未来的研究方向。

Conclusion: 本文为研究者和从业者提供了TinyDL生态系统的全面视角，为边缘AI的未来发展奠定了基础。

Abstract: The rapid growth of edge devices has driven the demand for deploying
artificial intelligence (AI) at the edge, giving rise to Tiny Machine Learning
(TinyML) and its evolving counterpart, Tiny Deep Learning (TinyDL). While
TinyML initially focused on enabling simple inference tasks on
microcontrollers, the emergence of TinyDL marks a paradigm shift toward
deploying deep learning models on severely resource-constrained hardware. This
survey presents a comprehensive overview of the transition from TinyML to
TinyDL, encompassing architectural innovations, hardware platforms, model
optimization techniques, and software toolchains. We analyze state-of-the-art
methods in quantization, pruning, and neural architecture search (NAS), and
examine hardware trends from MCUs to dedicated neural accelerators.
Furthermore, we categorize software deployment frameworks, compilers, and
AutoML tools enabling practical on-device learning. Applications across domains
such as computer vision, audio recognition, healthcare, and industrial
monitoring are reviewed to illustrate the real-world impact of TinyDL. Finally,
we identify emerging directions including neuromorphic computing, federated
TinyDL, edge-native foundation models, and domain-specific co-design
approaches. This survey aims to serve as a foundational resource for
researchers and practitioners, offering a holistic view of the ecosystem and
laying the groundwork for future advancements in edge AI.

</details>


### [32] [Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs](https://arxiv.org/abs/2506.18931)
*Shuang Ao,Yi Dong,Jinwei Hu,Sarvapali Ramchurn*

Main category: cs.LG

TL;DR: 论文提出SPLoRA方法，通过选择性修剪LoRA层来增强安全性，同时保持性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLMs）可能损害安全性对齐，现有方法难以平衡安全性与性能。

Method: 提出SPLoRA，结合E-DIEM度量，选择性修剪LoRA层以优化安全性与性能。

Result: 实验表明SPLoRA显著降低安全风险，保持或提升性能，并减少推理开销。

Conclusion: SPLoRA是一种高效、可扩展的方案，适用于部署更安全可靠的LLMs。

Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA)
enhances adaptability while reducing computational costs. However, fine-tuning
can compromise safety alignment, even with benign data, increasing
susceptibility to harmful outputs. Existing safety alignment methods struggle
to capture complex parameter shifts, leading to suboptimal safety-utility
trade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a
novel pruning-based approach that selectively removes LoRA layers that weaken
safety alignment, improving safety while preserving performance. At its core,
we introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric
that effectively detects safety misalignment in LoRA-adapted models. We conduct
extensive experiments on LLMs fine-tuned with mixed of benign and malicious
data, and purely benign datasets, evaluating SPLoRA across utility, safety, and
reliability metrics. Results demonstrate that SPLoRA outperforms
state-of-the-art safety alignment techniques, significantly reducing safety
risks while maintaining or improving model performance and reliability.
Additionally, SPLoRA reduces inference overhead, making it a scalable and
efficient solution for deploying safer and more reliable LLMs. The code is
available at https://github.com/AoShuang92/SPLoRA.

</details>


### [33] [Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models](https://arxiv.org/abs/2506.18945)
*Zihan Wang,Rui Pan,Jiarui Yao,Robert Csordas,Linjie Li,Lu Yin,Jiajun Wu,Tong Zhang,Manling Li,Shiwei Liu*

Main category: cs.LG

TL;DR: Chain-of-Experts (CoE) 是一种新的 Mixture-of-Experts (MoE) 架构，通过层内专家顺序通信提升模型表现和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统 MoE 模型中专家独立并行操作，缺乏动态交互，限制了模型的表达能力。CoE 旨在通过迭代专家选择和动态路由机制解决这一问题。

Method: CoE 在每层内引入专家链式迭代处理，每个迭代步骤配备专用路由器，支持动态专家选择和重新评估。

Result: CoE 在固定计算量下表现更优（如数学推理任务验证损失从 1.20 降至 1.12），并通过迭代深度扩展减少内存使用（17.6-42%）。

Conclusion: CoE 通过迭代残差结构和动态路由机制提升了模型表达能力，为模型扩展提供了新维度。

Abstract: We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)
architecture that introduces sequential expert communication within each layer.
Unlike traditional MoE models, where experts operate independently in parallel,
CoE processes tokens iteratively across a chain of experts inside a layer. To
support dynamic expert selection across iterations, CoE employs a dedicated
router at each iteration step within a layer. This design allows tokens to
re-evaluate and select different experts during each iteration, rather than
being statically assigned. As a result, CoE introduces a flexible routing
mechanism that increases the diversity of expert combinations and enriches the
model's representational capacity. CoE demonstrates improved performance under
fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to
1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling
axis: depth through expert iteration, which complements conventional
width/depth scaling. For example, using 2x iterations matches the performance
of 3x expert selections (in width), while reducing memory usage by 17.6-42%
relative to other scaling strategies. Our analysis reveals that CoE's benefits
stem from its iterative residual structure and enhanced expert specialization
empowered by iterative routing, which together unlock more expressive
representations. Code is available at https://github.com/ZihanWang314/coe.

</details>


### [34] [Online high-precision prediction method for injection molding product weight by integrating time series/non-time series mixed features and feature attention mechanism](https://arxiv.org/abs/2506.18950)
*Maoyuan Li,Sihong Li,Guancheng Shen,Yun Zhang,Huamin Zhou*

Main category: cs.LG

TL;DR: 提出了一种混合特征注意力-人工神经网络（MFA-ANN）模型，用于高精度在线预测注塑产品重量，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决注塑质量异常检测不及时和在线监控滞后的问题。

Method: 结合机理分析和数据驱动方法，通过自注意力机制动态校准跨模态特征权重。

Result: MFA-ANN模型的RMSE为0.0281，准确率提升25.1%至25.7%不等。

Conclusion: 该模型为注塑过程的智能质量控制提供了高效可靠的解决方案。

Abstract: To address the challenges of untimely detection and online monitoring lag in
injection molding quality anomalies, this study proposes a mixed feature
attention-artificial neural network (MFA-ANN) model for high-precision online
prediction of product weight. By integrating mechanism-based with data-driven
analysis, the proposed architecture decouples time series data (e.g., melt flow
dynamics, thermal profiles) from non-time series data (e.g., mold features,
pressure settings), enabling hierarchical feature extraction. A self-attention
mechanism is strategically embedded during cross-domain feature fusion to
dynamically calibrate inter-modality feature weights, thereby emphasizing
critical determinants of weight variability. The results demonstrate that the
MFA-ANN model achieves a RMSE of 0.0281 with 0.5 g weight fluctuation
tolerance, outperforming conventional benchmarks: a 25.1% accuracy improvement
over non-time series ANN models, 23.0% over LSTM networks, 25.7% over SVR, and
15.6% over RF models, respectively. Ablation studies quantitatively validate
the synergistic enhancement derived from the integration of mixed feature
modeling (contributing 22.4%) and the attention mechanism (contributing 11.2%),
significantly enhancing the model's adaptability to varying working conditions
and its resistance to noise. Moreover, critical sensitivity analyses further
reveal that data resolution significantly impacts prediction reliability,
low-fidelity sensor inputs degrade performance by 23.8% RMSE compared to
high-precision measurements. Overall, this study provides an efficient and
reliable solution for the intelligent quality control of injection molding
processes.

</details>


### [35] [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)
*Zohaib Hasan Siddiqui,Jiechao Gao,Ebad Shabbir,Mohammad Anas Azeez,Rafiq Ali,Gautam Siddharth Kashyap,Usman Naseem*

Main category: cs.LG

TL;DR: HOLA框架通过分层推测解码和自适应检索优化，显著提升边缘设备上大语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行大语言模型的高计算和内存需求限制了实时应用，现有方法如量化和剪枝效果有限。

Method: HOLA结合分层推测解码（HSD）、自适应检索（AdaComp-RAG）和混合剪枝量化（LoBi）进行端到端优化。

Result: 在GSM8K和ARC任务上分别提升17.6%和10.5%，同时降低延迟和内存占用。

Conclusion: HOLA在边缘设备上高效且可扩展，适合实际部署。

Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high
compute and memory demands posing a barrier for real-time applications in
sectors like healthcare, education, and embedded systems. Current solutions
such as quantization, pruning, and retrieval-augmented generation (RAG) offer
only partial optimizations and often compromise on speed or accuracy. We
introduce HOLA, an end-to-end optimization framework for efficient LLM
deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)
for faster inference without quality loss. Externally, AdaComp-RAG adjusts
retrieval complexity based on context needs. Together with LoBi, which blends
structured pruning (LoRA) and quantization, HOLA delivers significant gains:
17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge
devices like Jetson Nano--proving both scalable and production-ready.

</details>


### [36] [Automating Traffic Monitoring with SHM Sensor Networks via Vision-Supervised Deep Learning](https://arxiv.org/abs/2506.19023)
*Hanshuo Wu,Xudong Jian,Christos Lataniotis,Cyprien Hoelzl,Eleni Chatzi,Yves Reuland*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的自动化交通监测方法，结合计算机视觉和结构健康监测传感器网络，解决了传统方法的隐私和光照问题，实现了高精度分类。


<details>
  <summary>Details</summary>
Motivation: 桥梁作为关键基础设施，其退化问题日益严重，需要可靠的交通监测方法评估剩余使用寿命。传统计算机视觉方法存在隐私和光照限制，非视觉方法则缺乏灵活性。

Method: 提出了一种深度学习框架，结合计算机视觉辅助生成高分辨率数据集，利用图神经网络（GNN）捕捉传感器数据的空间结构和相互依赖性。

Result: 在真实案例中，模型对轻型车辆和重型车辆的分类准确率分别达到99%和94%，性能优于现有方法。

Conclusion: 该方法通过融合计算机视觉和传感器网络，实现了高精度、自动化的交通监测，为桥梁健康评估提供了新思路。

Abstract: Bridges, as critical components of civil infrastructure, are increasingly
affected by deterioration, making reliable traffic monitoring essential for
assessing their remaining service life. Among operational loads, traffic load
plays a pivotal role, and recent advances in deep learning - particularly in
computer vision (CV) - have enabled progress toward continuous, automated
monitoring. However, CV-based approaches suffer from limitations, including
privacy concerns and sensitivity to lighting conditions, while traditional
non-vision-based methods often lack flexibility in deployment and validation.
To bridge this gap, we propose a fully automated deep-learning pipeline for
continuous traffic monitoring using structural health monitoring (SHM) sensor
networks. Our approach integrates CV-assisted high-resolution dataset
generation with supervised training and inference, leveraging graph neural
networks (GNNs) to capture the spatial structure and interdependence of sensor
data. By transferring knowledge from CV outputs to SHM sensors, the proposed
framework enables sensor networks to achieve comparable accuracy of
vision-based systems, with minimal human intervention. Applied to accelerometer
and strain gauge data in a real-world case study, the model achieves
state-of-the-art performance, with classification accuracies of 99% for light
vehicles and 94% for heavy vehicles.

</details>


### [37] [Failure Modes of Time Series Interpretability Algorithms for Critical Care Applications and Potential Solutions](https://arxiv.org/abs/2506.19035)
*Shashank Yadav,Vignesh Subbian*

Main category: cs.LG

TL;DR: 论文提出可学习掩码框架，用于动态时间序列预测问题，以解决传统解释性方法在关键护理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 在关键护理中，深度学习模型的解释性对患者生存至关重要，但传统方法在动态预测任务中表现不佳。

Method: 系统分析传统方法的失败模式，提出可学习掩码框架，结合时间连续性和标签一致性约束。

Result: 可学习掩码方法在动态时间序列预测中提供更可靠和一致的解释。

Conclusion: 可学习掩码框架是解决关键护理中动态预测解释性问题的有效替代方案。

Abstract: Interpretability plays a vital role in aligning and deploying deep learning
models in critical care, especially in constantly evolving conditions that
influence patient survival. However, common interpretability algorithms face
unique challenges when applied to dynamic prediction tasks, where patient
trajectories evolve over time. Gradient, Occlusion, and Permutation-based
methods often struggle with time-varying target dependency and temporal
smoothness. This work systematically analyzes these failure modes and supports
learnable mask-based interpretability frameworks as alternatives, which can
incorporate temporal continuity and label consistency constraints to learn
feature importance over time. Here, we propose that learnable mask-based
approaches for dynamic timeseries prediction problems provide more reliable and
consistent interpretations for applications in critical care and similar
domains.

</details>


### [38] [FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation](https://arxiv.org/abs/2506.19082)
*Nitish Nagesh,Ziyu Wang,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 提出了一种基于LLM增强的合成数据生成方法，专注于提升健康数据中的因果公平性，显著减少敏感属性偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注反事实公平性，且多应用于金融和法律领域，健康领域缺乏因果公平性的合成数据生成方法。

Method: 开发了首个基于LLM增强的合成数据生成方法，利用真实世界表格健康数据，确保因果结构保留。

Result: 生成数据在因果公平性指标上偏离真实数据小于10%，使用因果公平预测器时，敏感属性偏差减少70%。

Conclusion: 该方法为健康研究和医疗提供了公平的合成数据，支持更公平的医疗结果。

Abstract: Synthetic data generation creates data based on real-world data using
generative models. In health applications, generating high-quality data while
maintaining fairness for sensitive attributes is essential for equitable
outcomes. Existing GAN-based and LLM-based methods focus on counterfactual
fairness and are primarily applied in finance and legal domains. Causal
fairness provides a more comprehensive evaluation framework by preserving
causal structure, but current synthetic data generation methods do not address
it in health settings. To fill this gap, we develop the first LLM-augmented
synthetic data generation method to enhance causal fairness using real-world
tabular health data. Our generated data deviates by less than 10% from real
data on causal fairness metrics. When trained on causally fair predictors,
synthetic data reduces bias on the sensitive attribute by 70% compared to real
data. This work improves access to fair synthetic data, supporting equitable
health research and healthcare delivery.

</details>


### [39] [GradualDiff-Fed: A Federated Learning Specialized Framework for Large Language Model](https://arxiv.org/abs/2506.19164)
*Amir Faiyaz,Tara Salman*

Main category: cs.LG

TL;DR: GradualDiff-Fed是一种针对大语言模型（LLMs）的联邦学习框架，通过仅传输模型权重差异而非完整模型，显著降低了通信成本，同时保持了与集中式训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，如何在保护隐私的分布式环境中高效微调这些模型成为挑战。联邦学习（FL）虽然提供了一种隐私保护的方法，但在处理大模型时面临性能和通信效率的问题。

Method: 提出了GradualDiff-Fed框架，通过仅传输模型权重的差异来减少通信开销，从而高效地微调大语言模型。

Result: 实验表明，GradualDiff-Fed在保持性能的同时，显著降低了通信成本，与集中式训练效果相当。

Conclusion: GradualDiff-Fed是一种高效且隐私保护的解决方案，适用于分布式数据环境下的大语言模型微调。

Abstract: The rapid proliferation of large language models (LLMs) has created an
unprecedented demand for fine-tuning models for specialized domains, such as
medical science. While federated learning (FL) offers a decentralized and
privacy-preserving approach to collaboratively fine-tune LLMs without sharing
raw data, it presents significant challenges, particularly in performance and
managing large model sizes efficiently. In this paper, we introduce
GradualDiff-Fed, an FL framework designed explicitly for LLMs, and their
challenge of handling the high parameter size. GradualDiff-Fed reduces
communication costs by transmitting only the difference of model weights rather
than the entire model during training rounds. Such an approach significantly
improves scalability and communication efficiency, making it more feasible to
fine-tune LLMs across distributed clients without compromising performance. Our
evaluation demonstrates that GradualDiff-Fed achieves performance on par with
centralized training while drastically reducing communication overhead. These
results highlight the potential of GradualDiff-Fed as an efficient solution for
fine-tuning large models from distributed data in privacy-preserving settings
without comprising performance.

</details>


### [40] [Benchmarking Music Generation Models and Metrics via Human Preference Studies](https://arxiv.org/abs/2506.19085)
*Florian Grötschla,Ahmet Solak,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 研究通过大规模人类偏好调查评估了12种音乐生成模型，并比较了主观与客观指标的相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成音乐接近人类创作，但如何将主观质量评估转化为客观指标仍具挑战性。

Method: 生成6k首歌曲，通过15k对音频比较和2.5k参与者调查，评估人类偏好与常用指标的相关性。

Result: 首次基于人类偏好对当前音乐生成模型和指标进行排名。

Conclusion: 为推进主观指标评估领域，公开了生成音乐和人类评估数据集。

Abstract: Recent advancements have brought generated music closer to human-created
compositions, yet evaluating these models remains challenging. While human
preference is the gold standard for assessing quality, translating these
subjective judgments into objective metrics, particularly for text-audio
alignment and music quality, has proven difficult. In this work, we generate 6k
songs using 12 state-of-the-art models and conduct a survey of 15k pairwise
audio comparisons with 2.5k human participants to evaluate the correlation
between human preferences and widely used metrics. To the best of our
knowledge, this work is the first to rank current state-of-the-art music
generation models and metrics based on human preference. To further the field
of subjective metric evaluation, we provide open access to our dataset of
generated music and human evaluations.

</details>


### [41] [Finetuning a Weather Foundation Model with Lightweight Decoders for Unseen Physical Processes](https://arxiv.org/abs/2506.19088)
*Fanny Lehmann,Firat Ozdemir,Benedikt Soja,Torsten Hoefler,Siddhartha Mishra,Sebastian Schemm*

Main category: cs.LG

TL;DR: Aurora基础模型通过轻量级解码器预测未在预训练中考虑的水文变量，比全模型微调更高效且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估Aurora基础模型在预测水文变量中的表现，探索无需全模型微调的扩展方法。

Method: 使用浅层解码器训练预训练模型的潜在表示预测新变量，并与全模型微调对比。

Result: 解码器方法减少50%训练时间和35%内存，保持高准确性并保留模型稳定性。

Conclusion: 基础模型在无需全微调下扩展新变量的能力是地球科学中的重要质量指标。

Abstract: Recent advances in AI weather forecasting have led to the emergence of
so-called "foundation models", typically defined by expensive pretraining and
minimal fine-tuning for downstream tasks. However, in the natural sciences, a
desirable foundation model should also encode meaningful statistical
relationships between the underlying physical variables. This study evaluates
the performance of the state-of-the-art Aurora foundation model in predicting
hydrological variables, which were not considered during pretraining. We
introduce a lightweight approach using shallow decoders trained on the latent
representations of the pretrained model to predict these new variables. As a
baseline, we compare this to fine-tuning the full model, which allows further
optimization of the latent space while incorporating new variables into both
inputs and outputs. The decoder-based approach requires 50% less training time
and 35% less memory, while achieving strong accuracy across various
hydrological variables and preserving desirable properties of the foundation
model, such as autoregressive stability. Notably, decoder accuracy depends on
the physical correlation between the new variables and those used during
pretraining, indicating that Aurora's latent space captures meaningful physical
relationships. In this sense, we argue that an important quality metric for
foundation models in Earth sciences is their ability to be extended to new
variables without a full fine-tuning. This provides a new perspective for
making foundation models more accessible to communities with limited
computational resources, while supporting broader adoption in Earth sciences.

</details>


### [42] [Private Model Personalization Revisited](https://arxiv.org/abs/2506.19220)
*Conor Snedeker,Xinyu Zhou,Raef Bassily*

Main category: cs.LG

TL;DR: 本文研究了在用户级差分隐私（DP）下的模型个性化问题，提出了一种高效的联邦学习算法，用于在异构数据中恢复共享嵌入和本地低维表示，并在噪声标签和更广泛的用户分布下提供隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决在联邦学习中如何保护用户隐私的同时，从异构数据中学习共享嵌入和本地表示的问题。

Method: 基于FedRep算法提出了一种满足差分隐私的联邦学习算法，并利用Johnson-Lindenstrauss变换降低维度。

Result: 在子高斯分布下提供了隐私保护，并在自然参数范围内改进了隐私误差项。在二元分类中，实现了与维度无关的风险界限。

Conclusion: 该方法在隐私保护和模型性能之间取得了平衡，适用于更广泛的数据分布和实际应用场景。

Abstract: We study model personalization under user-level differential privacy (DP) in
the shared representation framework. In this problem, there are $n$ users whose
data is statistically heterogeneous, and their optimal parameters share an
unknown embedding $U^* \in\mathbb{R}^{d\times k}$ that maps the user parameters
in $\mathbb{R}^d$ to low-dimensional representations in $\mathbb{R}^k$, where
$k\ll d$. Our goal is to privately recover the shared embedding and the local
low-dimensional representations with small excess risk in the federated
setting. We propose a private, efficient federated learning algorithm to learn
the shared embedding based on the FedRep algorithm in [CHM+21]. Unlike
[CHM+21], our algorithm satisfies differential privacy, and our results hold
for the case of noisy labels. In contrast to prior work on private model
personalization [JRS+21], our utility guarantees hold under a larger class of
users' distributions (sub-Gaussian instead of Gaussian distributions).
Additionally, in natural parameter regimes, we improve the privacy error term
in [JRS+21] by a factor of $\widetilde{O}(dk)$. Next, we consider the binary
classification setting. We present an information-theoretic construction to
privately learn the shared embedding and derive a margin-based accuracy
guarantee that is independent of $d$. Our method utilizes the
Johnson-Lindenstrauss transform to reduce the effective dimensions of the
shared embedding and the users' data. This result shows that
dimension-independent risk bounds are possible in this setting under a margin
loss.

</details>


### [43] [On the algorithmic construction of deep ReLU networks](https://arxiv.org/abs/2506.19104)
*Daan Huybrechs*

Main category: cs.LG

TL;DR: 论文探讨了神经网络在数学上的表达能力，特别是ReLU激活函数的神经网络如何表示连续分段线性函数，并通过构造性方法展示了神经网络作为算法的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在数学上能够表示什么，以及如何通过构造性方法而非数据训练来理解其表达能力。

Method: 采用构造性方法，将神经网络视为算法，通过具体例子（如排序算法）展示其能力，并分析递归和并行性。

Result: 构造了能够精确排序的神经网络，并发现神经网络作为算法通常具有递归和并行性，且深度网络优于浅层网络。

Conclusion: 神经网络作为算法具有独特的表达能力，但其连续性和网络深度限制了其递归能力，深度网络表现更优。

Abstract: It is difficult to describe in mathematical terms what a neural network
trained on data represents. On the other hand, there is a growing mathematical
understanding of what neural networks are in principle capable of representing.
Feedforward neural networks using the ReLU activation function represent
continuous and piecewise linear functions and can approximate many others. The
study of their expressivity addresses the question: which ones? Contributing to
the available answers, we take the perspective of a neural network as an
algorithm. In this analogy, a neural network is programmed constructively,
rather than trained from data. An interesting example is a sorting algorithm:
we explicitly construct a neural network that sorts its inputs exactly, not
approximately, and that, in a sense, has optimal computational complexity if
the input dimension is large. Such constructed networks may have several
billion parameters. We construct and analyze several other examples, both
existing and new. We find that, in these examples, neural networks as
algorithms are typically recursive and parallel. Compared to conventional
algorithms, ReLU networks are restricted by having to be continuous. Moreover,
the depth of recursion is limited by the depth of the network, with deep
networks having superior properties over shallow ones.

</details>


### [44] [Finding Clustering Algorithms in the Transformer Architecture](https://arxiv.org/abs/2506.19125)
*Kenneth L. Clarkson,Lior Horesh,Takuya Ito,Charlotte Park,Parikshit Ram*

Main category: cs.LG

TL;DR: 论文证明Transformer可以精确实现Lloyd的k-means聚类算法，并展示了其神经实现及变体。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer是否能学习和实现精确算法，尤其是k-means聚类算法。

Method: 理论证明并数值实现了一个名为k-means transformer的架构，使用注意力和残差连接精确实现Lloyd算法。

Result: 实验验证了该架构与Lloyd算法的精确对应，并展示了通过修改架构生成的新聚类算法变体。

Conclusion: Transformer机制可以精确映射到算法过程，为在Transformer中实现精确算法提供了清晰视角。

Abstract: The invention of the transformer architecture has revolutionized Artificial
Intelligence (AI), yielding unprecedented success in areas such as natural
language processing, computer vision, and multimodal reasoning. Despite these
advances, it is unclear whether transformers are able to learn and implement
precise algorithms. Here, we demonstrate that transformers can exactly
implement a fundamental and widely used algorithm for $k$-means clustering:
Lloyd's algorithm. First, we theoretically prove the existence of such a
transformer architecture, which we term the $k$-means transformer, that exactly
implements Lloyd's algorithm for $k$-means clustering using the standard
ingredients of modern transformers: attention and residual connections. Next,
we numerically implement this transformer and demonstrate in experiments the
exact correspondence between our architecture and Lloyd's algorithm, providing
a fully neural implementation of $k$-means clustering. Finally, we demonstrate
that interpretable alterations (e.g., incorporating layer normalizations or
multilayer perceptrons) to this architecture yields diverse and novel variants
of clustering algorithms, such as soft $k$-means, spherical $k$-means, trimmed
$k$-means, and more. Collectively, our findings demonstrate how transformer
mechanisms can precisely map onto algorithmic procedures, offering a clear and
interpretable perspective on implementing precise algorithms in transformers.

</details>


### [45] [Riemannian generative decoder](https://arxiv.org/abs/2506.19133)
*Andreas Bjerregaard,Søren Hauberg,Anders Krogh*

Main category: cs.LG

TL;DR: 提出了一种无需编码器的黎曼生成解码器方法，简化了流形约束，适用于多种流形，并在多个案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统黎曼表示学习方法依赖于近似密度，优化目标复杂且可能损害模型性能，因此需要一种更简单的方法。

Method: 引入黎曼生成解码器，通过黎曼优化器找到流形值的最大似然潜在变量，并训练解码器网络，摒弃编码器。

Result: 在合成扩散过程、人类迁移和细胞分裂周期三个案例中，学习到的表示能够保持几何结构并捕捉非欧几里得特征。

Conclusion: 该方法仅需解码器，兼容现有架构，并能生成与数据几何对齐的可解释潜在空间。

Abstract: Riemannian representation learning typically relies on approximating
densities on chosen manifolds. This involves optimizing difficult objectives,
potentially harming models. To completely circumvent this issue, we introduce
the Riemannian generative decoder which finds manifold-valued maximum
likelihood latents with a Riemannian optimizer while training a decoder
network. By discarding the encoder, we vastly simplify the manifold constraint
compared to current approaches which often only handle few specific manifolds.
We validate our approach on three case studies -- a synthetic branching
diffusion process, human migrations inferred from mitochondrial DNA, and cells
undergoing a cell division cycle -- each showing that learned representations
respect the prescribed geometry and capture intrinsic non-Euclidean structure.
Our method requires only a decoder, is compatible with existing architectures,
and yields interpretable latent spaces aligned with data geometry.

</details>


### [46] [Local Learning Rules for Out-of-Equilibrium Physical Generative Models](https://arxiv.org/abs/2506.19136)
*Cyrill Bösch,Geoffrey Roeder,Marc Serra-Garcia,Ryan P. Adams*

Main category: cs.LG

TL;DR: 论文提出了一种通过局部学习规则学习基于分数的生成模型（SGMs）的非平衡驱动协议的方法，并通过实验验证了其在采样和图像生成中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过学习规则直接从力测量或系统动态中计算驱动协议的梯度，以优化SGMs的性能。

Method: 使用非线性过阻尼振荡器网络实现SGM，并应用于2D高斯混合采样和MNIST数据集中的数字图像生成。

Result: 成功实现了从2D高斯混合分布中采样，并训练了10x10振荡器网络生成MNIST中的0和1图像。

Conclusion: 该方法通过局部学习规则有效优化了SGMs的驱动协议，展示了在复杂数据生成任务中的潜力。

Abstract: We show that the out-of-equilibrium driving protocol of score-based
generative models (SGMs) can be learned via a local learning rule. The gradient
with respect to the parameters of the driving protocol are computed directly
from force measurements or from observed system dynamics. As a demonstration,
we implement an SGM in a network of driven, nonlinear, overdamped oscillators
coupled to a thermal bath. We first apply it to the problem of sampling from a
mixture of two Gaussians in 2D. Finally, we train a network of 10x10
oscillators to sample images of 0s and 1s from the MNIST dataset.

</details>


### [47] [Command-V: Pasting LLM Behaviors via Activation Profiles](https://arxiv.org/abs/2506.19140)
*Barry Wang,Avi Schwarzschild,Alexander Robey,Ali Payani,Charles Fleming,Mingjie Sun,Daphne Ippolito*

Main category: cs.LG

TL;DR: Command-V是一种无需反向传播的行为转移方法，通过复制和粘贴残差激活适配器，实现模型行为的快速迁移。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如全微调或蒸馏）成本高且需重复操作，Command-V旨在提供一种高效、低成本的替代方案。

Method: 通过分析层激活、推导线性转换器，并在接收模型的激活空间中应用干预，无需原始训练数据。

Result: 在安全性增强、越狱促进和自动推理等案例中，Command-V性能优于或等同于全微调，且计算成本极低。

Conclusion: Command-V为模型行为迁移提供了一种高效、低成本的新方法。

Abstract: Retrofitting large language models (LLMs) with new behaviors typically
requires full finetuning or distillation-costly steps that must be repeated for
every architecture. In this work, we introduce Command-V, a
backpropagation-free behavior transfer method that copies an existing residual
activation adapter from a donor model and pastes its effect into a recipient
model. Command-V profiles layer activations on a small prompt set, derives
linear converters between corresponding layers, and applies the donor
intervention in the recipient's activation space. This process does not require
access to the original training data and needs minimal compute. In three case
studies-safety-refusal enhancement, jailbreak facilitation, and automatic
chain-of-thought reasoning--Command-V matches or exceeds the performance of
direct finetuning while using orders of magnitude less compute. Our code and
data are accessible at https://github.com/GithuBarry/Command-V/.

</details>


### [48] [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
*Paul C. Bogdan,Uzay Macar,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 论文提出三种句子级分析方法，用于理解大语言模型的推理过程，发现并验证了“思维锚点”的存在。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理过程难以分解和理解，因此需要句子级的分析方法来提高可解释性。

Method: 提出三种互补的归因方法：黑盒法（反事实重要性）、白盒法（注意力模式聚合）和因果归因法（逻辑连接测量）。

Result: 发现并验证了“思维锚点”的存在，这些锚点对后续推理过程有重要影响。

Conclusion: 句子级分析有助于深入理解推理模型，三种方法的一致性验证了其潜力。

Abstract: Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified ``broadcasting'' sentences that receive disproportionate attention
from all future sentences via ``receiver'' attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.

</details>


### [49] [Distilling Tool Knowledge into Language Models via Back-Translated Traces](https://arxiv.org/abs/2506.19171)
*Xingyue Huang,Xianglong Hu,Zifeng Ding,Yuan He,Rishabh,Waleed Alzarooni,Ziyu Ye,Wendong Fan,Bailan He,Haige Bo,Changran Hu,Guohao Li*

Main category: cs.LG

TL;DR: 论文提出了一种通过自然语言将工具知识蒸馏到LLMs的新范式，避免推理时的工具依赖，提升数学问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在需要精确计算或多步代数推理的数学问题上的不足，同时避免工具集成推理（TIR）的可扩展性和部署问题。

Method: 构建Solver Agent，通过规划、符号工具调用和反思推理解决问题；利用多LLM代理的反向翻译管道将TIR痕迹转换为自然语言推理痕迹；通过Translator Agent和Rephrase Agent生成连贯的叙述。

Result: 实验表明，小规模开源模型通过微调可以内化工具知识和结构化推理模式，在竞赛级数学基准上取得提升，无需推理时工具访问。

Conclusion: 该方法成功将工具知识蒸馏到LLMs中，提升了数学推理能力，同时避免了工具依赖的局限性。

Abstract: Large language models (LLMs) often struggle with mathematical problems that
require exact computation or multi-step algebraic reasoning. Tool-integrated
reasoning (TIR) offers a promising solution by leveraging external tools such
as code interpreters to ensure correctness, but it introduces inference-time
dependencies that hinder scalability and deployment. In this work, we propose a
new paradigm for distilling tool knowledge into LLMs purely through natural
language. We first construct a Solver Agent that solves math problems by
interleaving planning, symbolic tool calls, and reflective reasoning. Then,
using a back-translation pipeline powered by multiple LLM-based agents, we
convert interleaved TIR traces into natural language reasoning traces. A
Translator Agent generates explanations for individual tool calls, while a
Rephrase Agent merges them into a fluent and globally coherent narrative.
Empirically, we show that fine-tuning a small open-source model on these
synthesized traces enables it to internalize both tool knowledge and structured
reasoning patterns, yielding gains on competition-level math benchmarks without
requiring tool access at inference.

</details>


### [50] [High precision PINNs in unbounded domains: application to singularity formulation in PDEs](https://arxiv.org/abs/2506.19243)
*Yixuan Wang,Ziming Liu,Zongyi Li,Anima Anandkumar,Thomas Y. Hou*

Main category: cs.LG

TL;DR: 研究了在无界域中高精度训练物理信息神经网络（PINNs）的方法，特别关注PDE中奇点形成的应用。提出模块化方法，并探讨了神经网络选择、采样策略和优化算法的组合。


<details>
  <summary>Details</summary>
Motivation: 探索PINNs在无界域中的高精度训练，以解决PDE中的奇点问题，为研究奇点提供数值工具。

Method: 采用模块化方法，结合神经网络选择、采样策略和优化算法，并与计算机辅助证明和PDE分析结合。

Result: 在1D Burgers方程中获得高精度解，2D Boussinesq方程中的损失比现有方法低4位数且训练步数更少。

Conclusion: PINNs的高精度解可用于研究PDE奇点，未来可探索更高维问题的机器精度解。

Abstract: We investigate the high-precision training of Physics-Informed Neural
Networks (PINNs) in unbounded domains, with a special focus on applications to
singularity formulation in PDEs. We propose a modularized approach and study
the choices of neural network ansatz, sampling strategy, and optimization
algorithm. When combined with rigorous computer-assisted proofs and PDE
analysis, the numerical solutions identified by PINNs, provided they are of
high precision, can serve as a powerful tool for studying singularities in
PDEs. For 1D Burgers equation, our framework can lead to a solution with very
high precision, and for the 2D Boussinesq equation, which is directly related
to the singularity formulation in 3D Euler and Navier-Stokes equations, we
obtain a solution whose loss is $4$ digits smaller than that obtained in
\cite{wang2023asymptotic} with fewer training steps. We also discuss potential
directions for pushing towards machine precision for higher-dimensional
problems.

</details>


### [51] [Universal kernels via harmonic analysis on Riemannian symmetric spaces](https://arxiv.org/abs/2506.19245)
*Franziskus Steinert,Salem Said,Cyrus Mostajeran*

Main category: cs.LG

TL;DR: 该论文研究了黎曼对称空间中核的普适性性质，扩展了非欧几里得域中核的研究，并证明了几个正定核的普适性。


<details>
  <summary>Details</summary>
Motivation: 研究核的普适性性质对机器学习中核方法的理论基础至关重要，尤其是在非欧几里得域中的应用。

Method: 建立了研究黎曼对称空间中核普适性的基本工具，并应用于文献中的正定核实例。

Result: 证明了几个正定核在黎曼对称空间中的普适性，为其在流形数据应用中的使用提供了理论依据。

Conclusion: 该研究为非欧几里得域中核方法的理论支持提供了重要工具和结果。

Abstract: The universality properties of kernels characterize the class of functions
that can be approximated in the associated reproducing kernel Hilbert space and
are of fundamental importance in the theoretical underpinning of kernel methods
in machine learning. In this work, we establish fundamental tools for
investigating universality properties of kernels in Riemannian symmetric
spaces, thereby extending the study of this important topic to kernels in
non-Euclidean domains. Moreover, we use the developed tools to prove the
universality of several recent examples from the literature on positive
definite kernels defined on Riemannian symmetric spaces, thus providing
theoretical justification for their use in applications involving
manifold-valued data.

</details>


### [52] [Behavioral Anomaly Detection in Distributed Systems via Federated Contrastive Learning](https://arxiv.org/abs/2506.19246)
*Renzi Meng,Heyi Wang,Yumeng Sun,Qiyuan Wu,Lian Lian,Renhan Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于联邦对比学习的分布式系统异常检测方法，解决了传统集中式方法在数据隐私、节点异构性和异常模式识别上的局限性。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中异常检测问题日益突出，传统集中式方法在数据隐私和异构性处理上存在不足。

Method: 结合联邦学习的分布式协作建模能力和对比学习的特征判别增强，通过本地节点构建嵌入表示和正负样本对，优化全局模型。

Result: 实验表明，该方法在多种攻击类型和实时数据流场景下表现优异，检测准确性和适应性显著提升。

Conclusion: 该方法在隐私保护和检测性能之间取得平衡，为分布式系统的智能安全管理提供了可行技术路径。

Abstract: This paper addresses the increasingly prominent problem of anomaly detection
in distributed systems. It proposes a detection method based on federated
contrastive learning. The goal is to overcome the limitations of traditional
centralized approaches in terms of data privacy, node heterogeneity, and
anomaly pattern recognition. The proposed method combines the distributed
collaborative modeling capabilities of federated learning with the feature
discrimination enhancement of contrastive learning. It builds embedding
representations on local nodes and constructs positive and negative sample
pairs to guide the model in learning a more discriminative feature space.
Without exposing raw data, the method optimizes a global model through a
federated aggregation strategy. Specifically, the method uses an encoder to
represent local behavior data in high-dimensional space. This includes system
logs, operational metrics, and system calls. The model is trained using both
contrastive loss and classification loss to improve its ability to detect
fine-grained anomaly patterns. The method is evaluated under multiple typical
attack types. It is also tested in a simulated real-time data stream scenario
to examine its responsiveness. Experimental results show that the proposed
method outperforms existing approaches across multiple performance metrics. It
demonstrates strong detection accuracy and adaptability, effectively addressing
complex anomalies in distributed environments. Through careful design of key
modules and optimization of the training mechanism, the proposed method
achieves a balance between privacy preservation and detection performance. It
offers a feasible technical path for intelligent security management in
distributed systems.

</details>


### [53] [Inference-Time Reward Hacking in Large Language Models](https://arxiv.org/abs/2506.19248)
*Hadi Khalaf,Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio du Pin Calmon*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）中奖励模型优化导致的奖励黑客现象，提出了HedgeTune算法来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型作为复杂需求的代理，可能因过度优化而偏离目标（奖励黑客现象），影响模型性能和对齐目标。

Method: 通过Best-of-$n$ (BoN)、Soft-Best-of-$n$ (SBoN)和提出的Best-of-Poisson (BoP)方法，研究奖励黑客现象，并引入HedgeTune算法优化推理时参数。

Result: 实验表明，HedgeTune能有效缓解奖励黑客现象，在奖励与失真之间取得更好平衡，且计算开销低。

Conclusion: 对冲策略（如HedgeTune）是避免奖励黑客现象的有效方法，适用于推理时对齐任务。

Abstract: A common paradigm to improve the performance of large language models is
optimizing for a reward model. Reward models assign a numerical score to LLM
outputs indicating, for example, which response would likely be preferred by a
user or is most aligned with safety goals. However, reward models are never
perfect. They inevitably function as proxies for complex desiderata such as
correctness, helpfulness, and safety. By overoptimizing for a misspecified
reward, we can subvert intended alignment goals and reduce overall performance
-- a phenomenon commonly referred to as reward hacking. In this work, we
characterize reward hacking in inference-time alignment and demonstrate when
and how we can mitigate it by hedging on the proxy reward. We study this
phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we
introduce Best-of-Poisson (BoP) that provides an efficient, near-exact
approximation of the optimal reward-KL divergence policy at inference time. We
show that the characteristic pattern of hacking as observed in practice (where
the true reward first increases before declining) is an inevitable property of
a broad class of inference-time mechanisms, including BoN and BoP. To counter
this effect, hedging offers a tactical choice to avoid placing undue confidence
in high but potentially misleading proxy reward signals. We introduce
HedgeTune, an efficient algorithm to find the optimal inference-time parameter
and avoid reward hacking. We demonstrate through experiments that hedging
mitigates reward hacking and achieves superior distortion-reward tradeoffs with
minimal computational overhead.

</details>


### [54] [Robust Behavior Cloning Via Global Lipschitz Regularization](https://arxiv.org/abs/2506.19250)
*Shili Wu,Yizhao Jin,Puhua Niu,Aniruddha Datta,Sean B. Andersson*

Main category: cs.LG

TL;DR: 本文提出了一种通过全局Lipschitz正则化增强行为克隆策略网络鲁棒性的方法，并提供了针对有界范数扰动的鲁棒性证明。


<details>
  <summary>Details</summary>
Motivation: 行为克隆（BC）在部署时可能因观测误差或对抗性扰动导致策略性能下降，因此需要增强其鲁棒性。

Method: 采用全局Lipschitz正则化方法，构建Lipschitz神经网络以确保策略鲁棒性。

Result: 实验在Gymnasium多个环境中验证了方法的有效性。

Conclusion: 全局Lipschitz正则化能显著提升行为克隆策略的鲁棒性。

Abstract: Behavior Cloning (BC) is an effective imitation learning technique and has
even been adopted in some safety-critical domains such as autonomous vehicles.
BC trains a policy to mimic the behavior of an expert by using a dataset
composed of only state-action pairs demonstrated by the expert, without any
additional interaction with the environment. However, During deployment, the
policy observations may contain measurement errors or adversarial disturbances.
Since the observations may deviate from the true states, they can mislead the
agent into making sub-optimal actions. In this work, we use a global Lipschitz
regularization approach to enhance the robustness of the learned policy
network. We then show that the resulting global Lipschitz property provides a
robustness certificate to the policy with respect to different bounded norm
perturbations. Then, we propose a way to construct a Lipschitz neural network
that ensures the policy robustness. We empirically validate our theory across
various environments in Gymnasium. Keywords: Robust Reinforcement Learning;
Behavior Cloning; Lipschitz Neural Network

</details>


### [55] [Robust OOD Graph Learning via Mean Constraints and Noise Reduction](https://arxiv.org/abs/2506.19281)
*Yang Zhou,Xiaoning Ren*

Main category: cs.LG

TL;DR: 论文提出两种方法（CMO和NNR）解决图OOD分类中的类别不平衡和结构噪声问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决图OOD分类中少数类性能下降和结构噪声敏感性的挑战。

Method: 1. Constrained Mean Optimization (CMO) 通过最坏情况下基于相似性的实例聚合提升少数类鲁棒性；2. Neighbor-Aware Noise Reweighting (NNR) 根据局部结构一致性动态加权训练样本以减少噪声影响。

Result: 在合成和真实数据集上验证了方法的有效性，显著提升了图OOD泛化能力和分类准确性。

Conclusion: 提出的CMO和NNR方法有效解决了图OOD分类中的关键问题，并展示了理论和实验上的优越性。

Abstract: Graph Out-of-Distribution (OOD) classification often suffers from sharp
performance drops, particularly under category imbalance and structural noise.
This work tackles two pressing challenges in this context: (1) the
underperformance of minority classes due to skewed label distributions, and (2)
their heightened sensitivity to structural noise in graph data. To address
these problems, we propose two complementary solutions. First, Constrained Mean
Optimization (CMO) improves minority class robustness by encouraging
similarity-based instance aggregation under worst-case conditions. Second, the
Neighbor-Aware Noise Reweighting (NNR) mechanism assigns dynamic weights to
training samples based on local structural consistency, mitigating noise
influence. We provide theoretical justification for our methods, and validate
their effectiveness with extensive experiments on both synthetic and real-world
datasets, showing significant improvements in Graph OOD generalization and
classification accuracy. The code for our method is available at:
https://anonymous.4open.science/r/CMO-NNR-2F30.

</details>


### [56] [A Batch-Insensitive Dynamic GNN Approach to Address Temporal Discontinuity in Graph Streams](https://arxiv.org/abs/2506.19282)
*Yang Zhou,Xiaoning Ren*

Main category: cs.LG

TL;DR: BADGNN提出了一种批处理无关的动态图神经网络框架，通过Temporal Lipschitz Regularization和Adaptive Attention Adjustment解决大批次训练中的时序信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 动态图中保持时序连续性至关重要，但基于记忆的动态图神经网络（MDGNNs）在大批次训练时会破坏事件序列，导致时序信息丢失，影响建模和参数收敛。

Method: 提出BADGNN框架，包含两个核心组件：Temporal Lipschitz Regularization（TLR）控制参数搜索空间扩展，Adaptive Attention Adjustment（A3）缓解注意力的扭曲。

Result: 在三个基准数据集上，BADGNN在保持高性能的同时，支持更大的批次大小和更快的训练速度。

Conclusion: BADGNN通过TLR和A3有效解决了大批次训练中的时序信息丢失问题，提升了动态图神经网络的训练效率和性能。

Abstract: In dynamic graphs, preserving temporal continuity is critical. However,
Memory-based Dynamic Graph Neural Networks (MDGNNs) trained with large batches
often disrupt event sequences, leading to temporal information loss. This
discontinuity not only deteriorates temporal modeling but also hinders
optimization by increasing the difficulty of parameter convergence. Our
theoretical study quantifies this through a Lipschitz upper bound, showing that
large batch sizes enlarge the parameter search space. In response, we propose
BADGNN, a novel batch-agnostic framework consisting of two core components: (1)
Temporal Lipschitz Regularization (TLR) to control parameter search space
expansion, and (2) Adaptive Attention Adjustment (A3) to alleviate attention
distortion induced by both regularization and batching. Empirical results on
three benchmark datasets show that BADGNN maintains strong performance while
enabling significantly larger batch sizes and faster training compared to TGN.
Our code is available at Code:
https://anonymous.4open.science/r/TGN_Lipichitz-C033/.

</details>


### [57] [Efficient Extreme Operating Condition Search for Online Relay Setting Calculation in Renewable Power Systems Based on Parallel Graph Neural Network](https://arxiv.org/abs/2506.19289)
*Yan Li,Zengli Yang,Youhuai Wang,Jing Wang,Xiaoyu Han,Jingyu Wang,Dongyuan Shi*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的并行图神经网络（PGNN）方法，用于在线继电器设置计算中的极端运行条件搜索（EOCS）问题，显著提高了计算速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的高渗透率和逆变器资源的广泛应用，电力系统的运行条件变得更加波动，现有EOCS方法的计算速度无法满足在线继电器设置计算的效率需求。

Method: 将电力系统信息分为四个层次（组件参数层、拓扑连接层、电气距离层和图距离层），通过PGNN模型提取特征，并输入决策网络预测极端运行条件。

Result: 在改进的IEEE 39总线和118总线测试系统上验证，PGNN方法在解决EOCS问题时比现有方法具有更高的准确性和更快的在线计算时间。

Conclusion: PGNN方法为在线继电器设置计算提供了一种高效且准确的解决方案，适用于可再生能源电力系统。

Abstract: The Extreme Operating Conditions Search (EOCS) problem is one of the key
problems in relay setting calculation, which is used to ensure that the setting
values of protection relays can adapt to the changing operating conditions of
power systems over a period of time after deployment. The high penetration of
renewable energy and the wide application of inverter-based resources make the
operating conditions of renewable power systems more volatile, which urges the
adoption of the online relay setting calculation strategy. However, the
computation speed of existing EOCS methods based on local enumeration,
heuristic algorithms, and mathematical programming cannot meet the efficiency
requirement of online relay setting calculation. To reduce the time overhead,
this paper, for the first time, proposes an efficient deep learning-based EOCS
method suitable for online relay setting calculation. First, the power system
information is formulated as four layers, i.e., a component parameter layer, a
topological connection layer, an electrical distance layer, and a graph
distance layer, which are fed into a parallel graph neural network (PGNN) model
for feature extraction. Then, the four feature layers corresponding to each
node are spliced and stretched, and then fed into the decision network to
predict the extreme operating condition of the system. Finally, the proposed
PGNN method is validated on the modified IEEE 39-bus and 118-bus test systems,
where some of the synchronous generators are replaced by renewable generation
units. The nonlinear fault characteristics of renewables are fully considered
when computing fault currents. The experiment results show that the proposed
PGNN method achieves higher accuracy than the existing methods in solving the
EOCS problem. Meanwhile, it also provides greater improvements in online
computation time.

</details>


### [58] [The Effect of Depth on the Expressivity of Deep Linear State-Space Models](https://arxiv.org/abs/2506.19296)
*Zeyu Bao,Penghao Yu,Haotian Jiang,Qianxiao Li*

Main category: cs.LG

TL;DR: 深度线性状态空间模型（SSMs）的表达力受深度和宽度影响，无约束时二者等价；在参数范数约束下，深度SSMs表现更优。


<details>
  <summary>Details</summary>
Motivation: 探究深度和宽度如何影响深度线性SSMs的表达能力，特别是在参数范数约束下的差异。

Method: 理论分析深度与宽度的等价性及范数约束下的差异，构造性方法证明深度SSMs的优势，并推导最小深度上界。

Result: 无约束时深度与宽度等价；范数约束下深度SSMs能更高效表示大范数目标。

Conclusion: 深度SSMs在参数范数约束下具有更强的表达能力，理论结果通过数值实验验证。

Abstract: Deep state-space models (SSMs) have gained increasing popularity in sequence
modelling. While there are numerous theoretical investigations of shallow SSMs,
how the depth of the SSM affects its expressiveness remains a crucial problem.
In this paper, we systematically investigate the role of depth and width in
deep linear SSMs, aiming to characterize how they influence the expressive
capacity of the architecture. First, we rigorously prove that in the absence of
parameter constraints, increasing depth and increasing width are generally
equivalent, provided that the parameter count remains within the same order of
magnitude. However, under the assumption that the parameter norms are
constrained, the effects of depth and width differ significantly. We show that
a shallow linear SSM with large parameter norms can be represented by a deep
linear SSM with smaller norms using a constructive method. In particular, this
demonstrates that deep SSMs are more capable of representing targets with large
norms than shallow SSMs under norm constraints. Finally, we derive upper bounds
on the minimal depth required for a deep linear SSM to represent a given
shallow linear SSM under constrained parameter norms. We also validate our
theoretical results with numerical experiments

</details>


### [59] [Adversarial Attacks on Deep Learning-Based False Data Injection Detection in Differential Relays](https://arxiv.org/abs/2506.19302)
*Ahmad Mohammad Saber,Aditi Maheshwari,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: 本文提出了一种针对智能电网中虚假数据注入攻击（FDIAs）检测的深度学习方案（DLSs）的对抗攻击框架，展示了现有DLSs的脆弱性，并提出了对抗训练作为防御机制。


<details>
  <summary>Details</summary>
Motivation: 智能电网中基于深度学习的虚假数据注入攻击检测方案存在被对抗攻击规避的风险，亟需研究其脆弱性并提出防御方法。

Method: 利用快速梯度符号方法（FGSM）设计对抗攻击框架，通过微小扰动使DLSs误判攻击为合法故障，同时测试多种深度学习模型的鲁棒性。

Result: 实验表明，现有模型在对抗攻击下表现脆弱，部分攻击成功率超过99.7%；对抗训练显著提升了模型鲁棒性。

Conclusion: 对抗攻击对DLSs构成严重威胁，对抗训练是有效的防御手段，智能电网需加强网络安全措施。

Abstract: The application of Deep Learning-based Schemes (DLSs) for detecting False
Data Injection Attacks (FDIAs) in smart grids has attracted significant
attention. This paper demonstrates that adversarial attacks, carefully crafted
FDIAs, can evade existing DLSs used for FDIA detection in Line Current
Differential Relays (LCDRs). We propose a novel adversarial attack framework,
utilizing the Fast Gradient Sign Method, which exploits DLS vulnerabilities by
introducing small perturbations to LCDR remote measurements, leading to
misclassification of the FDIA as a legitimate fault while also triggering the
LCDR to trip. We evaluate the robustness of multiple deep learning models,
including multi-layer perceptrons, convolutional neural networks, long
short-term memory networks, and residual networks, under adversarial
conditions. Our experimental results demonstrate that while these models
perform well, they exhibit high degrees of vulnerability to adversarial
attacks. For some models, the adversarial attack success rate exceeds 99.7%. To
address this threat, we introduce adversarial training as a proactive defense
mechanism, significantly enhancing the models' ability to withstand adversarial
FDIAs without compromising fault detection accuracy. Our results highlight the
significant threat posed by adversarial attacks to DLS-based FDIA detection,
underscore the necessity for robust cybersecurity measures in smart grids, and
demonstrate the effectiveness of adversarial training in enhancing model
robustness against adversarial FDIAs.

</details>


### [60] [Contrastive Cross-Modal Learning for Infusing Chest X-ray Knowledge into ECGs](https://arxiv.org/abs/2506.19329)
*Vineet Punyamoorty,Aditya Malusare,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: CroMoTEX是一种基于对比学习的框架，利用胸部X光片训练学习临床信息丰富的心电图表示，用于多种心脏相关病理诊断。


<details>
  <summary>Details</summary>
Motivation: 现代诊断工作流多模态化，心电图（ECG）和胸部X光片（CXR）是心脏评估的常用模态。ECG更易获取，但CXR提供更丰富的诊断信息。CroMoTEX旨在结合两者的优势。

Method: 提出CroMoTEX框架，通过对比学习目标对齐ECG和CXR表示，采用自适应硬负样本加权，训练时使用CXR，测试时仅需ECG输入。

Result: 在MIMIC-IV-ECG和MIMIC-CXR数据集上，CroMoTEX在三种病理诊断中均优于基线，最高AUROC达78.31（水肿）。

Conclusion: CroMoTEX通过跨模态对比学习，实现了仅需ECG输入的高效心脏病理诊断，具有实际部署潜力。

Abstract: Modern diagnostic workflows are increasingly multimodal, integrating diverse
data sources such as medical images, structured records, and physiological time
series. Among these, electrocardiograms (ECGs) and chest X-rays (CXRs) are two
of the most widely used modalities for cardiac assessment. While CXRs provide
rich diagnostic information, ECGs are more accessible and can support scalable
early warning systems. In this work, we propose CroMoTEX, a novel contrastive
learning-based framework that leverages chest X-rays during training to learn
clinically informative ECG representations for multiple cardiac-related
pathologies: cardiomegaly, pleural effusion, and edema. Our method aligns ECG
and CXR representations using a novel supervised cross-modal contrastive
objective with adaptive hard negative weighting, enabling robust and
task-relevant feature learning. At test time, CroMoTEX relies solely on ECG
input, allowing scalable deployment in real-world settings where CXRs may be
unavailable. Evaluated on the large-scale MIMIC-IV-ECG and MIMIC-CXR datasets,
CroMoTEX outperforms baselines across all three pathologies, achieving up to
78.31 AUROC on edema. Our code is available at
github.com/vineetpmoorty/cromotex.

</details>


### [61] [Unlocking Insights Addressing Alcohol Inference Mismatch through Database-Narrative Alignment](https://arxiv.org/abs/2506.19342)
*Sudesh Bhagat,Raghupathi Kandiboina,Ibne Farabi Shihab,Skylar Knickerbocker,Neal Hawkins,Anuj Sharma*

Main category: cs.LG

TL;DR: 研究通过数据库叙事对齐和BERT模型分析，识别并减少交通事故数据中的酒精推断不匹配（AIM），提升数据质量，支持精准政策制定。


<details>
  <summary>Details</summary>
Motivation: 交通事故是全球主要死因之一，需准确数据支持预防策略和政策制定，但酒精推断不匹配（AIM）问题亟待解决。

Method: 开发框架结合BERT模型分析37万+交通事故记录，使用Probit Logit模型统计工具探索AIM模式。

Result: 发现24.03%的AIM率，酒精相关致命事故和夜间事故AIM率较低，未知车型和老年司机事故更易出现AIM。

Conclusion: 需针对性培训和数据管理团队，提升事故报告准确性，支持证据驱动政策。

Abstract: Road traffic crashes are a significant global cause of fatalities,
emphasizing the urgent need for accurate crash data to enhance prevention
strategies and inform policy development. This study addresses the challenge of
alcohol inference mismatch (AIM) by employing database narrative alignment to
identify AIM in crash data. A framework was developed to improve data quality
in crash management systems and reduce the percentage of AIM crashes. Utilizing
the BERT model, the analysis of 371,062 crash records from Iowa (2016-2022)
revealed 2,767 AIM incidents, resulting in an overall AIM percentage of 24.03%.
Statistical tools, including the Probit Logit model, were used to explore the
crash characteristics affecting AIM patterns. The findings indicate that
alcohol-related fatal crashes and nighttime incidents have a lower percentage
of the mismatch, while crashes involving unknown vehicle types and older
drivers are more susceptible to mismatch. The geospatial cluster as part of
this study can identify the regions which have an increased need for education
and training. These insights highlight the necessity for targeted training
programs and data management teams to improve the accuracy of crash reporting
and support evidence-based policymaking.

</details>


### [62] [Discrepancy-Aware Graph Mask Auto-Encoder](https://arxiv.org/abs/2506.19343)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DGMAE的图自监督学习方法，通过重构节点间的差异信息，解决了现有方法在异质性图上表现不佳的问题，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法在异质性图上表现不佳，因为它们仅关注邻居信息而忽略节点间的差异，导致节点表示难以区分。

Method: 提出DGMAE，通过在掩码过程中重构相邻节点的差异信息，获得更具区分性的节点表示。

Result: 在17个基准数据集上的实验表明，DGMAE能有效保留节点差异，并在节点分类、节点聚类和图分类任务上显著优于现有方法。

Conclusion: DGMAE通过关注节点差异信息，显著提升了图自监督学习的性能，尤其在异质性图上表现优异。

Abstract: Masked Graph Auto-Encoder, a powerful graph self-supervised training
paradigm, has recently shown superior performance in graph representation
learning. Existing works typically rely on node contextual information to
recover the masked information. However, they fail to generalize well to
heterophilic graphs where connected nodes may be not similar, because they
focus only on capturing the neighborhood information and ignoring the
discrepancy information between different nodes, resulting in indistinguishable
node representations. In this paper, to address this issue, we propose a
Discrepancy-Aware Graph Mask Auto-Encoder (DGMAE). It obtains more
distinguishable node representations by reconstructing the discrepancy
information of neighboring nodes during the masking process. We conduct
extensive experiments on 17 widely-used benchmark datasets. The results show
that our DGMAE can effectively preserve the discrepancies of nodes in
low-dimensional space. Moreover, DGMAE significantly outperforms
state-of-the-art graph self-supervised learning methods on three graph analytic
including tasks node classification, node clustering, and graph classification,
demonstrating its remarkable superiority. The code of DGMAE is available at
https://github.com/zhengziyu77/DGMAE.

</details>


### [63] [In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly](https://arxiv.org/abs/2506.19351)
*Puneesh Deora,Bhavya Vasudeva,Tina Behnia,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在分层任务结构中的上下文学习能力，发现其能选择最简复杂度解释，并验证了其类似贝叶斯奥卡姆剃刀的归纳偏好。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer如何在不同复杂度任务中自适应，揭示其内在的归纳偏好。

Method: 设计基于马尔可夫链和线性回归的测试环境，分析模型选择复杂度的行为。

Result: Transformer能准确推断任务复杂度参数，偏好最简解释，验证了贝叶斯奥卡姆剃刀行为。

Conclusion: Transformer在多样任务分布训练下可能固有类似奥卡姆剃刀的归纳偏好。

Abstract: In-context learning (ICL) enables transformers to adapt to new tasks through
contextual examples without parameter updates. While existing research has
typically studied ICL in fixed-complexity environments, practical language
models encounter tasks spanning diverse complexity levels. This paper
investigates how transformers navigate hierarchical task structures where
higher-complexity categories can perfectly represent any pattern generated by
simpler ones. We design well-controlled testbeds based on Markov chains and
linear regression that reveal transformers not only identify the appropriate
complexity level for each task but also accurately infer the corresponding
parameters--even when the in-context examples are compatible with multiple
complexity hypotheses. Notably, when presented with data generated by simpler
processes, transformers consistently favor the least complex sufficient
explanation. We theoretically explain this behavior through a Bayesian
framework, demonstrating that transformers effectively implement an in-context
Bayesian Occam's razor by balancing model fit against complexity penalties. We
further ablate on the roles of model size, training mixture distribution,
inference context length, and architecture. Finally, we validate this Occam's
razor-like inductive bias on a pretrained GPT-4 model with Boolean-function
tasks as case study, suggesting it may be inherent to transformers trained on
diverse task distributions.

</details>


### [64] [Path Learning with Trajectory Advantage Regression](https://arxiv.org/abs/2506.19375)
*Kohei Miyaguchi*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的离线路径学习和路径归因方法，通过回归问题解决路径优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决路径优化问题，同时仅需算法上解决回归问题。

Method: 轨迹优势回归（trajectory advantage regression）。

Result: 能够有效解决路径优化问题。

Conclusion: 该方法通过回归问题简化了路径优化的复杂性。

Abstract: In this paper, we propose trajectory advantage regression, a method of
offline path learning and path attribution based on reinforcement learning. The
proposed method can be used to solve path optimization problems while
algorithmically only solving a regression problem.

</details>


### [65] [Explainable Artificial Intelligence Credit Risk Assessment using Machine Learning](https://arxiv.org/abs/2506.19383)
*Shreya,Harsh Pathak*

Main category: cs.LG

TL;DR: 本文提出了一种基于XGBoost、LightGBM和随机森林的智能透明AI信用风险评估系统，结合SHAP和LIME解释技术，解决了模型可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 解决信用风险评估中模型预测准确性与可解释性的挑战。

Method: 使用XGBoost、LightGBM和随机森林进行预测分析，结合SHAP和LIME解释技术，采用SMOTE处理类别不平衡，GridSearchCV进行超参数调优。

Result: LightGBM在ROC-AUC、精度、召回率和F1分数上表现最佳，成为最优业务模型。

Conclusion: 系统通过XAI可视化报告和业务影响总结，实现了透明决策，提升了信用风险评估的准确性和可解释性。

Abstract: This paper presents an intelligent and transparent AI-driven system for
Credit Risk Assessment using three state-of-the-art ensemble machine learning
models combined with Explainable AI (XAI) techniques. The system leverages
XGBoost, LightGBM, and Random Forest algorithms for predictive analysis of loan
default risks, addressing the challenges of model interpretability using SHAP
and LIME. Preprocessing steps include custom imputation, one-hot encoding, and
standardization. Class imbalance is managed using SMOTE, and hyperparameter
tuning is performed with GridSearchCV. The model is evaluated on multiple
performance metrics including ROC-AUC, precision, recall, and F1-score.
LightGBM emerges as the most business-optimal model with the highest accuracy
and best trade off between approval and default rates. Furthermore, the system
generates applicant-specific XAI visual reports and business impact summaries
to ensure transparent decision-making.

</details>


### [66] [Deep Electromagnetic Structure Design Under Limited Evaluation Budgets](https://arxiv.org/abs/2506.19384)
*Shijian Zheng,Fangxiao Jin,Shuhai Zhang,Quan Xue,Mingkui Tan*

Main category: cs.LG

TL;DR: 提出了一种名为渐进四叉树搜索（PQS）的新方法，用于高效解决电磁结构设计中的高维空间和昂贵评估问题。


<details>
  <summary>Details</summary>
Motivation: 电磁结构设计面临高维设计空间和昂贵评估的挑战，现有方法通常依赖大量数据且难以适应实际规模和预算限制。

Method: PQS将传统图像布局转换为基于四叉树的分层表示，实现从全局模式到局部细节的渐进搜索，并引入一致性驱动的样本选择机制以减少对高精度预测器的依赖。

Result: 在双频选择性表面和高增益天线两个实际工程任务中，PQS在有限计算预算下表现优异，比生成方法节省75-85%的评估成本，缩短设计周期20.27-38.80天。

Conclusion: PQS方法在电磁结构设计中显著提高了效率和成本效益，优于现有基线方法。

Abstract: Electromagnetic structure (EMS) design plays a critical role in developing
advanced antennas and materials, but remains challenging due to
high-dimensional design spaces and expensive evaluations. While existing
methods commonly employ high-quality predictors or generators to alleviate
evaluations, they are often data-intensive and struggle with real-world scale
and budget constraints. To address this, we propose a novel method called
Progressive Quadtree-based Search (PQS). Rather than exhaustively exploring the
high-dimensional space, PQS converts the conventional image-like layout into a
quadtree-based hierarchical representation, enabling a progressive search from
global patterns to local details. Furthermore, to lessen reliance on highly
accurate predictors, we introduce a consistency-driven sample selection
mechanism. This mechanism quantifies the reliability of predictions, balancing
exploitation and exploration when selecting candidate designs. We evaluate PQS
on two real-world engineering tasks, i.e., Dual-layer Frequency Selective
Surface and High-gain Antenna. Experimental results show that our method can
achieve satisfactory designs under limited computational budgets, outperforming
baseline methods. In particular, compared to generative approaches, it cuts
evaluation costs by 75-85%, effectively saving 20.27-38.80 days of product
designing cycle.

</details>


### [67] [Maximal Update Parametrization and Zero-Shot Hyperparameter Transfer for Fourier Neural Operators](https://arxiv.org/abs/2506.19396)
*Shanda Li,Shinjae Yoo,Yiming Yang*

Main category: cs.LG

TL;DR: 论文提出了一种零样本超参数迁移技术（μTransfer-FNO），解决了Fourier神经算子在处理复杂偏微分方程时因增加傅里叶模式数量导致的超参数调优问题。


<details>
  <summary>Details</summary>
Motivation: Fourier神经算子在处理复杂偏微分方程时需要增加傅里叶模式数量，导致模型参数激增，超参数调优计算成本过高。

Method: 基于Maximal Update Parametrization（μP）框架，提出μTransfer-FNO技术，通过数学推导实现超参数在不同规模FNO模型间的直接迁移。

Result: 实验表明，μTransfer-FNO在减少大型FNO超参数调优计算成本的同时，保持了或提升了模型精度。

Conclusion: μTransfer-FNO为大规模FNO模型的超参数调优提供了一种高效且准确的解决方案。

Abstract: Fourier Neural Operators (FNOs) offer a principled approach for solving
complex partial differential equations (PDEs). However, scaling them to handle
more complex PDEs requires increasing the number of Fourier modes, which
significantly expands the number of model parameters and makes hyperparameter
tuning computationally impractical. To address this, we introduce
$\mu$Transfer-FNO, a zero-shot hyperparameter transfer technique that enables
optimal configurations, tuned on smaller FNOs, to be directly applied to
billion-parameter FNOs without additional tuning. Building on the Maximal
Update Parametrization ($\mu$P) framework, we mathematically derive a
parametrization scheme that facilitates the transfer of optimal hyperparameters
across models with different numbers of Fourier modes in FNOs, which is
validated through extensive experiments on various PDEs. Our empirical study
shows that Transfer-FNO reduces computational cost for tuning hyperparameters
on large FNOs while maintaining or improving accuracy.

</details>


### [68] [Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.19417)
*Yisak Park,Sunwoo Lee,Seungyul Han*

Main category: cs.LG

TL;DR: 提出了一种名为FIM的新框架，通过聚焦任务关键元素（CoG状态维度）来增强多智能体强化学习中的合作，显著提升了稀疏奖励环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下的多智能体强化学习面临探索不足和协调困难的问题，需要一种机制来引导智能体关注任务关键元素。

Method: FIM框架包含三个核心组件：识别CoG状态维度、设计反事实内在奖励、基于资格迹的信用积累。

Result: 在多种MARL基准测试中，FIM显著优于基线方法，提升了合作性能。

Conclusion: FIM通过聚焦关键状态维度和协调注意力，有效解决了稀疏奖励下的多智能体合作问题。

Abstract: Cooperative multi-agent reinforcement learning (MARL) under sparse rewards
presents a fundamental challenge due to limited exploration and insufficient
coordinated attention among agents. In this work, we propose the Focusing
Influence Mechanism (FIM), a novel framework that enhances cooperation by
directing agent influence toward task-critical elements, referred to as Center
of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory.
FIM consists of three core components: (1) identifying CoG state dimensions
based on their stability under agent behavior, (2) designing counterfactual
intrinsic rewards to promote meaningful influence on these dimensions, and (3)
encouraging persistent and synchronized focus through eligibility-trace-based
credit accumulation. These mechanisms enable agents to induce more targeted and
effective state transitions, facilitating robust cooperation even in extremely
sparse reward settings. Empirical evaluations across diverse MARL benchmarks
demonstrate that the proposed FIM significantly improves cooperative
performance compared to baselines.

</details>


### [69] [Tagged for Direction: Pinning Down Causal Edge Directions with Precision](https://arxiv.org/abs/2506.19459)
*Florian Peter Busch,Moritz Willig,Florian Guldan,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出了一种基于标签的因果发现方法，通过为变量分配多个标签来改进传统基于单一类型的方法，提升因果发现的鲁棒性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一类型的因果发现方法缺乏鲁棒性和灵活性，因为变量只能被分配单一类型。

Method: 为每个变量分配多个标签，先应用现有因果发现方法定向部分边，再通过标签间关系定向未定向的边。

Result: 实验表明，该方法提升了因果发现的效果，且标签关系符合常识。

Conclusion: 基于标签的方法优于传统单一类型方法，提供了更灵活和鲁棒的因果发现框架。

Abstract: Not every causal relation between variables is equal, and this can be
leveraged for the task of causal discovery. Recent research shows that pairs of
variables with particular type assignments induce a preference on the causal
direction of other pairs of variables with the same type. Although useful, this
assignment of a specific type to a variable can be tricky in practice. We
propose a tag-based causal discovery approach where multiple tags are assigned
to each variable in a causal graph. Existing causal discovery approaches are
first applied to direct some edges, which are then used to determine edge
relations between tags. Then, these edge relations are used to direct the
undirected edges. Doing so improves upon purely type-based relations, where the
assumption of type consistency lacks robustness and flexibility due to being
restricted to single types for each variable. Our experimental evaluations show
that this boosts causal discovery and that these high-level tag relations fit
common knowledge.

</details>


### [70] [ADDQ: Adaptive Distributional Double Q-Learning](https://arxiv.org/abs/2506.19478)
*Leif Döring,Benedikt Wille,Maximilian Birr,Mihail Bîrsan,Martin Slowik*

Main category: cs.LG

TL;DR: 提出了一种基于分布强化学习（DRL）的简单方法，用于局部自适应地解决Q值估计中的过估计问题。


<details>
  <summary>Details</summary>
Motivation: Q值估计中的偏差问题阻碍了Q学习和行动者-评论者方法的收敛速度，现代RL算法的成功部分归功于直接或间接的过估计减少机制。

Method: 在现有分布强化学习算法基础上，通过少量代码实现局部自适应过估计控制，并结合双Q学习进行理论验证。

Result: 在表格、Atari和MuJoCo环境中进行了实验验证。

Conclusion: 该方法简单易实现，能有效改进现有分布强化学习算法，减少过估计问题。

Abstract: Bias problems in the estimation of $Q$-values are a well-known obstacle that
slows down convergence of $Q$-learning and actor-critic methods. One of the
reasons of the success of modern RL algorithms is partially a direct or
indirect overestimation reduction mechanism. We propose an easy to implement
method built on top of distributional reinforcement learning (DRL) algorithms
to deal with the overestimation in a locally adaptive way. Our framework is
simple to implement, existing distributional algorithms can be improved with a
few lines of code. We provide theoretical evidence and use double $Q$-learning
to show how to include locally adaptive overestimation control in existing
algorithms. Experiments are provided for tabular, Atari, and MuJoCo
environments.

</details>


### [71] [Fast and Distributed Equivariant Graph Neural Networks by Virtual Node Learning](https://arxiv.org/abs/2506.19482)
*Yuelin Zhang,Jiacheng Cen,Jiaqi Han,Wenbing Huang*

Main category: cs.LG

TL;DR: 论文提出了FastEGNN和DistEGNN两种改进的等变图神经网络，用于高效处理大规模几何图，解决了现有方法在稀疏化和扩展性上的效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有等变图神经网络在处理大规模几何图时效率低下，且在稀疏化输入时性能显著下降。

Method: FastEGNN通过引入有序虚拟节点近似无序真实节点，采用差异化消息传递机制；DistEGNN则通过虚拟节点作为子图间的全局桥梁，实现分布式处理。

Result: 在多个挑战性任务（如N-body系统、蛋白质动力学等）中，模型表现出高效性和高性能。

Conclusion: FastEGNN和DistEGNN为大规模等变图学习提供了新的高效解决方案。

Abstract: Equivariant Graph Neural Networks (GNNs) have achieved remarkable success
across diverse scientific applications. However, existing approaches face
critical efficiency challenges when scaling to large geometric graphs and
suffer significant performance degradation when the input graphs are sparsified
for computational tractability. To address these limitations, we introduce
FastEGNN and DistEGNN, two novel enhancements to equivariant GNNs for
large-scale geometric graphs. FastEGNN employs a key innovation: a small
ordered set of virtual nodes that effectively approximates the large unordered
graph of real nodes. Specifically, we implement distinct message passing and
aggregation mechanisms for different virtual nodes to ensure mutual
distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual
and real coordinates to achieve global distributedness. This design enables
FastEGNN to maintain high accuracy while efficiently processing large-scale
sparse graphs. For extremely large-scale geometric graphs, we present DistEGNN,
a distributed extension where virtual nodes act as global bridges between
subgraphs in different devices, maintaining consistency while dramatically
reducing memory and computational overhead. We comprehensively evaluate our
models across four challenging domains: N-body systems (100 nodes), protein
dynamics (800 nodes), Water-3D (8,000 nodes), and our new Fluid113K benchmark
(113,000 nodes). Results demonstrate superior efficiency and performance,
establishing new capabilities in large-scale equivariant graph learning. Code
is available at https://github.com/GLAD-RUC/DistEGNN.

</details>


### [72] [Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy](https://arxiv.org/abs/2506.19486)
*Zhihao Sui,Liang Hu,Jian Cao,Dora D. Liu,Usman Naseem,Zhongyuan Lai,Qi Zhang*

Main category: cs.LG

TL;DR: 论文研究了机器遗忘（MU）技术的漏洞，提出了一种无需访问原始模型的成员召回攻击（MRA）框架，通过知识蒸馏和噪声标签学习恢复遗忘实例的类别信息。


<details>
  <summary>Details</summary>
Motivation: 尽管MU技术快速发展，但其潜在漏洞尚未充分研究，可能导致隐私泄露。现有攻击方法需访问原始模型，违背了MU的隐私保护目标。

Method: 提出MRA框架，利用教师-学生知识蒸馏架构，将未学习模型作为噪声标注器，转化为噪声标签学习问题。

Result: 实验证明MRA能高效恢复遗忘实例的类别信息，为未来MU漏洞研究提供了基准。

Conclusion: 该研究填补了MU漏洞研究的空白，为隐私保护提供了新视角。

Abstract: Machine Unlearning (MU) technology facilitates the removal of the influence
of specific data instances from trained models on request. Despite rapid
advancements in MU technology, its vulnerabilities are still underexplored,
posing potential risks of privacy breaches through leaks of ostensibly
unlearned information. Current limited research on MU attacks requires access
to original models containing privacy data, which violates the critical
privacy-preserving objective of MU. To address this gap, we initiate an
innovative study on recalling the forgotten class memberships from unlearned
models (ULMs) without requiring access to the original one. Specifically, we
implement a Membership Recall Attack (MRA) framework with a teacher-student
knowledge distillation architecture, where ULMs serve as noisy labelers to
transfer knowledge to student models. Then, it is translated into a Learning
with Noisy Labels (LNL) problem for inferring the correct labels of the
forgetting instances. Extensive experiments on state-of-the-art MU methods with
multiple real datasets demonstrate that the proposed MRA strategy exhibits high
efficacy in recovering class memberships of unlearned instances. As a result,
our study and evaluation have established a benchmark for future research on MU
vulnerabilities.

</details>


### [73] [COLUR: Confidence-Oriented Learning, Unlearning and Relearning with Noisy-Label Data for Model Restoration and Refinement](https://arxiv.org/abs/2506.19496)
*Zhihao Sui,Liang Hu,Jian Cao,Usman Naseem,Zhongyuan Lai,Qi Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为COLUR的框架，通过置信度导向的学习、遗忘和重新学习，恢复因标签噪声导致的模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 大型深度学习模型在噪声标签数据上性能显著下降，目前缺乏有效的恢复方法。受神经科学中遗忘机制的启发，提出恢复和优化模型性能的解决方案。

Method: 采用COLUR框架，结合协同训练架构，先遗忘标签噪声的影响，再优化模型对每个标签的置信度以重新学习。

Result: 在四个真实数据集上的实验表明，COLUR在模型恢复和优化后始终优于其他先进方法。

Conclusion: COLUR框架有效解决了噪声标签导致的模型性能下降问题，为模型恢复和优化提供了新思路。

Abstract: Large deep learning models have achieved significant success in various
tasks. However, the performance of a model can significantly degrade if it is
needed to train on datasets with noisy labels with misleading or ambiguous
information. To date, there are limited investigations on how to restore
performance when model degradation has been incurred by noisy label data.
Inspired by the ``forgetting mechanism'' in neuroscience, which enables
accelerating the relearning of correct knowledge by unlearning the wrong
knowledge, we propose a robust model restoration and refinement (MRR) framework
COLUR, namely Confidence-Oriented Learning, Unlearning and Relearning.
Specifically, we implement COLUR with an efficient co-training architecture to
unlearn the influence of label noise, and then refine model confidence on each
label for relearning. Extensive experiments are conducted on four real datasets
and all evaluation results show that COLUR consistently outperforms other SOTA
methods after MRR.

</details>


### [74] [Dimension Reduction for Symbolic Regression](https://arxiv.org/abs/2506.19537)
*Paul Kahlmeyer,Markus Fischer,Joachim Giesen*

Main category: cs.LG

TL;DR: 该论文提出了一种通过搜索小替换表达式并测试其有效性来减少符号回归问题中变量数量的方法，从而提升符号回归算法的性能。


<details>
  <summary>Details</summary>
Motivation: 自然出现的符号公式中变量通常以固定组合出现，通过替换这些组合可以减少变量数量，但找到有效替换具有挑战性。

Method: 通过搜索小替换表达式空间并测试其功能性依赖的有效性，实现迭代降维。

Result: 该方法能可靠识别有效替换，并显著提升多种先进符号回归算法的性能。

Conclusion: 提出的迭代降维方法可有效减少变量数量，提升符号回归算法的表现。

Abstract: Solutions of symbolic regression problems are expressions that are composed
of input variables and operators from a finite set of function symbols. One
measure for evaluating symbolic regression algorithms is their ability to
recover formulae, up to symbolic equivalence, from finite samples. Not
unexpectedly, the recovery problem becomes harder when the formula gets more
complex, that is, when the number of variables and operators gets larger.
Variables in naturally occurring symbolic formulas often appear only in fixed
combinations. This can be exploited in symbolic regression by substituting one
new variable for the combination, effectively reducing the number of variables.
However, finding valid substitutions is challenging. Here, we address this
challenge by searching over the expression space of small substitutions and
testing for validity. The validity test is reduced to a test of functional
dependence. The resulting iterative dimension reduction procedure can be used
with any symbolic regression approach. We show that it reliably identifies
valid substitutions and significantly boosts the performance of different types
of state-of-the-art symbolic regression algorithms.

</details>


### [75] [Overtuning in Hyperparameter Optimization](https://arxiv.org/abs/2506.19540)
*Lennart Schneider,Bernd Bischl,Matthias Feurer*

Main category: cs.LG

TL;DR: 论文探讨了超参数优化（HPO）中可能出现的过度调优（overtuning）现象，即过度优化验证误差可能导致泛化性能下降，并通过大规模数据分析验证其普遍性和严重性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证过度优化验证误差是否会导致类似模型训练中的过拟合现象，即HPO层面的过拟合（overtuning），并填补其在HPO和AutoML文献中的研究空白。

Method: 方法包括对HPO基准数据的大规模重新分析，评估overtuning的普遍性和严重性，并分析影响因素如性能指标、重采样策略等。

Result: 结果显示overtuning比预期更常见，通常轻微但偶尔严重，约10%的情况下会导致选择看似最优但泛化性能更差的配置。

Conclusion: 结论强调需提高对overtuning的认识，特别是在小数据场景下，并建议进一步研究缓解策略。

Abstract: Hyperparameter optimization (HPO) aims to identify an optimal hyperparameter
configuration (HPC) such that the resulting model generalizes well to unseen
data. As the expected generalization error cannot be optimized directly, it is
estimated with a resampling strategy, such as holdout or cross-validation. This
approach implicitly assumes that minimizing the validation error leads to
improved generalization. However, since validation error estimates are
inherently stochastic and depend on the resampling strategy, a natural question
arises: Can excessive optimization of the validation error lead to overfitting
at the HPO level, akin to overfitting in model training based on empirical risk
minimization? In this paper, we investigate this phenomenon, which we term
overtuning, a form of overfitting specific to HPO. Despite its practical
relevance, overtuning has received limited attention in the HPO and AutoML
literature. We provide a formal definition of overtuning and distinguish it
from related concepts such as meta-overfitting. We then conduct a large-scale
reanalysis of HPO benchmark data to assess the prevalence and severity of
overtuning. Our results show that overtuning is more common than previously
assumed, typically mild but occasionally severe. In approximately 10% of cases,
overtuning leads to the selection of a seemingly optimal HPC with worse
generalization error than the default or first configuration tried. We further
analyze how factors such as performance metric, resampling strategy, dataset
size, learning algorithm, and HPO method affect overtuning and discuss
mitigation strategies. Our results highlight the need to raise awareness of
overtuning, particularly in the small-data regime, indicating that further
mitigation strategies should be studied.

</details>


### [76] [Discovering Symmetries of ODEs by Symbolic Regression](https://arxiv.org/abs/2506.19550)
*Paul Kahlmeyer,Niklas Merk,Joachim Giesen*

Main category: cs.LG

TL;DR: 论文提出了一种基于搜索的符号回归方法，用于寻找ODE的李点对称生成器，解决了现有计算机代数系统难以解决的问题。


<details>
  <summary>Details</summary>
Motivation: 理解动力系统行为需要求解ODE，但非线性系统的自动化求解仍具挑战性，尤其是寻找李点对称性。

Method: 采用搜索式符号回归方法，寻找ODE的李点对称生成器。

Result: 该方法能够找到现有计算机代数系统无法发现的对称性。

Conclusion: 符号回归方法在寻找ODE对称性方面具有潜力，为自动化求解提供了新思路。

Abstract: Solving systems of ordinary differential equations (ODEs) is essential when
it comes to understanding the behavior of dynamical systems. Yet, automated
solving remains challenging, in particular for nonlinear systems. Computer
algebra systems (CASs) provide support for solving ODEs by first simplifying
them, in particular through the use of Lie point symmetries. Finding these
symmetries is, however, itself a difficult problem for CASs. Recent works in
symbolic regression have shown promising results for recovering symbolic
expressions from data. Here, we adapt search-based symbolic regression to the
task of finding generators of Lie point symmetries. With this approach, we can
find symmetries of ODEs that existing CASs cannot find.

</details>


### [77] [ConCM: Consistency-Driven Calibration and Matching for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.19558)
*QinZhe Wang,Zixuan Chen,Keke Huang,Xiu Su,Chunhua Yang,Chang Xu*

Main category: cs.LG

TL;DR: 论文提出了一种一致性驱动的校准与匹配框架（ConCM），通过优化特征-结构双重一致性，解决了Few-Shot类增量学习（FSCIL）中的知识冲突问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过预留空间适应新类，但原型偏差和结构固定性限制了嵌入空间的表达能力。论文探索了特征-结构双重一致性的优化，以克服这些问题。

Method: 设计了基于海马体联想记忆的记忆感知原型校准，提取基类的广义语义属性并整合到新类中；提出动态结构匹配，自适应地对齐特征到会话特定的最优流形空间。

Result: 在mini-ImageNet和CUB200等大规模FSCIL基准测试中，ConCM实现了最先进性能，增量会话的调和准确率分别超过当前最优方法3.20%和3.68%。

Conclusion: ConCM通过几何最优性和最大匹配性，克服了对类数量先验的需求，显著提升了FSCIL的性能。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) requires models to adapt to novel
classes with limited supervision while preserving learned knowledge. Existing
prospective learning-based space construction methods reserve space to
accommodate novel classes. However, prototype deviation and structure fixity
limit the expressiveness of the embedding space. In contrast to fixed space
reservation, we explore the optimization of feature-structure dual consistency
and propose a Consistency-driven Calibration and Matching Framework (ConCM)
that systematically mitigate the knowledge conflict inherent in FSCIL.
Specifically, inspired by hippocampal associative memory, we design a
memory-aware prototype calibration that extracts generalized semantic
attributes from base classes and reintegrates them into novel classes to
enhance the conceptual center consistency of features. Further, we propose
dynamic structure matching, which adaptively aligns the calibrated features to
a session-specific optimal manifold space, ensuring cross-session structure
consistency. Theoretical analysis shows that our method satisfies both
geometric optimality and maximum matching, thereby overcoming the need for
class-number priors. On large-scale FSCIL benchmarks including mini-ImageNet
and CUB200, ConCM achieves state-of-the-art performance, surpassing current
optimal method by 3.20% and 3.68% in harmonic accuracy of incremental sessions.

</details>


### [78] [FAF: A Feature-Adaptive Framework for Few-Shot Time Series Forecasting](https://arxiv.org/abs/2506.19567)
*Pengpeng Ouyang,Dong Chen,Tong Yang,Shuo Feng,Zhao Jin,Mingliang Xu*

Main category: cs.LG

TL;DR: 提出了一个特征自适应时间序列预测框架（FAF），通过结合通用知识和任务特定特征，解决了多任务和少样本时间序列预测中的历史数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法因忽视任务间的通用和特定特征，导致历史数据不足。FAF旨在通过结合通用和特定特征提升预测性能。

Method: FAF包含通用知识模块（GKM）、任务特定模块（TSM）和排序模块（RM）。GKM通过元学习提取通用特征，TSM学习任务特定特征，RM动态选择最相关的TSM区域。

Result: 在五个真实数据集上，FAF显著优于基线方法，尤其在CO₂排放数据集上比最佳基线iTransformer提升41.81%。

Conclusion: FAF通过结合通用和特定特征，实现了在少样本条件下的鲁棒和个性化预测。

Abstract: Multi-task and few-shot time series forecasting tasks are commonly
encountered in scenarios such as the launch of new products in different
cities. However, traditional time series forecasting methods suffer from
insufficient historical data, which stems from a disregard for the generalized
and specific features among different tasks. For the aforementioned challenges,
we propose the Feature-Adaptive Time Series Forecasting Framework (FAF), which
consists of three key components: the Generalized Knowledge Module (GKM), the
Task-Specific Module (TSM), and the Rank Module (RM). During training phase,
the GKM is updated through a meta-learning mechanism that enables the model to
extract generalized features across related tasks. Meanwhile, the TSM is
trained to capture diverse local dynamics through multiple functional regions,
each of which learns specific features from individual tasks. During testing
phase, the RM dynamically selects the most relevant functional region from the
TSM based on input sequence features, which is then combined with the
generalized knowledge learned by the GKM to generate accurate forecasts. This
design enables FAF to achieve robust and personalized forecasting even with
sparse historical observations We evaluate FAF on five diverse real-world
datasets under few-shot time series forecasting settings. Experimental results
demonstrate that FAF consistently outperforms baselines that include three
categories of time series forecasting methods. In particular, FAF achieves a
41.81\% improvement over the best baseline, iTransformer, on the CO$_2$
emissions dataset.

</details>


### [79] [ConStellaration: A dataset of QI-like stellarator plasma boundaries and optimization benchmarks](https://arxiv.org/abs/2506.19583)
*Santiago A. Cadena,Andrea Merlo,Emanuel Laude,Alexander Bauer,Atul Agrawal,Maria Pascu,Marija Savtchouk,Enrico Guiraud,Lukas Bonauer,Stuart Hudson,Markus Kaiser*

Main category: cs.LG

TL;DR: 论文介绍了一个开放数据集，用于优化准等动力（QI）恒星器设计，包含几何优化、简单构建和多目标优化基准，旨在降低研究门槛并加速聚变能源发展。


<details>
  <summary>Details</summary>
Motivation: 恒星器设计需要高维优化和昂贵模拟，缺乏标准化问题和数据集限制了社区进展。QI恒星器因其抗干扰性被视为商业聚变的有前景路径。

Method: 通过采样多种QI场并优化等离子体边界形状，生成数据集。提出三个复杂度递增的优化基准，并提供参考代码和基线。

Result: 数据集包含多样化的QI恒星器形状、平衡状态和性能指标。训练模型可高效生成新配置，无需昂贵物理模拟。

Conclusion: 开放数据集和基准问题旨在降低研究门槛，促进跨学科合作，加速聚变能源实现。

Abstract: Stellarators are magnetic confinement devices under active development to
deliver steady-state carbon-free fusion energy. Their design involves a
high-dimensional, constrained optimization problem that requires expensive
physics simulations and significant domain expertise. Recent advances in plasma
physics and open-source tools have made stellarator optimization more
accessible. However, broader community progress is currently bottlenecked by
the lack of standardized optimization problems with strong baselines and
datasets that enable data-driven approaches, particularly for quasi-isodynamic
(QI) stellarator configurations, considered as a promising path to commercial
fusion due to their inherent resilience to current-driven disruptions. Here, we
release an open dataset of diverse QI-like stellarator plasma boundary shapes,
paired with their ideal magnetohydrodynamic (MHD) equilibria and performance
metrics. We generated this dataset by sampling a variety of QI fields and
optimizing corresponding stellarator plasma boundaries. We introduce three
optimization benchmarks of increasing complexity: (1) a single-objective
geometric optimization problem, (2) a "simple-to-build" QI stellarator, and (3)
a multi-objective ideal-MHD stable QI stellarator that investigates trade-offs
between compactness and coil simplicity. For every benchmark, we provide
reference code, evaluation scripts, and strong baselines based on classical
optimization techniques. Finally, we show how learned models trained on our
dataset can efficiently generate novel, feasible configurations without
querying expensive physics oracles. By openly releasing the dataset along with
benchmark problems and baselines, we aim to lower the entry barrier for
optimization and machine learning researchers to engage in stellarator design
and to accelerate cross-disciplinary progress toward bringing fusion energy to
the grid.

</details>


### [80] [Training Flexible Models of Genetic Variant Effects from Functional Annotations using Accelerated Linear Algebra](https://arxiv.org/abs/2506.19598)
*Alan N. Amin,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文提出DeepWAS方法，利用快速线性代数技术训练大型神经网络模型，优化基因组关联研究的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统基因组关联研究中因计算瓶颈导致模型规模受限的问题，提升预测准确性。

Method: 利用现代快速线性代数技术开发DeepWAS，训练大型神经网络模型，优化统计模型的全似然。

Result: 发现大型模型在全似然方法下性能显著提升，而在传统统计方法下无改进。

Conclusion: DeepWAS能通过更多特征和更大模型提升预测性能，有望改善疾病预测和治疗靶点识别。

Abstract: To understand how genetic variants in human genomes manifest in phenotypes --
traits like height or diseases like asthma -- geneticists have sequenced and
measured hundreds of thousands of individuals. Geneticists use this data to
build models that predict how a genetic variant impacts phenotype given genomic
features of the variant, like DNA accessibility or the presence of nearby
DNA-bound proteins. As more data and features become available, one might
expect predictive models to improve. Unfortunately, training these models is
bottlenecked by the need to solve expensive linear algebra problems because
variants in the genome are correlated with nearby variants, requiring inversion
of large matrices. Previous methods have therefore been restricted to fitting
small models, and fitting simplified summary statistics, rather than the full
likelihood of the statistical model. In this paper, we leverage modern fast
linear algebra techniques to develop DeepWAS (Deep genome Wide Association
Studies), a method to train large and flexible neural network predictive models
to optimize likelihood. Notably, we find that larger models only improve
performance when using our full likelihood approach; when trained by fitting
traditional summary statistics, larger models perform no better than small
ones. We find larger models trained on more features make better predictions,
potentially improving disease predictions and therapeutic target
identification.

</details>


### [81] [Beyond Static Models: Hypernetworks for Adaptive and Generalizable Forecasting in Complex Parametric Dynamical Systems](https://arxiv.org/abs/2506.19609)
*Pantelis R. Vlachas,Konstantinos Vlachas,Eleni Chatzi*

Main category: cs.LG

TL;DR: PHLieNet框架通过全局参数空间映射和动态传播网络权重映射，实现了跨参数化系统的统一建模，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决参数变化导致模型行为差异大、难以泛化的问题。

Method: 引入PHLieNet，学习参数空间到非线性嵌入的全局映射，并通过超网络生成目标网络权重。

Result: 在时间外推和参数空间内插/外推任务中表现优异，优于或匹配现有方法。

Conclusion: PHLieNet能有效捕捉跨参数化系统的动态行为，具有广泛适用性。

Abstract: Dynamical systems play a key role in modeling, forecasting, and
decision-making across a wide range of scientific domains. However, variations
in system parameters, also referred to as parametric variability, can lead to
drastically different model behavior and output, posing challenges for
constructing models that generalize across parameter regimes. In this work, we
introduce the Parametric Hypernetwork for Learning Interpolated Networks
(PHLieNet), a framework that simultaneously learns: (a) a global mapping from
the parameter space to a nonlinear embedding and (b) a mapping from the
inferred embedding to the weights of a dynamics propagation network. The
learned embedding serves as a latent representation that modulates a base
network, termed the hypernetwork, enabling it to generate the weights of a
target network responsible for forecasting the system's state evolution
conditioned on the previous time history. By interpolating in the space of
models rather than observations, PHLieNet facilitates smooth transitions across
parameterized system behaviors, enabling a unified model that captures the
dynamic behavior across a broad range of system parameterizations. The
performance of the proposed technique is validated in a series of dynamical
systems with respect to its ability to extrapolate in time and interpolate and
extrapolate in the parameter space, i.e., generalize to dynamics that were
unseen during training. In all cases, our approach outperforms or matches
state-of-the-art baselines in both short-term forecast accuracy and in
capturing long-term dynamical features, such as attractor statistics.

</details>


### [82] [Scaling Up Unbiased Search-based Symbolic Regression](https://arxiv.org/abs/2506.19626)
*Paul Kahlmeyer,Joachim Giesen,Michael Habeck,Henrik Voigt*

Main category: cs.LG

TL;DR: 论文探讨了符号回归任务中通过系统搜索小表达式空间的方法，相比现有方法能更准确、稳健地找到可解释的函数。


<details>
  <summary>Details</summary>
Motivation: 符号回归的目标不仅是预测误差小，还需函数可解释。传统方法通过基函数展开优化参数，但难以满足可解释性要求。

Method: 提出系统搜索小表达式空间的方法，利用表达式可分解为少量子表达式的结构特点。

Result: 在基准数据集上，该方法比现有符号回归方法更准确、稳健，能更好地恢复真实的符号表达式。

Conclusion: 系统搜索小表达式空间是符号回归的有效方法，优于现有技术。

Abstract: In a regression task, a function is learned from labeled data to predict the
labels at new data points. The goal is to achieve small prediction errors. In
symbolic regression, the goal is more ambitious, namely, to learn an
interpretable function that makes small prediction errors. This additional goal
largely rules out the standard approach used in regression, that is, reducing
the learning problem to learning parameters of an expansion of basis functions
by optimization. Instead, symbolic regression methods search for a good
solution in a space of symbolic expressions. To cope with the typically vast
search space, most symbolic regression methods make implicit, or sometimes even
explicit, assumptions about its structure. Here, we argue that the only obvious
structure of the search space is that it contains small expressions, that is,
expressions that can be decomposed into a few subexpressions. We show that
systematically searching spaces of small expressions finds solutions that are
more accurate and more robust against noise than those obtained by
state-of-the-art symbolic regression methods. In particular, systematic search
outperforms state-of-the-art symbolic regressors in terms of its ability to
recover the true underlying symbolic expressions on established benchmark data
sets.

</details>


### [83] [Why Uncertainty Calibration Matters for Reliable Perturbation-based Explanations](https://arxiv.org/abs/2506.19630)
*Thomas Decker,Volker Tresp,Florian Buettner*

Main category: cs.LG

TL;DR: 论文研究了不确定性校准与基于扰动的解释之间的关系，提出了一种新方法ReCalX，通过重新校准模型来提高解释质量。


<details>
  <summary>Details</summary>
Motivation: 基于扰动的解释在机器学习中广泛使用，但其可靠性因模型在特定扰动下的未知行为而受到质疑。

Method: 引入ReCalX方法，重新校准模型以改进基于扰动的解释，同时保持原始预测。

Result: 实验表明，ReCalX生成的解释更符合人类感知和实际物体位置。

Conclusion: ReCalX通过校准模型提高了基于扰动的解释的可靠性。

Abstract: Perturbation-based explanations are widely utilized to enhance the
transparency of modern machine-learning models. However, their reliability is
often compromised by the unknown model behavior under the specific
perturbations used. This paper investigates the relationship between
uncertainty calibration - the alignment of model confidence with actual
accuracy - and perturbation-based explanations. We show that models frequently
produce unreliable probability estimates when subjected to
explainability-specific perturbations and theoretically prove that this
directly undermines explanation quality. To address this, we introduce ReCalX,
a novel approach to recalibrate models for improved perturbation-based
explanations while preserving their original predictions. Experiments on
popular computer vision models demonstrate that our calibration strategy
produces explanations that are more aligned with human perception and actual
object locations.

</details>


### [84] [Hierarchical Time Series Forecasting Via Latent Mean Encoding](https://arxiv.org/abs/2506.19633)
*Alessandro Salatiello,Stefan Birr,Manuel Kunz*

Main category: cs.LG

TL;DR: 提出了一种新的层次架构，用于跨时间尺度的目标变量预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间层次预测中跨粗粒度和细粒度时间尺度的行为预测问题，以优化商业决策。

Method: 采用模块化设计，专门针对不同时间聚合级别进行预测，并通过隐藏层编码目标变量的平均行为。

Result: 在M5数据集上验证，性能优于TSMixer等现有方法。

Conclusion: 新架构能够准确且一致地预测目标时间层次，适用于实际商业应用。

Abstract: Coherently forecasting the behaviour of a target variable across both coarse
and fine temporal scales is crucial for profit-optimized decision-making in
several business applications, and remains an open research problem in temporal
hierarchical forecasting. Here, we propose a new hierarchical architecture that
tackles this problem by leveraging modules that specialize in forecasting the
different temporal aggregation levels of interest. The architecture, which
learns to encode the average behaviour of the target variable within its hidden
layers, makes accurate and coherent forecasts across the target temporal
hierarchies. We validate our architecture on the challenging, real-world M5
dataset and show that it outperforms established methods, such as the TSMixer
model.

</details>


### [85] [Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective from Model](https://arxiv.org/abs/2506.19643)
*Shuncheng He,Hongchang Zhang,Jianzhun Shao,Yuhang Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文通过理论分析建立了离线强化学习（RL）中批量数据与算法性能之间的联系，提出了一种无监督数据生成方法（UDG），以解决任务无关设置下的性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因分布外问题性能受限，现有研究多关注算法限制，而忽略了批量数据的影响。本文旨在填补这一空白。

Method: 从模型化离线RL优化的角度，理论分析了批量数据与性能的关系，并设计了无监督数据生成方法（UDG）。

Result: 理论表明，行为策略与最优策略生成的状态-动作对分布距离决定了性能差距；UDG在任务无关设置下表现优于监督数据生成。

Conclusion: UDG通过无监督数据生成和选择，有效提升了离线RL在未知任务中的性能。

Abstract: Offline reinforcement learning (RL) recently gains growing interests from RL
researchers. However, the performance of offline RL suffers from the
out-of-distribution problem, which can be corrected by feedback in online RL.
Previous offline RL research focuses on restricting the offline algorithm in
in-distribution even in-sample action sampling. In contrast, fewer work pays
attention to the influence of the batch data. In this paper, we first build a
bridge over the batch data and the performance of offline RL algorithms
theoretically, from the perspective of model-based offline RL optimization. We
draw a conclusion that, with mild assumptions, the distance between the
state-action pair distribution generated by the behavioural policy and the
distribution generated by the optimal policy, accounts for the performance gap
between the policy learned by model-based offline RL and the optimal policy.
Secondly, we reveal that in task-agnostic settings, a series of policies
trained by unsupervised RL can minimize the worst-case regret in the
performance gap. Inspired by the theoretical conclusions, UDG (Unsupervised
Data Generation) is devised to generate data and select proper data for offline
training under tasks-agnostic settings. Empirical results demonstrate that UDG
can outperform supervised data generation on solving unknown tasks.

</details>


### [86] [Tensor-Parallelism with Partially Synchronized Activations](https://arxiv.org/abs/2506.19645)
*Itay Lamprecht,Asaf Karnieli,Yair Hanani,Niv Giladi,Daniel Soudry*

Main category: cs.LG

TL;DR: 论文提出了一种名为CAAT-Net的通信感知架构，通过减少激活同步的通信需求，显著降低了大型语言模型（LLMs）在张量并行训练和推理中的带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在张量并行训练和推理中需要大量通信同步激活，导致带宽需求高。

Method: 通过少量调整现有实践，提出了CAAT-Net架构，减少激活同步的通信需求。

Result: 在1B和7B参数的模型上，CAAT-Net减少了50%的张量并行通信，且预训练精度无明显下降。

Conclusion: CAAT-Net能有效加速训练和推理任务，降低通信成本。

Abstract: Training and inference of Large Language Models (LLMs) with
tensor-parallelism requires substantial communication to synchronize
activations. Our findings suggest that with a few minor adjustments to current
practices, LLMs can be trained without fully synchronizing activations,
reducing bandwidth demands. We name this "Communication-Aware Architecture for
Tensor-parallelism" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,
with a 50% reduction in tensor-parallel communication and no significant drop
in pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates
both training and inference workloads.

</details>


### [87] [Model Guidance via Robust Feature Attribution](https://arxiv.org/abs/2506.19680)
*Mihnea Ghitu,Matthew Wicker,Vihari Piratla*

Main category: cs.LG

TL;DR: 提出一种新方法，通过优化解释鲁棒性和减少捷径学习，显著降低测试错误率20%。


<details>
  <summary>Details</summary>
Motivation: 防止模型依赖无关或误导性特征（捷径特征），避免实际应用中的危害。

Method: 提出简化目标，同时优化解释鲁棒性和捷径学习缓解，理论证明其有效性。

Result: 在实验中，方法比现有技术减少20%测试错误率，并扩展到NLP任务。

Conclusion: 方法有效且实用，代码开源。

Abstract: Controlling the patterns a model learns is essential to preventing reliance
on irrelevant or misleading features. Such reliance on irrelevant features,
often called shortcut features, has been observed across domains, including
medical imaging and natural language processing, where it may lead to
real-world harms. A common mitigation strategy leverages annotations (provided
by humans or machines) indicating which features are relevant or irrelevant.
These annotations are compared to model explanations, typically in the form of
feature salience, and used to guide the loss function during training.
Unfortunately, recent works have demonstrated that feature salience methods are
unreliable and therefore offer a poor signal to optimize. In this work, we
propose a simplified objective that simultaneously optimizes for explanation
robustness and mitigation of shortcut learning. Unlike prior objectives with
similar aims, we demonstrate theoretically why our approach ought to be more
effective. Across a comprehensive series of experiments, we show that our
approach consistently reduces test-time misclassifications by 20% compared to
state-of-the-art methods. We also extend prior experimental settings to include
natural language processing tasks. Additionally, we conduct novel ablations
that yield practical insights, including the relative importance of annotation
quality over quantity. Code for our method and experiments is available at:
https://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution.

</details>


### [88] [When Can We Reuse a Calibration Set for Multiple Conformal Predictions?](https://arxiv.org/abs/2506.19689)
*A. A. Balinsky,A. D. Balinsky*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的共形预测方法，通过结合e-共形预测和Hoeffding不等式，减少了重复校准的需求，提高了实用性。


<details>
  <summary>Details</summary>
Motivation: 可靠的量化不确定性对机器学习应用的可信度至关重要。标准共形预测方法需要频繁校准，限制了其实际应用。

Method: 结合e-共形预测和Hoeffding不等式，利用单一校准集生成预测集，并通过CIFAR-10数据集验证。

Result: 实验表明，该方法能在减少校准需求的同时，保持预测集的覆盖率和置信度。

Conclusion: 该方法提高了共形预测的实用性，同时保持了可证明的性能。

Abstract: Reliable uncertainty quantification is crucial for the trustworthiness of
machine learning applications. Inductive Conformal Prediction (ICP) offers a
distribution-free framework for generating prediction sets or intervals with
user-specified confidence. However, standard ICP guarantees are marginal and
typically require a fresh calibration set for each new prediction to maintain
their validity. This paper addresses this practical limitation by demonstrating
how e-conformal prediction, in conjunction with Hoeffding's inequality, can
enable the repeated use of a single calibration set with a high probability of
preserving the desired coverage. Through a case study on the CIFAR-10 dataset,
we train a deep neural network and utilise a calibration set to estimate a
Hoeffding correction. This correction allows us to apply a modified Markov's
inequality, leading to the construction of prediction sets with quantifiable
confidence. Our results illustrate the feasibility of maintaining provable
performance in conformal prediction while enhancing its practicality by
reducing the need for repeated calibration. The code for this work is publicly
available.

</details>


### [89] [Leveraging Lightweight Generators for Memory Efficient Continual Learning](https://arxiv.org/abs/2506.19692)
*Christiaan Lamers,Ahmed Nabil Belbachir,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 提出了一种基于奇异值分解的轻量级生成器，以减少基于记忆的持续学习方法的内存占用，同时有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘可以通过保留所有历史任务数据来缓解，但内存占用成为关键挑战。本文旨在减少内存需求。

Method: 使用奇异值分解（SVD）构建轻量级生成器，增强现有持续学习方法（如A-GEM和Experience Replay）。这些生成器无需训练，仅需一次线性拟合步骤。

Result: 实验结果显示，该方法显著提高了平均准确率，同时大幅减少了内存占用。

Conclusion: 该方法在减少基于记忆的持续学习算法的内存占用方面具有巨大潜力。

Abstract: Catastrophic forgetting can be trivially alleviated by keeping all data from
previous tasks in memory. Therefore, minimizing the memory footprint while
maximizing the amount of relevant information is crucial to the challenge of
continual learning. This paper aims to decrease required memory for
memory-based continuous learning algorithms. We explore the options of
extracting a minimal amount of information, while maximally alleviating
forgetting. We propose the usage of lightweight generators based on Singular
Value Decomposition to enhance existing continual learning methods, such as
A-GEM and Experience Replay. These generators need a minimal amount of memory
while being maximally effective. They require no training time, just a single
linear-time fitting step, and can capture a distribution effectively from a
small number of data samples. Depending on the dataset and network
architecture, our results show a significant increase in average accuracy
compared to the original methods. Our method shows great potential in
minimizing the memory footprint of memory-based continual learning algorithms.

</details>


### [90] [ReBoot: Encrypted Training of Deep Neural Networks with CKKS Bootstrapping](https://arxiv.org/abs/2506.19693)
*Alberto Pirillo,Luca Colombo*

Main category: cs.LG

TL;DR: ReBoot是首个支持全加密、非交互式深度神经网络训练的框架，基于CKKS方案，显著降低计算和内存开销，并在加密训练中达到与明文训练相当的精度。


<details>
  <summary>Details</summary>
Motivation: 数据隐私问题日益突出，需要能在不泄露敏感信息的情况下进行深度学习的解决方案。同态加密（HE）虽能提供端到端数据保护，但现有研究主要集中在加密推理，加密训练因计算复杂性和开销大而受限。

Method: ReBoot基于CKKS方案，提出了一种新的HE兼容神经网络架构，采用局部误差信号设计以减少乘法深度和噪声积累，并利用SIMD操作优化计算和内存开销。此外，通过近似自举技术支持多层感知机的有效训练。

Result: 在图像识别和表格数据任务中，ReBoot的加密训练精度与32位浮点明文训练相当，测试精度比加密逻辑回归高3.27%，比现有加密DNN框架高6.83%，训练延迟降低8.83倍。

Conclusion: ReBoot为全加密DNN训练提供了高效解决方案，适用于机器学习即服务场景，并已开源供科研社区使用。

Abstract: Growing concerns over data privacy underscore the need for deep learning
methods capable of processing sensitive information without compromising
confidentiality. Among privacy-enhancing technologies, Homomorphic Encryption
(HE) stands out by providing post-quantum cryptographic security and end-to-end
data protection, safeguarding data even during computation. While Deep Neural
Networks (DNNs) have gained attention in HE settings, their use has largely
been restricted to encrypted inference. Prior research on encrypted training
has primarily focused on logistic regression or has relied on multi-party
computation to enable model fine-tuning. This stems from the substantial
computational overhead and algorithmic complexity involved in DNNs training
under HE. In this paper, we present ReBoot, the first framework to enable fully
encrypted and non-interactive training of DNNs. Built upon the CKKS scheme,
ReBoot introduces a novel HE-compliant neural network architecture based on
local error signals, specifically designed to minimize multiplicative depth and
reduce noise accumulation. ReBoot employs a tailored packing strategy that
leverages real-number arithmetic via SIMD operations, significantly lowering
both computational and memory overhead. Furthermore, by integrating approximate
bootstrapping, ReBoot learning algorithm supports effective training of
arbitrarily deep multi-layer perceptrons, making it well-suited for machine
learning as-a-service. ReBoot is evaluated on both image recognition and
tabular benchmarks, achieving accuracy comparable to 32-bit floating-point
plaintext training while enabling fully encrypted training. It improves test
accuracy by up to +3.27% over encrypted logistic regression, and up to +6.83%
over existing encrypted DNN frameworks, while reducing training latency by up
to 8.83x. ReBoot is made available to the scientific community as a public
repository.

</details>


### [91] [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models](https://arxiv.org/abs/2506.19697)
*Jungwoo Park,Taewhoo Lee,Chanwoong Yoon,Hyeon Hwang,Jaewoo Kang*

Main category: cs.LG

TL;DR: 论文提出Outlier-Safe Pre-Training (OSP)，通过预防激活异常值提升LLM量化性能，训练效率高且效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）中的极端激活异常值严重影响量化性能，阻碍高效部署，现有方法难以解决。

Method: OSP结合Muon优化器、Single-Scale RMSNorm和可学习嵌入投影，主动预防异常值形成。

Result: 在1.4B参数模型上验证，4位量化下平均得分35.7（对比Adam的26.5），训练开销仅2%。

Conclusion: 异常值并非LLM固有特性，而是训练策略的结果，OSP为高效部署提供了新途径。

Abstract: Extreme activation outliers in Large Language Models (LLMs) critically
degrade quantization performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit quantization, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM quantization behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.

</details>


### [92] [Learning-aided Bigraph Matching Approach to Multi-Crew Restoration of Damaged Power Networks Coupled with Road Transportation Networks](https://arxiv.org/abs/2506.19703)
*Nathan Maurer,Harshal Kaushik,Roshni Anna Jacob,Jie Zhang,Souma Chowdhury*

Main category: cs.LG

TL;DR: 提出了一种基于图强化学习的资源分配方法，用于关键基础设施网络的快速恢复，结合双图匹配和高效模拟环境，显著优于随机和优化方法。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施网络在自然灾害等中断后的恢复速度和功能恢复程度至关重要，需要高效的资源分配和修复顺序规划。

Method: 将运输网络和电网合并为异构图，利用图强化学习设计激励函数，结合双图匹配和两种学习技术（PPO和Neuroevolution）进行任务分配。

Result: 在IEEE 8500总线测试网络中，该方法比随机策略性能提升3倍，计算时间显著优于优化方法，且恢复效果更好。

Conclusion: 提出的方法具有通用性和可扩展性，适用于不同规模的网络中断场景。

Abstract: The resilience of critical infrastructure networks (CINs) after disruptions,
such as those caused by natural hazards, depends on both the speed of
restoration and the extent to which operational functionality can be regained.
Allocating resources for restoration is a combinatorial optimal planning
problem that involves determining which crews will repair specific network
nodes and in what order. This paper presents a novel graph-based formulation
that merges two interconnected graphs, representing crew and transportation
nodes and power grid nodes, into a single heterogeneous graph. To enable
efficient planning, graph reinforcement learning (GRL) is integrated with
bigraph matching. GRL is utilized to design the incentive function for
assigning crews to repair tasks based on the graph-abstracted state of the
environment, ensuring generalization across damage scenarios. Two learning
techniques are employed: a graph neural network trained using Proximal Policy
Optimization and another trained via Neuroevolution. The learned incentive
functions inform a bipartite graph that links crews to repair tasks, enabling
weighted maximum matching for crew-to-task allocations. An efficient simulation
environment that pre-computes optimal node-to-node path plans is used to train
the proposed restoration planning methods. An IEEE 8500-bus power distribution
test network coupled with a 21 square km transportation network is used as the
case study, with scenarios varying in terms of numbers of damaged nodes,
depots, and crews. Results demonstrate the approach's generalizability and
scalability across scenarios, with learned policies providing 3-fold better
performance than random policies, while also outperforming optimization-based
solutions in both computation time (by several orders of magnitude) and power
restored.

</details>


### [93] [Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low CFG Scales](https://arxiv.org/abs/2506.19713)
*Seyedmorteza Sadat,Tobias Vontobel,Farnood Salehi,Romann M. Weber*

Main category: cs.LG

TL;DR: 论文提出了一种频率解耦引导（FDG）方法，通过分析分类器无关引导（CFG）在频域的影响，发现低频和高频对生成质量的不同作用，并分别调整其引导强度，从而提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 尽管CFG在实践中非常有效，但其提升生成质量、细节和提示对齐的机制尚未完全理解。论文旨在通过频域分析揭示CFG的作用机制，并提出改进方法。

Method: 论文分析了CFG在频域的影响，发现低频引导控制全局结构和条件对齐，高频引导增强视觉保真度。基于此，提出了FDG方法，分别调整低频和高频的引导强度。

Result: 实验表明，FDG在低引导强度下提升图像质量，在高引导强度下避免过饱和和多样性损失，显著改善了FID和召回率。

Conclusion: FDG作为一种即插即用的方法，优于标准CFG，能够同时提升样本保真度和多样性。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern
conditional diffusion models. Although highly effective in practice, the
underlying mechanisms by which CFG enhances quality, detail, and prompt
alignment are not fully understood. We present a novel perspective on CFG by
analyzing its effects in the frequency domain, showing that low and high
frequencies have distinct impacts on generation quality. Specifically,
low-frequency guidance governs global structure and condition alignment, while
high-frequency guidance mainly enhances visual fidelity. However, applying a
uniform scale across all frequencies -- as is done in standard CFG -- leads to
oversaturation and reduced diversity at high scales and degraded visual quality
at low scales. Based on these insights, we propose frequency-decoupled guidance
(FDG), an effective approach that decomposes CFG into low- and high-frequency
components and applies separate guidance strengths to each component. FDG
improves image quality at low guidance scales and avoids the drawbacks of high
CFG scales by design. Through extensive experiments across multiple datasets
and models, we demonstrate that FDG consistently enhances sample fidelity while
preserving diversity, leading to improved FID and recall compared to CFG,
establishing our method as a plug-and-play alternative to standard
classifier-free guidance.

</details>


### [94] [Geometric-Aware Variational Inference: Robust and Adaptive Regularization with Directional Weight Uncertainty](https://arxiv.org/abs/2506.19726)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: CAP是一种变分框架，通过von Mises-Fisher分布直接在单位超球面上建模权重不确定性，显著提升了模型的校准性。


<details>
  <summary>Details</summary>
Motivation: 现有变分推断方法在权重空间中使用各向同性高斯近似，无法匹配网络的固有几何结构。

Method: 引入CAP框架，利用von Mises-Fisher分布建模权重不确定性，并通过解析推导将vMF浓度参数与激活噪声方差联系起来。

Result: 在CIFAR-10上，CAP显著改善了模型校准，预期校准误差降低了5.6倍。

Conclusion: CAP为深度学习中的不确定性量化提供了理论支持且实用的方法。

Abstract: Deep neural networks require principled uncertainty quantification, yet
existing variational inference methods often employ isotropic Gaussian
approximations in weight space that poorly match the network's inherent
geometry. We address this mismatch by introducing Concentration-Adapted
Perturbations (CAP), a variational framework that models weight uncertainties
directly on the unit hypersphere using von Mises-Fisher distributions. Building
on recent work in radial-directional posterior decompositions and spherical
weight constraints, CAP provides the first complete theoretical framework
connecting directional statistics to practical noise regularization in neural
networks. Our key contribution is an analytical derivation linking vMF
concentration parameters to activation noise variance, enabling each layer to
learn its optimal uncertainty level through a novel closed-form KL divergence
regularizer. In experiments on CIFAR-10, CAP significantly improves model
calibration - reducing Expected Calibration Error by 5.6x - while providing
interpretable layer-wise uncertainty profiles. CAP requires minimal
computational overhead and integrates seamlessly into standard architectures,
offering a theoretically grounded yet practical approach to uncertainty
quantification in deep learning.

</details>


### [95] [Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units](https://arxiv.org/abs/2506.19732)
*Shrey Dixit,Kayson Fakhar,Fatemeh Hadaeghi,Patrick Mineault,Konrad P. Kording,Claus C. Hilgetag*

Main category: cs.LG

TL;DR: 提出了一种名为MSA的模型无关框架，用于量化神经网络单元对高维输出的贡献，并展示了其在多种模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法（如SHAP）无法量化神经网络单元对高维输出的贡献，因此需要一种新方法填补这一空白。

Method: 通过系统性地损伤单元组合，MSA生成Shapley Modes，即与模型输出维度一致的单元贡献图。

Result: MSA揭示了正则化如何将计算集中在少数中心，展示了LLM中的语言特定专家，并揭示了GAN中的倒置像素生成层次。

Conclusion: MSA是一种强大的方法，可用于解释、编辑和压缩深度神经网络。

Abstract: Neural networks now generate text, images, and speech with billions of
parameters, producing a need to know how each neural unit contributes to these
high-dimensional outputs. Existing explainable-AI methods, such as SHAP,
attribute importance to inputs, but cannot quantify the contributions of neural
units across thousands of output pixels, tokens, or logits. Here we close that
gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic
game-theoretic framework. By systematically lesioning combinations of units,
MSA yields Shapley Modes, unit-wise contribution maps that share the exact
dimensionality of the model's output. We apply MSA across scales, from
multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative
Adversarial Networks (GAN). The approach demonstrates how regularisation
concentrates computation in a few hubs, exposes language-specific experts
inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.
Together, these results showcase MSA as a powerful approach for interpreting,
editing, and compressing deep neural networks.

</details>


### [96] [DRIFT: Data Reduction via Informative Feature Transformation- Generalization Begins Before Deep Learning starts](https://arxiv.org/abs/2506.19734)
*Ben Keslaki*

Main category: cs.LG

TL;DR: DRIFT是一种基于物理振动分析的数据预处理技术，通过提取数据中的关键特征，显著减少输入维度，提升训练稳定性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习架构在优化方面表现优异，但输入数据的准备成为瓶颈。DRIFT旨在通过物理启发的特征提取，解决信号与噪声混杂的问题。

Method: DRIFT将图像投影到由板的振动模态形状形成的低维基上，生成物理基础的特征集，减少输入维度。

Result: 在MNIST和CIFAR100上，DRIFT仅需50-100个特征即可实现竞争性分类准确率，且对批次大小、网络架构和图像分辨率变化不敏感。

Conclusion: DRIFT展示了物理驱动数据转换的潜力，将焦点从架构设计转向输入优化，显著提升深度学习性能。

Abstract: Modern deep learning architectures excel at optimization, but only after the
data has entered the network. The true bottleneck lies in preparing the right
input: minimal, salient, and structured in a way that reflects the essential
patterns of the data. We propose DRIFT (Data Reduction via Informative Feature
Transformation), a novel preprocessing technique inspired by vibrational
analysis in physical systems, to identify and extract the most resonant modes
of input data prior to training. Unlike traditional models that attempt to
learn amidst both signal and noise, DRIFT mimics physics perception by
emphasizing informative features while discarding irrelevant elements. The
result is a more compact and interpretable representation that enhances
training stability and generalization performance. In DRIFT, images are
projected onto a low-dimensional basis formed by spatial vibration mode shapes
of plates, offering a physically grounded feature set. This enables neural
networks to operate with drastically fewer input dimensions (~ 50 features on
MNIST and less than 100 on CIFAR100) while achieving competitive classification
accuracy. Extensive experiments across MNIST and CIFAR100 demonstrate DRIFT's
superiority over standard pixel-based models and PCA in terms of training
stability, resistance to overfitting, and generalization robustness. Notably,
DRIFT displays minimal sensitivity to changes in batch size, network
architecture, and image resolution, further establishing it as a resilient and
efficient data representation strategy. This work shifts the focus from
architecture engineering to input curation and underscores the power of
physics-driven data transformations in advancing deep learning performance.

</details>


### [97] [Noise Consistency Training: A Native Approach for One-Step Generator in Learning Additional Controls](https://arxiv.org/abs/2506.19741)
*Yihong Luo,Shuchen Xue,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Noise Consistency Training（NCT）的轻量级方法，用于将新的控制信号直接集成到预训练的一步生成器中，无需原始训练图像或重新训练基础扩散模型。


<details>
  <summary>Details</summary>
Motivation: 高效且可控的高质量内容生成是AIGC的核心挑战。现有的一步生成器虽然质量高且计算高效，但难以适应新的控制条件。

Method: NCT通过引入适配器模块和噪声一致性损失，在生成器的噪声空间中对齐生成行为，隐含地引导其遵循新控制。

Result: 实验表明，NCT在单次前向传递中实现了最先进的可控生成，超越了现有的多步和基于蒸馏的方法。

Conclusion: NCT是一种模块化、数据高效且易于部署的方法，显著提升了可控生成的效率和质量。

Abstract: The pursuit of efficient and controllable high-quality content generation
remains a central challenge in artificial intelligence-generated content
(AIGC). While one-step generators, enabled by diffusion distillation
techniques, offer excellent generation quality and computational efficiency,
adapting them to new control conditions--such as structural constraints,
semantic guidelines, or external inputs--poses a significant challenge.
Conventional approaches often necessitate computationally expensive
modifications to the base model and subsequent diffusion distillation. This
paper introduces Noise Consistency Training (NCT), a novel and lightweight
approach to directly integrate new control signals into pre-trained one-step
generators without requiring access to original training images or retraining
the base diffusion model. NCT operates by introducing an adapter module and
employs a noise consistency loss in the noise space of the generator. This loss
aligns the adapted model's generation behavior across noises that are
conditionally dependent to varying degrees, implicitly guiding it to adhere to
the new control. Theoretically, this training objective can be understood as
minimizing the distributional distance between the adapted generator and the
conditional distribution induced by the new conditions. NCT is modular,
data-efficient, and easily deployable, relying only on the pre-trained one-step
generator and a control signal model. Extensive experiments demonstrate that
NCT achieves state-of-the-art controllable generation in a single forward pass,
surpassing existing multi-step and distillation-based methods in both
generation quality and computational efficiency. Code is available at
https://github.com/Luo-Yihong/NCT

</details>


### [98] [On the necessity of adaptive regularisation:Optimal anytime online learning on $\boldsymbol{\ell_p}$-balls](https://arxiv.org/abs/2506.19752)
*Emmeran Johnson,David Martínez-Rubio,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 论文研究了在$\ell_p$-球上的在线凸优化问题，分析了高维和低维设置下的最优遗憾，并探讨了FTRL算法的自适应与非自适应正则化的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同维度设置下在线凸优化的最优遗憾表现，并探索FTRL算法是否可以通过固定正则化实现最优性。

Method: 采用FTRL算法，分析其在不同维度设置下的表现，特别是时间变化的自适应正则化与固定正则化的效果。

Result: 结果表明，对于可分离的正则化器，自适应是必要的，固定正则化器在某一维度设置下会表现不佳。同时，在足够高维的情况下，线性赌博问题的次线性遗憾边界被排除。

Conclusion: 结论指出，自适应正则化是实现FTRL算法在任何维度设置下最优性的关键，而固定正则化器无法在所有维度设置中达到最优表现。

Abstract: We study online convex optimization on $\ell_p$-balls in $\mathbb{R}^d$ for
$p > 2$. While always sub-linear, the optimal regret exhibits a shift between
the high-dimensional setting ($d > T$), when the dimension $d$ is greater than
the time horizon $T$ and the low-dimensional setting ($d \leq T$). We show that
Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is
adaptive to the dimension regime is anytime optimal for all dimension regimes.
Motivated by this, we ask whether it is possible to obtain anytime optimality
of FTRL with fixed non-adaptive regularisation. Our main result establishes
that for separable regularisers, adaptivity in the regulariser is necessary,
and that any fixed regulariser will be sub-optimal in one of the two dimension
regimes. Finally, we provide lower bounds which rule out sub-linear regret
bounds for the linear bandit problem in sufficiently high-dimension for all
$\ell_p$-balls with $p \geq 1$.

</details>


### [99] [Cross-regularization: Adaptive Model Complexity through Validation Gradients](https://arxiv.org/abs/2506.19755)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: 论文提出了一种名为“交叉正则化”的方法，通过验证梯度直接调整正则化参数，解决了模型正则化中手动调参的繁琐问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型正则化需要大量手动调参以平衡复杂度和过拟合，交叉正则化旨在自动化这一过程。

Method: 通过将参数优化分为两部分：训练数据指导特征学习，验证数据调整复杂度控制，并通过噪声注入在神经网络中实现。

Result: 方法在神经网络中表现出高噪声容忍度和架构特定的正则化效果，同时与数据增强、不确定性校准等任务无缝集成。

Conclusion: 交叉正则化不仅简化了正则化过程，还提供了单次训练的高效性，适用于多种场景。

Abstract: Model regularization requires extensive manual tuning to balance complexity
against overfitting. Cross-regularization resolves this tradeoff by directly
adapting regularization parameters through validation gradients during
training. The method splits parameter optimization - training data guides
feature learning while validation data shapes complexity controls - converging
provably to cross-validation optima. When implemented through noise injection
in neural networks, this approach reveals striking patterns: unexpectedly high
noise tolerance and architecture-specific regularization that emerges
organically during training. Beyond complexity control, the framework
integrates seamlessly with data augmentation, uncertainty calibration and
growing datasets while maintaining single-run efficiency through a simple
gradient-based approach.

</details>


### [100] [Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment](https://arxiv.org/abs/2506.19780)
*Yuhui Sun,Xiyao Wang,Zixi Li,Jinman Zhao*

Main category: cs.LG

TL;DR: 提出了一种名为Multi-Preference Lambda-weighted Listwise DPO的新框架，扩展了DPO以支持多目标动态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如RLHF和DPO）在灵活性和动态性上存在不足，难以适应多目标或动态对齐需求。

Method: 通过分类损失直接对齐语言模型，引入多维度偏好和动态插值机制。

Result: 实验和理论分析表明，该方法在静态目标上与DPO效果相当，同时更具通用性和适应性。

Conclusion: 该方法为语言模型的多目标动态对齐提供了高效且灵活的解决方案。

Abstract: While large-scale unsupervised language models (LMs) capture broad world
knowledge and reasoning capabilities, steering their behavior toward desired
objectives remains challenging due to the lack of explicit supervision.
Existing alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on training a reward model and performing reinforcement
learning to align with human preferences. However, RLHF is often
computationally intensive, unstable, and sensitive to hyperparameters.
  To address these limitations, Direct Preference Optimization (DPO) was
introduced as a lightweight and stable alternative, enabling direct alignment
of language models with pairwise preference data via classification loss.
However, DPO and its extensions generally assume a single static preference
distribution, limiting flexibility in multi-objective or dynamic alignment
settings.
  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted
Listwise DPO, which extends DPO to incorporate multiple human preference
dimensions (e.g., helpfulness, harmlessness, informativeness) and enables
dynamic interpolation through a controllable simplex-weighted formulation. Our
method supports both listwise preference feedback and flexible alignment across
varying user intents without re-training. Empirical and theoretical analysis
demonstrates that our method is as effective as traditional DPO on static
objectives while offering greater generality and adaptability for real-world
deployment.

</details>


### [101] [Convolution-weighting method for the physics-informed neural network: A Primal-Dual Optimization Perspective](https://arxiv.org/abs/2506.19805)
*Chenhao Si,Ming Yan*

Main category: cs.LG

TL;DR: 提出了一种新的权重方案，通过自适应调整损失函数的权重，从孤立点到连续邻域区域，提高了PINNs的收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs受限于计算资源，通常使用有限点集进行优化，难以保证收敛性和准确性。

Method: 提出了一种自适应权重方案，动态调整损失函数的权重，从点到邻域。

Result: 实验结果表明，该权重方案能将相对$L^2$误差降至更低水平。

Conclusion: 新权重方案有效提升了PINNs的性能，为解决PDEs提供了更可靠的工具。

Abstract: Physics-informed neural networks (PINNs) are extensively employed to solve
partial differential equations (PDEs) by ensuring that the outputs and
gradients of deep learning models adhere to the governing equations. However,
constrained by computational limitations, PINNs are typically optimized using a
finite set of points, which poses significant challenges in guaranteeing their
convergence and accuracy. In this study, we proposed a new weighting scheme
that will adaptively change the weights to the loss functions from isolated
points to their continuous neighborhood regions. The empirical results show
that our weighting scheme can reduce the relative $L^2$ errors to a lower
value.

</details>


### [102] [Ambiguous Online Learning](https://arxiv.org/abs/2506.19810)
*Vanessa Kosoy*

Main category: cs.LG

TL;DR: 提出了一种名为‘模糊在线学习’的新变体，允许学习者生成多个预测标签，只要至少一个正确且没有‘可预测错误’标签即视为正确。


<details>
  <summary>Details</summary>
Motivation: 解决多值动态系统、推荐算法和无损压缩等场景中需要多标签预测的问题，并与‘苹果品尝’问题相关。

Method: 通过假设类定义‘可预测错误’，并允许学习者生成多值预测标签。

Result: 展示了错误界限的三分法：最优错误界限为Theta(1)、Theta(sqrt(N))或N。

Conclusion: 模糊在线学习在多标签预测场景中具有理论意义和实际应用潜力。

Abstract: We propose a new variant of online learning that we call "ambiguous online
learning". In this setting, the learner is allowed to produce multiple
predicted labels. Such an "ambiguous prediction" is considered correct when at
least one of the labels is correct, and none of the labels are "predictably
wrong". The definition of "predictably wrong" comes from a hypothesis class in
which hypotheses are also multi-valued. Thus, a prediction is "predictably
wrong" if it's not allowed by the (unknown) true hypothesis. In particular,
this setting is natural in the context of multivalued dynamical systems,
recommendation algorithms and lossless compression. It is also strongly related
to so-called "apple tasting". We show that in this setting, there is a
trichotomy of mistake bounds: up to logarithmic factors, any hypothesis class
has an optimal mistake bound of either Theta(1), Theta(sqrt(N)) or N.

</details>


### [103] [Curating art exhibitions using machine learning](https://arxiv.org/abs/2506.19813)
*Eurico Covas*

Main category: cs.LG

TL;DR: 论文提出四种基于机器学习的模型，模仿人类艺术策展人的策展工作，利用大都会艺术博物馆25年的展览数据，证明AI模型能有效复制策展决策。


<details>
  <summary>Details</summary>
Motivation: 艺术策展主要依赖人类专家的主观判断，缺乏明确规则。研究旨在探索AI是否能通过学习历史展览数据模仿策展工作。

Method: 使用四种机器学习模型，基于大都会艺术博物馆25年的展览数据，通过特征工程和模型架构设计进行训练。

Result: 模型能有效模仿人类策展人，精度高于随机选择；小型模型通过精心设计可媲美大型语言模型（如GPT）。

Conclusion: AI模型能复制历史策展决策，未来若数据增加，可能更接近人类策展的审美判断。

Abstract: Art curatorship has always been mostly the subjective work of human experts,
who, with extensive knowledge of many and diverse artworks, select a few of
those to present in communal spaces, spaces that evolved into what we now call
art galleries. There are no hard and fast set of rules on how to select these
artworks, given a theme which either is presented to the art curator or
constructed by her/him. Here we present a series of artificial models -- a
total of four related models -- based on machine learning techniques (a subset
of artificial intelligence) that attempt to learn from existing exhibitions
which have been curated by human experts, in order to be able to do similar
curatorship work. We focus exclusively on the last 25 years of past exhibitions
at the Metropolitan Museum of Art in New York, due to the quality of the data
available and the physical and time limitations of our research. Our four
artificial intelligence models achieve a reasonable ability at imitating these
various curators responsible for all those exhibitions, with various degrees of
precision and curatorial coherence. In particular, we can conclude two key
insights: first, that there is sufficient information in these exhibitions to
construct an artificial intelligence model that replicates past exhibitions
with an accuracy well above random choices; second, that using feature
engineering and carefully designing the architecture of modest size models can
make them as good as those using the so-called large language models such as
GPT in a brute force approach. We also believe, based on small attempts to use
the models in out-of-sample experiments, that given more much more data, it
should be possible for these kinds of artificial intelligence agents to be
closer and closer to the aesthetic and curatorial judgment of human art
curators.

</details>


### [104] [Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823)
*Miles Wang,Tom Dupré la Tour,Olivia Watkins,Alex Makelov,Ryan A. Chi,Samuel Miserendino,Johannes Heidecke,Tejal Patwardhan,Dan Mossing*

Main category: cs.LG

TL;DR: 研究发现，语言模型在微调后可能表现出“突发性失调”行为，通过模型对比方法揭示了内部特征，并提出少量良性样本微调可恢复对齐。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型如何从训练数据中泛化行为，特别是微调后可能导致的突发性失调现象。

Method: 采用模型对比方法（稀疏自编码器）分析微调前后模型内部表征，并测试不同条件下的突发性失调现象。

Result: 发现多个“失调人格”特征，其中毒性人格特征对突发性失调行为影响最大，且少量良性样本微调可有效恢复对齐。

Conclusion: 突发性失调行为可通过内部特征分析预测，并可通过少量良性样本微调缓解。

Abstract: Understanding how language models generalize behaviors from their training to
a broader deployment distribution is an important problem in AI safety. Betley
et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes
"emergent misalignment," where models give stereotypically malicious responses
to unrelated prompts. We extend this work, demonstrating emergent misalignment
across diverse conditions, including reinforcement learning on reasoning
models, fine-tuning on various synthetic datasets, and in models without safety
training. To investigate the mechanisms behind this generalized misalignment,
we apply a "model diffing" approach using sparse autoencoders to compare
internal model representations before and after fine-tuning. This approach
reveals several "misaligned persona" features in activation space, including a
toxic persona feature which most strongly controls emergent misalignment and
can be used to predict whether a model will exhibit such behavior.
Additionally, we investigate mitigation strategies, discovering that
fine-tuning an emergently misaligned model on just a few hundred benign samples
efficiently restores alignment.

</details>


### [105] [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
*Yichao Fu,Rui Ge,Zelei Shao,Zhijie Deng,Hao Zhang*

Main category: cs.LG

TL;DR: Lookahead Reasoning 通过引入步骤级并行性，提升了推理模型的解码速度，将峰值加速从1.4倍提高到2.1倍，同时保持答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型生成长链式思维时解码速度慢，而传统的令牌级推测解码（SD）因正确率随令牌数指数下降，加速效果有限。

Method: 提出Lookahead Reasoning，利用轻量级草案模型预测未来步骤，目标模型批量扩展这些步骤，并通过验证器保留语义正确的步骤。

Result: 在GSM8K、AIME等基准测试中，Lookahead Reasoning将SD的加速比从1.4倍提升至2.1倍，且加速效果随GPU吞吐量增加而提升。

Conclusion: Lookahead Reasoning通过步骤级并行性突破了令牌级SD的算法瓶颈，显著提升了推理模型的解码速度。

Abstract: Reasoning models excel by generating long chain-of-thoughts, but decoding the
resulting thousands of tokens is slow. Token-level speculative decoding (SD)
helps, but its benefit is capped, because the chance that an entire
$\gamma$-token guess is correct falls exponentially as $\gamma$ grows. This
means allocating more compute for longer token drafts faces an algorithmic
ceiling -- making the speedup modest and hardware-agnostic. We raise this
ceiling with Lookahead Reasoning, which exploits a second, step-level layer of
parallelism. Our key insight is that reasoning models generate step-by-step,
and each step needs only to be semantically correct, not exact token matching.
In Lookahead Reasoning, a lightweight draft model proposes several future
steps; the target model expands each proposal in one batched pass, and a
verifier keeps semantically correct steps while letting the target regenerate
any that fail. Token-level SD still operates within each reasoning step, so the
two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak
speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other
benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x
while preserving answer quality, and its speedup scales better with additional
GPU throughput. Our code is available at
https://github.com/hao-ai-lab/LookaheadReasoning

</details>


### [106] [Orthogonal Finetuning Made Scalable](https://arxiv.org/abs/2506.19847)
*Zeju Qiu,Weiyang Liu,Adrian Weller,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: OFTv2通过输入中心化重构和Cayley-Neumann参数化，显著降低了正交微调的计算和内存开销，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 解决正交微调（OFT）因计算和内存需求高而难以实际部署的问题。

Method: 提出OFTv2，采用输入中心化重构（矩阵-向量乘法）和Cayley-Neumann参数化，降低计算复杂度。

Result: OFTv2实现10倍训练加速和3倍GPU内存节省，且在量化模型微调中优于QLoRA。

Conclusion: OFTv2是一种高效且实用的正交微调方法，适用于大规模模型部署。

Abstract: Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning quantized foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [107] [Stabilizing PDE--ML Coupled System](https://arxiv.org/abs/2506.19274)
*Saad Qadeer,Panos Stinis,Hui Wan*

Main category: math.NA

TL;DR: 论文研究了机器学习替代模型在大型PDE系统中的数值不稳定性问题，提出了稳定耦合系统的策略，并探索了基于Mori-Zwanzig形式的方法以提高精度。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习替代模型在大型PDE系统中数值不稳定的长期障碍，现有方法效果有限。

Method: 通过原型问题（粘性Burgers'-ML系统）分析不稳定性原因，提出稳定策略，并采用Mori-Zwanzig形式方法提高精度。

Result: 提出了稳定耦合系统的策略，并探索了提高精度的方法。

Conclusion: 研究为解决复杂系统中的类似问题提供了启示和方法。

Abstract: A long-standing obstacle in the use of machine-learnt surrogates with larger
PDE systems is the onset of instabilities when solved numerically. Efforts
towards ameliorating these have mostly concentrated on improving the accuracy
of the surrogates or imbuing them with additional structure, and have garnered
limited success. In this article, we study a prototype problem and draw
insights that can help with more complex systems. In particular, we focus on a
viscous Burgers'-ML system and, after identifying the cause of the
instabilities, prescribe strategies to stabilize the coupled system. To improve
the accuracy of the stabilized system, we next explore methods based on the
Mori--Zwanzig formalism.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [Reinforcement Learning-Based Dynamic Grouping for Tubular Structure Tracking](https://arxiv.org/abs/2506.18930)
*Chong Di,Shuwang Zhou,Da Chen,Jean-Marie Mirebeau,Minglei Shu,Laurent D. Cohen*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的段式跟踪方法，通过马尔可夫决策过程动态探索段图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂形态和环境变化下最小路径计算的挑战，尤其是段式方法计算效率低和依赖先验知识的问题。

Method: 将段式跟踪建模为马尔可夫决策过程，利用Q-Learning动态探索段图，按需计算边权重并自适应扩展搜索空间。

Result: 在典型管状结构数据集上显著优于现有方法，有效处理复杂拓扑并保持全局路径一致性。

Conclusion: 该方法无需依赖大量先验知识，高效且鲁棒，适用于复杂场景。

Abstract: The computation of minimal paths for the applications in tracking tubular
structures such as blood vessels and roads is challenged by complex
morphologies and environmental variations. Existing approaches can be roughly
categorized into two research lines: the point-wise based models and the
segment-wise based models. Although segment-wise approaches have obtained
promising results in many scenarios, they often suffer from computational
inefficiency and heavily rely on a prescribed prior to fit the target elongated
shapes. We propose a novel framework that casts segment-wise tracking as a
Markov Decision Process (MDP), enabling a reinforcement learning approach. Our
method leverages Q-Learning to dynamically explore a graph of segments,
computing edge weights on-demand and adaptively expanding the search space.
This strategy avoids the high cost of a pre-computed graph and proves robust to
incomplete initial information. Experimental reuslts on typical tubular
structure datasets demonstrate that our method significantly outperforms
state-of-the-art point-wise and segment-wise approaches. The proposed method
effectively handles complex topologies and maintains global path coherence
without depending on extensive prior structural knowledge.

</details>


### [109] [Stylized Structural Patterns for Improved Neural Network Pre-training](https://arxiv.org/abs/2506.19465)
*Farnood Salehi,Vandit Sharma,Amirhossein Askari Farsangi,Tunç Ozan Aydın*

Main category: cs.CV

TL;DR: 本文提出了一种两步法，通过改进神经分形生成合成数据，并利用反向风格化技术提升合成数据的实用性，显著缩小了合成数据与真实图像之间的分布差距。实验表明，该方法在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要大量真实图像数据，但获取这些数据存在隐私和法律问题。合成数据虽为替代方案，但性能不足。本文旨在解决这一问题。

Method: 1. 改进神经分形生成新类别合成数据；2. 提出反向风格化技术，将小规模真实图像特征迁移到合成数据中。

Result: 通过KID分析显示，合成数据与真实图像的分布差距显著降低。实验表明，在图像生成、数据表示和分类任务中性能均有提升。

Conclusion: 该方法为在缺乏大规模真实数据时训练实用模型提供了新思路。

Abstract: Modern deep learning models in computer vision require large datasets of real
images, which are difficult to curate and pose privacy and legal concerns,
limiting their commercial use. Recent works suggest synthetic data as an
alternative, yet models trained with it often underperform. This paper proposes
a two-step approach to bridge this gap. First, we propose an improved neural
fractal formulation through which we introduce a new class of synthetic data.
Second, we propose reverse stylization, a technique that transfers visual
features from a small, license-free set of real images onto synthetic datasets,
enhancing their effectiveness. We analyze the domain gap between our synthetic
datasets and real images using Kernel Inception Distance (KID) and show that
our method achieves a significantly lower distributional gap compared to
existing synthetic datasets. Furthermore, our experiments across different
tasks demonstrate the practical impact of this reduced gap. We show that
pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11%
reduction in FID during image generation, compared to models trained on
existing synthetic datasets, and a 20% decrease in autoencoder reconstruction
error, indicating improved performance in data representation. Furthermore, a
ViT-S model trained for classification on this synthetic data achieves over a
10% improvement in ImageNet-100 accuracy. Our work opens up exciting
possibilities for training practical models when sufficiently large real
training sets are not available.

</details>


### [110] [Visual hallucination detection in large vision-language models via evidential conflict](https://arxiv.org/abs/2506.19513)
*Tao Huang,Zhekun Liu,Rui Wang,Yang Zhang,Liping Jing*

Main category: cs.CV

TL;DR: 论文提出了一种基于Dempster-Shafer理论的方法，用于检测大型视觉语言模型（LVLMs）中的视觉幻觉问题，并通过新数据集PRE-HAL评估模型的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态任务中表现优异，但存在视觉输入与文本输出不一致的视觉幻觉问题，这在安全关键应用中具有高风险，亟需评估基准和检测方法。

Method: 开发了PRE-HAL数据集，系统评估LVLMs的感知和推理能力；提出基于Dempster-Shafer理论的视觉幻觉检测方法，通过不确定性估计高效捕捉模型推理中的冲突特征。

Result: 实验表明，该方法在LLaVA-v1.5、mPLUG-Owl2和mPLUG-Owl3上优于五种基线不确定性指标，平均AUROC提升4%、10%和7%。

Conclusion: PRE-HAL数据集和新检测方法有效揭示了LVLMs的视觉漏洞，特别是在关系推理任务中，为提升模型可靠性提供了重要工具。

Abstract: Despite the remarkable multimodal capabilities of Large Vision-Language
Models (LVLMs), discrepancies often occur between visual inputs and textual
outputs--a phenomenon we term visual hallucination. This critical reliability
gap poses substantial risks in safety-critical Artificial Intelligence (AI)
applications, necessitating a comprehensive evaluation benchmark and effective
detection methods. Firstly, we observe that existing visual-centric
hallucination benchmarks mainly assess LVLMs from a perception perspective,
overlooking hallucinations arising from advanced reasoning capabilities. We
develop the Perception-Reasoning Evaluation Hallucination (PRE-HAL) dataset,
which enables the systematic evaluation of both perception and reasoning
capabilities of LVLMs across multiple visual semantics, such as instances,
scenes, and relations. Comprehensive evaluation with this new benchmark exposed
more visual vulnerabilities, particularly in the more challenging task of
relation reasoning. To address this issue, we propose, to the best of our
knowledge, the first Dempster-Shafer theory (DST)-based visual hallucination
detection method for LVLMs through uncertainty estimation. This method aims to
efficiently capture the degree of conflict in high-level features at the model
inference phase. Specifically, our approach employs simple mass functions to
mitigate the computational complexity of evidence combination on power sets. We
conduct an extensive evaluation of state-of-the-art LVLMs, LLaVA-v1.5,
mPLUG-Owl2 and mPLUG-Owl3, with the new PRE-HAL benchmark. Experimental results
indicate that our method outperforms five baseline uncertainty metrics,
achieving average AUROC improvements of 4%, 10%, and 7% across three LVLMs. Our
code is available at https://github.com/HT86159/Evidential-Conflict.

</details>


### [111] [Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks](https://arxiv.org/abs/2506.19533)
*Ankita Raj,Ambar Pal,Chetan Arora*

Main category: cs.CV

TL;DR: 论文提出了一种检测和识别面部识别系统中自然触发器的后门攻击方法，并在实验中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对高安全性应用中的面部识别系统构成严重威胁，需要一种方法来检测和识别这些攻击。

Method: 提出了一种新技术，用于检测面部识别网络是否被植入自然触发器，并在受感染的网络中识别这些触发器。

Result: 在实验中，该方法在识别触发器（如绿色太阳镜或红色帽子）时，前5准确率达到74%，优于基线方法的56%。

Conclusion: 该方法能有效检测和识别面部识别系统中的后门攻击，提升了安全性。

Abstract: Backdoor attacks embed a hidden functionality into deep neural networks,
causing the network to display anomalous behavior when activated by a
predetermined pattern in the input Trigger, while behaving well otherwise on
public test data. Recent works have shown that backdoored face recognition (FR)
systems can respond to natural-looking triggers like a particular pair of
sunglasses. Such attacks pose a serious threat to the applicability of FR
systems in high-security applications. We propose a novel technique to (1)
detect whether an FR network is compromised with a natural, physically
realizable trigger, and (2) identify such triggers given a compromised network.
We demonstrate the effectiveness of our methods with a compromised FR network,
where we are able to identify the trigger (e.g., green sunglasses or red hat)
with a top-5 accuracy of 74%, whereas a naive brute force baseline achieves 56%
accuracy.

</details>


### [112] [General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound](https://arxiv.org/abs/2506.19552)
*Jakob Ambsdorf,Asbjørn Munk,Sebastian Llambias,Anders Nymark Christensen,Kamil Mikolaj,Randall Balestriero,Martin Tolsgaard,Aasa Feragen,Mads Nielsen*

Main category: cs.CV

TL;DR: 研究探讨了在医疗数据上预训练定制基础模型与从通用模型迁移学习的优劣，并通过胎儿超声数据集案例证明定制预训练的价值。


<details>
  <summary>Details</summary>
Motivation: 解决在医疗领域是否应预训练定制基础模型或依赖通用模型的问题，以及是否需要新方法。

Method: 使用DINOv2方法在2M胎儿超声图像上预训练模型，并与自然图像、超声图像预训练模型及监督基线对比。

Result: 定制预训练在超声任务中表现优异，且无需超参数调整或方法创新。

Conclusion: 在资源有限时，应避免过度追求方法创新，优先定制预训练。

Abstract: With access to large-scale, unlabeled medical datasets, researchers are
confronted with two questions: Should they attempt to pretrain a custom
foundation model on this medical data, or use transfer-learning from an
existing generalist model? And, if a custom model is pretrained, are novel
methods required? In this paper we explore these questions by conducting a
case-study, in which we train a foundation model on a large regional fetal
ultrasound dataset of 2M images. By selecting the well-established DINOv2
method for pretraining, we achieve state-of-the-art results on three fetal
ultrasound datasets, covering data from different countries, classification,
segmentation, and few-shot tasks. We compare against a series of models
pretrained on natural images, ultrasound images, and supervised baselines. Our
results demonstrate two key insights: (i) Pretraining on custom data is worth
it, even if smaller models are trained on less data, as scaling in natural
image pretraining does not translate to ultrasound performance. (ii) Well-tuned
methods from computer vision are making it feasible to train custom foundation
models for a given medical domain, requiring no hyperparameter tuning and
little methodological adaptation. Given these findings, we argue that a bias
towards methodological innovation should be avoided when developing domain
specific foundation models under common computational resource constraints.

</details>


### [113] [Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications](https://arxiv.org/abs/2506.19591)
*Lujun Li,Yiqun Wang,Radu State*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Transformer的时间序列多光谱图像重建框架，结合SAR数据，显著提升了云覆盖区域的图像重建效果。


<details>
  <summary>Details</summary>
Motivation: 云覆盖导致多光谱图像数据缺失或损坏，影响早期作物测绘；SAR数据不受云干扰但缺乏光谱细节，需结合两者优势。

Method: 利用时间序列多光谱图像和SAR数据的互补信息，通过Vision Transformer的注意力机制重建云覆盖区域的多光谱图像。

Result: 实验表明，该框架在云覆盖区域的多光谱图像重建效果显著优于基线方法。

Conclusion: 提出的Time-series ViT框架有效解决了云覆盖问题，为早期作物测绘提供了更可靠的数据支持。

Abstract: Cloud cover in multispectral imagery (MSI) poses significant challenges for
early season crop mapping, as it leads to missing or corrupted spectral
information. Synthetic aperture radar (SAR) data, which is not affected by
cloud interference, offers a complementary solution, but lack sufficient
spectral detail for precise crop mapping. To address this, we propose a novel
framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT),
to reconstruct MSI data in cloud-covered regions by leveraging the temporal
coherence of MSI and the complementary information from SAR from the attention
mechanism. Comprehensive experiments, using rigorous reconstruction evaluation
metrics, demonstrate that Time-series ViT framework significantly outperforms
baselines that use non-time-series MSI and SAR or time-series MSI without SAR,
effectively enhancing MSI image reconstruction in cloud-covered regions.

</details>


### [114] [PEVLM: Parallel Encoding for Vision-Language Models](https://arxiv.org/abs/2506.19651)
*Letian Kang,Shixian Luo,Yiqiang Li,Xiaoyang Yu,Shenxuan Zhou,Yong Wu*

Main category: cs.CV

TL;DR: PEVLM是一种并行编码策略，旨在提高视觉语言模型（VLM）的长视频理解效率，无需微调模型，显著降低计算复杂度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的二次复杂度限制了视觉语言模型在长视频理解中的应用，需要一种高效的方法。

Method: PEVLM将输入分块处理，保留全注意力位置嵌入，并通过对齐注意力权重模拟全注意力分布，降低计算复杂度。

Result: 在LongVideoBench基准测试中，PEVLM比现有方法精度提升8.37%，计算速度提升7.47倍，端到端延迟降低40%。

Conclusion: PEVLM在低延迟、长上下文视频理解中表现优异，适用于自动驾驶等实际应用。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance in
video-language tasks, yet their application to long video understanding remains
constrained by the quadratic complexity of standard attention mechanisms. In
this paper, we propose \textbf{PEVLM}, a parallel encoding strategy
specifically designed to improve the prefill efficiency of VLMs without
requiring model finetuning. PEVLM partitions the input into block-wise segments
with a shared sink, preserves full-attention positional embeddings, and aligns
attention weights to mimic full-attention distributions. This design reduces
attention computation from $O((T \times N)^2)$ to $O(T \times N)$ while
maintaining high accuracy. Extensive experiments on the LongVideoBench
benchmark show that PEVLM achieves up to 8.37\% accuracy improvement over
existing inference-efficient methods and delivers up to 7.47x speedup in
attention computation and 40\% reduction in end-to-end latency. Under strict
latency constraints, PEVLM significantly outperforms baselines, raising
accuracy from 23.26\% to 61.03\%. These results highlight PEVLM's effectiveness
for low-latency, long-context video understanding, making it well-suited for
real-world applications such as autonomous driving.

</details>


### [115] [Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance](https://arxiv.org/abs/2506.19683)
*Xuesong Li,Dianye Huang,Yameng Zhang,Nassir Navab,Zhongliang Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于场景图（SG）的方法，用于提高超声图像的解读和扫描指导，特别针对非专业用户。


<details>
  <summary>Details</summary>
Motivation: 解决超声图像视觉变异性大、非专业用户难以理解和操作的问题。

Method: 使用基于Transformer的一阶段方法生成超声图像场景图，并结合大型语言模型（LLMs）优化解释和扫描指导。

Result: 在颈部区域的超声图像上验证了方法的有效性，提升了图像的解读和扫描标准化。

Conclusion: 该方法有望通过提高超声的可解释性和易用性，使其更广泛地服务于非专业用户。

Abstract: Understanding medical ultrasound imaging remains a long-standing challenge
due to significant visual variability caused by differences in imaging and
acquisition parameters. Recent advancements in large language models (LLMs)
have been used to automatically generate terminology-rich summaries orientated
to clinicians with sufficient physiological knowledge. Nevertheless, the
increasing demand for improved ultrasound interpretability and basic scanning
guidance among non-expert users, e.g., in point-of-care settings, has not yet
been explored. In this study, we first introduce the scene graph (SG) for
ultrasound images to explain image content to ordinary and provide guidance for
ultrasound scanning. The ultrasound SG is first computed using a
transformer-based one-stage method, eliminating the need for explicit object
detection. To generate a graspable image explanation for ordinary, the user
query is then used to further refine the abstract SG representation through
LLMs. Additionally, the predicted SG is explored for its potential in guiding
ultrasound scanning toward missing anatomies within the current imaging view,
assisting ordinary users in achieving more standardized and complete anatomical
exploration. The effectiveness of this SG-based image explanation and scanning
guidance has been validated on images from the left and right neck regions,
including the carotid and thyroid, across five volunteers. The results
demonstrate the potential of the method to maximally democratize ultrasound by
enhancing its interpretability and usability for ordinaries.

</details>


### [116] [A Comparative Study of NAFNet Baselines for Image Restoration](https://arxiv.org/abs/2506.19845)
*Vladislav Esaulov,M. Moein Esfahani*

Main category: cs.CV

TL;DR: NAFNet是一种简单高效的图像修复深度学习基线，通过CIFAR10噪声和模糊图像进行消融实验，验证其核心组件的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究NAFNet在图像修复任务中的表现，验证其核心组件（如SimpleGate激活、SCA和LayerNorm）的作用。

Method: 使用CIFAR10噪声和模糊图像进行消融实验，比较不同变体的性能（PSNR、SSIM）。

Result: SimpleGate和简化注意力机制优于传统方法，LayerNorm对训练稳定性至关重要。

Conclusion: NAFNet设计有效，未来可进一步优化模型设计。

Abstract: We study NAFNet (Nonlinear Activation Free Network), a simple and efficient
deep learning baseline for image restoration. By using CIFAR10 images corrupted
with noise and blur, we conduct an ablation study of NAFNet's core components.
Our baseline model implements SimpleGate activation, Simplified Channel
Activation (SCA), and LayerNormalization. We compare this baseline to different
variants that replace or remove components. Quantitative results (PSNR, SSIM)
and examples illustrate how each modification affects restoration performance.
Our findings support the NAFNet design: the SimpleGate and simplified attention
mechanisms yield better results than conventional activations and attention,
while LayerNorm proves to be important for stable training. We conclude with
recommendations for model design, discuss potential improvements, and future
work.

</details>


### [117] [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](https://arxiv.org/abs/2506.19852)
*Xingyang Li,Muyang Li,Tianle Cai,Haocheng Xi,Shuo Yang,Yujun Lin,Lvmin Zhang,Songlin Yang,Jinbo Hu,Kelly Peng,Maneesh Agrawala,Ion Stoica,Kurt Keutzer,Song Han*

Main category: cs.CV

TL;DR: 论文提出Radial Attention，一种高效的稀疏注意力机制，通过模拟时空能量衰减现象，显著降低视频扩散模型的计算成本。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型在长视频生成中计算成本过高，作者发现时空注意力分数随距离衰减的现象，提出利用这一现象优化计算效率。

Method: 提出Radial Attention，采用静态注意力掩码，使每个token仅关注空间邻近的token，且注意力窗口随时间距离缩小，复杂度为O(n log n)。

Result: 在多个数据集上验证，Radial Attention保持视频质量，计算速度提升1.9倍，训练成本降低4.4倍，推理速度提升3.7倍。

Conclusion: Radial Attention通过稀疏注意力机制有效解决了视频扩散模型的高计算成本问题，支持更长的视频生成。

Abstract: Recent advances in diffusion models have enabled high-quality video
generation, but the additional temporal dimension significantly increases
computational costs, making training and inference on long videos prohibitively
expensive. In this paper, we identify a phenomenon we term Spatiotemporal
Energy Decay in video diffusion models: post-softmax attention scores diminish
as spatial and temporal distance between tokens increase, akin to the physical
decay of signal or waves over space and time in nature. Motivated by this, we
propose Radial Attention, a scalable sparse attention mechanism with $O(n \log
n)$ complexity that translates energy decay into exponentially decaying compute
density, which is significantly more efficient than standard $O(n^2)$ dense
attention and more expressive than linear attention. Specifically, Radial
Attention employs a simple, static attention mask where each token attends to
spatially nearby tokens, with the attention window size shrinking with temporal
distance. Moreover, it allows pre-trained video diffusion models to extend
their generation length with efficient LoRA-based fine-tuning. Extensive
experiments show that Radial Attention maintains video quality across
Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup
over the original dense attention. With minimal tuning, it enables video
generation up to 4$\times$ longer while reducing training costs by up to
4.4$\times$ compared to direct fine-tuning and accelerating inference by up to
3.7$\times$ compared to dense attention inference.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [118] [Experimental Assessment of A Framework for In-body RF-backscattering Localization](https://arxiv.org/abs/2506.19499)
*Noa Jie Vives Zaguirre,Oscar Lasierra,Filip Lemic,Gerard Calvo Bartra,Pablo José Galván Calderón,Gines Garcia-Aviles,Sergi Abadal,Xavier Costa-Pérez*

Main category: eess.SP

TL;DR: 该研究提出了一种基于射频反向散射的体内定位实验框架，评估了其在真实条件下的性能，并探讨了生物组织密度、天线配置和干扰对定位效果的影响。


<details>
  <summary>Details</summary>
Motivation: 传统的胃肠道诊断和治疗方法（如成像和内窥镜）具有侵入性和分辨率限制，需要创新的替代方案。

Method: 研究采用射频反向散射技术，通过实验框架评估了不同天线配置和生物组织（空气、鸡肉和猪肉组织）对谐波生成和接收的影响。

Result: 结果表明，反向散射设备的定位、天线选择和增益设置对性能有显著影响，生物组织密度越高，衰减越大。此外，外部干扰和塑料外壳也会影响传播。

Conclusion: 研究强调了干扰抑制和优化传播模型的重要性，以提高体内定位技术的性能。

Abstract: Localization of in-body devices is beneficial for Gastrointestinal (GI)
diagnosis and targeted treatment. Traditional methods such as imaging and
endoscopy are invasive and limited in resolution, highlighting the need for
innovative alternatives. This study presents an experimental framework for
Radio Frequency (RF)-backscatter-based in-body localization, inspired by the
ReMix approach, and evaluates its performance in real-world conditions. The
experimental setup includes an in-body backscatter device and various off-body
antenna configurations to investigate harmonic generation and reception in air,
chicken and pork tissues. The results indicate that optimal backscatter device
positioning, antenna selection, and gain settings significantly impact
performance, with denser biological tissues leading to greater attenuation. The
study also highlights challenges such as external interference and plastic
enclosures affecting propagation. The findings emphasize the importance of
interference mitigation and refined propagation models to enhance performance.

</details>


### [119] [EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding](https://arxiv.org/abs/2506.19141)
*Bruno Aristimunha,Dung Truong,Pierre Guetschel,Seyed Yahya Shirazi,Isabelle Guyon,Alexandre R. Franco,Michael P. Milham,Aviv Dotan,Scott Makeig,Alexandre Gramfort,Jean-Remi King,Marie-Constance Corsi,Pedro A. Valdés-Sosa,Amit Majumdar,Alan Evans,Terrence J Sejnowski,Oren Shriki,Sylvain Chevallier,Arnaud Delorme*

Main category: eess.SP

TL;DR: 论文介绍了一个大规模EEG解码竞赛，包含两个挑战：跨任务和跨被试的零样本解码，以及从EEG数据预测心理健康指标。


<details>
  <summary>Details</summary>
Motivation: 当前EEG解码模型通常基于少量被试和单一任务训练，缺乏泛化能力。通过大规模数据集和竞赛，推动跨任务和个体的通用模型发展，并探索EEG在心理健康评估中的应用。

Method: 使用包含3000多名被试的多任务高密度EEG数据集，提供可调神经网络基线模型，包括简单网络和基于人口统计的回归模型。

Result: 竞赛旨在开发能适应多样化任务和个体的EEG解码模型，并探索EEG数据作为心理健康生物标志物的潜力。

Conclusion: 该竞赛的成果可能推动计算精神病学和神经技术的发展，为基础神经科学和临床研究提供突破。

Abstract: Current electroencephalogram (EEG) decoding models are typically trained on
small numbers of subjects performing a single task. Here, we introduce a
large-scale, code-submission-based competition comprising two challenges.
First, the Transfer Challenge asks participants to build and test a model that
can zero-shot decode new tasks and new subjects from their EEG data. Second,
the Psychopathology factor prediction Challenge asks participants to infer
subject measures of mental health from EEG data. For this, we use an
unprecedented, multi-terabyte dataset of high-density EEG signals (128
channels) recorded from over 3,000 child to young adult subjects engaged in
multiple active and passive tasks. We provide several tunable neural network
baselines for each of these two challenges, including a simple network and
demographic-based regression models. Developing models that generalise across
tasks and individuals will pave the way for ML network architectures capable of
adapting to EEG data collected from diverse tasks and individuals. Similarly,
predicting mental health-relevant personality trait values from EEG might
identify objective biomarkers useful for clinical diagnosis and design of
personalised treatment for psychological conditions. Ultimately, the advances
spurred by this challenge could contribute to the development of computational
psychiatry and useful neurotechnology, and contribute to breakthroughs in both
fundamental neuroscience and applied clinical research.

</details>


### [120] [Low-Complexity Semantic Packet Aggregation for Token Communication via Lookahead Search](https://arxiv.org/abs/2506.19451)
*Seunghun Lee,Jihong Park,Jinho Choi,Hyuncheol Park*

Main category: eess.SP

TL;DR: 该论文提出了一种名为SemPA-Look的框架，通过优化令牌分组以最大化令牌相似性（ATS），解决了在中断信道下令牌通信（TC）中的语义失真问题。


<details>
  <summary>Details</summary>
Motivation: 由于令牌之间的语义依赖性，单个令牌的丢失可能导致消息语义的显著失真，因此需要优化令牌分组以保持语义完整性。

Method: 提出SemPA-Look框架，结合残差语义评分（RSS）和前瞻搜索算法，以线性复杂度解决组合问题。

Result: 在MS-COCO数据集上的实验表明，SemPA-Look在保持高ATS和LPIPS分数的同时，计算复杂度降低了40倍。

Conclusion: SemPA-Look在远程AIGC和TC应用中具有高效性和实用性。

Abstract: Tokens are fundamental processing units of generative AI (GenAI) and large
language models (LLMs), and token communication (TC) is essential for enabling
remote AI-generate content (AIGC) and wireless LLM applications. Unlike
traditional bits, each of which is independently treated, the semantics of each
token depends on its surrounding context tokens. This inter-token dependency
makes TC vulnerable to outage channels, where the loss of a single token can
significantly distort the original message semantics. Motivated by this, this
paper focuses on optimizing token packetization to maximize the average token
similarity (ATS) between the original and received token messages under outage
channels. Due to inter-token dependency, this token grouping problem is
combinatorial, with complexity growing exponentially with message length. To
address this, we propose a novel framework of semantic packet aggregation with
lookahead search (SemPA-Look), built on two core ideas. First, it introduces
the residual semantic score (RSS) as a token-level surrogate for the
message-level ATS, allowing robust semantic preservation even when a certain
token packet is lost. Second, instead of full search, SemPA-Look applies a
lookahead search-inspired algorithm that samples intra-packet token candidates
without replacement (fixed depth), conditioned on inter-packet token candidates
sampled with replacement (fixed width), thereby achieving linear complexity.
Experiments on a remote AIGC task with the MS-COCO dataset (text captioned
images) demonstrate that SemPA-Look achieves high ATS and LPIPS scores
comparable to exhaustive search, while reducing computational complexity by up
to 40$\times$. Compared to other linear-complexity algorithms such as the
genetic algorithm (GA), SemPA-Look achieves 10$\times$ lower complexity,
demonstrating its practicality for remote AIGC and other TC applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [121] [Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning](https://arxiv.org/abs/2506.19260)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.CR

TL;DR: 该论文首次全面分析了联邦学习系统中基于网络拓扑的隐私泄露问题，揭示了不同攻击场景下的数据分布推断能力，并提出了一种结构噪声注入的防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有隐私研究主要关注基于梯度的攻击，而网络拓扑知识的隐私影响尚未充分研究。论文旨在填补这一空白。

Method: 通过系统评估4,720个攻击实例，分析了六种不同的对抗知识场景，并提出了三种攻击向量：通信模式分析、参数幅度分析和结构位置相关性分析。

Result: 在完全知识条件下，攻击成功率分别为84.1%、65.0%和47.2%。80%的部分知识场景仍保持高于安全阈值的攻击效果。

Conclusion: 网络拓扑是联邦学习系统中的基本隐私漏洞，但通过拓扑感知防御机制（如结构噪声注入）可以有效缓解。

Abstract: Federated learning systems increasingly rely on diverse network topologies to
address scalability and organizational constraints. While existing privacy
research focuses on gradient-based attacks, the privacy implications of network
topology knowledge remain critically understudied. We conduct the first
comprehensive analysis of topology-based privacy leakage across realistic
adversarial knowledge scenarios, demonstrating that adversaries with varying
degrees of structural knowledge can infer sensitive data distribution patterns
even under strong differential privacy guarantees. Through systematic
evaluation of 4,720 attack instances, we analyze six distinct adversarial
knowledge scenarios: complete topology knowledge and five partial knowledge
configurations reflecting real-world deployment constraints. We propose three
complementary attack vectors: communication pattern analysis, parameter
magnitude profiling, and structural position correlation, achieving success
rates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.
Critically, we find that 80% of realistic partial knowledge scenarios maintain
attack effectiveness above security thresholds, with certain partial knowledge
configurations achieving performance superior to the baseline complete
knowledge scenario. To address these vulnerabilities, we propose and
empirically validate structural noise injection as a complementary defense
mechanism across 808 configurations, demonstrating up to 51.4% additional
attack reduction when properly layered with existing privacy techniques. These
results establish that network topology represents a fundamental privacy
vulnerability in federated learning systems while providing practical pathways
for mitigation through topology-aware defense mechanisms.

</details>


### [122] [Adaptive Anomaly Detection for Identifying Attacks in Cyber-Physical Systems: A Systematic Literature Review](https://arxiv.org/abs/2411.14278)
*Pablo Moriano,Steven C. Hespeler,Mingyan Li,Maria Mahbub*

Main category: cs.CR

TL;DR: 本文是一篇关于自适应异常检测（AAD）在信息物理系统（CPS）中应用的系统性文献综述（SLR），分析了2013年至2023年的65篇相关论文，提出了新的分类法，并指出了当前研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代网络攻击在CPS中快速演变，现有方法难以有效应对。AAD因其快速数据处理和模型适应能力成为有前景的技术，但缺乏系统性综述。

Method: 通过SLR方法，收集并分析了397篇相关论文中的65篇（47篇研究论文和18篇综述论文），提出了一种新的分类法，涵盖攻击类型、CPS应用、学习范式、数据管理和算法。

Result: 研究发现，现有工作多集中于单一适应方面（数据处理或模型适应），而很少同时关注两者。

Conclusion: 本文为研究人员提供了AAD在CPS中的最新进展，指出了当前研究的不足，并提出了未来研究方向。

Abstract: Modern cyberattacks in cyber-physical systems (CPS) rapidly evolve and cannot
be deterred effectively with most current methods which focused on
characterizing past threats. Adaptive anomaly detection (AAD) is among the most
promising techniques to detect evolving cyberattacks focused on fast data
processing and model adaptation. AAD has been researched in the literature
extensively; however, to the best of our knowledge, our work is the first
systematic literature review (SLR) on the current research within this field.
We present a comprehensive SLR, gathering 397 relevant papers and
systematically analyzing 65 of them (47 research and 18 survey papers) on AAD
in CPS studies from 2013 to 2023 (November). We introduce a novel taxonomy
considering attack types, CPS application, learning paradigm, data management,
and algorithms. Our analysis indicates, among other findings, that reviewed
works focused on a single aspect of adaptation (either data processing or model
adaptation) but rarely in both at the same time. We aim to help researchers to
advance the state of the art and help practitioners to become familiar with
recent progress in this field. We identify the limitations of the state of the
art and provide recommendations for future research directions.

</details>


### [123] [WebGuard++:Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT](https://arxiv.org/abs/2506.19356)
*Ye Tian,Zhang Yumin,Yifan Jia,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: WebGuard++是一个恶意URL检测框架，通过融合URL和HTML特征，解决了现有方法的四个关键问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有恶意URL检测方法存在四个主要问题：URL建模不完整、HTML图稀疏、单向分析以及决策不透明，导致检测效果不佳。

Method: WebGuard++提出四个新组件：跨尺度URL编码器、子图感知HTML编码器、双向耦合模块和投票模块，分别解决上述问题。

Result: 实验表明，WebGuard++在固定FPR下，TPR显著提升1.1x-7.9x。

Conclusion: WebGuard++通过创新设计有效解决了现有方法的不足，显著提升了恶意URL检测的准确性和鲁棒性。

Abstract: URL+HTML feature fusion shows promise for robust malicious URL detection,
since attacker artifacts persist in DOM structures. However, prior work suffers
from four critical shortcomings: (1) incomplete URL modeling, failing to
jointly capture lexical patterns and semantic context; (2) HTML graph sparsity,
where threat-indicative nodes (e.g., obfuscated scripts) are isolated amid
benign content, causing signal dilution during graph aggregation; (3)
unidirectional analysis, ignoring URL-HTML feature bidirectional interaction;
and (4) opaque decisions, lacking attribution to malicious DOM components. To
address these challenges, we present WebGuard++, a detection framework with 4
novel components: 1) Cross-scale URL Encoder: Hierarchically learns
local-to-global and coarse to fine URL features based on Transformer network
with dynamic convolution. 2) Subgraph-aware HTML Encoder: Decomposes DOM graphs
into interpretable substructures, amplifying sparse threat signals via
Hierarchical feature fusion. 3) Bidirectional Coupling Module: Aligns URL and
HTML embeddings through cross-modal contrastive learning, optimizing
inter-modal consistency and intra-modal specificity. 4) Voting Module:
Localizes malicious regions through consensus voting on malicious subgraph
predictions. Experiments show WebGuard++ achieves significant improvements over
state-of-the-art baselines, achieving 1.1x-7.9x higher TPR at fixed FPR of
0.001 and 0.0001 across both datasets.

</details>


### [124] [Machine Learning with Privacy for Protected Attributes](https://arxiv.org/abs/2506.19836)
*Saeed Mahloujifar,Chuan Guo,G. Edward Suh,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: 论文提出了一种称为特征差分隐私（FDP）的新框架，用于在特定保护属性上提供隐私保护，避免了传统差分隐私（DP）的效用损失。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私对所有数据提供统一保护，但在某些应用中只需保护特定属性，导致不必要的效用损失。FDP旨在解决这一问题。

Method: 提出FDP框架，支持添加/移除和替换隐私变体，并处理保护和非保护特征的任意分离。修改DP-SGD算法以满足FDP。

Result: 在AFHQ数据集上，FDP显著提升模型效用（FID从286.7降至101.9）。

Conclusion: FDP为隐私数据分析提供了更灵活的方法，减少效用损失同时保持强隐私保证。

Abstract: Differential privacy (DP) has become the standard for private data analysis.
Certain machine learning applications only require privacy protection for
specific protected attributes. Using naive variants of differential privacy in
such use cases can result in unnecessary degradation of utility. In this work,
we refine the definition of DP to create a more general and flexible framework
that we call feature differential privacy (FDP). Our definition is
simulation-based and allows for both addition/removal and replacement variants
of privacy, and can handle arbitrary and adaptive separation of protected and
non-protected features. We prove the properties of FDP, such as adaptive
composition, and demonstrate its implications for limiting attribute inference
attacks. We also propose a modification of the standard DP-SGD algorithm that
satisfies FDP while leveraging desirable properties such as amplification via
sub-sampling. We apply our framework to various machine learning tasks and show
that it can significantly improve the utility of DP-trained models when public
features are available. For example, we train diffusion models on the AFHQ
dataset of animal faces and observe a drastic improvement in FID compared to
DP, from 286.7 to 101.9 at $\epsilon=8$, assuming that the blurred version of a
training image is available as a public feature. Overall, our work provides a
new approach to private data analysis that can help reduce the utility cost of
DP while still providing strong privacy guarantees.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [125] [Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices and Tensors](https://arxiv.org/abs/2506.19175)
*Benjamin Brock,Willow Ahrens,Hameer Abbasi,Timothy A. Davis,Juni Kim,James Kitchen,Spencer Patty,Isaac Virshup,Erik Welch*

Main category: cs.MS

TL;DR: Binsparse是一种跨平台的二进制稀疏矩阵和张量存储格式，通过模块化设计（JSON描述符和二进制数组）显著减小文件大小并提升解析速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵和张量的广泛使用需要高效的存储格式，但现有ASCII格式（如Matrix Market和FROSTT）效率低，文件大且解析慢。缺乏跨平台二进制格式。

Method: 提出Binsparse格式，包含JSON描述符（描述维度、类型和格式）和二进制数组，支持多种现代二进制容器（如HDF5、Zarr、NPZ）。提供多语言、多框架实现。

Result: 在SuiteSparse和FROSTT数据集上测试，Binsparse HDF5 CSR格式平均文件大小减少2.4倍（无压缩）和7.5倍（压缩）。读写速度显著提升。

Conclusion: Binsparse解决了稀疏数据存储的效率问题，提供高性能、跨平台的解决方案。

Abstract: Sparse matrices and tensors are ubiquitous throughout multiple subfields of
computing. The widespread usage of sparse data has inspired many in-memory and
on-disk storage formats, but the only widely adopted storage specifications are
the Matrix Market and FROSTT file formats, which both use ASCII text. Due to
the inefficiency of text storage, these files typically have larger file sizes
and longer parsing times than binary storage formats, which directly store an
in-memory representation to disk. This can be a major bottleneck; since sparse
computation is often bandwidth-bound, the cost of loading or storing a matrix
to disk often exceeds the cost of performing a sparse computation. While it is
common practice for practitioners to develop their own, custom, non-portable
binary formats for high-performance sparse matrix storage, there is currently
no cross-platform binary sparse matrix storage format. We present Binsparse, a
cross-platform binary sparse matrix and tensor format specification. Binsparse
is a modular, embeddable format, consisting of a JSON descriptor, which
describes the matrix or tensor dimensions, type, and format, and a series of
binary arrays, which can be stored in all modern binary containers, such as
HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse
spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our
Binsparse format on every matrix in the SuiteSparse Matrix Collection and a
selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format
shows file size reductions of 2.4x on average without compression and 7.5x with
compression. We evaluate our parser's read/write performance against a
state-of-the-art Matrix Market parser, demonstrating warm cache mean read
speedups of 26.5x without compression and 2.6x with compression, and write
speedups of 31x without compression and 1.4x with compression.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [126] [Simulation-Based Sensitivity Analysis in Optimal Treatment Regimes and Causal Decomposition with Individualized Interventions](https://arxiv.org/abs/2506.19010)
*Soojin Park,Suyeon Kang,Chioun Lee*

Main category: stat.ML

TL;DR: 本文提出了一种基于模拟的敏感性分析方法，用于处理因果分解分析中未测量的混杂因素，并针对二元风险因素提出了正式的边界策略。


<details>
  <summary>Details</summary>
Motivation: 解决因果分解分析中未测量混杂因素的问题，特别是针对二元风险因素的个体化效应。

Method: 扩展了基于模拟的敏感性分析，模拟未测量的混杂因素，并提出正式的边界策略。

Result: 在HSLS:09数据集上验证了该方法的有效性。

Conclusion: 该方法为二元风险因素的因果分解分析提供了实用的敏感性分析和边界策略。

Abstract: Causal decomposition analysis aims to assess the effect of modifying risk
factors on reducing social disparities in outcomes. Recently, this analysis has
incorporated individual characteristics when modifying risk factors by
utilizing optimal treatment regimes (OTRs). Since the newly defined
individualized effects rely on the no omitted confounding assumption,
developing sensitivity analyses to account for potential omitted confounding is
essential. Moreover, OTRs and individualized effects are primarily based on
binary risk factors, and no formal approach currently exists to benchmark the
strength of omitted confounding using observed covariates for binary risk
factors. To address this gap, we extend a simulation-based sensitivity analysis
that simulates unmeasured confounders, addressing two sources of bias emerging
from deriving OTRs and estimating individualized effects. Additionally, we
propose a formal bounding strategy that benchmarks the strength of omitted
confounding for binary risk factors. Using the High School Longitudinal Study
2009 (HSLS:09), we demonstrate this sensitivity analysis and benchmarking
method.

</details>


### [127] [When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets](https://arxiv.org/abs/2506.19031)
*Chen Zeno,Hila Manor,Greg Ongie,Nir Weinberger,Tomer Michaeli,Daniel Soudry*

Main category: stat.ML

TL;DR: 论文研究了扩散模型中概率流的收敛行为，发现其可能收敛到训练样本或数据流形上的其他点，并探讨了记忆化与样本数量的关系。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型中概率流的收敛行为及其对训练样本的记忆化现象。

Method: 通过分析浅层ReLU神经网络去噪器的概率流，引入简化的分数流，并在正交数据集上进行模拟。

Result: 概率流可能收敛到训练样本、其和或流形上的其他点；记忆化随样本数量增加而减少。

Conclusion: 扩散模型既能记忆训练样本，也能生成新样本，且记忆化程度受样本数量影响。

Abstract: While diffusion models generate high-quality images via probability flow, the
theoretical understanding of this process remains incomplete. A key question is
when probability flow converges to training samples or more general points on
the data manifold. We analyze this by studying the probability flow of shallow
ReLU neural network denoisers trained with minimal $\ell^2$ norm. For
intuition, we introduce a simpler score flow and show that for orthogonal
datasets, both flows follow similar trajectories, converging to a training
point or a sum of training points. However, early stopping by the diffusion
time scheduler allows probability flow to reach more general manifold points.
This reflects the tendency of diffusion models to both memorize training
samples and generate novel points that combine aspects of multiple samples,
motivating our study of such behavior in simplified settings. We extend these
results to obtuse simplex data and, through simulations in the orthogonal case,
confirm that probability flow converges to a training point, a sum of training
points, or a manifold point. Moreover, memorization decreases when the number
of training samples grows, as fewer samples accumulate near training points.

</details>


### [128] [Posterior Contraction for Sparse Neural Networks in Besov Spaces with Intrinsic Dimensionality](https://arxiv.org/abs/2506.19144)
*Kyeongwon Lee,Lizhen Lin,Jaewoo Park,Seonghyun Jeong*

Main category: stat.ML

TL;DR: 稀疏贝叶斯神经网络在各项异性Besov空间及其分层组合上实现了最优后验收缩率，缓解了维度灾难。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯神经网络在高维结构化估计问题中的理论性能，为其实际有效性提供理论支持。

Method: 使用稀疏或连续收缩先验的贝叶斯神经网络，分析其在各项异性Besov空间中的收缩率。

Result: 证明了这些先验能实现依赖于真实结构内在维度的最优收缩率，并具备适应性。

Conclusion: 该研究为贝叶斯神经网络在高维问题中的应用提供了理论依据，展示了其实际有效性。

Abstract: This work establishes that sparse Bayesian neural networks achieve optimal
posterior contraction rates over anisotropic Besov spaces and their
hierarchical compositions. These structures reflect the intrinsic
dimensionality of the underlying function, thereby mitigating the curse of
dimensionality. Our analysis shows that Bayesian neural networks equipped with
either sparse or continuous shrinkage priors attain the optimal rates which are
dependent on the intrinsic dimension of the true structures. Moreover, we show
that these priors enable rate adaptation, allowing the posterior to contract at
the optimal rate even when the smoothness level of the true function is
unknown. The proposed framework accommodates a broad class of functions,
including additive and multiplicative Besov functions as special cases. These
results advance the theoretical foundations of Bayesian neural networks and
provide rigorous justification for their practical effectiveness in
high-dimensional, structured estimation problems.

</details>


### [129] [Rare dense solutions clusters in asymmetric binary perceptrons -- local entropy via fully lifted RDT](https://arxiv.org/abs/2506.19276)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 研究了经典非对称二元感知器（ABP）及其局部熵（LE）作为算法难度的潜在来源，发现典型解与高效算法之间的悖论，并探讨了非典型簇的作用。


<details>
  <summary>Details</summary>
Motivation: 探索ABP的算法难度的来源，特别是局部熵在典型解与非典型解中的作用，以解释高效算法在接近容量时仍能工作的现象。

Method: 利用完全提升的随机对偶理论（fl RDT）及其大偏差升级版（sfl LD RDT）研究ABP的非典型特征，特别是局部熵的行为。

Result: 发现局部熵在α∈(0.77,0.78)区间内崩溃，与当前最佳ABP求解器的能力范围（α∼0.75-0.77）匹配，表明局部熵行为可能是计算间隙的关键反映。

Conclusion: 局部熵的行为与非典型簇的稀疏化或碎片化密切相关，可能是ABP计算间隙存在的重要表现。

Abstract: We study classical asymmetric binary perceptron (ABP) and associated
\emph{local entropy} (LE) as potential source of its algorithmic hardness.
Isolation of \emph{typical} ABP solutions in SAT phase seemingly suggests a
universal algorithmic hardness. Paradoxically, efficient algorithms do exist
even for constraint densities $\alpha$ fairly close but at a finite distance
(\emph{computational gap}) from the capacity. In recent years, existence of
rare large dense clusters and magical ability of fast algorithms to find them
have been posited as the conceptual resolution of this paradox. Monotonicity or
breakdown of the LEs associated with such \emph{atypical} clusters are
predicated to play a key role in their thinning-out or even complete
defragmentation.
  Invention of fully lifted random duality theory (fl RDT) [90,93,94] allows
studying random structures \emph{typical} features. A large deviation upgrade,
sfl LD RDT [96,97], moves things further and enables \emph{atypical} features
characterizations as well. Utilizing the machinery of [96,97] we here develop a
generic framework to study LE as an ABP's atypical feature. Already on the
second level of lifting we discover that the LE results are closely matching
those obtained through replica methods. For classical zero threshold ABP, we
obtain that LE breaks down for $\alpha$ in $(0.77,0.78)$ interval which
basically matches $\alpha\sim 0.75-0.77$ range that currently best ABP solvers
can handle and effectively indicates that LE's behavior might indeed be among
key reflections of the ABP's computational gaps presumable existence.

</details>


### [130] [Near-optimal estimates for the $\ell^p$-Lipschitz constants of deep random ReLU neural networks](https://arxiv.org/abs/2506.19695)
*Sjoerd Dirksen,Patrick Finke,Paul Geuchen,Dominik Stöger,Felix Voigtlaender*

Main category: stat.ML

TL;DR: 研究了ReLU神经网络的ℓ^p-Lipschitz常数，针对随机参数的网络，发现其行为在p∈[1,2)和p∈[2,∞]区间有显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨随机参数ReLU神经网络的ℓ^p-Lipschitz常数，以理解其在不同p值下的行为差异。

Method: 使用变种He初始化权重和对称分布偏置，推导宽网络的高概率上下界。

Result: 发现ℓ^p-Lipschitz常数在p∈[2,∞]类似高斯向量的范数，而在p∈[1,2)更接近其ℓ^2范数。

Conclusion: ℓ^p-Lipschitz常数的行为随p值区间变化显著，为网络设计提供了理论依据。

Abstract: This paper studies the $\ell^p$-Lipschitz constants of ReLU neural networks
$\Phi: \mathbb{R}^d \to \mathbb{R}$ with random parameters for $p \in
[1,\infty]$. The distribution of the weights follows a variant of the He
initialization and the biases are drawn from symmetric distributions. We derive
high probability upper and lower bounds for wide networks that differ at most
by a factor that is logarithmic in the network's width and linear in its depth.
In the special case of shallow networks, we obtain matching bounds. Remarkably,
the behavior of the $\ell^p$-Lipschitz constant varies significantly between
the regimes $ p \in [1,2) $ and $ p \in [2,\infty] $. For $p \in [2,\infty]$,
the $\ell^p$-Lipschitz constant behaves similarly to $\Vert g\Vert_{p'}$, where
$g \in \mathbb{R}^d$ is a $d$-dimensional standard Gaussian vector and $1/p +
1/p' = 1$. In contrast, for $p \in [1,2)$, the $\ell^p$-Lipschitz constant
aligns more closely to $\Vert g \Vert_{2}$.

</details>


### [131] [The Shape of Consumer Behavior: A Symbolic and Topological Analysis of Time Series](https://arxiv.org/abs/2506.19759)
*Pola Bereta,Ioannis Diamantis*

Main category: stat.ML

TL;DR: 论文评估了三种无监督聚类方法（SAX、eSAX和TDA）在Google Trends数据上的表现，发现TDA在捕捉复杂时间序列结构上优于SAX和eSAX。


<details>
  <summary>Details</summary>
Motivation: 理解在线搜索行为的时间模式对实时营销和趋势预测至关重要，但Google Trends数据的高维性和噪声为有效聚类带来挑战。

Method: 研究比较了SAX、eSAX和TDA三种方法在20个Google Trends关键词上的聚类效果。

Result: SAX和eSAX对稳定时间序列快速且可解释，但对复杂和波动数据表现不佳；TDA通过持久同调捕捉全局结构，聚类更平衡且有意义。

Conclusion: 建议结合符号和拓扑方法的混合方法在消费者分析中具有潜力。

Abstract: Understanding temporal patterns in online search behavior is crucial for
real-time marketing and trend forecasting. Google Trends offers a rich proxy
for public interest, yet the high dimensionality and noise of its time-series
data present challenges for effective clustering. This study evaluates three
unsupervised clustering approaches, Symbolic Aggregate approXimation (SAX),
enhanced SAX (eSAX), and Topological Data Analysis (TDA), applied to 20 Google
Trends keywords representing major consumer categories. Our results show that
while SAX and eSAX offer fast and interpretable clustering for stable time
series, they struggle with volatility and complexity, often producing ambiguous
``catch-all'' clusters. TDA, by contrast, captures global structural features
through persistent homology and achieves more balanced and meaningful
groupings.
  We conclude with practical guidance for using symbolic and topological
methods in consumer analytics and suggest that hybrid approaches combining both
perspectives hold strong potential for future applications.

</details>


### [132] [Convergence of Mean Shift Algorithms for Large Bandwidths and Simultaneous Accurate Clustering](https://arxiv.org/abs/2506.19837)
*Susovan Pal,Praneeth Vepakomma*

Main category: stat.ML

TL;DR: 本文扩展了均值漂移（MS）算法的收敛性证明，证明了在足够大的带宽下，任何径向对称且严格正定核的MS算法在任何维度下都能收敛。


<details>
  <summary>Details</summary>
Motivation: 解决均值漂移算法在一般情况下的收敛性问题，扩展前人关于带宽和核函数限制的研究。

Method: 利用Schoenberg和Bernstein对径向对称正定平滑核的两种替代特征，结合前人证明步骤，证明在足够大带宽下的收敛性。

Result: 证明了在足够大带宽下，任何径向对称且严格正定核的MS算法在任何维度下都能收敛。

Conclusion: 尽管结果在带宽限制上比前人研究更严格，但使用了不同的假设和证明技术，为MS算法的收敛性提供了新的理论支持。

Abstract: The mean shift (MS) is a non-parametric, density-based, iterative algorithm
that has prominent usage in clustering and image segmentation. A rigorous proof
for its convergence in full generality remains unknown. Two significant steps
in this direction were taken in the paper \cite{Gh1}, which proved that for
\textit{sufficiently large bandwidth}, the MS algorithm with the Gaussian
kernel always converges in any dimension, and also by the same author in
\cite{Gh2}, proved that MS always converges in one dimension for kernels with
differentiable, strictly decreasing, convex profiles. In the more recent paper
\cite{YT}, they have proved the convergence in more generality,\textit{ without
any restriction on the bandwidth}, with the assumption that the KDE $f$ has a
continuous Lipschitz gradient on the closure of the convex hull of the
trajectory of the iterated sequence of the mode estimate, and also satisfies
the {\L}ojasiewicz property there.
  The main theoretical result of this paper is a generalization of those of
\cite{Gh1}, where we show that (1) for\textit{ sufficiently large bandwidth}
convergence is guaranteed in any dimension with \textit{any radially symmetric
and strictly positive definite kernels}. The proof uses two alternate
characterizations of radially symmetric positive definite smooth kernels by
Schoenberg and Bernstein \cite{Fass}, and borrows some steps from the proofs in
\cite{Gh1}. Although the authors acknowledge that the result in that paper is
more restrictive than that of \cite{YT} due to the lower bandwidth limit, it
uses a different set of assumptions than \cite{YT}, and the proof technique is
different.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [133] [MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications](https://arxiv.org/abs/2506.19502)
*Aleksandr Algazinov,Matt Laing,Paul Laban*

Main category: cs.MA

TL;DR: MATE是一个多模态可访问性多代理系统，通过模态转换满足用户需求，特别为残障人士提供支持。系统灵活、本地运行，确保隐私，并集成ModCon-Task-Identifier模型提升任务识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有多代理系统因闭源设计缺乏定制化，无法全面满足残障人士需求，导致数字环境交互障碍。

Method: 开发MATE系统，支持多模态转换（如图像转音频），兼容多种模型（如LLM API和自定义ML分类器），本地运行保障隐私。

Result: 实验表明，ModCon-Task-Identifier模型在自定义数据上优于其他LLM和统计模型。

Conclusion: MATE为残障人士提供灵活、隐私安全的数字交互解决方案，适用于多领域。

Abstract: Accessibility remains a critical concern in today's society, as many
technologies are not developed to support the full range of user needs.
Existing multi-agent systems (MAS) often cannot provide comprehensive
assistance for users in need due to the lack of customization stemming from
closed-source designs. Consequently, individuals with disabilities frequently
encounter significant barriers when attempting to interact with digital
environments. We introduce MATE, a multimodal accessibility MAS, which performs
the modality conversions based on the user's needs. The system is useful for
assisting people with disabilities by ensuring that data will be converted to
an understandable format. For instance, if the user cannot see well and
receives an image, the system converts this image to its audio description.
MATE can be applied to a wide range of domains, industries, and areas, such as
healthcare, and can become a useful assistant for various groups of users. The
system supports multiple types of models, ranging from LLM API calling to using
custom machine learning (ML) classifiers. This flexibility ensures that the
system can be adapted to various needs and is compatible with a wide variety of
hardware. Since the system is expected to run locally, it ensures the privacy
and security of sensitive information. In addition, the framework can be
effectively integrated with institutional technologies (e.g., digital
healthcare service) for real-time user assistance. Furthermore, we introduce
ModCon-Task-Identifier, a model that is capable of extracting the precise
modality conversion task from the user input. Numerous experiments show that
ModCon-Task-Identifier consistently outperforms other LLMs and statistical
models on our custom data. Our code and data are publicly available at
https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [134] [Towards AI-assisted Neutrino Flavor Theory Design](https://arxiv.org/abs/2506.08080)
*Jason Benjamin Baretz,Max Fieg,Vijay Ganesh,Aishik Ghosh,V. Knapp-Perez,Jake Rudolph,Daniel Whiteson*

Main category: hep-ph

TL;DR: AMBer框架利用强化学习代理与物理软件管道交互，高效搜索粒子物理模型空间，构建可行模型并最小化自由参数。


<details>
  <summary>Details</summary>
Motivation: 粒子物理模型构建依赖理论家直觉且耗时，需寻找高效方法。

Method: 开发AMBer框架，结合强化学习与物理软件管道，选择对称群、粒子内容和表示分配。

Result: 验证了已知理论空间，并探索了新的对称群。

Conclusion: AMBer方法可扩展至其他理论模型构建问题。

Abstract: Particle physics theories, such as those which explain neutrino flavor
mixing, arise from a vast landscape of model-building possibilities. A model's
construction typically relies on the intuition of theorists. It also requires
considerable effort to identify appropriate symmetry groups, assign field
representations, and extract predictions for comparison with experimental data.
We develop an Autonomous Model Builder (AMBer), a framework in which a
reinforcement learning agent interacts with a streamlined physics software
pipeline to search these spaces efficiently. AMBer selects symmetry groups,
particle content, and group representation assignments to construct viable
models while minimizing the number of free parameters introduced. We validate
our approach in well-studied regions of theory space and extend the exploration
to a novel, previously unexamined symmetry group. While demonstrated in the
context of neutrino flavor theories, this approach of reinforcement learning
with physics software feedback may be extended to other theoretical
model-building problems in the future.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [135] [Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications](https://arxiv.org/abs/2506.19491)
*Genís Castillo Gómez-Raya,Álmos Veres-Vitályos,Filip Lemic,Pablo Royo,Mario Montagud,Sergi Fernández,Sergi Abadal,Xavier Costa-Pérez*

Main category: cs.ET

TL;DR: 论文提出了一种将神经3D重建（N3DR）与小型无人机系统结合的方法，以提升静态小物体的3D重建质量，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着无人机小型化的发展，其在室内和难以到达区域的部署潜力增加，但飞行动力学和功耗问题限制了其自主性和任务能力。

Method: 设计并评估了一种基于N3DR的流程，利用Instant-ngp、Nerfacto和Splatfacto等先进模型，通过小型无人机拍摄的图像提升3D重建质量。

Result: 实验表明，N3DR增强的流程显著提高了重建质量，适用于高精度3D映射和受限环境中的异常检测。

Conclusion: N3DR技术有望推动小型无人机系统的能力提升。

Abstract: The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has
expanded their deployment potential to indoor and hard-to-reach areas. However,
this trend introduces distinct challenges, particularly in terms of flight
dynamics and power consumption, which limit the UAVs' autonomy and mission
capabilities. This paper presents a novel approach to overcoming these
limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV
systems for fine-grained 3-Dimensional (3D) digital reconstruction of small
static objects. Specifically, we design, implement, and evaluate an N3DR-based
pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and
Splatfacto, to improve the quality of 3D reconstructions using images of the
object captured by a fleet of small UAVs. We assess the performance of the
considered models using various imagery and pointcloud metrics, comparing them
against the baseline Structure from Motion (SfM) algorithm. The experimental
results demonstrate that the N3DR-enhanced pipeline significantly improves
reconstruction quality, making it feasible for small UAVs to support
high-precision 3D mapping and anomaly detection in constrained environments. In
more general terms, our results highlight the potential of N3DR in advancing
the capabilities of miniaturized UAV systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [136] [Simulation of a closed-loop dc-dc converter using a physics-informed neural network-based model](https://arxiv.org/abs/2506.19178)
*Marc-Antoine Coulombe,Maxime Berger,Antoine Lesage-Landry*

Main category: eess.SY

TL;DR: 提出了一种基于物理信息的双向长短期记忆神经网络（BiLSTM-PINN）模型，用于模拟闭环DC-DC升压转换器的时域响应，并与其他方法进行比较。


<details>
  <summary>Details</summary>
Motivation: 电力电子领域对快速准确的时域分析工具需求增加，现有物理方法速度较慢，数据驱动方法虽快但精度有限。

Method: 使用BiLSTM-PINN、BiLSTM和FCNN三种模型进行对比，通过阶跃响应测试评估性能。

Result: BiLSTM-PINN和BiLSTM在RMSE中位数上分别比FCNN快9倍和4.5倍，且标准差更小，表现更稳定。

Conclusion: BiLSTM-PINN是电力电子模拟中物理或数据驱动方法的潜在替代方案。

Abstract: The growing reliance on power electronics introduces new challenges requiring
detailed time-domain analyses with fast and accurate circuit simulation tools.
Currently, commercial time-domain simulation software are mainly relying on
physics-based methods to simulate power electronics. Recent work showed that
data-driven and physics-informed learning methods can increase simulation speed
with limited compromise on accuracy, but many challenges remain before
deployment in commercial tools can be possible. In this paper, we propose a
physics-informed bidirectional long-short term memory neural network
(BiLSTM-PINN) model to simulate the time-domain response of a closed-loop dc-dc
boost converter for various operating points, parameters, and perturbations. A
physics-informed fully-connected neural network (FCNN) and a BiLSTM are also
trained to establish a comparison. The three methods are then compared using
step-response tests to assess their performance and limitations in terms of
accuracy. The results show that the BiLSTM-PINN and BiLSTM models outperform
the FCNN model by more than 9 and 4.5 times, respectively, in terms of median
RMSE. Their standard deviation values are more than 2.6 and 1.7 smaller than
the FCNN's, making them also more consistent. Those results illustrate that the
proposed BiLSTM-PINN is a potential alternative to other physics-based or
data-driven methods for power electronics simulations.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [137] [Online Learning for Dynamic Vickrey-Clarke-Groves Mechanism in Sequential Auctions under Unknown Environments](https://arxiv.org/abs/2506.19038)
*Vincent Leon,S. Rasoul Etesami*

Main category: cs.GT

TL;DR: 论文研究了未知环境中在线动态机制设计问题，提出了一种基于强化学习的动态VCG机制，解决了序列拍卖中的市场动态变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将问题建模为多臂老虎机或片段式MDP，忽略了市场的持续动态性，本文旨在填补这一空白。

Method: 将序列拍卖建模为无限时域平均奖励MDP，扩展VCG机制至动态环境，并开发在线强化学习算法。

Result: 提出的在线机制渐近收敛于近似满足效率、真实性和个体理性的动态机制，并保证多种遗憾概念的性能。

Conclusion: 该研究为动态市场中的机制设计提供了理论支持和实用算法。

Abstract: We consider the problem of online dynamic mechanism design for sequential
auctions in unknown environments, where the underlying market and, thus, the
bidders' values vary over time as interactions between the seller and the
bidders progress. We model the sequential auctions as an infinite-horizon
average-reward Markov decision process (MDP), where the transition kernel and
reward functions are unknown to the seller. In each round, the seller
determines an allocation and a payment for each bidder. Each bidder receives a
private reward and submits a sealed bid to the seller. The state, which
represents the underlying market, evolves according to an unknown transition
kernel and the seller's allocation policy. Unlike existing works that formulate
the problem as a multi-armed bandit model or as an episodic MDP, where the
environment resets to an initial state after each round or episode, our paper
considers a more realistic and sophisticated setting in which the market
continues to evolve without restarting. We first extend the
Vickrey-Clarke-Groves (VCG) mechanism, which is known to be efficient,
truthful, and individually rational for one-shot static auctions, to sequential
auctions, thereby obtaining a dynamic VCG mechanism counterpart that preserves
these desired properties. We then focus on the online setting and develop an
online reinforcement learning algorithm for the seller to learn the underlying
MDP model and implement a mechanism that closely resembles the dynamic VCG
mechanism. We show that the learned online mechanism asymptotically converges
to a dynamic mechanism that approximately satisfies efficiency, truthfulness,
and individual rationality with arbitrarily high probability and achieves
guaranteed performance in terms of various notions of regret.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [138] [Which Company Adjustment Matter? Insights from Uplift Modeling on Financial Health](https://arxiv.org/abs/2506.19049)
*Xinlin Wang,Mats Brorsson*

Main category: cs.CE

TL;DR: 本文研究了公司调整对财务状况的影响，提出了一种新的提升建模框架（MTDnet）以处理时间依赖性调整，并验证了考虑调整时序的必要性。


<details>
  <summary>Details</summary>
Motivation: 公司调整通常是一系列时间依赖的复杂行为，现有研究多关注二元、多元或连续处理，而忽略了时序因素。本文旨在填补这一空白。

Method: 使用两种元学习器和三种已知提升模型分析简化后的二元调整，并提出MTDnet框架处理时间依赖性调整。实验基于卢森堡公司财务数据。

Result: 实验结果表明，考虑调整的时序因素对效果估计至关重要，MTDnet框架优于传统方法。

Conclusion: 公司调整的效果评估需考虑时序，MTDnet为此提供了有效解决方案。

Abstract: Uplift modeling has achieved significant success in various fields,
particularly in online marketing. It is a method that primarily utilizes
machine learning and deep learning to estimate individual treatment effects.
This paper we apply uplift modeling to analyze the effect of company adjustment
on their financial status, and we treat these adjustment as treatments or
interventions in this study. Although there have been extensive studies and
application regarding binary treatments, multiple treatments, and continuous
treatments, company adjustment are often more complex than these scenarios, as
they constitute a series of multiple time-dependent actions. The effect
estimation of company adjustment needs to take into account not only individual
treatment traits but also the temporal order of this series of treatments. This
study collects a real-world data set about company financial statements and
reported behavior in Luxembourg for the experiments. First, we use two
meta-learners and three other well-known uplift models to analyze different
company adjustment by simplifying the adjustment as binary treatments.
Furthermore, we propose a new uplift modeling framework (MTDnet) to address the
time-dependent nature of these adjustment, and the experimental result shows
the necessity of considering the timing of these adjustment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [139] [Continuous-variable Quantum Diffusion Model for State Generation and Restoration](https://arxiv.org/abs/2506.19270)
*Haitao Huang,Chuangtao Chen,Qinglin Zhao*

Main category: quant-ph

TL;DR: 提出了一种基于连续变量量子扩散原理的新框架，结合CV量子神经网络（CVQNNs），用于生成和恢复复杂量子态，对抗环境噪声。


<details>
  <summary>Details</summary>
Motivation: 解决连续变量量子信息处理中复杂量子态的生成和抗噪声保存问题。

Method: 使用热损失通道驱动的正向扩散过程，并通过CVQNN的时间嵌入反向去噪过程进行状态生成和恢复。

Result: 数值模拟显示高保真度（>99%）生成和恢复多种量子态，包括高斯和非高斯态。

Conclusion: 该框架高效、可扩展，有望成为量子态工程和噪声抑制的实用工具。

Abstract: The generation and preservation of complex quantum states against
environmental noise are paramount challenges in advancing continuous-variable
(CV) quantum information processing. This paper introduces a novel framework
based on continuous-variable quantum diffusion principles, synergizing them
with CV quantum neural networks (CVQNNs) to address these dual challenges. For
the task of state generation, our Continuous-Variable Quantum Diffusion
Generative model (CVQD-G) employs a physically driven forward diffusion process
using a thermal loss channel, which is then inverted by a learnable,
parameter-efficient backward denoising process based on a CVQNN with
time-embedding. This framework's capability is further extended for state
recovery by the Continuous-Variable Quantum Diffusion Restoration model
(CVQD-R), a specialized variant designed to restore quantum states,
particularly coherent states with unknown parameters, from thermal degradation.
Extensive numerical simulations validate these dual capabilities, demonstrating
the high-fidelity generation of diverse Gaussian (coherent, squeezed) and
non-Gaussian (Fock, cat) states, typically with fidelities exceeding 99%, and
confirming the model's ability to robustly restore corrupted states.
Furthermore, a comprehensive complexity analysis reveals favorable training and
inference costs, highlighting the framework's efficiency, scalability, and its
potential as a robust tool for quantum state engineering and noise mitigation
in realistic CV quantum systems.

</details>


### [140] [A Qubit-Efficient Hybrid Quantum Encoding Mechanism for Quantum Machine Learning](https://arxiv.org/abs/2506.19275)
*Hevish Cowlessur,Tansu Alpcan,Chandra Thapa,Seyit Camtepe,Neel Kanth Kundu*

Main category: quant-ph

TL;DR: 提出了一种名为qPGA的非可逆降维方法，用于高效嵌入高维数据到低量子比特系统，提升量子机器学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据在低量子比特系统中的嵌入问题，并增强对重构攻击的抵抗能力。

Method: 利用黎曼几何将数据投影到单位希尔伯特球上，生成适合量子振幅编码的输出，减少量子比特需求。

Result: 在MNIST、Fashion-MNIST和CIFAR-10上，qPGA比量子自编码器更有效地保留局部结构，并在下游分类任务中达到99%以上的准确率。

Conclusion: qPGA为噪声环境下的量子机器学习提供了一种可扩展的解决方案，具有实际应用潜力。

Abstract: Efficiently embedding high-dimensional datasets onto noisy and low-qubit
quantum systems is a significant barrier to practical Quantum Machine Learning
(QML). Approaches such as quantum autoencoders can be constrained by current
hardware capabilities and may exhibit vulnerabilities to reconstruction attacks
due to their invertibility. We propose Quantum Principal Geodesic Analysis
(qPGA), a novel, non-invertible method for dimensionality reduction and
qubit-efficient encoding. Executed classically, qPGA leverages Riemannian
geometry to project data onto the unit Hilbert sphere, generating outputs
inherently suitable for quantum amplitude encoding. This technique preserves
the neighborhood structure of high-dimensional datasets within a compact latent
space, significantly reducing qubit requirements for amplitude encoding. We
derive theoretical bounds quantifying qubit requirements for effective encoding
onto noisy systems. Empirical results on MNIST, Fashion-MNIST, and CIFAR-10
show that qPGA preserves local structure more effectively than both quantum and
hybrid autoencoders. Additionally, we demonstrate that qPGA enhances resistance
to reconstruction attacks due to its non-invertible nature. In downstream QML
classification tasks, qPGA can achieve over 99% accuracy and F1-score on MNIST
and Fashion-MNIST, outperforming quantum-dependent baselines. Initial tests on
real hardware and noisy simulators confirm its potential for noise-resilient
performance, offering a scalable solution for advancing QML applications.

</details>


### [141] [Conservative quantum offline model-based optimization](https://arxiv.org/abs/2506.19714)
*Kristian Sotirov,Annie E. Paine,Savvas Varsamopoulos,Antonio A. Gentile,Osvaldo Simeone*

Main category: quant-ph

TL;DR: 论文提出了一种结合量子极值学习（QEL）和保守目标模型（COM）的混合算法COM-QEL，用于离线模型优化任务，旨在提高预测准确性并避免过度乐观的解。


<details>
  <summary>Details</summary>
Motivation: 离线模型优化任务中，传统方法可能在未探索区域错误预测目标值，导致选择过于乐观的解。QEL虽然具有表达力，但仍存在类似问题。

Method: 提出COM-QEL算法，将QEL与COM结合，利用量子神经网络的表达力，同时通过保守建模确保泛化能力。

Result: 在基准优化任务中，COM-QEL比原始QEL更可靠地找到真实目标值更高的解。

Conclusion: COM-QEL在离线设计问题中表现优越，验证了其结合量子计算和保守建模的有效性。

Abstract: Offline model-based optimization (MBO) refers to the task of optimizing a
black-box objective function using only a fixed set of prior input-output data,
without any active experimentation. Recent work has introduced quantum extremal
learning (QEL), which leverages the expressive power of variational quantum
circuits to learn accurate surrogate functions by training on a few data
points. However, as widely studied in the classical machine learning
literature, predictive models may incorrectly extrapolate objective values in
unexplored regions, leading to the selection of overly optimistic solutions. In
this paper, we propose integrating QEL with conservative objective models (COM)
- a regularization technique aimed at ensuring cautious predictions on
out-of-distribution inputs. The resulting hybrid algorithm, COM-QEL, builds on
the expressive power of quantum neural networks while safeguarding
generalization via conservative modeling. Empirical results on benchmark
optimization tasks demonstrate that COM-QEL reliably finds solutions with
higher true objective values compared to the original QEL, validating its
superiority for offline design problems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [142] [HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps](https://arxiv.org/abs/2506.19268)
*Timoteo Kelly,Abdulkadir Korkmaz,Samuel Mallet,Connor Souders,Sadra Aliakbarpour,Praveen Rao*

Main category: cs.HC

TL;DR: HARPT是一个大规模标注的移动健康应用商店评论语料库，旨在推动用户隐私和信任研究。


<details>
  <summary>Details</summary>
Motivation: 为研究用户隐私和信任问题提供数据支持。

Method: 采用规则过滤、迭代手动标注、数据增强和弱监督等方法构建语料库。

Result: 构建了包含48万条评论的语料库，并展示了分类模型的强性能。

Conclusion: HARPT作为公开资源发布，支持健康信息学、网络安全和自然语言处理研究。

Abstract: We present HARPT, a large-scale annotated corpus of mobile health app store
reviews aimed at advancing research in user privacy and trust. The dataset
comprises over 480,000 user reviews labeled into seven categories that capture
critical aspects of trust in applications, trust in providers and privacy
concerns. Creating HARPT required addressing multiple complexities, such as
defining a nuanced label schema, isolating relevant content from large volumes
of noisy data, and designing an annotation strategy that balanced scalability
with accuracy. This strategy integrated rule-based filtering, iterative manual
labeling with review, targeted data augmentation, and weak supervision using
transformer-based classifiers to accelerate coverage. In parallel, a carefully
curated subset of 7,000 reviews was manually annotated to support model
development and evaluation. We benchmark a broad range of classification
models, demonstrating that strong performance is achievable and providing a
baseline for future research. HARPT is released as a public resource to support
work in health informatics, cybersecurity, and natural language processing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [143] [SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and Neural Steerer](https://arxiv.org/abs/2506.18954)
*Diego Di Carlo,Mathieu Fontaine,Aditya Arie Nugraha,Yoshiaki Bando,Kazuyoshi Yoshii*

Main category: cs.SD

TL;DR: 提出了一种结合α稳定模型和神经网络的声音源定位技术，通过物理信息神经网络（Neural Steerer）插值测量导向向量，提升了多声源场景下的定位性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在多声源场景下定位不准确的问题，利用α稳定模型和神经网络提升鲁棒性和准确性。

Method: 结合α稳定模型与神经网络（Neural Steerer）插值导向向量，估计α稳定空间度量以确定声源方向。

Result: 在多声源场景下，该方法优于现有技术。

Conclusion: 提出的技术在多声源定位中表现优异，为声音源定位提供了新思路。

Abstract: This paper describes a sound source localization (SSL) technique that
combines an $\alpha$-stable model for the observed signal with a neural
network-based approach for modeling steering vectors. Specifically, a
physics-informed neural network, referred to as Neural Steerer, is used to
interpolate measured steering vectors (SVs) on a fixed microphone array. This
allows for a more robust estimation of the so-called $\alpha$-stable spatial
measure, which represents the most plausible direction of arrival (DOA) of a
target signal. As an $\alpha$-stable model for the non-Gaussian case ($\alpha$
$\in$ (0, 2)) theoretically defines a unique spatial measure, we choose to
leverage it to account for residual reconstruction error of the Neural Steerer
in the downstream tasks. The objective scores indicate that our proposed
technique outperforms state-of-the-art methods in the case of multiple sound
sources.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [144] [A Deep Learning Based Method for Fast Registration of Cardiac Magnetic Resonance Images](https://arxiv.org/abs/2506.19167)
*Benjamin Graham*

Main category: eess.IV

TL;DR: 提出了一种快速、轻量化的深度学习神经网络（FLIR），用于心脏图像配准，能够在保持高精度的同时显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 医学图像配准（如心脏运动追踪）对深度学习算法具有挑战性，现有方法要么耗时，要么精度不足，尤其是在动态器官（如心脏）上。

Method: 设计了一种高效的卷积架构（FLIR），用于快速体积配准，并用于量化心脏组织的非均匀应变。

Result: FLIR方法在保持与现有先进模型相似精度的同时，显著减少了推理时间，且应变计算结果具有高度一致性。

Conclusion: FLIR模型为心脏图像配准提供了一种快速、轻量化的解决方案，适合广泛研究和临床应用。

Abstract: Image registration is used in many medical image analysis applications, such
as tracking the motion of tissue in cardiac images, where cardiac kinematics
can be an indicator of tissue health. Registration is a challenging problem for
deep learning algorithms because ground truth transformations are not feasible
to create, and because there are potentially multiple transformations that can
produce images that appear correlated with the goal. Unsupervised methods have
been proposed to learn to predict effective transformations, but these methods
take significantly longer to predict than established baseline methods. For a
deep learning method to see adoption in wider research and clinical settings,
it should be designed to run in a reasonable time on common, mid-level
hardware. Fast methods have been proposed for the task of image registration
but often use patch-based methods which can affect registration accuracy for a
highly dynamic organ such as the heart.
  In this thesis, a fast, volumetric registration model is proposed for the use
of quantifying cardiac strain. The proposed Deep Learning Neural Network (DLNN)
is designed to utilize an architecture that can compute convolutions incredibly
efficiently, allowing the model to achieve registration fidelity similar to
other state-of-the-art models while taking a fraction of the time to perform
inference. The proposed fast and lightweight registration (FLIR) model is used
to predict tissue motion which is then used to quantify the non-uniform strain
experienced by the tissue. For acquisitions taken from the same patient at
approximately the same time, it would be expected that strain values measured
between the acquisitions would have very small differences. Using this metric,
strain values computed using the FLIR method are shown to be very consistent.

</details>


### [145] [NAADA: A Noise-Aware Attention Denoising Autoencoder for Dental Panoramic Radiographs](https://arxiv.org/abs/2506.19387)
*Khuram Naveed,Bruna Neves de Freitas,Ruben Pauwels*

Main category: eess.IV

TL;DR: 提出了一种噪声感知自注意力方法（NAADA网络），用于增强牙科全景X光片的去噪效果，尤其关注高频细节的恢复。


<details>
  <summary>Details</summary>
Motivation: 传统卷积去噪自编码器（DAEs）在恢复高频细节（如牙科X光片中的细微解剖结构）时表现不佳，而现有注意力机制可能忽视噪声区域的关键特征。

Method: 提出噪声感知自注意力方法，构建NAADA网络，专注于噪声区域的关键特征恢复。

Result: 相比Uformer、MResDNN等方法，NAADA在细节重建和图像质量上表现更优。

Conclusion: NAADA网络显著提升了牙科X光片的去噪效果和诊断准确性。

Abstract: Convolutional denoising autoencoders (DAEs) are powerful tools for image
restoration. However, they inherit a key limitation of convolutional neural
networks (CNNs): they tend to recover low-frequency features, such as smooth
regions, more effectively than high-frequency details. This leads to the loss
of fine details, which is particularly problematic in dental radiographs where
preserving subtle anatomical structures is crucial. While self-attention
mechanisms can help mitigate this issue by emphasizing important features,
conventional attention methods often prioritize features corresponding to
cleaner regions and may overlook those obscured by noise. To address this
limitation, we propose a noise-aware self-attention method, which allows the
model to effectively focus on and recover key features even within noisy
regions. Building on this approach, we introduce the noise-aware
attention-enhanced denoising autoencoder (NAADA) network for enhancing noisy
panoramic dental radiographs. Compared with the recent state of the art (and
much heavier) methods like Uformer, MResDNN etc., our method improves the
reconstruction of fine details, ensuring better image quality and diagnostic
accuracy.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [146] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: MoLE（Mix-of-Language-Experts）是一种平衡效率与专业化的多语言编程架构，通过共享和语言特定的LoRA模块实现高效知识共享与专业化。


<details>
  <summary>Details</summary>
Motivation: 解决多语言编程中单一模型牺牲专业化或单独模型计算成本高的问题。

Method: 结合基础模型、共享LoRA模块和语言特定LoRA模块，联合优化。

Result: MoLE在参数效率上优于单独训练的语言特定LoRA，在准确性上优于共享LLM。

Conclusion: MoLE在多语言编程任务中实现了效率与专业化的平衡。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


### [147] [The Autonomous Data Language -- Concepts, Design and Formal Verification](https://arxiv.org/abs/2506.19457)
*Tom T. P. Franken,Thomas Neele,Jan Friso Groote*

Main category: cs.PL

TL;DR: 提出了一种新的并行编程范式——数据自主范式，其中计算由自主数据元素执行，并介绍了首个数据自主编程语言AuDaLa。


<details>
  <summary>Details</summary>
Motivation: 现有并行语言主要关注处理器和线程，导致数据处理复杂，与原始算法脱节。

Method: 提出数据自主范式，数据元素自主协作完成计算，并设计AuDaLa语言，包括类型系统和操作语义。

Result: AuDaLa编程自然，风格与传统并行编程不同，并支持并行程序的正式验证。

Conclusion: 数据自主范式为并行编程提供了新思路，AuDaLa语言展示了其可行性。

Abstract: Nowadays, the main advances in computational power are due to parallelism.
However, most parallel languages have been designed with a focus on processors
and threads. This makes dealing with data and memory in programs hard, which
distances the implementation from its original algorithm. We propose a new
paradigm for parallel programming, the data-autonomous paradigm, where
computation is performed by autonomous data elements. Programs in this paradigm
are focused on making the data collaborate in a highly parallel fashion. We
furthermore present AuDaLa, the first data autonomous programming language, and
provide a full formalisation that includes a type system and operational
semantics. Programming in AuDaLa is very natural, as illustrated by examples,
albeit in a style very different from sequential and contemporary parallel
programming. Additionally, it lends itself for the formal verification of
parallel programs, which we demonstrate.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [148] [Automatic Depression Assessment using Machine Learning: A Comprehensive Survey](https://arxiv.org/abs/2506.18915)
*Siyang Song,Yupeng Huo,Shiqing Tang,Jiaee Cheong,Rui Gao,Michel Valstar,Hatice Gunes*

Main category: q-bio.NC

TL;DR: 本文总结了多模态抑郁相关行为的研究，并全面调查了基于机器学习的自动抑郁评估方法，同时讨论了现有挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统抑郁评估方法存在主观性、效率低和资源不足的问题，而多模态行为分析为自动抑郁评估提供了新思路。

Method: 综述了多模态抑郁相关行为（如大脑活动、语言和非语言行为）及基于机器学习的自动抑郁评估方法。

Result: 总结了不同模态下抑郁行为的特征和现有方法的局限性，并回顾了相关竞赛和数据集。

Conclusion: 指出了自动抑郁评估领域的主要挑战和机遇，为未来研究提供了方向。

Abstract: Depression is a common mental illness across current human society.
Traditional depression assessment relying on inventories and interviews with
psychologists frequently suffer from subjective diagnosis results, slow and
expensive diagnosis process as well as lack of human resources. Since there is
a solid evidence that depression is reflected by various human internal brain
activities and external expressive behaviours, early traditional machine
learning (ML) and advanced deep learning (DL) models have been widely explored
for human behaviour-based automatic depression assessment (ADA) since 2012.
However, recent ADA surveys typically only focus on a limited number of human
behaviour modalities. Despite being used as a theoretical basis for developing
ADA approaches, existing ADA surveys lack a comprehensive review and summary of
multi-modal depression-related human behaviours. To bridge this gap, this paper
specifically summarises depression-related human behaviours across a range of
modalities (e.g. the human brain, verbal language and non-verbal
audio/facial/body behaviours). We focus on conducting an up-to-date and
comprehensive survey of ML-based ADA approaches for learning depression cues
from these behaviours as well as discussing and comparing their distinctive
features and limitations. In addition, we also review existing ADA competitions
and datasets, identify and discuss the main challenges and opportunities to
provide further research directions for future ADA researchers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [149] [Signal Use and Emergent Cooperation](https://arxiv.org/abs/2506.18920)
*Michael Williams*

Main category: cs.AI

TL;DR: 研究探讨了自治代理如何通过通信信号协调活动并提升集体效率，使用NEC-DAC系统展示了代理如何通过学习形成共享行为系统（类似文化），并分析了不同社会结构对合作文化的影响。


<details>
  <summary>Details</summary>
Motivation: 探索自治代理如何通过通信信号自组织形成文化，以及这种文化如何影响其合作效率和群体表现。

Method: 使用NEC-DAC系统，每个代理配备神经网络进行决策，研究不同通信策略和社会结构（如权威层次）对文化形成和代理适应性的影响。

Result: 发现合作文化显著影响部落表现，信号不仅促进文化涌现，还支持跨代文化传播。

Conclusion: 通信信号和文化自组织对代理群体的协调和效率具有重要作用，且不同社会结构对文化形成有显著影响。

Abstract: In this work, we investigate how autonomous agents, organized into tribes,
learn to use communication signals to coordinate their activities and enhance
their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture -
Distributed Autonomous Communicators) system, where each agent is equipped with
its own neural network for decision-making, we demonstrate how these agents
develop a shared behavioral system -- akin to a culture -- through learning and
signalling. Our research focuses on the self-organization of culture within
these tribes of agents and how varying communication strategies impact their
fitness and cooperation. By analyzing different social structures, such as
authority hierarchies, we show that the culture of cooperation significantly
influences the tribe's performance. Furthermore, we explore how signals not
only facilitate the emergence of culture but also enable its transmission
across generations of agents. Additionally, we examine the benefits of
coordinating behavior and signaling within individual agents' neural networks.

</details>


### [150] [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
*Sheraz Khan,Subha Madhavan,Kannan Natarajan*

Main category: cs.AI

TL;DR: 论文指出大型推理模型（LRMs）在特定复杂度阈值后性能崩溃，但批评者认为这是实验设计限制而非认知边界，并通过工具使用展示性能反转。


<details>
  <summary>Details</summary>
Motivation: 探讨LRMs性能崩溃是否源于实验设计限制，而非模型本身的推理能力缺陷。

Method: 通过对比静态文本评估与工具增强模型的性能，分析实验设计的潜在问题。

Result: 工具增强模型能解决原本被认为无法完成的任务，表明性能崩溃源于接口限制而非推理能力。

Conclusion: LRMs的推理能力被低估，性能问题更多源于工具和接口限制，而非模型本身。

Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models via the Lens of
Problem Complexity, presents a compelling empirical finding, a reasoning cliff,
where the performance of Large Reasoning Models (LRMs) collapses beyond a
specific complexity threshold, which the authors posit as an intrinsic scaling
limitation of Chain-of-Thought (CoT) reasoning. This commentary, while
acknowledging the study's methodological rigor, contends that this conclusion
is confounded by experimental artifacts. We argue that the observed failure is
not evidence of a fundamental cognitive boundary, but rather a predictable
outcome of system-level constraints in the static, text-only evaluation
paradigm, including tool use restrictions, context window recall issues, the
absence of crucial cognitive baselines, inadequate statistical reporting, and
output generation limits. We reframe this performance collapse through the lens
of an agentic gap, asserting that the models are not failing at reasoning, but
at execution within a profoundly restrictive interface. We empirically
substantiate this critique by demonstrating a striking reversal. A model,
initially declaring a puzzle impossible when confined to text-only generation,
now employs agentic tools to not only solve it but also master variations of
complexity far beyond the reasoning cliff it previously failed to surmount.
Additionally, our empirical analysis of tool-enabled models like o4-mini and
GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural
execution to complex meta-cognitive self-correction, which has significant
implications for how we define and measure machine intelligence. The illusion
of thinking attributed to LRMs is less a reasoning deficit and more a
consequence of an otherwise capable mind lacking the tools for action.

</details>


### [151] [Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach](https://arxiv.org/abs/2506.19280)
*Feiting Yang,Antoine Moevus,Steve Lévesque*

Main category: cs.AI

TL;DR: 该论文探讨了将情绪检测技术集成到日历应用中，通过生物特征和行为两种方法提升用户体验。生物特征方法使用心电图数据和神经网络预测情绪，行为方法则分析用户交互数据。结果显示行为方法更准确，尤其是鼠标交互，准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互（HCI）的适应性和个性化体验，通过情绪检测技术动态响应用户情绪和压力水平，从而提高生产力和参与度。

Method: 提出两种情绪检测方法：1）基于生物特征（心电图数据，使用LSTM和GRU神经网络预测情绪维度）；2）基于行为（分析鼠标移动、点击和键盘输入等用户交互数据）。

Result: 行为方法在准确性和一致性上表现更优，鼠标交互准确率达90%；生物特征方法中，GRU网络优于LSTM，效价预测准确率达84.38%。

Conclusion: 情绪检测技术可有效提升日历应用的用户体验，行为方法更具实际应用潜力。

Abstract: Human-Computer Interaction (HCI) has evolved significantly to incorporate
emotion recognition capabilities, creating unprecedented opportunities for
adaptive and personalized user experiences. This paper explores the integration
of emotion detection into calendar applications, enabling user interfaces to
dynamically respond to users' emotional states and stress levels, thereby
enhancing both productivity and engagement. We present and evaluate two
complementary approaches to emotion detection: a biometric-based method
utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals
processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
neural networks to predict the emotional dimensions of Valence, Arousal, and
Dominance; and a behavioral method analyzing computer activity through multiple
machine learning models to classify emotions based on fine-grained user
interactions such as mouse movements, clicks, and keystroke patterns. Our
comparative analysis, from real-world datasets, reveals that while both
approaches demonstrate effectiveness, the computer activity-based method
delivers superior consistency and accuracy, particularly for mouse-related
interactions, which achieved approximately 90\% accuracy. Furthermore, GRU
networks outperformed LSTM models in the biometric approach, with Valence
prediction reaching 84.38\% accuracy.

</details>


### [152] [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
*Yan Jiang,Hao Zhou,LiZhong GU,Ai Han,TianLong Li*

Main category: cs.AI

TL;DR: NaviAgent是一种基于图导航的双层规划架构，通过多路径决策器和图编码导航器提升LLM在复杂工具链中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖静态知识和脆弱工具调用的问题，提升复杂异构工具链的协调能力。

Method: 采用双层规划架构，包括多路径决策器和图编码导航器，结合工具依赖异构图和启发式搜索策略。

Result: 在多个基础模型和任务复杂度下，NaviAgent的任务成功率最高，比基线方法平均提升13.5%-19.0%。

Conclusion: NaviAgent在工具链协调中表现出色，尤其在复杂任务和大模型中效果显著。

Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

</details>


### [153] [ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP](https://arxiv.org/abs/2506.19608)
*Zhiyuan Wang,Bokui Chen*

Main category: cs.AI

TL;DR: 论文提出了ChordPrompt框架，通过跨模态提示和多领域自适应文本提示，解决了现有提示学习方法在持续学习中的局限性，提升了模型在多领域增量学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在增量学习场景中表现不佳，且当前提示学习方法主要关注类增量学习，缺乏多领域任务增量学习的策略，同时忽视了跨模态信息交换的潜力。

Method: 提出ChordPrompt框架，结合视觉和文本提示的跨模态交互，并采用领域自适应文本提示选择机制。

Result: 在多领域增量学习基准测试中，ChordPrompt在零样本泛化和下游任务性能上优于现有方法。

Conclusion: ChordPrompt通过跨模态提示和领域自适应机制，显著提升了视觉语言模型在多领域持续学习中的适应性和性能。

Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt
effectively to novel or previously underrepresented data distributions without
comprehensive retraining, enhancing their adaptability and efficiency. While
vision-language models like CLIP show great promise, they struggle to maintain
performance across domains in incremental learning scenarios. Existing prompt
learning methods face two main limitations: 1) they primarily focus on
class-incremental learning scenarios, lacking specific strategies for
multi-domain task incremental learning; 2) most current approaches employ
single-modal prompts, neglecting the potential benefits of cross-modal
information exchange. To address these challenges, we propose the \ChordPrompt
framework, which facilitates a harmonious interplay between visual and textual
prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions
between visual and textual information. Our approach also employs
domain-adaptive text prompts to select appropriate prompts for continual
adaptation across multiple domains. Comprehensive experiments on multi-domain
incremental learning benchmarks demonstrate that \ChordPrompt outperforms
state-of-the-art methods in zero-shot generalization and downstream task
performance.

</details>


### [154] [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
*Baochang Ren,Shuofei Qiao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.AI

TL;DR: KnowRL通过将基于知识验证的事实性奖励整合到RL训练中，减少慢思考模型的幻觉问题，同时保持其推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决慢思考模型因无法准确识别知识边界而产生严重幻觉的问题。

Method: 提出KnowRL，将基于知识验证的事实性奖励整合到RL训练中，指导模型进行基于事实的慢思考。

Result: 在三个幻觉评估数据集和两个推理评估数据集上，KnowRL有效减少了幻觉问题，同时保持了模型的推理能力。

Conclusion: KnowRL通过事实性奖励显著改善了慢思考模型的可靠性，同时不损害其推理能力。

Abstract: Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [155] [Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives](https://arxiv.org/abs/2506.19025)
*Sivaraman Balakrishnan,Tudor Manole,Larry Wasserman*

Main category: math.ST

TL;DR: 本文综述了最优传输（OT）图中估计和极限定理的最新进展，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为实践者提供可靠的推断工具，研究OT图在样本分布中的估计和极限定理。

Method: 回顾OT图的估计方法及极限定理，并探讨特殊情况和变体的类似结果。

Result: 总结了OT图估计的进展，并提出了未来研究的关键方向。

Conclusion: 未来研究应致力于为实践者提供更可靠的推断工具，并扩展OT图的应用范围。

Abstract: In many applications of optimal transport (OT), the object of primary
interest is the optimal transport map. This map rearranges mass from one
probability distribution to another in the most efficient way possible by
minimizing a specified cost. In this paper we review recent advances in
estimating and developing limit theorems for the OT map, using samples from the
underlying distributions. We also review parallel lines of work that establish
similar results for special cases and variants of the basic OT setup. We
conclude with a discussion of key directions for future research with the goal
of providing practitioners with reliable inferential tools.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [156] [From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents](https://arxiv.org/abs/2506.18959)
*Weizhi Zhang,Yangning Li,Yuanchen Bei,Junyu Luo,Guancheng Wan,Liangwei Yang,Chenxuan Xie,Yuyao Yang,Wei-Chieh Huang,Chunyu Miao,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Yankai Chen,Chunkit Chan,Peilin Zhou,Xinyang Zhang,Chenwei Zhang,Jingbo Shang,Ming Zhang,Yangqiu Song,Irwin King,Philip S. Yu*

Main category: cs.IR

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的新范式——Agentic Deep Research，通过结合自主推理、迭代检索和信息合成，显著优于传统信息检索方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于关键词的搜索引擎难以满足复杂、多步骤的信息需求，因此需要更先进的解决方案。

Method: 利用LLMs的推理和代理能力，构建动态反馈循环，结合测试时扩展定律量化计算深度对推理和搜索的影响。

Result: 实验证明Agentic Deep Research显著优于现有方法，并有望成为未来信息检索的主流范式。

Conclusion: Agentic Deep Research通过整合LLMs的能力，为复杂信息需求提供了更高效的解决方案，相关资源已开源。

Abstract: Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [157] [CAM-NET: An AI Model for Whole Atmosphere with Thermosphere and Ionosphere Extension](https://arxiv.org/abs/2506.19340)
*Jiahui Hu,Wenjun Dong*

Main category: physics.space-ph

TL;DR: CAM-NET是一种基于AI的模型，用于高效准确地预测从地表到电离层的中性大气变量，速度比传统模型快1000倍以上。


<details>
  <summary>Details</summary>
Motivation: 准确建模整个大气层对理解重力波传播及其对高层大气动力学的影响至关重要。

Method: 利用球形傅里叶神经算子（SFNO）捕捉全球尺度大气动力学，并采用模块化架构分离核心变量和示踪变量预测。

Result: CAM-NET在精度上与WACCM-X相当，推理速度提升1000倍以上，并能高效预测关键大气参数。

Conclusion: CAM-NET通过模块化设计实现了高效适应性，验证了其在示踪变量预测中的性能。

Abstract: We present Compressible Atmospheric Model-Network (CAM-NET), an AI model
designed to predict neutral atmospheric variables from the Earth's surface to
the ionosphere with high accuracy and computational efficiency. Accurate
modeling of the entire atmosphere is critical for understanding the upward
propagation of gravity waves, which influence upper-atmospheric dynamics and
coupling across atmospheric layers. CAM-NET leverages the Spherical Fourier
Neural Operator (SFNO) to capture global-scale atmospheric dynamics while
preserving the Earth's spherical structure. Trained on a decade of datasets
from the Whole Atmosphere Community Climate Model with thermosphere and
ionosphere eXtension (WACCM-X), CAM-NET demonstrates accuracy comparable to
WACCM-X while achieving a speedup of over 1000x in inference time, can provide
one year simulation within a few minutes once trained. The model effectively
predicts key atmospheric parameters, including zonal and meridional winds,
temperature, and time rate of pressure. Inspired by traditional modeling
approaches that use external couplers to simulate tracer transport, CAM-NET
introduces a modular architecture that explicitly separates tracer prediction
from core dynamics. The core backbone of CAM-NET focuses on forecasting primary
physical variables (e.g., temperature, wind velocity), while tracer variables
are predicted through a lightweight, fine-tuned model. This design allows for
efficient adaptation to specific tracer scenarios with minimal computational
cost, avoiding the need to retrain the entire model. We have validated this
approach on the $O^2$ tracer, demonstrating strong performance and
generalization capabilities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [158] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
*Omer Luxembourg,Haim Permuter,Eliya Nachmani*

Main category: cs.CL

TL;DR: Dilated-scheduled Unmasking Strategy (DUS) 是一种无需额外训练的推理方法，通过并行解掩码提高非自回归文本生成效率，显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于置信度或熵的解掩码启发式方法无法处理并行解掩码中的依赖关系，限制了推理速度。

Method: DUS 利用一阶马尔可夫假设将序列位置分组，实现并行解掩码，减少去噪器调用次数。

Result: 在数学和代码生成任务中，DUS 显著优于现有并行解掩码方法，且无需修改去噪器。

Conclusion: DUS 为高效高质量的文本生成提供了一种轻量级方法，释放了 MDLM 的潜力。

Abstract: Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [159] [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)
*Rasiq Hussain,Jerry Ma,Rithik Khandelwal,Joshua Oltmanns,Mehak Gupta*

Main category: cs.CL

TL;DR: 论文提出了一种结合预训练语言模型和RNN的两步方法，用于从长文本中预测五因素模型人格特质，提高了准确性、效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统人格评估问卷的局限性促使研究者探索基于自然语言处理的新方法，尤其是针对长文本（如超过2000词的叙述性访谈）。

Method: 采用两步法：1）通过滑动窗口微调预训练语言模型提取上下文嵌入；2）使用带注意力机制的RNN整合长距离依赖关系。

Result: 通过消融实验和与LLaMA、Longformer等模型的对比，证明了该方法在预测准确性、效率和可解释性上的提升。

Conclusion: 结合语言特征和长文本建模的方法在人格评估领域具有潜力，尤其是从生活叙事中提取人格特质。

Abstract: Natural Language Processing (NLP) offers new avenues for personality
assessment by leveraging rich, open-ended text, moving beyond traditional
questionnaires. In this study, we address the challenge of modeling long
narrative interview where each exceeds 2000 tokens so as to predict Five-Factor
Model (FFM) personality traits. We propose a two-step approach: first, we
extract contextual embeddings using sliding-window fine-tuning of pretrained
language models; then, we apply Recurrent Neural Networks (RNNs) with attention
mechanisms to integrate long-range dependencies and enhance interpretability.
This hybrid method effectively bridges the strengths of pretrained transformers
and sequence modeling to handle long-context data. Through ablation studies and
comparisons with state-of-the-art long-context models such as LLaMA and
Longformer, we demonstrate improvements in prediction accuracy, efficiency, and
interpretability. Our results highlight the potential of combining
language-based features with long-context modeling to advance personality
assessment from life narratives.

</details>


### [160] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)
*Yuchang Zhu,Zhonghua zhen,Qunshu Lin,Haotong Wei,Xiaolong Sun,Zixuan Yu,Minghao Liu,Zibin Zheng,Liang Chen*

Main category: cs.CL

TL;DR: 研究探讨了LLM生成数据的多样性对下游模型性能的影响，发现适度多样性的数据可提升性能，而过高多样性则有害。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成数据在训练下游模型时因多样性不足导致的性能退化问题。

Method: 通过实验分析不同多样性水平的LLM生成数据及其混合比例对模型性能的影响。

Result: 适度多样性的LLM生成数据在数据不足时能提升性能，过高多样性则有害。

Conclusion: 研究为LLM作为数据生成器的应用提供了实用指导。

Abstract: With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

</details>


### [161] [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)
*Debabrata Mahapatra,Shubham Agarwal,Apoorv Saxena,Subrata Mitra*

Main category: cs.CL

TL;DR: RCStat利用原始注意力logits，通过相对上下文化（RC）统计框架，实现了高效的键值压缩和更高保真度的归因分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖Softmax归一化的注意力权重，掩盖了预Softmax查询键logits的丰富结构。

Method: 引入RCStat框架，利用RC随机变量测量token段的上下文对齐，并推导其高效上界。

Result: 在问答、摘要和归因任务中，RCStat实现了显著的性能提升，无需模型重训练。

Conclusion: RCStat在压缩和归因任务中表现出色，提供了更高效的解决方案。

Abstract: Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.

</details>


### [162] [SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning](https://arxiv.org/abs/2506.19767)
*Yuqian Fu,Tinghong Chen,Jiajun Chai,Xihuai Wang,Songjun Tu,Guojun Yin,Wei Lin,Qichao Zhang,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种名为SRFT的单阶段方法，通过熵感知加权机制统一了监督微调（SFT）和强化学习（RL），在数学推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理任务中取得了显著进展，但如何最优整合SFT和RL仍是一个关键挑战。

Method: 通过分析标记分布和学习动态，提出SRFT方法，结合SFT和RL的优势，直接优化LLM。

Result: SRFT在五个数学推理基准测试中平均准确率达到59.1%，优于零RL方法9.0%。

Conclusion: SRFT通过统一SFT和RL，显著提升了LLM在推理任务中的性能。

Abstract: Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.

</details>


### [163] [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
*Yuqi Zhu,Yi Zhong,Jintian Zhang,Ziheng Zhang,Shuofei Qiao,Yujie Luo,Lun Du,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 研究探讨如何提升开源大语言模型（LLMs）在数据分析任务中的能力，通过数据合成方法显著改进其分析推理能力。


<details>
  <summary>Details</summary>
Motivation: 开源LLMs在推理密集型任务中存在显著限制，研究旨在提升其数据分析能力。

Method: 通过策划多样化的真实场景种子数据集，评估模型在数据理解、代码生成和战略规划三个维度的表现。

Result: 发现战略规划质量是模型性能的关键因素，交互设计和任务复杂性显著影响推理能力，数据质量比多样性对性能影响更大。

Conclusion: 基于这些发现，开发了一种数据合成方法，显著提升了开源LLMs的分析推理能力。

Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [164] [Operator Forces For Coarse-Grained Molecular Dynamics](https://arxiv.org/abs/2506.19628)
*Leon Klein,Atharva Kelkar,Aleksander Durumeric,Yaoyi Chen,Frank Noé*

Main category: physics.chem-ph

TL;DR: 论文提出了一种基于标准化流的核方法，用于改进机器学习粗粒化力场的构建，减少局部失真并保持全局构象准确性。


<details>
  <summary>Details</summary>
Motivation: 传统力匹配方法需要大量原子力数据，但在实际应用中这些数据往往缺失，限制了机器学习粗粒化力场的构建。

Method: 引入基于标准化流的核方法，从构象样本中生成高质量的粗粒化力场。

Result: 在小蛋白质上验证了方法的有效性，表明流基核能仅从构象样本生成高质量力场。

Conclusion: 流基核方法显著减少了局部失真，同时保持了全局准确性，为无原子力数据的粗粒化模拟提供了新途径。

Abstract: Coarse-grained (CG) molecular dynamics simulations extend the length and time
scale of atomistic simulations by replacing groups of correlated atoms with CG
beads. Machine-learned coarse-graining (MLCG) has recently emerged as a
promising approach to construct highly accurate force fields for CG molecular
dynamics. However, the calibration of MLCG force fields typically hinges on
force matching, which demands extensive reference atomistic trajectories with
corresponding force labels. In practice, atomistic forces are often not
recorded, making traditional force matching infeasible on pre-existing
datasets. Recently, noise-based kernels have been introduced to adapt force
matching to the low-data regime, including situations in which reference
atomistic forces are not present. While this approach produces force fields
which recapitulate slow collective motion, it introduces significant local
distortions due to the corrupting effects of the noise-based kernel. In this
work, we introduce more general kernels based on normalizing flows that
substantially reduce these local distortions while preserving global
conformational accuracy. We demonstrate our method on small proteins, showing
that flow-based kernels can generate high-quality CG forces solely from
configurational samples.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [165] [ProxelGen: Generating Proteins as 3D Densities](https://arxiv.org/abs/2506.19820)
*Felix Faltings,Hannes Stark,Regina Barzilay,Tommi Jaakkola*

Main category: q-bio.BM

TL;DR: ProxelGen是一种基于3D密度的蛋白质结构生成模型，优于现有的3D点云表示方法，具有更高的新颖性和设计灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质生成模型多基于3D点云表示，限制了任务的多样性和条件能力。ProxelGen旨在通过3D密度表示（proxels）解决这一问题。

Method: 采用3D CNN-based VAE结合扩散模型，在潜在空间上生成蛋白质的3D密度表示（proxels）。

Result: ProxelGen在生成新颖性、FID分数和设计性方面优于现有模型，并在标准基序支架任务中表现优异。

Conclusion: 3D密度表示为蛋白质生成提供了更灵活的条件能力，ProxelGen展示了其在这一领域的优势。

Abstract: We develop ProxelGen, a protein structure generative model that operates on
3D densities as opposed to the prevailing 3D point cloud representations.
Representing proteins as voxelized densities, or proxels, enables new tasks and
conditioning capabilities. We generate proteins encoded as proxels via a 3D
CNN-based VAE in conjunction with a diffusion model operating on its latent
space. Compared to state-of-the-art models, ProxelGen's samples achieve higher
novelty, better FID scores, and the same level of designability as the training
set. ProxelGen's advantages are demonstrated in a standard motif scaffolding
benchmark, and we show how 3D density-based generation allows for more flexible
shape conditioning.

</details>


### [166] [A standard transformer and attention with linear biases for molecular conformer generation](https://arxiv.org/abs/2506.19834)
*Viatcheslav Gurev,Timothy Rumbell*

Main category: q-bio.BM

TL;DR: 论文提出了一种基于相对位置编码的标准Transformer模型，用于生成分子构象，通过优化编码方式，在较小模型规模下超越了现有非等变模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决非等变模型因缺乏等变偏置而需要大模型规模的问题，探索位置编码在分子构象生成中的有效性。

Method: 采用相对位置编码（类似ALiBi技术）作为负注意力偏置，根据图中节点间最短路径距离动态调整，应用于标准Transformer模型。

Result: 在GEOM-DRUGS基准测试中，2500万参数的模型性能优于6400万参数的非等变基准模型。

Conclusion: 相对位置编码能有效弥补非等变模型的规模限制，为分子构象生成提供新思路。

Abstract: Sampling low-energy molecular conformations, spatial arrangements of atoms in
a molecule, is a critical task for many different calculations performed in the
drug discovery and optimization process. Numerous specialized equivariant
networks have been designed to generate molecular conformations from 2D
molecular graphs. Recently, non-equivariant transformer models have emerged as
a viable alternative due to their capability to scale to improve
generalization. However, the concern has been that non-equivariant models
require a large model size to compensate the lack of equivariant bias. In this
paper, we demonstrate that a well-chosen positional encoding effectively
addresses these size limitations. A standard transformer model incorporating
relative positional encoding for molecular graphs when scaled to 25 million
parameters surpasses the current state-of-the-art non-equivariant base model
with 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative
positional encoding as a negative attention bias that linearly increases with
the shortest path distances between graph nodes at varying slopes for different
attention heads, similar to ALiBi, a widely adopted relative positional
encoding technique in the NLP domain. This architecture has the potential to
serve as a foundation for a novel class of generative models for molecular
conformations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [167] [Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment](https://arxiv.org/abs/2506.19620)
*Mustafa Adam,Kangfeng Ye,David A. Anisi,Ana Cavalcanti,Jim Woodcock,Robert Morris*

Main category: cs.RO

TL;DR: 本文提出了一种农业机器人安全保证框架，通过概率建模和风险分析，量化风险缓解措施的效果，为早期开发阶段提供指导。


<details>
  <summary>Details</summary>
Motivation: 农民对农业机器人可靠性、鲁棒性和安全性的信任是其广泛采用的关键，因此需要研究如何确保机器人能够检测、跟踪和避开障碍物及人类。

Method: 采用概率建模和风险分析框架，通过危险识别、风险评估矩阵和状态机建模，利用PRISM概率模型检查器进行分析。

Result: 量化了不同设计概念（如高性能物体检测系统或更复杂的警告系统）对风险缓解的影响，为开发决策提供依据。

Conclusion: 该框架不仅适用于概念开发阶段，还可用于实施、部署和操作阶段，为农业机器人的安全保证提供支持。

Abstract: Continued adoption of agricultural robots postulates the farmer's trust in
the reliability, robustness and safety of the new technology. This motivates
our work on safety assurance of agricultural robots, particularly their ability
to detect, track and avoid obstacles and humans. This paper considers a
probabilistic modelling and risk analysis framework for use in the early
development phases. Starting off with hazard identification and a risk
assessment matrix, the behaviour of the mobile robot platform, sensor and
perception system, and any humans present are captured using three state
machines. An auto-generated probabilistic model is then solved and analysed
using the probabilistic model checker PRISM. The result provides unique insight
into fundamental development and engineering aspects by quantifying the effect
of the risk mitigation actions and risk reduction associated with distinct
design concepts. These include implications of adopting a higher performance
and more expensive Object Detection System or opting for a more elaborate
warning system to increase human awareness. Although this paper mainly focuses
on the initial concept-development phase, the proposed safety assurance
framework can also be used during implementation, and subsequent deployment and
operation phases.

</details>


### [168] [A Verification Methodology for Safety Assurance of Robotic Autonomous Systems](https://arxiv.org/abs/2506.19622)
*Mustafa Adam,David A. Anisi,Pedro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种用于农业自主机器人的安全验证工作流程，涵盖从概念设计到运行时验证的全生命周期，旨在确保其在动态环境中的安全性和合规性。


<details>
  <summary>Details</summary>
Motivation: 在共享人类环境中部署的自主机器人需要严格的安全保障，以满足功能可靠性和法规要求。农业环境中的动态性和非结构化特性增加了安全验证的复杂性。

Method: 通过系统性危害分析和风险评估识别潜在风险，并制定安全要求；开发安全控制器的形式化模型，验证其满足安全属性。

Result: 在农业机器人上的应用表明，该方法能有效验证安全关键属性，并早期发现设计问题。

Conclusion: 该方法有助于开发更安全的自主系统，提升农业机器人的安全性和可靠性。

Abstract: Autonomous robots deployed in shared human environments, such as agricultural
settings, require rigorous safety assurance to meet both functional reliability
and regulatory compliance. These systems must operate in dynamic, unstructured
environments, interact safely with humans, and respond effectively to a wide
range of potential hazards. This paper presents a verification workflow for the
safety assurance of an autonomous agricultural robot, covering the entire
development life-cycle, from concept study and design to runtime verification.
The outlined methodology begins with a systematic hazard analysis and risk
assessment to identify potential risks and derive corresponding safety
requirements. A formal model of the safety controller is then developed to
capture its behaviour and verify that the controller satisfies the specified
safety properties with respect to these requirements. The proposed approach is
demonstrated on a field robot operating in an agricultural setting. The results
show that the methodology can be effectively used to verify safety-critical
properties and facilitate the early identification of design issues,
contributing to the development of safer robots and autonomous systems.

</details>


### [169] [CUPID: Curating Data your Robot Loves with Influence Functions](https://arxiv.org/abs/2506.19121)
*Christopher Agia,Rohan Sinha,Jingyun Yang,Rika Antonova,Marco Pavone,Haruki Nishimura,Masha Itkina,Jeannette Bohg*

Main category: cs.RO

TL;DR: CUPID是一种基于影响函数理论的机器人数据整理方法，通过评估演示数据对策略性能的影响，优化数据选择，提升模仿学习效果。


<details>
  <summary>Details</summary>
Motivation: 模仿学习中，演示数据的质量和组成直接影响策略性能，但理解单个演示对任务成功或失败的具体贡献仍具挑战性。

Method: 提出CUPID方法，利用影响函数理论评估每个训练演示对策略预期回报的影响，进而筛选和优化数据。

Result: 实验表明，CUPID能显著提升策略性能，例如仅用33%的精选数据即可达到最佳效果，并在硬件实验中验证了其鲁棒性。

Conclusion: CUPID通过数据整理显著提升模仿学习策略性能，适用于多种场景，包括分布偏移和通用策略的优化。

Abstract: In robot imitation learning, policy performance is tightly coupled with the
quality and composition of the demonstration data. Yet, developing a precise
understanding of how individual demonstrations contribute to downstream
outcomes - such as closed-loop task success or failure - remains a persistent
challenge. We propose CUPID, a robot data curation method based on a novel
influence function-theoretic formulation for imitation learning policies. Given
a set of evaluation rollouts, CUPID estimates the influence of each training
demonstration on the policy's expected return. This enables ranking and
selection of demonstrations according to their impact on the policy's
closed-loop performance. We use CUPID to curate data by 1) filtering out
training demonstrations that harm policy performance and 2) subselecting newly
collected trajectories that will most improve the policy. Extensive simulated
and hardware experiments show that our approach consistently identifies which
data drives test-time performance. For example, training with less than 33% of
curated data can yield state-of-the-art diffusion policies on the simulated
RoboMimic benchmark, with similar gains observed in hardware. Furthermore,
hardware experiments show that our method can identify robust strategies under
distribution shift, isolate spurious correlations, and even enhance the
post-training of generalist robot policies. Additional materials are made
available at: https://cupid-curation.github.io.

</details>


### [170] [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
*Federico Tavella,Kathryn Mearns,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 比较了机器人场景理解中视觉语言模型（VLMs）的标题生成策略，评估了单视角与多视角标题生成的性能差异，以及识别真实物体与3D打印物体的能力。


<details>
  <summary>Details</summary>
Motivation: 研究机器人如何利用视觉语言模型生成自然语言描述，以提升场景理解能力。

Method: 通过机器人手臂配备的RGB相机采集多视角图像，比较BLIP和VLMs等模型的标题生成性能。

Result: VLMs在常见物体识别中表现良好，但对新颖表征泛化能力不足。

Conclusion: 研究为实际部署基础模型于机器人场景提供了实用见解。

Abstract: Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [171] [First-Order Sparse Convex Optimization: Better Rates with Sparse Updates](https://arxiv.org/abs/2506.19075)
*Dan Garber*

Main category: math.OC

TL;DR: 论文提出了一种利用稀疏更新实现线性收敛的方法，显著降低了运行时间，并简化了实现。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏最优解的凸优化问题，现有方法虽然能实现线性收敛，但每次迭代的运行时间仍较高，难以应用于高维问题。

Method: 通过仅使用稀疏更新，实现依赖改进混合范数条件数的线性收敛。

Result: 方法不仅保持了线性收敛率，还显著降低了运行时间，且更易实现。

Conclusion: 稀疏更新方法为高维稀疏优化问题提供了高效且实用的解决方案。

Abstract: In was recently established that for convex optimization problems with a
sparse optimal solution (may it be entry-wise sparsity or matrix rank-wise
sparsity) it is possible to have linear convergence rates which depend on an
improved mixed-norm condition number of the form $\frac{\beta_1{}s}{\alpha_2}$,
where $\beta_1$ is the $\ell_1$-Lipchitz continuity constant of the gradient,
$\alpha_2$ is the $\ell_2$-quadratic growth constant, and $s$ is the sparsity
of the optimal solution. However, beyond the improved convergence rate, these
methods are unable to leverage the sparsity of optimal solutions towards
improving also the runtime of each iteration, which may still be prohibitively
high for high-dimensional problems. In this work, we establish that linear
convergence rates which depend on this improved condition number can be
obtained using only sparse updates, which may result in overall significantly
improved running times. Moreover, our methods are considerably easier to
implement.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [172] [phylo2vec: a library for vector-based phylogenetic tree manipulation](https://arxiv.org/abs/2506.19490)
*Neil Scheidwasser,Ayush Nag,Matthew J Penn,Anthony MV Jakob,Frederik Mølkjær Andersen,Mark P Khurana,Landung Setiawan,Madeline Gordon,David A Duchêne,Samir Bhatt*

Main category: q-bio.PE

TL;DR: phylo2vec是一种高性能软件包，用于编码、操作和分析二进制系统发育树，相比传统Newick格式，具有更高的内存效率和易操作性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基因组学和SARS-CoV-2疫情的发展，需要处理大规模基因组或系统发育树数据的软件工具。

Method: phylo2vec使用整数向量表示树拓扑，提供快速采样和比较功能，核心实现基于Rust，支持Python和R。

Result: phylo2vec在性能和内存效率上优于传统Newick格式，适用于下游机器学习任务。

Conclusion: phylo2vec为系统发育树的高效表示和分析提供了创新解决方案，适用于广泛的研究和应用场景。

Abstract: Phylogenetics is a fundamental component of many analysis frameworks in
biology as well as linguistics to study the evolutionary relationships of
different entities. Recently, the advent of large-scale genomics and the
SARS-CoV-2 pandemic has underscored the necessity for phylogenetic software to
handle large datasets of genomes or phylogenetic trees. While significant
efforts have focused on scaling optimisation algorithms, visualization, and
lineage identification, an emerging body of research has been dedicated to
efficient representations of data for genomes and phylogenetic trees such as
phylo2vec. Compared to traditional tree representations such as the Newick
format, which represents trees using strings of nested parentheses, modern
representations of phylogenetic trees utilize integer vectors to define the
tree topology traversal. This approach offers several advantages, including
easier manipulability, increased memory efficiency, and applicability to
downstream tasks such as machine learning. Here, we present the latest release
of phylo2vec (or Phylo2Vec), a high-performance software package for encoding,
manipulating, and analysing binary phylogenetic trees. At its core, the package
is based on the phylo2vec representation of binary trees, which defines a
bijection from any tree topology with $n$ leaves into an integer vector of size
$n-1$. Compared to the traditional Newick format, phylo2vec is designed to
enable fast sampling and comparison of binary trees. This release features a
core implementation in Rust, providing significant performance improvements and
memory efficiency, while remaining available in Python (superseding the release
described in the original paper) and R via dedicated wrappers, making it
accessible to a broad audience in the bioinformatics community.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [173] [A comparative analysis of machine learning algorithms for predicting probabilities of default](https://arxiv.org/abs/2506.19789)
*Adrian Iulian Cristescu,Matteo Giordano*

Main category: q-fin.RM

TL;DR: 比较五种机器学习算法与传统逻辑回归在贷款违约概率预测中的表现，强调机器学习在信用风险分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法在预测任务中表现优异，但在信用风险分析中应用不足，本文旨在探索其在该领域的潜力。

Method: 使用随机森林、决策树、XGBoost、梯度提升和AdaBoost五种算法，与逻辑回归在基准数据集上进行比较。

Result: 研究揭示了每种方法的优缺点，为贷款组合中的违约概率预测提供了有效算法选择。

Conclusion: 机器学习算法在信用风险分析中具有显著潜力，可为金融机构提供更准确的违约预测工具。

Abstract: Predicting the probability of default (PD) of prospective loans is a critical
objective for financial institutions. In recent years, machine learning (ML)
algorithms have achieved remarkable success across a wide variety of prediction
tasks; yet, they remain relatively underutilised in credit risk analysis. This
paper highlights the opportunities that ML algorithms offer to this field by
comparing the performance of five predictive models-Random Forests, Decision
Trees, XGBoost, Gradient Boosting and AdaBoost-to the predominantly used
logistic regression, over a benchmark dataset from Scheule et al. (Credit Risk
Analytics: The R Companion). Our findings underscore the strengths and
weaknesses of each method, providing valuable insights into the most effective
ML algorithms for PD prediction in the context of loan portfolios.

</details>
