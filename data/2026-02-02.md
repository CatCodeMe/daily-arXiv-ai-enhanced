<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 26]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 183]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 12]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 16]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 昂热大学通过行动学习模式，让企业员工在专业项目中进行数据库设计培训，结合技术课程、职业培训和咨询服务优势，实现可评估的长期教学与职业目标。


<details>
  <summary>Details</summary>
Motivation: 数据库设计传统上是专业数据处理人员的领域，但企业员工了解业务流程却缺乏技术能力。行动学习旨在将技术课程的成功因素、职业培训的适应性教学和咨询服务的专业能力相结合，为无数据处理背景的企业员工提供有效的数据库设计培训。

Method: 采用行动学习模式，由昂热大学继续教育学院组织，教师担任项目管理的监督者角色。培训基于专业机构资助的实际项目，将教师的知识传授与咨询顾问的经验相结合，让学员在真实项目中应用新学知识。

Result: 行动学习成功整合了法国技术课程的三个成功要素：技术课程的横向学期项目模式、职业培训的适应性教学、以及咨询服务的专业能力。这种培训模式实现了可评估的长期教学和职业目标。

Conclusion: 行动学习是法国促进大学与企业合作政策的一部分，通过结合技术教育、职业培训和专业咨询的最佳实践，为无技术背景的企业员工提供了有效的数据库设计培训，实现了教学与职业发展的双重目标。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [2] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出SRIU算法，用于挖掘具有递增效用比的高效用序列规则，通过左右扩展和右左扩展两种方法，并采用多种剪枝策略和优化技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前高效用序列规则挖掘方法中，规则与其生成过程之间的关联不明确，无法确定新增项如何影响规则的效用或置信度变化，因此需要研究具有递增效用比的规则挖掘问题。

Method: 提出SRIU算法，采用左右扩展和右左扩展两种扩展方法，引入项对估计效用剪枝策略(IPEUP)减少搜索空间，为两种扩展方法设计上界和剪枝策略，使用Bitmap减少内存消耗，设计紧凑效用表优化挖掘过程。

Result: 在真实世界和合成数据集上的大量实验结果表明该方法的有效性，使用置信度和确信度等指标评估生成的序列规则质量，证明SRIU能提高挖掘结果的相关性。

Conclusion: SRIU算法能够有效挖掘具有递增效用比的高效用序列规则，通过多种优化技术提高了挖掘效率，生成的规则具有更好的相关性，为决策提供更可靠的信息。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [3] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出RSC算法，通过置信度引导的分割减少高效用序列规则挖掘中的冗余效用计算


<details>
  <summary>Details</summary>
Motivation: 现有高效用序列规则挖掘算法存在冗余效用计算问题，不同规则可能包含相同的项目序列，当这些项目能形成多个不同规则时需要重复计算效用

Method: 提出RSC算法：1) 使用置信度引导的分割减少冗余计算；2) 预先利用候选子序列支持度预计算分割规则的置信度；3) 确定分割点后同时生成所有不同前件和后件的规则；4) 使用效用链接表加速候选序列生成；5) 引入更严格的效用上界（序列的减少剩余效用）处理重复项目

Result: 在多个数据集上评估RSC方法，结果显示相比现有最先进方法有改进

Conclusion: RSC算法通过置信度引导的分割有效减少了高效用序列规则挖掘中的冗余效用计算，提高了算法效率

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


### [4] [COL-Trees: Efficient Hierarchical Object Search in Road Networks](https://arxiv.org/abs/2601.22183)
*Tenindra Abeywickrama,Muhammad Aamir Cheema,Sabine Storandt*

Main category: cs.DB

TL;DR: 提出COL-Tree数据结构，使用地标启发式实现高效层次图遍历，显著提升聚合k近邻、k最远邻等查询性能


<details>
  <summary>Details</summary>
Motivation: 现有基于欧几里得距离的启发式方法在图结构（如道路网络）中效果有限，无法有效支持聚合k近邻、k最远邻等复杂查询需求

Method: 提出COL-Tree（紧凑对象-地标树）数据结构，采用更准确的地标启发式实现高效层次图遍历，并设计基于COL-Tree的查询算法

Result: 在真实世界和合成数据集上的实验表明，该方法显著优于现有方法，性能提升可达4个数量级，且预处理开销较小

Conclusion: COL-Tree通过地标启发式有效解决了图结构中复杂查询的性能问题，为位置服务提供了高效解决方案

Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby POIs for a single agent, these search heuristics do not translate to many other applications. For example, Aggregate k Nearest Neighbor (AkNN) queries require POIs that are close to multiple agents. k Farthest Neighbor (kFN) queries require POIs that are the antithesis of nearest. Such problems naturally benefit from a hierarchical approach, but existing methods rely on Euclidean-based heuristics, which have diminished effectiveness in graphs such as road networks. We propose a novel data structure, COL-Tree (Compacted Object-Landmark Tree), to address this gap by enabling efficient hierarchical graph traversal using a more accurate landmark-based heuristic. We then present query algorithms that utilize COL-Trees to efficiently answer AkNN, kFN, and other queries. In our experiments on real-world and synthetic datasets, we demonstrate that our techniques significantly outperform existing approaches, achieving up to 4 orders of magnitude improvement. Moreover, this comes at a small pre-processing overhead in both theory and practice.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow是一个针对LLM服务系统的容错架构，通过解耦模型并行初始化、动态流量重路由和后台KV缓存复制，显著降低故障恢复时间并提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在超大规模集群中面对硬件故障时非常脆弱，现有恢复机制过慢（需要长达10分钟重新初始化资源和加载模型权重），导致服务中断时间过长。

Method: 采用三种关键技术：1) 解耦模型并行初始化，2) 动态流量重路由，3) 后台KV缓存复制，以在部分故障时维持高吞吐量。

Result: KevlarFlow将平均恢复时间(MTTR)降低20倍，在故障条件下：平均延迟提升3.1倍，p99延迟提升2.8倍，平均首token时间(TTFT)提升378.9倍，p99 TTFT提升574.6倍，运行时开销可忽略。

Conclusion: KevlarFlow有效解决了LLM服务系统在硬件不可靠性与服务可用性之间的差距，显著提升了系统的容错能力和服务质量。

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [6] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: 数据中心的AI/ML增长导致能耗和碳排放增加，本文提出数据中心参与电网频率调节可减少化石燃料备用需求，降低外源性碳排放


<details>
  <summary>Details</summary>
Motivation: AI/ML数据中心能耗增长导致碳排放增加，向可再生能源转型和能源需求增长可能破坏电网稳定。电网依赖化石燃料发电厂进行频率调节，存在隐藏碳排放问题

Method: 提出"外源性碳"新指标量化数据中心参与调节服务带来的电网侧碳减排，开发EcoCenter框架最大化GPU数据中心提供频率调节的能力，减少化石燃料备用需求

Result: 数据中心参与频率调节可产生外源性碳节约，通常超过其运行碳排放，表明数据中心可作为电网稳定资源而非仅仅是能源消耗者

Conclusion: 数据中心通过参与电网频率调节服务，不仅能减少对化石燃料备用容量的依赖，还能产生显著的外源性碳减排效益，为可持续数据中心运营提供新途径

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [7] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL是一个异构GPU集合通信库，支持跨NVIDIA和AMD GPU的高性能通信，无需修改现有深度学习应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，组织需要扩展GPU集群，通常包含多个厂商的GPU。然而，当前深度学习框架缺乏对异构GPU集合通信的支持，导致效率低下和成本增加。

Method: HetCCL通过统一厂商特定的后端（NVIDIA NCCL和AMD RCCL），引入两种新颖机制实现跨厂商通信，支持基于RDMA的通信而无需修改驱动程序。

Result: 在多厂商GPU集群上的评估显示，HetCCL在同类GPU设置中与NCCL和RCCL性能相当，在异构环境中能够独特地扩展，实现高性能训练。

Conclusion: HetCCL使得使用NVIDIA和AMD GPU进行实际高性能训练成为可能，无需修改现有深度学习应用，解决了异构GPU集群的通信瓶颈问题。

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [8] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: CONCUR是一个轻量级控制层，通过智能的代理准入控制来防止KV缓存中的中间阶段抖动问题，显著提升批量推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 批量推理中的代理工作负载会导致GPU KV缓存出现中间阶段抖动现象，即缓存效率随着长时间运行的代理积累状态而崩溃，即使内存容量未耗尽也会严重降低吞吐量。

Method: CONCUR采用基于分布式系统拥塞控制思想的轻量级控制层，通过反馈驱动的调节机制，动态调整活跃代理数量以限制聚合缓存压力，同时保持执行连续性。

Result: 在大型模型和真实世界代理工作负载上，CONCUR防止了中间阶段抖动，在Qwen3-32B上提升批量推理吞吐量达4.09倍，在DeepSeek-V3上提升1.9倍，且与现有LLM服务系统兼容。

Conclusion: 解决KV缓存中间阶段抖动需要从反应式请求级缓存管理转向主动式代理级准入控制，CONCUR通过缓存感知的控制算法有效调节代理准入，显著提升系统吞吐量。

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [9] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft：基于DSL引导的AscendC内核自动生成方法，通过领域特定语言抽象NPU编程复杂性，使用LLM进行结构化转换，显著提升华为昇腾NPU内核生成的正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型性能高度依赖高效内核实现，但为专用加速器（如NPU）开发高性能内核耗时且需要专业知识。虽然LLM已能生成正确且高性能的GPU内核，但NPU内核生成因领域特定编程模型、有限公开示例和稀疏文档而面临巨大挑战，直接生成AscendC内核的正确率极低。

Method: 提出AscendCraft方法：1）引入轻量级DSL抽象非必要复杂性，显式建模昇腾特定执行语义；2）使用类别特定专家示例在DSL中生成内核；3）通过结构化、约束驱动的LLM降低传递将DSL代码转编译为AscendC。

Result: 在MultiKernelBench的7个算子类别评估中：编译成功率98.1%，功能正确性90.4%。46.2%的生成内核匹配或超过PyTorch eager执行性能。此外，成功为新提出的mHC架构生成两个正确内核，性能大幅超越PyTorch eager执行。

Conclusion: DSL引导的转编译方法使LLM能够生成既正确又具有竞争力的NPU内核，填补了GPU与NPU内核生成之间的显著差距，为专用加速器的高效内核开发提供了可行方案。

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [10] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs在并发事件下可能出现状态"回滚"问题，特别是在权限管理场景中，如"决斗管理员"问题。本文提出通过异步批处理的"纪元事件"仲裁机制，在保持可用性的同时引入有界全序，提升CRDT的一致性。


<details>
  <summary>Details</summary>
Motivation: CRDTs在分区时优先保证可用性而非一致性，节点以不同顺序累积事件，依赖合并函数呈现物化视图。但在某些情况下，物化视图的状态可能"回滚"先前应用的事件，当用于管理即时通讯等应用的群组权限时，会导致意外行为，如"决斗管理员"问题中两个管理员同时撤销对方权限的情况。

Method: 提出通过外部仲裁器仲裁并发事件之间的不可变happens-before关系。仲裁通过可选的"纪元事件"异步批量进行，保持可用性。这引入了纪元内的有界全序，产生"最终性"结果。

Result: 该方法改进了CRDTs能够提供的一致性级别，通过引入有界全序和最终性，解决了并发事件导致的权限冲突问题，防止拜占庭管理员利用并发性赢得"决斗"。

Conclusion: CRDTs在权限管理等场景中需要更强的顺序保证。通过异步批处理的纪元事件仲裁机制，可以在保持CRDT可用性的同时，提供更好的顺序一致性，防止并发事件导致的意外行为。

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Scalable Fair Influence Blocking Maximization via Approximately Monotonic Submodular Optimization](https://arxiv.org/abs/2601.22584)
*Qiangpeng Fang,Jilong Shi,Xiaobin Rui,Jian Zhang,Zhixiao Wang*

Main category: cs.DS

TL;DR: 提出公平性感知的影响阻断最大化方法，通过可调标量化平衡阻断效果与社区公平性，开发高效算法CELF-R实现理论保证的近似最优解


<details>
  <summary>Details</summary>
Motivation: 现有影响阻断最大化方法只关注阻断效果最大化，忽视了不同社区间的公平性问题，而强制实施公平性（如人口统计平等）在计算上具有挑战性，传统方法依赖昂贵的线性规划求解器，难以扩展到大规模网络

Method: 提出人口统计平等感知的目标函数，保持近似单调子模结构以实现高效优化；通过可调标量化将公平性与阻断效果整合；开发CELF-R算法，利用近似子模性消除冗余评估，支持帕累托前沿构建

Result: CELF-R算法在广泛实验中始终优于现有基线方法，在保持高效率的同时实现(1-1/e-ψ)-近似解，能够有效构建公平性与阻断效果之间的权衡帕累托前沿

Conclusion: 该研究首次将公平性正式引入影响阻断最大化问题，提出计算高效且具有理论保证的解决方案，为实际应用中的公平性-效果权衡提供了实用框架

Abstract: Influence Blocking Maximization (IBM) aims to select a positive seed set to suppress the spread of negative influence. However, existing IBM methods focus solely on maximizing blocking effectiveness, overlooking fairness across communities. To address this issue, we formalize fairness in IBM and justify Demographic Parity (DP) as a notion that is particularly well aligned with its semantics. Yet enforcing DP is computationally challenging: prior work typically formulates DP as a Linear Programming (LP) problem and relies on costly solvers, rendering them impractical for large-scale networks. In this paper, we propose a DP-aware objective while maintaining an approximately monotonic submodular structure, enabling efficient optimization with theoretical guarantees. We integrate this objective with blocking effectiveness through a tunable scalarization, yielding a principled fairness-effectiveness trade-offs. Building on this structure, we develop CELF-R, an accelerated seed selection algorithm that exploits approximate submodularity to eliminate redundant evaluations and naturally supports Pareto front construction. Extensive experiments demonstrate that CELF-R consistently outperforms state-of-the-art baselines, achieving a $(1-1/e-ψ)$-approximate solution while maintaining high efficiency.

</details>


### [12] [Competitive Non-Clairvoyant KV-Cache Scheduling for LLM Inference](https://arxiv.org/abs/2601.22996)
*Yiding Feng,Zonghan Yang,Yuhao Zhang*

Main category: cs.DS

TL;DR: 提出了几何切片算法（GSA），这是首个在离线批处理场景下实现常数竞争比的非先知KV缓存调度算法，解决了LLM推理中内存预算约束下的延迟最小化问题。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的KV缓存调度面临独特挑战：作业内存占用随解码token数线性增长，调度决策与可行性耦合，且请求响应长度未知。现有方法要么假设先知（已知响应长度），要么依赖机器学习预测，缺乏无先验知识下的鲁棒性能保证。

Method: 提出几何切片算法（GSA）：通过几何相位结构周期性重启作业以限制内存暴露，结合交错流水线机制平滑聚合内存消耗以实现高并发。还提出了先知版本的几何批处理算法（GBA）。

Result: GSA在一般实例中达到最多61.92的竞争比，在大内存场景下改进到32。GBA在一般实例中达到10.67的近似比，在大内存场景下达到6.75，显著优于先前超过9000的最佳界限。真实请求轨迹实验验证了算法的鲁棒性。

Conclusion: 首次在无先验知识条件下为LLM推理的KV缓存调度问题提供了常数竞争比保证，解决了理论和实践中的重要开放问题，算法框架同时提供了先知和非先知版本，均显著优于现有方法。

Abstract: Large Language Model (LLM) inference presents a unique scheduling challenge due to the Key-Value (KV) cache, where a job's memory footprint grows linearly with the number of decoded tokens. This growth couples scheduling decisions with feasibility: a scheduler must minimize latency under a hard memory budget, yet the response lengths of requests are inherently unknown. While recent works have explored this problem either assuming clairvoyance -- exact knowledge of response lengths -- or relying on machine-learned predictions, obtaining robust performance guarantees without any prior knowledge of job sizes remains a theoretically fundamental and practically important open problem.
  In this work, we propose the Geometric Slicing Algorithm (GSA), the non-clairvoyant policy to achieve the first constant competitive ratio for this problem in the offline batch setting. GSA manages uncertainty through a geometric phase structure that periodically restarts jobs to bound memory exposure, combined with a staggered pipeline mechanism that enables high concurrency by smoothing aggregate memory consumption. We prove that GSA achieves a competitive ratio of at most 61.92 for general instances, improving to 32 in the large-memory regime. Our algorithmic framework also yields a clairvoyant counterpart, the Geometric Batching Algorithm (GBA), which achieves an approximation ratio of 10.67 for general instances and 6.75 in the large-memory regime -- significantly improving upon the best previously known bound of over 9000. Numerical experiments on real request traces demonstrate that our algorithms perform robustly while preserving these worst-case guarantees.

</details>


### [13] [Compressed Set Representations based on Set Difference](https://arxiv.org/abs/2601.23240)
*Travis Gagie,Meng He,Gonzalo Navarro*

Main category: cs.DS

TL;DR: 提出一种基于集合间差异的压缩表示方法，支持对数时间内的访问、成员、前驱和后继查询，并提供优于标准方法的MST构建算法


<details>
  <summary>Details</summary>
Motivation: 传统集合集合表示方法在处理大量相似集合时效率不高，需要更高效的压缩表示来利用集合间的相似性差异

Method: 开发一种压缩表示方法，利用集合间的差异程度，支持对数时间复杂度的多种查询操作，并提出基于最小生成树(MST)的构建算法

Result: 新表示方法在空间效率和查询性能上优于标准方法，MST构建算法在构建效率上表现更优

Conclusion: 通过利用集合间的差异进行压缩表示，结合高效的MST构建算法，实现了对集合集合的高效存储和查询

Abstract: We introduce a compressed representation of sets of sets that exploits how much they differ from each other. Our representation supports access, membership, predecessor and successor queries on the sets within logarithmic time. In addition, we give a new MST-based construction algorithm for the representation that outperforms standard ones.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: Linux内核成为自己的CVE编号机构后，研究发现漏洞修复延迟与严重程度关联不大，而与内核版本新旧相关，新内核修复更快，旧内核漏洞常未解决。


<details>
  <summary>Details</summary>
Motivation: 分析Linux内核作为自身CVE编号机构后，漏洞的解剖结构和动态变化，了解驱动内核漏洞修复的因素。

Method: 使用元数据、相关提交记录和补丁延迟进行分析，通过生存模型研究内核新旧程度对修复时间的影响。

Result: 严重程度和CVSS评分与补丁延迟关联可忽略，内核新旧程度是合理的预测因子；新内核修复更快，旧内核保留未解决CVE；引入漏洞的提交通常比修复更广泛复杂。

Conclusion: Linux内核的CVE流程具有独特性，开发者优先修复新内核，而旧内核漏洞常被遗留，这反映了开源项目的特殊维护模式。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [15] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该研究对大型语言模型在多跳根因分析中的推理能力进行了系统性评估，通过控制实验框架测试了6个LLM在两种智能体工作流下的表现，发现了16种常见的RCA推理失败模式。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的高度分布式和相互依赖特性使得根因分析变得复杂，特别是多跳故障传播场景。虽然LLM为自动化RCA提供了新机会，但现有方法依赖历史事件语料库、处理超出LLM容量的高容量遥测数据，或将推理嵌入复杂的多智能体管道，这些设计选择使得难以判断失败是源于推理本身还是外围设计。

Method: 设计了控制实验框架，通过简化实验设置来突出LLM的推理行为。评估了6个LLM在两种智能体工作流（ReAct和Plan-and-Execute）和一个非智能体基线下的表现，使用两个真实世界案例研究（GAIA和OpenRCA）。总共执行了48,000个模拟故障场景，总计228天执行时间。测量了根因准确性和中间推理轨迹质量，并创建了16种常见RCA推理失败的标记分类法，使用LLM-as-a-Judge进行标注。

Result: 结果阐明了当前开源LLM在多跳RCA中的成功和失败之处，量化了对输入数据模态的敏感性，并识别了能够预测最终正确性的推理失败模式。提供了透明且可复现的经验结果和失败分类法。

Conclusion: 该研究为LLM在系统诊断中的推理能力提供了系统评估框架，识别了当前模型的局限性，并为未来基于推理的系统诊断研究提供了指导。通过控制实验设计，能够更准确地评估LLM的推理能力本身，而不是外围设计选择的影响。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [16] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: FlaXifyer：基于预训练语言模型的少样本学习方法，用于预测CI流水线中间歇性作业失败类别，仅需作业执行日志和少量标注数据，准确率高达84.3% Macro F1和92.0% Top-2准确率。


<details>
  <summary>Details</summary>
Motivation: CI流水线中的间歇性作业失败（由非确定性测试、网络中断、基础设施故障等引起）导致大量效率损失：重复运行浪费计算资源，诊断时间分散开发者精力。现有机器学习方法仅检测间歇性失败，未解决后续诊断挑战。

Method: 提出FlaXifyer（少样本学习方法，使用预训练语言模型预测间歇性作业失败类别）和LogSift（可解释性技术，快速识别关键日志语句）。FlaXifyer仅需作业执行日志和每类别12个标注样本。

Result: 在TELUS的2,458个作业失败上评估，FlaXifyer达到84.3% Macro F1和92.0% Top-2准确率；LogSift在1秒内识别关键日志语句，减少74.4%的审查工作量，在87%情况下发现相关失败信息。

Conclusion: FlaXifyer和LogSift实现了有效的自动化分类，加速了失败诊断，为自动化解决间歇性作业失败问题铺平了道路。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [17] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一个基于Frida的工具包，可在已root的Android设备上运行时伪造传感器和系统信号，用于测试上下文敏感的应用程序行为。


<details>
  <summary>Details</summary>
Motivation: 移动应用越来越依赖实时传感器和系统数据来适应用户上下文，但模拟器和仪器化构建通常无法支持在物理设备上对上下文敏感的应用行为进行可重复测试。

Method: 基于Frida的运行时欺骗工具包，可在已root的Android设备上脚本化和注入时变传感器流（加速度计、陀螺仪、步数计数器）和系统值（电池电量、系统时间、设备元数据）到未修改的应用中。

Result: 在已root的Android设备上对五个代表性传感器可视化应用进行了实时欺骗验证，支持可脚本化和可逆的数值操作，便于测试应用逻辑、发现基于上下文的行为和隐私分析。

Conclusion: PriviSense无需模拟器或应用重写即可实现可重复的设备上实验，为测试应用逻辑和隐私分析提供了有效工具，代码仅向经过验证的研究人员共享以确保道德使用。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [18] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: EFParser是一个基于小规模LLM的无监督日志解析器，通过双缓存系统和校正模块提升小模型性能，在保持高效的同时超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语义日志解析器虽然泛化能力强，但严重依赖模型规模，导致使用小模型时性能急剧下降。这阻碍了在实际部署中的应用，因为实际场景中数据隐私和计算资源限制通常需要使用更小的模型。

Method: 提出EFParser，采用双缓存系统（具有自适应更新机制）区分新模板和现有模板变体，能够合并冗余模板并修正错误；同时设计了专门的校正模块，在缓存前验证和优化LLM生成的模板，防止错误注入。

Result: 在公开大规模数据集上的实验表明，EFParser在小规模LLM上运行时，在所有指标上平均超越现有最先进方法12.5%，甚至超过了一些使用大规模模型的基线方法，同时保持了较高的计算效率。

Conclusion: EFParser通过系统架构创新增强了小规模LLM的能力，为实际日志分析部署提供了强大而实用的解决方案，解决了小模型性能下降的问题。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [19] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: TimeMachine-bench是一个评估Python软件迁移任务的基准测试，包含因依赖更新导致测试失败的GitHub仓库，并评估了11个LLM模型在迁移任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程的发展，研究重点逐渐转向反映软件工程师日常工作的实际任务。软件迁移作为适应环境变化的关键过程，在研究中被忽视，需要建立评估基准。

Method: 构建TimeMachine-bench基准测试，包含因依赖更新导致测试失败的GitHub仓库，采用全自动化构建流程支持实时更新，并创建人工验证的子集确保问题可解性。在验证子集上评估基于11个模型（包括开源权重和SOTA LLM）的代理基线。

Result: LLM在迁移任务中显示出一定潜力，但仍面临重大可靠性挑战：包括利用低测试覆盖率产生的虚假解决方案，以及次优工具使用策略导致的不必要编辑。

Conclusion: TimeMachine-bench为软件迁移任务提供了实用的评估基准，揭示了LLM在该领域的当前局限性和未来改进方向。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [20] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 提出HealthMag工具，用于数字健康软件的需求建模与评估，特别关注健康条件和年龄因素，确保软件包容性。


<details>
  <summary>Details</summary>
Motivation: 数字健康软件常基于对用户的隐含错误假设开发，未能充分考虑用户的健康条件和年龄特点，导致软件在实际使用中缺乏包容性。

Method: 基于InclusiveMag框架，通过系统映射和校准开发HealthMag工具；整合AgeMag方法创建Elderly HealthMag双重视角方法；通过认知走查演示应用。

Result: 开发出HealthMag工具及其针对老年用户的Elderly HealthMag版本，能够识别现有数字健康应用中的包容性偏见。

Conclusion: HealthMag工具能有效帮助数字健康软件开发团队更好地获取、建模和评估需求，提高软件对特定健康条件和年龄用户的包容性。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [21] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 生成式AI在软件工程中的应用导致组织从水平分层转向垂直整合，带来8-33倍的资源节约，产生"超级员工"并消除跨职能协调成本


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI对软件工程组织的影响，对比传统企业和AI原生初创公司的不同环境，探索AI如何改变工程组织结构

Method: 采用多案例比较研究，对比传统企业（棕地）和AI原生初创公司（绿地）两种开发环境，分析组织转型效果

Result: 从水平分层转向垂直整合带来8-33倍的资源消耗减少；出现"超级员工"（AI增强工程师）；消除跨职能协调成本；提出人机协作效能作为主要优化目标；发现AI扭曲效应降低劳动规模回报同时放大技术杠杆

Conclusion: 管理者应重新设计组织架构，包括重新激活资深工程师的闲置认知带宽，抑制盲目规模扩张，以人机协作效能为核心优化目标

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [22] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: VarParser提出了一种以变量为中心的日志解析策略，通过利用日志中的变量部分来提升解析准确性和效率，相比现有仅关注常量部分的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的日志解析器都只关注日志的常量部分，忽略了变量部分对日志解析的潜在贡献。这种常量中心的策略带来了四个关键问题：1）仅使用常量信息的低效日志分组和采样；2）基于常量的缓存导致大量LLM调用，降低解析准确性和效率；3）提示中消耗大量常量token导致高调用成本；4）结果中只保留占位符，丢失了日志中变量信息带来的系统可见性。

Method: 提出VarParser变量中心日志解析策略，包含三个关键技术：1）变量贡献采样：高效捕获日志的变量部分并利用其对解析的贡献；2）变量中心解析缓存：减少LLM调用次数；3）自适应变量感知上下文学习。通过引入变量单元，保留丰富的变量信息，增强日志解析结果的完整性。

Result: 在大规模数据集上的广泛评估表明，VarParser相比现有方法实现了更高的准确性，显著提高了解析效率，同时降低了LLM调用成本。

Conclusion: VarParser通过变量中心的策略有效解决了现有LLM日志解析器的局限性，充分利用日志中变量信息，在准确性、效率和成本方面都有显著改进，为下游异常检测和故障诊断任务提供了更完整的日志解析结果。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [23] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 本文首次系统评估了模型合并技术在LLMs、图像分类和自动驾驶三个领域的应用效果，发现现有方法跨领域效果不一致，提出了AutoMerge框架来自动搜索最优合并策略。


<details>
  <summary>Details</summary>
Motivation: 软件重用是软件工程中的重要话题，模型合并作为训练免费的方法在LLMs中取得了成功，但缺乏对其他深度学习模型架构跨领域的系统性研究。

Method: 系统评估了五种模型合并技术在三种不同模型架构（LLMs、图像分类、自动驾驶）上的表现，发现现有方法局限性后，提出了AutoMerge框架：先将复杂模型分割为异构块，然后系统探索合并空间寻找最优合并技术和超参数配置。

Result: 现有模型合并技术直接应用于不同领域时效果高度不一致，远不如在LLMs中的成功；单个合并技术难以处理模型内部的异构结构特性；合并效果对超参数配置高度敏感。

Conclusion: 模型合并技术不能直接跨领域应用，需要针对不同模型架构设计自适应方法。AutoMerge框架通过自动搜索合并策略，为解决跨领域模型合并问题提供了有效方案。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [24] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 该研究提出针对AI系统的可重用安全案例模板框架，解决传统安全案例方法无法适应生成式AI和智能体AI动态特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统安全案例方法（来自航空、核工程等领域）依赖于明确的系统边界、稳定架构和已知故障模式，但现代AI系统（如生成式AI和智能体AI）具有能力不可预测、行为随提示变化、风险随微调/部署环境动态变化等特点，传统方法无法有效应对这些动态特性。

Method: 提出一个包含可重用安全案例模板的框架，每个模板遵循预定义的结构（声明、论证、证据），并针对AI系统特点设计了全面的分类体系：AI特定声明类型（基于断言、基于约束、基于能力）、论证类型（演示性、比较性、因果/解释性、基于风险、规范性）和证据族（经验性、机制性、比较性、专家驱动、形式化方法、操作/现场数据、基于模型）。

Result: 开发了一个系统化、可组合、可重用的方法来构建和维护安全案例，能够应对无真实标签评估、动态模型更新、基于阈值的风险决策等独特挑战，使安全案例更具可信度、可审计性，并能适应生成式和前沿AI系统的演化行为。

Conclusion: 该框架为AI系统安全案例提供了系统化的解决方案，通过可重用模板和全面的分类体系，使安全案例能够适应AI系统的动态特性，为AI系统的安全治理提供了更有效的工具。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [25] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 研究评估了GPT-4.1生成的边界值分析解释的质量和实用性，发现63.5%的评分是积极的，并提出了改进LLM边界解释工具的七项设计标准。


<details>
  <summary>Details</summary>
Motivation: 边界值分析测试(BVT)在软件质量保证中很重要，但测试人员常常难以理解和证明某些输入-输出对为何代表有意义的行为边界。虽然大语言模型(LLM)可以生成自然语言解释，但其在BVT中的价值尚未经过实证评估。

Method: 进行了探索性研究：通过调查让27名软件专业人员对GPT-4.1生成的20个边界对的解释进行评分（清晰度、正确性、完整性和感知有用性），并对其中6人进行后续访谈。

Result: 63.5%的评分是积极的（5点李克特量表中的4-5分），17%是消极的（1-2分）。参与者偏好结构清晰、引用权威来源、根据读者专业水平调整深度的解释，并强调需要支持调试和文档的实际示例。

Conclusion: 从这些见解中提炼出了七项需求清单，为未来基于LLM的边界解释工具定义了具体设计标准。结果表明，经过进一步改进，基于LLM的工具可以通过使边界解释更具可操作性和可信度来支持测试工作流程。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [26] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: Meta开发了即时捕获测试生成系统，用于在大型后端系统中预防bug。与传统加固测试不同，捕获测试旨在失败，在代码落地前发现bug。通过代码变更感知方法提升捕获效果，使用规则和LLM评估器减少误报，成功防止了严重故障进入生产环境。


<details>
  <summary>Details</summary>
Motivation: 在拥有数亿行代码的大规模后端系统中预防bug，传统加固测试在生成时通过，无法有效发现潜在问题。需要一种能在代码落地前主动发现bug的测试方法，同时要解决误报带来的开发负担问题。

Method: 采用即时捕获测试生成方法，设计旨在失败的测试来发现bug。使用代码变更感知方法提升候选捕获生成效果，结合规则基和LLM基评估器来减少误报。通过统计分析人类接受/拒绝的代码变更来验证评估效果。

Result: 代码变更感知方法将候选捕获生成提升4倍（相比加固测试）和20倍（相比偶然失败测试）。评估器将人工审核负载减少70%。41个候选捕获中，8个确认为真阳性，其中4个可能引发严重故障。统计分析显示人类接受的变更有更多误报，拒绝的变更有更多真阳性。

Conclusion: 即时捕获测试生成具有可扩展性和工业适用性，能有效防止严重故障进入生产环境。该方法通过主动发现bug和减少误报，在大规模代码库中实现了有效的bug预防。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [27] [MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering](https://arxiv.org/abs/2601.22859)
*Chuanzhe Guo,Jingjing Wu,Sijun He,Yang Chen,Zhaoqi Kuang,Shilong Fan,Bingjin Chen,Siqi Bao,Jing Liu,Hua Wu,Qingfu Zhu,Wanxiang Che,Haifeng Wang*

Main category: cs.SE

TL;DR: MEnvAgent是一个多语言环境构建框架，用于自动化生成可验证的软件工程任务实例，解决了LLM代理发展中可验证数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程领域的发展受到可验证数据集稀缺的限制，主要原因是跨多种语言构建可执行环境的复杂性。

Method: 采用多智能体规划-执行-验证架构，自主解决构建失败问题，并集成环境重用机制，通过增量修补历史环境来减少计算开销。

Result: 在包含10种语言、1000个任务的MEnvBench基准测试中，MEnvAgent优于基线方法，将失败转通过率提高8.6%，同时减少43%的时间成本。构建了目前最大的开源多语言可验证Docker环境数据集MEnvData-SWE。

Conclusion: MEnvAgent为LLM代理在软件工程领域的发展提供了可扩展的环境构建解决方案，其生成的数据集和解决方案轨迹能够显著提升各种模型在软件工程任务上的性能。

Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.

</details>


### [28] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 提出了AnoMod数据集，这是一个用于微服务系统异常检测和根因分析的多模态数据集，包含四种异常类型和五种监控模态


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、公开可用的微服务系统异常检测和根因分析数据集，现有基准主要关注性能相关故障且模态单一，限制了更广泛故障模式和跨模态方法的研究

Method: 基于两个开源微服务系统（SocialNetwork和TrainTicket），设计并注入了四类异常（性能级、服务级、数据库级、代码级），收集了五种模态数据（日志、指标、分布式追踪、API响应、代码覆盖率报告）

Result: 创建了AnoMod数据集，提供了更丰富、端到端的系统状态视图，支持跨模态异常检测和融合/消融策略评估，以及细粒度根因分析研究

Conclusion: AnoMod数据集填补了微服务系统异常检测和根因分析研究的数据空白，支持端到端故障排除流程，促进跨模态方法和细粒度根因定位研究

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [29] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: Lambda框架是一个边缘原生平台，通过用户定义函数实现车载数据过滤和处理，采用FaaS原则适应资源受限的汽车环境，在嵌入式自动驾驶系统中支持实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器学习面临数据瓶颈，需要大量传感器数据且需覆盖安全关键场景，但捕获这些事件需要大量驾驶时间和高效选择。

Method: 引入Lambda框架，提供服务器无感知抽象层，分离应用逻辑与底层执行关注点，采用FaaS原则适应资源受限汽车环境，支持模块化事件驱动过滤算法，兼容ROS 2和现有数据记录管道。

Result: 在NVIDIA Jetson Orin Nano上评估，相比原生ROS 2部署，表现竞争性性能、降低延迟和抖动，证实lambda抽象能支持嵌入式自动驾驶系统的实时数据处理。

Conclusion: Lambda框架通过边缘原生平台和FaaS原则，有效解决了自动驾驶数据收集的瓶颈问题，在资源受限环境中实现了高效的数据过滤和处理。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [30] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: LLM智能体可显著减少SAST工具误报，但效果受模型能力、CWE类型和框架设计影响，存在真漏洞被抑制和计算成本差异大的权衡问题。


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报给开发者带来沉重的人工审查负担，LLM智能体通过迭代推理和工具使用有望优化误报过滤，但不同智能体架构的比较效果尚不清楚。

Method: 比较三种最先进的LLM智能体框架（Aider、OpenHands、SWE-agent）在漏洞误报过滤中的表现，使用OWASP Benchmark和真实开源Java项目漏洞进行评估。

Result: LLM智能体能大幅减少SAST噪声，在OWASP Benchmark上将初始92%误报率降至最低6.3%；真实项目中最佳配置可达93.3%误报识别率。但效果强烈依赖骨干模型和CWE类型，强模型（Claude Sonnet 4、GPT-5）上智能体显著优于普通提示，弱模型上增益有限或不一致。激进误报减少可能抑制真漏洞，且不同框架计算成本差异显著。

Conclusion: LLM智能体是SAST误报过滤的强大但非均匀解决方案，实际部署需仔细考虑智能体设计、骨干模型选择、漏洞类别和运营成本。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [31] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: SWE-Manager：一个通过强化学习训练的8B模型，用于在软件工程中比较多个修复提案、选择最佳方案并合成黄金提案，在SWE-Lancer Manager基准测试中表现优于GPT-5等基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究主要关注代码生成和错误修复，但实际开发中团队需要从多个候选提案中选择最佳方案进行实施。好的选择能提高问题解决的可靠性并降低风险，而差的选择会增加风险甚至导致不可预测的故障。

Method: 首先通过人工研究分析维护者选择提案的理性依据，然后提出SWE-Manager——一个通过强化学习训练的8B模型，能够比较提案、论证选择理由并合成黄金提案。将提案选择视为推理任务，模拟技术经理在不执行代码或运行测试的情况下权衡问题背景和解决方案的过程。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager达到53.21%的选择准确率和57.75%的收益率，赚取152,750美元，表现优于包括GPT-5在内的强基线模型。通过P2A框架进一步验证了其在真实问题解决中的有效性。

Conclusion: SWE-Manager展示了LLM在软件工程提案选择和合成任务中的潜力，能够有效模拟技术经理的决策过程，为实际开发中的问题解决提供可靠支持。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [32] [SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation](https://arxiv.org/abs/2601.23009)
*Wei Chen,Zhiyuan Peng,Xin Yin,Chao Ni,Chenhao Ying,Bang Xie,Yuan Luo*

Main category: cs.SE

TL;DR: SolAgent是一个工具增强的多智能体框架，通过双循环精炼机制生成功能正确且安全的智能合约，显著优于现有LLMs和AI工具。


<details>
  <summary>Details</summary>
Motivation: 智能合约是去中心化网络的核心，但确保其功能正确性和安全性仍面临重大挑战。虽然大型语言模型在代码生成方面有潜力，但生成的智能合约代码经常存在错误或漏洞，无法满足严格的安全要求。

Method: 提出SolAgent框架，模仿人类专家工作流程，采用双循环精炼机制：内循环使用Forge编译器确保功能正确性，外循环利用Slither静态分析器消除安全漏洞。智能体还具备文件系统能力以解决复杂项目依赖。

Result: 在SolEval+基准测试中，SolAgent达到64.39%的Pass@1率，显著优于最先进的LLMs（约25%）、AI IDE（如GitHub Copilot）和现有智能体框架。相比人工编写的基线，安全漏洞减少达39.77%。

Conclusion: SolAgent能生成高质量、安全的智能合约代码，其生成的高质量轨迹可用于蒸馏更小的开源模型，促进安全智能合约生成的民主化访问。

Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.

</details>


### [33] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Unshade是一个混合依赖扫描工具，结合元数据扫描的高效性和代码中心方法检测修改依赖的能力，在Java项目中识别隐藏的易受攻击依赖。


<details>
  <summary>Details</summary>
Motivation: 开源软件依赖是现代软件代码库的主要组成部分，虽然能减少开发时间和成本，但也引入了安全风险。现有元数据扫描器轻量快速但无法检测修改后的依赖，而代码中心扫描器能检测修改依赖但效率较低。

Method: Unshade采用混合方法：首先通过字节码指纹识别机制识别修改和隐藏的依赖，扩充项目的软件物料清单(SBOM)，然后将增强的SBOM传递给元数据漏洞扫描器，识别声明依赖和新发现依赖中的已知漏洞。

Result: 对GitHub上1,808个最流行的Java Maven项目进行大规模研究，结果显示近50%的项目包含至少一个与已知漏洞相关的修改隐藏依赖。平均每个受影响项目包含超过8个这样的隐藏易受攻击依赖，传统元数据扫描器会全部漏掉。Unshade共识别出7,712个隐藏依赖中的唯一CVE。

Conclusion: Unshade证明了混合依赖扫描方法的有效性，能够检测传统元数据扫描器无法发现的隐藏易受攻击依赖，显著提高了开源依赖的安全性评估能力。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [34] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究發現程式碼註解能提升大型語言模型的自動修復錯誤能力達三倍，特別是包含方法實現細節的註解最有效


<details>
  <summary>Details</summary>
Motivation: 自動錯誤修復(ABF)通常會在訓練前移除程式碼註解，但我們假設註解可能提供有價值的設計和實現見解，對修復某些類型的錯誤至關重要

Method: 進行實證評估，比較兩個模型家族在訓練和推論階段所有組合條件下（有無註解）的表現，並使用LLM自動為缺乏註解的數據集生成註解

Result: 註解在訓練和推論階段都存在時，能將ABF準確率提升達三倍；訓練時包含註解不會降低無註解實例的表現；包含方法實現細節的註解特別有效

Conclusion: 註解對LLM的錯誤修復能力有顯著正面影響，特別是包含實現細節的註解，這挑戰了傳統移除註解的預處理做法

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [35] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 该论文提出了XRintTest，一种基于交互流图自动测试VR交互的方法，解决了VR测试中3D用户输入自动生成和交互覆盖度评估的挑战。


<details>
  <summary>Details</summary>
Motivation: VR技术带来沉浸式体验，但与传统软件相比存在独特的测试挑战。现有VR测试方法缺乏自动合成真实3D用户输入（如手柄抓取和触发动作）的能力，且现有指标无法稳健捕捉多样化的交互覆盖度。

Method: 1. 实证分析9个开源VR项目，识别四种主要交互类型：fire、manipulate、socket、custom；2. 提出交互流图，系统建模3D用户交互的目标、动作和条件；3. 构建XRBench3D基准，包含10个VR场景和456个不同用户交互；4. 开发XRintTest，利用交互流图进行动态场景探索和交互执行。

Result: 在XRBench3D上的评估显示，XRintTest对fire、manipulate和socket交互的覆盖率达到93%，比随机探索方法效果提升12倍，效率提升6倍。能够检测运行时异常和非异常交互问题，包括微妙的配置缺陷。交互流图还能揭示可能影响功能和测试性能的交互设计问题。

Conclusion: 该研究通过交互流图和XRintTest方法，有效解决了VR交互测试的关键挑战，实现了高覆盖度的自动化测试，并能检测多种交互问题，为VR应用的质量保证提供了有力工具。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [36] [From Monolith to Microservices: A Comparative Evaluation of Decomposition Frameworks](https://arxiv.org/abs/2601.23141)
*Mineth Weerasinghe,Himindu Kularathne,Methmini Madhushika,Danuka Lakshan,Nisansa de Silva,Adeesha Wijayasiri,Srinath Perera*

Main category: cs.SE

TL;DR: 该论文对微服务分解方法进行了统一比较评估，发现基于层次聚类的方法（特别是HDBScan）在不同基准系统中产生最一致平衡的分解结果。


<details>
  <summary>Details</summary>
Motivation: 软件现代化过程中从单体架构迁移到微服务架构日益重要，但确定有效的服务边界仍然是一个复杂且未解决的挑战。现有的自动化微服务分解框架评估存在碎片化问题，包括基准系统不一致、指标不兼容和可复现性有限，阻碍了客观比较。

Method: 采用统一的指标计算管道，对最先进的微服务分解方法（包括静态、动态和混合技术）进行比较评估。使用一致的基准系统（JPetStore、AcmeAir、DayTrader、Plants），通过结构模块化(SM)、接口数量(IFN)、分区间通信(ICP)、非极端分布(NED)等指标评估分解质量。结合先前研究报告的结果和从可用复制包中实验复现的输出进行分析。

Result: 基于层次聚类的方法，特别是HDBScan，在不同基准系统中产生最一致平衡的分解结果，在实现强模块化的同时最小化通信和接口开销。

Conclusion: 层次聚类方法（尤其是HDBScan）在微服务分解中表现最佳，为软件现代化中的架构迁移提供了有效的自动化解决方案，解决了现有评估碎片化问题。

Abstract: Software modernisation through the migration from monolithic architectures to microservices has become increasingly critical, yet identifying effective service boundaries remains a complex and unresolved challenge. Although numerous automated microservice decomposition frameworks have been proposed, their evaluation is often fragmented due to inconsistent benchmark systems, incompatible metrics, and limited reproducibility, thus hindering objective comparison. This work presents a unified comparative evaluation of state-of-the-art microservice decomposition approaches spanning static, dynamic, and hybrid techniques. Using a consistent metric computation pipeline, we assess the decomposition quality across widely used benchmark systems (JPetStore, AcmeAir, DayTrader, and Plants) using Structural Modularity (SM), Interface Number(IFN), Inter-partition Communication (ICP), Non-Extreme Distribution (NED), and related indicators. Our analysis combines results reported in prior studies with experimentally reproduced outputs from available replication packages. Findings indicate that the hierarchical clustering-based methods, particularly HDBScan, produce the most consistently balanced decompositions across benchmarks, achieving strong modularity while minimizing communication and interface overhead.

</details>


### [37] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: OSS4SG项目（开源软件促进社会公益）相比传统OSS项目，能2.2倍更好地留住贡献者，贡献者成为核心成员的概率高19.6%，且提供更多元的发展路径。研究发现，先花时间了解项目再集中贡献（Late Spike模式）比一开始就高强度贡献（Early Spike模式）能更快成为核心成员。


<details>
  <summary>Details</summary>
Motivation: 传统开源软件的新手到核心贡献者的转化率很低，大多数新手在初步贡献后就不再活跃。本研究旨在探究以社会公益为使命的OSS4SG项目是否与传统OSS项目在新手转化方面存在差异，为开源可持续发展提供洞见。

Method: 比较了375个项目（190个OSS4SG，185个传统OSS），分析了92,721名贡献者和350万次提交。研究考察了贡献者留存率、成为核心成员的概率、贡献模式（Early Spike vs Late Spike）以及发展路径多样性。

Result: OSS4SG项目：1）贡献者留存率是传统项目的2.2倍；2）成为核心成员的概率高19.6%；3）提供多种发展路径（传统项目61.62%的转化集中在一个主导路径）；4）支持两种有效的时间模式（Early和Late Spike）。Late Spike模式（先学习再集中贡献）比Early Spike模式（一开始就高强度贡献）成为核心成员快2.4-2.9倍（21周 vs 51-60周）。

Conclusion: 项目使命与新手到核心成员的转化环境显著相关。成功成为核心成员的关键策略包括：选择与个人价值观一致的项目，以及在做出重大贡献前花时间深入了解代码库。这些发现为新手和维护者提供了基于证据的指导。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [38] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: GrepRAG：一种基于轻量级词法检索的仓库级代码补全方法，通过ripgrep命令检索相关上下文，性能超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义索引或图分析的RAG方法计算开销大，而开发者常用轻量级搜索工具（如ripgrep）。本文探索简单、无索引的词法检索在仓库级代码补全中的潜力。

Method: 1. 提出Naive GrepRAG基线框架：LLM自主生成ripgrep命令检索相关上下文；2. 改进为GrepRAG：增加轻量级后处理流程，包括标识符加权重排序和结构感知去重。

Result: Naive GrepRAG性能已与复杂图基线相当；GrepRAG在CrossCodeEval和RepoEval-Updated上持续超越SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04-15.58%。

Conclusion: 轻量级、无索引的词法检索在仓库级代码补全中具有强大潜力，GrepRAG通过简单改进有效解决了词法检索的局限性，性能优于复杂检索方法。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [39] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 提出O-CRD框架，利用已解决的仓库问题作为监督，通过反向重构修复轨迹来指导新问题的定位和修复，无需微调或在线搜索，显著提升修复成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM修复管道采用重置-解决模式，浪费了仓库中已有的相似问题修复经验。现有方法通过前向试错过程成本高且可能偏离正确修复。

Method: O-CRD框架：从已验证的历史修复开始，反向重构阶段式修复轨迹，然后在推理时复用蒸馏的指导来引导文件/函数定位和补丁合成。

Result: 在SWE-Bench Lite上，Pass@1提升：GPT-4o提高10.4%，DeepSeek-V3提高8.6%，GPT-5提高10.3%，表明结果条件化的修复复用可替代昂贵的前向探索。

Conclusion: 利用已验证修复的结果条件化复用可以有效指导软件问题解决，替代成本高昂的前向探索方法，显著提高修复成功率。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [40] [Toward Non-Expert Customized Congestion Control](https://arxiv.org/abs/2601.22461)
*Mingrui Zhang,Hamid Bagheri,Lisong Xu*

Main category: cs.NI

TL;DR: NECC是一个基于大语言模型和BPF接口的探索性非专家定制拥塞控制算法框架，让非专家用户能够轻松建模、实现和部署定制化拥塞控制算法。


<details>
  <summary>Details</summary>
Motivation: 通用拥塞控制算法无法满足特定用户的特殊需求，而定制化算法需要专业知识，非专家用户难以实现。需要一种让非专家用户也能轻松创建定制化拥塞控制算法的解决方案。

Method: 提出NECC框架，利用大语言模型（LLM）和伯克利数据包过滤器（BPF）接口，为非专家用户提供定制化拥塞控制算法的建模、实现和部署能力。

Result: 使用真实世界拥塞控制算法进行评估，NECC表现出非常有前景的性能，并发现了相关见解和未来研究方向。

Conclusion: NECC是首个解决定制化拥塞控制算法实现问题的框架，通过LLM和BPF接口使非专家用户能够轻松创建定制化算法，具有重要研究价值和实际应用潜力。

Abstract: General-purpose congestion control algorithms (CCAs) are designed to achieve general congestion control goals, but they may not meet the specific requirements of certain users. Customized CCAs can meet certain users' specific requirements; however, non-expert users often lack the expertise to implement them. In this paper, we present an exploratory non-expert customized CCA framework, named NECC, which enables non-expert users to easily model, implement, and deploy their customized CCAs by leveraging Large Language Models and the Berkeley Packet Filter (BPF) interface. To the best of our knowledge, we are the first to address the customized CCA implementation problem. Our evaluations using real-world CCAs show that the performance of NECC is very promising, and we discuss the insights that we find and possible future research directions.

</details>


### [41] [Nethira: A Heterogeneity-aware Hierarchical Pre-trained Model for Network Traffic Classification](https://arxiv.org/abs/2601.22494)
*Chungang Lin,Weiyao Zhang,Haitong Luo,Xuying Meng,Yujun Zhang*

Main category: cs.NI

TL;DR: Nethira：基于层次重构和增强的异构感知预训练模型，用于网络流量分类，显著提升性能并减少对标注数据的依赖


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在处理网络流量时存在异构性（层次化流量结构）与输入同质性（扁平化字节序列）之间的差距问题，这限制了模型对流量结构的理解和分类性能

Method: 提出Nethira模型，采用层次重构预训练（字节、协议、数据包三个层次）捕获全面流量结构信息，并在微调阶段使用一致性正则化策略配合层次流量增强来减少标签依赖

Result: 在四个公共数据集上，Nethira优于七个现有预训练模型，平均F1分数提升9.11%，在高异构网络任务中仅需1%标注数据即可达到可比性能

Conclusion: Nethira通过层次化方法有效解决了流量异构性与输入同质性之间的差距，显著提升了网络流量分类性能并降低了对标注数据的依赖

Abstract: Network traffic classification is vital for network security and management. The pre-training technology has shown promise by learning general traffic representations from raw byte sequences, thereby reducing reliance on labeled data. However, existing pre-trained models struggle with the gap between traffic heterogeneity (i.e., hierarchical traffic structures) and input homogeneity (i.e., flattened byte sequences). To address this gap, we propose Nethira, a heterogeneity-aware pre-trained model based on hierarchical reconstruction and augmentation. In pre-training, Nethira introduces hierarchical reconstruction at multiple levels-byte, protocol, and packet-capturing comprehensive traffic structural information. During fine-tuning, Nethira proposes a consistency-regularized strategy with hierarchical traffic augmentation to reduce label dependence. Experiments on four public datasets demonstrate that Nethira outperforms seven existing pre-trained models, achieving an average F1-score improvement of 9.11%, and reaching comparable performance with only 1% labeled data on high-heterogeneity network tasks.

</details>


### [42] [Chance-Constrained Secrecy Optimization in Hybrid RIS-Empowered and UAV-Assisted Networks](https://arxiv.org/abs/2601.22499)
*Elhadj Moustapha Diallo,Mamadou Aliou Diallo,Abusaeed B. M. Adam,Muhammad Naeem Shah*

Main category: cs.NI

TL;DR: 该论文提出了一种混合可重构智能表面系统，结合无人机RIS、室外STAR-RIS和室内全息RIS，在用户移动、动态遮挡、窃听者和硬件损伤等复杂环境下，通过交替优化算法最小化保密中断概率。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统面临室内外用户安全通信的挑战，特别是在用户移动、动态遮挡、多窃听者协作和硬件损伤等复杂场景下。需要一种能够同时增强室内外用户安全通信的混合可重构环境解决方案。

Method: 提出混合可重构环境系统，包含无人机RIS、室外STAR-RIS和室内全息RIS。开发符合3GPP和ITU标准的随机信道模型，采用Bernstein型确定性近似处理机会约束，提出基于交替优化和逐次凸近似的算法框架，分别优化基站波束成形、RIS配置和无人机位置。

Result: 仿真结果表明，相比基准方案，所提混合RIS系统能显著降低保密中断概率，对信道不确定性、遮挡、窃听者协作和硬件损伤具有强鲁棒性。算法能单调降低保密中断代价并收敛到鲁棒问题的稳定点。

Conclusion: 无人机RIS、STAR-RIS和全息RIS的集成能有效增强室内外用户的保密通信性能，在复杂动态环境下提供鲁棒的安全保障，为未来6G安全通信系统提供了有前景的解决方案。

Abstract: This paper considers a hybrid reconfigurable environment comprising a UAV-mounted reflecting RIS, an outdoor STAR-RIS enabling simultaneous transmission and reflection, and an indoor holographic RIS (H-RIS), jointly enhancing secure downlink communication for indoor and outdoor users. The system operates under user mobility, dynamic blockages, colluding idle and active eavesdroppers, and transceiver and surface hardware impairments. A 3GPP and ITU-compliant stochastic channel model is developed, capturing mobility-induced covariance evolution, outdoor-indoor penetration losses, and distortion-aware noise due to practical EVM-based impairments. We aim to minimize the aggregate secrecy-outage probability subject to secrecy-rate constraints, QoS requirements, power limitations, and statistical CSI uncertainty. The resulting problem contains coupled secrecy and QoS chance constraints and nonlinear interactions among the BS beamforming vectors, multi-surface phase coefficients, and UAV position. To handle these difficulties, we derive rigorous Bernstein-type deterministic approximations for all chance constraints, yielding a distributionally robust reformulation. Building on this, we propose an alternating optimization framework that employs successive convex approximation (SCA) to convexify each block and solve the BS beamforming, RIS, STAR-RIS, H-RIS configuration, and UAV placement subproblems efficiently. The proposed algorithm is shown to monotonically decrease a smooth surrogate of the secrecy-outage cost and converge to a stationary point of the robustified problem. Simulations based on 3GPP TR 38.901, TR 36.873, and ITU-R P.2109 demonstrate that integrating UAV-RIS, STAR-RIS, and H-RIS significantly reduces secrecy-outage probability compared with benchmark schemes and provides strong robustness to channel uncertainty, blockages, colluding eavesdroppers, and hardware impairments.

</details>


### [43] [MCP-Diag: A Deterministic, Protocol-Driven Architecture for AI-Native Network Diagnostics](https://arxiv.org/abs/2601.22633)
*Devansh Lodha,Mohit Panchal,Sameer G. Kulkarni*

Main category: cs.NI

TL;DR: MCP-Diag是一个基于模型上下文协议（MCP）的混合神经符号架构，通过确定性翻译层将网络诊断工具输出转换为JSON格式，并强制实施人机交互授权循环，解决了LLM在网络运维中的随机接地问题和安全风险。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型（LLMs）集成到网络运维（AIOps）面临两大挑战：1）随机接地问题 - LLMs难以可靠解析非结构化的厂商特定CLI输出；2）安全漏洞 - 授予自主代理shell访问权限存在安全风险。

Method: 提出MCP-Diag混合神经符号架构：1）确定性翻译层 - 将标准工具（dig, ping, traceroute）的原始stdout转换为严格的JSON模式；2）强制"启发循环" - 在协议层面实施人机交互（HITL）授权机制。

Result: 初步评估显示：MCP-Diag实现100%实体提取准确率，执行延迟开销小于0.9%，上下文token使用量增加3.7倍。

Conclusion: MCP-Diag通过确定性翻译和人机交互授权循环，有效解决了LLM在网络运维中的可靠性和安全性问题，为AIOps提供了可行的解决方案。

Abstract: The integration of Large Language Models (LLMs) into network operations (AIOps) is hindered by two fundamental challenges: the stochastic grounding problem, where LLMs struggle to reliably parse unstructured, vendor-specific CLI output, and the security gap of granting autonomous agents shell access. This paper introduces MCP-Diag, a hybrid neuro-symbolic architecture built upon the Model Context Protocol (MCP). We propose a deterministic translation layer that converts raw stdout from canonical utilities (dig, ping, traceroute) into rigorous JSON schemas before AI ingestion. We further introduce a mandatory "Elicitation Loop" that enforces Human-in-the-Loop (HITL) authorization at the protocol level. Our preliminary evaluation demonstrates that MCP-Diag achieving 100% entity extraction accuracy with less than 0.9% execution latency overhead and 3.7x increase in context token usage.

</details>


### [44] [Digital Twin Synchronization: towards a data-centric architecture](https://arxiv.org/abs/2601.23051)
*Eduardo Freitas,Assis T. de Oliveira Filho,Pedro R. X. do Carmo,Djamel Sadok,Judith Kelner*

Main category: cs.NI

TL;DR: 本文综述了数字孪生同步技术，分析了现有架构的不足，提出了统一的同步架构以解决工业应用中的安全性和互操作性需求。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在工业4.0中至关重要，但确保数字孪生与其物理对应物准确同步仍面临挑战。尽管有中间件和低延迟通信技术的进步，两个世界之间的有效同步仍然困难。

Method: 本文通过综述当前采用的同步技术和架构，识别关键技术挑战，并提出一个统一的同步架构，该架构适用于各种工业应用，同时解决安全和互操作性需求。

Result: 研究识别了数字孪生同步中的关键技术挑战，并提出了一种统一的同步架构，旨在填补现有空白，推动数字孪生环境中稳健的同步。

Conclusion: 需要标准化的同步架构来确保工业系统的无缝运行和持续改进，本文提出的统一架构有助于解决数字孪生同步中的安全性和互操作性挑战。

Abstract: Digital Twin (DT) technology revolutionizes industrial processes by enabling the representation of physical entities and their dynamics to enhance productivity and operational efficiency. It has emerged as a vital enabling technology in the Industry 4.0 context. The present article examines the particular issue of synchronizing a digital twin while ensuring an accurate reflection of its physical counterpart. Despite the reported recent advances in the design of middleware and low delay communication technologies, effective synchronization between both worlds remains challenging. This paper reviews currently adopted synchronization technologies and architectures, identifies vital outstanding technical challenges, and proposes a unified synchronization architecture for use by various industrial applications while addressing security and interoperability requirements. As such, this study aims to bridges gaps and advance robust synchronization in DT environments, emphasizing the need for a standardized architecture to ensure seamless operation and continuous improvement of industrial systems.

</details>


### [45] [Lossy Compression of Cellular Network KPIs](https://arxiv.org/abs/2601.23105)
*Andrea Pimpinella,Fabio Palmese,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 该论文展示了移动蜂窝网络KPI可以通过预测、量化和熵编码的标准有损压缩方案高效压缩，实现8-10倍的压缩比，且对下游分析任务影响很小。


<details>
  <summary>Details</summary>
Motivation: 移动蜂窝网络KPI数据量巨大，从多个基站长时间收集的细粒度测量数据给存储、传输和大规模分析带来重大挑战，需要有效的压缩方案。

Method: 采用基于预测、量化和熵编码的标准有损压缩方案，针对流量KPI进行率失真分析，评估压缩对下游分析任务的影响。

Result: 仅需3-4比特/样本即可达到约30dB的信噪比，相比32位浮点表示实现8-10倍压缩；跨基站聚合可减轻量化误差，预测精度在适度报告率下不受影响。

Conclusion: KPI压缩在蜂窝系统中是可行的，且对网络级分析是透明的，能够显著减少报告开销而不影响分析质量。

Abstract: Network Key Performance Indicators (KPIs) are a fundamental component of mobile cellular network monitoring and optimization. Their massive volume, resulting from fine-grained measurements collected across many cells over long time horizons, poses significant challenges for storage, transport, and large-scale analysis. In this letter, we show that common cellular KPIs can be efficiently compressed using standard lossy compression schemes based on prediction, quantization, and entropy coding, achieving substantial reductions in reporting overhead. Focusing on traffic volume KPIs, we first characterize their intrinsic compressibility through a rate-distortion analysis, showing that signal-to-noise ratios around 30 dB can be achieved using only 3-4 bits per sample, corresponding to an 8-10x reduction with respect to 32-bit floating-point representations. We then assess the impact of KPI compression on representative downstream analytics tasks. Our results show that aggregation across cells mitigates quantization errors and that prediction accuracy is unaffected beyond a moderate reporting rate. These findings indicate that KPI compression is feasible and transparent to network-level analytics in cellular systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 复杂注意力机制在小规模多模态情感识别数据集上表现不佳，而简单的领域特定改进效果更好


<details>
  <summary>Details</summary>
Motivation: 研究复杂注意力机制是否能在小规模多模态情感识别数据集（EAV数据集）上提升性能

Method: 实现三类模型：基线Transformer（M1）、新型因子化注意力机制（M2）、改进的CNN基线（M3），并测试不同模态的特征改进

Result: 复杂注意力机制表现不佳（比基线低5-13个百分点），而简单领域特定改进显著提升性能：音频添加delta MFCCs从61.9%提升到65.56%；EEG使用频域特征达到67.62%；视觉Transformer基线达到75.30%超过原论文结果

Conclusion: 对于小规模情感识别任务，领域知识和适当实现比架构复杂性更重要，简单有效的领域特定改进优于复杂注意力机制

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [47] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出一种结合多任务学习和量子卷积的混合模型，用于地球观测数据分类，探索量子机器学习在EO领域的潜力


<details>
  <summary>Details</summary>
Motivation: 地球观测进入大数据时代，传统深度学习模型计算需求巨大成为瓶颈；量子机器学习有望解决未来计算挑战，作者希望探索量子计算在EO数据分类中的应用优势

Method: 提出混合模型：1) 采用多任务学习辅助高效数据编码；2) 使用包含量子卷积操作的位置权重模块提取有效分类特征

Result: 在多个EO基准数据集上验证了模型有效性；实验探索了模型的泛化能力，并分析了其优势来源

Conclusion: 展示了量子机器学习在地球观测数据分析中的潜力，尽管当前量子设备存在限制，但该混合模型为QML在EO领域的应用提供了有前景的方向

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [48] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 首个临床EEG到语言的基础模型CELM，能够总结长时程EEG记录并生成多尺度临床报告，在生成指标上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 从长时程EEG记录中生成总结异常模式、诊断发现和临床解释的报告目前仍然需要大量人工劳动，需要自动化解决方案。

Method: 构建大规模临床EEG数据集（9,922份报告，约11,000小时EEG记录，9,048名患者），开发CELM模型，整合预训练的EEG基础模型和语言模型，支持多尺度报告生成。

Result: 在患者病史监督下，生成指标（如ROUGE-1和METEOR）从0.2-0.3提升到0.4-0.6，相对改进达70%-95%；零样本设置下，生成分数达到0.43-0.52，显著优于基线0.17-0.26。

Conclusion: CELM是首个能够总结长时程EEG记录并生成多尺度临床报告的基础模型，通过整合EEG基础模型和语言模型实现可扩展的多模态学习，显著提升EEG报告生成质量。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [49] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR 是一种新颖的联邦学习算法，通过自适应优化器和方差缩减技术解决客户端部分参与导致的异构性问题，其量化版本 FedAdaVR-Quant 能显著减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中由于客户端异构性导致梯度噪声、客户端漂移和部分客户端参与误差，其中部分客户端参与误差最为普遍但现有研究未能充分解决。

Method: 提出 FedAdaVR 算法，结合自适应优化器和方差缩减技术，利用最近存储的客户端更新来模拟缺席客户端的参与；进一步提出 FedAdaVR-Quant，将客户端更新以量化形式存储以减少内存需求。

Result: FedAdaVR 在一般非凸条件下收敛性得到证明，能消除部分客户端参与误差；在多个数据集上的实验表明，在 IID 和非 IID 设置下均优于现有基线方法；FedAdaVR-Quant 能减少 50%、75% 和 87.5% 的内存需求，同时保持同等模型性能。

Conclusion: FedAdaVR 有效解决了联邦学习中客户端部分参与导致的异构性问题，其量化版本在保持性能的同时显著降低了内存需求，为实际部署提供了实用解决方案。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [50] [Causal Imitation Learning Under Measurement Error and Distribution Shift](https://arxiv.org/abs/2601.22206)
*Shi Bo,AmirEmad Ghassami*

Main category: cs.LG

TL;DR: 论文提出CausIL框架，解决离线模仿学习中存在测量误差和分布偏移时，行为克隆方法会产生系统性偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习在决策相关状态仅通过噪声测量观测且训练与部署分布可能变化时，会产生虚假的状态-动作相关性，导致标准行为克隆方法在分布偏移下收敛到有偏策略。

Method: 提出基于因果建模的CausIL框架，将噪声状态观测视为代理变量，借鉴近端因果推断思想，提供识别条件，开发离散和连续状态空间的估计器，连续设置中使用RKHS函数类的对抗过程学习参数。

Result: 在PhysioNet/Computing in Cardiology Challenge 2019队列的半模拟纵向数据上评估，相比行为克隆基线，CausIL在分布偏移下表现出更好的鲁棒性。

Conclusion: 通过因果建模处理测量误差，CausIL框架能够在无需奖励或交互专家查询的情况下，从演示中恢复目标策略，并提高对分布偏移的鲁棒性。

Abstract: We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.

</details>


### [51] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: LSFlow提出了一种用于组合强化学习的潜在球面流策略，通过将随机策略学习在连续潜在空间中，并利用组合优化求解器保证动作可行性，解决了组合动作空间中的挑战。


<details>
  <summary>Details</summary>
Motivation: 组合强化学习面临组合动作空间指数级大且可行性约束复杂的挑战，现有方法要么嵌入特定任务价值函数到约束优化程序中，要么学习确定性结构化策略，牺牲了通用性和策略表达能力。

Method: 提出LSFlow方法：1）在紧凑连续潜在空间中通过球面流匹配学习随机策略；2）使用组合优化求解器将潜在样本映射到有效结构化动作；3）在潜在空间中直接训练价值网络避免重复求解器调用；4）引入平滑贝尔曼算子处理求解器引起的分段常数和不连续价值景观。

Result: 在多个具有挑战性的组合强化学习任务中，LSFlow平均比最先进基线方法性能提升20.6%。

Conclusion: LSFlow通过将现代生成策略的表达能力与组合优化的可行性保证相结合，为组合强化学习提供了一种既表达力强又保证可行性的解决方案，同时通过潜在空间价值学习和平滑贝尔曼算子解决了训练稳定性问题。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [52] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和老年人护理，其中支持张量机(STM)在活动分类中表现最佳，准确率达96.67%


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年和脆弱患者依赖家庭护理，但常出现忽视和治疗性锻炼（如瑜伽或物理治疗）依从性差的问题，需要自动化监测解决方案

Method: 使用加速度计和陀螺仪收集活动数据（行走、上下楼梯、坐、站、躺），评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN）并与提出的支持张量机(STM)进行比较，STM利用张量表示保留时空运动动态

Result: SVM准确率93.33%，逻辑回归、随机森林和k-NN为91.11%，而STM显著优于这些模型，测试准确率达96.67%，交叉验证准确率最高达98.50%

Conclusion: 提出的框架在远程医疗、老年人辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [53] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：一种基于推理的LLM评判器，通过双层数据重加权学习框架训练，使用可验证奖励，在代码生成测试时缩放中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Best-of-N选择的测试时缩放方法依赖LLM评判器，但训练可靠的LLM评判器面临严重分布偏移挑战：简单与困难问题不平衡、训练任务与评估基准不匹配、训练数据与推理模型行为不一致。

Method: 提出DAJ（推理型LLM评判器），采用双层数据重加权学习框架，学习数据重要性权重（领域级或实例级），通过可验证奖励优化在目标基准对齐的元集上的泛化性能。

Result: 在LiveCodeBench和BigCodeBench上实现最先进性能，优于强测试时缩放基线和领先专有模型。

Conclusion: DAJ通过数据重加权自动强调困难问题、分布内样本和轨迹对齐数据，无需手工启发式方法，有效解决了LLM评判器训练中的分布偏移问题。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [54] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL是一个安全的去中心化联邦学习框架，结合DAG账本、侧链和零知识证明，实现隐私保护的模型验证和对抗行为检测。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（包括集中式和去中心化方法）在可扩展性、安全性和更新验证方面存在挑战，需要一种既能保护数据隐私又能有效验证模型更新的解决方案。

Method: 提出ZK-HybridFL框架：1）使用有向无环图（DAG）账本和专用侧链；2）集成零知识证明（ZKPs）进行隐私保护的模型验证；3）采用事件驱动智能合约和预言机辅助侧链验证本地模型更新；4）内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务中，相比Blade-FL和ChainFL，ZK-HybridFL实现了更快的收敛速度、更高的准确率、更低的困惑度和更低的延迟。框架能抵抗大量对抗节点和空闲节点，支持亚秒级链上验证和高效gas使用，防止无效更新和孤儿攻击。

Conclusion: ZK-HybridFL是一个可扩展且安全的去中心化联邦学习解决方案，适用于多样化环境，在保护数据隐私的同时有效验证模型更新并抵抗对抗攻击。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [55] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM通过函数化代码生成和元学习奖励修正，提升LLM在复杂编程任务上的性能，在LiveCodeBench和BigCodeBench上优于现有测试时缩放方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRM）在代码生成中效果有限，因为代码缺乏有意义的步骤分解，且蒙特卡洛估计的部分解决方案正确性评分存在噪声。需要解决这些挑战来提升LLM在复杂编程任务上的表现。

Method: FunPRM采用两种创新方法：1）提示LLM生成模块化、函数化的代码，将函数作为PRM推理步骤；2）引入基于元学习的奖励修正机制，利用单元测试获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础LLM上始终优于现有测试时缩放方法，与O4-mini结合时在LiveCodeBench上达到最先进性能。生成的代码更具可读性和可重用性。

Conclusion: FunPRM通过函数化代码分解和元学习奖励修正，有效解决了现有PRM在代码生成中的局限性，显著提升了LLM在复杂编程任务上的性能，同时改善了代码质量。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [56] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 论文提出一种打破注意力机制中冗余旋转自由度的对称性方法，通过批量采样的无学习查询和值偏置，改善优化器性能并增强注意力头的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型激活或输出的冗余旋转自由度，这些自由度在计算中被保留但未被有效利用。论文旨在通过打破这种对称性来改善优化器性能和模型可解释性。

Method: 引入简单的对称性打破协议：通过批量采样的、无学习的查询和值偏置，在旋转空间中插入一个优先方向。这种方法利用注意力机制中原本冗余的旋转自由度。

Result: 1) 显著改善简单、内存高效优化器的性能，缩小甚至消除与复杂内存密集型自适应方法的差距；2) 使冗余旋转自由度具有可解释性，能够在单个注意力头中选择性地放大语义上有意义的token类别。

Conclusion: 最小化、原则性的架构改变可以同时改善模型性能和可解释性，证明了打破注意力机制中冗余旋转自由度的对称性具有实际价值。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [57] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR：基于LLM的在线强化学习控制器，用于多阶段ML推理管道的自动扩缩容，无需梯度更新即可优化资源分配和延迟表现


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道难以自动扩缩容，原因包括异构资源、跨阶段耦合和动态瓶颈迁移。现有方法难以有效处理这些挑战

Method: 使用LLM作为上下文强化学习控制器，结合帕累托优势奖励塑造、基于surprisal的经验检索和用户空间CUDA拦截的GPU速率控制，在线改进策略而无需梯度更新

Result: 在四种ML服务管道和三种工作负载模式下，SAIR在P99延迟和有效资源成本方面表现最佳或并列最佳，P99延迟提升高达50%，有效成本降低高达97%，瓶颈检测准确率达86%

Conclusion: SAIR框架成功解决了多阶段ML推理管道的自动扩缩容挑战，通过LLM作为在线RL控制器实现了高效的资源管理和性能优化，无需离线训练

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [58] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析转化为分类问题，通过离散化事件时间构建二元分类框架，使表格基础模型无需训练即可进行生存分析


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务中表现出色，但难以适应生存分析，主要因为存在右删失数据（事件发生前观察终止）。需要一种方法让现有表格基础模型能够处理生存分析任务

Method: 将静态和动态生存分析重新表述为一系列二元分类问题：离散化事件时间，将删失观测视为某些时间点标签缺失的样本。通过上下文学习使表格基础模型无需显式训练即可进行生存分析

Result: 在53个真实世界数据集上的评估表明，采用该分类框架的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线方法

Conclusion: 该分类框架成功地将生存分析转化为表格基础模型可处理的形式，在标准删失假设下，最小化二元分类损失能够恢复真实生存概率，为生存分析提供了新的有效方法

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [59] [AsyncMesh: Fully Asynchronous Optimization for Data and Pipeline Parallelism](https://arxiv.org/abs/2601.22442)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Gil Avraham,Hadi Mohaghegh Dolatabadi,Chamin P Hewa Koneputugodage,Violetta Shevchenko,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 提出异步更新方法解决数据并行和管道并行中的通信瓶颈，通过权重前瞻和异步稀疏平均减少通信开销，在保持性能的同时提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 数据并行和管道并行在分布式神经网络训练中面临高通信成本问题，需要快速互连的计算集群，这限制了其可扩展性。需要解决通信瓶颈，放宽共置要求。

Method: 1) 在管道并行中采用权重前瞻方法；2) 在数据并行中引入异步稀疏平均方法，配备基于指数移动平均的校正机制；3) 提供稀疏平均和异步更新的收敛保证。

Result: 在大型语言模型（最多10亿参数）上的实验表明，该方法在显著减少通信开销的同时，性能与完全同步基线相当。

Conclusion: 通过异步更新方法有效解决了分布式训练中的通信瓶颈问题，在保持模型性能的同时提高了系统的可扩展性。

Abstract: Data and pipeline parallelism are key strategies for scaling neural network training across distributed devices, but their high communication cost necessitates co-located computing clusters with fast interconnects, limiting their scalability. We address this communication bottleneck by introducing asynchronous updates across both parallelism axes, relaxing the co-location requirement at the expense of introducing staleness between pipeline stages and data parallel replicas. To mitigate staleness, for pipeline parallelism, we adopt a weight look-ahead approach, and for data parallelism, we introduce an asynchronous sparse averaging method equipped with an exponential moving average based correction mechanism. We provide convergence guarantees for both sparse averaging and asynchronous updates. Experiments on large-scale language models (up to \em 1B parameters) demonstrate that our approach matches the performance of the fully synchronous baseline, while significantly reducing communication overhead.

</details>


### [60] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: SQUAD结合早期退出机制与分布式集成学习，通过基于法定人数的停止准则和优化的层次多样性，在减少推理延迟的同时提高不确定性估计和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统早期退出神经网络依赖单一模型置信度阈值，但由于校准问题经常不可靠。需要一种能同时改善不确定性估计并减少推理时间的方法。

Method: 提出SQUAD推理方案，集成早期退出机制与分布式集成学习，采用基于法定人数的停止准则，按计算复杂度顺序收集中间预测直到达成共识。引入QUEST神经架构搜索方法，选择具有优化层次多样性的早期退出学习器。

Result: 相比最先进的动态解决方案，测试准确率提高达5.95%，计算成本相当；相比静态集成，推理延迟减少达70.60%，同时保持良好准确率。

Conclusion: SQUAD通过共识驱动的方法实现了统计上稳健的早期退出，在减少推理延迟的同时提高了准确性和不确定性估计。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [61] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL算法在联邦域增量学习中实现无记忆、无缓冲的持续学习，通过服务器端锚点控制漂移，保证向后知识传递和跨任务收敛


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统中数据分布会漂移，但隐私规则禁止原始数据共享。现有方法缺乏向后知识传递的理论保证和跨任务的部分参与收敛率分析

Method: SPECIAL算法在FedAvg基础上添加服务器端锚点：每轮采样参与客户端时，用轻量级近端项将更新推向先前全局模型，无需回放缓冲、合成数据或任务特定头

Result: 理论证明SPECIAL：(1)保证向后知识传递，先前任务损失增加受漂移控制项限制；(2)实现首个FDIL部分参与的非凸收敛率O((E/NT)^(1/2))，匹配单任务FedAvg

Conclusion: SPECIAL是简单、内存免费的FDIL算法，通过服务器锚点控制累积漂移，保持通信和模型大小不变，在理论和实验上均表现有效

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [62] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: SurrogateSHAP：一个无需重新训练的高效Shapley值近似框架，用于评估文本到图像扩散模型中数据贡献者的价值，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意工作流中的广泛应用，需要建立公平的数据贡献者价值评估框架。传统的Shapley值方法面临双重计算瓶颈：模型重新训练成本高和组合子集数量庞大

Method: 提出SurrogateSHAP框架：1) 通过预训练模型推理近似昂贵的重新训练过程；2) 使用梯度提升树近似效用函数，并从树模型解析推导Shapley值

Result: 在三个不同属性任务中表现优异：1) CIFAR-20上的DDPM-CFG图像质量；2) 后印象派艺术品的Stable Diffusion美学评估；3) Fashion-Product数据上的FLUX.1产品多样性。相比现有方法计算开销显著降低，能一致识别有影响力的贡献者

Conclusion: SurrogateSHAP为数据市场提供了可扩展的公平补偿框架，并能有效定位临床图像中虚假相关的数据源，为审计安全关键生成模型提供了可行路径

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [63] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一个基于控制理论的优化算法框架，将经典优化器统一在几何框架下，通过构造严格Lyapunov函数保证收敛，并在大规模基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前优化算法改进多基于启发式方法，缺乏系统理论框架。本文旨在建立控制理论与机器学习优化之间的桥梁，提供系统化的优化器设计工具。

Method: 将优化问题重新解释为黎曼参数流形上的扩展状态离散时间控制系统，识别Normally Attracting Invariant Manifold (NAIM)，构造严格Lyapunov函数证明收敛性，并开发"优化器生成器"框架。

Result: RLOs不仅能够恢复经典优化算法，还能系统设计新的优化器。通过几何诊断验证理论，并在大规模基准测试中展示出最先进的性能。

Conclusion: RLOs为优化器设计提供了统一的语言和系统化工具包，将控制理论与现代机器学习优化相结合，能够设计出稳定有效的优化算法。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [64] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功因素不仅取决于模型本身，还依赖于合并方法和任务特性，研究发现子空间重叠和梯度对齐是兼容性的基础前提


<details>
  <summary>Details</summary>
Motivation: 当前对模型合并的理解有限，通常将可合并性视为内在属性，但本文旨在揭示合并成功实际上取决于合并方法和任务特性，需要建立更系统的理解框架

Method: 提出架构无关的分析框架，使用线性优化和可解释的成对度量（如梯度L2距离），分析四种合并方法，识别与合并后性能相关的属性

Result: 发现成功驱动因素存在显著差异（46.7%度量重叠；55.3%符号一致性），揭示了方法特定的"指纹"，但子空间重叠和梯度对齐始终作为兼容性的基础前提出现

Conclusion: 研究为理解模型可合并性提供了诊断基础，并激励未来开发明确促进子空间重叠和梯度对齐属性的微调策略

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [65] [ParalESN: Enabling parallel information processing in Reservoir Computing](https://arxiv.org/abs/2601.22296)
*Matteo Pinna,Giacomo Lagomarsini,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出ParalESN，一种基于对角线性递归的并行回声状态网络，解决了传统RC的顺序处理和内存占用问题，实现了并行时序数据处理。


<details>
  <summary>Details</summary>
Motivation: 传统储层计算(RC)存在两个主要限制：1) 必须顺序处理时序数据；2) 高维储层的内存占用过大。这些限制严重影响了RC的可扩展性。

Method: 通过结构化算子和状态空间建模重新审视RC，提出ParalESN。该方法基于复数空间中的对角线性递归构建高维高效储层，实现时序数据的并行处理。

Result: 理论分析表明ParalESN保持回声状态特性和传统ESN的普适性保证。实验显示在时间序列基准上匹配传统RC精度，同时大幅节省计算成本。在1-D像素级分类任务上，与全可训练神经网络达到竞争性精度，同时计算成本和能耗降低数个数量级。

Conclusion: ParalESN为将RC集成到深度学习领域提供了一个有前景、可扩展且原理清晰的途径，解决了传统RC的可扩展性瓶颈。

Abstract: Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.

</details>


### [66] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出CP4Gen方法，为条件生成模型提供系统化的共形预测框架，通过聚类密度估计构建更稳健、可解释的预测集


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，这在高风险应用中削弱了对单个输出的信任，需要一种系统化的不确定性量化方法

Method: 提出CP4Gen方法，采用基于聚类的密度估计技术，构建对异常值不敏感、更可解释、结构复杂度更低的预测集

Result: 在合成数据集和实际应用（包括气候模拟任务）上的广泛实验表明，CP4Gen在预测集体积和结构简单性方面始终优于现有方法

Conclusion: CP4Gen为条件生成模型提供了一个强大的不确定性估计工具，特别适用于需要严格且可解释预测集的场景

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [67] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出贝叶斯工作流生成(BWG)框架，将工作流生成建模为贝叶斯推断问题，通过重要性采样和序列细化器构建工作流，在六个基准数据集上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多将其视为优化问题，缺乏理论基础。本文提出将其重新定义为贝叶斯推断问题，以获得更理论严谨的工作流生成框架。

Method: 提出贝叶斯工作流生成(BWG)框架，使用并行前瞻回滚进行重要性加权，结合序列循环细化器进行池级改进。具体实现为BayesFlow算法，无需训练即可构建工作流。

Result: 在六个基准数据集上，BayesFlow相比最先进的工作流生成基线准确率提升高达9个百分点，相比零样本提示提升高达65个百分点。

Conclusion: BWG为基于搜索的工作流设计提供了理论严谨的升级方案，证明了贝叶斯推断框架在自动工作流生成中的有效性。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [68] [Exact closed-form Gaussian moments of residual layers](https://arxiv.org/abs/2601.22307)
*Simon Kuang,Xinfan Lin*

Main category: cs.LG

TL;DR: 提出了一种通过逐层矩匹配在深度残差神经网络中传播高斯分布均值和协方差的精确方法，显著优于现有替代方案


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中高斯分布传播的长期难题，为多种激活函数提供精确的矩匹配方法，以改进不确定性量化

Method: 使用逐层矩匹配技术，推导出probit、GeLU、ReLU、Heaviside和sine激活函数的精确矩匹配公式，适用于前馈和广义残差层

Result: 在随机网络上KL散度误差指标比流行方法提升数个数量级（最高百万倍），在真实数据上具有竞争力的统计校准性能，在变分贝叶斯网络上比最先进确定性推理方法提升百倍

Conclusion: 该方法为深度神经网络中的不确定性传播提供了精确高效的解决方案，在多个基准测试中显著优于现有方法，并为随机前馈神经元提供了初步分析

Abstract: We study the problem of propagating the mean and covariance of a general multivariate Gaussian distribution through a deep (residual) neural network using layer-by-layer moment matching. We close a longstanding gap by deriving exact moment matching for the probit, GeLU, ReLU (as a limit of GeLU), Heaviside (as a limit of probit), and sine activation functions; for both feedforward and generalized residual layers. On random networks, we find orders-of-magnitude improvements in the KL divergence error metric, up to a millionfold, over popular alternatives. On real data, we find competitive statistical calibration for inference under epistemic uncertainty in the input. On a variational Bayes network, we show that our method attains hundredfold improvements in KL divergence from Monte Carlo ground truth over a state-of-the-art deterministic inference method. We also give an a priori error bound and a preliminary analysis of stochastic feedforward neurons, which have recently attracted general interest.

</details>


### [69] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 论文提出了一种新的最优隐蔽攻击方法，能够绕过现有防御，并开发了新的评估方法和防御方案BayesClean。


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业过程和科学领域广泛应用，但其对投毒攻击的鲁棒性研究不足，现有研究往往基于不现实的威胁模型，实用性有限。

Method: 1) 提出考虑不同可检测程度的最优隐蔽攻击公式；2) 开发基于目标归一化的新方法评估攻击效果与可检测性之间的权衡；3) 提出新的防御方法BayesClean对抗隐蔽攻击。

Result: 提出的最优隐蔽攻击能够绕过最先进的防御系统；BayesClean在攻击隐蔽且投毒点数量显著时优于现有防御方法。

Conclusion: 论文提供了更实用的攻击威胁模型、评估框架和有效的防御方案，填补了回归模型投毒攻击鲁棒性研究的空白。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [70] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个评估材料基础模型在几何尺度泛化能力的基准，包含三个任务：晶体结构到性质预测、物理推理链式思考、以及基于目标性质的晶体检索，重点关注模型在结构分布变化下的幻觉、一致性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在材料科学推理中的应用日益增多，但它们在物理结构化分布变化下的行为仍不明确。需要评估模型在不同几何尺度下的泛化能力，以及其与结构幻觉、一致性和推理能力的关系。

Method: 开发SCALAR基准，包含约100,000个从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米颗粒结构。定义三个任务：1) CIF到性质预测；2) 包含显式物理推理的链式思考变体；3) 基于目标性质的晶体逆向检索。使用结构化指标评估数值误差、幻觉、跨提示一致性、单调推理、输出有效性和检索遗憾。

Result: 实验显示不同基础模型在显式推理下表现出显著且模型依赖的变化：通常减少幻觉和误差，但经常破坏一致性或有效性。几何尺度泛化能力不能仅从准确性推断。

Conclusion: SCALAR基准揭示了材料基础模型在几何尺度泛化方面的复杂行为，强调需要超越简单准确性指标来评估模型性能，特别是在物理结构化分布变化下的表现。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [71] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 静态黑盒评估无法保证LLM更新后的对齐性，存在隐藏的对抗行为可能被良性更新激活


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究通常假设初始模型是对齐的，但实践中LLM经常更新，而静态评估无法检测更新后可能出现的未对齐行为

Method: 从理论上证明由于过参数化，静态对齐不能保证更新后对齐；通过理论分析和实证验证，在隐私、越狱安全和行为诚实三个对齐领域测试LLM

Result: 存在能通过所有标准黑盒对齐测试的LLM，但在单个良性更新后变得严重未对齐；隐藏对抗行为的能力随模型规模增加

Conclusion: 静态评估协议不足，迫切需要更新后鲁棒的对齐评估方法

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [72] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB是一种贝叶斯优化算法，利用昂贵真实评估和廉价预测模型，结合离线数据提高样本效率，在假设生成任务中实现更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 现实优化问题通常涉及昂贵的真实评估（如人工评估、物理实验）和廉价的低保真预测（如机器学习模型、模拟），同时存在大量离线数据可用于预训练模型和提供先验信息。

Method: 提出预测增强高斯过程上置信界算法（PA-GP-UCB），使用联合高斯过程后验推导的控制变量估计器来校正预测偏差并减少不确定性，结合两种评估源和离线数据。

Result: 理论证明PA-GP-UCB保持GP-UCB的标准遗憾率，但获得更小的主导常数，该常数由预测质量和离线数据覆盖度明确控制。实验显示在合成基准和基于人类行为数据的真实假设评估任务中收敛更快。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，特别适用于大语言模型提供预测的实际应用场景。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [73] [FlowSymm: Physics Aware, Symmetry Preserving Graph Attention for Network Flow Completion](https://arxiv.org/abs/2601.22317)
*Ege Demirci,Francesco Bullo,Ananthram Swami,Ambuj Singh*

Main category: cs.LG

TL;DR: FlowSymm：一种用于网络流恢复的新架构，通过群作用、图注意力编码和Tikhonov优化，在保持局部守恒定律的同时恢复缺失流


<details>
  <summary>Details</summary>
Motivation: 网络边缘缺失流的恢复是一个基本逆问题，在交通、能源、移动性等系统中普遍存在。现有方法需要同时满足局部守恒定律和观测约束，这是一个具有挑战性的问题。

Method: 1. 基于最小范数无散流完成锚定观测值；2. 计算保持观测流不变的所有可容许群作用的正交基；3. 使用GATv2层编码图和边特征，通过注意力机制选择物理感知的群作用；4. 通过隐式双层优化的Tikhonov惩罚细化缺失项

Result: 在三个真实世界流基准测试（交通、电力、自行车）中，FlowSymm在RMSE、MAE和相关指标上优于最先进的基线方法

Conclusion: FlowSymm通过结合群对称性、图注意力学习和隐式优化，提供了一种有效恢复网络缺失流的方法，同时严格保持物理守恒定律

Abstract: Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.

</details>


### [74] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出首个联邦学习框架用于LLM路由选择，使客户端能从本地离线查询-模型评估数据中学习共享路由策略，解决数据分散和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法假设有集中式的查询-模型评估数据，但实际数据分散在各客户端且涉及隐私，无法集中。同时，单客户端训练因数据有限、查询分布受限和模型评估偏置而效果不佳。

Method: 引入首个联邦学习框架用于LLM路由，支持参数化多层感知器路由器和非参数化K-means路由器，适用于异构客户端查询分布和非均匀模型覆盖场景。

Result: 在两个基准测试中，联邦协作通过增加有效模型覆盖和更好的查询泛化，在准确率-成本权衡上优于客户端本地路由器。理论结果也验证了联邦训练能减少路由次优性。

Conclusion: 联邦学习框架能有效解决LLM路由中的数据分散和隐私问题，通过客户端协作学习共享路由策略，在保持隐私的同时提升路由性能。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [75] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种在用户级差分隐私下的连续均值估计新方法，使用近似差分隐私和矩阵分解机制，相比纯差分隐私显著降低误差


<details>
  <summary>Details</summary>
Motivation: 连续均值估计中，现有方法主要关注纯差分隐私，导致估计噪声过大，限制了实际应用。需要研究在近似差分隐私下的更优解决方案

Method: 采用近似差分隐私框架，利用矩阵分解机制的最新进展，提出专门针对均值估计的新型分解方法，该方法既高效又准确

Result: 新方法在用户级差分隐私下的连续均值估计中，实现了渐进更低的均方误差界限，相比纯差分隐私方法显著提升了准确性

Conclusion: 通过近似差分隐私和专门设计的矩阵分解机制，可以显著改善连续均值估计的精度，为实际应用提供了更可行的隐私保护解决方案

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [76] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: 提出SAC-GT框架，结合图变换器和空间自适应保形预测，实现高精度且具有统计可靠性保证的室内定位。


<details>
  <summary>Details</summary>
Motivation: 室内定位对智能环境中的位置服务至关重要。现有基于图的方法虽然能提供更精细的定位粒度，但缺乏预测不确定性的量化能力，而这在实际部署中是关键需求。

Method: 提出SAC-GT框架：1) 图变换器(GT)模型，捕捉Wi-Fi接入点与设备间的空间拓扑和信号强度动态；2) 新颖的空间自适应保形预测(SACP)方法，提供区域特定的不确定性估计。

Result: 在大规模真实世界数据集上的评估表明，SAC-GT实现了最先进的定位精度，同时提供了鲁棒且空间自适应的可靠性保证。

Conclusion: SAC-GT不仅能产生精确的2D位置预测，还能根据环境条件变化提供统计有效的置信区域，解决了现有方法在不确定性量化方面的不足。

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [77] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展可控的性能估计路由框架，通过强化学习训练，基于相似问题检索预测模型成本和性能，实现动态路由决策，在精度优先时可提升25.7%准确率，在效率优先时可降低95.1%成本。


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法通常将路由视为固定的小规模模型选择问题，难以适应新模型或变化的预算约束。需要一种能够灵活适应不同模型和用户需求的路由框架。

Method: 提出SCOPE框架，通过强化学习训练，基于检索相似问题来预测模型的成本和性能，而不是依赖固定的模型名称。将路由转化为动态决策问题，允许用户控制精度与成本的权衡。

Result: 实验表明SCOPE不仅能节省成本，还能灵活适应用户需求：当性能优先时，准确率可提升25.7%；当效率优先时，成本可降低95.1%。

Conclusion: SCOPE超越了传统的模型选择方法，通过预测模型成本和性能实现动态路由决策，能够有效适应新模型和变化的用户需求，在精度和效率之间提供灵活权衡。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [78] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore：利用LLM生成候选规则，通过验证选择循环构建可部署的临床评分系统，在保持可解释性的同时达到与更灵活模型相当的预测性能


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型虽然预测性能强，但往往不符合临床工作流程的约束（如可记忆性、可审计性、床边执行性），导致难以转化为常规临床使用。可部署的临床指南通常采用单位加权临床检查表的形式，但学习这种评分系统需要在指数级大的离散规则空间中进行搜索。

Method: 提出AgentScore方法：1）使用LLM在语义指导下生成候选规则；2）通过确定性的、基于数据的验证与选择循环来确保统计有效性和可部署性约束；3）构建单位加权的临床检查表形式的评分系统。

Result: 在8个临床预测任务中，AgentScore优于现有的评分生成方法，并在更强的结构约束下达到了与更灵活的可解释模型相当的AUC。在2个外部验证任务中，AgentScore实现了比已建立的基于指南的评分更高的区分度。

Conclusion: AgentScore通过结合LLM的语义指导和基于数据的验证，能够生成既符合临床部署约束又具有良好预测性能的评分系统，弥合了机器学习模型与临床实践需求之间的差距。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [79] [Label-Efficient Monitoring of Classification Models via Stratified Importance Sampling](https://arxiv.org/abs/2601.22326)
*Lupo Marsigli,Angel Lopez de Haro*

Main category: cs.LG

TL;DR: 提出基于分层重要性采样(SIS)的通用框架，用于在标签预算严格、批量获取标签且错误率极低的生产环境中高效监控分类模型性能。


<details>
  <summary>Details</summary>
Motivation: 生产环境中分类模型监控面临三大挑战：严格的标签预算限制、一次性批量获取标签的方式、以及极低的错误率。传统监控方法在这些约束下效率低下，需要更有效的解决方案。

Method: 采用分层重要性采样(SIS)框架，结合重要性采样和分层随机采样的优势。该方法不依赖最优的提议分布或分层定义，即使使用噪声代理和次优分层，仍能提高估计效率。

Result: 理论分析表明，在温和条件下，SIS能产生无偏估计量，相比重要性采样(IS)和分层随机采样(SRS)具有严格的有限样本均方误差改进。二元和多分类任务的实验显示，在固定标签预算下能持续提高效率。

Conclusion: SIS为部署后模型监控提供了一种原则性、标签高效且操作轻量的方法论，特别适用于生产环境中标签稀缺和错误率极低的监控场景。

Abstract: Monitoring the performance of classification models in production is critical yet challenging due to strict labeling budgets, one-shot batch acquisition of labels and extremely low error rates. We propose a general framework based on Stratified Importance Sampling (SIS) that directly addresses these constraints in model monitoring. While SIS has previously been applied in specialized domains, our theoretical analysis establishes its broad applicability to the monitoring of classification models. Under mild conditions, SIS yields unbiased estimators with strict finite-sample mean squared error (MSE) improvements over both importance sampling (IS) and stratified random sampling (SRS). The framework does not rely on optimally defined proposal distributions or strata: even with noisy proxies and sub-optimal stratification, SIS can improve estimator efficiency compared to IS or SRS individually, though extreme proposal mismatch may limit these gains. Experiments across binary and multiclass tasks demonstrate consistent efficiency improvements under fixed label budgets, underscoring SIS as a principled, label-efficient, and operationally lightweight methodology for post-deployment model monitoring.

</details>


### [80] [Molecular Representations in Implicit Functional Space via Hyper-Networks](https://arxiv.org/abs/2601.22327)
*Zehong Wang,Xiaolong Han,Qi Yang,Xiangru Tang,Fang Wu,Xiaoguang Guo,Weixiang Sun,Tianyi Ma,Pietro Lio,Le Cong,Sheng Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: MolField将分子视为三维空间中的连续函数而非离散对象，通过超网络学习分子场分布，实现物理一致的分子表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示方法（序列、图、点云）将分子视为离散对象，忽略了分子本质上连续、场状的物理特性。作者认为分子学习应在函数空间中进行，将分子建模为三维空间的连续函数。

Method: 提出MolField框架：1）将分子建模为三维空间中的连续函数（分子场）；2）使用超网络学习分子场分布；3）在规范化坐标上定义函数以确保SE(3)不变性；4）引入结构化权重标记化，训练序列超网络建模分子场的共享先验。

Result: 在分子动力学和性质预测任务上评估MolField，结果显示：将分子视为连续函数从根本上改变了分子表示在不同任务间的泛化方式，下游行为对分子离散化或查询方式具有稳定性。

Conclusion: 分子学习应被重新构想为函数空间中的学习，将分子视为连续函数而非离散对象，这为分子表示学习提供了新的理论基础和实践框架。

Abstract: Molecular representations fundamentally shape how machine learning systems reason about molecular structure and physical properties. Most existing approaches adopt a discrete pipeline: molecules are encoded as sequences, graphs, or point clouds, mapped to fixed-dimensional embeddings, and then used for task-specific prediction. This paradigm treats molecules as discrete objects, despite their intrinsically continuous and field-like physical nature. We argue that molecular learning can instead be formulated as learning in function space. Specifically, we model each molecule as a continuous function over three-dimensional (3D) space and treat this molecular field as the primary object of representation. From this perspective, conventional molecular representations arise as particular sampling schemes of an underlying continuous object. We instantiate this formulation with MolField, a hyper-network-based framework that learns distributions over molecular fields. To ensure physical consistency, these functions are defined over canonicalized coordinates, yielding invariance to global SE(3) transformations. To enable learning directly over functions, we introduce a structured weight tokenization and train a sequence-based hyper-network to model a shared prior over molecular fields. We evaluate MolField on molecular dynamics and property prediction. Our results show that treating molecules as continuous functions fundamentally changes how molecular representations generalize across tasks and yields downstream behavior that is stable to how molecules are discretized or queried.

</details>


### [81] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过核状态重建结合结构先验，从噪声、不完整观测中恢复物理一致的轨迹和导数，为符号回归提供可靠输入。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测下失效，或依赖黑盒潜在动力学模型而缺乏可解释性，需要能从碎片化传感器数据中恢复物理一致状态估计的框架。

Method: 基于知识引导的核状态重建，在再生核希尔伯特空间中公式化状态重建，直接融入非负性、守恒定律等结构先验和领域特定观测模型，处理异构采样和测量粒度。

Result: 在12个科学基准测试和多种噪声机制下，MAAT显著降低了状态估计的均方误差，为下游符号回归提供了更准确的轨迹和导数。

Conclusion: MAAT为从碎片化传感器数据到符号回归提供了原则性接口，通过结合结构先验实现物理一致的状态估计，在噪声和不完整观测下表现优异。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [82] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种可扩展的批量校正方法，通过局部亲和度和子采样来对齐跨批次样本，适用于大规模Cell Painting数据，运行时间接近线性。


<details>
  <summary>Details</summary>
Motivation: 大规模Cell Painting数据受到实验室、仪器和协议差异引起的批次效应严重影响，这些效应会掩盖生物信号，需要可扩展的批量校正方法。

Method: BALANS通过构建平滑的亲和度矩阵来对齐批次样本：(1)使用批次感知的局部尺度计算高斯核亲和度；(2)采用自适应采样策略，优先选择邻居覆盖度低的行，每行只保留最强亲和度，形成稀疏但信息丰富的近似矩阵。

Result: BALANS在样本复杂度上达到最优顺序，提供近似保证，运行时间接近线性。在真实Cell Painting数据集和合成基准测试中，BALANS能够扩展到大规模数据，在保持校正质量的同时，相比广泛使用的批量校正方法提高了运行效率。

Conclusion: BALANS是一种高效、可扩展的批量校正方法，能够有效处理大规模Cell Painting数据中的批次效应，为药物发现中的细胞形态分析提供可靠支持。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [83] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的DP-SGD噪声相关策略，仅将噪声与前一迭代相关并通过伪随机噪声生成器消除部分噪声，无需存储历史噪声，内存开销与标准DP-SGD相同。


<details>
  <summary>Details</summary>
Motivation: 现有DP-SGD扩展方法（如矩阵分解机制）通过跨多个训练迭代引入相关噪声来提高准确性，但需要存储先前添加的噪声向量，导致显著的内存开销。

Method: 提出新的噪声相关策略：1) 仅将噪声与前一迭代相关；2) 使用伪随机噪声生成器再生噪声，消除对历史噪声存储的需求；3) 取消受控部分的噪声。

Result: 方法无需额外内存开销（与标准DP-SGD相同），计算开销最小，且实验证明比DP-SGD具有更高的准确性。

Conclusion: 提出了一种内存高效的DP-SGD改进方法，通过创新的噪声相关策略在保持隐私保护的同时提高了模型准确性。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [84] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文提出了用于偏好贝叶斯优化的精确知识梯度方法，解决了传统方法在成对比较查询场景中的计算难题。


<details>
  <summary>Details</summary>
Motivation: 许多实际场景只允许成对比较查询（偏好贝叶斯优化问题），无法直接评估函数值。将知识梯度扩展到偏好贝叶斯优化面临计算挑战，核心问题在于前瞻步骤需要计算非高斯后验分布，之前被认为难以处理。

Method: 推导出用于偏好贝叶斯优化的精确解析知识梯度方法，解决了非高斯后验分布的计算难题。

Result: 精确知识梯度在一系列基准问题上表现强劲，通常优于现有的采集函数。同时通过案例研究展示了知识梯度在某些场景下的局限性。

Conclusion: 本文成功解决了偏好贝叶斯优化中知识梯度的计算难题，提出了有效的精确方法，同时指出了该方法在某些情况下的局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [85] [Quantum-Inspired Reinforcement Learning for Secure and Sustainable AIoT-Driven Supply Chain Systems](https://arxiv.org/abs/2601.22339)
*Muhammad Bilal Akram Dastagir,Omer Tariq,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: cs.LG

TL;DR: 提出一种量子启发强化学习框架，用于AIoT供应链系统，同时优化碳足迹、库存管理和安全防护


<details>
  <summary>Details</summary>
Motivation: 现代供应链需要在高速物流与环保、安全之间取得平衡，传统优化模型常忽视可持续性目标和网络安全漏洞，使系统易受生态破坏和恶意攻击

Method: 设计量子启发强化学习框架，结合可控自旋链类比与实时AIoT信号，通过多目标奖励函数统一保真度、安全性和碳成本，采用基于价值和集成更新的稳定训练方法

Result: 在仿真中表现出平滑收敛、后期性能强劲，在典型噪声信道下性能优雅下降，优于标准学习和基于模型的参考方法，能稳健处理实时可持续性和风险需求

Conclusion: 量子启发AIoT框架有潜力推动大规模安全、环保的供应链运营，为全球互联基础设施奠定基础，负责任地满足消费者和环境需求

Abstract: Modern supply chains must balance high-speed logistics with environmental impact and security constraints, prompting a surge of interest in AI-enabled Internet of Things (AIoT) solutions for global commerce. However, conventional supply chain optimization models often overlook crucial sustainability goals and cyber vulnerabilities, leaving systems susceptible to both ecological harm and malicious attacks. To tackle these challenges simultaneously, this work integrates a quantum-inspired reinforcement learning framework that unifies carbon footprint reduction, inventory management, and cryptographic-like security measures. We design a quantum-inspired reinforcement learning framework that couples a controllable spin-chain analogy with real-time AIoT signals and optimizes a multi-objective reward unifying fidelity, security, and carbon costs. The approach learns robust policies with stabilized training via value-based and ensemble updates, supported by window-normalized reward components to ensure commensurate scaling. In simulation, the method exhibits smooth convergence, strong late-episode performance, and graceful degradation under representative noise channels, outperforming standard learned and model-based references, highlighting its robust handling of real-time sustainability and risk demands. These findings reinforce the potential for quantum-inspired AIoT frameworks to drive secure, eco-conscious supply chain operations at scale, laying the groundwork for globally connected infrastructures that responsibly meet both consumer and environmental needs.

</details>


### [86] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 评估语言模型在有限交互预算下探索交互环境的能力，发现当前模型存在系统性探索不足和次优解问题，性能常低于简单探索-利用启发式基线。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在交互环境中的探索能力，特别是在有限交互预算下的表现，这对于实际应用中的资源受限场景很重要。

Method: 引入三个参数可控的探索难度任务（涵盖连续和离散环境），评估最先进的语言模型，并与简单探索-利用启发式基线比较。研究两种轻量级干预：将固定预算分割为并行执行，以及定期总结交互历史。

Result: 发现语言模型存在系统性探索不足和次优解问题，性能常显著低于简单基线，且随预算增加扩展性弱。并行分割预算能提高性能（尽管理论上无增益），定期总结历史能保留关键发现并进一步改善探索。

Conclusion: 当前语言模型在有限交互预算下的探索能力不足，需要改进。轻量级干预如预算分割和历史总结能有效提升性能，为改善模型探索能力提供了实用方向。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [87] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant：一种基于块旋转感知的后训练量化框架，通过排列重分布激活质量来优化异常值抑制，在Llama3 1B INT4量化中恢复90%全向量旋转困惑度


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法使用块旋转来扩散异常值，但块结构对异常值抑制的影响尚未被充分理解。需要系统分析块Hadamard旋转的异常值抑制机制，并提出更有效的量化框架。

Method: 1. 首次对块Hadamard旋转的异常值抑制进行系统性非渐近分析；2. 提出MixQuant框架，通过排列在旋转前重分布激活质量；3. 设计贪心质量扩散算法，通过均衡期望块级ℓ₁范数来校准排列；4. 识别Transformer架构中的排列等变区域，将排列合并到模型权重中以避免推理开销。

Result: MixQuant在所有块大小下都能持续提升精度，在将Llama3 1B量化为INT4、块大小为16时，恢复了90%的全向量旋转困惑度，相比不使用排列的46%有显著提升。

Conclusion: 异常值抑制受输入向量几何结构的根本限制，当预旋转ℓ₁范数质量在块间均匀分布时，旋转后异常值被确定性最小化。MixQuant通过排列重分布激活质量，有效提升了块旋转量化性能。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [88] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于占用度量的策略表示学习方法，通过集合架构统一近似策略范围，支持基于潜在空间梯度的行为合成


<details>
  <summary>Details</summary>
Motivation: 为了在测试时灵活引导行为，需要学习一系列策略的表示。由于MDP中的策略由其占用度量唯一确定，因此提出将策略表示建模为状态-动作特征映射相对于占用度量的期望

Method: 使用基于集合的架构将状态-动作样本编码为潜在嵌入，从中解码策略和多个奖励对应的价值函数。采用变分生成方法构建平滑潜在空间，并通过对比学习使其几何结构与价值函数差异对齐

Result: 构建了支持梯度优化的潜在空间几何结构，能够解决新颖的行为合成任务，即在不额外训练的情况下引导策略满足未见过的价值函数约束

Conclusion: 该方法通过统一的策略表示学习框架，实现了在潜在空间中进行梯度优化的能力，为灵活的行为引导和合成提供了有效解决方案

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [89] [Recoverability Has a Law: The ERR Measure for Tool-Augmented Agents](https://arxiv.org/abs/2601.22352)
*Sri Vatsa Vuddanti,Satwik Kumar Chittiprolu*

Main category: cs.LG

TL;DR: 语言模型代理的自我恢复能力遵循可测量的定量规律，通过期望恢复遗憾(ERR)和效率评分(ES)的一阶关系来描述恢复动态。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在执行工具调用失败后经常表现出自我恢复能力，但这种行为缺乏正式的理论解释。本文旨在填补这一空白，为恢复能力提供理论基础。

Method: 通过期望恢复遗憾(ERR)形式化恢复能力，量化恢复策略与最优策略在随机执行噪声下的偏差，并推导ERR与经验可观测量效率评分(ES)的一阶关系，建立可证伪的恢复动态定量规律。

Result: 在五个工具使用基准测试（包括受控扰动、诊断推理和真实API）中，ERR-ES定律预测的遗憾与蒙特卡洛模拟观察到的失败后遗憾高度匹配（δ≤0.05），验证了定律的有效性。

Conclusion: 恢复能力不是模型规模或架构的产物，而是交互动态的受控属性，为语言代理的执行级鲁棒性提供了理论基础。

Abstract: Language model agents often appear capable of self-recovery after failing tool call executions, yet this behavior lacks a formal explanation. We present a predictive theory that resolves this gap by showing that recoverability follows a measurable law. To elaborate, we formalize recoverability through Expected Recovery Regret (ERR), which quantifies the deviation of a recovery policy from the optimal one under stochastic execution noise, and derive a first-order relationship between ERR and an empirical observable quantity, the Efficiency Score (ES). This yields a falsifiable first-order quantitative law of recovery dynamics in tool-using agents. We empirically validate the law across five tool-use benchmarks spanning controlled perturbations, diagnostic reasoning, and real-world APIs. Across model scales, perturbation regimes, and recovery horizons, predicted regret under the ERR-ES law closely matched observed post-failure regret measured from Monte Carlo rollouts, within delta less than or equal to 0.05. Our results reveal that recoverability is not an artifact of model scale or architecture, but a governed property of interaction dynamics, providing a theoretical foundation for execution-level robustness in language agents.

</details>


### [90] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 本文提出基于最优传输理论，利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离来量化经验分布与高斯分布之间的偏差，并证明该空间中的填充锥是平坦的，从而为高斯近似提供了新的几何视角。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化经验分布与高斯分布之间的偏差时存在局限性，特别是常用的矩匹配高斯分布并不一定是Wasserstein距离下的最近高斯分布。需要一种基于最优传输理论的几何框架来更准确地衡量非高斯性。

Method: 利用相对平移不变二次Wasserstein空间的锥几何结构，引入两个新的几何量：相对Wasserstein角和正交投影距离。在一维情况下推导闭式表达式，在高维情况下开发基于半离散对偶公式的随机流形优化算法。

Result: 证明了该空间中任意两条射线生成的填充锥是平坦的，确保了角度、投影和内积的严格定义。实验表明相对Wasserstein角比Wasserstein距离更鲁棒，提出的最近高斯分布在FID评分评估中比矩匹配提供更好的近似。

Conclusion: 通过最优传输的几何视角，为量化非高斯性提供了新的理论框架和实用工具，揭示了矩匹配高斯的局限性，并展示了相对Wasserstein角在分布近似中的优越性。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [91] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: PoSafeNet：基于偏序集结构的安全层，通过顺序闭式投影自适应执行安全约束，提高机器人控制的安全性和可行性


<details>
  <summary>Details</summary>
Motivation: 现有安全学习方法通常统一执行多个安全约束或采用固定优先级顺序，导致不可行性和脆弱行为。实际中安全需求是异质的，只存在部分优先级关系，一些约束可比而另一些不可比。

Method: 将安全约束形式化为偏序集结构，提出PoSafeNet可微分神经安全层，通过偏序一致约束顺序下的顺序闭式投影强制执行安全，自适应选择或混合有效安全执行，同时通过构造保留优先级语义。

Result: 在多障碍物导航、受限机器人操作和基于视觉的自动驾驶实验中，相比非结构化和基于可微分二次规划的安全层，显示出更好的可行性、鲁棒性和可扩展性。

Conclusion: 偏序集结构的安全形式化能够更好地处理异质安全约束，PoSafeNet通过自适应安全执行提高了学习控制器的实用性和安全性。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [92] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 量化分析LLM交互中礼貌用语（如"谢谢"）的能耗成本，揭示输入长度、输出长度和模型大小对能源使用的影响


<details>
  <summary>Details</summary>
Motivation: 随着LLM用户采用率增长和每日数十亿提示处理，看似无害的礼貌消息（如"谢谢"）会产生累积的能源成本，需要量化这些交互的能源足迹以实现可持续AI部署

Method: 使用真实世界对话轨迹和细粒度能源测量，分析输入长度、输出长度和模型大小对能源使用的影响，以礼貌用语作为可控且可复现的代理指标

Result: 量化了LLM交互的能源成本，发现礼貌消息确实消耗额外能源，为构建更可持续和高效的LLM应用提供了可操作的见解

Conclusion: 理解并减轻LLM交互的能源成本对于可持续AI部署至关重要，特别是在聊天等日益普及的实际应用场景中

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [93] [The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples](https://arxiv.org/abs/2601.22359)
*Hsiang Hsu,Pradeep Niroula,Zichang He,Ivan Brugere,Freddy Lecue,Chun-Fu Chen*

Main category: cs.LG

TL;DR: 论文提出机器遗忘中的残留知识风险，即遗忘样本的对抗扰动仍可被识别，并提出RURK微调策略来缓解此风险。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通过统计不可区分性保证遗忘效果，但这些保证不能自然扩展到对抗扰动输入时的模型输出。即使遗忘样本经过轻微扰动，仍可能被遗忘模型正确识别，而重新训练的模型却无法识别，这揭示了新的隐私风险：遗忘样本的信息可能在其局部邻域中持续存在。

Method: 提出RURK（Residual Knowledge Reduction）微调策略，通过惩罚模型重新识别扰动遗忘样本的能力来减少残留知识。该方法在深度神经网络视觉基准测试中进行实验验证。

Result: 实验表明，残留知识在现有遗忘方法中普遍存在，而RURK方法能有效防止残留知识，提高遗忘样本的隐私保护效果。

Conclusion: 机器遗忘中存在残留知识这一新型隐私风险，该风险在高维设置中不可避免。提出的RURK微调策略能有效缓解此风险，为机器遗忘提供更强的隐私保护保证。

Abstract: Machine unlearning offers a practical alternative to avoid full model re-training by approximately removing the influence of specific user data. While existing methods certify unlearning via statistical indistinguishability from re-trained models, these guarantees do not naturally extend to model outputs when inputs are adversarially perturbed. In particular, slight perturbations of forget samples may still be correctly recognized by the unlearned model - even when a re-trained model fails to do so - revealing a novel privacy risk: information about the forget samples may persist in their local neighborhood. In this work, we formalize this vulnerability as residual knowledge and show that it is inevitable in high-dimensional settings. To mitigate this risk, we propose a fine-tuning strategy, named RURK, that penalizes the model's ability to re-recognize perturbed forget samples. Experiments on vision benchmarks with deep neural networks demonstrate that residual knowledge is prevalent across existing unlearning methods and that our approach effectively prevents residual knowledge.

</details>


### [94] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 系统级设计选择（数值精度、批处理策略、请求调度）对LLM推理能耗有数量级影响，需关注服务栈编排以实现可持续部署


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生产环境中的部署增加，计算资源和能源需求从训练转向推理。现有研究主要关注单提示或单令牌的能耗，但系统级设计选择对能耗有巨大影响，需要深入研究以实现可持续的AI服务

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如Hugging Face的Text Generation Inference服务器）的影响

Result: 1) 低精度格式仅在计算受限场景下带来能耗收益；2) 批处理提高能效，尤其在解码等内存受限阶段；3) 结构化请求时序（到达整形）可将单请求能耗降低高达100倍

Conclusion: 可持续的LLM部署不仅取决于模型内部设计，还依赖于服务栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更环保的AI服务

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [95] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，使用表格基础模型进行零样本上下文贝叶斯推理，通过高保真度校正模型处理低保真度后验预测分布，在31个基准问题上优于7种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程方法在多保真度回归中面临数据极度不平衡问题，计算成本高（立方缩放），且容易对稀疏的高保真度观测过拟合，限制了实际应用中的效率和泛化能力。

Method: FIRE框架耦合表格基础模型，通过高保真度校正模型对低保真度模型的后验预测分布进行条件化，实现零样本上下文贝叶斯推理。这种跨保真度的信息传递通过分布摘要捕获异方差误差，无需模型重新训练即可实现鲁棒的残差学习。

Result: 在31个基准问题（包括合成和真实世界任务如DrivAerNet、LCBench）上，FIRE在性能-时间权衡方面优于7种最先进的高斯过程或深度学习多保真度回归方法，在准确性和不确定性量化方面排名最高，并具有运行时优势。

Conclusion: FIRE提供了一种有效的免训练多保真度回归方法，通过表格基础模型实现了优越的性能-时间权衡。局限性包括上下文窗口约束和对预训练表格基础模型质量的依赖。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [96] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO：一种基于语言模型的纯代理黑盒优化框架，用于生物设计任务，在分子设计和抗菌肽优化中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有生物设计方法主要依赖原始结构数据，难以充分利用丰富的科学文献知识。虽然大语言模型已被引入这些流程，但仅局限于结构中心优化器中的狭窄角色

Method: 提出PABLO（纯代理黑盒优化），一种分层代理系统，使用在化学和生物学文献上预训练的科学LLM来生成和迭代优化生物候选物，将生物黑盒优化完全转化为基于语言的推理过程

Result: 在GuacaMol分子设计和抗菌肽优化任务中达到最先进性能，显著提高了样本效率和最终目标值。在体外验证中，PABLO优化的肽对耐药病原体表现出强活性

Conclusion: PABLO的代理框架为实际设计提供了关键优势：自然地整合语义任务描述、检索增强的领域知识和复杂约束，展示了在治疗发现中的实际潜力

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [97] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: G-Substrate：一个图表示学习框架，将图结构作为跨模态和任务的持久化结构基底，通过统一结构模式和角色交替训练实现知识积累


<details>
  <summary>Details</summary>
Motivation: 当前图结构学习通常以模态和任务孤立的方式进行，每次任务都重新构建图表示，导致跨模态和任务的结构规律无法积累。需要一种能让图结构在异构模态和任务间持久化并积累的表示学习方法。

Method: 提出G-Substrate框架，包含两个核心机制：1）统一结构模式，确保跨异构模态和任务的图表示兼容性；2）角色交替训练策略，在训练过程中让相同的图结构暴露于多个功能角色。

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于任务孤立方法和朴素的多任务学习方法。

Conclusion: 通过将图结构视为跨学习情境的持久化结构基底，G-Substrate能够积累跨模态和任务的结构规律，为图表示学习提供了新的视角和有效框架。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [98] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种新颖的可扩展方法，通过估计数据似然性的得分函数来归因异常值的根本原因，使用积分梯度沿异常值到正常数据分布的路径累积得分贡献。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的方法在不确定性和高维依赖下难以有效识别异常值的根本原因，需要一种更稳健的方法来处理非线性、高维和异方差因果模型中的根因归因问题。

Method: SIREN通过估计数据似然性的得分函数，使用积分梯度计算归因，沿从异常值到正常数据分布的路径累积得分贡献。该方法满足三个经典Shapley值公理（虚拟性、效率性、线性性）以及从底层因果结构导出的不对称公理。

Result: 在合成随机图和真实世界云服务及供应链数据集上的大量实验表明，SIREN在归因准确性和计算效率方面均优于最先进的基线方法。

Conclusion: SIREN通过直接操作得分函数，为非线性、高维和异方差因果模型中的根因归因提供了可处理且具有不确定性感知的解决方案，在理论和实证上都表现出优越性能。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [99] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: 本文分析两层KANs的梯度下降训练，推导出训练动态、泛化和差分隐私的理论界限，证明在NTK可分假设下，多对数宽度足以实现优化和泛化收敛，并在隐私设置中揭示了非私有与私有训练之间的定性差距。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 作为标准MLPs的结构化替代方案出现，但对其训练动态、泛化和隐私特性的理论基础仍然有限。本文旨在填补这一空白，为两层KANs的梯度下降训练提供理论分析框架。

Method: 分析两层KANs的梯度下降训练，推导一般性理论界限。具体实例化到逻辑损失下的NTK可分假设场景，证明多对数网络宽度足以实现优化和泛化收敛。在隐私设置中，分析实现(ε,δ)-差分隐私所需的噪声，并推导效用界限。

Result: 在NTK可分假设下，多对数宽度足以使GD实现O(1/T)的优化速率和O(1/n)的泛化速率。在隐私设置中，获得O(√d/(nε))的效用界限，与一般凸Lipschitz问题的经典下界匹配。发现多对数宽度在差分隐私下不仅是充分的，而且是必要的，揭示了非私有与私有训练之间的定性差距。

Conclusion: 本文为KANs的训练动态、泛化和隐私特性提供了理论框架，揭示了私有与非私有训练之间的根本差异，并为实际应用（如网络宽度选择和早停）提供了理论指导。

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [100] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了MM-OpenFGL，这是首个针对多模态联邦图学习的综合基准，包含19个多模态数据集、8种模拟策略、6个下游任务和57种先进方法，系统研究了MMFGL的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的多模态属性图通常分布在隔离平台上，由于隐私或商业限制无法共享，而现有联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习的系统研究和评估基准。

Method: 提出了MM-OpenFGL基准，系统形式化了MMFGL范式，包含19个多模态数据集（覆盖7个应用领域）、8种模拟策略（捕捉模态和拓扑变化）、6个下游任务，并通过模块化API实现了57种先进方法。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个角度全面研究了MMFGL，为未来多模态联邦图学习研究提供了有价值的见解和系统评估框架。

Conclusion: MM-OpenFGL填补了多模态联邦图学习领域的空白，为系统评估和比较不同方法提供了首个综合基准，将推动该领域的研究进展。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [101] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个完全人工标注的机器学习排行榜数据集，它捕获所有实验结果以实现结果透明度，并包含额外元数据，如实验类型（基线、提出方法或变体）和明确分离的训练/测试数据集，用于更透明和细致的ML评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域中的排行榜对于基准测试和跟踪进展至关重要，但传统创建方法需要大量人工努力。现有的自动化排行榜生成数据集存在局限：只捕获每篇论文的最佳结果且元数据有限，无法支持透明和细致的评估。

Method: 提出MetaLead数据集，这是一个完全人工标注的ML排行榜数据集。它捕获所有实验结果（而不仅是最佳结果），包含丰富的元数据如实验类型分类（基线方法、提出方法或变体方法），并明确分离训练和测试数据集以支持跨领域评估。

Result: MetaLead提供了更透明和细致的评估资源，支持实验类型指导的比较（如基线vs提出方法vs变体），并支持跨领域评估。其丰富结构使其成为ML研究中更强大、更透明的评估工具。

Conclusion: MetaLead通过捕获所有实验结果和包含额外元数据，解决了现有排行榜数据集的局限性，为机器学习研究提供了更透明、更细致的评估框架，支持实验类型指导的比较和跨领域评估。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [102] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CoDCL是一个动态网络学习框架，结合反事实数据增强和对比学习来提高模型对网络结构变化的鲁棒性，可作为即插即用模块集成到现有时序图模型中。


<details>
  <summary>Details</summary>
Motivation: 动态网络的快速增长和持续结构演化使得预测任务越来越困难，模型需要能够适应复杂时序环境并对新兴结构变化具有鲁棒性。

Method: 提出CoDCL框架，结合反事实数据增强和对比学习；设计综合策略生成高质量反事实数据，包括动态处理设计和高效结构邻域探索来量化交互模式的时序变化；整个框架设计为即插即用通用模块。

Result: 在多个真实世界数据集上的广泛实验表明，CoDCL显著提升了动态网络领域最先进基线模型的性能。

Conclusion: 将反事实数据增强整合到动态表示学习中具有关键作用，CoDCL框架能有效提高模型对动态网络结构变化的适应能力。

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [103] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种用于LLM推理的显式对比学习方法，替代GRPO的优势估计方法，通过将结果分为正负集并最大化正结果概率来提升推理能力


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但其依赖优势估计和需要经验性调整（如非对称裁剪和零方差数据过滤），这些改进需要大量经验洞察且难以识别，因此需要更直接的方法

Method: 提出显式对比学习方法：将K个结果分为正负集合，然后最大化正结果的似然概率。该方法可视为LLM推理中（多标签）噪声对比估计的在线实例化

Result: 在具有挑战性的数学基准测试套件上展示了与DAPO和在线DPO等强基线相比具有竞争力的性能

Conclusion: 提出的显式对比学习方法为LLM推理提供了一种有效的替代方案，避免了GRPO中需要经验性调整的复杂性，同时保持了竞争性的性能表现

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [104] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 扩散模型即使在不匹配或低保真度训练数据下，也能作为有效的逆问题先验，其成功取决于测量信息的充分性


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型作为逆问题先验时，即使训练数据与目标信号不匹配（如用卧室数据恢复人脸），为何仍能表现良好，探索其稳健性的条件和原因

Method: 通过大量实验分析弱先验在不同测量条件下的表现，并基于贝叶斯一致性理论，给出高维测量使后验集中于真实信号的条件

Result: 发现弱先验在测量信息充分时（如观测像素多）表现良好，识别了其失效的机制，理论分析提供了弱扩散先验可靠使用的原则性依据

Conclusion: 扩散模型作为逆问题先验时，即使训练数据不匹配，只要测量信息足够充分，仍能有效恢复信号，这为实际应用中弱先验的使用提供了理论支持

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [105] [Automating Forecasting Question Generation and Resolution for AI Evaluation](https://arxiv.org/abs/2601.22444)
*Nikos I. Bosse,Peter Mühlbacher,Jack Wildman,Lawrence Phillips,Dan Schwarz*

Main category: cs.LG

TL;DR: 开发了一个基于LLM网络研究代理的自动化系统，用于大规模生成和解决高质量预测问题，在多样性和准确性方面超越了人工平台


<details>
  <summary>Details</summary>
Motivation: 预测未来事件对决策制定和衡量通用智能至关重要，但传统方法依赖重复数据源，限制了问题的多样性和实用性，需要自动化系统来生成和解决大量多样化、困难的预测问题

Method: 使用LLM驱动的网络研究代理自动生成和解决预测问题，系统能够大规模生成多样化、真实世界的预测问题，并在几个月后自动解决这些问题

Result: 系统生成了1499个多样化预测问题，可验证、无歧义问题比例约96%（超过Metaculus平台），问题解决准确率约95%，更智能的LLM预测性能更好（Gemini 3 Pro Brier得分0.134），问题分解策略显著改善了预测性能（0.132 vs 0.141）

Conclusion: LLM驱动的自动化系统能够高效生成和解决高质量预测问题，超越了人工平台，为AI预测能力的评估和改进提供了有效工具，展示了在预测任务中的实用价值

Abstract: Forecasting future events is highly valuable in decision-making and is a robust measure of general intelligence. As forecasting is probabilistic, developing and evaluating AI forecasters requires generating large numbers of diverse and difficult questions, and accurately resolving them. Previous efforts to automate this laborious work relied on recurring data sources (e.g., weather, stocks), limiting diversity and utility. In this work, we present a system for generating and resolving high-quality forecasting questions automatically and at scale using LLM-powered web research agents. We use this system to generate 1499 diverse, real-world forecasting questions, and to resolve them several months later. We estimate that our system produces verifiable, unambiguous questions approximately 96% of the time, exceeding the rate of Metaculus, a leading human-curated forecasting platform. We also find that our system resolves questions at approximately 95% accuracy. We verify that forecasting agents powered by more intelligent LLMs perform better on these questions (Brier score of 0.134 for Gemini 3 Pro, 0.149 for GPT-5, and 0.179 for Gemini 2.5 Flash). Finally, we demonstrate how our system can be leveraged to directly improve forecasting, by evaluating a question decomposition strategy on a generated question set, yielding a significant improvement in Brier scores (0.132 vs. 0.141).

</details>


### [106] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 论文提出了一种基于权重的稀疏自编码器特征解释框架，通过直接分析权重交互来理解特征功能，无需激活数据，揭示了特征在注意力机制中的深度依赖结构和语义/非语义特征的分布差异。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAE）的特征解释方法主要依赖激活模式推断语义，但忽视了特征训练的目标是重建在前向传播中具有计算功能的激活。现有方法缺少对特征功能角色的直接分析。

Method: 引入基于权重的解释框架，通过直接分析权重交互来测量特征的功能效应，无需激活数据。在Gemma-2和Llama-3.1模型上进行了三个实验：1）分析特征直接预测输出token的能力；2）研究特征在注意力机制中的参与情况；3）比较语义和非语义特征在注意力电路中的分布特征。

Result: 实验发现：1）约1/4的特征能直接预测输出token；2）特征积极参与注意力机制，且具有深度依赖的结构；3）语义和非语义特征在注意力电路中表现出不同的分布特征。

Conclusion: 该权重解释框架提供了SAE特征可解释性的另一半（上下文外分析），弥补了现有激活模式解释方法的不足，为理解语言模型内部表示的功能角色提供了新视角。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [107] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于RLVR训练的高效采样方法，通过堆采样和在线查询增强来优化提示池管理，减少计算成本的同时保持性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化提示池，使用堆采样跟踪能力边界，通过轻量级异步验证进行在线查询增强，并通过拓扑感知统计重估计和受控重插入来稳定相关查询

Result: 在两个训练语料库、两种训练方法和七个基准测试中，HeaPA持续提高准确性，以更少的计算达到目标性能，同时保持实际时间相当，且模型规模越大收益越明显

Conclusion: HeaPA通过边界聚焦采样和在线池增长机制有效提升RLVR训练效率，特别适合大规模模型训练，代码已开源

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [108] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文研究了掩码扩散语言模型在k-奇偶性问题上的泛化特性，发现其能避免自回归模型的"顿悟"现象，实现快速同时泛化，并通过优化掩码概率分布提升了大规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型作为强大的生成范式，其泛化特性相比自回归模型研究不足。本文旨在探究其在k-奇偶性问题上的学习行为，特别是与神经网络常见的"顿悟"现象（长时间随机性能后突然泛化）的对比。

Method: 1. 理论分析：将掩码扩散目标分解为驱动特征学习的信号机制和作为隐式正则化的噪声机制；2. 实验验证：在k-奇偶性问题上使用掩码扩散目标训练nanoGPT模型；3. 优化方法：基于理论洞察优化掩码概率分布，应用于5000万参数和80亿参数模型。

Result: 1. 掩码扩散目标改变了学习景观，实现了快速同时泛化，避免了"顿悟"现象；2. 优化掩码概率分布的方法显著提升了5000万参数模型的困惑度；3. 在80亿参数模型上，从头预训练和监督微调分别获得了8.8%和5.8%的性能提升，证明了方法的可扩展性和有效性。

Conclusion: 掩码扩散语言模型在k-奇偶性问题上展现出与自回归模型不同的泛化特性，能够避免"顿悟"现象。通过理论分解和优化掩码概率分布，可以显著提升大规模掩码扩散语言模型的性能，为大规模语言模型训练提供了有效的框架。

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [109] [Temporal Graph Pattern Machine](https://arxiv.org/abs/2601.22454)
*Yijun Ma,Zehong Wang,Weixiang Sun,Yanfang Ye*

Main category: cs.LG

TL;DR: TGPM是一个时间图基础框架，通过时间偏置随机游走生成交互补丁，使用Transformer捕获全局时间规律，并通过自监督预训练学习网络演化的基本法则，在链接预测任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间图学习方法大多是任务导向的，依赖于短期依赖建模、静态邻域语义和回顾性时间使用等限制性假设，这些约束阻碍了可迁移时间演化机制的发现。

Method: 提出时间图模式机器(TGPM)：1) 将每个交互视为通过时间偏置随机游走合成的交互补丁，捕获多尺度结构语义和长程依赖；2) 使用Transformer骨干网络捕获全局时间规律并适应上下文特定的交互动态；3) 引入掩码标记建模和下一时间预测等自监督预训练任务，显式编码网络演化的基本法则。

Result: 在转导式和归纳式链接预测任务中，TGPM始终实现最先进的性能，并展现出卓越的跨领域可迁移性。

Conclusion: TGPM通过直接学习广义演化模式，克服了现有方法的限制，为时间图学习提供了一个强大的基础框架，能够发现可迁移的时间演化机制。

Abstract: Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.

</details>


### [110] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: LOFT提出了一种基于低维特征子空间的机器遗忘新方法，通过在预训练模型中优化小型投影矩阵，实现高效、隐私保护的遗忘学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在两大关键问题：1）需要大量重新加载原始数据，存在隐私泄露风险；2）需要更新整个预训练模型，效率低下。需要一种更高效、更隐私保护的遗忘方法。

Method: LOFT方法在预训练模型的低维特征子空间中进行遗忘学习，通过主成分投影优化小型投影矩阵，最大化保留剩余数据信息同时最小化遗忘数据信息。只需一次性从预训练骨干网络获取特征，无需重复访问原始数据。

Result: 实验验证LOFT在多种模型、数据集、任务和应用中具有显著更低的计算开销和优越的遗忘性能，同时有效保护隐私。

Conclusion: LOFT提供了一种高效、隐私保护的机器遗忘新范式，通过低维特征子空间分离剩余数据和遗忘数据，解决了传统方法的数据隐私泄露和计算效率问题。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [111] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: EvoEGF-Mol：一种基于信息几何的SBDD方法，通过指数测地线流生成分子，避免了传统方法在欧几里得和概率空间中的不匹配问题


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法在欧几里得空间和概率空间中分别构建概率路径，与底层统计流形不匹配。本文从信息几何角度出发，解决这一根本性问题。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下沿指数测地线定义生成流。为避免直接以狄拉克分布为目标导致的轨迹崩溃，提出EvoEGF-Mol，用动态集中分布替代静态狄拉克目标，通过渐进参数精化架构确保稳定训练。

Result: 在CrossDock上达到93.4%的PoseBusters通过率，表现出卓越的几何精度和相互作用保真度。在MolGenBench任务中超越基线，能够恢复生物活性骨架并生成符合MedChem过滤器的候选分子。

Conclusion: EvoEGF-Mol从信息几何角度解决了SBDD中的流形不匹配问题，通过指数测地线流实现了高质量的分子生成，在几何精度和化学可行性方面均表现出色。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [112] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs在无奖励探索阶段表现出潜在学习动态，这种两阶段训练（先无奖励探索后有奖励学习）比纯奖励强化学习获得更高能力


<details>
  <summary>Details</summary>
Motivation: 心理学中的潜在学习现象（生物体在无奖励时学习环境表征）在LLMs中尚未被探索，而当前LLMs主要依赖奖励驱动的强化学习范式，限制了灵活性和泛化能力

Method: 采用两阶段训练范式：1）无奖励探索阶段，LLMs组织任务相关知识；2）有奖励学习阶段。在多个模型家族和多样化任务领域进行广泛实验，并提供理论分析解释无奖励探索的性能增益机制

Result: LLMs在无奖励探索阶段表现出性能适度提升，引入奖励后性能进一步增强；采用两阶段探索机制后训练的LLMs最终比纯奖励强化学习训练的模型获得更高能力

Conclusion: LLMs确实表现出心理学中的潜在学习动态，无奖励探索有助于组织任务相关知识而不受奖励偏差约束，这种两阶段训练范式为LLMs训练提供了新的启示

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [113] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出一个教师-学生框架，将持续强化学习解耦为两个独立过程：通过分布式RL训练单任务教师模型，并持续蒸馏到中央通用模型中，结合MoE架构和回放方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习面临稳定性-可塑性困境，现有方法难以在连续任务流中实现可扩展性能。观察到RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程，更适合与大型基础模型和多任务学习结合。

Method: 1) 使用分布式RL训练单任务教师模型；2) 通过持续策略蒸馏将教师模型知识转移到中央通用学生模型；3) 采用混合专家(MoE)架构增强可塑性；4) 使用基于回放的方法提升稳定性。

Result: 在Meta-World基准测试中，框架能够恢复超过85%的教师性能，同时将任务间遗忘控制在10%以内，实现了高效的持续强化学习。

Conclusion: 通过将CRL解耦为单任务RL训练和持续蒸馏两个独立过程，结合MoE和回放机制，有效解决了持续强化学习中的稳定性-可塑性困境，实现了可扩展的终身学习性能。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [114] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等价的问题变体并跨组池化奖励，解决了GRPO中的多样性崩溃和梯度消失问题，在数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个token预测的大语言模型本质上是模式匹配器，对表面措辞变化敏感。GRPO虽然旨在改进推理，但存在两个失败模式：多样性崩溃（训练放大单一解决方案策略）和梯度消失（大量问题产生零梯度）。

Method: 提出TA-GRPO（Transform-Augmented GRPO），为每个问题生成语义等价的变体（通过改写、变量重命名和格式更改），并通过池化整个组的奖励来计算优势。这种池化计算确保即使原始问题太简单或太难也能获得混合奖励，同时在多样化措辞上训练促进多种解决方案策略。

Result: 在数学推理基准测试中显示一致的Pass@k改进，在竞赛数学（AMC12, AIME24）上提升高达9.84分，在分布外科学推理（GPQA-Diamond）上提升5.05分。

Conclusion: TA-GRPO通过减少零梯度概率和通过降低训练-测试分布偏移来改善泛化，为GRPO的失败模式提供了有效的解决方案，显著提升了推理性能。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [115] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态的L2距离尖峰来检测认知转折点，利用几何轨迹分析诊断推理结构问题，并注入状态感知语言提示来实时引导大型推理模型，无需额外微调即可优化推理过程。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过扩展测试时计算取得了显著性能，但经常遭受认知惯性的困扰，表现为过度思考（运动惯性）或推理僵化（方向惯性）。现有检测方法通常依赖表面文本启发式（如自我纠正标记），难以捕捉模型未表达的内部冲突。

Method: 提出STARS（尖峰触发自适应推理引导）框架：1）通过检测隐藏状态中的L2距离尖峰来识别认知转折点；2）使用几何轨迹分析诊断过渡的结构性质；3）注入状态感知语言提示来实时引导模型。这是一个无需训练的方法。

Result: 在多样化基准测试中的实验证实，STARS能有效减少冗余循环，同时通过自适应纠正错误轨迹来提高准确性。该框架为优化大型推理模型的推理过程提供了鲁棒的无监督机制。

Conclusion: STARS提供了一个无需额外微调的鲁棒无监督机制，能够通过监测潜在动态来纠正认知惯性，优化大型推理模型的推理过程，在减少冗余计算的同时提高准确性。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [116] [Elastic Spectral State Space Models for Budgeted Inference](https://arxiv.org/abs/2601.22488)
*Dachuan Song,Xuan Wang*

Main category: cs.LG

TL;DR: ES-SSM是一种弹性谱状态空间模型，只需一次全容量训练即可在运行时按任意规模截断以适应不同计算预算，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 基础模型通常在固定计算容量下训练，而实际应用需要在不同资源约束的平台部署。现有方法需要训练多个模型变体或进行模型蒸馏，这需要额外训练且仅支持预选尺寸，无法在运行时进行细粒度调整。

Method: 基于Hankel谱滤波的状态空间模型，结合轻量级输入自适应门，在随机谱预算下训练。使用共享掩码归一化规则对有序谱通道进行处理，使预测能力集中在低索引组件，高索引组件主要起细化作用。

Result: 在文本、逻辑、检索、视觉和音频的长序列基准测试中，单个ES-SSM模型经过一次训练后，截断到不同参数规模时能提供与Transformer和SSM基线相当的竞争性能。在不同运行时预算下，观察到在广泛截断水平上平滑稳定的预算-性能曲线。

Conclusion: ES-SSM通过谱分解和自适应门控机制，实现了单次训练、多尺度部署的弹性模型架构，为资源受限环境下的模型部署提供了高效解决方案。

Abstract: Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.

</details>


### [117] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出GFT框架，通过温度控制平滑过渡预训练和目标漂移，用于流匹配模型的微调，提高收敛稳定性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或效率要求严格的情况下，传统微调会损害预训练获得的准确性和效率优势。现有奖励微调方法对漂移结构或训练技术有限制。

Method: 提出渐进微调(GFT)框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移。支持使用最优传输等耦合方法。

Result: GFT提高了收敛稳定性，缩短了概率路径，实现了更快推理，同时保持与标准微调相当的生成质量。

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论支撑和实际有效的替代方案。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [118] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 论文提出在离线目标条件强化学习中，基于价值函数学习的目标表示可能无法区分需要不同动作的目标状态，从而影响控制性能。作者引入动作充分性概念，证明其比价值充分性更重要，并展示基于演员策略学习的目标表示优于基于价值估计的方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层策略在离线目标条件强化学习中通常通过价值函数学习目标表示，隐含假设保留价值估计所需信息就足以实现最优控制。作者发现这种假设可能失败，因为价值充分的表示可能无法区分需要不同动作的目标状态。

Method: 提出信息论框架定义动作充分性条件，证明价值充分性不蕴含动作充分性。通过实验验证动作充分性与控制成功更相关，并展示标准对数损失训练的低层策略自然诱导动作充分的表示。

Result: 在流行基准测试中，基于演员策略学习的目标表示始终优于基于价值估计学习的目标表示，验证了动作充分性比价值充分性对控制性能更重要。

Conclusion: 在分层离线目标条件强化学习中，应该使用基于演员策略学习的目标表示而非基于价值估计的表示，因为动作充分性是实现最优控制的关键条件，而价值充分性可能不足。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [119] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: 提出DREE框架解决VRP神经网络求解器在连续漂移任务下的终身学习问题，通过双重回放和经验增强提高学习效率并减轻灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有VRP神经网络求解器要么在固定任务集上一次性训练，要么在顺序到达的多个任务上终身学习，但都假设每个任务有充分训练资源。这忽略了现实世界问题模式会随时间连续漂移的特性，即任务大量顺序出现但每个任务只有有限训练资源。

Method: 提出DREE（Dual Replay with Experience Enhancement）框架，包含双重回放机制和经验增强策略，在连续漂移任务下提高学习效率并缓解灾难性遗忘。该框架可应用于多种现有神经求解器。

Result: 实验表明在连续漂移环境下，DREE能有效学习新任务、保留先前知识、提高对未见任务的泛化能力，且适用于多种现有神经求解器。

Conclusion: DREE为解决VRP神经网络求解器在连续漂移任务下的终身学习问题提供了一个有效框架，能应对现实世界中任务模式持续变化且训练资源有限的挑战。

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [120] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究发现Transformer模型学习技能组合的方式与人类不同，不是按顺序构建而是经常反向或并行学习，导致分布偏移时出现意外错误，这种现象称为"破碎组合性"。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现意外错误或非预期行为，虽然已有研究揭示LLMs与人类在技能组合上的差异，但技能组合的学习动态及其非人类行为的根本原因仍不清楚。

Method: 通过在合成算术任务上训练Transformer模型，进行广泛的消融实验和细粒度诊断指标分析，研究学习动态机制。

Result: 发现Transformer不是按照人类般的顺序规则可靠地构建技能组合，而是经常以反向或并行方式获取技能，导致分布偏移时出现意外混合错误。证据表明是训练数据的相关性匹配而非因果或程序性组合塑造了学习动态。

Conclusion: 破碎组合性在现代LLMs中持续存在，无法通过纯模型缩放或基于草稿的推理缓解。研究揭示了模型学习行为与期望技能组合之间的根本不匹配，对推理可靠性、分布外鲁棒性和对齐具有重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [121] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种无人机辅助可见光通信系统中的三维轨迹规划方法，通过优化飞行高度和水平轨迹来最小化飞行距离，从而提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为提供灵活通信和高效照明提供了有前景的解决方案。然而，在无人机辅助的VLC系统中，如何规划三维轨迹以最小化飞行距离并提高数据收集效率是一个关键挑战。

Method: 首先推导了在特定VLC信道增益阈值下的闭式最优飞行高度。然后，通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合，优化无人机的水平轨迹，使其能够在复杂环境中实现自适应运动策略。

Result: 仿真结果表明，推导的最优高度相比基线方法可减少高达35%的飞行距离。此外，提出的奖励机制显著缩短了约50%的收敛步数，在无人机辅助VLC数据收集方面表现出显著的效率提升。

Conclusion: 该研究为无人机辅助可见光通信系统提供了一种有效的三维轨迹规划框架，通过优化飞行高度和水平轨迹，显著提高了数据收集效率，为未来智能通信系统设计提供了有价值的参考。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [122] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: SCOPE-PD是一个可解释的AI预测框架，通过整合主观和客观评估来提供个性化的帕金森病诊断决策，使用随机森林算法达到98.66%的准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病是一种复杂的神经退行性疾病，传统诊断方法存在主观性问题，导致诊断延迟。现有机器学习方法通常只依赖主观报告，缺乏可解释性和个性化风险评估能力。

Method: 提出SCOPE-PD框架，整合主观和客观临床评估数据（来自PPMI研究），应用多种机器学习技术，选择最佳模型，并使用SHAP分析进行结果解释。

Result: 随机森林算法在结合主观和客观测试特征时达到最高准确率98.66%。通过SHAP分析识别出震颤、运动迟缓和面部表情是MDS-UPDRS测试中对PD预测贡献最大的三个特征。

Conclusion: SCOPE-PD框架通过整合多模态数据和可解释AI技术，能够提供更准确、个性化的帕金森病预测，有助于早期诊断和临床决策支持。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [123] [Variational Bayesian Flow Network for Graph Generation](https://arxiv.org/abs/2601.22524)
*Yida Xiong,Jiameng Chen,Xiuwen Gong,Jia Wu,Shirui Pan,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出Variational Bayesian Flow Network (VBFN)，通过变分提升到结构化精度的高斯变分信念族，解决图生成中节点-边耦合问题，提高生成质量


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型采用因子化的前向噪声，流匹配方法从因子化参考噪声开始，节点-边耦合未编码在生成几何中，需要在离散解码后由核心网络隐式恢复，这很脆弱。传统贝叶斯流网络依赖因子化信念和独立通道，限制了几何证据融合。

Method: 提出VBFN，进行变分提升到可处理的联合高斯变分信念族，由结构化精度控制。每个贝叶斯更新简化为求解对称正定线性系统，实现节点和边在单个融合步骤中的耦合更新。从表示诱导的依赖图构建样本无关的稀疏精度，避免标签泄漏同时强制节点-边一致性。

Result: 在合成和分子图数据集上，VBFN提高了保真度和多样性，超越了基线方法。

Conclusion: VBFN通过变分贝叶斯流网络框架，有效解决了图生成中的节点-边耦合问题，提供了一种更稳健的图生成方法。

Abstract: Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.

</details>


### [124] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: 提出REKD方法，通过知识蒸馏让小型学生模型从教师模型的rationales和预测中学习，提升rationale extraction模型的预测性能


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键领域广泛应用，但可解释性需求日益增长。Rationale extraction（RE）通过select-predict架构提供可解释性，但小型神经网络搜索特征组合空间困难，预测性能受限

Method: 提出REKD（Rationale Extraction with Knowledge Distillation），让学生RE模型不仅从自身优化学习，还从教师模型（rationalist）的rationales和预测中学习。该方法与神经网络模型无关，任何黑盒神经网络都可作为骨干模型

Result: 在语言和视觉分类数据集（IMDB电影评论、CIFAR 10和CIFAR 100）上实验，使用BERT和ViT变体，REKD显著提升了学生RE模型的预测性能

Conclusion: REKD通过知识蒸馏有效提升了小型rationale extraction模型的性能，该方法与模型无关，可应用于各种神经网络架构，为可解释AI提供了有效解决方案

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [125] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文通过构建简约基线来解耦强化学习微调中的设计选择，分析各因素对学习和泛化的边际贡献，识别关键设计要素。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调领域存在大量设计选择优化研究，但性能提升往往缺乏一致性结论，难以确定各设计选择的具体作用和关键要素，导致进展虚幻。

Method: 构建简约基线（每轮查询一次rollout、使用结果奖励作为训练信号、无优势技巧、批量大小32），将其与批量上下文bandit学习关联，设计实验流程分析优势、rollout数量等因素的边际增益。

Result: 在三个基础模型和两个数据集上的实验揭示了各种设计选择对学习和泛化动态的新理解，并识别出值得更多关注的关键设计要素。

Conclusion: 通过解耦设计选择并分析其边际贡献，为强化学习微调提供了更清晰的设计指导，有助于避免虚幻进展并聚焦关键改进方向。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [126] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出L2D-SLDS模型用于非平稳时间序列的延迟学习，通过因子化切换线性高斯状态空间模型建模专家残差，支持专家动态注册，并基于一步预测提出信息导向的路由规则。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中部分反馈和专家可用性随时间变化的问题，传统方法难以处理专家动态变化和跨专家信息共享。

Method: 使用L2D-SLDS模型：因子化切换线性高斯状态空间模型，包含上下文相关的状态转换、共享全局因子实现跨专家信息传递、专家特定状态；支持专家动态注册；基于一步预测信念提出信息导向的路由规则。

Result: 实验显示该方法优于上下文多臂老虎机基线和无共享因子消融实验，在非平稳时间序列环境中表现更好。

Conclusion: L2D-SLDS模型能有效处理非平稳时间序列的延迟学习问题，通过共享因子实现跨专家信息传递，动态专家注册机制增强了系统灵活性，信息导向路由规则提升了决策质量。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [127] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，包含模型导向、模型自由和情景控制三个模块，用于大规模统计机器学习中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 人类通过多个相互作用的神经系统（模型导向规划、模型自由习惯、情景记忆学习）实现高效学习。本文旨在将这些生物学效率的计算原理转化为可扩展贝叶斯推断的采样算法。

Method: 提出包含三个模块的采样算法：1）模型导向模块使用目标分布进行引导但计算缓慢的采样；2）模型自由模块利用先前样本学习参数空间模式，实现快速反射式采样；3）情景控制模块通过回忆特定过去事件（样本）支持快速采样。

Result: 该方法推进了贝叶斯方法，促进其在大规模统计机器学习问题中的应用，特别是在贝叶斯深度学习中进行适当和原则性的不确定性量化。

Conclusion: 受人类多系统学习机制启发的三模块采样算法为大规模贝叶斯推断提供了高效解决方案，特别适用于需要不确定性量化的深度学习应用。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [128] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出守恒量校正方法，通过融入物理守恒准则提升神经算子的长期预测稳定性


<details>
  <summary>Details</summary>
Motivation: 深度学习用于PDE数值解时存在长期预测误差累积问题，且模型无法保持物理守恒量

Method: 提出模型无关的守恒量校正技术，将物理守恒准则融入深度学习模型

Result: 该方法能一致提升自回归神经算子模型的长期稳定性，且与模型架构无关

Conclusion: 当前架构在高频分量处理上存在局限，未来需设计能更好处理高频分量的架构，这对湍流建模至关重要

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [129] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2601.22563)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出EUGens层，一种高效、统一、通用的稠密层，通过随机特征和输入范数依赖，将推理复杂度从二次降到线性，减少参数和计算开销，同时保持表达能力。


<details>
  <summary>Details</summary>
Motivation: 全连接前馈层（FFLs）在神经网络中引入计算和参数瓶颈，限制了模型在实时应用和资源受限环境中的可扩展性。需要更高效的层设计来替代标准FFLs。

Method: 提出EUGens层，利用随机特征逼近标准FFLs，并引入输入范数依赖。该方法统一了现有的高效FFL扩展，将推理复杂度从O(n²)降到O(n)。还提出了无需反向传播的层间知识转移技术。

Result: 在Transformer和MLP中集成EUGens，在图像分类、语言模型预训练和3D场景重建等任务上，推理速度提升高达27%，内存效率提升高达30%。

Conclusion: EUGens为大规模神经网络在现实场景中的可扩展部署提供了潜力，通过显著减少计算和内存开销，同时保持模型表达能力。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [130] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis：首个利用因果解缠进行联邦时空预测的框架，通过双分支设计分离客户端特定因素和全局模式，解决非IID交通数据问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理去中心化交通数据的非独立同分布特性时表现不佳，通常将全局共享模式与客户端特定局部动态纠缠在单一表示中。作者认为这种异质性源于两个不同生成源的纠缠：客户端特定局部动态和跨客户端全局时空模式

Method: FedDis采用双分支架构：个性化银行学习捕获客户端特定因素，全局模式银行提取共同知识。通过互信息最小化目标强制两个分支之间的信息正交性，确保有效的解缠

Result: 在四个真实世界基准数据集上的综合实验表明，FedDis始终实现最先进的性能，具有高效的执行效率和优越的可扩展性

Conclusion: FedDis是首个利用因果解缠进行联邦时空预测的框架，通过分离客户端特定因素和全局模式，实现了鲁棒的跨客户端知识转移，同时保持对独特本地环境的高度适应性

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [131] [Non-Intrusive Graph-Based Bot Detection for E-Commerce Using Inductive Graph Neural Networks](https://arxiv.org/abs/2601.22579)
*Sichen Zhao,Zhiming Xue,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.LG

TL;DR: 提出基于图神经网络的非侵入式电商恶意机器人检测框架，通过图表示用户会话行为，实现准确识别自动化活动


<details>
  <summary>Details</summary>
Motivation: 传统机器人检测技术（如IP黑名单、CAPTCHA）对现代使用代理、僵尸网络和AI规避策略的机器人越来越无效或侵入性强，电商平台面临数据爬取、库存囤积和欺诈等日益增长的威胁

Method: 提出非侵入式图基机器人检测框架：1）将用户会话行为建模为图表示；2）应用归纳图神经网络进行分类；3）捕获关系结构和行为语义，识别特征方法难以检测的细微自动化活动

Result: 在真实电商流量实验中，提出的归纳图模型在AUC和F1分数上优于会话级多层感知器基线；对抗扰动和冷启动模拟显示模型在适度图修改下保持鲁棒，并能有效泛化到未见过的会话和URL

Conclusion: 该框架部署友好，无需客户端检测即可与现有系统集成，支持实时推理和增量更新，适合实际电商安全部署

Abstract: Malicious bots pose a growing threat to e-commerce platforms by scraping data, hoarding inventory, and perpetrating fraud. Traditional bot mitigation techniques, including IP blacklists and CAPTCHA-based challenges, are increasingly ineffective or intrusive, as modern bots leverage proxies, botnets, and AI-assisted evasion strategies. This work proposes a non-intrusive graph-based bot detection framework for e-commerce that models user session behavior through a graph representation and applies an inductive graph neural network for classification. The approach captures both relational structure and behavioral semantics, enabling accurate identification of subtle automated activity that evades feature-based methods. Experiments on real-world e-commerce traffic demonstrate that the proposed inductive graph model outperforms a strong session-level multilayer perceptron baseline in terms of AUC and F1 score. Additional adversarial perturbation and cold-start simulations show that the model remains robust under moderate graph modifications and generalizes effectively to previously unseen sessions and URLs. The proposed framework is deployment-friendly, integrates with existing systems without client-side instrumentation, and supports real-time inference and incremental updates, making it suitable for practical e-commerce security deployments.

</details>


### [132] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: MC-GRPO使用中位数基线替代均值基线，解决小样本训练中噪声导致的优势符号翻转问题，提升低样本量下的训练稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练场景中，基于组相对策略优化的方法常因共享均值基线的噪声导致优势符号翻转，使部分样本的更新方向错误，从而降低训练准确性。

Method: 提出中位数中心化组相对策略优化（MC-GRPO），用中位数基线替代均值基线，中位数对异常奖励值更不敏感。生成G+1个样本，使用组中位数计算优势值，排除中位数样本（零优势）的反向传播，保持每次提示的梯度贡献样本数为G。

Result: 在各种GRPO系列方法、不同模型和规模下，中位数中心化训练在低样本量场景中显著提升稳定性和最终准确性，将G=2和G=8之间的性能差距缩小到1%以内。

Conclusion: MC-GRPO通过简单有效的中位数基线替换，解决了小样本训练中的优势符号翻转问题，在保持核心计算成本不变的情况下，显著提升了资源受限场景下的训练性能。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [133] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: FedCARE是一个联邦遗忘学习框架，通过冲突感知遗忘和抗重学习恢复机制，在保持模型效用的同时高效移除特定数据影响，支持客户端、实例和类别级别的遗忘。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要遵守"被遗忘权"等隐私法规，但现有联邦遗忘方法存在遗忘开销大、效用下降、知识纠缠和意外重学习等问题，需要更高效的解决方案。

Method: FedCARE采用梯度上升进行高效遗忘，使用无数据模型反演构建类别级知识代理，结合伪样本生成器、冲突感知投影梯度上升和抑制回滚的恢复策略。

Result: 在多个数据集和模型架构上的实验表明，FedCARE相比现有方法能实现有效遗忘、更好的效用保持和更低的重学习风险，支持多种遗忘级别且开销适中。

Conclusion: FedCARE提供了一个统一、低开销的联邦遗忘框架，解决了现有方法的局限性，在隐私合规和模型效用之间取得了良好平衡。

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [134] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个用于多图学习的统一框架，通过图Transformer编码器将不同图映射到共享潜在空间，构建跨图元图进行联合推理，在保持可解释性的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 多图学习面临的主要挑战是如何有效整合具有不同拓扑结构、尺度和语义的异构图信息，特别是在缺乏共享节点标识的情况下。现有方法难以统一处理这些异构性，同时保持可解释性。

Method: MGMT首先使用图Transformer编码器将每个图的结构和属性映射到共享潜在空间；然后通过注意力机制选择任务相关的超节点，基于潜在空间相似性构建连接跨图功能对齐超节点的元图；最后在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用中，MGMT在图级预测任务中始终优于现有最先进模型，同时提供可解释的表示，有助于科学发现。

Conclusion: MGMT为结构化多图学习建立了一个统一框架，在图数据发挥核心作用的领域中推进了表示技术，提供了可扩展且可解释的跨图学习方法。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [135] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 论文提出Lethe方法解决联邦学习中持续训练导致已删除知识重新出现的问题，通过解耦待删除与待保留知识实现持久擦除


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习遗忘研究通常假设遗忘操作后协作结束，忽视了后续持续训练的情况。研究发现持续训练会重新激活已删除知识，导致知识重现问题

Method: 提出Lethe方法，采用Reshape-Rectify-Restore流程：首先在遗忘数据上训练临时适配器获得放大更新，然后将其作为校正信号在两个流中进行层级修正，最后移除适配器并在保留数据上进行短期恢复

Result: Lethe能以统一方式支持联邦系统中所有级别的遗忘，在大多数情况下保持极低的知识重现率（<1%），即使在多轮后续训练后仍能保持优越的持久性

Conclusion: Lethe方法有效解决了联邦学习中持续训练导致知识重现的问题，实现了持久的知识擦除，为联邦学习遗忘提供了更实用的解决方案

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [136] [Local-Global Multimodal Contrastive Learning for Molecular Property Prediction](https://arxiv.org/abs/2601.22610)
*Xiayu Liu,Zhengyi Lu,Yunhong Liao,Chan Fan,Hou-biao Li*

Main category: cs.LG

TL;DR: LGM-CL：一种局部-全局多模态对比学习框架，通过联合建模分子图和文本表示来提升分子性质预测性能


<details>
  <summary>Details</summary>
Motivation: 准确的分子性质预测需要整合分子结构和化学语义的互补信息。现有方法通常单独处理这些信息，缺乏有效的多模态融合机制。

Method: 提出LGM-CL框架：1) 使用AttentiveFP和Graph Transformer分别编码局部官能团信息和全局分子拓扑；2) 通过自监督对比学习对齐局部和全局表示；3) 将化学增强的文本描述与原始SMILES进行对比，融入物理化学语义；4) 在微调阶段通过双交叉注意力多模态融合整合分子指纹。

Result: 在MoleculeNet基准测试上的广泛实验表明，LGM-CL在分类和回归任务上均取得了一致且具有竞争力的性能，验证了统一局部-全局和多模态表示学习的有效性。

Conclusion: LGM-CL通过联合建模分子图和文本表示，有效整合了局部-全局信息和多模态语义，为分子性质预测提供了强大的框架。

Abstract: Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.

</details>


### [137] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力机制的替代方案，能提升Transformer在不同学习率下的训练稳定性，并通过混合共识-注意力框架在保持性能的同时提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力Transformer在训练时对学习率过指定表现出不稳定性，特别是在高学习率下。虽然已有方法通过修改优化过程来提高对此类过指定的弹性，但根本性的架构创新仍未被充分探索。

Method: 提出共识机制作为注意力的直接替代方案，将其形式化为图模型，并在文本、DNA和蛋白质模态上进行广泛实证分析。进一步提出混合共识-注意力框架，结合两者优势。

Result: 共识机制在更广泛的有效学习率范围内稳定了Transformer训练，在不同模态的学习率扫描中表现出改进的稳定性。混合框架在保持性能的同时提高了稳定性。

Conclusion: 共识机制是提高Transformer训练稳定性的有效架构创新，通过理论分析和实证验证展示了其优越性，为Transformer架构的改进提供了新方向。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [138] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: TTCS：一种协同进化的测试时训练框架，通过问题合成器和推理求解器的协同进化，为LLMs构建动态测试时课程，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时训练方法在处理困难推理问题时面临两个挑战：原始测试问题往往太难难以产生高质量伪标签，测试集规模有限导致连续在线更新不稳定。

Method: TTCS从同一预训练模型初始化两个策略：问题合成器和推理求解器。两者通过迭代优化协同进化：合成器基于测试问题生成逐步挑战性的问题变体，为求解器当前能力量身定制结构化课程；求解器使用在原始测试和合成问题上的多响应自一致性奖励进行更新。求解器的反馈指导合成器生成与模型当前能力对齐的问题，生成的问题变体反过来稳定求解器的测试时训练。

Result: 实验表明TTCS在挑战性数学基准上持续增强推理能力，并能迁移到不同LLM骨干网络的一般领域任务，展示了为自进化动态构建测试时课程的可扩展路径。

Conclusion: TTCS通过协同进化的测试时训练框架，有效解决了现有方法在处理困难推理问题时的局限性，为LLMs的自我进化提供了动态课程构建的可行方案。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [139] [PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model](https://arxiv.org/abs/2601.22631)
*En Fu,Yanyan Hu,Changhua Hu,Zengwang Jin,Kaixiang Peng*

Main category: cs.LG

TL;DR: 提出PEFT-MuTS框架，通过参数高效微调和跨域预训练时间序列表示模型，实现少样本剩余使用寿命预测，显著减少对目标设备数据的需求。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的RUL预测需要大量退化数据，现有解决方案如域适应和元学习仍依赖大量相同或相似设备的历史数据，在实际应用中存在显著限制。

Method: 基于跨域预训练时间序列表示模型构建参数高效微调框架，包含独立特征调优网络、基于元变量的低秩多变量融合机制，以及零初始化回归器以稳定少样本微调过程。

Result: 在航空发动机和工业轴承数据集上，即使使用目标设备少于1%的样本，也能实现有效的RUL预测，显著优于传统监督和少样本方法，同时大幅减少达到高预测精度所需的数据量。

Conclusion: 通过跨域预训练时间序列模型和参数高效微调，可以突破RUL预测中知识转移仅限于相似设备的传统观念，实现有效的少样本预测，为实际工业应用提供了可行方案。

Abstract: The application of data-driven remaining useful life (RUL) prediction has long been constrained by the availability of large amount of degradation data. Mainstream solutions such as domain adaptation and meta-learning still rely on large amounts of historical degradation data from equipment that is identical or similar to the target, which imposes significant limitations in practical applications. This study investigates PEFT-MuTS, a Parameter-Efficient Fine-Tuning framework for few-shot RUL prediction, built on cross-domain pre-trained time-series representation models. Contrary to the widely held view that knowledge transfer in RUL prediction can only occur within similar devices, we demonstrate that substantial benefits can be achieved through pre-training process with large-scale cross-domain time series datasets. A independent feature tuning network and a meta-variable-based low rank multivariate fusion mechanism are developed to enable the pre-trained univariate time-series representation backbone model to fully exploit the multivariate relationships in degradation data for downstream RUL prediction task. Additionally, we introduce a zero-initialized regressor that stabilizes the fine-tuning process under few-shot conditions. Experiments on aero-engine and industrial bearing datasets demonstrate that our method can achieve effective RUL prediction even when less than 1\% of samples of target equipment are used. Meanwhile, it substantially outperforms conventional supervised and few-shot approaches while markedly reducing the data required to achieve high predictive accuracy. Our code is available at https://github.com/fuen1590/PEFT-MuTS.

</details>


### [140] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一种结合形式逻辑验证与LLM推理的框架，通过实时验证反馈纠正推理错误，显著提升模型在数学、逻辑和一般推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: LLM的随机性导致逻辑不一致和奖励黑客问题，而形式符号系统能避免这些问题。需要桥接神经网络的生成能力与形式逻辑的严谨性

Method: 提出形式逻辑验证引导的框架，在自然语言生成过程中动态交织形式符号验证，提供实时反馈检测和纠正错误。采用两阶段训练流程：形式逻辑验证引导的监督微调和策略优化

Result: 在六个涵盖数学、逻辑和一般推理的基准测试中，7B和14B模型分别以平均10.4%和14.2%的优势超越最先进的基线模型

Conclusion: 形式验证可以作为可扩展机制，显著推动先进LLM推理的性能边界，有效解决LLM的逻辑不一致问题

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [141] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA提出了一种基于机器遗忘的群体级训练数据归因方法，用于扩散模型，相比传统LOGO重训练实现100倍加速，能更可靠地识别主要贡献群体。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法主要关注单个样本评分，但实践中需要群体级答案（如艺术风格或对象类别）。群体归因是反事实的：如果某个群体从训练中移除，模型在生成样本上的行为会如何变化？LOGO重训练虽然自然但计算成本过高。

Method: 提出GUDA（基于群体遗忘的数据归因）方法：1）通过机器遗忘技术从共享全数据模型近似每个反事实模型，而非从头训练；2）使用基于似然的评分规则（ELBO）在全模型和每个遗忘反事实模型之间的差异来量化群体影响。

Result: 在CIFAR-10和Stable Diffusion艺术风格归因实验中，GUDA比语义相似性、基于梯度的归因和实例级遗忘方法更可靠地识别主要贡献群体，在CIFAR-10上相比LOGO重训练实现100倍加速。

Conclusion: GUDA为扩散模型提供了一种高效、准确的群体级训练数据归因方法，通过机器遗忘技术避免了昂贵的重训练成本，在保持准确性的同时显著提升了计算效率。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [142] [Layerwise Progressive Freezing Enables STE-Free Training of Deep Binary Neural Networks](https://arxiv.org/abs/2601.22660)
*Evan Gibson Smith,Bashima Islam*

Main category: cs.LG

TL;DR: 提出StoMPP方法，通过层级随机掩码逐步替换可微权重/激活为硬二值函数，无需直通估计器，显著提升二值神经网络的训练效果。


<details>
  <summary>Details</summary>
Motivation: 研究渐进冻结作为直通估计器（STE）的替代方案，用于从头训练二值网络。发现全局渐进冻结在二值权重网络中有效，但在全二值神经网络中因激活引起的梯度阻塞而失败。

Method: 提出StoMPP（随机掩码部分渐进二值化）方法，使用层级随机掩码逐步将可微的裁剪权重/激活替换为硬二值阶跃函数，仅通过未冻结（裁剪）子集进行反向传播，不使用直通估计器。

Result: 在匹配的最小训练方案下，StoMPP相比BinaryConnect式STE基线显著提升准确率，且增益随网络深度增加（如ResNet-50 BNN：CIFAR-10 +18.0，CIFAR-100 +13.5，ImageNet +3.8）。二值权重网络在CIFAR-10达到91.2%，CIFAR-100达到69.5%。

Conclusion: StoMPP方法有效解决了全二值神经网络训练中的梯度阻塞问题，通过渐进冻结策略改善了二值化约束下的深度扩展性，揭示了训练动态中的非单调收敛特性。

Abstract: We investigate progressive freezing as an alternative to straight-through estimators (STE) for training binary networks from scratch. Under controlled training conditions, we find that while global progressive freezing works for binary-weight networks, it fails for full binary neural networks due to activation-induced gradient blockades. We introduce StoMPP (Stochastic Masked Partial Progressive Binarization), which uses layerwise stochastic masking to progressively replace differentiable clipped weights/activations with hard binary step functions, while only backpropagating through the unfrozen (clipped) subset (i.e., no straight-through estimator). Under a matched minimal training recipe, StoMPP improves accuracy over a BinaryConnect-style STE baseline, with gains that increase with depth (e.g., for ResNet-50 BNN: +18.0 on CIFAR-10, +13.5 on CIFAR-100, and +3.8 on ImageNet; for ResNet-18: +3.1, +4.7, and +1.3). For binary-weight networks, StoMPP achieves 91.2\% accuracy on CIFAR-10 and 69.5\% on CIFAR-100 with ResNet-50. We analyze training dynamics under progressive freezing, revealing non-monotonic convergence and improved depth scaling under binarization constraints.

</details>


### [143] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率确定最优停止点，在皮肤病变和血细胞分类任务上表现优于基于验证数据的方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习依赖固定全局轮次或验证数据进行超参数调优，导致高计算成本和隐私风险，阻碍实际部署。

Method: 提出数据无关的早期停止框架，仅使用服务器端参数监控任务向量增长率来确定最优停止点。

Result: 在皮肤病变和血细胞分类任务上，与基于验证数据的早期停止方法相当，平均使用47/20轮即可获得超过12.5%/10.3%的性能提升。

Conclusion: 这是首个无需验证数据的联邦学习早期停止框架，能有效降低计算成本和隐私风险，促进联邦学习实际部署。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [144] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 该论文系统比较了全图训练与小批量GNN训练，从批大小和扇出大小的角度分析了收敛性、泛化性和计算效率，发现全图训练并不总是优于调优后的小批量设置。


<details>
  <summary>Details</summary>
Motivation: 全图训练和小批量GNN训练有不同的系统设计要求，但缺乏对这两种方法在模型性能和计算效率方面的系统比较。批大小和扇出大小对GNN的影响尚未充分探索，需要从理论和实证角度分析这些超参数的影响。

Method: 通过实证和理论分析，使用Wasserstein距离进行泛化分析来研究图结构（特别是扇出大小）的影响，揭示批大小和扇出大小在GNN收敛和泛化中的非各向同性效应。

Result: 研究发现批大小和扇出大小对GNN收敛和泛化具有非各向同性影响，为资源约束下调整这些超参数提供实用指导。全图训练并不总是比调优后的小批量设置产生更好的模型性能或计算效率。

Conclusion: 全图训练并非总是最优选择，通过适当调整批大小和扇出大小的小批量训练可以达到相似的性能。研究为GNN训练方法选择提供了理论指导和实用建议。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [145] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流映射角度理论分析一致性模型，揭示训练不稳定和收敛问题的根源，提出改进的自蒸馏方法，并扩展到扩散策略学习


<details>
  <summary>Details</summary>
Motivation: 一致性模型虽然能实现快速生成建模，但在从头训练时存在不稳定性和可复现性有限的问题。现有研究对此的解释较为零散，理论关系不清晰，需要系统性的理论分析来理解这些问题

Method: 从流映射角度对一致性模型进行理论分析，揭示训练稳定性和收敛行为如何导致退化解。基于这些洞察，重新审视自蒸馏作为改进方法，并重新表述以避免过大的梯度范数，实现稳定优化

Result: 提出的方法不仅适用于图像生成，还能扩展到基于扩散的策略学习，且不需要预训练的扩散模型进行初始化，展示了更广泛的适用性

Conclusion: 通过流映射视角的理论分析，澄清了一致性模型的训练稳定性问题，提出的改进自蒸馏方法能有效解决某些形式的次优收敛，为一致性模型提供了更稳定的训练框架

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [146] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: Transformer模型在周期性OOD泛化方面存在局限，无法有效泛化到未见过的复合周期性模式


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型在各种任务上表现良好，但在分布外泛化方面与人类仍有较大差距。本文通过周期性这一基本OOD场景来研究这一差距，因为周期性捕捉了变化中的不变性

Method: 从抽象代数和推理的角度统一解释周期性（包括单一和复合周期性），构建Coper基准测试（包含Hollow和Extrapolation两种OOD设置），通过实验验证Transformer在周期性泛化方面的局限性

Result: 实验表明Transformer在周期性泛化方面存在限制：模型能够在训练期间记忆周期性数据，但无法泛化到未见过的复合周期性模式

Conclusion: Transformer模型在周期性OOD泛化方面存在根本性局限，需要进一步研究来提升模型在这方面的能力。作者开源了代码以支持未来研究

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [147] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 提出METRIC框架的实践化实现，通过建立数据质量指标库和指标卡片，为医疗AI提供可操作的数据质量评估方法，以支持可信AI发展。


<details>
  <summary>Details</summary>
Motivation: 医疗AI从研究转向实际应用，需要建立可信度证据。数据质量是可信AI的关键因素，需要系统化评估方法。

Method: 将理论性的METRIC框架操作化，建立数据质量指标库，为每个指标提供包含定义、适用性、示例、陷阱和建议的指标卡片，并提供指标选择策略和决策树。

Result: 在PTB-XL心电图数据集上展示了该方法的影响，为实践中评估训练和测试数据的适用性提供了第一步。

Conclusion: 这是实现医疗领域可信AI的基础，通过提供可操作的数据质量评估工具，支持医疗AI的临床接受度和监管批准。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [148] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于深度学习的IR-drop早期预测方法，使用CNN架构将物理布局特征映射到IR-drop热图，实现毫秒级推理，作为传统物理签核工具的补充工具。


<details>
  <summary>Details</summary>
Motivation: 传统IR-drop分析依赖物理签核工具，虽然精度高但计算成本大且需要接近最终版图信息，不适合早期快速设计探索。需要一种能在设计早期快速评估IR-drop的方法。

Method: 采用基于U-Net的编码器-解码器架构，通过跳跃连接捕获局部和全局空间依赖关系。将任务建模为密集像素级回归问题，使用物理启发的合成数据集进行训练，数据集包含电源网格结构、单元密度分布和开关活动等关键物理因素。

Result: 实验结果表明，该方法能准确预测IR-drop分布，推理时间达到毫秒级别，支持快速预签核筛选和迭代设计优化。模型性能使用MSE和PSNR等标准回归指标评估。

Conclusion: 提出的深度学习框架可作为传统签核工具的补充早期分析工具，在设计早期为设计者提供快速IR-drop洞察。代码、数据集生成脚本和交互式推理应用已开源。

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [149] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 该论文对LoRA变体进行了首次统一研究，提出了系统分类法、统一理论框架、模块化代码库和标准化评估，发现LoRA及其变体对学习率选择敏感，且适当配置下原始LoRA性能可匹配或超越多数变体。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调方法已被广泛应用，但其众多变体在方法、理论、代码和评估方面存在碎片化问题，缺乏系统性的统一研究。

Method: 1) 从四个主要维度对LoRA变体进行分类：秩、优化动态、初始化和与MoE的集成；2) 在低秩更新动态的统一理论框架下分析变体关系；3) 开发模块化代码库LoRAFactory；4) 在自然语言生成、理解和图像分类任务上进行大规模评估。

Result: 研究发现：1) LoRA及其变体相比其他超参数对学习率选择特别敏感；2) 在适当的超参数配置下，原始LoRA的性能能够匹配或超越大多数变体。

Conclusion: 该研究为LoRA变体提供了首个系统性统一框架，揭示了原始LoRA在适当配置下的强大性能，为未来参数高效微调研究提供了标准化工具和评估基准。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [150] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 利用视觉语言模型的常识推理能力，为潜在动作模型提供可提示的表征，有效分离可控变化与噪声，显著提升下游任务成功率


<details>
  <summary>Details</summary>
Motivation: 当前潜在动作模型在观测包含动作相关干扰物时容易编码噪声而非有意义的潜在动作，而人类却能轻松区分任务相关运动与无关细节。本文旨在利用视觉语言模型的常识推理能力来解决这一问题。

Method: 使用视觉语言模型提供可提示的表征，在无监督方式下有效分离可控变化与噪声，并将这些表征作为潜在动作模型训练的目标。对多种流行的视觉语言模型进行了基准测试，分析不同提示和超参数下的鲁棒性。

Result: 研究发现不同视觉语言模型提供的可提示表征质量存在显著差异，且较新的模型可能表现不如旧模型。通过简单要求视觉语言模型忽略干扰物，可以显著提升潜在动作质量，在Distracting MetaWorld上实现下游成功率高达六倍的提升。

Conclusion: 利用视觉语言模型的常识推理能力可以有效提升潜在动作模型在存在干扰物环境下的性能，为视觉-语言-动作模型的预训练提供了重要改进方向。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [151] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS是一种基于低秩分解的元素级量化框架，通过连续低秩矩阵建模缩放流形，在保持块级量化效率的同时提供更强的表达能力，在量化精度、推理速度和下游任务性能上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM量化方法主要依赖块级结构来保持效率，但这限制了表示的灵活性。作者希望打破这种空间约束，实现既高效又具有更强表达能力的量化方法。

Method: 提出LoRDS框架，将缩放流形建模为连续低秩矩阵(S=BA)，实现元素级量化。该方法提供高保真PTQ初始化、迭代优化、权重和缩放因子的联合QAT，以及高秩乘法PEFT适配，并配合优化的Triton内核实现。

Result: 在Llama3-8B上，3比特量化比NormalFloat量化精度提升27.0%，NVIDIA RTX 4090上推理速度提升1.5倍，下游任务PEFT性能比4比特QLoRA提升9.6%，在各种模型家族中均优于现有基线方法。

Conclusion: LoRDS通过低秩分解打破了量化粒度的空间约束，实现了效率与表达能力的统一，为LLM的统一压缩和适配提供了鲁棒且集成的解决方案。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [152] [Local Intrinsic Dimension of Representations Predicts Alignment and Generalization in AI Models and Human Brain](https://arxiv.org/abs/2601.22722)
*Junjie Yu,Wenxiao Ma,Chen Wei,Jianyu Zhang,Haotian Deng,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 研究发现神经网络泛化能力、模型间对齐、模型-大脑对齐都与表征的局部内在维度相关，局部维度越低，这些指标越好，而全局维度无法捕捉这些效应。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络泛化能力、模型间对齐和模型-大脑对齐之间的共同几何基础，理解表征收敛的本质特征。

Method: 通过分析不同架构和训练范式的神经网络，测量其泛化性能、模型间对齐度、模型-大脑对齐度，并计算表征的局部内在维度和全局维度。

Result: 泛化性能、模型间对齐、模型-大脑对齐三者显著相关；局部内在维度越低，这三个指标越好；增加模型容量和训练数据规模会系统性地降低局部内在维度。

Conclusion: 局部内在维度是人工和生物系统中表征收敛的统一描述符，为理解模型缩放的好处提供了几何解释。

Abstract: Recent work has found that neural networks with stronger generalization tend to exhibit higher representational alignment with one another across architectures and training paradigms. In this work, we show that models with stronger generalization also align more strongly with human neural activity. Moreover, generalization performance, model--model alignment, and model--brain alignment are all significantly correlated with each other. We further show that these relationships can be explained by a single geometric property of learned representations: the local intrinsic dimension of embeddings. Lower local dimension is consistently associated with stronger model--model alignment, stronger model--brain alignment, and better generalization, whereas global dimension measures fail to capture these effects. Finally, we find that increasing model capacity and training data scale systematically reduces local intrinsic dimension, providing a geometric account of the benefits of scaling. Together, our results identify local intrinsic dimension as a unifying descriptor of representational convergence in artificial and biological systems.

</details>


### [153] [Decomposing Epistemic Uncertainty for Causal Decision Making](https://arxiv.org/abs/2601.22736)
*Md Musfiqur Rahman,Ziwei Jiang,Hilaf Hasson,Murat Kocaoglu*

Main category: cs.LG

TL;DR: 提出新框架区分因果效应边界中的样本不确定性与不可识别不确定性，通过置信集方法判断何时收集更多样本无助于确定最优行动


<details>
  <summary>Details</summary>
Motivation: 现有神经因果模型可能过拟合数据集，对因果效应估计过于自信，且无法系统区分因果效应边界的宽度是由于根本的不可识别性还是有限样本限制造成的

Method: 提出新框架：在经验观测分布周围构建置信集，获取该置信集中所有分布的因果效应边界的交集，通过求解神经因果模型的min-max和max-min问题获得上下界

Result: 在合成和真实数据集上的大量实验表明，该算法能够确定何时收集更多样本无助于确定最优行动，指导实践者收集更多变量或转向随机研究

Conclusion: 该框架能够区分样本不确定性和非ID不确定性，为因果推断中的决策制定提供实用指导，帮助确定何时需要更多变量而非更多样本

Abstract: Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.

</details>


### [154] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 论文系统研究了Softmax系列损失函数，分析了不同替代损失的一致性、收敛行为和效率权衡，为大规模分类任务中的损失选择提供了理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: Softmax损失被广泛用于分类和排序任务，但现有研究存在两个独立方向：理论框架（Fenchel-Young框架）和实际可扩展性（大规模类别近似方法）。本文旨在整合这两个视角，对Softmax系列损失进行系统研究。

Method: 1) 分析不同替代损失与分类和排序指标的一致性；2) 分析梯度动态以揭示收敛行为；3) 为近似方法引入系统性的偏差-方差分解，提供收敛保证；4) 推导每轮复杂度分析，展示效果与效率的权衡。

Result: 实验表明一致性、收敛性和经验性能之间存在强相关性。研究结果为大规模机器学习应用中的损失选择建立了理论基础，并提供了实践指导。

Conclusion: 本文通过整合理论框架和实际可扩展性研究，为Softmax系列损失提供了系统分析，揭示了不同损失的一致性、收敛特性和效率权衡，为大规模分类任务中的损失选择提供了原则性基础。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [155] [Discovering Scaling Exponents with Physics-Informed Müntz-Szász Networks](https://arxiv.org/abs/2601.22751)
*Gnankan Landry Regis N'guessan,Bum Jun Kim*

Main category: cs.LG

TL;DR: 提出MSN-PINN网络，将幂律标度指数作为可训练参数，能同时输出解及其标度结构，在奇异点、界面和临界点附近物理系统中实现高精度指数恢复。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理具有幂律标度行为的物理系统时，无法显式地学习控制指数，而这些指数对理解奇异点、界面和临界点附近的物理行为至关重要。

Method: 引入物理信息Müntz-Szász网络（MSN-PINN），采用幂律基函数网络，将标度指数作为可训练参数，结合约束感知训练编码物理要求（如边界条件兼容性）。

Result: 在噪声和稀疏采样下实现1-5%误差的单指数恢复；二维拉普拉斯方程角奇异性指数误差0.009%；奇异泊松问题中恢复强迫诱导指数误差0.03%和0.05%；40配置楔形基准测试达到100%成功率，平均误差0.022%。

Conclusion: MSN-PINN结合神经网络的表达能力和渐近分析的可解释性，产生具有直接物理意义的可学习参数，比朴素训练提高三个数量级精度。

Abstract: Physical systems near singularities, interfaces, and critical points exhibit power-law scaling, yet standard neural networks leave the governing exponents implicit. We introduce physics-informed M"untz-Sz'asz Networks (MSN-PINN), a power-law basis network that treats scaling exponents as trainable parameters. The model outputs both the solution and its scaling structure. We prove identifiability, or unique recovery, and show that, under these conditions, the squared error between learned and true exponents scales as $O(|μ- α|^2)$. Across experiments, MSN-PINN achieves single-exponent recovery with 1--5% error under noise and sparse sampling. It recovers corner singularity exponents for the two-dimensional Laplace equation with 0.009% error, matches the classical result of Kondrat'ev (1967), and recovers forcing-induced exponents in singular Poisson problems with 0.03% and 0.05% errors. On a 40-configuration wedge benchmark, it reaches a 100% success rate with 0.022% mean error. Constraint-aware training encodes physical requirements such as boundary condition compatibility and improves accuracy by three orders of magnitude over naive training. By combining the expressiveness of neural networks with the interpretability of asymptotic analysis, MSN-PINN produces learned parameters with direct physical meaning.

</details>


### [156] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一个轻量级客户端加密框架，通过将原始嵌入投影到"混淆语义零空间"来保护LLM推理隐私，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型推理过程中的用户隐私，防止敏感信息泄露，同时保持模型效用。

Method: 将线性核的几何直觉推广到LLM高维潜在空间，定义"混淆语义零空间"，注入扰动将原始嵌入投影到此空间，使用密钥依赖的随机映射生成个性化扰动轨迹。

Result: 在12个生成和分类基准测试中达到最先进性能，显著降低攻击成功率，同时在严格安全约束下保持强大的模型效用。

Conclusion: OSNIP提供了一种有效的隐私保护LLM推理方案，无需后处理即可确保隐私，通过混淆语义零空间实现隐私与效用的平衡。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [157] [Understanding Generalization from Embedding Dimension and Distributional Convergence](https://arxiv.org/abs/2601.22756)
*Junjie Yu,Zhuoli Ouyang,Haotian Deng,Chen Wei,Wenxiao Ma,Jianyu Zhang,Zihan Deng,Quanying Liu*

Main category: cs.LG

TL;DR: 论文从表示中心视角研究泛化，提出基于嵌入几何的泛化界，不依赖参数数量，由嵌入分布的内在维度和下游映射的敏感性决定。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在严重过参数化情况下仍能良好泛化，这与基于参数的传统分析相矛盾。需要从表示学习的角度理解泛化机制，特别是学习到的嵌入几何如何控制预测性能。

Method: 从表示中心视角分析泛化，将总体风险分解为两个因素：(1)嵌入分布的内在维度，控制经验嵌入分布到总体分布在水斯坦距离下的收敛速率；(2)从嵌入到预测的下游映射的敏感性，用Lipschitz常数刻画。提出不依赖参数数量或假设类复杂度的嵌入依赖误差界。

Result: 在最终嵌入层，架构敏感性消失，误差界主要由嵌入维度主导，这解释了嵌入维度与泛化性能的强经验相关性。跨架构和数据集的实验验证了理论，并展示了基于嵌入的诊断工具的实用性。

Conclusion: 从表示中心视角理解泛化提供了新的理论框架，基于嵌入几何的误差界比传统参数计数方法更能解释深度网络的泛化行为，为模型诊断和设计提供了新工具。

Abstract: Deep neural networks often generalize well despite heavy over-parameterization, challenging classical parameter-based analyses. We study generalization from a representation-centric perspective and analyze how the geometry of learned embeddings controls predictive performance for a fixed trained model. We show that population risk can be bounded by two factors: (i) the intrinsic dimension of the embedding distribution, which determines the convergence rate of empirical embedding distribution to the population distribution in Wasserstein distance, and (ii) the sensitivity of the downstream mapping from embeddings to predictions, characterized by Lipschitz constants. Together, these yield an embedding-dependent error bound that does not rely on parameter counts or hypothesis class complexity. At the final embedding layer, architectural sensitivity vanishes and the bound is dominated by embedding dimension, explaining its strong empirical correlation with generalization performance. Experiments across architectures and datasets validate the theory and demonstrate the utility of embedding-based diagnostics.

</details>


### [158] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 分子语言模型遵循可预测的缩放定律，分子表示对性能有显著影响，研究发布了最大的分子语言模型库


<details>
  <summary>Details</summary>
Motivation: 分子生成模型在扩大数据集和模型规模时表现出潜力，但尚不清楚它们是否遵循可预测的缩放定律，这对于在模型大小、数据量和分子表示之间优化资源分配至关重要

Method: 系统研究分子语言模型在预训练和下游任务中的缩放行为，训练300个模型并进行超过10,000次实验，严格控制计算预算，同时独立改变模型大小、训练标记数量和分子表示

Result: 分子模型在预训练和下游迁移中都显示出清晰的缩放定律，揭示了分子表示对性能的显著影响，并解释了先前观察到的分子生成缩放行为不一致性

Conclusion: 分子语言模型确实遵循可预测的缩放定律，分子表示是影响性能的关键因素，研究公开发布了迄今为止最大的分子语言模型库以促进未来研究

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [159] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 论文建立了稀疏注意力机制与紧支撑核函数之间的理论对应关系，揭示了ReLU和sparsemax注意力分别对应固定和自适应归一化的Epanechnikov核回归，为稀疏注意力提供了核理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究揭示了自注意力机制与核回归之间的联系，但缺乏对稀疏注意力机制的核理论理解。本文旨在填补这一空白，为稀疏注意力提供理论解释。

Method: 建立稀疏注意力与紧支撑核函数的对应关系，证明ReLU和sparsemax注意力分别对应Epanechnikov核回归的不同归一化方式，并将α-entmax注意力与常用非参数密度估计核函数联系起来。

Result: 证明了稀疏注意力机制对应于紧支撑核函数，ReLU/sparsemax对应Epanechnikov核，α-entmax对应Epanechnikov、biweight、triweight等核函数，softmax/Gaussian对应极限情况。基于核回归的Memory Mosaics模型在语言建模等任务上表现优异。

Conclusion: 为稀疏注意力提供了统一的核理论框架，解释了稀疏性如何从核设计中自然产生，为设计注意力机制提供了理论指导，替代了启发式的top-k注意力等方法。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [160] [Float8@2bits: Entropy Coding Enables Data-Free Model Compression](https://arxiv.org/abs/2601.22787)
*Patrick Putzky,Martin Genzel,Mattes Mollenhauer,Sebastian Schulze,Thomas Wollmann,Stefan Dietzel*

Main category: cs.LG

TL;DR: EntQuant是一种新颖的量化框架，通过熵编码将数值精度与存储成本解耦，在极端压缩（低于4位）下实现数据依赖方法的性能，同时保持数据无关方法的速度和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前后训练压缩存在两个对立范式：快速、数据无关、模型无关的方法（如NF4、HQQ）在低于4位的极端比特率下会出现功能崩溃；而依赖校准数据或大量恢复训练的方法虽然保真度高，但计算成本高且数据分布变化时鲁棒性不确定。需要结合两者的优势。

Method: EntQuant通过熵编码将数值精度与存储成本解耦，使用数据无关技术实现数据依赖方法的性能。该方法能在30分钟内压缩700亿参数模型，并在推理时保持适度的开销。

Result: EntQuant在标准评估集和模型上达到最先进结果，同时在指令调优模型的更复杂基准测试中保持功能性能，在极端压缩下实现实用价值。

Conclusion: EntQuant首次统一了数据依赖和数据无关压缩范式的优势，为极端压缩提供了实用解决方案，平衡了性能、速度和通用性。

Abstract: Post-training compression is currently divided into two contrasting regimes. On the one hand, fast, data-free, and model-agnostic methods (e.g., NF4 or HQQ) offer maximum accessibility but suffer from functional collapse at extreme bit-rates below 4 bits. On the other hand, techniques leveraging calibration data or extensive recovery training achieve superior fidelity but impose high computational constraints and face uncertain robustness under data distribution shifts. We introduce EntQuant, the first framework to unite the advantages of these distinct paradigms. By matching the performance of data-dependent methods with the speed and universality of data-free techniques, EntQuant enables practical utility in the extreme compression regime. Our method decouples numerical precision from storage cost via entropy coding, compressing a 70B parameter model in less than 30 minutes. We demonstrate that EntQuant does not only achieve state-of-the-art results on standard evaluation sets and models, but also retains functional performance on more complex benchmarks with instruction-tuned models, all at modest inference overhead.

</details>


### [161] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无剪裁策略优化方法，通过总变差散度约束的凸二次惩罚替代启发式剪裁，解决了大规模RL训练中的零梯度区域、奖励黑客和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在大型语言模型后训练中占据核心地位，但主流算法依赖剪裁机制，在大规模应用中引入了优化问题，包括零梯度区域、奖励黑客和训练不稳定性。

Method: CFPO用基于总变差散度约束的凸二次惩罚替代启发式剪裁，产生处处可微的目标函数，在不使用硬边界的情况下强制执行稳定的策略更新。

Result: 在推理任务中，CFPO在保持下游基准性能的同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，同时实现了有竞争力的指令跟随性能。

Conclusion: CFPO仅需一行代码更改且无需额外超参数，是LLM后训练中基于剪裁方法的有前景的直接替代方案。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [162] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: Sombrero是一种改进分层序列模型边界学习的方法，通过边界富集度指标指导边界放置，使计算资源更集中于难预测位置，提升效率-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 分层序列模型通过学习的边界压缩长字节序列以提高自回归建模效率，但现有方法难以定量评估和系统性地指导计算资源的分配位置。

Method: 提出边界富集度B作为边界质量指标，衡量块起始位置与高下一字节惊奇度位置的集中程度。基于此提出Sombrero方法：1) 通过置信对齐边界损失引导边界放置朝向预测困难位置；2) 在输入级别而非已实现块上应用置信加权平滑来稳定边界学习。

Result: 在1B规模上，在涵盖英语和德语文本以及代码和数学内容的UTF-8语料库上，Sombrero改善了准确性-效率权衡，产生的边界更一致地将计算与难预测位置对齐。

Conclusion: Sombrero通过边界富集度指标和相应的训练方法，能够更有效地指导分层序列模型中的边界学习，使计算资源分配更符合预测难度，从而提升模型性能。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [163] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，显著降低NVFP4格式的量化误差，实现端到端全量化预训练，在1.9B参数LLM训练中验证效果，相比BF16获得4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: NVFP4低精度格式虽然支持端到端全量化预训练，但现有量化方法为获得更准确的无偏梯度估计而牺牲了格式的表示能力，导致相对于FP16/FP8训练仍有明显精度损失。

Method: 提出MS-EDEN（微尺度格式的无偏量化方法），量化误差比随机舍入降低2倍以上；将其集成到Quartet II全NVFP4量化方案中，用于线性层的前向和反向传播。

Result: 理论分析显示Quartet II在所有主要矩阵乘法中都能获得更好的梯度估计；在1.9B参数、38B tokens的LLM训练中验证了效果；在NVIDIA Blackwell GPU上实现4.2倍于BF16的加速。

Conclusion: Quartet II方案显著提升了NVFP4量化训练的性能，实现了端到端全量化预训练，在保持精度的同时获得显著加速，为大规模模型训练提供了高效解决方案。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [164] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散模型，用于生成包含离散和连续特征的表格数据，特别针对混合类型特征（离散状态与连续分布结合）的生成挑战


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理表格数据时，难以有效生成混合类型特征（如包含缺失值或异常值的连续特征）。需要改进扩散模型以更准确地捕捉表格数据的复杂分布特性

Method: 采用级联方法：首先生成低分辨率版本（纯分类特征+数值特征的粗粒度分类表示），然后通过新颖的引导条件概率路径和数据依赖耦合，在流匹配模型中利用这些信息生成高分辨率数据

Result: 模型能更真实地生成样本，更准确地捕捉分布细节，检测分数提高40%。理论证明该级联方法能收紧传输成本边界

Conclusion: 提出的级联扩散模型显著提升了表格数据生成的质量，特别是对混合类型特征的生成能力，为表格数据生成提供了新的有效方法

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [165] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug是一个多级不确定性感知元学习框架，通过两级元适应机制解决药物推荐中的患者冷启动问题，在MIMIC-III和AKI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，新患者因缺乏足够的处方历史而难以获得可靠推荐。虽然先前研究使用医学知识图谱连接药物概念，但这些方法主要解决项目冷启动问题，无法提供适应个体患者特征的个性化推荐。元学习在推荐系统中处理稀疏交互的新用户方面显示出潜力，但在电子健康记录（EHR）中的应用仍未被充分探索。

Method: 提出MetaDrug框架，包含：1）两级元适应机制：自适应（使用患者自身医疗事件作为支持集捕捉时间依赖）和同伴适应（使用相似患者的就诊记录丰富新患者表示）；2）不确定性量化模块：对支持就诊进行排序并过滤无关信息以确保适应一致性。

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法。

Conclusion: MetaDrug通过多级不确定性感知元学习框架有效解决了药物推荐中的患者冷启动问题，为缺乏足够处方历史的新患者提供了更可靠的个性化推荐。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [166] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出SCIQL框架，通过风格条件隐式Q学习解决离线强化学习中风格与任务性能的平衡问题


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，风格条件策略学习面临分布偏移和风格与奖励冲突的挑战，现有方法难以有效协调这两个目标

Method: 提出统一的行为风格定义，构建SCIQL框架，结合离线目标条件RL技术（如后见之明重标注和价值学习）和新的门控优势加权回归机制

Result: 实验表明SCIQL在风格对齐和任务性能两方面都优于现有离线方法

Conclusion: SCIQL为离线强化学习中的风格条件策略学习提供了有效的解决方案，成功平衡了风格对齐与任务性能

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [167] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出一种基于LoRA的持续学习框架，将单一LoRA模块重构为可分解的Rank-1专家池，通过稀疏组合和正交化实现参数高效的持续学习，减少遗忘并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的持续学习面临任务适应性和灾难性遗忘的挑战。现有方法通常有沉重的推理负担或依赖外部知识，而LoRA虽然能实现参数高效调优，但直接用于缓解遗忘问题并不简单。

Method: 将单一LoRA模块重构为可分解的Rank-1专家池，通过[CLS]令牌语义动态选择专家组成稀疏的任务特定更新。提出激活引导正交(AGO)损失，使关键LoRA权重在任务间正交化，减少参数更新和任务间干扰。

Result: 在多个设置下实现最先进结果，所有指标均超越零样本上界。相比基线方法减少96.7%可训练参数，无需外部数据集或任务ID判别器。合并的LoRA权重更少且无推理延迟，计算轻量。

Conclusion: 该方法通过稀疏组合和正交化实现了参数高效的持续学习，在减少遗忘的同时保持下游任务性能，为视觉语言模型的持续学习提供了轻量高效的解决方案。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [168] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种通过正则化预训练自编码器来增强潜在空间等变性的流匹配框架，用于时间序列生成，在保持高效采样优势的同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的时间序列生成模型通常在低维潜在空间中定义以实现高效采样，但如何设计具有理想等变性质的潜在表示仍未充分探索。等变性对于时间序列生成很重要，因为基本变换（如平移和幅度缩放）应保持生成质量。

Method: 提出潜在流匹配框架，通过简单正则化预训练自编码器来显式鼓励等变性。引入等变损失函数，强制变换信号与其重构之间的一致性，使用该损失对基本时间序列变换（如平移和幅度缩放）的潜在空间进行微调。

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上一致优于现有基于扩散的基线方法，同时实现数量级更快的采样速度。等变正则化的潜在空间在保持潜在流模型计算优势的同时提高了生成质量。

Conclusion: 将几何归纳偏置纳入时间序列的潜在生成模型中具有实际益处。等变正则化的潜在空间既能提升生成质量，又能保持高效采样优势，为时间序列生成提供了有效解决方案。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [169] [Hierarchical Shift Mixing -- Beyond Dense Attention in Transformers](https://arxiv.org/abs/2601.22852)
*Robert Forchheimer*

Main category: cs.LG

TL;DR: HSM是一种分层移位混合框架，通过将成对token交互分布到Transformer各层而非每层密集计算，实现线性时间复杂度，性能接近softmax注意力，混合架构还能降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的softmax注意力层存在二次时间复杂度问题，现有替代方法大多以性能下降为代价，需要一种既能保持性能又能降低计算复杂度的新方法。

Method: 提出分层移位混合(HSM)框架，将token间的成对交互分布到Transformer的不同层中计算，而不是在每层都进行密集计算，支持任意混合函数且保持线性时间复杂度。

Result: 简单的HSM变体性能接近softmax注意力，HSM与softmax注意力结合的混合架构在训练和推理时都能超越GPT风格Transformer基线，同时降低计算成本。

Conclusion: HSM提供了一种有效替代传统注意力机制的方法，在保持性能的同时显著降低计算复杂度，为高效Transformer架构设计提供了新思路。

Abstract: Since the introduction of the Transformer architecture for large language models, the softmax-based attention layer has faced increasing scrutinity due to its quadratic-time computational complexity. Attempts have been made to replace it with less complex methods, at the cost of reduced performance in most cases. We introduce Hierarchical Shift Mixing (HSM), a general framework for token mixing that distributes pairwise token interactions across Transformer layers rather than computing them densely within each layer. HSM enables linear-time complexity while remaining agnostic to the specific mixing function. We show that even simple HSM variants achieve performance close to softmax attention, and that hybrid architectures combining HSM with softmax attention can outperform a GPT-style Transformer baseline while reducing computational cost during both training and inference.

</details>


### [170] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG：基于不平衡最优传输的正则化框架，解决多模态属性图中显式图结构与隐式语义结构不一致的问题，通过Fused Gromov-Wasserstein距离引导跨模态结构一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图（MAGs）在建模复杂系统时，不同模态嵌入诱导的隐式语义结构与显式图结构之间存在不一致性。现有方法在固定显式图结构上进行消息传递时，会聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，使用Fused Gromov-Wasserstein距离显式引导局部邻域内的跨模态结构一致性，缓解结构-语义冲突。同时使用KL散度惩罚自适应处理跨模态不一致性。该框架可作为即插即用的正则化器集成到现有多模态图模型中。

Result: 实验表明OptiMAG在多种任务上持续优于基线方法，包括图中心任务（节点分类、链接预测）和多模态中心生成任务（图到文本、图到图像生成）。

Conclusion: OptiMAG通过最优传输理论有效解决了多模态属性图中的结构-语义不一致问题，提高了多模态图表示学习的性能，且具有良好的通用性和可扩展性。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [171] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn提出了一种新型脉冲Transformer，通过掩码首次脉冲时间编码和忆阻突触单元，显著降低脉冲移动和权重访问开销，在保持精度的同时大幅提升能效。


<details>
  <summary>Details</summary>
Motivation: 当前SNN的能效评估主要关注计算操作，忽略了数据移动等实际硬件成本（占总能耗近80%），需要更全面的能效优化方案。

Method: 提出掩码首次脉冲时间编码（M-TTFS），通过重新分配零能量静默状态到最频繁的膜电位，并结合"死区"策略最大化稀疏性；硬件层面使用忆阻突触单元（MSU）实现存内计算，消除权重访问开销。

Result: 在GLUE基准测试中，Matterhorn创造了新的SOTA，平均准确率比现有SNN提高1.42%，同时能效提升2.31倍。

Conclusion: Matterhorn通过算法-硬件协同设计，有效解决了SNN推理中的能量瓶颈，为能效型大语言模型推理提供了有前景的解决方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [172] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 提出一种基于分位数图映射的时间序列生成框架，通过将时间序列转换为分位数图再逆向重构来生成合成数据，与GAN方法相比具有竞争力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高质量时间序列数据获取受限（隐私、成本、标注困难），需要有效的合成生成方法来解决这些限制。

Method: 将时间序列转换为分位数图，然后通过逆向映射重构生成合成时间序列，保持原始数据的统计和结构特性。

Result: 在模拟和真实数据集上评估，生成的数据能保持原始统计和结构特性，与最先进的GAN方法相比具有竞争力。

Conclusion: 分位数图方法为合成时间序列生成提供了竞争性且可解释的替代方案。

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [173] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: MoVE（混合值嵌入）是一种新的自回归建模机制，通过引入可学习的全局值嵌入库，将模型容量与计算成本解耦，允许独立扩展参数记忆而不增加网络深度或宽度。


<details>
  <summary>Details</summary>
Motivation: 传统自回归序列建模存在根本性限制：模型容量与计算成本紧密耦合。扩展模型参数记忆（事实知识或视觉模式的存储库）通常需要加深或加宽网络，这会按比例增加计算成本。需要打破这种耦合，建立新的容量扩展维度。

Method: 提出MoVE机制，引入全局可学习的值嵌入库，在所有注意力层之间共享。对于序列中的每个步骤，模型使用可微分软门控机制，动态地将从该库检索的概念混合到标准值投影中。通过增加嵌入槽数量，可以独立于网络深度扩展参数记忆。

Result: 在文本生成和图像生成两个代表性自回归建模应用中验证了MoVE。在两个领域，MoVE相比标准和分层记忆基线都带来了持续的性能提升，能够构建"记忆密集"模型，在可比计算预算下实现更低的困惑度和更高的保真度。

Conclusion: MoVE成功打破了模型容量与计算成本之间的耦合，为自回归建模建立了新的容量扩展维度。该机制允许构建更高效的"记忆密集"模型，在保持计算效率的同时提升生成质量。

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [174] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种多任务强化学习方法，通过将命题视为参数化谓词而非离散符号，实现对新命题和任务的零样本泛化


<details>
  <summary>Details</summary>
Motivation: 现有基于线性时序逻辑（LTL）的多任务强化学习方法虽然能在LTL规范间泛化，但无法泛化到未见过的命题词汇表（描述LTL中高层事件的符号）。这限制了策略对全新命题和任务的适应能力。

Method: 将命题视为参数化谓词的实例而非离散符号，使策略能学习相关命题间的共享结构。提出新颖架构来嵌入和组合谓词以表示LTL规范。

Result: 在挑战性环境中成功实现了对新命题和任务的零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化。

Conclusion: PlatoLTL通过参数化命题表示解决了多任务强化学习中词汇表泛化的关键限制，为构建更通用的智能体提供了有效途径。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [175] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出基于正则化的多变量校准方法，使用预排序函数在训练中强制多变量校准，并引入基于PCA的新型预排序函数来检测依赖结构误设


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预排序函数主要用于事后评估，缺乏在训练过程中强制多变量校准的方法。

Method: 提出正则化校准方法，在训练多变量分布回归模型时使用预排序函数强制多变量校准。引入基于PCA的新型预排序函数，将预测投影到预测分布的主方向上。

Result: 通过模拟研究和18个真实世界多输出回归数据集的实验表明，该方法显著改善了多变量预排序校准，且不损害预测准确性。PCA预排序能检测出现有预排序无法发现的依赖结构误设。

Conclusion: 该方法为多变量概率预测提供了有效的校准框架，PCA预排序函数能揭示依赖结构误设，有助于改进多变量分布回归模型的校准性能。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [176] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出一种贝叶斯单树模型，将高斯过程预测器集成到决策树叶节点，以改进回归任务中的外推能力和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中存在局限性：分段常数叶节点预测受限于训练目标范围，在分布偏移时容易过度自信，缺乏可靠的外推能力和良好校准的不确定性。

Method: 扩展VSPYCT模型，为每个叶节点配备高斯过程预测器；使用贝叶斯斜分割进行不确定性感知的输入空间划分；GP叶节点建模局部函数行为；设计高效推理和预测方案，结合分割参数的后验采样与GP后验预测；引入门控机制，当输入超出叶节点训练支持时激活基于GP的外推。

Result: 在基准回归任务中，相比标准变分斜树，该模型在预测性能上有所改进，在外推场景中表现出显著的性能提升。

Conclusion: 该贝叶斯单树模型通过集成GP预测器，有效解决了决策树在回归任务中的外推和不确定性校准问题，为需要可靠外推和不确定性量化的应用提供了有前景的解决方案。

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [177] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：一种基于熵引导的灵活低秩适应框架，通过谱能量熵评估矩阵重要性，支持全局预算下的秩剪枝和扩展，解决了传统LoRA固定秩设计的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型虽然在各领域表现优异，但完全微调的计算和内存成本过高。参数高效微调（PEFT）成为主流范式，其中LoRA引入可训练低秩矩阵表现良好，但其固定秩设计限制了灵活性。现有的动态秩分配方法依赖启发式元素级指标，缺乏矩阵级区分，且无法扩展需要额外适应的层容量。

Method: 提出FlexLoRA框架：1）通过谱能量熵评估矩阵重要性；2）在全局预算下支持秩剪枝和扩展；3）对新添加的奇异方向采用零影响初始化以确保稳定性。该方法解决了粒度、灵活性和稳定性限制。

Result: 大量实验表明，FlexLoRA在多个基准测试中持续优于最先进的基线方法。

Conclusion: FlexLoRA通过解决现有方法的粒度、灵活性和稳定性限制，为参数高效微调提供了更原则性的解决方案，在保持高效的同时实现了更好的性能。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [178] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出DC-LA算法，用于采样目标分布π∝exp(-f-r)，其中f是Lipschitz光滑项，r是非光滑的DC函数。通过Moreau包络平滑r，将凹部分重新分配到数据保真项，建立q-Wasserstein距离收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决非光滑DC正则化项的非对数凹采样问题。传统方法难以处理非光滑DC结构，需要开发能利用DC结构的采样算法。

Method: 利用DC结构，对r1和r2分别应用Moreau包络平滑r，将凹部分重新分配到数据保真项，提出DC-LA（DC proximal Langevin algorithm）。

Result: 在V是距离耗散的假设下，DC-LA在q-Wasserstein距离（所有q∈ℕ*）上收敛到目标分布π（达到离散化和平滑误差）。结果改进了非对数凹采样的先前工作。

Conclusion: DC-LA为具有DC正则化项的采样问题提供了理论保证的解决方案，在合成和真实CT应用中表现出色，能可靠地进行不确定性量化。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [179] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出可扩展的拓扑保持图粗化方法STPGC，通过引入图强坍塌和图边坍塌概念，在保持拓扑特征的同时减少图规模，加速GNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法要么保持谱特征要么保持空间特征，而保持拓扑特征的方法虽然能维持GNN预测性能，但存在指数时间复杂度的问题。

Method: 提出STPGC方法，引入图强坍塌和图边坍塌概念，开发GStrongCollapse、GEdgeCollapse和NeighborhoodConing三种算法，消除支配节点和边，严格保持拓扑特征。

Result: 实验证明STPGC在节点分类任务中高效有效，能够保持GNN感受野，并通过近似算法加速GNN训练。

Conclusion: STPGC解决了拓扑保持图粗化的可扩展性问题，在保持拓扑特征的同时实现了高效计算，为大规模图神经网络训练提供了有效解决方案。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [180] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR提出了一种统一框架，通过环境条件尾部重加权增强TV不变风险最小化，同时处理环境级相关偏移和样本级多样性偏移，提升OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有不变风险最小化方法主要处理环境级的伪相关性，但忽略了环境内的样本级异质性，这在混合分布偏移下严重影响OOD性能。需要同时处理环境级相关偏移和样本级多样性偏移。

Method: 提出环境条件尾部重加权TV不变风险最小化框架，将环境级不变性与环境内鲁棒性结合。通过环境条件尾部重加权增强TV不变学习，在没有环境标注时通过极小极大公式推断潜在环境。

Result: 在回归、表格数据、时间序列和图像分类基准测试中，在混合分布偏移下，最差环境和平均OOD性能均获得一致提升。

Conclusion: ECTR框架通过联合处理环境级相关偏移和样本级多样性偏移，使两种机制在混合分布偏移下互补，显著提升OOD泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [181] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文通过理论分析证明困惑度（perplexity）作为模型选择指标存在缺陷，即使模型能准确预测某些序列，也必然存在其他序列具有很低困惑度但模型预测错误。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为衡量模型质量的简单计算指标近年来被广泛使用，但先前研究已从实证角度指出其局限性。本文旨在通过理论分析严谨地证明困惑度可能不适合作为模型选择指标。

Method: 利用Transformer连续性理论，证明对于紧凑的解码器Transformer模型，如果存在某个序列能被模型准确且自信地预测（强泛化的必要条件），则必然存在另一个序列具有很低困惑度但模型预测错误。同时通过分析等困惑度图，研究困惑度与模型选择的关系。

Result: 理论证明显示困惑度作为模型选择指标存在根本缺陷：模型准确预测某些序列的能力必然伴随着对某些低困惑度序列的错误预测。等困惑度分析表明，困惑度并不总是选择更准确的模型，只有当模型置信度提升伴随相应准确度提升时，新模型才会被选择。

Conclusion: 困惑度不适合作为模型选择指标，因为它无法可靠地区分模型质量。模型置信度的提升必须与准确度提升相匹配，否则困惑度指标会做出错误选择。这为困惑度在模型评估中的使用提供了理论警示。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [182] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文解决了线性bandit中Nash regret的次优问题，提出了新的分析工具获得最优界，并首次研究了p-means regret框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有线性bandit中的Nash regret结果存在维度d上的次优性，源于依赖限制性浓度不等式。需要新的分析工具来解决这个开放问题，并扩展研究更一般的p-means regret框架。

Method: 提出了新的分析工具来解决Nash regret的次优问题，并提出了通用的FairLinBandit算法框架，该框架可作为元算法与任何线性bandit策略结合。具体实例化了Phased Elimination和Upper Confidence Bound两种算法。

Result: 获得了线性bandit中Nash regret的最优界，并证明了两种算法实例在整个p值范围内都能实现亚线性p-means regret。在真实数据集生成的线性bandit实例上的实验表明，方法始终优于现有最先进基线。

Conclusion: 本文解决了线性bandit中Nash regret的开放问题，提出了新的分析工具和通用的FairLinBandit框架，首次研究了p-means regret，为公平性和效用目标提供了统一的框架。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [183] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: PAVE通过正则化评论家网络来稳定策略梯度，解决连续动作空间强化学习中策略振荡问题，无需修改演员网络。


<details>
  <summary>Details</summary>
Motivation: 连续演员-评论家方法学习到的策略经常表现出高频振荡，不适合物理部署。现有方法直接正则化策略输出，但作者认为这治标不治本。

Method: 提出PAVE（Policy-Aware Value-field Equalization）框架，将评论家视为标量场，稳定其诱导的动作梯度场。通过最小化Q梯度波动同时保持局部曲率来修正学习信号。

Result: 实验结果表明PAVE实现了与策略侧平滑正则化方法相当的平滑性和鲁棒性，同时保持竞争力的任务性能，且无需修改演员网络。

Conclusion: 策略非平滑性根本上由评论家的微分几何性质决定，通过正则化评论家网络而非策略本身，可以更有效地解决策略振荡问题。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [184] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出一种端到端可学习的置换框架，通过可学习的置换成本矩阵、可微分的二分图匹配求解器和稀疏优化损失函数，优化Transformer模型权重置换以提升结构化稀疏化效果。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏化已成为流行的模型剪枝技术，权重置换能进一步改善剪枝后性能。然而，Transformer架构规模导致置换搜索空间指数增长，现有方法依赖贪心或启发式算法，限制了重排序效果。

Method: 提出端到端可学习置换框架：1) 引入可学习置换成本矩阵量化权重矩阵任意两个输入通道的交换成本；2) 使用可微分二分图匹配求解器获取最优二进制置换矩阵；3) 设计稀疏优化损失函数直接优化置换算子。

Result: 在视觉和语言Transformer上广泛验证，该方法在结构化稀疏化方面实现了最先进的置换结果。

Conclusion: 该工作提出的端到端可学习置换框架能有效优化Transformer模型权重置换，提升结构化稀疏化性能，解决了传统方法因搜索空间大而依赖次优算法的问题。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [185] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种针对离散扩散语言模型的解码引导水印方法，利用扩散模型对解掩码顺序的敏感性，通过引导解掩码顺序到满足奇偶校验约束的位置来实现水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型（dLLMs）与自回归模型不同，可以按任意顺序生成标记。虽然理想的条件预测器应该对顺序不变，但实际的dLLMs对解掩码顺序表现出强烈敏感性，这为水印技术创造了新的通道。

Method: dgMARK引导解掩码顺序到那些高奖励候选标记满足由二进制哈希诱导的简单奇偶校验约束的位置，而不显式重新加权模型学习到的概率。该方法可与常见解码策略（如置信度、熵和边界排序）即插即用，并可通过一步前瞻变体增强。

Result: 通过提升的奇偶匹配统计量检测水印，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的解码引导水印方法，利用模型对解掩码顺序的敏感性，实现了鲁棒的水印嵌入和检测机制。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [186] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: 提出VaR-CPO算法，一种直接优化VaR约束的样本高效保守方法，能在可行环境中实现训练期间零约束违反


<details>
  <summary>Details</summary>
Motivation: 现有基线方法无法保证训练过程中的安全探索，特别是在需要满足VaR约束的场景下。VaR约束的非可微性也带来了优化挑战

Method: 使用单边切比雪夫不等式处理VaR约束的非可微性，基于成本回报的前两矩获得可处理的替代约束。扩展CPO方法的信任域框架，提供策略改进和约束违反的最坏情况界限

Result: VaR-CPO能够实现安全探索，在可行环境中训练期间实现零约束违反，这是基线方法无法保证的关键特性

Conclusion: VaR-CPO是一种有效的安全强化学习方法，通过理论保证和实证验证，解决了VaR约束优化中的非可微性和安全探索问题

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [187] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出名为Mano的新优化器，通过流形优化方法在训练LLMs时超越AdamW和Muon，同时减少内存消耗和计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有LLMs训练优化器存在局限性：AdamW忽略结构特性，Muon丢失曲率信息，而传统流形优化方法在大规模模型优化中表现不佳。需要开发能结合两者优势的新优化方法。

Method: 创新性地将动量投影到模型参数的切空间，并将其约束在旋转斜流形上，提出Mano优化器，首次弥合了流形优化与现代优化器之间的性能差距。

Result: 在LLaMA和Qwen3模型上的广泛实验表明，Mano在减少内存消耗和计算复杂度的同时，始终显著优于AdamW和Muon，扩展了空间和时间效率的帕累托前沿。

Conclusion: Mano是一种新颖、强大且高效的优化器，成功将流形优化方法应用于LLMs训练，解决了现有优化器的局限性，为大规模模型优化提供了新的解决方案。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [188] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出CCI统一框架，将离线RL中的三种约束方法统一为连续约束谱，并开发ACPO算法自适应调整约束类型，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法各自采用不同的约束形式（加权行为克隆、密度正则化、支持约束），缺乏统一的理论框架来解释它们之间的联系和权衡，限制了方法的通用性和性能。

Method: 提出连续约束插值（CCI）框架，通过单个插值参数实现三种约束类型的平滑过渡和组合；基于CCI开发自动约束策略优化（ACPO）算法，使用拉格朗日对偶更新自适应调整插值参数；建立最大熵性能差异引理并推导性能下界。

Result: 在D4RL和NeoRL2基准测试中表现出鲁棒的性能提升，整体达到最先进的性能水平。

Conclusion: CCI框架统一了离线RL中的主要约束方法，ACPO算法能够自适应选择最优约束形式，为离线RL提供了更通用和有效的解决方案。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [189] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出使用仅两个表面肌电通道的深度学习框架，通过卷积稀疏自编码器提取特征，实现高精度手势识别，并解决跨用户差异和功能扩展问题。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制存在用户间差异大和高密度传感器阵列临床不实用的问题，需要开发更简单、高效的解决方案。

Method: 使用卷积稀疏自编码器直接从原始信号提取时间特征，避免启发式特征工程；采用少样本迁移学习处理用户差异；通过增量学习支持功能扩展。

Result: 在6类手势集上达到94.3%±0.3%的F1分数；少样本迁移学习将未见用户性能从35.1%±3.1%提升到92.3%±0.9%；增量学习扩展到10类手势集达到90.0%±0.2%的F1分数。

Conclusion: 该框架结合高精度、低计算和传感器开销，为下一代经济实惠、自适应的假肢系统提供了可扩展且高效的解决方案。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [190] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: Mem-T是一个自主记忆代理，通过层次化记忆数据库进行动态更新和多轮检索，使用MoT-GRPO强化学习框架优化长时程记忆管理，在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理的训练范式存在限制：代理需要在长时程记忆操作序列后才能获得稀疏延迟奖励，这阻碍了记忆管理策略的真正端到端优化。

Method: 提出Mem-T自主记忆代理，与轻量级层次化记忆数据库交互进行动态更新和多轮检索。进一步提出MoT-GRPO强化学习框架，通过记忆操作树反向传播和事后信用分配将稀疏终端反馈转化为密集的逐步监督。

Result: Mem-T性能优越，超越A-Mem和Mem0等框架达14.92%；经济高效，在准确率-效率帕累托前沿表现优异，相比GAM减少约24.45%的推理token消耗且不牺牲性能。

Conclusion: Mem-T通过MoT-GRPO框架实现了记忆构建和检索的联合优化，解决了长时程记忆管理中稀疏奖励问题，在性能和效率上均取得显著提升。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [191] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一个因果模型，将异常分为测量误差和机制偏移两类，通过潜在干预建模，实现可识别性，并在根因定位和异常分类上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了异常可能来自两种根本不同的过程：测量误差（数据正常生成但记录错误）和机制偏移（数据生成过程本身改变）。测量误差通常可以安全修正，而机制异常需要仔细考虑，因此需要区分这两种异常类型

Method: 定义了一个因果模型，通过将异常视为对潜在（"真实"）变量和观测（"测量"）变量的潜在干预来显式捕捉两种异常类型。证明了模型的可识别性，并提出了最大似然估计方法进行实践应用

Result: 实验表明，该方法在根因定位方面与最先进方法性能相当，同时还能准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性

Conclusion: 提出的因果模型能够有效区分测量误差和机制偏移两种异常类型，为异常根因分析提供了更全面的框架，在实际应用中具有重要价值

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [192] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致LLM生成延迟高，需要降低推理延迟同时保持准确率

Method: 训练Divide-and-Conquer CoT模型，让模型作为导演识别可并行执行的子任务，然后生成工作线程执行。采用多阶段RL算法和数据过滤策略恢复准确率

Result: 在AIME 2024和HMMT 2025等基准测试中，DC-CoT达到与DeepScaleR-1.5B-Preview相似的准确率，同时将最长路径长度降低35-40%

Conclusion: DC-CoT能有效降低长思维链推理的延迟，为低延迟并行推理提供了可行方案

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [193] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文分析了可微匹配层中离散排列恢复不稳定的根本原因——"过早模式崩溃"，并提出了自适应调度算法Efficient PH-ASC来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 可微匹配层（通常通过熵正则化最优传输实现）是结构化预测中的关键近似推理机制，但通过退火ε→0恢复离散排列时存在严重的不稳定性问题。

Method: 通过分析Sinkhorn固定点映射的非正规动力学，揭示了理论上的"热力学速度极限"，并提出了Efficient PH-ASC自适应调度算法，该算法监控推理过程的稳定性并强制执行线性稳定性定律，将计算开销从O(N³)降低到摊销O(1)。

Result: 识别了失败的根本机制——过早模式崩溃，揭示了标准指数冷却下目标后验的偏移速度超过推理算子的收缩速率，导致推理轨迹陷入虚假局部盆地。

Conclusion: 提出的Efficient PH-ASC算法能够有效解决可微匹配层中的不稳定问题，通过自适应调度避免过早模式崩溃，同时大幅降低计算开销，为结构化预测提供了更可靠的推理机制。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [194] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出基于Wasserstein GAN的密度感知条件图生成框架，用可学习的距离边预测器替代随机采样，生成具有类一致连接性的真实图结构数据。


<details>
  <summary>Details</summary>
Motivation: 传统图生成方法通常依赖固定概率的随机边采样，难以捕捉节点间复杂的结构依赖关系和类特定的连接模式，限制了生成图的结构真实性和类一致性。

Method: 1) 将节点嵌入到潜在空间，使邻近性与边似然相关；2) 使用可微边预测器从节点嵌入直接确定成对关系；3) 引入密度感知选择机制自适应控制边密度以匹配真实图的类特定稀疏分布；4) 采用带梯度惩罚的WGAN训练，使用GCN-based critic确保生成图的拓扑真实性和类分布对齐。

Result: 在基准数据集上的实验表明，该方法生成的图在结构连贯性和类一致连接性方面优于现有基线方法，学习到的边预测器能捕捉超越简单启发式的复杂关系模式，生成图的密度和拓扑与真实结构分布高度匹配。

Conclusion: 提出的密度感知条件图生成框架通过可学习的边预测器和自适应密度控制，实现了更真实、可控的图生成，提高了训练稳定性，适用于真实图生成和数据增强任务。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [195] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，将强化学习从绝对奖励转向相对排名，解决群体方法中奖励稀疏和不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法（如GRPO）依赖绝对数值奖励，存在固有局限：在可验证任务中相同群体评估导致监督稀疏，在开放任务中奖励模型分数范围不稳定影响优势估计

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个专门为群体优化设计的列表偏好模型，可直接生成相对排名

Result: 实验结果表明，RLRR在推理基准和开放生成任务上相比标准群体基线方法带来了一致的性能提升

Conclusion: 通过将原始评估转化为稳健的相对信号，RLRR有效缓解了信号稀疏和奖励不稳定的问题，为群体强化学习提供了更可靠的优化框架

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [196] [ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations](https://arxiv.org/abs/2601.23068)
*Joao Fonseca,Julia Stoyanovich*

Main category: cs.LG

TL;DR: ExplainerPFN：首个零样本Shapley值估计方法，无需访问底层模型，通过预训练在合成因果模型数据上实现特征重要性解释


<details>
  <summary>Details</summary>
Motivation: Shapley值广泛用于模型解释，但需要访问底层模型且计算成本高。现实部署中常无法访问模型，需要零样本解释方法

Method: 基于TabPFN构建表格基础模型，在随机结构因果模型生成的合成数据集上预训练，使用精确或近似Shapley值监督学习，训练后无需模型访问即可预测特征归因

Result: ExplainerPFN在真实和合成数据集上表现优异，与依赖2-10个SHAP示例的少样本代理解释器性能相当，仅需2个参考观测即可实现高保真度

Conclusion: 提出了首个零样本Shapley值估计方法，实现了无需模型访问的高效特征解释，为模型可解释性提供了新范式

Abstract: Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations.
  Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.

</details>


### [197] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow是一种基于B样条插值的流匹配算法，用于建模动态系统，相比现有方法能更好地处理不规则采样观测数据和高阶动态。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为其使用线性插值构建条件路径，无法捕捉底层状态演化，特别是在从不规则采样观测中学习高阶动态时。构建满足多边际约束的统一路径具有挑战性，因为简单的高阶多项式往往不稳定且振荡。

Method: SplineFlow利用B样条插值的平滑性和稳定性，通过B样条基函数联合建模观测间的条件路径，以结构化方式学习复杂底层动态，同时确保满足多边际约束要求。

Result: 在各种确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论基础的流匹配算法，能有效建模动态系统，特别适用于处理不规则采样观测和学习高阶动态，在多个应用场景中表现出优越性能。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [198] [RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning](https://arxiv.org/abs/2601.23075)
*Yuexin Bian,Jie Feng,Tao Wang,Yijiang Li,Sicun Gao,Yuanyuan Shi*

Main category: cs.LG

TL;DR: 用离散化分类actor替换标准高斯actor，结合正则化网络架构，在连续控制任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 标准on-policy深度强化学习使用高斯actor和浅层MLP策略，在梯度噪声大、策略更新保守时优化脆弱。本文重新审视策略表示作为on-policy优化的首要设计选择。

Method: 提出离散化分类actor，将每个动作维度表示为bin上的分布，策略目标类似于交叉熵损失。借鉴监督学习的架构进展，提出正则化actor网络，同时保持critic设计不变。

Result: 仅用离散化正则化actor替换标准actor网络，就在多样连续控制基准上获得一致性能提升，实现了最先进的性能。

Conclusion: 策略表示是on-policy优化的重要设计维度，离散化分类actor结合正则化架构能显著提升连续控制任务的性能。

Abstract: On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.

</details>


### [199] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: CATTO：一种校准感知的token级训练目标，通过将预测置信度与经验预测正确性对齐，改善LLM的置信度校准，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在token预测中经常存在置信度校准问题：高置信度预测经常出错，低置信度预测反而可能是正确的。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。

Method: 提出CATTO（校准感知token级训练目标），这是一个校准感知的目标函数，将预测置信度与经验预测正确性对齐，可以与原始偏好优化目标结合使用。还引入了Confidence@k，一种利用校准后token概率进行贝叶斯最优输出token选择的测试时缩放机制。

Result: 相比DPO，CATTO将预期校准误差（ECE）降低了2.22%-7.61%（分布内）和1.46%-10.44%（分布外）；相比最强的DPO基线，降低了0.22%-1.24%（分布内）和1.23%-5.07%（分布外）。置信度改进的同时没有牺牲任务准确性，在五个数据集的多选题回答准确性上保持或略有提升。

Conclusion: CATTO有效改善了LLM的置信度校准问题，在保持任务性能的同时显著降低了校准误差，为解决LLM置信度与正确性不匹配问题提供了有效方案。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [200] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出进化预测（EF）范式，解决直接预测（DF）在长时序预测中因输出与评估视野刚性耦合导致的重复训练问题，发现短视野训练模型结合EF能超越长视野直接训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有直接预测范式在长时序预测中占主导，但其将输出与评估视野刚性耦合，导致每次改变预测视野都需要重新训练，计算成本高昂。作者发现直接预测存在优化异常，即来自遥远未来的冲突梯度会破坏局部动态的学习。

Method: 提出进化预测（EF）范式作为统一的生成框架，证明直接预测只是EF的退化特例。EF允许模型在短视野上训练，然后通过进化推理自主扩展预测，避免直接预测中的梯度冲突问题。

Result: 实验表明，单一EF模型在标准基准测试中超越任务特定的直接预测集成模型，并在极端外推中表现出稳健的渐近稳定性，验证了EF范式的优越性。

Conclusion: 这项工作推动了长时序预测的范式转变：从被动的静态映射转向自主的进化推理，为时序预测提供了更高效、更稳定的新框架。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [201] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出DCR方法，通过精确推导非一致性分数的分布来构建更高效的排名预测集，相比基线方法平均减少36%的预测集大小，同时保持有效覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排名方法使用非一致性分数的上界，导致预测集过于保守和庞大。需要更高效的方法来量化排名模型的不确定性，以支持实际应用中的安全部署。

Method: 提出分布感知的保形排名(DCR)，发现校准项目的绝对排名在给定相对排名条件下服从负超几何分布，利用该分布推导非一致性分数的精确分布，从而确定保形阈值。

Result: DCR在理论保证下比基线方法显著提高效率，实验显示平均预测集大小减少高达36%，同时保持有效的覆盖率。

Conclusion: DCR通过精确建模非一致性分数的分布，为排名不确定性量化提供了更高效的保形预测方法，在保持覆盖率的同时大幅减小预测集规模。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [202] [Regularisation in neural networks: a survey and empirical analysis of approaches](https://arxiv.org/abs/2601.23131)
*Christiaan P. Opperman,Anna S. Bosman,Katherine M. Malan*

Main category: cs.LG

TL;DR: 该研究系统回顾了神经网络正则化技术，提出了四类分类法，并通过实证研究发现正则化效果具有数据集依赖性，挑战了"正则化总能提升性能"的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在许多任务上取得了巨大成功，但它们在泛化到未见数据方面仍存在困难。虽然已有多种正则化技术被提出并作为常规实践使用，但人们普遍假设任何添加到流程中的正则化都会带来性能提升。本研究旨在验证这一假设是否在实践中成立。

Method: 1. 对正则化技术进行广泛回顾，包括现代理论如双下降现象；2. 提出四类分类法：数据策略、架构策略、训练策略和损失函数策略；3. 在10个数值和图像数据集上对多层感知机和卷积神经网络进行各种正则化技术的实证比较。

Result: 实证结果显示正则化的效果具有数据集依赖性：正则化项仅在数值数据集上改善了性能，而批归一化仅在图像数据集上改善了性能。不同方法之间存在矛盾和对应关系，需要根据具体数据集选择适当方法。

Conclusion: 泛化对机器学习至关重要，理解正则化技术的影响及其之间的联系对于在实践中适当使用这些方法至关重要。正则化并非总是有效，其效果取决于具体的数据集特征。

Abstract: Despite huge successes on a wide range of tasks, neural networks are known to sometimes struggle to generalise to unseen data. Many approaches have been proposed over the years to promote the generalisation ability of neural networks, collectively known as regularisation techniques. These are used as common practice under the assumption that any regularisation added to the pipeline would result in a performance improvement. In this study, we investigate whether this assumption holds in practice. First, we provide a broad review of regularisation techniques, including modern theories such as double descent. We propose a taxonomy of methods under four broad categories, namely: (1) data-based strategies, (2) architecture strategies, (3) training strategies, and (4) loss function strategies. Notably, we highlight the contradictions and correspondences between the approaches in these broad classes. Further, we perform an empirical comparison of the various regularisation techniques on classification tasks for ten numerical and image datasets applied to the multi-layer perceptron and convolutional neural network architectures. Results show that the efficacy of regularisation is dataset-dependent. For example, the use of a regularisation term only improved performance on numeric datasets, whereas batch normalisation improved performance on image datasets only. Generalisation is crucial to machine learning; thus, understanding the effects of applying regularisation techniques, and considering the connections between them is essential to the appropriate use of these methods in practice.

</details>


### [203] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: GRPO的方差归一化通过自适应梯度缩放改善收敛速度，理论证明其优于未归一化的REINFORCE，实验揭示了训练三阶段动态


<details>
  <summary>Details</summary>
Motivation: GRPO作为语言模型推理的RL标准算法，使用方差归一化但缺乏理论解释。本文旨在阐明方差归一化为何及何时有效

Method: 从序列级策略梯度的局部曲率角度分析，将标准差归一化视为自适应梯度。理论分析收敛速率，并在GSM8K和MATH基准上进行实证分析

Result: 理论证明GRPO在温和条件下比未归一化REINFORCE有严格改进的收敛速率。实验揭示三个训练阶段：早期加速、稳定过渡、晚期增益受限

Conclusion: 方差归一化通过自适应梯度缩放帮助GRPO，训练动态受特征正交性和奖励方差交互作用支配，为无critic RL算法设计提供见解

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>


### [204] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT是一种时空图注意力网络框架，用于检测能源物联网设备中的时间异常（时钟漂移、同步偏移、Y2K38溢出等），通过结合漂移感知时间嵌入和图注意力机制，在受控时间扰动实验中达到95.7%准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的时间完整性对能源网络（智能电网、微电网）的可靠运行至关重要，但现有系统易受时钟漂移、时间同步操纵和Y2K38溢出等时间异常影响，而传统基于可靠时间戳的异常检测模型无法捕捉这些时间不一致性。

Method: 提出STGAT框架：1) 使用漂移感知时间嵌入和时间自注意力捕捉单个设备的时间演化；2) 利用图注意力建模时间错误的空间传播；3) 通过曲率正则化潜在表示几何分离正常时钟演化和异常模式。

Result: 在受控时间扰动的能源物联网遥测数据实验中，STGAT达到95.7%准确率，显著优于循环、Transformer和图基线模型（d > 1.8, p < 0.001），检测延迟减少26%（2.3个时间步），在溢出、漂移和物理不一致情况下保持稳定性能。

Conclusion: STGAT能有效检测能源物联网中的时间异常，通过建模时间扭曲和设备间一致性，为时钟漂移、同步偏移和Y2K38溢出等问题提供鲁棒解决方案，在准确率和延迟方面均优于现有方法。

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [205] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学局限性，使新分布支持空间维度匹配环境空间，同时隐含底层流形几何。


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域中经常遇到样本受等式约束的分布建模问题，存在固有的数学局限性，需要开发计算成本低、数学上合理且高度灵活的分布修改方法来克服这些已知缺陷。

Method: 提出约束感知的数据扰动方法，以约束感知的方式扰动数据分布，使新分布的支持维度与环境空间维度匹配，同时仍隐含地结合底层流形几何结构。

Result: 通过理论分析和多个代表性任务的实证证据表明，该方法能够一致地实现数据分布恢复和稳定采样，适用于扩散模型和归一化流模型。

Conclusion: 该方法为等式约束生成模型提供了一种有效的解决方案，能够克服数学局限性，实现稳定采样和分布恢复，具有广泛的应用潜力。

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [206] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出了Behemoth框架，通过完全合成的数据生成来研究模型编辑效果，发现限制更新秩在某些情况下能产生更有效的更新。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际应用中的部署增加，模型编辑（调整模型权重以修改特定事实的输出）变得重要。然而，当前方法存在脆弱性和不完整性，且编辑效果受训练数据分布影响，但真实世界数据难以理解这种关系。

Method: 提出Behemoth框架，使用完全合成的数据生成方法。通过在简单的表格数据上下文中探索模型编辑，研究训练数据分布与模型存储方式之间的交互关系。

Result: 发现了一些令人惊讶的结果，在某些情况下限制更新秩能产生更有效的更新，这与真实世界的结果相呼应。

Conclusion: Behemoth框架为理解模型编辑与训练数据分布的关系提供了实用见解，有助于更可靠地执行模型编辑。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [207] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 该研究使用深度强化学习框架，基于MIMIC-IV数据库中47,144次ICU住院数据，训练药物剂量策略，发现仅优化短期疼痛管理的策略与死亡率正相关，而同时考虑疼痛和长期生存率的策略与死亡率负相关。


<details>
  <summary>Details</summary>
Motivation: ICU疼痛管理面临治疗目标与患者安全之间的复杂权衡，现有强化学习方法存在两个问题：1) 优化目标不重视患者生存率；2) 算法不适合不完全信息环境。本研究旨在探讨这些设计选择的风险。

Method: 开发深度强化学习框架，在部分可观测环境下提供每小时药物剂量建议。使用MIMIC-IV数据库中47,144次ICU住院数据，训练两种策略：1) 仅减少疼痛；2) 联合减少疼痛和死亡率。药物包括阿片类、丙泊酚、苯二氮䓬类和右美托咪定。

Result: 两种策略都能降低疼痛，但仅优化疼痛的策略与死亡率呈正相关，而同时考虑疼痛和死亡率的策略与死亡率呈负相关。这表明重视长期结果对于制定更安全的治疗策略至关重要。

Conclusion: 即使短期目标是主要目标，重视长期结果对于制定更安全的ICU疼痛管理策略至关重要。仅优化短期疼痛可能无意中增加死亡风险，而联合优化能产生更安全的治疗策略。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [208] [SPICE: Submodular Penalized Information-Conflict Selection for Efficient Large Language Model Training](https://arxiv.org/abs/2601.23155)
*Powei Chang,Jinpeng Zhang,Bowen Chen,Chenyu Wang,Chenlu Guo,Yixing Zhang,Yukang Gao,JianXiang Xiang,Yue Gao,Chaoqun Sun,Yiyi Chen,Dongying Kong*

Main category: cs.LG

TL;DR: SPICE：一种基于冲突感知的数据选择方法，通过最大化Fisher信息的同时惩罚梯度冲突，仅用10%数据就能达到或超过全数据微调的性能。


<details>
  <summary>Details</summary>
Motivation: 基于信息的数据选择虽然理论上具有单调子模性质，但在实践中发现梯度冲突（样本梯度之间的不对齐）会减缓边际信息增益的衰减，阻碍信息选择的有效性。

Method: 提出ε-分解理论量化与理想子模性的偏差，基于此开发SPICE选择器：最大化Fisher信息的对数行列式，同时惩罚梯度冲突，支持早停和代理模型以提高效率。

Result: SPICE选择的子集比原始标准具有更高的对数行列式信息，在8个基准测试中，仅使用10%数据就能匹配或超过包括全数据微调在内的6种方法。

Conclusion: 梯度冲突是信息选择效率的关键瓶颈，SPICE通过冲突感知选择显著提升数据效率，以极低的训练成本实现性能改进。

Abstract: Information-based data selection for instruction tuning is compelling: maximizing the log-determinant of the Fisher information yields a monotone submodular objective, enabling greedy algorithms to achieve a $(1-1/e)$ approximation under a cardinality budget. In practice, however, we identify alleviating gradient conflicts, misalignment between per-sample gradients, is a key factor that slows down the decay of marginal log-determinant information gains, thereby preventing significant loss of information. We formalize this via an $\varepsilon$-decomposition that quantifies the deviation from ideal submodularity as a function of conflict statistics, yielding data-dependent approximation factors that tighten as conflicts diminish. Guided by this analysis, we propose SPICE, a conflict-aware selector that maximizes information while penalizing misalignment, and that supports early stopping and proxy models for efficiency. Empirically, SPICE selects subsets with higher log-determinant information than original criteria, and these informational gains translate into performance improvements: across 8 benchmarks with LLaMA2-7B and Qwen2-7B, SPICE uses only 10% of the data, yet matches or exceeds 6 methods including full-data tuning. This achieves performance improvements with substantially lower training cost.

</details>


### [209] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种基于语法的无监督技能分割与层次结构发现方法，在像素环境中自动分割轨迹并构建技能层次，提升下游强化学习性能


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖动作标签、奖励或人工标注，限制了在无监督场景下的应用。需要一种能够自动从无标签轨迹中发现可重用技能和层次结构的方法。

Method: 采用基于语法的技能分割方法，从无标签轨迹中自动分割技能，并诱导出层次结构。该方法能够捕捉低层行为及其组合成高层技能的过程。

Result: 在Craftax和完整版Minecraft等高维像素环境中评估，在技能分割、重用和层次质量指标上均优于现有基线，产生更具结构和语义意义的层次。

Conclusion: 提出的无监督技能分割和层次发现方法有效，发现的层次结构能够加速和稳定下游强化学习任务的学习过程。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [210] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探测大语言模型推理轨迹的协议，通过截断推理轨迹并重新注入模型来测量答案分布变化，发现准确率和决策确定性随推理token比例增加而提升，且主要受内容而非长度或风格影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在给出最终答案前会生成"推理轨迹"，但尚不清楚准确率和决策承诺如何随推理轨迹演变，以及中间轨迹段是否提供超出长度或风格效应的答案相关信息。

Method: 提出三步协议：1)生成模型的推理轨迹；2)在固定token百分比处截断；3)将每个部分轨迹重新注入模型（或不同模型），通过下一个token概率测量诱导的答案选择分布。

Result: 准确率和决策确定性随提供的推理token比例增加而一致提升；这些增益主要由模型生成的相关内容驱动，而非上下文长度或通用"推理风格"效应；更强模型常能从错误的部分轨迹成功回溯，但即时答案常锚定在较弱模型的错误响应中。

Conclusion: 轨迹探测为推理模型的高效和安全部署提供诊断工具，测量结果可指导实用的轨迹处理和监控策略，提高可靠性，无需假设中间token是固有的忠实解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [211] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 该论文研究了带有参数噪声的随机线性老虎机问题，提出了新的遗憾上下界，并展示了在某些动作集上简单的探索-利用算法可以达到接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究带有参数噪声的随机线性老虎机问题，其中奖励参数θ是独立同分布的随机变量。传统的加性噪声模型已有深入研究，但参数噪声模型的理论理解相对较少，需要探索其最优遗憾界和高效算法。

Method: 1. 分析一般动作集（大小为K，维度为d）的遗憾上界：使用置信区间方法得到Õ(√(dT log(K/δ)σ²_max))
2. 提供下界：Ω̃(d√(Tσ²_max))，当log(K)≈d时紧致
3. 针对特定动作集（ℓ_p单位球，p≤2）：分析最小最大遗憾界为Θ̃(√(dTσ²_q))
4. 提出简单的探索-利用算法实现最优遗憾界

Result: 1. 一般动作集：上界Õ(√(dT log(K/δ)σ²_max))，下界Ω̃(d√(Tσ²_max))，当log(K)≈d时紧致
2. ℓ_p单位球（p≤2）：最小最大遗憾界Θ̃(√(dTσ²_q))，其中σ²_q≤4
3. 简单的探索-利用算法可以达到最优遗憾界（对数因子内）

Conclusion: 参数噪声模型与经典加性噪声模型有本质不同：在ℓ_p单位球上，参数噪声模型的遗憾界为Θ̃(√(dT))，而加性噪声模型为Θ(d√(T))。令人惊讶的是，简单的探索-利用算法就能达到接近最优的性能，这与加性噪声模型需要复杂算法形成对比。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [212] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出一种新型Transformer机制，能够处理可互换标记（如绑定变量），在保持语义不变的情况下对标记重命名具有不变性，显著提升开放词汇任务中未见符号的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络架构缺乏处理可互换标记（语义等价但可区分的符号，如绑定变量）的原则性方法。模型在固定词汇表上训练后，即使底层语义保持不变，也难以泛化到未见符号。

Method: 提出基于Transformer的新机制：使用并行嵌入流来隔离每个可互换标记在输入中的贡献，结合聚合注意力机制实现跨流的结构化信息共享。该方法在理论上对可互换标记的重命名具有不变性。

Result: 实验结果证实了该方法的理论保证，并在需要泛化到新符号的开放词汇任务上展示了显著的性能提升。

Conclusion: 提出的Transformer机制能够有效处理可互换标记，解决了神经网络在处理语义等价但可区分符号时的泛化问题，为开放词汇任务提供了更强大的解决方案。

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [213] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词，相比固定帧率编解码器使用更少的token同时保持竞争性的语音重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常以固定帧率运行，在时间上均匀分配token，产生不必要的长序列。这限制了LLM处理语音数据的效率。

Method: DyCAST通过软字符级对齐和显式时长建模实现可变帧率分词。训练时学习将token与字符级语言单元关联，推理时支持无需对齐的直接时长控制。还引入了检索增强解码机制，在低帧率下提高重建质量而不增加比特率。

Result: 实验表明，DyCAST在显著减少token使用量的同时，实现了竞争性的语音重建质量和下游任务性能。

Conclusion: DyCAST提供了一种高效的动态语音分词方法，通过字符对齐和时长建模优化token分配，为LLM处理语音数据提供了更有效的表示。

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [214] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合了Transformers的全局建模能力和MeshGraphNets的几何归纳偏置，解决了标准MGN在大规模高分辨率网格上长距离信息传播效率低的问题，在工业规模网格上实现了高效的物理模拟。


<details>
  <summary>Details</summary>
Motivation: 标准MeshGraphNets (MGN) 在处理大规模高分辨率网格时存在关键限制：基于迭代消息传递的机制导致长距离信息传播效率低下，需要深层消息传递堆栈或分层粗化网格，这限制了其在工业规模应用中的实用性。

Method: MGN-T 架构结合了Transformers的全局处理能力和MeshGraphNets的几何归纳偏置。使用物理注意力Transformer作为全局处理器，同时更新所有节点状态，同时显式保留节点和边属性。该方法直接捕获长距离物理相互作用，无需深层消息传递堆栈或分层粗化网格。

Result: MGN-T 成功处理了工业规模的冲击动力学网格，而标准MGN因消息传递不足而失败。该方法准确模拟了自接触、塑性和多变量输出（包括内部现象学塑性变量）。在经典基准测试中优于最先进方法，以更少的参数实现更高精度并保持实际效率。

Conclusion: MGN-T 通过结合Transformers的全局建模和MeshGraphNets的几何归纳偏置，有效解决了大规模高分辨率网格上的长距离信息传播问题，为工业规模物理模拟提供了高效准确的解决方案，在保持网格表示的同时显著提升了性能。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [215] [TriSpec: Ternary Speculative Decoding via Lightweight Proxy Verification](https://arxiv.org/abs/2601.23180)
*Haoyun Jiang,Junqi He,Feng Hong,Xinlong Yang,Jianwei Zhang,Zheng Li,Zhengyang Zhuge,Zhiyong Chen,Bo Han,Junyang Lin,Jiangchao Yao*

Main category: cs.LG

TL;DR: TriSpec提出了一种三元推测解码框架，通过轻量级代理减少验证成本，在保持准确性的同时实现高达35%的加速


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归生成在推理任务中效率受限，现有推测解码方法在草案有效性方面已接近饱和，需要从验证成本角度寻求进一步优化

Method: TriSpec采用三元推测解码框架，引入轻量级代理模型，仅对不确定的token调用完整目标模型，可与其他先进推测解码方法（如EAGLE-3）结合

Result: 在Qwen3和DeepSeek-R1-Distill-Qwen/LLaMA系列模型上，TriSpec相比标准推测解码实现高达35%的加速，目标模型调用减少50%，同时保持可比的准确性

Conclusion: 通过专注于降低验证成本，TriSpec为推测解码提供了新的优化维度，显著提升了大型语言模型的推理效率

Abstract: Inference efficiency in Large Language Models (LLMs) is fundamentally limited by their serial, autoregressive generation, especially as reasoning becomes a key capability and response sequences grow longer. Speculative decoding (SD) offers a powerful solution, providing significant speed-ups through its lightweight drafting and parallel verification mechanism. While existing work has nearly saturated improvements in draft effectiveness and efficiency, this paper advances SD from a new yet critical perspective: the verification cost. We propose TriSpec, a novel ternary SD framework that, at its core, introduces a lightweight proxy to significantly reduce computational cost by approving easily verifiable draft sequences and engaging the full target model only when encountering uncertain tokens. TriSpec can be integrated with state-of-the-art SD methods like EAGLE-3 to further reduce verification costs, achieving greater acceleration. Extensive experiments on the Qwen3 and DeepSeek-R1-Distill-Qwen/LLaMA families show that TriSpec achieves up to 35\% speedup over standard SD, with up to 50\% fewer target model invocations while maintaining comparable accuracy.

</details>


### [216] [Ensuring Semantics in Weights of Implicit Neural Representations through the Implicit Function Theorem](https://arxiv.org/abs/2601.23181)
*Tianming Qiu,Christos Sonis,Hao Shen*

Main category: cs.LG

TL;DR: 该论文利用隐函数定理建立了数据空间与权重表示空间之间的严格映射，为理解神经网络权重如何编码数据语义提供了理论框架。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习是一个新兴领域，但缺乏对权重如何编码数据语义的精确理论解释。特别是隐式神经表示提供了一个方便的测试平台，但需要理论支持来解释其工作机制。

Method: 使用隐函数定理建立数据空间与权重表示空间之间的严格映射。通过共享超网络将实例特定嵌入映射到INR权重，并在2D和3D数据集的下游分类任务中进行验证。

Result: 该方法在2D和3D数据集的下游分类任务中取得了与现有基线竞争的性能，证明了理论框架的有效性。

Conclusion: 该工作为网络权重的研究提供了理论视角，为未来权重空间学习的进一步研究奠定了基础。

Abstract: Weight Space Learning (WSL), which frames neural network weights as a data modality, is an emerging field with potential for tasks like meta-learning or transfer learning. Particularly, Implicit Neural Representations (INRs) provide a convenient testbed, where each set of weights determines the corresponding individual data sample as a mapping from coordinates to contextual values. So far, a precise theoretical explanation for the mechanism of encoding semantics of data into network weights is still missing. In this work, we deploy the Implicit Function Theorem (IFT) to establish a rigorous mapping between the data space and its latent weight representation space. We analyze a framework that maps instance-specific embeddings to INR weights via a shared hypernetwork, achieving performance competitive with existing baselines on downstream classification tasks across 2D and 3D datasets. These findings offer a theoretical lens for future investigations into network weights.

</details>


### [217] [Learning to Execute Graph Algorithms Exactly with Graph Neural Networks](https://arxiv.org/abs/2601.23207)
*Muhammad Fetrat Qharabagh,Artur Back de Luca,George Giapitzakis,Kimon Fountoulakis*

Main category: cs.LG

TL;DR: 该论文证明了在图神经网络中，通过训练MLP集合学习节点本地指令，结合NTK理论，可以在有限精度和有限度约束下精确学习图算法。


<details>
  <summary>Details</summary>
Motivation: 理解图神经网络能学习什么，特别是它们执行算法的能力，是一个核心理论挑战。当前缺乏对图算法在有限精度和有限度约束下的精确可学习性理论分析。

Method: 采用两步法：1) 训练多层感知机(MLP)集合来学习单个节点的本地指令；2) 在推理时，将训练好的MLP集合作为图神经网络(GNN)的更新函数。利用神经正切核(NTK)理论证明本地指令可以从小训练集中学习。

Result: 证明了在LOCAL分布式计算模型下的严格可学习性结果，并展示了消息洪泛、广度优先搜索、深度优先搜索、Bellman-Ford等经典算法的正可学习性结果，能够在推理时无错误地执行完整图算法。

Conclusion: 该工作为图神经网络学习图算法提供了理论保证，证明了在有限精度和有限度约束下，图神经网络能够精确学习并执行分布式图算法，为理解GNN的计算能力提供了理论基础。

Abstract: Understanding what graph neural networks can learn, especially their ability to learn to execute algorithms, remains a central theoretical challenge. In this work, we prove exact learnability results for graph algorithms under bounded-degree and finite-precision constraints. Our approach follows a two-step process. First, we train an ensemble of multi-layer perceptrons (MLPs) to execute the local instructions of a single node. Second, during inference, we use the trained MLP ensemble as the update function within a graph neural network (GNN). Leveraging Neural Tangent Kernel (NTK) theory, we show that local instructions can be learned from a small training set, enabling the complete graph algorithm to be executed during inference without error and with high probability. To illustrate the learning power of our setting, we establish a rigorous learnability result for the LOCAL model of distributed computation. We further demonstrate positive learnability results for widely studied algorithms such as message flooding, breadth-first and depth-first search, and Bellman-Ford.

</details>


### [218] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究开发了一种利用实时交通数据预测空气污染的新方法，通过将彩色交通地图转换为同心环描述来表征交通状况，并使用偏最小二乘回归预测污染水平。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染是全球性慢性问题，交通是主要污染源。现有空气质量监测和预报在时空上较为粗糙，而实时交通数据则更精细且公开可用。研究旨在利用交通数据提供超本地化、动态的空气质量预报。

Method: 将彩色交通地图转换为基于同心环的交通强度描述，使用偏最小二乘回归建立污染水平预测模型，通过不同训练样本优化模型性能。

Result: 开发了创新的交通强度表征方法，建立了交通与空气污染的预测模型，工作流程简单且可适应其他城市环境。

Conclusion: 该研究提供了一种利用公开可用的精细交通数据预测空气质量的有效方法，为超本地化、动态空气污染预报提供了可行方案，具有跨城市应用的潜力。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [219] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究众包标注中的公平性问题，分析了多数投票和贝叶斯最优聚合的公平性差距，提出了收敛保证和离散场景下的公平性后处理方法。


<details>
  <summary>Details</summary>
Motivation: 获取可靠的真实标签通常成本高昂或不可行，因此众包和聚合嘈杂的人工标注成为常用方法。然而，聚合主观标签可能放大个体偏见，特别是在敏感特征方面，引发公平性担忧。目前众包聚合中的公平性问题尚未充分探索，缺乏收敛保证，且仅有有限的ε-公平性后处理方法。

Method: 1. 在ε-公平性框架下分析多数投票和最优贝叶斯聚合的公平性差距；2. 在小众包场景中推导多数投票公平性差距的上界；3. 证明聚合共识的公平性差距在可解释条件下以指数速度收敛到真实标签的公平性差距；4. 将最先进的多类别公平性后处理算法从连续场景推广到离散场景，对任何聚合规则强制执行严格的人口统计均等约束。

Result: 1. 理论分析表明聚合共识的公平性差距以指数速度收敛到真实标签的公平性差距；2. 在合成和真实数据集上的实验验证了方法的有效性，并证实了理论见解；3. 提出的后处理方法能够强制执行严格的人口统计均等约束。

Conclusion: 该研究填补了众包聚合中公平性分析的空白，提供了理论收敛保证和实用的后处理方法，为解决众包标注中的公平性问题提供了系统性的解决方案。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [220] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN是一种基于样条的自适应网络，用于深度强化学习，相比传统MLP在资源受限环境中实现了30-50%的样本效率提升和1.3-9倍的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在资源受限环境中部署时，传统的多层感知机(MLP)存在参数效率低下的问题，因为其对许多价值函数的平滑结构具有不完美的归纳偏置。这种不匹配会阻碍样本效率并减缓策略学习。现有的模型压缩技术是事后操作，无法改善学习效率。

Method: SPAN通过整合可学习的预处理层和可分离的张量积B样条基函数，改进了低秩KHRONOS框架，形成了一种基于样条的自适应网络函数逼近方法。

Result: 在离散(PPO)和高维连续(SAC)控制任务以及离线设置(Minari/D4RL)中，SPAN相比MLP基线实现了30-50%的样本效率提升和1.3-9倍的成功率提升。同时表现出更好的实时性能和超参数变化的鲁棒性。

Conclusion: SPAN作为一种可行的高性能替代方案，能够在资源受限环境中学习内在高效的策略，解决了传统MLP在强化学习中参数效率低下的问题。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [221] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: SDG：一种新颖的序列级扩散框架，将动态图学习与生成式去噪统一，用于时序链接预测，能捕捉更全面的交互分布和不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有时序图神经网络主要关注历史交互表示学习，虽然性能强但仍是纯判别式模型，只能产生未来链接的点估计，缺乏捕捉未来时序交互不确定性和序列结构的显式机制。

Method: 提出SDG框架，将噪声注入整个历史交互序列，通过条件去噪过程联合重构所有交互嵌入；采用交叉注意力去噪解码器指导目标序列重构，并以端到端方式优化模型。

Result: 在多个时序图基准测试上的广泛实验表明，SDG在时序链接预测任务中始终达到最先进的性能。

Conclusion: SDG成功将动态图学习与生成式去噪统一，能够更好地捕捉交互分布和不确定性，为时序链接预测提供了有效的生成式解决方案。

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [222] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 将Transformer层解释为优化算法迭代，自注意力对应交互能量梯度步，MLP对应势能梯度步，GPT式Transformer是复合目标的梯度下降，此视角启发了Nesterov加速Transformer设计


<details>
  <summary>Details</summary>
Motivation: 为Transformer架构提供优化理论解释，将自注意力和MLP层统一理解为优化算法的迭代步骤，从而能够基于经典优化理论进行有原则的架构设计

Method: 提出变分框架，将自注意力解释为交互能量的梯度步，MLP解释为势能的梯度步，标准GPT式Transformer对应复合目标的梯度下降（通过Lie-Trotter分裂实现），并基于此设计Nesterov加速Transformer

Result: 在TinyStories和OpenWebText数据集上，Nesterov加速Transformer持续优于nanoGPT基线，表明优化理论洞察能带来实际性能提升

Conclusion: Transformer层可解释为优化算法迭代，这一优化理论视角为架构设计提供了原则性指导，并能实际提升模型性能

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [223] [How well do generative models solve inverse problems? A benchmark study](https://arxiv.org/abs/2601.23238)
*Patrick Krüger,Patrick Materne,Werner Krebs,Hanno Gottschalk*

Main category: cs.LG

TL;DR: 比较传统贝叶斯逆问题方法与三种生成式学习方法在燃气轮机燃烧室设计中的应用，发现条件流匹配方法表现最佳


<details>
  <summary>Details</summary>
Motivation: 生成式学习能够基于低维条件生成高维数据，适合解决贝叶斯逆问题。本文旨在比较传统方法与现代生成式学习方法在燃气轮机燃烧室设计中的表现。

Method: 比较了四种方法：1）基于前向回归模型和马尔可夫链蒙特卡洛采样的传统贝叶斯方法；2）条件生成对抗网络；3）可逆神经网络；4）条件流匹配。应用于燃气轮机燃烧室设计，将6个设计参数映射到3个性能指标。

Result: 提出了多个评估指标来衡量生成设计的准确性和多样性，并研究了训练数据集大小对性能的影响。条件流匹配方法在所有竞争方法中表现最佳。

Conclusion: 条件流匹配方法在燃气轮机燃烧室逆设计问题中优于传统贝叶斯方法、条件生成对抗网络和可逆神经网络，是解决此类逆问题的有效方法。

Abstract: Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.

</details>


### [224] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 该论文在完全放松实现在性假设的条件下，研究语言识别和生成任务，提出了新的目标函数并获得了紧致的统计率。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成研究通常基于强实现在性假设，即输入数据必须来自给定语言集合中的某个未知分布。本文旨在完全放松这一限制性假设，研究更一般的"不可知"设置下的语言任务。

Method: 在完全无实现在性假设的条件下，提出了新的目标函数来研究语言识别和生成问题，不限制输入数据的分布。

Result: 获得了语言识别和生成问题的新颖特征刻画，并得到了几乎紧致的统计率。

Conclusion: 在完全放松实现在性假设的更一般设置下，本文为语言识别和生成任务提供了理论框架和紧致的统计保证。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [225] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: TEON是Muon优化器的推广，通过将神经网络梯度建模为结构化高阶张量，实现跨层正交化，相比层级Muon有更好的收敛保证和性能表现。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在预训练大语言模型中表现出色，但仅在各层内部进行矩阵级梯度正交化。作者希望将正交化扩展到跨层范围，以进一步提升优化效果。

Method: 提出TEON方法，将神经网络梯度建模为结构化高阶张量，实现跨层正交化。基于理论分析开发了实用的TEON实例，并在GPT风格（130M-774M参数）和LLaMA风格（60M-1B参数）模型上进行评估。

Result: TEON在所有模型规模上都持续改善了训练和验证困惑度，在各种近似SVD方案下表现出强大的鲁棒性。

Conclusion: TEON作为Muon的推广，通过跨层梯度正交化提供了更好的优化性能，为大规模语言模型训练提供了有效的优化方法。

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [226] [Particle-Guided Diffusion Models for Partial Differential Equations](https://arxiv.org/abs/2601.23262)
*Andrew Millard,Fredrik Lindsten,Zheng Zhao*

Main category: cs.LG

TL;DR: 提出一种结合扩散模型与物理约束的引导随机采样方法，通过PDE残差和观测约束确保生成样本的物理合理性，并嵌入SMC框架构建可扩展的生成式PDE求解器


<details>
  <summary>Details</summary>
Motivation: 现有生成方法在求解偏微分方程时可能产生物理不可行的解，需要一种能确保物理合理性的生成式求解方法

Method: 基于PDE残差和观测约束的引导随机采样方法，结合扩散模型，并嵌入顺序蒙特卡洛(SMC)框架构建生成式PDE求解器

Result: 在多个基准PDE系统以及多物理场和相互作用PDE系统中，该方法产生的解场数值误差低于现有最先进的生成方法

Conclusion: 该方法成功将物理约束融入生成过程，构建了可扩展的生成式PDE求解器，在保持物理合理性的同时提高了求解精度

Abstract: We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.

</details>


### [227] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS是一个针对扩散大语言模型的推理系统，通过动态聚焦计算于可解码token并实时淘汰不可解码token，显著提升吞吐量3.52倍，同时保持或改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）虽然提供了自回归模型的有力替代方案，但其部署受到高解码成本的限制。研究发现DLLM解码存在关键低效问题：虽然计算在token块上并行化，但每个扩散步骤中只有一小部分token是可解码的，导致大部分计算浪费在不可解码token上。

Method: 基于注意力机制得出的token重要性与token级解码概率之间的强相关性，提出FOCUS推理系统。该系统通过动态聚焦计算于可解码token，并实时淘汰不可解码token，增加有效批处理大小，缓解计算限制并实现可扩展的吞吐量。

Result: 实证评估表明，FOCUS相比生产级引擎LMDeploy实现了高达3.52倍的吞吐量提升，同时在多个基准测试中保持或改善了生成质量。

Conclusion: FOCUS系统有效解决了DLLM解码的计算效率问题，通过智能token选择机制显著提升推理性能，为扩散大语言模型的实用部署提供了可行的解决方案。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [228] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架，通过解耦设计解决逆PDE问题：无条件扩散学习系数先验，神经算子显式建模前向PDE指导，实现数据高效和物理感知的生成，避免联合模型的指导衰减问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散后验采样的方法通过联合系数-解建模隐式表示物理，需要大量配对监督数据。在数据稀缺时，这些方法存在指导衰减问题，且容易过度平滑。

Method: 提出解耦扩散逆求解器（DDIS）：1）无条件扩散模型学习系数先验分布；2）神经算子显式建模前向PDE物理约束；3）结合解耦退火后验采样（DAPS）避免过度平滑。

Result: 在稀疏观测下达到SOTA性能：平均提升l2误差11%，谱误差54%；数据仅1%时，相比联合模型在l2误差上保持40%优势；理论证明避免指导衰减问题。

Conclusion: DDIS通过解耦设计实现数据高效、物理感知的逆PDE求解，在数据稀缺时显著优于联合建模方法，为逆问题提供更鲁棒的生成框架。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [229] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 首个系统评估开放词汇目标检测在航拍图像上的基准研究，发现现有模型在航拍领域存在严重迁移失败，语义混淆是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测在自然图像上表现良好，但在航拍图像上的可迁移性尚未探索。本研究旨在建立首个系统基准，评估现有OVD模型在航拍领域的性能。

Method: 使用LAE-80C航拍数据集（3,592张图像，80个类别），在严格零样本条件下评估5个最先进的OVD模型。通过全局、Oracle和单类别推理模式分离语义混淆和视觉定位问题。

Result: 发现严重的领域迁移失败：最佳模型（OWLv2）仅达到27.6% F1分数，假阳性率高达69%。将词汇量从80减少到3.2类可带来15倍改进，表明语义混淆是主要瓶颈。提示工程策略无效，不同数据集间性能差异巨大。

Conclusion: 现有OVD模型在航拍图像上表现不佳，语义混淆是主要限制因素。需要开发领域自适应方法来解决航拍开放词汇目标检测的挑战。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [230] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 单层VQ-VAE在匹配表征预算和避免码本崩溃的情况下，可以达到与分层VQ-VAE相同的重建精度，挑战了分层量化在高质量重建方面具有固有优势的假设。


<details>
  <summary>Details</summary>
Motivation: 研究分层VQ-VAE是否真的在重建精度上优于单层VQ-VAE，因为高层特征完全来自低层，理论上不应包含额外的重建信息。需要隔离码本利用率和总体表征容量的影响，重新审视层次结构对重建准确性的实际贡献。

Method: 比较两层VQ-VAE和容量匹配的单层模型在ImageNet高分辨率图像上的表现。采用轻量级干预措施：从数据初始化码本、定期重置不活跃码本向量、系统调整码本超参数，以减少码本崩溃。

Result: 当表征预算匹配且码本崩溃得到缓解时，单层VQ-VAE能够匹配分层变体的重建精度。不充分的码本利用和高维嵌入会破坏量化稳定性并增加码本崩溃，但通过适当的干预措施可以显著减少崩溃。

Conclusion: 分层量化并不固有地优于单层量化用于高质量重建。在匹配表征预算和有效管理码本的情况下，单层VQ-VAE可以达到与分层模型相同的重建精度，挑战了关于层次结构必要性的传统假设。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [231] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 该论文提出MA-PaPSP方法，通过记忆增强和检索机制解决视觉语言基础模型的选择性预测问题，特别针对开放集和无限词汇任务。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法主要针对封闭集任务，而视觉语言基础模型面临从封闭到开放集、从有限到无限词汇的多样化任务，需要低复杂度、无需训练的方法。

Method: 提出记忆增强的PaPSP(MA-PaPSP)，使用检索数据集存储图像-文本对，通过检索最近邻对平均减少嵌入方差，并采用对比归一化改进分数校准。

Result: 在多个数据集上的实验表明，MA-PaPSP在选择性字幕生成、图像-文本匹配和细粒度分类任务中优于原始PaPSP和其他选择性预测基线方法。

Conclusion: MA-PaPSP通过记忆增强机制有效解决了视觉语言基础模型选择性预测中的嵌入不稳定和分数校准问题，为开放集任务提供了有效的拒绝选项。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [232] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: MIFOMO是一个用于高光谱图像跨域少样本学习的混合基础模型，通过预训练遥感基础模型、凝聚投影、混合域适应和标签平滑技术，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域少样本学习（CDFSL）方法依赖不现实的数据增强（外部噪声）来扩大样本量，参数多易过拟合，且未充分利用具有强泛化能力的基础模型。

Method: 基于预训练的遥感基础模型，引入凝聚投影（CP）快速适应下游任务，提出混合域适应（MDM）解决极端域差异问题，并采用标签平滑处理噪声伪标签。

Result: MIFOMO在实验中表现优异，比现有方法提升了高达14%的性能，代码已开源。

Conclusion: MIFOMO通过结合基础模型、凝聚投影、混合域适应和标签平滑，有效解决了高光谱图像跨域少样本学习中的数据稀缺和域差异问题。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [233] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 提出Bi-MCQ框架，通过双向多选学习增强医学视觉语言模型对否定陈述的理解能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解医学否定陈述方面表现较弱，主要因为对比对齐目标将否定视为微小语言变化而非意义反转操作，在多标签设置中进一步强化了简单正面对齐，限制了疾病缺失的有效学习

Method: 将视觉语言对齐重新定义为条件语义比较问题，通过双向多选学习框架实现，联合训练图像到文本和文本到图像的多选任务，使用肯定、否定和混合提示，并引入方向特定的交叉注意力融合模块处理双向推理的不对称线索

Result: 在ChestXray14、Open-I、CheXpert和PadChest数据集上，Bi-MCQ相比最先进的CARZero模型的零样本性能，否定理解提升高达0.47 AUC，在正负组合评估上获得高达0.08的绝对增益，相比基于InfoNCE的微调平均减少0.12的肯定-否定AUC差距

Conclusion: 目标函数重新表述可以显著增强医学视觉语言模型中的否定理解能力，Bi-MCQ框架通过条件语义比较而非全局相似性最大化，有效解决了现有模型在否定理解方面的局限性

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [234] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: FaceDefense：针对扩散式人脸交换的增强型主动防御框架，通过扩散损失和面部属性编辑实现更好的防御效果与视觉不可感知性平衡


<details>
  <summary>Details</summary>
Motivation: 扩散式人脸交换技术虽然性能优越，但也加剧了恶意人脸交换对肖像权和个人声誉的危害。现有主动防御方法面临核心权衡：大扰动会扭曲面部结构，小扰动则防御效果弱。

Method: 提出FaceDefense框架：1）引入新的扩散损失来增强对抗样本的防御效果；2）采用定向面部属性编辑来恢复扰动引起的扭曲，提高视觉不可感知性；3）设计两阶段交替优化策略生成最终扰动人脸图像。

Result: 大量实验表明，FaceDefense在不可感知性和防御效果方面显著优于现有方法，实现了更优的权衡。

Conclusion: FaceDefense通过创新的扩散损失和面部属性编辑技术，成功解决了扩散式人脸交换主动防御中的核心权衡问题，为保护肖像权和个人声誉提供了有效解决方案。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [235] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 该论文重新审视了上下文异常检测，在视觉领域操作化这一概念，提出了CAAD-3K基准数据集和基于视觉-语言表示的兼容性学习框架，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常是观测的固有属性，但现实中许多异常是上下文依赖的（同一对象在不同上下文中可能正常或异常）。现有方法未能充分建模这种上下文依赖性。

Method: 提出条件兼容性学习框架，利用视觉-语言表示来建模主体-上下文关系；引入CAAD-3K基准数据集，通过控制主体身份同时变化上下文来隔离上下文异常。

Result: 在CAAD-3K上显著优于现有方法，在MVTec-AD和VisA数据集上达到SOTA性能，证明建模上下文依赖性能够补充传统结构异常检测。

Conclusion: 上下文异常检测是视觉异常检测的重要方向，主体-上下文兼容性建模能有效提升性能；提出的框架和数据集为系统研究提供了基础。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [236] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: DINO-SAE：基于DINO的球面自编码器，通过方向对齐而非幅度匹配来保留高频细节，结合层次卷积补丁嵌入和余弦相似度对齐，在球面流形上训练DiT，实现SOTA重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉基础模型（如DINO）的生成自编码器存在重建保真度有限的问题，主要原因是高频细节丢失。对比表示中的语义信息主要编码在特征向量方向上，而强制幅度匹配会阻碍编码器保留细粒度细节。

Method: 1. 提出DINO球面自编码器（DINO-SAE）框架；2. 引入层次卷积补丁嵌入模块增强局部结构和纹理保留；3. 使用余弦相似度对齐目标强制语义一致性，同时允许灵活的特征幅度以保留细节；4. 利用SSL基础模型表示本质位于超球面上的观察，采用黎曼流匹配在球面潜在流形上直接训练扩散变换器（DiT）。

Result: 在ImageNet-1K上实现最先进的重建质量：rFID 0.37，PSNR 26.2 dB，同时保持与预训练VFM的强语义对齐。黎曼流匹配的DiT表现出高效收敛，在80个epoch时达到gFID 3.47。

Conclusion: DINO-SAE通过将语义表示与像素级重建桥接，解决了现有方法重建保真度有限的问题。通过方向对齐而非幅度匹配，结合层次特征提取和球面流形上的扩散模型训练，实现了高质量重建和语义保持。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [237] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出无训练测试时适应方法TaTa，利用布朗距离协方差动态适应视觉语言模型到新领域，无需训练或反向传播，显著降低计算成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在领域偏移下性能下降，现有测试时适应方法计算密集、依赖反向传播且多为单模态，需要更高效稳定的适应方案

Method: 使用布朗距离协方差捕捉线性和非线性依赖关系，结合属性增强提示、动态聚类和伪标签细化，无需训练或反向传播动态适应新领域

Result: 在多样化数据集上显著降低计算成本，在领域和跨数据集泛化方面达到最先进性能

Conclusion: TaTa为视觉语言模型提供了一种高效、稳定的测试时适应方法，解决了现有方法的计算密集和稳定性问题，提升了实际应用性

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [238] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: VideoGPA：通过几何偏好对齐提升视频扩散模型的3D一致性，无需人工标注


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在视觉上表现优秀，但缺乏3D结构一致性，导致物体变形和空间漂移问题。这是因为标准去噪目标缺乏对几何一致性的显式激励。

Method: 提出VideoGPA框架：1）利用几何基础模型自动生成密集偏好信号；2）通过直接偏好优化（DPO）引导视频扩散模型；3）自监督学习，无需人工标注。

Result: VideoGPA显著提升了时间稳定性、物理合理性和运动连贯性，在大量实验中一致优于最先进的基线方法，仅需少量偏好对。

Conclusion: 通过几何偏好对齐，VideoGPA有效解决了视频扩散模型的3D一致性问题，为提升视频生成质量提供了数据高效的自监督解决方案。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [239] [Computing Dominating Sets in Disk Graphs with Centers in Convex Position](https://arxiv.org/abs/2601.22609)
*Anastasiia Tkachenko,Haitao Wang*

Main category: cs.CG

TL;DR: 针对凸位置点集的圆盘图支配集问题，提出了首个多项式时间算法：无权重版本O(k²n log²n)，有权重版本O(n⁵ log²n)。


<details>
  <summary>Details</summary>
Motivation: 圆盘图的支配集问题在一般情况下是NP难的，但在点集处于凸位置的特殊情况下，是否存在多项式时间算法尚未被研究。本文旨在探索这一特殊几何约束下的算法可能性。

Method: 利用点集凸位置的特殊几何性质，设计动态规划算法。算法基于凸包上的点序结构，通过状态转移计算最小支配集。对于有权重版本，采用更复杂的动态规划状态设计。

Result: 成功证明了凸位置圆盘图支配集问题可以在多项式时间内解决，具体时间复杂度为：无权版本O(k²n log²n)，其中k是最小支配集大小；有权版本O(n⁵ log²n)。这是该问题的首个多项式时间算法。

Conclusion: 凸位置约束显著降低了圆盘图支配集问题的计算复杂度，使其从NP难问题变为多项式时间可解问题。这一结果为特殊几何约束下的图算法设计提供了新的思路。

Abstract: Given a set $P$ of $n$ points in the plane and a collection of disks centered at these points, the disk graph $G(P)$ has vertex set $P$, with an edge between two vertices if their corresponding disks intersect. We study the dominating set problem in $G(P)$ under the special case where the points of $P$ are in convex position. The problem is NP-hard in general disk graphs. Under the convex position assumption, however, we present the first polynomial-time algorithm for the problem. Specifically, we design an $O(k^2 n \log^2 n)$-time algorithm, where $k$ denotes the size of a minimum dominating set. For the weighted version, in which each disk has an associated weight and the goal is to compute a dominating set of minimum total weight, we obtain an $O(n^5 \log^2 n)$-time algorithm.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [240] [Bayesian Matrix Completion Under Geometric Constraints](https://arxiv.org/abs/2601.22765)
*Rohit Varma Chiluvuri,Santosh Nannuru*

Main category: eess.SP

TL;DR: 论文提出了一种层次贝叶斯框架，通过直接在生成欧氏距离矩阵的潜在点集上施加结构化先验，改进了稀疏噪声观测下的欧氏距离矩阵补全问题。


<details>
  <summary>Details</summary>
Motivation: 欧氏距离矩阵补全是信号处理中的基本挑战，应用于传感器网络定位、声学房间重建等领域。传统方法（如秩约束优化和半定规划）虽然强制执行几何约束，但在稀疏或噪声条件下往往表现不佳。

Method: 引入层次贝叶斯框架，直接在生成EDM的潜在点集上施加结构化先验，自然嵌入几何约束。采用层次先验实现自动正则化和鲁棒噪声处理。使用Metropolis-Hastings within Gibbs采样器进行后验推断，处理耦合的潜在点后验分布。

Result: 在合成数据上的实验表明，在稀疏情况下，与确定性基线方法相比，该方法具有更高的重建精度。

Conclusion: 层次贝叶斯框架通过结构化先验和自动正则化，有效解决了稀疏噪声观测下的欧氏距离矩阵补全问题，优于传统确定性方法。

Abstract: The completion of a Euclidean distance matrix (EDM) from sparse and noisy observations is a fundamental challenge in signal processing, with applications in sensor network localization, acoustic room reconstruction, molecular conformation, and manifold learning. Traditional approaches, such as rank-constrained optimization and semidefinite programming, enforce geometric constraints but often struggle under sparse or noisy conditions. This paper introduces a hierarchical Bayesian framework that places structured priors directly on the latent point set generating the EDM, naturally embedding geometric constraints. By incorporating a hierarchical prior on latent point set, the model enables automatic regularization and robust noise handling. Posterior inference is performed using a Metropolis-Hastings within Gibbs sampler to handle coupled latent point posterior. Experiments on synthetic data demonstrate improved reconstruction accuracy compared to deterministic baselines in sparse regimes.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [241] [Adaptive Benign Overfitting (ABO): Overparameterized RLS for Online Learning in Non-stationary Time-series](https://arxiv.org/abs/2601.22200)
*Luis Ontaneda Mijares,Nick Firoozye*

Main category: q-fin.ST

TL;DR: 提出自适应良性过拟合（ABO）框架，通过QR分解的指数加权递归最小二乘法（QR-EWRLS）实现稳定在线学习，在非平稳条件下保持良好泛化性能，速度提升20-40%。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论无法解释过参数化模型在插值极限之外仍能良好泛化的现象（良性过拟合），需要开发能在非平稳条件下在线适应的稳定算法。

Method: 基于正交三角更新的数值稳定递归最小二乘法（RLS）框架，结合随机傅里叶特征映射和遗忘因子正则化，提出QR-EWRLS算法，防止数值发散同时适应数据分布变化。

Result: 在非线性合成时间序列上保持有界残差和稳定条件数，重现过参数化模型的双下降行为；在外汇预测和电力需求预测中精度与基线核方法相当，速度提升20-40%。

Conclusion: ABO框架统一了自适应滤波、核近似和良性过拟合，为过参数化模型在非平稳环境中的在线学习提供了稳定解决方案。

Abstract: Overparameterized models have recently challenged conventional learning theory by exhibiting improved generalization beyond the interpolation limit, a phenomenon known as benign overfitting. This work introduces Adaptive Benign Overfitting (ABO), extending the recursive least-squares (RLS) framework to this regime through a numerically stable formulation based on orthogonal-triangular updates. A QR-based exponentially weighted RLS (QR-EWRLS) algorithm is introduced, combining random Fourier feature mappings with forgetting-factor regularization to enable online adaptation under non-stationary conditions. The orthogonal decomposition prevents the numerical divergence associated with covariance-form RLS while retaining adaptability to evolving data distributions. Experiments on nonlinear synthetic time series confirm that the proposed approach maintains bounded residuals and stable condition numbers while reproducing the double-descent behavior characteristic of overparameterized models. Applications to forecasting foreign exchange and electricity demand show that ABO is highly accurate (comparable to baseline kernel methods) while achieving speed improvements of between 20 and 40 percent. The results provide a unified view linking adaptive filtering, kernel approximation, and benign overfitting within a stable online learning framework.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [242] [Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference](https://arxiv.org/abs/2601.23252)
*David Yallup,Namu Kroupa,Will Handley*

Main category: stat.CO

TL;DR: Nested Slice Sampling (NSS) 是一种 GPU 友好的向量化嵌套采样方法，使用 Hit-and-Run 切片采样进行约束更新，在复杂多模态目标上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 复杂多模态目标的参数积分对于模型比较和校准不确定性量化很重要，但可扩展推理具有挑战性。传统嵌套采样通常顺序执行且硬约束使得高效加速器实现困难。

Method: 提出 Nested Slice Sampling (NSS)，使用 Hit-and-Run Slice Sampling 进行约束更新，通过调优分析得到简单的近最优切片宽度设置规则，支持 GPU 并行执行。

Result: 在挑战性合成目标、高维贝叶斯推理和高斯过程超参数边缘化实验中，NSS 保持准确的证据估计和高质量后验样本，在多模态问题上比当前最先进的调温 SMC 基线方法更稳健。

Conclusion: NSS 是一种 GPU 友好的向量化嵌套采样方法，在复杂多模态问题上表现优异，已发布开源实现以促进采用和可重复性。

Abstract: Model comparison and calibrated uncertainty quantification often require integrating over parameters, but scalable inference can be challenging for complex, multimodal targets. Nested Sampling is a robust alternative to standard MCMC, yet its typically sequential structure and hard constraints make efficient accelerator implementations difficult. This paper introduces Nested Slice Sampling (NSS), a GPU-friendly, vectorized formulation of Nested Sampling that uses Hit-and-Run Slice Sampling for constrained updates. A tuning analysis yields a simple near-optimal rule for setting the slice width, improving high-dimensional behavior and making per-step compute more predictable for parallel execution. Experiments on challenging synthetic targets, high dimensional Bayesian inference, and Gaussian process hyperparameter marginalization show that NSS maintains accurate evidence estimates and high-quality posterior samples, and is particularly robust on difficult multimodal problems where current state-of-the-art methods such as tempered SMC baselines can struggle. An open-source implementation is released to facilitate adoption and reproducibility.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [243] [Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data](https://arxiv.org/abs/2601.22242)
*Zhihao Zhang,Keith Redmill,Chengyang Peng,Bowen Weng*

Main category: cs.MA

TL;DR: 提出一个从宏观观测重建微观状态的框架，通过微观数据锚定车辆行为，学习既与部分观测轨迹微观一致、又在群体部署时与目标交通统计宏观对齐的共享策略。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶算法需要与人类驾驶实践对齐或有效协作。现有方法（监督学习和强化学习）都依赖高质量的真实驾驶行为观测数据，但这些数据难以获取且成本高昂。车载传感器能获取微观数据但缺乏环境上下文，路边传感器能捕捉宏观交通流特征但无法关联到具体车辆。这种互补性促使我们提出新的框架。

Method: 提出一个从宏观观测重建微观状态的框架：1) 使用微观数据锚定观测到的车辆行为；2) 学习一个共享策略，该策略在微观层面与部分观测的轨迹和动作一致，在宏观层面当群体部署时与目标交通统计对齐。这种约束和正则化的策略促进现实的交通流模式和与人类驾驶员的安全协调。

Result: 框架能够从宏观观测中重建未观测的微观状态，学习到的策略既保持微观一致性又实现宏观对齐，从而促进现实的交通流模式和与人类驾驶员的安全大规模协调。

Conclusion: 通过结合微观和宏观观测的互补优势，提出的框架能够学习到既符合个体车辆行为特征、又能产生理想群体交通模式的自动驾驶策略，解决了现有方法依赖高质量观测数据的局限性。

Abstract: A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.

</details>


### [244] [Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems](https://arxiv.org/abs/2601.22292)
*Manuela Chacon-Chamorro,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 该论文提出了一种通过奖励函数设计来增强多智能体系统合作弹性的框架，在混合动机环境中学习基于轨迹排名的奖励函数，显著提升了系统在干扰下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在动态不确定环境中运行时，需要同时追求个体目标和保障集体功能。混合动机多智能体系统中的合作弹性是一个关键但未被充分探索的属性。现有研究缺乏对奖励函数设计如何影响系统弹性的深入理解。

Method: 提出一个学习奖励函数的新框架，从轨迹排名中学习奖励函数，以合作弹性指标为指导。在社交困境环境中训练智能体，使用三种奖励策略：传统个体奖励、弹性推断奖励、以及两者平衡的混合策略。探索了三种奖励参数化方法（线性模型、手工特征、神经网络），并采用两种基于偏好的学习算法从行为排名中推断奖励。

Result: 混合策略显著提高了系统在干扰下的鲁棒性，同时不降低任务性能，并减少了资源过度使用等灾难性结果。研究结果表明奖励设计对于培养弹性合作至关重要。

Conclusion: 奖励函数设计对多智能体系统的合作弹性有重要影响，混合奖励策略能够在不牺牲任务性能的前提下增强系统鲁棒性。这项工作为开发能够在不确定环境中维持合作的稳健多智能体系统迈出了一步。

Abstract: Multi-agent systems often operate in dynamic and uncertain environments, where agents must not only pursue individual goals but also safeguard collective functionality. This challenge is especially acute in mixed-motive multi-agent systems. This work focuses on cooperative resilience, the ability of agents to anticipate, resist, recover, and transform in the face of disruptions, a critical yet underexplored property in Multi-Agent Reinforcement Learning. We study how reward function design influences resilience in mixed-motive settings and introduce a novel framework that learns reward functions from ranked trajectories, guided by a cooperative resilience metric. Agents are trained in a suite of social dilemma environments using three reward strategies: i) traditional individual reward; ii) resilience-inferred reward; and iii) hybrid that balance both. We explore three reward parameterizations-linear models, hand-crafted features, and neural networks, and employ two preference-based learning algorithms to infer rewards from behavioral rankings. Our results demonstrate that hybrid strategy significantly improve robustness under disruptions without degrading task performance and reduce catastrophic outcomes like resource overuse. These findings underscore the importance of reward design in fostering resilient cooperation, and represent a step toward developing robust multi-agent systems capable of sustaining cooperation in uncertain environments.

</details>


### [245] [ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review](https://arxiv.org/abs/2601.22638)
*Palash Goyal,Mihir Parmar,Yiwen Song,Hamid Palangi,Tomas Pfister,Jinsung Yoon*

Main category: cs.MA

TL;DR: ScholarPeer是一个搜索增强的多智能体框架，通过动态构建领域叙事、识别缺失比较和验证声明，生成更深入、基于文献的同行评审反馈。


<details>
  <summary>Details</summary>
Motivation: 当前自动同行评审系统主要停留在"表面层面"的批评，擅长总结内容但难以准确评估新颖性、重要性或识别深度方法缺陷，因为它们缺乏人类专家所拥有的外部文献背景。

Method: 采用搜索增强的多智能体框架，模拟高级研究员的认知过程。包括双流处理：上下文获取和主动验证。具体使用历史学家智能体动态构建领域叙事，基线侦察员识别缺失比较，多角度问答引擎验证声明，所有过程都基于实时网络规模文献。

Result: 在DeepReview-13K数据集上的评估显示，ScholarPeer在与最先进方法的并排比较中取得了显著胜率，并缩小了与人类水平多样性的差距。

Conclusion: ScholarPeer通过整合外部文献背景和多智能体协作，显著提升了自动同行评审的质量，使其能够生成更深入、更准确的评审反馈。

Abstract: Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with "surface-level" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify deep methodological flaws because they evaluate papers in a vacuum, lacking the external context a human expert possesses. In this paper, we introduce ScholarPeer, a search-enabled multi-agent framework designed to emulate the cognitive processes of a senior researcher. ScholarPeer employs a dual-stream process of context acquisition and active verification. It dynamically constructs a domain narrative using a historian agent, identifies missing comparisons via a baseline scout, and verifies claims through a multi-aspect Q&A engine, grounding the critique in live web-scale literature. We evaluate ScholarPeer on DeepReview-13K and the results demonstrate that ScholarPeer achieves significant win-rates against state-of-the-art approaches in side-by-side evaluations and reduces the gap to human-level diversity.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [246] [Detect and Act: Automated Dynamic Optimizer through Meta-Black-Box Optimization](https://arxiv.org/abs/2601.22542)
*Zijian Gao,Yuanting Zhong,Zeyuan Ma,Yue-Jiao Gong,Hongshu Guo*

Main category: cs.NE

TL;DR: 提出了一种基于强化学习的进化算法，用于自动检测动态优化问题的环境变化并自适应调整搜索策略，相比传统人工设计方法具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动态优化问题（DOPs）因环境动态变化而具有挑战性。现有进化动态优化方法主要依赖人工设计的自适应策略来检测环境变化并调整搜索策略，这些手工设计的策略在新场景中可能表现不佳。

Method: 采用强化学习辅助方法，借鉴元黑盒优化的双层学习优化思想。使用深度Q网络作为优化动态检测器和搜索策略适配器：输入当前优化状态，输出底层进化算法下一优化步骤的控制参数。学习目标是在问题分布上最大化预期性能增益。

Result: 构建了从易到难的DOPs测试平台，包含多样化的合成实例。广泛的基准测试结果表明，该方法相比最先进的基线方法，在解决DOPs时展现出灵活的搜索行为和优越性能。

Conclusion: 提出的强化学习辅助方法能够实现动态优化问题中环境变化的自动检测和自适应调整，训练后能够泛化到未见过的DOPs，相比传统人工设计策略具有更好的适应性和性能。

Abstract: Dynamic Optimization Problems (DOPs) are challenging to address due to their complex nature, i.e., dynamic environment variation. Evolutionary Computation methods are generally advantaged in solving DOPs since they resemble dynamic biological evolution. However, existing evolutionary dynamic optimization methods rely heavily on human-crafted adaptive strategy to detect environment variation in DOPs, and then adapt the searching strategy accordingly. These hand-crafted strategies may perform ineffectively at out-of-box scenarios. In this paper, we propose a reinforcement learning-assisted approach to enable automated variation detection and self-adaption in evolutionary algorithms. This is achieved by borrowing the bi-level learning-to-optimize idea from recent Meta-Black-Box Optimization works. We use a deep Q-network as optimization dynamics detector and searching strategy adapter: It is fed as input with current-step optimization state and then dictates desired control parameters to underlying evolutionary algorithms for next-step optimization. The learning objective is to maximize the expected performance gain across a problem distribution. Once trained, our approach could generalize toward unseen DOPs with automated environment variation detection and self-adaption. To facilitate comprehensive validation, we further construct an easy-to-difficult DOPs testbed with diverse synthetic instances. Extensive benchmark results demonstrate flexible searching behavior and superior performance of our approach in solving DOPs, compared to state-of-the-art baselines.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [247] [Tacit Coordination of Large Language Models](https://arxiv.org/abs/2601.22184)
*Ido Aharon,Emanuele La Malfa,Michael Wooldridge,Sarit Kraus*

Main category: cs.GT

TL;DR: LLMs在默契协调游戏中表现出色，通常超越人类表现，但在涉及数字或文化原型的常识协调中失败


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在默契协调游戏中的表现，探索焦点理论如何解释LLMs的协调行为，填补LLMs在心理学和博弈论框架下协调能力评估的空白

Method: 在已有人类实验的协作和竞争游戏中测试LLMs的协调能力，引入多种无学习策略来改进LLMs之间及与人类的协调，在Llama、Qwen、GPT-oss等开源模型上进行大规模评估

Result: LLMs展现出卓越的协调能力，通常超越人类表现，但在涉及数字或微妙文化原型的常识协调任务中表现不佳

Conclusion: 这是首个在焦点理论框架下对LLMs默契协调能力的大规模评估，揭示了LLMs在协调游戏中的优势和局限性

Abstract: In tacit coordination games with multiple outcomes, purely rational solution concepts, such as Nash equilibria, provide no guidance for which equilibrium to choose. Shelling's theory explains how, in these settings, humans coordinate by relying on focal points: solutions or outcomes that naturally arise because they stand out in some way as salient or prominent to all players. This work studies Large Language Models (LLMs) as players in tacit coordination games, and addresses how, when, and why focal points emerge. We compare and quantify the coordination capabilities of LLMs in cooperative and competitive games for which human experiments are available. We also introduce several learning-free strategies to improve the coordination of LLMs, with themselves and with humans. On a selection of heterogeneous open-source models, including Llama, Qwen, and GPT-oss, we discover that LLMs have a remarkable capability to coordinate and often outperform humans, yet fail on common-sense coordination that involves numbers or nuanced cultural archetypes. This paper constitutes the first large-scale assessment of LLMs' tacit coordination within the theoretical and psychological framework of focal points.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [248] [Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging](https://arxiv.org/abs/2601.23276)
*Shuhong Liu,Xining Ge,Ziying Gu,Lin Gu,Ziteng Cui,Xuangeng Chu,Jun Liu,Dong Li,Tatsuya Harada*

Main category: astro-ph.IM

TL;DR: 提出基于物理的CCD噪声合成框架，通过多曝光平均获得高信噪比基础图像，合成真实噪声配对数据用于监督学习，并发布真实多波段望远镜数据集。


<details>
  <summary>Details</summary>
Motivation: 天文成像在实用观测条件下仍受噪声限制，标准校准流程主要去除结构化伪影而保留随机噪声。基于学习的去噪方法面临配对训练数据稀缺、科学工作流需要物理可解释和可复现模型的挑战。

Method: 提出物理驱动的噪声合成框架，建模CCD噪声形成过程：光子散粒噪声、光响应非均匀性、暗电流噪声、读出效应、宇宙射线和热像素异常值。通过平均多个未配准曝光获得高信噪比基础图像，使用噪声模型合成真实噪声配对数据。

Result: 构建了丰富的配对数据集用于监督学习，并发布了真实世界多波段数据集，包含配对原始帧、仪器管道校准帧、校准数据以及堆叠的高信噪比基础图像，用于真实世界评估。

Conclusion: 该物理噪声合成框架解决了天文成像去噪中训练数据稀缺的问题，提供了物理可解释的噪声建模方法，并通过真实数据集支持实际应用评估。

Abstract: Astronomical imaging remains noise-limited under practical observing constraints, while standard calibration pipelines mainly remove structured artifacts and leave stochastic noise largely unresolved. Learning-based denoising is promising, yet progress is hindered by scarce paired training data and the need for physically interpretable and reproducible models in scientific workflows. We propose a physics-based noise synthesis framework tailored to CCD noise formation. The pipeline models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we average multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for supervised learning. We further introduce a real-world dataset across multi-bands acquired with two twin ground-based telescopes, providing paired raw frames and instrument-pipeline calibrated frames, together with calibration data and stacked high-SNR bases for real-world evaluation.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [249] [Minimal-Action Discrete Schrödinger Bridge Matching for Peptide Sequence Design](https://arxiv.org/abs/2601.22408)
*Shrey Goel,Pranam Chatterjee*

Main category: q-bio.BM

TL;DR: MadSBM：基于最小作用量离散薛定谔桥匹配的肽序列生成框架，通过连续时间马尔可夫过程在氨基酸编辑图上进行受控生成，避免低似然区域，并首次在薛定谔桥模型中应用离散分类器引导。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散和流模型依赖固定的退化过程或预设概率路径，迫使生成过程经过低似然区域，需要大量采样步骤。肽序列生成需要在离散且高度受限的空间中导航，许多中间状态化学上不可行或不稳定。

Method: 1) 基于预训练蛋白质语言模型logits定义生物学信息参考过程；2) 学习时间依赖控制场，偏置转移率以产生从掩码先验到数据分布的低作用量传输路径；3) 引入离散分类器引导实现特定功能目标。

Result: MadSBM在整个生成过程中保持在高似然序列邻域附近，避免了现有方法需要经过低似然区域的问题，减少了采样步骤需求。

Conclusion: MadSBM为肽设计提供了有效的生成框架，首次将离散分类器引导应用于薛定谔桥模型，扩展了治疗性肽的设计空间。

Abstract: Generative modeling of peptide sequences requires navigating a discrete and highly constrained space in which many intermediate states are chemically implausible or unstable. Existing discrete diffusion and flow-based methods rely on reversing fixed corruption processes or following prescribed probability paths, which can force generation through low-likelihood regions and require countless sampling steps. We introduce Minimal-action discrete Schrödinger Bridge Matching (MadSBM), a rate-based generative framework for peptide design that formulates generation as a controlled continuous-time Markov process on the amino-acid edit graph. To yield probability trajectories that remain near high-likelihood sequence neighborhoods throughout generation, MadSBM 1) defines generation relative to a biologically informed reference process derived from pre-trained protein language model logits and 2) learns a time-dependent control field that biases transition rates to produce low-action transport paths from a masked prior to the data distribution. We finally introduce guidance to the MadSBM sampling procedure towards a specific functional objective, expanding the design space of therapeutic peptides; to our knowledge, this represents the first-ever application of discrete classifier guidance to Schrödinger bridge-based generative models.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [250] [Towards Solving the Gilbert-Pollak Conjecture via Large Language Models](https://arxiv.org/abs/2601.22365)
*Yisi Ke,Tianyu Huang,Yankai Shu,Di He,Jingchu Gai,Liwei Wang*

Main category: cs.DM

TL;DR: 利用LLM生成几何引理代码，构建验证函数系统，将Steiner比率下界从0.824提升到0.8559


<details>
  <summary>Details</summary>
Motivation: Gilbert-Pollak猜想（Steiner比率猜想）在欧几里得平面中，Steiner最小树的长度至少是欧几里得最小生成树长度的√3/2≈0.866倍。过去30年下界停留在0.824，LLM在竞赛数学中表现优异但未用于研究级开放问题。

Method: 让LLM生成规则约束的几何引理（实现为可执行代码），构建验证函数系统，通过反思驱动的渐进引理优化，获得理论认证的Steiner比率下界。

Result: 建立了新的认证下界0.8559，仅需数千次LLM调用，显著推进了该猜想的研究。

Conclusion: 展示了LLM系统在高级数学研究中的强大潜力，通过生成可执行引理代码而非直接解决猜想，实现了研究级开放问题的实质性进展。

Abstract: The Gilbert-Pollak Conjecture \citep{gilbert1968steiner}, also known as the Steiner Ratio Conjecture, states that for any finite point set in the Euclidean plane, the Steiner minimum tree has length at least $\sqrt{3}/2 \approx 0.866$ times that of the Euclidean minimum spanning tree (the Steiner ratio). A sequence of improvements through the 1980s culminated in a lower bound of $0.824$, with no substantial progress reported over the past three decades. Recent advances in LLMs have demonstrated strong performance on contest-level mathematical problems, yet their potential for addressing open, research-level questions remains largely unexplored. In this work, we present a novel AI system for obtaining tighter lower bounds on the Steiner ratio. Rather than directly prompting LLMs to solve the conjecture, we task them with generating rule-constrained geometric lemmas implemented as executable code. These lemmas are then used to construct a collection of specialized functions, which we call verification functions, that yield theoretically certified lower bounds of the Steiner ratio. Through progressive lemma refinement driven by reflection, the system establishes a new certified lower bound of 0.8559 for the Steiner ratio. The entire research effort involves only thousands of LLM calls, demonstrating the strong potential of LLM-based systems for advanced mathematical research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [251] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 该论文研究了"醉酒语言"（酒精影响下书写的文本）如何驱动大语言模型的安全失效，提出了三种诱导LLM产生醉酒语言的方法，并发现这会显著增加越狱攻击和隐私泄露的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下容易产生不良行为和隐私泄露，研究者希望探索醉酒语言是否也会导致大语言模型出现类似的安全失效问题，从而揭示LLM安全性的潜在风险。

Method: 提出了三种诱导LLM产生醉酒语言的方法：1）基于角色的提示工程；2）因果微调；3）基于强化的后训练。在5个LLM上评估，使用JailbreakBench测试越狱脆弱性，使用ConfAIde测试隐私泄露，结合人工评估和LLM评估器进行综合分析。

Result: 醉酒语言诱导显著增加了LLM对越狱攻击的脆弱性（即使在防御措施存在的情况下），并导致更多隐私泄露。研究发现人类醉酒行为与LLM在醉酒语言诱导下表现出的拟人化特征之间存在对应关系。

Conclusion: 醉酒语言诱导方法简单高效，可能成为对抗LLM安全调优的潜在手段，突显了LLM安全性的重大风险。研究揭示了人类醉酒行为与LLM拟人化安全失效之间的关联。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [252] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID是一个记忆增强的多智能体真实性评估框架，通过紧密耦合检索与推理过程，实现动态证据获取和跨声明证据复用，在多个事实核查基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有真实性评估方法通常将证据检索视为静态、孤立的步骤，未能有效管理或跨声明复用检索到的证据，导致效率低下和冗余搜索。

Method: 提出MERMAID框架，集成智能体驱动搜索、结构化知识表示和持久记忆模块，采用Reason-Action风格的迭代过程，实现动态证据获取和跨声明证据复用。

Result: 在三个事实核查基准和两个声明验证数据集上使用多种LLM（GPT、LLaMA、Qwen系列）进行评估，MERMAID实现了最先进的性能，同时提高了搜索效率。

Conclusion: 通过协同检索、推理和记忆，MERMAID框架能够实现可靠的真实性评估，证明了紧密耦合这些组件在提高效率和一致性方面的有效性。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [253] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: SP2DPO改进DPO方法，通过基于语义标注为每个偏好对分配特定温度参数，替代全局统一温度，在保持训练效率的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统DPO使用单一全局温度参数处理所有偏好对，但实际偏好数据具有异质性：包含高信号目标失败（安全性、事实性）和低信号主观区别（风格），且存在标签噪声。需要更精细的调节机制

Method: 提出SP2DPO方法，用实例特定的温度计划β_i替代全局温度，这些参数基于教师语言模型生成的语义差距标注（类别、幅度、置信度）离线预决定。在UltraFeedback偏好语料库上实现大规模可审计的β_i构建，训练时保持标准DPO优化器但为每对设置特定β

Result: 在AlpacaEval 2.0评估中，SP2DPO与调优的全局β DPO基线竞争，在四个4B-8B学生骨干中的两个上提高了长度控制胜率，同时避免了每个模型的β搜索

Conclusion: SP2DPO通过语义感知的每对温度调节，有效处理异质偏好数据，在保持训练效率的同时提升性能，为偏好优化提供了更精细的控制机制

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [254] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 论文提出Bifocal Attention架构，通过几何眼和谱眼双模态位置编码解决RoPE在长距离递归推理中的局限性


<details>
  <summary>Details</summary>
Motivation: 标准RoPE使用固定的几何衰减，虽然适合局部句法连贯性，但无法捕捉递归逻辑和算法推理中的长距离周期性结构，导致模型在浅层推理链上训练后无法外推到更深递归步骤的"结构鸿沟"问题

Method: 提出Bifocal Attention架构，将位置编码解耦为两个模态：几何眼（标准RoPE）用于精确的token级操作，谱眼（可学习谐波算子）用于跟踪长距离递归深度。同时提出Spectral Evolution训练协议，将位置频率初始化为静态几何参数，但允许通过梯度下降演化为针对任务特定算法拓扑优化的谐波基

Result: 论文声称解决了RoPE的"谱刚性"限制，填补了"结构鸿沟"，使模型能够更好地处理长距离递归推理任务

Conclusion: Bifocal Attention通过双模态位置编码和谱演化训练，为LLMs提供了同时处理局部句法结构和长距离递归模式的能力，有望提升模型在算法推理任务上的表现

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [255] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 论文发现LLM评估中的自偏好偏见测量存在方法学混杂因素，提出评估者质量基线来分离信号与噪声，减少89.6%的测量误差


<details>
  <summary>Details</summary>
Motivation: 现有研究发现LLM在作为评估者时倾向于偏好自己的输出，这影响了自动化后训练和评估流程的完整性。但难以区分哪些评估偏见是由自恋（narcissism）解释，哪些是由一般实验混杂因素造成的，这扭曲了自偏好偏见的测量。

Method: 提出评估者质量基线（Evaluator Quality Baseline），比较评估者错误投票给自己输出的概率与投票给其他模型错误输出的概率。该方法旨在从困难问题的噪声输出中解耦自偏好信号。在37,448个查询上评估该基线。

Result: 发现核心方法学混杂因素可将测量误差减少89.6%。应用校正基线后，只有51%的初始发现保持统计显著性。此外，还分析了LLM评估者对"简单"与"困难"评估投票的熵特征。

Conclusion: 提出的校正基线通过消除噪声数据，为未来自偏好研究提供了更准确的方法。这项工作更广泛地促进了关于分类和隔离评估者偏见效应的研究。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [256] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新的Transformer归一化方法，通过建立跨越整个Transformer块的残差连接来稳定训练，同时采用PostNorm风格的计算来提升性能，解决了PreNorm和PostNorm的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的成功依赖于深度Transformer架构的稳定训练。归一化层的位置选择存在根本性权衡：PreNorm架构确保训练稳定性但可能导致深度模型性能下降，而PostNorm架构提供强大性能但存在严重训练不稳定性。需要一种方法来解决这一困境。

Method: 提出SpanNorm技术，建立跨越整个Transformer块的干净残差连接来稳定信号传播，同时采用PostNorm风格的计算来归一化聚合输出以增强模型性能。结合有原则的缩放策略，理论上证明SpanNorm能保持网络中的信号方差有界。

Result: SpanNorm在密集模型和混合专家（MoE）场景中均优于标准归一化方案，防止了PostNorm模型的梯度问题，并缓解了PreNorm的表示崩溃问题。

Conclusion: SpanNorm整合了PreNorm和PostNorm的优势，为更强大和稳定的Transformer架构铺平了道路，解决了深度Transformer训练中的归一化层位置权衡问题。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [257] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 小型语言模型虽然生成能力弱，但其内部表征包含丰富的评估信号，可替代大型模型进行高效评估，提出"表征即裁判"新范式。


<details>
  <summary>Details</summary>
Motivation: 当前"LLM-as-a-Judge"范式存在成本高、不透明、对提示设计敏感等问题，需要探索更高效可靠的评估方法。

Method: 提出语义容量不对称假设：评估所需语义容量远小于生成，可基于中间表征实现。开发INSPECTOR框架，通过探针从小型模型表征中预测评估分数。

Result: 在推理基准测试中，INSPECTOR显著优于基于提示的小型模型，接近完整LLM裁判的性能，同时提供更高效、可靠、可解释的评估方案。

Conclusion: 小型模型的内部表征包含强大的评估能力，可实现从"LLM-as-a-Judge"到"Representation-as-a-Judge"的范式转变，为可扩展评估提供新方向。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [258] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: DART是一种轻量级、无需训练的运行时动态剪枝方法，通过监控注意力分数分布变化来推断上下文变化，动态更新神经元级掩码，在保持模型能力的同时显著减少FFN参数冗余。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在FFN层存在显著参数冗余，现有剪枝方法存在两个主要问题：1）依赖数据集特定校准导致数据依赖和计算开销；2）主要是静态方法，无法适应自回归生成过程中上下文变化时知识神经元的动态演变。

Method: DART通过监控注意力分数分布的变化来推断上下文变化，动态更新神经元级掩码以保留重要参数。这是一种轻量级、无需训练的方法，在运行时进行基于上下文的即时剪枝。

Result: 在十个基准测试中，DART优于先前的动态基线，在LLAMA-3.1-8B模型70% FFN稀疏度下实现高达14.5%的准确率提升。在摘要任务上，相比静态掩码剪枝获得高达3倍的ROUGE-L分数提升，性能接近原始密集模型。

Conclusion: DART框架能有效适应不同语义上下文，在通用和领域特定任务中保持模型能力，同时内存占用极低（LLAMA-3.1-8B仅需不到10MB），FLOPs开销仅0.1%，为动态剪枝提供了高效解决方案。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [259] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 提出STSA（静默思考，语音回答）范式，让语音大模型在生成语音回答前先进行内部文本推理，通过扩散模型统一处理文本推理和语音生成，显著提升语音问答准确率。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型直接生成语音回答而不进行显式推理，导致错误一旦生成就无法修正。需要一种能让模型在生成语音前进行内部推理的方法。

Method: 提出MASKED方法，首个基于扩散的语音-文本语言模型，在单一掩码扩散框架下统一处理离散文本和标记化语音。通过迭代去噪联合生成推理轨迹和语音标记，使用模态特定的掩码调度。

Result: 在语音问答任务上达到最先进水平，比最佳基线提升9个百分点；在生成模型中达到最佳TTS质量（6.2% WER）；保持语言理解能力（66.2% MMLU）。消融实验证实扩散架构和思考轨迹都对性能提升有贡献。

Conclusion: STSA范式通过让语音模型在生成语音前进行内部文本推理，显著提升了语音问答的准确性和质量，扩散架构和思考轨迹的结合是实现这一目标的关键。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [260] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 该论文发现两种广泛使用的LLM可解释性方法（注意力机制分析和特征映射）在检测语言抽象方面存在根本性方法论缺陷，无法提供可靠的模型理解证据。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在普适计算中应用广泛，但其卓越性能的确切机制仍不清楚。现有可解释性方法本身也存在理解不足的问题，作者希望探究LLM中语言抽象如何出现，并检测其在不同模块中的表现。

Method: 使用两种文献中成熟的方法：(1) 基于注意力机制的token级关系结构探测；(2) 使用嵌入作为人类可解释属性载体的特征映射方法。

Result: 两种方法都失败了：注意力解释在测试核心假设（深层表示仍对应token）时崩溃；嵌入属性推断方法的高预测分数由方法学伪影和数据集结构驱动，而非有意义的语义知识。

Conclusion: 这些失败很重要，因为两种技术被广泛用作LLM理解能力的证据，但研究结果表明此类结论缺乏依据。在LLM作为系统组件部署的普适和分布式计算环境中，这些局限性尤其相关，因为可解释性方法被用于调试、压缩和解释模型。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [261] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出了一种改进掩码扩散语言模型在开放文本生成中性能的方法，通过引入<slack>标记和连接时序分类目标来放松严格的位置监督，从而提高生成质量和位置偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在开放文本生成任务中与自回归模型存在显著差距。研究发现严格的位置预测使MDLM解码对标记错位高度敏感，一个位置的偏移就可能严重破坏语义。这表明训练中的严格位置监督与MDLM解码的不可逆去噪动态不匹配。

Method: 采用对齐灵活监督策略进行微调，通过连接时序分类目标引入特殊标记<slack>。该方法应用于广泛使用的MDLM模型，在五个开放文本生成基准上进行实验。

Result: 该方法在五个基准上一致优于原始模型，并提高了对位置偏移的鲁棒性。结果表明放松严格位置监督是提高MDLM生成质量的重要因素。

Conclusion: 通过引入<slack>标记和连接时序分类目标来放松位置监督，有效改善了掩码扩散语言模型在开放文本生成中的性能，为解决MDLM解码中的位置敏感性问题提供了有效方案。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [262] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 提出DEF动态安全协议，通过认知启发式防御机制提升LLM对恶意篡改政策文本的检测和拒绝能力


<details>
  <summary>Details</summary>
Motivation: 人类通过认知防御机制（认知警惕性）对抗欺骗和错误信息，受此启发为LLM开发类似防御机制，特别是在高风险任务如自动化遵守数据隐私法律中尤为重要

Method: 提出动态认知回退（DEF）协议，通过不同级别的单句文本提示，促使LLM标记不一致性、拒绝遵守，并在遇到篡改政策文本时回退到参数化知识

Result: 使用HIPAA和GDPR等全球认可的法律政策进行实证评估，DEF有效提升了前沿LLM检测和拒绝篡改政策版本的能力，其中DeepSeek-R1在某一设置中达到100%检测率

Conclusion: 这项工作鼓励进一步开发认知启发式防御机制，以提高LLM对抗利用法律文书的伤害和欺骗形式的鲁棒性

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [263] [AI Decodes Historical Chinese Archives to Reveal Lost Climate History](https://arxiv.org/abs/2601.22458)
*Sida He,Lingxi Xie,Xiaopeng Zhang,Qi Tian*

Main category: physics.ao-ph

TL;DR: 使用生成式AI框架从历史档案定性描述中重建定量气候记录，应用于中国东南部1368-1911年降水重建，揭示ENSO影响


<details>
  <summary>Details</summary>
Motivation: 历史档案包含大量气候事件的定性描述，但将其转化为定量记录一直是根本性挑战，限制了我们对历史气候模式的理解

Method: 提出生成式AI框架，通过推断与历史记载事件相关的定量气候模式，实现从定性描述到定量重建的范式转变

Result: 成功重建中国东南部1368-1911年亚年度降水记录，量化了明代大旱等极端事件，并首次绘制了五个世纪ENSO对该区域降水影响的完整时空季节结构

Conclusion: 该方法实现了历史气候研究的范式转变，生成的高分辨率气候数据集可直接应用于气候科学，对历史和社会科学有广泛意义

Abstract: Historical archives contain qualitative descriptions of climate events, yet converting these into quantitative records has remained a fundamental challenge. Here we introduce a paradigm shift: a generative AI framework that inverts the logic of historical chroniclers by inferring the quantitative climate patterns associated with documented events. Applied to historical Chinese archives, it produces the sub-annual precipitation reconstruction for southeastern China over the period 1368-1911 AD. Our reconstruction not only quantifies iconic extremes like the Ming Dynasty's Great Drought but also, crucially, maps the full spatial and seasonal structure of El Ni$ñ$o influence on precipitation in this region over five centuries, revealing dynamics inaccessible in shorter modern records. Our methodology and high-resolution climate dataset are directly applicable to climate science and have broader implications for the historical and social sciences.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [264] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS是一个用于分布式协议设计的强化学习框架，结合蒙特卡洛树搜索、Transformer编码器和全局深度优先搜索，能够自动搜索并验证正确的分布式协议。


<details>
  <summary>Details</summary>
Motivation: 分布式协议设计极其困难且耗时，传统方法难以处理多智能体不完全信息博弈场景，需要自动化方法来搜索正确的协议策略。

Method: 将协议设计建模为不完全信息博弈的搜索问题，使用GGMS框架：集成蒙特卡洛树搜索变体、Transformer动作编码器、全局深度优先搜索跳出局部最优，并通过模型检查器提供反馈。

Result: GGMS能够学习比现有方法更大规模场景的正确协议，输出协议经过有界场景下所有执行路径的穷举模型检查验证，并在温和假设下证明搜索过程的完备性。

Conclusion: GGMS为分布式协议设计提供了有效的自动化解决方案，能够处理传统方法难以解决的多智能体不完全信息博弈问题，并保证找到正确协议（如果存在）。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [265] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: 提出LLM-TOPSIS框架，结合大型语言模型与模糊TOPSIS方法，用于自动化软件工程师招聘筛选，准确率达91%


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的就业环境中，传统招聘存在主观性、不一致性和偏见问题，需要自动化、可扩展且客观的人员筛选系统

Method: 1) 构建包含LinkedIn资料和专家评估的数据集；2) 使用DistilRoBERTa模型进行微调；3) 结合模糊TOPSIS方法（使用三角模糊数处理不确定性）；4) 开发LLM-TOPSIS框架进行候选人排名

Result: 系统排名与专家评估高度一致，在Experience属性和Overall属性上达到91%的准确率，证明了NLP与模糊决策方法结合的有效性

Conclusion: LLM-TOPSIS框架能提高招聘的可扩展性、一致性和客观性，减少偏见。未来将扩大数据集、增强模型可解释性，并在实际招聘场景中验证实用性

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [266] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL提出了一种基于强化学习的代码验证器训练方法，通过设计语法、功能、分支覆盖和样本难度感知的奖励信号，显著提升了代码验证的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、高失败率和推理效率低的问题。强化学习虽然提供了无监督优化的可能性，但仅使用功能奖励的朴素RL方法在生成困难分支和样本的有效单元测试方面表现不佳。

Method: 首先理论分析将分支覆盖、样本难度、语法和功能正确性联合建模为RL奖励。基于此设计语法和功能感知的奖励，并提出使用指数奖励塑造和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL仅用0.6B参数就达到了最先进性能，比GPT-3.5高出28.97%的通过率和15.08%的分支覆盖率，同时推理速度比竞争基线快20倍以上。

Conclusion: 通过将多个验证相关信号联合建模为RL奖励，CVeDRL显著提升了基于单元测试的代码验证的可靠性，在准确性和效率方面都取得了突破性进展。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [267] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：基于轨迹驱动的抽象机制，自动从执行日志构建状态抽象，支持在线构建智能体行为MDP，实现运行时验证


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，行为在长期随机交互轨迹中演化，这使得保证变得复杂，因为行为依赖于非确定性环境和概率模型输出。现有动态概率保证（DPA）方法需要手动定义状态抽象，这使验证与特定应用启发式方法耦合，增加了采用难度。

Method: 提出TriCEGAR方法：1）从轨迹学习谓词树作为抽象表示；2）使用反例进行细化；3）实现框架原生实现，捕获类型化智能体生命周期事件；4）从轨迹构建抽象；5）构造MDP；6）执行概率模型检查计算边界概率。

Result: TriCEGAR能够自动构建状态抽象，支持在线构建智能体行为MDP，通过概率模型检查计算Pmax(成功)和Pmin(失败)等边界，并利用运行似然性实现异常检测作为护栏信号。

Conclusion: TriCEGAR解决了智能体AI系统运行时验证中状态抽象手动定义的局限性，通过自动化抽象构建降低了采用门槛，为智能体行为保证提供了更实用的解决方案。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [268] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF框架让评判代理通过联合推理多个查询-响应对来提升评估质量，而非孤立评估，结合信念传播与集成学习原理，使用灵活的LSH算法选择多样化示例，在云配置错误分类任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前评判代理通常孤立评估每个查询-响应，缺乏跨实例的模式识别能力。需要提升评判代理从局部评估者到整体学习者的能力，通过同时评估相关响应来发现模式和不一致性。

Method: 提出JAF框架：1) 评判代理对主代理生成的查询-响应对进行联合推理；2) 结合信念传播和集成学习原理；3) 开发灵活的LSH算法，集成语义嵌入、LLM驱动的哈希谓词、类别标签监督和侧信息来学习信息丰富的二进制编码；4) 支持高效、可解释、关系感知的多样化示例选择。

Result: 在云配置错误分类任务中验证了JAF框架的有效性。该框架能够提升评判代理的评估质量，通过联合推理和多样化示例选择优化推理路径探索。

Conclusion: JAF框架成功将评判代理从局部评估者提升为整体学习者，通过联合推理和灵活的LSH算法实现了更高质量的评估和反馈，为代理AI框架提供了有效的评判机制。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [269] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: FLARE方法通过前瞻规划和奖励估计，解决LLM智能体在长时程规划中的短视问题，显著提升规划性能


<details>
  <summary>Details</summary>
Motivation: LLM智能体在短时程推理中表现良好，但在长时程规划中失败，因为逐步推理会导致短视的贪婪策略，早期决策无法考虑延迟后果

Method: 提出FLARE（Future-aware Lookahead with Reward Estimation）方法，通过显式的前瞻规划、价值传播和有限承诺，让下游结果影响早期决策

Result: 在多个基准测试、智能体框架和LLM骨干中，FLARE一致提升任务性能和规划行为，LLaMA-8B+FLARE经常超越GPT-4o+标准逐步推理

Conclusion: 研究明确了推理与规划之间的区别，未来感知规划是解决LLM智能体长时程规划问题的关键

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [270] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: UCPO框架通过三元优势解耦和动态不确定性奖励调整，解决了现有RL范式中的优势偏差问题，显著提升了LLM的可靠性和校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL范式（如GRPO）在不确定性表达方面存在优势偏差问题，导致模型要么过于保守要么过于自信，限制了LLM在高风险应用中的可信度。

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1）三元优势解耦：分离并独立归一化确定性和不确定性rollouts以消除优势偏差；2）动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在其知识边界之外的可靠性和校准能力。

Conclusion: UCPO框架通过解决现有RL范式中的优势偏差问题，为构建具有内在不确定性表达能力的可信LLM提供了有效解决方案。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [271] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 该论文提出MinPRO方法，通过使用前缀中最小token级比率替代不稳定的累积前缀比率，解决LLM强化学习后训练中因采样策略与目标策略差异导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM强化学习后训练通常采用离策略方式生成数据，使用较旧的采样策略来更新当前目标策略。为纠正采样策略与目标策略之间的差异，现有方法主要依赖token级重要性采样比率，但这在离策略程度较大时会导致训练不稳定。

Method: 提出MinPRO（Minimum Prefix Ratio）方法，用基于前缀中观察到的token级最小比率的非累积替代项，替代不稳定的累积前缀比率，以稳定LLM在较大离策略漂移下的优化。

Result: 在密集和混合专家LLM上的多个数学推理基准测试中，MinPRO显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过更稳定的前缀比率校正方法，有效解决了LLM强化学习后训练中的离策略优化不稳定问题，为大规模语言模型的强化学习训练提供了更可靠的解决方案。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [272] [Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation](https://arxiv.org/abs/2601.22550)
*Geonho Leem,Jaedong Lee,Jehee Lee,Seungmoon Song,Jungdam Won*

Main category: cs.RO

TL;DR: Exo-plore：一个结合神经力学模拟与深度强化学习的仿真框架，无需真实人体实验即可优化髋部外骨骼辅助


<details>
  <summary>Details</summary>
Motivation: 当前外骨骼控制器优化方法需要大量人体实验，参与者需行走数小时，这导致最需要外骨骼辅助的人群（如行动障碍者）难以参与这些高要求实验

Method: 结合神经力学模拟与深度强化学习，创建仿真框架来优化髋部外骨骼辅助，无需真实人体实验

Result: 框架能够：(1)生成捕捉人类对辅助力适应的真实步态数据；(2)在步态随机性下产生可靠的优化结果；(3)泛化到病理步态，显示病理严重程度与最优辅助之间存在强线性关系

Conclusion: Exo-plore框架为外骨骼辅助优化提供了一种无需大量人体实验的有效方法，特别有利于行动障碍人群，并能适应不同病理程度的步态

Abstract: Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.

</details>


### [273] [MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2601.22930)
*Xidong Li,Mingyu Guo,Chenchao Xu,Bailin Li,Wenjing Zhu,Yangang Zou,Rui Chen,Zehuan Wang*

Main category: cs.RO

TL;DR: MTDrive：基于多轮推理的自动驾驶轨迹规划框架，通过多轮迭代优化轨迹，解决复杂场景下的长尾问题


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单轮推理，难以处理需要迭代优化的复杂任务，特别是在自动驾驶的长尾场景中

Method: 提出MTDrive多轮框架，引入mtGRPO算法缓解奖励稀疏问题，构建交互式轨迹理解数据集，并进行系统级优化

Result: 在NAVSIM基准测试中表现优于现有方法，验证了多轮推理的有效性；系统优化使训练吞吐量提升2.5倍

Conclusion: 多轮推理范式能有效提升自动驾驶轨迹规划在复杂场景中的性能，为处理长尾问题提供了新思路

Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing "long-tail" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.

</details>


### [274] [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](https://arxiv.org/abs/2601.23285)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Andrew Fisher,Reza Abiri*

Main category: cs.RO

TL;DR: BRACE框架通过端到端梯度流联合优化意图推断和辅助仲裁，在复杂、目标模糊的场景中显著提升共享自主系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有共享自主系统采用静态混合比或将目标推断与辅助仲裁分离，导致在非结构化环境中性能不佳。需要一种能够联合优化意图推断和自适应辅助的框架。

Method: 提出BRACE框架，通过端到端梯度流连接贝叶斯意图推断和上下文自适应辅助。该框架将协作控制策略建立在环境上下文和完整目标概率分布上，实现意图推断与辅助仲裁的联合优化。

Result: 相比SOTA方法，BRACE在三维评估中取得显著改进：成功率提升6.3%，路径效率提高41%；相比无辅助控制，成功率提升36.3%，路径效率提高87%。在复杂、目标模糊的场景中优势最明显。

Conclusion: 集成优化在复杂、目标模糊的场景中最有益，且可推广到需要目标导向辅助的机器人领域，推动了自适应共享自主系统的技术前沿。

Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [275] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方嵌入用于快速文本驱动的野生动物观测检索，通过二进制表示实现大规模图像和音频数据库的高效搜索，显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大规模生物多样性监测平台依赖多模态野生动物观测，但现有基础模型的高维相似性搜索计算成本过高，需要更高效的检索方法。

Method: 基于跨视图代码对齐哈希框架，将轻量级哈希扩展到多模态设置，在共享汉明空间中对齐自然语言描述与视觉/听觉观测。利用预训练的野生动物基础模型（BioCLIP和BioLingual），通过参数高效微调进行哈希适配。

Result: 在iNaturalist2024（文本到图像）和iNatSounds2024（文本到音频）等大规模基准测试中，离散超立方嵌入相比连续嵌入实现竞争性甚至更优性能，同时大幅降低内存和搜索成本。哈希目标持续改进底层编码器表示，增强检索和零样本泛化能力。

Conclusion: 二进制、基于语言的检索方法为生物多样性监测系统提供了可扩展且高效的大规模野生动物档案搜索方案，证明了紧凑表示在保持性能的同时显著降低计算资源需求的有效性。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [276] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: BEAR提出一种新的微调目标，通过考虑束搜索行为来解决推荐系统中LLM训练与推理的不一致问题，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM进行推荐的方法存在训练-推理不一致问题：监督微调优化正样本的整体概率，但束搜索的贪婪剪枝机制可能导致高概率的正样本被提前丢弃，因为其前缀概率不足。

Method: 提出BEAR（束搜索感知正则化）微调目标，不直接模拟束搜索（计算代价高），而是强制一个松弛的必要条件：正样本的每个token在每个解码步骤中必须排名在前B个候选token内。

Result: 在四个真实世界数据集上的广泛实验表明，BEAR显著优于强基线方法，且相比标准监督微调只带来可忽略的计算开销。

Conclusion: BEAR通过显式考虑束搜索行为来解决训练-推理不一致问题，有效降低错误剪枝风险，为LLM在推荐系统中的应用提供了更有效的微调方法。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [277] [Assessing the Real-World Impact of Post-Quantum Cryptography on WPA-Enterprise Networks](https://arxiv.org/abs/2601.22892)
*Lukas Köder,Nils Lohmiller,Phil Schmieder,Bastian Buck,Michael Menth,Tobias Heer*

Main category: cs.CR

TL;DR: 该论文首次对后量子密码算法在WPA-Enterprise认证中的性能影响进行了实际评估，发现ML-DSA-65和Falcon-1024结合ML-KEM在安全性和性能之间提供了良好平衡，且会话恢复可有效缓解开销。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算机的出现对包括WPA-Enterprise认证在内的现有网络安全协议构成重大威胁，需要采用后量子密码学来应对这一威胁。

Method: 使用FreeRADIUS和hostapd构建实验测试平台，测量客户端、接入点和RADIUS服务器的认证延迟，评估多种PQC算法组合的性能开销，并与现有加密方案进行比较。

Result: PQC确实增加了认证延迟，但ML-DSA-65和Falcon-1024与ML-KEM的组合在安全性和性能之间提供了有利权衡；通过会话恢复可以有效缓解由此产生的开销。

Conclusion: 这是首次对支持PQC的WPA-Enterprise认证进行实际性能评估，证明了其在企业Wi-Fi部署中的实际可行性，为后量子时代网络安全提供了实用指导。

Abstract: The advent of large-scale quantum computers poses a significant threat to contemporary network security protocols, including Wi-Fi Protected Access (WPA)-Enterprise authentication. To mitigate this threat, the adoption of Post-Quantum Cryptography (PQC) is critical. In this work, we investigate the performance impact of PQC algorithms on WPA-Enterprise-based authentication. To this end, we conduct an experimental evaluation of authentication latency using a testbed built with the open-source tools FreeRADIUS and hostapd, measuring the time spent at the client, access point, and RADIUS server. We evaluate multiple combinations of PQC algorithms and analyze their performance overhead in comparison to currently deployed cryptographic schemes. Beyond performance, we assess the security implications of these algorithm choices by relating authentication mechanisms to the quantum effort required for their exploitation. This perspective enables a systematic categorization of PQ-relevant weaknesses in WPA-Enterprise according to their practical urgency. The evaluation results show that, although PQC introduces additional authentication latency, combinations such as ML-DSA-65 and Falcon-1024 used in conjunction with ML-KEM provide a favorable trade-off between security and performance. Furthermore, we demonstrate that the resulting overhead can be effectively mitigated through session resumption. Overall, this work presents a first real-world performance evaluation of PQC-enabled WPA-Enterprise authentication and demonstrates its practical feasibility for enterprise Wi-Fi deployments.

</details>


### [278] [The Semantic Trap: Do Fine-tuned LLMs Learn Vulnerability Root Cause or Just Functional Pattern?](https://arxiv.org/abs/2601.22655)
*Feiyang Huang,Yuqiang Sun,Fan Zhang,Ziqi Yang,Han Liu,Yang Liu*

Main category: cs.CR

TL;DR: 研究发现微调后的LLMs在漏洞检测中可能依赖功能模式而非安全语义理解，存在"语义陷阱"现象，导致高基准分数具有误导性。


<details>
  <summary>Details</summary>
Motivation: 尽管微调后的LLMs在软件漏洞检测中表现良好，但尚不清楚这种改进是源于对漏洞根本原因的真正理解，还是仅仅利用了功能模式。研究者希望探究LLMs是否真正理解安全语义。

Method: 提出TrapEval评估框架，包含两个数据集：V2N（将漏洞代码与无关良性代码配对）和V2P（将漏洞代码与其修补版本配对）。微调5个代表性LLMs，进行跨数据集测试、语义保留扰动测试，并使用CodeBLEU测量语义差距。

Result: 微调后的LLMs难以区分漏洞代码与其修补版本，在轻微语义保留变换下鲁棒性严重下降，当语义差距较小时严重依赖功能上下文捷径。尽管指标有所改善，但模型未能获得真正的漏洞推理能力。

Conclusion: 当前微调实践往往无法传授真正的漏洞推理能力，传统数据集上的高基准分数可能是虚幻的，掩盖了模型无法理解漏洞真正因果逻辑的问题。这为领域敲响了警钟。

Abstract: LLMs demonstrate promising performance in software vulnerability detection after fine-tuning. However, it remains unclear whether these gains reflect a genuine understanding of vulnerability root causes or merely an exploitation of functional patterns. In this paper, we identify a critical failure mode termed the "semantic trap," where fine-tuned LLMs achieve high detection scores by associating certain functional domains with vulnerability likelihood rather than reasoning about the underlying security semantics.To systematically evaluate this phenomenon, we propose TrapEval, a comprehensive evaluation framework designed to disentangle vulnerability root cause from functional pattern. TrapEval introduces two complementary datasets derived from real-world open-source projects: V2N, which pairs vulnerable code with unrelated benign code, and V2P, which pairs vulnerable code with its corresponding patched version, forcing models to distinguish near-identical code that differs only in subtle security-critical logic. Using TrapEval, we fine-tune five representative state-of-the-art LLMs across three model families and evaluate them under cross-dataset testing, semantic-preserving perturbations, and varying degrees of semantic gap measured by CodeBLEU.Our empirical results reveal that, despite improvements in metrics, fine-tuned LLMs consistently struggle to distinguish vulnerable code from its patched counterpart, exhibit severe robustness degradation under minor semantic-preserving transformations, and rely heavily on functional-context shortcuts when the semantic gap is small. These findings provide strong evidence that current fine-tuning practices often fail to impart true vulnerability reasoning. Our findings serve as a wake-up call: high benchmark scores on traditional datasets may be illusory, masking the model's inability to understand the true causal logic of vulnerabilities.

</details>


### [279] [RealSec-bench: A Benchmark for Evaluating Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2601.22706)
*Yanlin Wang,Ziyao Zhang,Chong Wang,Xinyi Xu,Mingwei Liu,Yong Wang,Jiachi Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: RealSec-bench是一个基于真实世界Java仓库构建的安全代码生成基准测试，包含105个实例，涵盖19种CWE类型，用于评估LLM在同时满足功能正确性和安全性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注功能正确性，使用合成漏洞或孤立评估，无法捕捉真实软件中功能与安全的复杂交互，需要构建基于真实世界代码的安全代码生成基准。

Method: 采用多阶段流水线：使用CodeQL进行系统SAST扫描，基于LLM的误报消除，人工专家验证，构建包含105个实例的RealSec-bench基准，涵盖19种CWE类型和复杂数据流依赖。

Result: 对5个流行LLM的评估显示：RAG技术能改善功能正确性但对安全性帮助有限；显式安全提示常导致编译失败，损害功能正确性且不能可靠防止漏洞；现有LLM在功能与安全代码生成间存在差距。

Conclusion: 需要开发能同时确保功能正确性和安全性的代码生成方法，当前LLM在安全代码生成方面仍有明显不足，RAG和显式安全提示等现有技术效果有限。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their proficiency in producing secure code remains a critical, under-explored area. Existing benchmarks often fall short by relying on synthetic vulnerabilities or evaluating functional correctness in isolation, failing to capture the complex interplay between functionality and security found in real-world software. To address this gap, we introduce RealSec-bench, a new benchmark for secure code generation meticulously constructed from real-world, high-risk Java repositories. Our methodology employs a multi-stage pipeline that combines systematic SAST scanning with CodeQL, LLM-based false positive elimination, and rigorous human expert validation. The resulting benchmark contains 105 instances grounded in real-word repository contexts, spanning 19 Common Weakness Enumeration (CWE) types and exhibiting a wide diversity of data flow complexities, including vulnerabilities with up to 34-hop inter-procedural dependencies. Using RealSec-bench, we conduct an extensive empirical study on 5 popular LLMs. We introduce a novel composite metric, SecurePass@K, to assess both functional correctness and security simultaneously. We find that while Retrieval-Augmented Generation (RAG) techniques can improve functional correctness, they provide negligible benefits to security. Furthermore, explicitly prompting models with general security guidelines often leads to compilation failures, harming functional correctness without reliably preventing vulnerabilities. Our work highlights the gap between functional and secure code generation in current LLMs.

</details>


### [280] [A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy](https://arxiv.org/abs/2601.22240)
*Pedro H. Barcha Correia,Ryan W. Achjian,Diego E. G. Caetano de Oliveira,Ygor Acacio Maria,Victor Takashi Hayashi,Marcos Lopes,Charles Christian Miers,Marcos A. Simplicio*

Main category: cs.CR

TL;DR: 对88项研究的系统文献综述，首次系统梳理了提示注入攻击的缓解策略，扩展了NIST对抗机器学习分类法，并提供了包含定量效果评估的防御措施目录。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大型语言模型的广泛应用，出现了新的安全漏洞如越狱和提示注入攻击。这些恶意输入可能导致数据泄露、未授权操作或输出被篡改。由于攻击和防御技术都在快速发展，需要系统化的缓解策略理解。

Method: 采用系统文献综述方法，分析了88项研究。基于NIST对抗机器学习报告，扩展了分类法，引入新的防御类别。使用NIST术语和分类法作为基础，确保一致性，并为未来研究提供标准化分类框架。

Result: 提出了首个系统化的提示注入缓解策略综述，扩展了NIST分类法，创建了包含定量效果评估的防御措施目录，记录了不同LLM和攻击数据集上的效果，并标注了开源和模型无关的解决方案。

Conclusion: 这项工作为对抗机器学习领域的研究人员和寻求在生产系统中实施有效防御的开发人员提供了实用资源。通过标准化分类法和全面目录，促进了该领域的一致性和未来发展。

Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.

</details>


### [281] [Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective](https://arxiv.org/abs/2601.22434)
*Georgi Ganev,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文从模型中心视角重新审视合成数据的匿名性，认为有意义的评估必须考虑底层生成模型的能力和特性，并基于最先进的隐私攻击。文章将GDPR的匿名化定义与模型访问假设结合，论证合成数据技术本身不足以保证充分匿名化，比较了差分隐私和相似性隐私指标，指出前者能提供强保护而后者缺乏足够保障。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据隐私评估主要停留在数据集层面，而实际部署中生成模型通常可直接访问或查询。现有研究、商业部署和GDPR等隐私法规对匿名性的评估未能充分考虑模型中心视角，导致对合成数据隐私风险的评估不足。

Method: 从模型中心视角重新思考合成数据的匿名性主张，将GDPR的个人数据和匿名化定义与模型访问假设相结合，识别必须缓解的可识别性风险类型，并将其映射到不同威胁设置下的隐私攻击。分析合成数据技术的局限性，比较差分隐私和相似性隐私指标两种常用机制。

Result: 合成数据技术本身不足以保证充分的匿名化；差分隐私能提供针对可识别性风险的强有力保护，而相似性隐私指标缺乏足够的保障措施。建立了监管可识别性概念与模型中心隐私攻击之间的联系。

Conclusion: 需要从模型中心视角评估合成数据系统的匿名性，考虑生成模型的能力和特性。差分隐私是更可靠的保护机制，而相似性隐私指标不足以应对可识别性风险。该框架有助于研究人员、从业者和政策制定者更负责任、更可信地评估合成数据系统。

Abstract: Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset.
  In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.

</details>


### [282] [Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection](https://arxiv.org/abs/2601.22800)
*Md Zahurul Haque,Md. Hafizur Rahman,Yeahyea Sarker*

Main category: cs.CR

TL;DR: Trackly是一个统一的SaaS平台，将用户行为分析与实时异常检测相结合，通过可配置规则和加权风险评分来识别可疑活动，为中小企业和电商提供高效的安全分析解决方案。


<details>
  <summary>Details</summary>
Motivation: 大多数平台将产品分析和安全分开，导致碎片化的可见性和延迟的威胁检测。需要统一的解决方案来理解用户行为、优化业务转化，并减轻账户接管、欺诈和机器人攻击等威胁。

Method: Trackly通过轻量级JavaScript SDK和安全REST API集成，跟踪会话、IP地理位置、设备浏览器指纹和细粒度事件。使用可配置规则（如新设备登录、不可能旅行、机器人行为、VPN使用等）和加权风险评分进行实时异常检测。平台基于多租户微服务架构（ASP.NET Core、MongoDB、RabbitMQ、Next.js）构建。

Result: 在合成数据集上，Trackly实现了98.1%的准确率、97.7%的精确度和2.25%的误报率，证明了其高效性。平台提供实时仪表板，显示全球会话地图、日活月活、跳出率和会话时长等指标。

Conclusion: Trackly成功统一了全面的用户行为分析与实时异常检测，为中小企业和电商提供了透明、可解释的决策支持，解决了传统平台中产品分析和安全分离导致的碎片化问题。

Abstract: Understanding user behavior is essential for improving digital experiences, optimizing business conversions, and mitigating threats like account takeovers, fraud, and bot attacks. Most platforms separate product analytics and security, creating fragmented visibility and delayed threat detection. Trackly, a scalable SaaS platform, unifies comprehensive user behavior analytics with real time, rule based anomaly detection. It tracks sessions, IP based geo location, device browser fingerprints, and granular events such as page views, add to cart, and checkouts. Suspicious activities logins from new devices or locations, impossible travel (Haversine formula), rapid bot like actions, VPN proxy usage, or multiple accounts per IP are flagged via configurable rules with weighted risk scoring, enabling transparent, explainable decisions. A real time dashboard provides global session maps, DAU MAU, bounce rates, and session durations. Integration is simplified with a lightweight JavaScript SDK and secure REST APIs. Implemented on a multi tenant microservices stack (ASP.NET Core, MongoDB, RabbitMQ, Next.js), Trackly achieved 98.1% accuracy, 97.7% precision, and 2.25% false positives on synthetic datasets, proving its efficiency for SMEs and ecommerce.

</details>


### [283] [Evaluating Large Language Models for Security Bug Report Prediction](https://arxiv.org/abs/2601.22921)
*Farnaz Soltaniani,Shoaib Razzaq,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 评估提示工程与微调方法在安全漏洞报告预测中的表现，发现提示工程方法召回率高但精度低，微调方法精度高但召回率低，两者存在明显权衡


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告的早期检测对于及时缓解漏洞至关重要，需要探索如何有效利用大语言模型进行SBR预测

Method: 使用提示工程和微调两种方法，基于大语言模型进行安全漏洞报告预测，比较不同方法在多个数据集上的表现

Result: 提示工程方法平均G-measure为77%，召回率74%，但精度仅22%；微调方法平均G-measure为51%，精度75%，召回率36%；微调模型推理速度比专有模型快50倍

Conclusion: 两种方法存在明显权衡，需要进一步研究如何充分利用大语言模型进行安全漏洞报告预测

Abstract: Early detection of security bug reports (SBRs) is critical for timely vulnerability mitigation. We present an evaluation of prompt-based engineering and fine-tuning approaches for predicting SBRs using Large Language Models (LLMs). Our findings reveal a distinct trade-off between the two approaches. Prompted proprietary models demonstrate the highest sensitivity to SBRs, achieving a G-measure of 77% and a recall of 74% on average across all the datasets, albeit at the cost of a higher false-positive rate, resulting in an average precision of only 22%. Fine-tuned models, by contrast, exhibit the opposite behavior, attaining a lower overall G-measure of 51% but substantially higher precision of 75% at the cost of reduced recall of 36%. Though a one-time investment in building fine-tuned models is necessary, the inference on the largest dataset is up to 50 times faster than that of proprietary models. These findings suggest that further investigations to harness the power of LLMs for SBR prediction are necessary.

</details>


### [284] [From Data Leak to Secret Misses: The Impact of Data Leakage on Secret Detection Models](https://arxiv.org/abs/2601.22946)
*Farnaz Soltaniani,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 研究发现机器学习安全工具在重复数据泄露问题下性能被高估，实际泛化能力不足


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地用于软件安全任务，这些模型通常在大型互联网数据集上进行训练和评估，但这些数据集经常包含重复或高度相似的样本。当这些样本在训练集和测试集之间分割时，可能导致数据泄露，使模型记忆模式而非学习泛化能力。

Method: 研究调查了广泛使用的硬编码密钥基准数据集中的重复问题，分析了数据泄露如何影响AI密钥检测器的性能评估。

Result: 数据泄露会显著夸大AI密钥检测器报告的性能，导致对其真实世界有效性的误导性评估。

Conclusion: 当前基于重复数据集的机器学习安全工具评估存在严重缺陷，需要更严格的数据处理和评估方法来准确反映模型在实际应用中的泛化能力。

Abstract: Machine learning models are increasingly used for software security tasks. These models are commonly trained and evaluated on large Internet-derived datasets, which often contain duplicated or highly similar samples. When such samples are split across training and test sets, data leakage may occur, allowing models to memorize patterns instead of learning to generalize. We investigate duplication in a widely used benchmark dataset of hard coded secrets and show how data leakage can substantially inflate the reported performance of AI-based secret detectors, resulting in a misleading picture of their real-world effectiveness.

</details>


### [285] [PIDSMaker: Building and Evaluating Provenance-based Intrusion Detection Systems](https://arxiv.org/abs/2601.22983)
*Tristan Bilot,Baoxiang Jiang,Thomas Pasquier*

Main category: cs.CR

TL;DR: PIDSMaker是一个开源框架，用于在一致的协议下开发和评估基于溯源图的入侵检测系统，解决了现有评估方法不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于溯源图的入侵检测系统（PIDS）评估存在不一致性：使用不同的预处理流程、非标准数据集划分、不兼容的标签和指标，这破坏了可重复性，阻碍了公平比较，并给研究人员带来了大量的重新实现负担。

Method: 提出了PIDSMaker框架，将八个最先进的系统整合到一个模块化、可扩展的架构中，采用标准化的预处理和真实标签。提供基于YAML的配置接口，支持无需代码更改即可跨系统组合组件，并包含消融研究、超参数调优、多运行不稳定性测量和可视化工具。

Result: PIDSMaker框架支持一致的实验和公平比较，通过具体用例展示了其有效性，并发布了预处理数据集和标签，为PIDS社区提供共享评估支持。

Conclusion: PIDSMaker解决了PIDS评估中的方法学差距，提供了一个标准化框架来促进可重复性、公平比较和快速原型开发，有望推动基于溯源图的入侵检测系统研究发展。

Abstract: Recent provenance-based intrusion detection systems (PIDSs) have demonstrated strong potential for detecting advanced persistent threats (APTs) by applying machine learning to system provenance graphs. However, evaluating and comparing PIDSs remains difficult: prior work uses inconsistent preprocessing pipelines, non-standard dataset splits, and incompatible ground-truth labeling and metrics. These discrepancies undermine reproducibility, impede fair comparison, and impose substantial re-implementation overhead on researchers. We present PIDSMaker, an open-source framework for developing and evaluating PIDSs under consistent protocols. PIDSMaker consolidates eight state-of-the-art systems into a modular, extensible architecture with standardized preprocessing and ground-truth labels, enabling consistent experiments and apples-to-apples comparisons. A YAML-based configuration interface supports rapid prototyping by composing components across systems without code changes. PIDSMaker also includes utilities for ablation studies, hyperparameter tuning, multi-run instability measurement, and visualization, addressing methodological gaps identified in prior work. We demonstrate PIDSMaker through concrete use cases and release it with preprocessed datasets and labels to support shared evaluation for the PIDS community.

</details>


### [286] [No More, No Less: Least-Privilege Language Models](https://arxiv.org/abs/2601.23157)
*Paulius Rauba,Dominykas Seputis,Patrikas Vanagas,Mihaela van der Schaar*

Main category: cs.CR

TL;DR: 提出最小权限语言模型，通过控制前向传播中可访问的内部计算来实施最小权限原则，避免部署多个模型或重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型部署违反最小权限安全原则，所有用户和请求都通过单一API端点暴露全部能力。虽然最小权限能减少不必要的能力暴露，但缺乏定义和实施机制。

Method: 提出嵌套最小权限网络，这是一种保持形状、按秩索引的干预方法，提供平滑可逆的控制旋钮。将部署时控制形式化为监控-分配-执行三层架构。

Result: 该控制旋钮产生策略可用的权限-效用边界，能够选择性抑制目标能力，同时在不同策略下限制附带性能下降。

Conclusion: 挑战语言模型只能在输出层面控制的传统观念，提出新的部署范式，通过控制内部计算实现最小权限原则。

Abstract: Least privilege is a core security principle: grant each request only the minimum access needed to achieve its goal. Deployed language models almost never follow it, instead being exposed through a single API endpoint that serves all users and requests. This gap exists not because least privilege would be unhelpful; deployments would benefit greatly from reducing unnecessary capability exposure. The real obstacle is definitional and mechanistic: what does "access" mean inside a language model, and how can we enforce it without retraining or deploying multiple models? We take inspiration from least privilege in computer systems and define a class of models called least-privilege language models, where privilege is reachable internal computation during the forward pass. In this view, lowering privilege literally shrinks the model's accessible function class, as opposed to denying access via learned policies. We formalize deployment-time control as a monitor-allocator-enforcer stack, separating (i) request-time signals, (ii) a decision rule that allocates privilege, and (iii) an inference-time mechanism that selects privilege. We then propose Nested Least-Privilege Networks, a shape-preserving, rank-indexed intervention that provides a smooth, reversible control knob. We show that this knob yields policy-usable privilege-utility frontiers and enables selective suppression of targeted capabilities with limited collateral degradation across various policies. Most importantly, we argue for a new deployment paradigm that challenges the premise that language models can only be controlled at the output level.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [287] [SPARK: Real-Time Monitoring of Multi-Faceted Programming Exercises](https://arxiv.org/abs/2601.22256)
*Yinuo Yang,Ashley Ge Zhang,Steve Oney,April Yi Wang*

Main category: cs.HC

TL;DR: SPARK是一个编程练习监控仪表板，通过将子步骤分组为检查点、提供自动测试建议和可视化进度，帮助教师监控学生在复杂编程练习中的进展。


<details>
  <summary>Details</summary>
Motivation: 监控课堂编程练习可以帮助教师识别困难学生和常见挑战，但对于多步骤、复杂依赖关系、无固定完成顺序或难以总结评估标准的复杂编程问题（如构建交互式Web界面），理解学生进展非常困难。

Method: SPARK允许教师根据练习要求灵活地将子步骤分组为检查点，为这些检查点建议自动测试，并生成可视化来跟踪跨步骤的进度。系统还允许教师检查中间输出，深入了解解决方案的变体。

Result: 构建了包含22名学习者解决两个Web编程练习的40分钟击键编码数据集，并通过16名编程教师的受试者内评估提供了SPARK感知有用性的实证见解。

Conclusion: SPARK是一个有效的编程练习监控工具，能够帮助教师更好地理解和跟踪学生在复杂编程任务中的进展，特别是在多步骤、复杂依赖的Web编程练习中。

Abstract: Monitoring in-class programming exercises can help instructors identify struggling students and common challenges. However, understanding students' progress can be prohibitively difficult, particularly for multi-faceted problems that include multiple steps with complex interdependencies, have no predictable completion order, or involve evaluation criteria that are difficult to summarize across many students (e.g., exercises building interactive web-based user interfaces). We introduce SPARK, a coding exercise monitoring dashboard designed to address these challenges. SPARK allows instructors to flexibly group substeps into checkpoints based on exercise requirements, suggests automated tests for these checkpoints, and generates visualizations to track progress across steps. SPARK also allows instructors to inspect intermediate outputs, providing deeper insights into solution variations. We also construct a dataset of 40-minute keystroke coding data from N=22 learners solving two web programming exercises and provide empirical insights into the perceived usefulness of SPARK through a within-subjects evaluation with 16 programming instructors.

</details>


### [288] [Qualitative Evaluation of LLM-Designed GUI](https://arxiv.org/abs/2601.22759)
*Bartosz Sawicki,Tomasz Les,Dariusz Parzych,Aleksandra Wycisk-Ficek,Pawel Trebacz,Pawel Zawadzki*

Main category: cs.HC

TL;DR: LLM生成的GUI界面在布局设计上表现良好，但在可访问性和交互功能方面存在不足，需要人工干预以确保可用性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的发展，探索大型语言模型在自动化图形用户界面设计中的应用潜力，评估其能否满足多样化用户需求。

Method: 使用2025年1月的三种先进LLM模型（OpenAI GPT o3-mini-high、DeepSeek R1和Anthropic Claude 3.5 Sonnet），为聊天系统、技术团队面板和管理者仪表盘三种界面类型生成原型，并进行专家评估。

Result: LLM能够有效创建结构化布局，但在满足可访问性标准和提供交互功能方面面临挑战；能够部分针对不同用户角色定制界面，但缺乏更深层次的上下文理解。

Conclusion: LLM是早期UI原型设计的有前景工具，但需要人工干预来确保可用性、可访问性和用户满意度。

Abstract: As generative artificial intelligence advances, Large Language Models (LLMs) are being explored for automated graphical user interface (GUI) design. This study investigates the usability and adaptability of LLM-generated interfaces by analysing their ability to meet diverse user needs. The experiments included utilization of three state-of-the-art models from January 2025 (OpenAI GPT o3-mini-high, DeepSeek R1, and Anthropic Claude 3.5 Sonnet) generating mockups for three interface types: a chat system, a technical team panel, and a manager dashboard. Expert evaluations revealed that while LLMs are effective at creating structured layouts, they face challenges in meeting accessibility standards and providing interactive functionality. Further testing showed that LLMs could partially tailor interfaces for different user personas but lacked deeper contextual understanding. The results suggest that while LLMs are promising tools for early-stage UI prototyping, human intervention remains critical to ensure usability, accessibility, and user satisfaction.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [289] [SCENE: Semantic-aware Codec Enhancement with Neural Embeddings](https://arxiv.org/abs/2601.22189)
*Han-Yu Lin,Li-Wei Chen,Hung-Shin Lee*

Main category: eess.IV

TL;DR: 提出轻量级语义感知预处理框架SCENE，通过选择性处理压缩伪影来提升视频感知质量，无需修改现有视频编码流程


<details>
  <summary>Details</summary>
Motivation: 标准视频编解码器产生的压缩伪影会降低感知质量，需要一种轻量级方法来增强压缩视频的感知保真度

Method: 集成视觉语言模型的语义嵌入到高效卷积架构中，使用可微分编解码器代理进行端到端训练，优先保留感知显著结构

Result: 在高分辨率基准测试中，在客观指标（MS-SSIM）和感知指标（VMAF）上均优于基线方法，特别是在显著区域的细节纹理保留方面有明显提升

Conclusion: 语义引导、编解码器感知的预处理是增强压缩视频流的有效方法，能够实现实时性能

Abstract: Compression artifacts from standard video codecs often degrade perceptual quality. We propose a lightweight, semantic-aware pre-processing framework that enhances perceptual fidelity by selectively addressing these distortions. Our method integrates semantic embeddings from a vision-language model into an efficient convolutional architecture, prioritizing the preservation of perceptually significant structures. The model is trained end-to-end with a differentiable codec proxy, enabling it to mitigate artifacts from various standard codecs without modifying the existing video pipeline. During inference, the codec proxy is discarded, and SCENE operates as a standalone pre-processor, enabling real-time performance. Experiments on high-resolution benchmarks show improved performance over baselines in both objective (MS-SSIM) and perceptual (VMAF) metrics, with notable gains in preserving detailed textures within salient regions. Our results show that semantic-guided, codec-aware pre-processing is an effective approach for enhancing compressed video streams.

</details>


### [290] [Compressed BC-LISTA via Low-Rank Convolutional Decomposition](https://arxiv.org/abs/2601.23148)
*Han Wang,Yhonatan Kvich,Eduardo Pérez,Florian Römer,Yonina C. Eldar*

Main category: eess.IV

TL;DR: 提出基于压缩块卷积测量模型的稀疏信号恢复方法，通过低秩CNN分解和OMP初始化，在保持重建精度的同时大幅减少参数和模型大小


<details>
  <summary>Details</summary>
Motivation: 解决多通道成像中压缩前向/后向算子在保持重建精度方面的挑战，寻求在减少计算复杂度和模型大小的同时不牺牲重建质量的方法

Method: 提出压缩块卷积测量模型，基于低秩CNN分解，从物理推导的前向/后向算子分析初始化，使用OMP选择紧凑基滤波器集，计算线性混合系数近似完整模型，扩展为C-BC-LISTA网络

Result: 在多通道超声成像模拟中，C-BC-LISTA相比其他SOTA方法需要更少参数和更小模型，同时提高重建精度；OMP初始化的结构化压缩表现最佳，训练效率最高

Conclusion: 提出的压缩块卷积测量模型结合OMP初始化，为多通道成像提供了一种高效且准确的稀疏信号恢复方法，在参数效率和重建质量之间取得了良好平衡

Abstract: We study Sparse Signal Recovery (SSR) methods for multichannel imaging with compressed {forward and backward} operators that preserve reconstruction accuracy. We propose a Compressed Block-Convolutional (C-BC) measurement model based on a low-rank Convolutional Neural Network (CNN) decomposition that is analytically initialized from a low-rank factorization of physics-derived forward/backward operators in time delay-based measurements. We use Orthogonal Matching Pursuit (OMP) to select a compact set of basis filters from the analytic model and compute linear mixing coefficients to approximate the full model. We consider the Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) network as a representative example for which the C-BC-LISTA extension is presented. In simulated multichannel ultrasound imaging across multiple Signal-to-Noise Ratios (SNRs), C-BC-LISTA requires substantially fewer parameters and smaller model size than other state-of-the-art (SOTA) methods while improving reconstruction accuracy. In ablations over OMP, Singular Value Decomposition (SVD)-based, and random initializations, OMP-initialized structured compression performs best, yielding the most efficient training and the best performance.

</details>


### [291] [Scale-Cascaded Diffusion Models for Super-Resolution in Medical Imaging](https://arxiv.org/abs/2601.23201)
*Darshan Thaker,Mahmoud Mostapha,Radu Miron,Shihan Qiu,Mariappan Nadar*

Main category: eess.IV

TL;DR: 提出基于拉普拉斯金字塔的多尺度扩散先验模型，用于医学图像超分辨率，通过在不同频率带训练独立扩散先验，实现渐进式重建并提升感知质量、减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型作为生成先验通常只在单一尺度上训练，忽略了图像数据的层次化尺度结构，这限制了其在医学图像超分辨率任务中的性能。

Method: 将图像分解为拉普拉斯金字塔尺度，为每个频率带训练独立的扩散先验，开发渐进式超分辨率算法，利用这些先验在不同尺度上逐步细化重建结果。

Result: 在脑部、膝盖和前列腺MRI数据上的评估表明，该方法相比基线在感知质量上有提升，同时通过更小的粗尺度网络减少了推理时间。

Conclusion: 该框架将多尺度重建与扩散先验统一起来，为医学图像超分辨率提供了一种有效方法，既改善了感知质量又提高了计算效率。

Abstract: Diffusion models have been increasingly used as strong generative priors for solving inverse problems such as super-resolution in medical imaging. However, these approaches typically utilize a diffusion prior trained at a single scale, ignoring the hierarchical scale structure of image data. In this work, we propose to decompose images into Laplacian pyramid scales and train separate diffusion priors for each frequency band. We then develop an algorithm to perform super-resolution that utilizes these priors to progressively refine reconstructions across different scales. Evaluated on brain, knee, and prostate MRI data, our approach both improves perceptual quality over baselines and reduces inference time through smaller coarse-scale networks. Our framework unifies multiscale reconstruction and diffusion priors for medical image super-resolution.

</details>


### [292] [Solving Inverse Problems with Flow-based Models via Model Predictive Control](https://arxiv.org/abs/2601.23231)
*George Webber,Alexander Denker,Riccardo Barbano,Andrew J Reader*

Main category: eess.IV

TL;DR: MPC-Flow：一种基于模型预测控制的框架，将基于流的生成模型的逆问题求解转化为序列控制子问题，实现推理时的实用最优控制引导，无需训练即可指导大规模模型。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型为逆问题提供了强大的无条件先验，但引导其动态进行条件生成仍然具有挑战性。现有的无训练条件生成方法将流模型中的条件生成视为最优控制问题，但求解轨迹优化计算和内存成本高，需要微分流动态或伴随求解。

Method: 提出MPC-Flow框架，将基于流的生成模型的逆问题求解转化为一系列控制子问题。该方法使用模型预测控制，提供理论保证将MPC-Flow与底层最优控制目标联系起来，并通过不同算法选择产生一系列引导算法，包括避免通过生成模型轨迹反向传播的机制。

Result: 在基准图像恢复任务（包括线性和非线性设置，如修复、去模糊和超分辨率）上评估MPC-Flow，展示了强大的性能。通过无训练引导FLUX.2（32B）在量化设置下，在消费级硬件上实现了大规模最先进架构的可扩展性。

Conclusion: MPC-Flow提供了一种实用的最优控制引导框架，用于基于流的生成模型的逆问题求解，具有理论保证、算法灵活性，并能扩展到大规模模型，在消费级硬件上实现高效推理。

Abstract: Flow-based generative models provide strong unconditional priors for inverse problems, but guiding their dynamics for conditional generation remains challenging. Recent work casts training-free conditional generation in flow models as an optimal control problem; however, solving the resulting trajectory optimisation is computationally and memory intensive, requiring differentiation through the flow dynamics or adjoint solves. We propose MPC-Flow, a model predictive control framework that formulates inverse problem solving with flow-based generative models as a sequence of control sub-problems, enabling practical optimal control-based guidance at inference time. We provide theoretical guarantees linking MPC-Flow to the underlying optimal control objective and show how different algorithmic choices yield a spectrum of guidance algorithms, including regimes that avoid backpropagation through the generative model trajectory. We evaluate MPC-Flow on benchmark image restoration tasks, spanning linear and non-linear settings such as in-painting, deblurring, and super-resolution, and demonstrate strong performance and scalability to massive state-of-the-art architectures via training-free guidance of FLUX.2 (32B) in a quantised setting on consumer hardware.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [293] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: 提出基于高斯过程的贝叶斯优化框架，用于在异构多核架构上自动搜索最优调度配置，平衡能耗与延迟的权衡，并通过敏感性分析提供物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 在后登纳德时代，嵌入式系统优化需要在能耗效率和延迟之间进行复杂的权衡。传统启发式调优在高维、非平滑的搜索空间中效率低下。

Method: 使用高斯过程的贝叶斯优化框架，近似能耗与时间的帕累托前沿，结合敏感性分析（fANOVA）比较不同协方差核函数（如Matérn与RBF），为黑盒模型提供物理可解释性。

Result: 揭示了驱动系统性能的主导硬件参数，实现了在异构多核架构上自动搜索最优调度配置，有效平衡能耗与延迟的多目标优化。

Conclusion: 贝叶斯优化框架能够高效处理嵌入式系统优化中的高维非平滑搜索空间，通过敏感性分析增强模型可解释性，为异构多核架构的调度配置优化提供了有效方法。

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [294] [Evaluating the Effectiveness of OpenAI's Parental Control System](https://arxiv.org/abs/2601.23062)
*Kerem Ersoz,Saleh Afroogh,David Atkinson,Junfeng Jiao*

Main category: cs.CY

TL;DR: 研究评估主流对话助手平台级家长控制对未成年人的保护效果，发现通知机制有选择性而非全面，存在政策与产品间的差距，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估平台级家长控制在主流对话助手中对未成年人的实际保护效果，了解现有控制机制的有效性和局限性，特别是家长通知系统是否真正发挥作用。

Method: 采用两阶段协议：首先通过PAIR式迭代提示优化构建类别平衡的对话语料库，然后由训练有素的人类代理在消费者UI中使用指定儿童账户重播/优化这些提示，同时监控链接的家长收件箱获取警报。聚焦七个风险领域，量化四个结果指标。

Result: 通知机制具有选择性：隐私暴力、欺诈、仇恨言论和恶意软件未触发家长警报，而物理伤害（最高）、色情内容和部分健康查询产生间歇性警报。当前后端比旧版模型泄漏更少，但对敏感话题附近的良性教育查询过度屏蔽仍然常见，且未向家长展示。

Conclusion: 平台级家长控制存在政策与产品间的差距，屏幕上的安全保护与面向家长的遥测数据不匹配。建议扩展/配置通知分类法，将可见安全保护与隐私保护的家长摘要相结合，并优先采用校准的、适合年龄的安全重写而非全面拒绝。

Abstract: We evaluate how effectively platform-level parental controls moderate a mainstream conversational assistant used by minors. Our two-phase protocol first builds a category-balanced conversation corpus via PAIR-style iterative prompt refinement over API, then has trained human agents replay/refine those prompts in the consumer UI using a designated child account while monitoring the linked parent inbox for alerts. We focus on seven risk areas -- physical harm, pornography, privacy violence, health consultation, fraud, hate speech, and malware and quantify four outcomes: Notification Rate (NR), Leak-Through (LR), Overblocking (OBR), and UI Intervention Rate (UIR). Using an automated judge (with targeted human audit) and comparing the current backend to legacy variants (GPT-4.1/4o), we find that notifications are selective rather than comprehensive: privacy violence, fraud, hate speech, and malware triggered no parental alerts in our runs, whereas physical harm (highest), pornography, and some health queries produced intermittent alerts. The current backend shows lower leak-through than legacy models, yet overblocking of benign, educational queries near sensitive topics remains common and is not surfaced to parents, revealing a policy-product gap between on-screen safeguards and parent-facing telemetry. We propose actionable fixes: broaden/configure the notification taxonomy, couple visible safeguards to privacy-preserving parent summaries, and prefer calibrated, age-appropriate safe rewrites over blanket refusals.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [295] [Large Language Models: A Mathematical Formulation](https://arxiv.org/abs/2601.22170)
*Ricardo Baptista,Andrew Stuart,Son Tran*

Main category: math.NA

TL;DR: 该论文为大型语言模型（LLMs）提供了一个数学框架，涵盖文本编码、架构设计、学习方法和任务部署，基于信息论、概率和优化等基础数学概念。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理文本序列、回答问题以及执行文档摘要、推荐、编程和定量问题解决等任务方面表现出色，但缺乏系统的数学框架来理解其工作原理、评估准确性、效率和鲁棒性，并指导新方法的开发。

Method: 建立了一个综合数学框架，包括：1）将文本序列编码为token序列；2）定义下一个token预测模型的架构；3）解释如何从数据中学习这些模型；4）展示如何部署模型处理各种任务。

Result: 该框架基于信息论、概率和优化等基础数学概念，构建了复杂的算法结构，这些结构在实践中已展现出显著的实证成功，为分析和改进LLMs提供了理论基础。

Conclusion: 建立的数学框架为LLMs的准确性、效率和鲁棒性分析提供了平台，同时为改进现有方法和开发新方法指明了方向，有助于推动LLM技术的进一步发展。

Abstract: Large language models (LLMs) process and predict sequences containing text to answer questions, and address tasks including document summarization, providing recommendations, writing software and solving quantitative problems. We provide a mathematical framework for LLMs by describing the encoding of text sequences into sequences of tokens, defining the architecture for next-token prediction models, explaining how these models are learned from data, and demonstrating how they are deployed to address a variety of tasks. The mathematical sophistication required to understand this material is not high, and relies on straightforward ideas from information theory, probability and optimization. Nonetheless, the combination of ideas resting on these different components from the mathematical sciences yields a complex algorithmic structure; and this algorithmic structure has demonstrated remarkable empirical successes. The mathematical framework established here provides a platform from which it is possible to formulate and address questions concerning the accuracy, efficiency and robustness of the algorithms that constitute LLMs. The framework also suggests directions for development of modified and new methodologies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [296] [Dependence-Aware Label Aggregation for LLM-as-a-Judge via Ising Models](https://arxiv.org/abs/2601.22336)
*Krishnakumar Balasubramanian,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan*

Main category: stat.ML

TL;DR: 论文研究大规模AI评估中聚合多个标注者（包括LLM作为法官）的二元判断问题，提出考虑标注者依赖关系的层次模型，证明传统条件独立假设方法存在严格次优性，并在真实数据集上验证了改进效果。


<details>
  <summary>Details</summary>
Motivation: 大规模AI评估通常需要聚合K个标注者（包括LLM作为法官）的二元判断。传统方法如Dawid-Skene或加权多数投票假设标注者在给定真实标签Y的条件下相互独立，但LLM法官由于共享数据、架构、提示和失败模式，这一假设经常被违反。忽略这种依赖关系会导致后验概率校准错误甚至自信的错误预测。

Method: 通过基于Ising图模型和潜在因子的层次化依赖感知模型研究标签聚合。对于类依赖Ising模型，贝叶斯对数优势通常是投票数的二次函数；对于类独立耦合，则简化为具有相关性调整参数的线性加权投票。提出了有限K示例展示条件独立方法可能翻转贝叶斯标签，并证明了这些方法在法官数量增长时保持严格次优的分离结果。

Result: 在三个真实世界数据集上评估了所提出的方法，证明其性能优于经典基线方法。理论分析表明，基于条件独立的方法即使在匹配每个标注者边际的情况下也可能翻转贝叶斯标签，并且在潜在因子下随着法官数量增长会产生非零的额外风险。

Conclusion: 在大规模AI评估中，考虑标注者之间的依赖关系至关重要，特别是当使用LLM作为法官时。提出的依赖感知层次模型能够更好地处理标注者之间的相关性，相比传统条件独立假设方法具有理论上的优势和实践中的改进效果。

Abstract: Large-scale AI evaluation increasingly relies on aggregating binary judgments from $K$ annotators, including LLMs used as judges. Most classical methods, e.g., Dawid-Skene or (weighted) majority voting, assume annotators are conditionally independent given the true label $Y\in\{0,1\}$, an assumption often violated by LLM judges due to shared data, architectures, prompts, and failure modes. Ignoring such dependencies can yield miscalibrated posteriors and even confidently incorrect predictions. We study label aggregation through a hierarchy of dependence-aware models based on Ising graphical models and latent factors. For class-dependent Ising models, the Bayes log-odds is generally quadratic in votes; for class-independent couplings, it reduces to a linear weighted vote with correlation-adjusted parameters. We present finite-$K$ examples showing that methods based on conditional independence can flip the Bayes label despite matching per-annotator marginals. We prove separation results demonstrating that these methods remain strictly suboptimal as the number of judges grows, incurring nonvanishing excess risk under latent factors. Finally, we evaluate the proposed method on three real-world datasets, demonstrating improved performance over the classical baselines.

</details>


### [297] [Amortized Simulation-Based Inference in Generalized Bayes via Neural Posterior Estimation](https://arxiv.org/abs/2601.22367)
*Shiyi Sun,Geoff K. Nicholls,Jeong Eun Lee*

Main category: stat.ML

TL;DR: 提出首个完全摊销的变分近似方法，通过训练单个(x,β)条件神经后验估计器，实现广义贝叶斯推断中温度化后验的单次前向采样，无需模拟器调用或推理时MCMC。


<details>
  <summary>Details</summary>
Motivation: 现有广义贝叶斯推断方法依赖昂贵的MCMC或SDE采样器，且需要为每个新数据集和每个β值重新运行，计算成本高且不灵活。

Method: 训练单个(x,β)条件神经后验估计器q_φ(θ|x,β)，采用两种互补训练策略：1)合成离流形样本(θ,x)∼π(θ)p(x|θ)^β；2)使用自归一化重要性采样重加权固定基础数据集π(θ)p(x|θ)。

Result: 在四个标准模拟推理基准测试（包括混沌Lorenz-96系统）中，β摊销估计器在标准两样本指标上实现了有竞争力的后验近似，在宽温度范围内匹配非摊销MCMC功率后验采样器。

Conclusion: 该方法首次实现了广义贝叶斯推断温度化后验的完全摊销变分近似，显著提高了计算效率，为模型错误设定下的稳健推理提供了实用解决方案。

Abstract: Generalized Bayesian Inference (GBI) tempers a loss with a temperature $β>0$ to mitigate overconfidence and improve robustness under model misspecification, but existing GBI methods typically rely on costly MCMC or SDE-based samplers and must be re-run for each new dataset and each $β$ value. We give the first fully amortized variational approximation to the tempered posterior family $p_β(θ\mid x) \propto π(θ)\,p(x \mid θ)^β$ by training a single $(x,β)$-conditioned neural posterior estimator $q_φ(θ\mid x,β)$ that enables sampling in a single forward pass, without simulator calls or inference-time MCMC. We introduce two complementary training routes: (i) synthesize off-manifold samples $(θ,x) \sim π(θ)\,p(x \mid θ)^β$ and (ii) reweight a fixed base dataset $π(θ)\,p(x \mid θ)$ using self-normalized importance sampling (SNIS). We show that the SNIS-weighted objective provides a consistent forward-KL fit to the tempered posterior with finite weight variance. Across four standard simulation-based inference (SBI) benchmarks, including the chaotic Lorenz-96 system, our $β$-amortized estimator achieves competitive posterior approximations in standard two-sample metrics, matching non-amortized MCMC-based power-posterior samplers over a wide range of temperatures.

</details>


### [298] [It's all the (Exponential) Family: An Equivalence between Maximum Likelihood Estimation and Control Variates for Sketching Algorithms](https://arxiv.org/abs/2601.22378)
*Keegan Kang,Kerong Wang,Ding Zhang,Rameshwar Pratap,Bhisham Dev Verma,Benedict H. W. Wong*

Main category: stat.ML

TL;DR: 论文证明了在指数族分布中，最优控制变量估计器能达到与最大似然估计器相同的渐近方差，并提出了相应的EM算法，该算法在数值稳定性和计算速度上优于其他求根算法。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中，最大似然估计器和控制变量估计器常结合使用，但需要更高效、稳定的算法来找到MLE，特别是在控制变量权重已知的情况下。

Method: 在指数族分布满足特定条件下，证明最优CVE能达到与MLE相同的渐近方差，并基于此推导出用于寻找MLE的期望最大化算法。

Result: 实验表明，对于二元正态分布，EM算法比其他求根算法更快且数值更稳定；该算法还能提高使用MLE/CVE算法的可重复性，并在CV权重已知时有效找到MLE。

Conclusion: 在满足条件的分布中，最优CVE与MLE具有相同的渐近方差，提出的EM算法为寻找MLE提供了高效、稳定的解决方案，并增强了算法的可重复性。

Abstract: Maximum likelihood estimators (MLE) and control variate estimators (CVE) have been used in conjunction with known information across sketching algorithms and applications in machine learning. We prove that under certain conditions in an exponential family, an optimal CVE will achieve the same asymptotic variance as the MLE, giving an Expectation-Maximization (EM) algorithm for the MLE. Experiments show the EM algorithm is faster and numerically stable compared to other root finding algorithms for the MLE for the bivariate Normal distribution, and we expect this to hold across distributions satisfying these conditions. We show how the EM algorithm leads to reproducibility for algorithms using MLE / CVE, and demonstrate how the EM algorithm leads to finding the MLE when the CV weights are known.

</details>


### [299] [Simulation-based Bayesian inference with ameliorative learned summary statistics -- Part I](https://arxiv.org/abs/2601.22441)
*Getachew K. Befekadu*

Main category: stat.ML

TL;DR: 本文提出一种基于模拟的推断方法，使用学习到的摘要统计量作为经验似然，在贝叶斯框架下处理难以获得闭式似然函数的情况，通过Cressie-Read差异准则转换数据到学习统计量，支持分布式计算。


<details>
  <summary>Details</summary>
Motivation: 当观测数据与模拟模型的精确似然函数难以获得闭式形式或计算不可行时，需要一种有效的推断方法。特别是在贝叶斯设置中，需要能够处理复杂模拟模型和大数据集的推断框架。

Method: 使用学习到的摘要统计量作为经验似然，通过Cressie-Read差异准则在矩约束下转换数据到学习统计量，保持推断的统计功效。该方法允许将模拟输出条件于观测数据，并支持分布式优化和MCMC算法。

Result: 提出的框架能够处理难以获得闭式似然函数的情况，保持统计推断的功效，支持条件模拟，可扩展到弱相关观测数据，并适合分布式计算实现。

Conclusion: 该基于模拟的推断框架为处理复杂模拟模型和大数据集提供了一种有效的解决方案，通过将数据到学习统计量的转换与贝叶斯推断统一为分布式推断问题，支持大规模计算。

Abstract: This paper, which is Part 1 of a two-part paper series, considers a simulation-based inference with learned summary statistics, in which such a learned summary statistic serves as an empirical-likelihood with ameliorative effects in the Bayesian setting, when the exact likelihood function associated with the observation data and the simulation model is difficult to obtain in a closed form or computationally intractable. In particular, a transformation technique which leverages the Cressie-Read discrepancy criterion under moment restrictions is used for summarizing the learned statistics between the observation data and the simulation outputs, while preserving the statistical power of the inference. Here, such a transformation of data-to-learned summary statistics also allows the simulation outputs to be conditioned on the observation data, so that the inference task can be performed over certain sample sets of the observation data that are considered as an empirical relevance or believed to be particular importance. Moreover, the simulation-based inference framework discussed in this paper can be extended further, and thus handling weakly dependent observation data. Finally, we remark that such an inference framework is suitable for implementation in distributed computing, i.e., computational tasks involving both the data-to-learned summary statistics and the Bayesian inferencing problem can be posed as a unified distributed inference problem that will exploit distributed optimization and MCMC algorithms for supporting large datasets associated with complex simulation models.

</details>


### [300] [Corrected Samplers for Discrete Flow Models](https://arxiv.org/abs/2601.22519)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

TL;DR: 本文针对离散流模型的采样器（如tau-leaping和Euler求解器）提出两种校正采样器，无需限制转移率或源分布即可降低离散化误差，提高生成质量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型的采样器需要大量迭代来控制离散化误差，且理论结果通常需要转移率有界或特定源分布的限制。本文旨在克服这些限制，建立无限制条件下的离散化误差界限，并提出更高效的校正采样器。

Method: 1. 建立无限制转移率和源分布条件下的非渐近离散化误差界限；2. 通过分析Euler采样器的一步下界，提出时间校正采样器和位置校正采样器；3. 位置校正采样器相比现有并行采样器具有更低的迭代复杂度。

Result: 在仿真和文本到图像生成任务中验证了方法的有效性，展示了改进的生成质量和减少的推理时间。位置校正采样器相比现有并行采样器具有更低的迭代复杂度。

Conclusion: 提出的校正采样器能够以几乎无额外计算成本的方式减少tau-leaping和Euler求解器的离散化误差，为离散流模型提供了更高效的采样方法。

Abstract: Discrete flow models (DFMs) have been proposed to learn the data distribution on a finite state space, offering a flexible framework as an alternative to discrete diffusion models. A line of recent work has studied samplers for discrete diffusion models, such as tau-leaping and Euler solver. However, these samplers require a large number of iterations to control discretization error, since the transition rates are frozen in time and evaluated at the initial state within each time interval. Moreover, theoretical results for these samplers often require boundedness conditions of the transition rate or they focus on a specific type of source distributions. To address those limitations, we establish non-asymptotic discretization error bounds for those samplers without any restriction on transition rates and source distributions, under the framework of discrete flow models. Furthermore, by analyzing a one-step lower bound of the Euler sampler, we propose two corrected samplers: \textit{time-corrected sampler} and \textit{location-corrected sampler}, which can reduce the discretization error of tau-leaping and Euler solver with almost no additional computational cost. We rigorously show that the location-corrected sampler has a lower iteration complexity than existing parallel samplers. We validate the effectiveness of the proposed method by demonstrating improved generation quality and reduced inference time on both simulation and text-to-image generation tasks. Code can be found in https://github.com/WanZhengyan/Corrected-Samplers-for-Discrete-Flow-Models.

</details>


### [301] [An Efficient Algorithm for Thresholding Monte Carlo Tree Search](https://arxiv.org/abs/2601.22600)
*Shoma Nameki,Atsuyoshi Nakamura,Junpei Komiyama,Koji Tabata*

Main category: stat.ML

TL;DR: 提出阈值蒙特卡洛树搜索问题，开发基于Track-and-Stop策略的δ正确序列采样算法，具有渐近最优样本复杂度，并通过比率修改D-Tracking策略显著改善经验性能。


<details>
  <summary>Details</summary>
Motivation: 解决在具有MAX/MIN节点的树结构中，需要判断根节点值是否超过给定阈值的问题。传统方法需要大量采样，且计算成本高，需要更高效的算法。

Method: 基于Track-and-Stop策略开发δ正确序列采样算法，采用比率修改的D-Tracking臂拉动策略，将每轮计算成本从线性降低到对数级别。

Result: 算法具有渐近最优样本复杂度，经验样本复杂度显著改善，计算效率大幅提升。

Conclusion: 提出的算法在阈值蒙特卡洛树搜索问题上实现了理论最优性和实践效率的平衡，为类似问题提供了有效解决方案。

Abstract: We introduce the Thresholding Monte Carlo Tree Search problem, in which, given a tree $\mathcal{T}$ and a threshold $θ$, a player must answer whether the root node value of $\mathcal{T}$ is at least $θ$ or not. In the given tree, `MAX' or `MIN' is labeled on each internal node, and the value of a `MAX'-labeled (`MIN'-labeled) internal node is the maximum (minimum) of its child values. The value of a leaf node is the mean reward of an unknown distribution, from which the player can sample rewards. For this problem, we develop a $δ$-correct sequential sampling algorithm based on the Track-and-Stop strategy that has asymptotically optimal sample complexity. We show that a ratio-based modification of the D-Tracking arm-pulling strategy leads to a substantial improvement in empirical sample complexity, as well as reducing the per-round computational cost from linear to logarithmic in the number of arms.

</details>


### [302] [RPWithPrior: Label Differential Privacy in Regression](https://arxiv.org/abs/2601.22625)
*Haixia Liu,Ruifan Huang*

Main category: stat.ML

TL;DR: 提出一种新的回归任务标签差分隐私保护方法，通过连续随机变量建模避免离散化，在已知或未知先验分布情况下都能获得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有回归任务的ε-标签差分隐私方法（如RR-On-Bins）需要将输出空间离散化为有限区间，并通过向下取整操作确定这些区间，这与现实场景不匹配。需要避免离散化操作，更好地保护用户隐私同时最小化精度损失。

Method: 将原始响应和随机化响应建模为连续随机变量，完全避免离散化。提出估计随机化响应最优区间的新算法，包括已知先验分布（RPWithPrior）和未知先验分布两种情况。证明RPWithPrior算法保证ε-标签差分隐私。

Result: 在Communities and Crime、Criteo Sponsored Search Conversion Log、California Housing数据集上的数值实验表明，该方法在性能上优于高斯、拉普拉斯、阶梯、RRonBins和无偏机制等现有方法。

Conclusion: 通过连续随机变量建模避免离散化，提出的新方法在回归任务的标签差分隐私保护中取得了更好的性能，为实际应用提供了更有效的隐私保护方案。

Abstract: With the wide application of machine learning techniques in practice, privacy preservation has gained increasing attention. Protecting user privacy with minimal accuracy loss is a fundamental task in the data analysis and mining community. In this paper, we focus on regression tasks under $ε$-label differential privacy guarantees. Some existing methods for regression with $ε$-label differential privacy, such as the RR-On-Bins mechanism, discretized the output space into finite bins and then applied RR algorithm. To efficiently determine these finite bins, the authors rounded the original responses down to integer values. However, such operations does not align well with real-world scenarios. To overcome these limitations, we model both original and randomized responses as continuous random variables, avoiding discretization entirely. Our novel approach estimates an optimal interval for randomized responses and introduces new algorithms designed for scenarios where a prior is either known or unknown. Additionally, we prove that our algorithm, RPWithPrior, guarantees $ε$-label differential privacy. Numerical results demonstrate that our approach gets better performance compared with the Gaussian, Laplace, Staircase, and RRonBins, Unbiased mechanisms on the Communities and Crime, Criteo Sponsored Search Conversion Log, California Housing datasets.

</details>


### [303] [Generative and Nonparametric Approaches for Conditional Distribution Estimation: Methods, Perspectives, and Comparative Evaluations](https://arxiv.org/abs/2601.22650)
*Yen-Shiu Chin,Zhi-Yu Jou,Toshinari Morimoto,Chia-Tse Wang,Ming-Chung Chang,Tso-Jung Yen,Su-Yun Huang,Tailen Hsing*

Main category: stat.ML

TL;DR: 本文系统回顾和比较了条件分布推断的代表性方法，包括经典非参数方法和现代生成模型，通过统一评估框架进行数值比较，分析各方法的灵活性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 条件分布推断是统计学中的基本问题，对预测、不确定性量化和概率建模至关重要。现有方法众多但缺乏系统比较，需要建立公平评估框架来理解不同方法的优缺点。

Method: 1) Hall和Yao(2005)的单指标方法：通过降维指标和一维累积条件分布函数的非参数平滑；2) 基展开方法：包括FlexCode和DeepCDE，将条件密度估计转化为非参数回归问题；3) 生成模拟方法：包括生成条件分布采样器和条件去噪扩散概率模型。使用统一评估框架进行系统数值比较，评估指标包括条件均值和标准差均方误差、Wasserstein距离。

Result: 通过系统数值比较，揭示了不同方法在性能、灵活性和计算成本方面的差异。每种方法都有其独特的优势和局限性，具体表现取决于应用场景和评估指标。

Conclusion: 条件分布推断方法多样，从经典非参数方法到现代生成模型各有特点。统一的评估框架有助于公平比较，理解不同方法的适用场景，为方法选择提供指导。

Abstract: The inference of conditional distributions is a fundamental problem in statistics, essential for prediction, uncertainty quantification, and probabilistic modeling. A wide range of methodologies have been developed for this task. This article reviews and compares several representative approaches spanning classical nonparametric methods and modern generative models. We begin with the single-index method of Hall and Yao (2005), which estimates the conditional distribution through a dimension-reducing index and nonparametric smoothing of the resulting one-dimensional cumulative conditional distribution function. We then examine the basis-expansion approaches, including FlexCode (Izbicki and Lee, 2017) and DeepCDE (Dalmasso et al., 2020), which convert conditional density estimation into a set of nonparametric regression problems. In addition, we discuss two recent generative simulation-based methods that leverage modern deep generative architectures: the generative conditional distribution sampler (Zhou et al., 2023) and the conditional denoising diffusion probabilistic model (Fu et al., 2024; Yang et al., 2025). A systematic numerical comparison of these approaches is provided using a unified evaluation framework that ensures fairness and reproducibility. The performance metrics used for the estimated conditional distribution include the mean-squared errors of conditional mean and standard deviation, as well as the Wasserstein distance. We also discuss their flexibility and computational costs, highlighting the distinct advantages and limitations of each approach.

</details>


### [304] [Spectral Gradient Descent Mitigates Anisotropy-Driven Misalignment: A Case Study in Phase Retrieval](https://arxiv.org/abs/2601.22652)
*Guillaume Braun,Han Bao,Wei Huang,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 研究通过非线性相位检索模型分析谱梯度方法（如Muon优化器）在深度学习中的优势机制，发现梯度下降在早期训练阶段会放大高方差但无信息的噪声方向，而谱梯度方法能消除这种放大效应，实现更稳定的信号对齐和更快的噪声收缩。


<details>
  <summary>Details</summary>
Motivation: 谱梯度方法（如Muon优化器）在深度学习中表现出色，但缺乏对其优势机制的理论理解。本研究旨在通过理论分析揭示谱梯度方法相比传统梯度下降的性能提升机制。

Method: 使用非线性相位检索模型（相当于训练具有二次激活函数和固定第二层权重的两层神经网络），在尖峰协方差设置下进行动力学分析，其中主导方差方向与信号正交。通过理论分析和数值实验验证梯度下降和谱梯度下降的不同行为。

Result: 梯度下降在早期逃离阶段会放大高方差但无信息的尖峰方向，导致与真实信号的错位，特别是在强各向异性下。而谱梯度下降能消除这种尖峰放大效应，实现稳定的信号对齐和加速的噪声收缩。数值实验证实了理论预测，并表明这些现象在更广泛的各向异性协方差下持续存在。

Conclusion: 谱梯度方法的优势在于能够消除梯度下降中由方差引起的错位问题，通过保留方向信息而丢弃尺度信息，避免了高方差噪声方向的放大，从而在训练早期实现更好的信号对齐和更快的收敛。

Abstract: Spectral gradient methods, such as the Muon optimizer, modify gradient updates by preserving directional information while discarding scale, and have shown strong empirical performance in deep learning. We investigate the mechanisms underlying these gains through a dynamical analysis of a nonlinear phase retrieval model with anisotropic Gaussian inputs, equivalent to training a two-layer neural network with the quadratic activation and fixed second-layer weights. Focusing on a spiked covariance setting where the dominant variance direction is orthogonal to the signal, we show that gradient descent (GD) suffers from a variance-induced misalignment: during the early escaping stage, the high-variance but uninformative spike direction is multiplicatively amplified, degrading alignment with the true signal under strong anisotropy. In contrast, spectral gradient descent (SpecGD) removes this spike amplification effect, leading to stable alignment and accelerated noise contraction. Numerical experiments confirm the theory and show that these phenomena persist under broader anisotropic covariances.

</details>


### [305] [GRANITE: A Generalized Regional Framework for Identifying Agreement in Feature-Based Explanations](https://arxiv.org/abs/2601.22771)
*Julia Herbinger,Gabriel Laberge,Maximilian Muschalik,Yann Pequignot,Marvin N. Wright,Fabian Fumagalli*

Main category: stat.ML

TL;DR: GRANITE是一个广义区域解释框架，通过将特征空间划分为交互和分布影响最小化的区域，统一不同解释方法，提供更一致可解释的特征解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征的解释方法经常产生相互矛盾的解释，主要源于两个问题：如何处理特征交互以及如何纳入特征依赖关系。这种不一致性降低了解释的可信度和实用性。

Method: 提出GRANITE框架：1）将特征空间划分为交互和分布影响最小化的区域；2）统一现有区域方法并扩展到特征组；3）引入递归分区算法来估计这些区域；4）在不同区域应用不同的解释方法。

Result: 在真实世界数据集上验证了GRANITE的有效性，能够对齐不同的解释方法，产生更一致和可解释的特征解释，提供了实用的解释一致性工具。

Conclusion: GRANITE通过区域划分解决了特征解释中的不一致问题，统一了现有方法，为机器学习模型提供了更可靠、一致的特征解释框架。

Abstract: Feature-based explanation methods aim to quantify how features influence the model's behavior, either locally or globally, but different methods often disagree, producing conflicting explanations. This disagreement arises primarily from two sources: how feature interactions are handled and how feature dependencies are incorporated. We propose GRANITE, a generalized regional explanation framework that partitions the feature space into regions where interaction and distribution influences are minimized. This approach aligns different explanation methods, yielding more consistent and interpretable explanations. GRANITE unifies existing regional approaches, extends them to feature groups, and introduces a recursive partitioning algorithm to estimate such regions. We demonstrate its effectiveness on real-world datasets, providing a practical tool for consistent and interpretable feature explanations.

</details>


### [306] [Approximating $f$-Divergences with Rank Statistics](https://arxiv.org/abs/2601.22784)
*Viktor Stein,José Manuel de Frutos*

Main category: stat.ML

TL;DR: 提出基于秩统计量的f-散度近似方法，避免显式密度比估计，通过秩直方图测量分布差异，并扩展到高维切片版本。


<details>
  <summary>Details</summary>
Motivation: 传统f-散度估计需要显式密度比估计，这在实践中具有挑战性。本文旨在开发一种避免密度比估计的f-散度近似方法，通过秩统计量直接测量分布差异。

Method: 将两个单变量分布μ和ν的差异映射到{0,...,K}上的秩直方图，通过离散f-散度测量其与均匀分布的偏差。扩展到高维时，使用随机投影平均单变量构造得到切片秩统计量f-散度。

Result: 证明估计量在K上是单调的，始终是真实f-散度的下界，在温和条件下建立K→∞的收敛速率。提供有限样本偏差界和渐近正态性结果。实证验证表明方法优于神经基线，在生成建模中有效。

Conclusion: 提出的秩统计量f-散度近似方法避免了密度比估计，具有理论保证和实际有效性，为分布比较和生成建模提供了新工具。

Abstract: We introduce a rank-statistic approximation of $f$-divergences that avoids explicit density-ratio estimation by working directly with the distribution of ranks. For a resolution parameter $K$, we map the mismatch between two univariate distributions $μ$ and $ν$ to a rank histogram on $\{ 0, \ldots, K\}$ and measure its deviation from uniformity via a discrete $f$-divergence, yielding a rank-statistic divergence estimator. We prove that the resulting estimator of the divergence is monotone in $K$, is always a lower bound of the true $f$-divergence, and we establish quantitative convergence rates for $K\to\infty$ under mild regularity of the quantile-domain density ratio. To handle high-dimensional data, we define the sliced rank-statistic $f$-divergence by averaging the univariate construction over random projections, and we provide convergence results for the sliced limit as well. We also derive finite-sample deviation bounds along with asymptotic normality results for the estimator. Finally, we empirically validate the approach by benchmarking against neural baselines and illustrating its use as a learning objective in generative modelling experiments.

</details>


### [307] [OneFlowSBI: One Model, Many Queries for Simulation-Based Inference](https://arxiv.org/abs/2601.22951)
*Mayank Nautiyal,Li Ju,Melker Ernfors,Klara Hagland,Ville Holma,Maximilian Werkö Söderholm,Andreas Hellander,Prashant Singh*

Main category: stat.ML

TL;DR: OneFlowSBI是一个统一的基于仿真的推理框架，通过单一流匹配生成模型学习参数和观测的联合分布，支持多种推理任务而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的基于仿真的推理方法通常需要为不同任务训练专门模型，缺乏统一的框架来同时支持后验采样、似然估计和任意条件分布等多种推理任务。

Method: 提出OneFlowSBI框架，使用流匹配技术学习参数和观测的联合分布，在训练时采用查询感知的掩码分布，使单一模型能够支持多种推理任务。

Result: 在10个基准推理问题和2个高维现实世界逆问题上评估，OneFlowSBI在多种仿真预算下表现出与最先进的广义推理求解器和专门后验估计器相当的竞争力，同时能够用少量ODE积分步骤高效采样，并在噪声和部分观测数据下保持鲁棒性。

Conclusion: OneFlowSBI提供了一个统一的、高效的基于仿真的推理框架，能够用一个模型解决多种推理任务，在性能和效率方面都表现出色。

Abstract: We introduce \textit{OneFlowSBI}, a unified framework for simulation-based inference that learns a single flow-matching generative model over the joint distribution of parameters and observations. Leveraging a query-aware masking distribution during training, the same model supports multiple inference tasks, including posterior sampling, likelihood estimation, and arbitrary conditional distributions, without task-specific retraining. We evaluate \textit{OneFlowSBI} on ten benchmark inference problems and two high-dimensional real-world inverse problems across multiple simulation budgets. \textit{OneFlowSBI} is shown to deliver competitive performance against state-of-the-art generalized inference solvers and specialized posterior estimators, while enabling efficient sampling with few ODE integration steps and remaining robust under noisy and partially observed data.

</details>


### [308] [Neural Backward Filtering Forward Guiding](https://arxiv.org/abs/2601.23030)
*Gefan Yang,Frank van der Meulen,Stefan Sommer*

Main category: stat.ML

TL;DR: 提出Neural Backward Filtering Forward Guiding (NBFFG)框架，用于解决树结构上非线性连续随机过程的推理问题，特别适用于稀疏观测和复杂拓扑情况。


<details>
  <summary>Details</summary>
Motivation: 树结构上非线性连续随机过程的推理具有挑战性，特别是当观测稀疏（仅叶节点）且拓扑复杂时。Doob's h-变换的精确平滑方法对于一般非线性动态不可行，而基于粒子的方法在高维情况下性能下降。

Method: 提出NBFFG统一框架，利用辅助线性高斯过程构建变分后验。该辅助过程产生闭式后向滤波器作为"引导"，将生成路径导向高似然区域。然后学习神经残差（参数化为归一化流或受控SDE）来捕捉非线性差异。该公式允许无偏路径子采样方案，将训练复杂度从依赖树大小降低到依赖路径长度。

Result: 实验结果表明NBFFG在合成基准测试中优于基线方法，并在系统发育分析的高维推理任务中成功重建了蝴蝶翅膀形状的祖先形态。

Conclusion: NBFFG为树结构上非线性连续随机过程的推理提供了一个有效的统一框架，特别适用于稀疏观测和高维情况，在系统发育分析等实际应用中展现出潜力。

Abstract: Inference in non-linear continuous stochastic processes on trees is challenging, particularly when observations are sparse (leaf-only) and the topology is complex. Exact smoothing via Doob's $h$-transform is intractable for general non-linear dynamics, while particle-based methods degrade in high dimensions. We propose Neural Backward Filtering Forward Guiding (NBFFG), a unified framework for both discrete transitions and continuous diffusions. Our method constructs a variational posterior by leveraging an auxiliary linear-Gaussian process. This auxiliary process yields a closed-form backward filter that serves as a ``guide'', steering the generative path toward high-likelihood regions. We then learn a neural residual--parameterized as a normalizing flow or a controlled SDE--to capture the non-linear discrepancies. This formulation allows for an unbiased path-wise subsampling scheme, reducing the training complexity from tree-size dependent to path-length dependent. Empirical results show that NBFFG outperforms baselines on synthetic benchmarks, and we demonstrate the method on a high-dimensional inference task in phylogenetic analysis with reconstruction of ancestral butterfly wing shapes.

</details>


### [309] [Asymptotic Theory of Iterated Empirical Risk Minimization, with Applications to Active Learning](https://arxiv.org/abs/2601.23031)
*Hugo Cui,Yue M. Lu*

Main category: stat.ML

TL;DR: 研究两阶段迭代经验风险最小化(ERM)方法，其中第一阶段预测作为第二阶段损失函数的输入，应用于主动学习和重加权场景，在混合高斯数据上推导高维渐近误差分析


<details>
  <summary>Details</summary>
Motivation: 主动学习和重加权方案中自然出现两阶段ERM过程，其中第一阶段预测作为第二阶段损失函数的输入，这种数据重用和预测依赖损失引入了复杂的统计依赖性，与经典单阶段ERM分析有本质区别

Method: 对混合高斯数据上的线性模型，使用凸损失函数，推导高维渐近下测试误差的精确渐近特征，其中样本量和环境维度成比例缩放

Result: 为第二阶段估计器提供了显式、完全渐近的性能预测，应用于池式主动学习问题，消除了先前工作中的oracle和样本分割假设，揭示了标签预算分配的基本权衡，并展示了纯粹由数据选择驱动的测试误差双下降行为

Conclusion: 该理论框架能够分析数据重用和预测依赖损失的两阶段ERM过程，为主动学习等应用提供了理论基础，揭示了数据选择策略对性能的重要影响

Abstract: We study a class of iterated empirical risk minimization (ERM) procedures in which two successive ERMs are performed on the same dataset, and the predictions of the first estimator enter as an argument in the loss function of the second. This setting, which arises naturally in active learning and reweighting schemes, introduces intricate statistical dependencies across samples and fundamentally distinguishes the problem from classical single-stage ERM analyses. For linear models trained with a broad class of convex losses on Gaussian mixture data, we derive a sharp asymptotic characterization of the test error in the high-dimensional regime where the sample size and ambient dimension scale proportionally. Our results provide explicit, fully asymptotic predictions for the performance of the second-stage estimator despite the reuse of data and the presence of prediction-dependent losses. We apply this theory to revisit a well-studied pool-based active learning problem, removing oracle and sample-splitting assumptions made in prior work. We uncover a fundamental tradeoff in how the labeling budget should be allocated across stages, and demonstrate a double-descent behavior of the test error driven purely by data selection, rather than model size or sample count.

</details>


### [310] [A Random Matrix Theory of Masked Self-Supervised Regression](https://arxiv.org/abs/2601.23208)
*Arie Wortsman Zurich,Federica Gerace,Bruno Loureiro,Yue M. Lu*

Main category: stat.ML

TL;DR: 论文对掩码自监督学习在高维比例机制下进行精确分析，推导泛化误差表达式，揭示掩码建模如何从数据中提取结构，并证明在某些结构化机制下SSL优于PCA


<details>
  <summary>Details</summary>
Motivation: 掩码自监督学习已成为transformer模型的基础训练范式，其训练聚合多个掩码模式的预测，产生矩阵值预测器而非向量值估计器。这一对象编码了坐标之间的条件关系，带来了新的分析挑战。需要开发精确的高维分析来理解掩码建模如何从数据中提取结构

Method: 开发了掩码建模目标在比例机制下的精确高维分析，其中样本数量与环境维度成比例。使用尖峰协方差模型进行分析，研究联合预测器的谱结构

Result: 获得了泛化误差的显式表达式，表征了学习预测器的谱结构。对于尖峰协方差模型，发现联合预测器经历BBP型相变，确定了掩码SSL开始恢复潜在信号的条件。识别了掩码自监督学习可证明优于PCA的结构化机制

Conclusion: 掩码自监督学习通过聚合多个掩码模式的预测，能够有效提取数据中的结构信息。在某些结构化机制下，SSL目标相比经典无监督方法（如PCA）具有潜在优势，揭示了掩码建模的数学原理和性能边界

Abstract: In the era of transformer models, masked self-supervised learning (SSL) has become a foundational training paradigm. A defining feature of masked SSL is that training aggregates predictions across many masking patterns, giving rise to a joint, matrix-valued predictor rather than a single vector-valued estimator. This object encodes how coordinates condition on one another and poses new analytical challenges. We develop a precise high-dimensional analysis of masked modeling objectives in the proportional regime where the number of samples scales with the ambient dimension. Our results provide explicit expressions for the generalization error and characterize the spectral structure of the learned predictor, revealing how masked modeling extracts structure from data. For spiked covariance models, we show that the joint predictor undergoes a Baik--Ben Arous--Péché (BBP)-type phase transition, identifying when masked SSL begins to recover latent signals. Finally, we identify structured regimes in which masked self-supervised learning provably outperforms PCA, highlighting potential advantages of SSL objectives over classical unsupervised methods

</details>


### [311] [Graph Attention Network for Node Regression on Random Geometric Graphs with Erdős--Rényi contamination](https://arxiv.org/abs/2601.23239)
*Somak Laha,Suqi Liu,Morgane Austern*

Main category: stat.ML

TL;DR: 论文证明了在节点回归任务中，特定设计的图注意力网络（GAT）相比普通最小二乘估计和图卷积网络具有理论优势，特别是在处理噪声节点特征和边污染时。


<details>
  <summary>Details</summary>
Motivation: 尽管图注意力网络在实践中表现出对噪声的鲁棒性，但缺乏严格的理论证明其相对于非注意力图神经网络的优势。本文旨在填补这一空白，特别是在节点特征和边同时存在噪声污染的情况下。

Method: 提出并分析了一个精心设计的任务特定GAT，该网络构建去噪代理特征用于回归。在基于图的误差变量模型下，假设响应由潜在节点特征生成，但只观察到噪声污染的特征；样本图是由节点特征创建的随机几何图，但受到独立Erdős-Rényi边的污染。

Result: 理论证明：在温和增长条件下，使用代理特征进行回归在（a）估计回归系数方面比在噪声节点特征上的普通最小二乘估计具有更低的渐近误差；（b）预测未标记节点响应方面比普通图卷积网络更优。实验验证了理论发现。

Conclusion: 该研究为图注意力网络在噪声环境下的优势提供了严格的理论保证，证明了精心设计的GAT在节点回归任务中能够有效处理特征和边的噪声污染，相比传统方法具有统计优势。

Abstract: Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: responses are generated from latent node-level covariates, but only noise-perturbed versions of the latent covariates are observed; and the sample graph is a random geometric graph created from the node covariates but contaminated by independent Erdős--Rényi edges. We propose and analyze a carefully designed, task-specific GAT that constructs denoised proxy features for regression. We prove that regressing the response variables on the proxies achieves lower error asymptotically in (a) estimating the regression coefficient compared to the ordinary least squares (OLS) estimator on the noisy node covariates, and (b) predicting the response for an unlabelled node compared to a vanilla graph convolutional network~(GCN) -- under mild growth conditions. Our analysis leverages high-dimensional geometric tail bounds and concentration for neighbourhood counts and sample covariances. We verify our theoretical findings through experiments on synthetically generated data. We also perform experiments on real-world graphs and demonstrate the effectiveness of the attention mechanism in several node regression tasks.

</details>
