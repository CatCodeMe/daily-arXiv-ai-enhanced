<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 68]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.CV](#cs.CV) [Total: 9]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.PL](#cs.PL) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment](https://arxiv.org/abs/2512.05203)
*Vinicius Stein Dani,Xixi Lu,Iris Beerepoot*

Main category: cs.DB

TL;DR: 探索将可穿戴设备数据融入事件日志的三种方法：作为事件属性、案例属性或衍生新事件，用于个人生产力与健康分析


<details>
  <summary>Details</summary>
Motivation: 将可穿戴设备收集的健康数据与个人事件日志（如数字日历）结合，以增强过程挖掘在个人生产力与健康分析中的应用价值

Method: 提出三种方法：1) 将可穿戴数据作为事件属性直接关联到单个事件；2) 作为案例属性使用聚合的日级评分；3) 从可穿戴数据衍生新事件（如睡眠时段、体育活动）。使用真实个人数据（智能手表健康数据与数字日历事件）进行说明

Result: 展示了三种方法在实际数据中的应用示例，说明了如何将健康数据与日历事件进行匹配和整合

Conclusion: 整合可穿戴数据到过程挖掘面临技术和概念挑战，但为个人生产力与健康分析提供了新视角和可能性

Abstract: In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being.

</details>


### [2] [Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees](https://arxiv.org/abs/2512.05399)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 提出Featurized-Decomposition Join (FDJ)方法，通过特征提取和逻辑表达式大幅降低语义连接成本，相比现有方法成本减少高达10倍


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理大规模文本数据时，语义连接任务（如实体匹配、记录分类）需要评估每对元组的自然语言谓词，朴素实现成本极高。现有基于语义相似度过滤的方法效果有限，因为语义相似度不能可靠预测连接结果

Method: 提出FDJ方法：自动提取文本特征并组合成合取范式逻辑表达式（特征化分解），通过廉价的特征比较有效剪除非匹配对。使用LLM自动提取可靠特征并组合成逻辑表达式，同时提供统计保证

Result: 在真实数据集上的实验显示，相比最先进方法，成本降低高达10倍，同时保持相同的质量保证

Conclusion: FDJ方法通过特征化分解有效解决了语义连接的高成本问题，在保证质量的前提下显著降低了计算开销，为LLM在数据处理系统中的应用提供了更高效的解决方案

Abstract: Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees.

</details>


### [3] [PETGraphDB: A Property Evolution Temporal Graph Data Management System](https://arxiv.org/abs/2512.05417)
*Jinghe Song,Zongyu Zuo,Xuelian Lin,Yang Wang,Shuai Ma*

Main category: cs.DB

TL;DR: PETGraph是一个专门为属性演化时序图设计的数据管理系统，相比现有方案能节省67%存储空间，提升58.8倍事务吞吐和267倍查询性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统发展，属性演化时序图（节点/边属性频繁变化而拓扑结构基本不变）快速增长，但现有时序图管理方案不适用于此类数据，导致数据建模复杂且查询性能低下。

Method: 采用有效时间时序属性图数据模型简化数据建模，支持ACID事务；设计空间高效的时序属性存储和细粒度多级锁机制提升性能。

Result: PETGraph平均只需当前最佳方案33%的存储空间；在HTAP工作负载中实现平均58.8倍的事务吞吐提升；查询延迟平均比当前最佳方案快267倍。

Conclusion: PETGraph专门针对属性演化时序图设计，通过创新的数据模型和存储机制，显著提升了存储效率和查询性能，解决了现有方案在此类数据管理中的不足。

Abstract: Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency.

</details>


### [4] [Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments](https://arxiv.org/abs/2512.05453)
*Luc Moreau,Alfred Rossi,Sophie Stalla-Bourdillon*

Main category: cs.DB

TL;DR: Parajudica是一个基于RDF/SPARQL的开放、模块化、可扩展规则系统，用于评估多合规框架下的数据合规状态


<details>
  <summary>Details</summary>
Motivation: 解决在多个同时适用的合规框架下实施基于策略的数据访问控制(PBAC)的挑战

Method: 开发了基于RDF/SPARQL的开放、模块化、可扩展规则系统，包含配套的元模型，应用于现有法律框架和行业标准

Result: 展示了该资源及其元模型在现有法律框架和行业标准中的应用效用，为比较框架分析提供了见解

Conclusion: Parajudica系统可用于合规策略执行、合规监控、数据发现和风险评估等多个应用场景

Abstract: Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.

</details>


### [5] [Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement](https://arxiv.org/abs/2512.05525)
*Nils Strassenburg,Boris Glavic,Tilmann Rabl*

Main category: cs.DB

TL;DR: 提出JITR（即时模型替换）框架，在识别出LLM中的重复任务后，自动用更便宜的小模型透明替换，以降低成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 企业越来越多地使用大语言模型（LLMs）来自动化简单重复任务，但这带来了比小模型高得多的资源和能源消耗，而小模型在简单任务上通常能达到相似的预测性能。

Method: 提出JITR（即时模型替换）框架：1）识别LLM调用中的重复任务；2）透明地替换为针对该特定任务的更便宜替代模型；3）利用模型搜索和迁移学习高效识别和微调模型。

Result: 通过原型系统Poodle在示例任务上实现了显著的节省，证明了JITR框架的可行性。

Conclusion: JITR框架保留了LLMs的易用性和低开发成本优势，同时显著降低了成本和能源消耗，模型搜索和迁移学习将在实现这一愿景中发挥关键作用。

Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 研究基于真实轨迹数据，探讨在公交车上部署边缘服务器的可行性，通过数学模型和贪心算法优化公交选择以最大化需求点覆盖，证明公交车载边缘服务器在车联网中具有可行性、效益和价值。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置边缘服务器（如路边单元或基站）部署后位置和容量固定，难以有效处理时空动态变化的用户需求。移动服务器（如公交车）有潜力为车联网系统增加计算弹性，但需要验证其实际可行性。

Method: 1. 使用上海公交/出租车/电信数据集分析公交车和基站的覆盖情况；2. 建立数学模型并设计简单的贪心启发式算法，在有限预算下选择最优公交车以最大化需求点覆盖；3. 进行基于真实轨迹的仿真验证算法性能。

Result: 公交车覆盖了大部分地理区域和需求点，显示出巨大潜力。提出的公交选择算法能有效处理动态用户需求，并在服务器容量和购买数量等现实约束下表现良好。

Conclusion: 公交车载边缘服务器在城市车联网中是可行的、有益的且有价值的，能够有效补充固定边缘服务器，提高系统弹性。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [7] [Incorporating indel channels into average-case analysis of seed-chain-extend](https://arxiv.org/abs/2512.05247)
*Spencer Gibson,Yun William Yu*

Main category: cs.DS

TL;DR: 该论文将种子链扩展算法的理论分析扩展到包含插入和删除的突变通道，证明了在总突变率小于0.159时，最优链的期望恢复率至少为1-O(1/√m)，期望运行时间为O(mn^{3.15θ_T} log n)。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析仅适用于仅包含替换的突变通道，而实际基因组数据包含插入和删除(indels)，理论与实际之间存在差距。需要开发能够处理indels通道的数学工具，以解释种子链扩展算法在实际中的有效性。

Method: 引入新的数学工具来处理indels通道带来的两个新障碍：相邻锚点的依赖性和部分正确锚点的存在。通过理论分析证明在包含替换、插入和删除的总突变通道下，最优线性间隙成本链的性能边界。

Result: 证明了在总突变率θ_T = θ_i + θ_d + θ_s < 0.159时，最优链的期望恢复率至少为1-O(1/√m)，期望运行时间为O(mn^{3.15θ_T} log n)。这扩展了先前仅适用于替换通道的结果。

Conclusion: 该研究填补了种子链扩展算法理论与实际应用之间的差距，为包含indels的真实基因组数据提供了理论保证，解释了为什么该启发式方法在实际中仍然有效。

Abstract: Given a sequence $s_1$ of $n$ letters drawn i.i.d. from an alphabet of size $σ$ and a mutated substring $s_2$ of length $m < n$, we often want to recover the mutation history that generated $s_2$ from $s_1$. Modern sequence aligners are widely used for this task, and many employ the seed-chain-extend heuristic with $k$-mer seeds. Previously, Shaw and Yu showed that optimal linear-gap cost chaining can produce a chain with $1 - O\left(\frac{1}{\sqrt{m}}\right)$ recoverability, the proportion of the mutation history that is recovered, in $O\left(mn^{2.43θ} \log n\right)$ expected time, where $θ< 0.206$ is the mutation rate under a substitution-only channel and $s_1$ is assumed to be uniformly random. However, a gap remains between theory and practice, since real genomic data includes insertions and deletions (indels), and yet seed-chain-extend remains effective. In this paper, we generalize those prior results by introducing mathematical machinery to deal with the two new obstacles introduced by indel channels: the dependence of neighboring anchors and the presence of anchors that are only partially correct. We are thus able
  to prove that the expected recoverability of an optimal chain is $\ge 1 - O\Bigl(\frac{1}{\sqrt{m}}\Bigr)$ and the expected runtime is $O(mn^{3.15 \cdot θ_T}\log n)$, when the total mutation rate given by the sum of the substitution, insertion, and deletion mutation rates ($θ_T = θ_i + θ_d + θ_s$) is less than $0.159$.

</details>


### [8] [Crude Approximation of Directed Minimum Cut and Arborescence Packing in Almost Linear Time](https://arxiv.org/abs/2512.05300)
*Yonggang Jiang,Yaowei Long,Thatchaphol Saranurak,Benyu Wang*

Main category: cs.DS

TL;DR: 本文提出了在有向图中近似计算根最小割和最大树状图包装的几乎线性时间算法，解决了这两个对偶问题的计算效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的根最小割精确算法需要超线性时间（如Õ(mk)或Õ(m^{1+o(1)}min{√n,n/m^{1/3}})），而根树状图包装算法需要Õ(m·poly(k))时间且无拥塞。这些算法的时间复杂度限制了在大规模图上的应用，因此需要开发更高效的近似算法。

Method: 提出了两个几乎线性时间算法：1）对于n顶点m边的有向图，当根最小割值为k时，第一个算法在m^{1+o(1)}时间内计算大小为O(k log^5 n)的根割；2）第二个算法在m^{1+o(1)}时间内包装k个根树状图，拥塞为n^{o(1)}，证明根最小割至少为k/n^{o(1)}。第一个算法也适用于加权图。

Result: 算法实现了m^{1+o(1)}的几乎线性时间复杂度，相比之前的超线性时间算法有显著改进。近似因子为O(log^5 n)的割大小和n^{o(1)}的拥塞，在可接受的近似误差范围内大幅提升了计算效率。

Conclusion: 本文首次为有向图中的根最小割和最大树状图包装问题提供了几乎线性时间的近似算法，打破了这两个对偶问题的计算效率瓶颈，为大规模图算法的实际应用提供了重要工具。

Abstract: We give almost-linear-time algorithms for approximating rooted minimum cut and maximum arborescence packing in directed graphs, two problems that are dual to each other [Edm73]. More specifically, for an $n$-vertex, $m$-edge directed graph $G$ whose $s$-rooted minimum cut value is $k$, our first algorithm computes an $s$-rooted cut of size at most $O(k\log^{5} n)$ in $m^{1+o(1)}$ time, and our second algorithm packs $k$ $s$-rooted arborescences with $n^{o(1)}$ congestion in $m^{1+o(1)}$ time, certifying that the $s$-rooted minimum cut is at least $k / n^{o(1)}$. Our first algorithm also works for weighted graphs.
  Prior to our work, the fastest algorithms for computing the $s$-rooted minimum cut were exact but had super-linear running time: either $\tilde{O}(mk)$ [Gab91] or $\tilde{O}(m^{1+o(1)}\min\{\sqrt{n},n/m^{1/3}\})$ [CLN+22]. The fastest known algorithms for packing $s$-rooted arborescences had no congestion, but required $\tilde{O}(m \cdot \mathrm{poly}(k))$ time [BHKP08].

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: Stellis：一种用于纯化分离逻辑蕴含的策略语言，通过移除空间公式将蕴含简化为纯蕴含，支持灵活的策略编码和自动正确性证明。


<details>
  <summary>Details</summary>
Motivation: 自动证明分离逻辑蕴含是验证中的基本挑战。基于规则的方法依赖分离逻辑规则进行自动化，但这些规则语句不足以描述自动化策略，特别是在特定场景中对齐和消除相应内存布局的策略。

Method: 提出Stellis策略语言，具有强大的匹配机制和灵活的动作描述，能够直接编码各种策略。引入算法为每个策略生成正确性条件，将策略的正确性简化为其正确性条件的验证。基于机械化的归约正确性定理，原型实现生成整体自动化的正确性证明。

Result: 在包含229个蕴含的基准测试中（来自标准链表数据结构和微内核内存模块的验证），使用5个库和98个策略，系统自动纯化了95.6%（219个）的蕴含，表现出高效性和灵活性。

Conclusion: Stellis提供了一种灵活方便的策略语言，用于纯化分离逻辑蕴含，同时确保策略的正确性，在验证实践中表现出高效性。

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [10] [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176)
*Brittany Johnson,Erin Reddick,Angela D. R. Smith*

Main category: cs.SE

TL;DR: 该研究提出CIVIQ基准测试，用于评估LLM与社区社会价值观和常识的文化对齐，以解决现有国家对齐基准在美国多元文化背景下的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要与西方白人叙事对齐，与多元文化背景存在错位。虽然已有文化感知LLM（如ChatBlackGPT）和韩国国家对齐基准（KorNAT），但美国文化多样性使得国家对齐基准无法有效代表广泛群体，需要社区层面的文化对齐评估工具。

Method: 采用复制研究方法，将韩国国家LLM对齐基准KorNAT的开发流程转化为美国语境，开发CIVIQ（文化智能与价值观推断质量）基准，专注于社区社会价值观和常识的对齐评估。

Result: 提出了CIVIQ基准测试，为AI技术文化对齐的研究和开发提供了关键基础，能够更好地评估LLM与不同社区文化价值观的对齐程度。

Conclusion: CIVIQ基准填补了美国多元文化背景下文化对齐评估的空白，为开发更具包容性和代表性的文化感知LLM提供了重要工具，有助于推动AI技术更好地服务多样化社区。

Abstract: Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as "general purpose" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of "culturally-informed" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.

</details>


### [11] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: 该论文系统分析了AI生成代码中的缺陷和错误类型，为模型改进和质量评估提供参考


<details>
  <summary>Details</summary>
Motivation: AI代码生成模型被广泛使用以提高生产力，但生成的代码存在质量问题和缺陷，这些问题可能导致开发过程中的信任和维护挑战。现有研究结果分散且缺乏系统总结，需要全面综述来揭示错误类型、分布、修复策略及其与特定模型的关联。

Method: 系统分析现有AI生成代码文献，建立对生成代码中缺陷和错误的整体理解，提供分类框架

Result: 建立了AI生成代码中缺陷类型的分类体系，分析了不同模型生成的代码中存在的错误模式和分布，讨论了可能的修复和缓解策略

Conclusion: 该研究为未来模型改进和质量评估提供了系统参考，帮助理解AI生成代码中缺陷的性质和程度，促进更可靠的代码生成

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [12] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 研究探讨生成式AI工具在软件工程教育项目课程中的应用，通过游戏开发项目分析学生使用AI的情况，并开发基于RAG的本地LLM助手提供项目上下文支持。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具的快速发展正在改变软件开发方式，软件工程教育需要适应这一变化，确保学生不仅学习传统开发方法，还能有意义且负责任地使用这些新技术。

Method: 在大学编程项目中开展用户研究，学生协作开发电脑游戏；研究分析学生在软件开发各阶段如何使用生成式AI工具；进一步开发基于检索增强生成（RAG）的本地部署大型语言模型助手，提供项目上下文支持。

Result: 研究揭示了学生在软件开发过程中使用生成式AI工具的模式，识别了这些工具最有效的任务类型，分析了学生遇到的挑战；通过RAG系统实现了项目上下文感知的AI支持，并对其行为、参数敏感性和常见失败模式进行了定性分析。

Conclusion: 研究结果加深了对教育软件项目中上下文感知AI支持的理解，为未来将AI辅助工具整合到软件工程课程提供了信息基础，有助于更好地设计AI支持的软件工程教育。

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [13] [Model Gateway: Model Management Platform for Model-Driven Drug Discovery](https://arxiv.org/abs/2512.05462)
*Yan-Shiun Wu,Nathan A. Morin*

Main category: cs.SE

TL;DR: Model Gateway是一个用于药物发现流程中管理机器学习和科学计算模型的平台，支持LLM代理和生成式AI工具，实现模型管理、动态共识模型、异步执行等功能，在超过1万并发客户端测试中达到0%失败率。


<details>
  <summary>Details</summary>
Motivation: 药物发现流程中需要管理大量机器学习和科学计算模型，传统方式难以高效管理模型注册、执行和结果获取。需要构建一个统一平台来支持MLOps流程，特别是整合LLM代理和生成式AI工具来加速药物研发。

Method: 开发Model Gateway平台，包含三个核心组件：模型所有者控制面板、平台管理工具和Model Gateway API服务。平台支持动态共识模型（聚合多个科学计算模型）、模型注册管理、模型信息检索、异步提交/执行模型以及结果接收。

Result: 平台在超过1万个同时应用客户端消费模型的扩展测试中实现了0%的失败率。Model Gateway已成为模型驱动的药物发现流程的基础组成部分，能够显著加速新药开发。

Conclusion: Model Gateway通过成熟的MLOps基础设施和LLM代理与生成式AI工具的集成，有潜力显著加速新药开发。该平台为药物发现中的模型管理提供了高效可靠的解决方案。

Abstract: This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.

</details>


### [14] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 研究比较了软件工程师在LLM辅助代码审查与人工同行审查中的情感反应、自我调节策略和参与行为，发现LLM辅助审查减少了情感成本和自我调节需求，将参与重点从情绪管理转向认知负荷管理。


<details>
  <summary>Details</summary>
Motivation: 代码审查是一种社会技术实践，但软件工程师在LLM辅助代码审查与人工同行审查中的参与方式差异尚不清楚。研究旨在理解工程师在这两种审查模式下的情感反应、自我调节策略和行为参与。

Method: 采用两阶段定性研究：第一阶段，20名软件工程师进行同行审查并接受访谈，了解情感反应和参与决策；第二阶段，引入符合工程师偏好的新提示，探究特征如何影响他们的反应。开发了一个整合模型，将情感自我调节与行为参与和解决方案联系起来。

Result: 识别了工程师应对负面反馈的四种自我调节策略：重构、对话调节、回避和防御性。参与通过社会校准进行，工程师根据关系氛围和团队规范调整反应和行为。在同行审查中，解决方案轨迹因焦点（个人/二人/团队）和内部意义建构过程而异。LLM辅助审查降低了情感成本和自我调节需求，当LLM反馈符合工程师认知期望时，减少了处理努力并提高了采纳倾向。

Conclusion: LLM辅助审查将参与重点从情绪管理转向认知负荷管理。AI最适合作为支持性伙伴，减少认知和情感负荷，同时保留人类责任和同行审查等社会技术活动的社会意义。贡献了一个整合参与模型，展示了情感和认知过程如何影响同行和LLM辅助代码审查中的反馈采纳。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [15] [WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp](https://arxiv.org/abs/2512.05314)
*Ke Mao,Timotej Kapus,Cons T Åhs,Matteo Marescotti,Daniel Ip,Ákos Hajdu,Sopot Cela,Aparup Banerjee*

Main category: cs.SE

TL;DR: WhatsCode是WhatsApp的领域特定AI开发系统，在25个月部署中显著提升隐私验证覆盖率3.5倍，生成3000+代码变更，识别出两种稳定的人机协作模式，证明组织因素与技术能力同等重要。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助开发工具在工业环境中应用日益广泛，但合规相关的大规模工业部署在学术文献中存在显著空白。本研究旨在填补这一空白，通过WhatsCode在WhatsApp（服务20亿用户）的实际部署经验，为大规模AI工具在合规环境中的部署提供实证指导。

Method: WhatsCode是一个领域特定的AI开发系统，支持WhatsApp平台，处理数百万行代码。系统从针对性隐私自动化演变为自主代理工作流，集成端到端功能开发和DevOps流程。研究基于25个月（2023-2025）的工业部署数据，分析自动化效果和人机协作模式。

Result: WhatsCode取得显著量化影响：隐私验证覆盖率从15%提升至53%（3.5倍增长），识别隐私需求，生成3000+被接受的代码变更（接受率9%-100%），提交692个自动重构/修复变更，711个框架采用，141个功能开发辅助，bug分类保持86%精确度。研究发现两种稳定的人机协作模式：一键部署（60%高置信度变更）和接管-修订（40%复杂决策）。

Conclusion: 组织因素（所有权模型、采用动态、风险管理）与技术能力同等重要，决定企业级AI成功。有效的人机协作（而非完全自动化）驱动可持续业务影响。研究为合规相关环境中大规模AI工具部署提供实证指导，强调人机协作的重要性。

Abstract: The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes.
  WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.

</details>


### [16] [Metronome: Differentiated Delay Scheduling for Serverless Functions](https://arxiv.org/abs/2512.05703)
*Zhuangbin Chen,Juzheng Zheng,Zibin Zheng*

Main category: cs.SE

TL;DR: Metronome：针对无服务器函数优化的差异化延迟调度框架，通过预测模型选择最优数据本地性节点，显著减少函数执行时间


<details>
  <summary>Details</summary>
Motivation: 无服务器计算（FaaS）的动态性和事件驱动特性使得调度优化具有挑战性。传统集群计算中的数据本地性优化方法（如延迟调度）在无服务器环境中效果有限，因为函数执行时间异构、本地性模式更复杂，且基于规则的延迟阈值无效

Method: 提出Metronome差异化延迟调度框架：1）识别数据本地性和基础设施本地性两种模式；2）使用在线随机森林回归模型预测函数在不同节点上的执行时间；3）基于预测结果做出延迟调度决策，防止SLA违规

Result: 在OpenLambda上的实现显示，Metronome相比基线方法显著提升性能：函数平均执行时间减少64.88%-95.83%，在高并发下仍保持性能优势，同时确保SLA合规

Conclusion: Metronome通过预测性差异化延迟调度有效解决了无服务器环境中数据本地性优化的挑战，为函数即服务计算提供了高效的调度解决方案

Abstract: Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.

</details>


### [17] [Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering](https://arxiv.org/abs/2512.05350)
*Munazza Zaib,Wei Wang,Dulaji Hidellaarachchi,Isma Farah Siddiqui*

Main category: cs.SE

TL;DR: 该论文提出了一种混合方法，结合InclusiveMag框架和GenderMag走查流程，专门针对软件工程中神经多样性女性面临的独特挑战，旨在通过三阶段设计开发包容性分析方法。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的神经多样性女性面临性别偏见和神经差异交叉的独特挑战，但现有研究尚未系统性地关注这一群体。误诊、掩饰行为和男性中心的工作文化加剧了压力、倦怠和流失问题。

Method: 提出混合方法学方法：整合InclusiveMag包容性框架和GenderMag走查流程，专门针对神经多样性女性在SE中的情境。设计分为三个阶段：文献综述范围界定、角色和分析流程推导、协作工作坊应用。

Result: 通过文献综述将挑战综合为认知、社交、组织、结构和职业发展五个维度，揭示了误诊/延迟诊断和掩饰行为如何加剧排斥问题，为后续开发包容性分析方法奠定基础。

Conclusion: 该研究为理解和支持软件工程中神经多样性女性提供了系统框架，通过开发和应用包容性分析方法，旨在促进可操作的变革，改善这一群体的工作环境和职业发展。

Abstract: Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.

</details>


### [18] [BGPFuzz: Automated Configuration Fuzzing of the Border Gateway Protocol](https://arxiv.org/abs/2512.05358)
*Chenlu Zhang,Amirmohammad Pasdar,Van-Thuan Pham*

Main category: cs.SE

TL;DR: BGPFuzz：一个结构感知、有状态的模糊测试框架，用于检测BGP配置错误，无需预定义正确性属性，通过运行时预言机捕获实际异常症状。


<details>
  <summary>Details</summary>
Motivation: 电信网络依赖配置定义路由行为，BGP中的错误配置可能导致严重中断和安全漏洞（如2021年Facebook中断）。现有方法依赖合成或验证，需要一种成本效益高的方法来识别由BGP固有复杂性或供应商特定实现导致的错误配置。

Method: 提出BGPFuzz框架：结构感知、有状态的模糊测试方法，系统性地变异BGP配置并在虚拟化网络中评估其效果。通过运行时预言机捕获实际症状（会话重置、黑洞、流量重定向），无需静态分析中的预定义正确性属性。

Result: 实验表明BGPFuzz能够可靠地复现和检测已知故障，包括最大前缀违规和子前缀劫持等配置错误。

Conclusion: BGPFuzz提供了一种有效的BGP配置错误检测方法，通过模糊测试和运行时监控，能够识别实际网络中可能出现的配置问题，弥补了传统静态分析的不足。

Abstract: Telecommunications networks rely on configurations to define routing behavior, especially in the Border Gateway Protocol (BGP), where misconfigurations can lead to severe outages and security breaches, as demonstrated by the 2021 Facebook outage. Unlike existing approaches that rely on synthesis or verification, our work offers a cost-effective method for identifying misconfigurations resulting from BGP's inherent complexity or vendor-specific implementations. We present BGPFuzz, a structure-aware and stateful fuzzing framework that systematically mutates BGP configurations and evaluates their effects in virtualized network. Without requiring predefined correctness properties as in static analysis, BGPFuzz detects anomalies through runtime oracles that capture practical symptoms such as session resets, blackholing, and traffic redirection. Our experiments show that BGPFuzz can reliably reproduce and detect known failures, including max-prefix violations and sub-prefix hijacks.

</details>


### [19] [Legacy Modernization with AI -- Mainframe modernization](https://arxiv.org/abs/2512.05375)
*Sunil Khemka,Arunava Majumdar*

Main category: cs.SE

TL;DR: AI辅助的遗留系统现代化将传统大型机系统转变为灵活、可扩展的智能架构，通过自动化代码重构、智能数据迁移和预测性维护等技术，帮助企业迁移到微服务、容器化和混合云平台。


<details>
  <summary>Details</summary>
Motivation: 传统大型机系统虽然可靠，但维护成本高、技能短缺、难以与云系统集成，需要现代化改造以适应现代技术环境。

Method: 采用AI驱动的现代化策略，包括：自动化代码重构、智能工具进行数据迁移、预测性维护、机器学习模型分析遗留代码库、识别效率机会、自动化测试和部署。

Result: 企业能够顺利迁移到微服务、容器化环境和混合云平台，提高运营效率，实现工作负载均衡和异常检测，保留核心业务逻辑的同时加速创新。

Conclusion: AI在大型机现代化中的应用是数字化转型和企业可持续增长的关键催化剂，能够减少停机时间、增强系统弹性，推动企业持续创新。

Abstract: Artificial Intelligence-assisted legacy modernization is essential in changing the stalwart mainframe systems of the past into flexible, scalable, and smart architecture. While mainframes are generally dependable, they can be difficult to maintain due to their high maintenance costs, the shortage of skills, and the problems in integrating them with cloud-based systems. By adopting AI-driven modernization strategies such as automated code refactoring, migration of data using smart tools, and predictive maintenance, companies can easily move to microservices, containerized environments, and hybrid cloud platforms. Machine learning models have the capability to go through legacy codebases, figure out efficiency opportunities, and carry out automated testing and deployment. Besides that, AI improves the organization's operational efficiency by generating the insights that can be used to level the workload and detect the anomalies. The coupling of the two is not only about saving the core business logic but also about enabling quicker innovation, less downtime, and enhanced system resilience. Therefore, the use of AI in mainframe modernization is a catalyst for digital transformation and enterprise growth that is sustainable over time.

</details>


### [20] [Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation](https://arxiv.org/abs/2512.05383)
*Mara Downing,Matthew Peng,Jacob Granley,Michael Beyeler,Tevfik Bultan*

Main category: cs.SE

TL;DR: 研究人员提出了一种基于覆盖引导模糊测试的方法，用于检测机器学习驱动的神经刺激系统中的不安全刺激模式，通过扰动模型输入并检查是否违反生物物理安全限制。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地用于神经假体设备（如视觉假体）中生成电刺激模式。虽然这些模型提供了精确和个性化的控制，但当模型输出直接传递到神经组织时，也引入了新的安全风险。需要一种系统化的定量方法来检测和表征ML驱动神经刺激系统中的不安全刺激模式。

Method: 将自动软件测试技术——覆盖引导模糊测试——适应到神经刺激领域。通过扰动模型输入并跟踪产生的刺激是否违反电荷密度、瞬时电流或电极共激活的生物物理限制。该框架将编码器视为黑盒，并使用覆盖度量来引导探索，量化测试用例在可能输出空间和违规类型上的覆盖广度。

Result: 应用于视网膜和皮层的深度刺激编码器时，该方法系统地揭示了超出既定安全限制的多样化刺激模式。两个违规输出覆盖度量识别了最高数量和多样性的不安全输出，使得能够跨架构和训练策略进行可解释的比较。

Conclusion: 违规聚焦的模糊测试将安全评估重新定义为经验性、可重复的过程。通过将安全从训练启发式转变为部署模型的可测量属性，为下一代神经接口的基于证据的基准测试、监管准备和伦理保证奠定了基础。

Abstract: Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.

</details>


### [21] [Bita: A Conversational Assistant for Fairness Testing](https://arxiv.org/abs/2512.05428)
*Keeryn Johnson,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Bita是一个基于对话的AI助手，帮助软件测试人员检测AI系统中的偏见，通过集成大语言模型和检索增强生成技术，使公平性测试更易用和系统化。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统中的偏见可能导致不公平和歧视性结果，而现有的公平性测试工具通常难以使用，需要高级专业知识，且对实际工作流程支持有限。

Method: 开发了Bita对话助手，集成大语言模型与检索增强生成技术，将响应基于精选的公平性文献，帮助检测偏见来源、评估测试计划并生成公平性导向的探索性测试章程。

Result: 验证显示Bita能够支持真实世界AI系统的公平性测试任务，提供结构化、可复现的证据证明其效用。

Conclusion: Bita是一个实用工具，使公平性测试变得可访问、系统化，并可直接应用于工业实践。

Abstract: Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.

</details>


### [22] [Everything is Context: Agentic File System Abstraction for Context Engineering](https://arxiv.org/abs/2512.05470)
*Xiwei Xu,Robert Mao,Quan Bai,Xuewu Gu,Yechao Li,Liming Zhu*

Main category: cs.SE

TL;DR: 提出基于文件系统抽象的情境工程架构，将异构情境构件统一管理，实现可验证、可追溯的生成式AI系统设计。


<details>
  <summary>Details</summary>
Motivation: 生成式AI重塑了软件系统设计，但现有情境工程实践（如提示工程、RAG、工具集成）碎片化，缺乏持久性和可追溯性，限制了系统的可信度和问责制。

Method: 提出受Unix"万物皆文件"启发的文件系统抽象，通过统一挂载、元数据和访问控制管理异构情境构件。在AIGNE框架中实现包含情境构造器、加载器和评估器的可验证情境工程管道。

Result: 在AIGNE框架中实现该架构，通过两个示例（具有记忆的代理和基于MCP的GitHub助手）展示了可操作化应用，支持可验证、可维护、工业就绪的生成式AI系统。

Conclusion: 该架构为负责任、以人为中心的AI协作建立了可重用基础，将人类定位为策展人、验证者和共同推理者，解决了生成式AI系统中情境管理的核心挑战。

Abstract: Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.

</details>


### [23] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: iEcoreGen是一个结合EMF模型驱动开发与LLM的混合代码生成方法，通过Ecore模型定义系统结构，使用模板生成初始代码，再用LLM补全和修复方法，在代码生成任务中表现优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 模板驱动的代码生成能保证正确性但灵活性不足，而LLM方法灵活但可能产生错误代码。需要结合两者优势，实现既保证正确性又具备灵活性的代码生成方案。

Method: iEcoreGen采用混合方法：1) 使用Eclipse Modeling Framework (EMF)的Ecore模型定义系统结构；2) 分解需求生成操作规范；3) 用EMF模板生成器生成初始Java代码；4) 将规范序列化为文档字符串；5) 调用LLM补全和修复未实现的方法。

Result: 在20个代码生成任务和5个LLM上的评估显示：iEcoreGen在pass@k指标上优于纯LLM基线，在compilation@k指标上与基线相当。消融研究阐明了iEcoreGen各组件的作用。

Conclusion: LLM增强的模型驱动开发是提高软件自动化效率的有前景方向，iEcoreGen展示了结合模板驱动和LLM方法的优势，为更高效的软件自动化提供了可行路径。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [24] [Generative AI in Simulation-Based Test Environments for Large-Scale Cyber-Physical Systems: An Industrial Study](https://arxiv.org/abs/2512.05507)
*Masoud Sadrnezhaad,José Antonio Hernández López,Torvald Mårtensson,Daniel Varro*

Main category: cs.SE

TL;DR: 该研究通过跨公司研讨会收集了从业者对生成式AI在基于仿真的大规模信息物理系统测试中应用的观点，识别了挑战并提出了包含三个优先方向的研究议程。


<details>
  <summary>Details</summary>
Motivation: 大规模信息物理系统的质量保证需要复杂的测试环境和仿真模型，开发维护成本高。生成式AI在软件测试中已显示出潜力，但在仿真测试中的应用仍未被充分探索，需要了解从业者的实际挑战和需求。

Method: 基于跨公司研讨会，收集了来自六个组织的从业者观点，通过经验分享和讨论获取实践洞察。

Result: 识别了工程师面临的具体挑战，并提出了包含三个高优先级方向的研究议程：AI生成的场景和环境模型、CI/CD管道中的仿真器和AI、生成式AI在仿真中的可信度。

Conclusion: 生成式AI在仿真测试中具有巨大潜力，但仍面临未解决的挑战。该研究为未来的学术界-产业界合作提供了指导，以负责任地采用生成式AI技术。

Abstract: Quality assurance for large-scale cyber-physical systems relies on sophisticated test activities using complex test environments investigated with the help of numerous types of simulators. As these systems grow, extensive resources are required to develop and maintain simulation models of hardware and software components, as well as physical environments. Meanwhile, recent advances in generative AI have led to tools that can produce executable test cases for software systems, offering potential benefits such as reducing manual efforts or increasing test coverage. However, the application of generative AI techniques to simulation-based testing of large-scale cyber-physical systems remains underexplored. To better understand this gap, this study captures practitioners' perspectives on leveraging generative AI, based on a cross-company workshop with six organizations. Our contribution is twofold: (1) detailed, experience-based insights into challenges faced by engineers, and (2) a research agenda comprising three high-priority directions: (a) AI-generated scenarios and environment models, (b) simulators and AI in CI/CD pipelines, and (c) trustworthiness in generative AI for simulation. While participants acknowledged substantial potential, they also highlighted unresolved challenges. By detailing these issues, the paper aims to guide future academia-industry collaboration towards the responsible adoption of generative AI in simulation-based testing.

</details>


### [25] [From Challenge to Change: Design Principles for AI Transformations](https://arxiv.org/abs/2512.05533)
*Theocharis Tavantzis,Stefano Lambiase,Daniel Russo,Robert Feldt*

Main category: cs.SE

TL;DR: 提出基于行为软件工程(BSE)的人类中心框架，支持软件工程组织在早期AI采用阶段应对社会技术复杂性，包含9个维度的可操作指导。


<details>
  <summary>Details</summary>
Motivation: AI正在重塑软件工程，但现有研究过于关注技术问题，缺乏对团队如何适应和信任AI的深入理解。需要人类中心的方法来应对AI集成中的行为和非技术挑战。

Method: 采用混合方法：通过文献综述分析组织变革模型，通过访谈数据的主题分析构建框架，并通过调查(N=105)和专家研讨会(N=4)收集初步实践者反馈。

Result: 开发了包含9个维度的框架：AI战略设计、AI战略评估、协作、沟通、治理与伦理、领导力、组织文化、组织动态、技能提升。调查显示技能提升(15.2%)和AI战略设计(15.1%)被认为最重要。

Conclusion: 该框架为早期AI采用提供了实用路线图，发现组织当前优先考虑程序性元素如战略设计，而人类中心防护措施仍需发展。强调了将框架与实际实践结合的重要性。

Abstract: The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.

</details>


### [26] [Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub](https://arxiv.org/abs/2512.05551)
*Jai Lal Lulla,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: GitHub的CODEOWNERS功能能自动指定代码文件审查者，但实际使用情况未知。本研究首次大规模分析CODEOWNERS在844,000个PR中的使用情况，发现代码所有者遵守规则、与传统所有权指标行为相似，但能带来更流畅快速的PR工作流。采用CODEOWNERS后，审查责任从核心开发者转移。


<details>
  <summary>Details</summary>
Motivation: 代码所有权对大型软件开发中的责任和质量管理至关重要。随着软件供应链攻击等外部威胁增加，分配和执行责任的机制变得愈发关键。GitHub在2017年引入CODEOWNERS功能来自动指定特定文件的审查者，但对其实际采用和实践情况知之甚少。

Method: 对超过844,000个拉取请求、190万条评论和200万次审查进行首次大规模实证研究，识别了10,287名代码所有者并跟踪他们的审查活动。使用回归不连续设计(RDD)分析来评估CODEOWNERS采用对审查动态的影响。

Result: 代码所有者倾向于遵守CODEOWNERS文件中指定的规则，表现出与传统所有权指标相似的协作行为，但随着时间的推移能带来更流畅、更快速的PR工作流。采用CODEOWNERS后，审查责任从核心开发者重新分配，改变了审查动态。

Conclusion: CODEOWNERS是一种有前景但未充分利用的机制，可改善软件治理和韧性。项目可以利用这种替代所有权方法来增强开源开发中的安全性、责任性和工作流效率。

Abstract: Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.

</details>


### [27] [Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing](https://arxiv.org/abs/2512.05653)
*Stefan Schönig,Leo Poss,Fabrizio Maria Maggi*

Main category: cs.SE

TL;DR: 提出基于复杂事件处理(CEP)的实时执行架构，用于执行和强制执行混合声明式业务流程模型，解决传统BPM无法处理连续传感器数据的问题


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理(BPM)只关注离散事件，无法处理网络物理环境中的连续传感器数据。现有的混合声明式规范虽然使用信号时序逻辑(STL)解决了这一问题，但仅限于监控和事后一致性检查，缺乏实时执行和强制执行能力。

Method: 提出三层架构的复杂事件处理(CEP)执行框架，将STL启发的谓词集成到执行流程中，使系统能够基于连续传感器行为主动触发活动并强制执行流程边界。

Result: 该架构实现了混合声明式模型的实时执行和强制执行，能够主动响应连续传感器数据的变化，并在运行时执行流程控制。

Conclusion: 提出的CEP执行架构填补了混合规范与操作控制之间的空白，为网络物理环境中的业务流程管理提供了实时执行和强制执行能力。

Abstract: Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.

</details>


### [28] [MicroRacer: Detecting Concurrency Bugs for Cloud Service Systems](https://arxiv.org/abs/2512.05716)
*Zhiling Deng,Juepeng Wang,Zhuangbin Chen*

Main category: cs.SE

TL;DR: MicroRacer：一种用于微服务架构中并发bug检测的非侵入式自动化框架，通过动态插桩和运行时分析来识别并发问题


<details>
  <summary>Details</summary>
Motivation: 现代云应用采用微服务架构，端到端用户请求会经过多个不同服务和机器，表现出复杂的交互。这种分布式系统容易受到并发bug的影响，而现有检测方法往往因为侵入性强且无法处理微服务架构的复杂性而效果不佳。

Method: MicroRacer通过动态插桩广泛使用的库来收集详细的跟踪数据，无需修改应用代码。利用这些数据分析服务系统中常见操作的happened-before关系和资源访问模式，识别可疑的并发操作，并采用三阶段验证过程来测试和确认并发bug。

Result: 在开源微服务基准测试和复现的工业bug上的实验表明，MicroRacer能够有效且高效地准确检测和定位并发问题。

Conclusion: MicroRacer为非侵入式并发bug检测提供了一种有效的解决方案，特别适用于复杂的微服务架构环境，能够提高云服务系统的可靠性。

Abstract: Modern cloud applications delivering global services are often built on distributed systems with a microservice architecture. In such systems, end-to-end user requests traverse multiple different services and machines, exhibiting intricate interactions. Consequently, cloud service systems are vulnerable to concurrency bugs, which pose significant challenges to their reliability. Existing methods for concurrency bug detection often fall short due to their intrusive nature and inability to handle the architectural complexities of microservices. To address these limitations, we propose MicroRacer, a non-intrusive and automated framework for detecting concurrency bugs in such environments. By dynamically instrumenting widely-used libraries at runtime, MicroRacer collects detailed trace data without modifying the application code. Such data are utilized to analyze the happened-before relationship and resource access patterns of common operations within service systems. Based on this information, MicroRacer identifies suspicious concurrent operations and employs a three-stage validation process to test and confirm concurrency bugs. Experiments on open-source microservice benchmarks with replicated industrial bugs demonstrate MicroRacer's effectiveness and efficiency in accurately detecting and pinpointing concurrency issues.

</details>


### [29] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: Germinator：基于语法和覆盖引导的模糊测试方法，用于可扩展编译器框架（如MLIR），自动生成多样化种子输入，提高测试覆盖率并发现未知bug


<details>
  <summary>Details</summary>
Motivation: 现代可扩展编译器框架（如MLIR）虽然支持快速创建领域特定语言方言，但其灵活性使得正确性验证更加困难。现有测试生成方法要么需要手动构建种子语料库，要么无法有效针对方言特定特性，缺乏既方言无关又方言有效的自动化测试生成方案。

Method: 提出一种基于语法和覆盖引导的模糊测试方法，结合两个关键洞见：1）从方言规范中自动提取语法（编码结构和类型约束）；2）使用预训练大语言模型基于这些语法自动生成多样化种子输入，无需手动输入或训练数据，然后用这些种子引导覆盖引导的模糊测试器。

Result: 在6个MLIR项目的91个方言上评估，Germinator生成的种子相比基于语法的基线提高10-120%的行覆盖率，发现了88个先前未知的bug（40个已确认），其中23个在之前没有自动化测试生成器的方言中。

Conclusion: Germinator实现了方言无关且方言有效的自动化测试生成，能够大规模有效且可控地测试低资源方言，解决了可扩展编译器框架中的测试基础设施维护难题。

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>


### [30] [Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures](https://arxiv.org/abs/2512.05908)
*Amirkia Rafiei Oskooei,S. Selcan Yukcu,Mehmet Cevheri Bozoglan,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 将多仓库微服务架构中的bug定位重构为自然语言推理任务，通过将代码库转换为层次化NL摘要，使用NL-to-NL搜索而非跨模态检索，显著提升定位效果。


<details>
  <summary>Details</summary>
Motivation: 多仓库微服务架构中的bug定位面临三大挑战：自然语言bug报告与代码之间的语义鸿沟、LLM上下文限制、以及需要先确定正确仓库。现有方法效果有限。

Method: 将代码库转换为层次化自然语言摘要（文件、目录、仓库级别），采用两阶段搜索：首先将bug报告路由到相关仓库，然后在仓库内进行自上而下的定位。

Result: 在包含46个仓库、110万行代码的工业系统DNext上评估，Pass@10达到0.82，MRR达到0.50，显著优于检索基线和GitHub Copilot、Cursor等代理RAG系统。

Conclusion: 工程化的自然语言表示比原始源代码更适合可扩展的bug定位，提供可解释的仓库->目录->文件搜索路径，这对构建企业AI工具的信任至关重要。

Abstract: Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [31] [MuMeNet: A Network Simulator for Musical Metaverse Communications](https://arxiv.org/abs/2512.05201)
*Ali Al Housseini,Jaime Llorca,Luca Turchet,Tiziano Leidi,Cristina Rottondi,Omran Ayoub*

Main category: cs.NI

TL;DR: 本文提出MuMeNet，一个专为音乐元宇宙设计的离散事件网络模拟器，用于解决5G/6G网络中音乐元宇宙会话的服务配置问题。


<details>
  <summary>Details</summary>
Motivation: 音乐元宇宙作为元宇宙的重要应用场景，其增长受到底层网络和服务基础设施要求的限制。当前缺乏针对音乐元宇宙独特特性（交互性、异构性、多播导向）的建模模拟范式和服务配置策略。

Method: 首先形式化音乐元宇宙的服务和网络图模型，以虚拟音乐会中的实时观众互动为参考场景。然后开发MuMeNet——专门针对音乐元宇宙需求和流量动态设计的离散事件网络模拟器。

Result: 通过在参考场景上运行基于线性规划的编排策略，并在现实的音乐元宇宙工作负载下进行性能分析，展示了MuMeNet的有效性。

Conclusion: 本文首次形式化建模并分析了5G/6G网络中音乐元宇宙会话的服务配置问题，提出的MuMeNet模拟器为解决音乐元宇宙的网络基础设施挑战提供了专门工具。

Abstract: The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using "live audience interaction in a virtual concert" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.

</details>


### [32] [Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem](https://arxiv.org/abs/2512.05207)
*Ali Al Housseini,Cristina Rottondi,Omran Ayoub*

Main category: cs.NI

TL;DR: 提出HRL-VNEAP分层强化学习方法，用于处理具有可选拓扑结构的虚拟网络嵌入问题，在动态到达场景下显著提升接受率、总收益和收益成本比。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟网络嵌入假设每个请求具有固定拓扑，但实际中虚拟网络请求可能具有多个功能等效但资源需求不同的拓扑选项。这种灵活性增加了可行解空间，但也引入了额外的决策层，使得动态嵌入更具挑战性。

Method: 采用分层强化学习方法：高层策略选择最合适的替代拓扑（或拒绝请求），低层策略将选定的拓扑嵌入到底层物理网络中。

Result: 在真实底层网络拓扑和多种流量负载下的实验表明，HRL-VNEAP在所有指标上均表现最佳：相比最强基线，接受率提升达20.7%，总收益提升达36.2%，收益成本比提升达22.1%。

Conclusion: HRL-VNEAP在动态虚拟网络嵌入中具有显著优势，通过分层强化学习有效处理拓扑选择与嵌入的联合决策问题。与MILP的对比揭示了与最优解的差距，为未来学习与优化结合的VNEAP解决方案提供了方向。

Abstract: Virtual Network Embedding (VNE) is a key enabler of network slicing, yet most formulations assume that each Virtual Network Request (VNR) has a fixed topology. Recently, VNE with Alternative topologies (VNEAP) was introduced to capture malleable VNRs, where each request can be instantiated using one of several functionally equivalent topologies that trade resources differently. While this flexibility enlarges the feasible space, it also introduces an additional decision layer, making dynamic embedding more challenging. This paper proposes HRL-VNEAP, a hierarchical reinforcement learning approach for VNEAP under dynamic arrivals. A high-level policy selects the most suitable alternative topology (or rejects the request), and a low-level policy embeds the chosen topology onto the substrate network. Experiments on realistic substrate topologies under multiple traffic loads show that naive exploitation strategies provide only modest gains, whereas HRL-VNEAP consistently achieves the best performance across all metrics. Compared to the strongest tested baselines, HRL-VNEAP improves acceptance ratio by up to \textbf{20.7\%}, total revenue by up to \textbf{36.2\%}, and revenue-over-cost by up to \textbf{22.1\%}. Finally, we benchmark against an MILP formulation on tractable instances to quantify the remaining gap to optimality and motivate future work on learning- and optimization-based VNEAP solutions.

</details>


### [33] [AIORA: An AI-Native Multi-Stakeholder Orchestration Architecture for 6G Continuum](https://arxiv.org/abs/2512.05744)
*Nuria Molner,Luis Rosa,Fulvio Risso,Konstantinos Samdanis,David Artuñedo,Rob Smets,Tarik Taleb,David Gomez-Barquero*

Main category: cs.NI

TL;DR: AIORA：面向6G系统的AI原生架构，通过开放API和智能编排实现边缘-云连续体的零接触管理


<details>
  <summary>Details</summary>
Motivation: 6G系统需要满足未来服务和应用对高灵活性、可用性、效率、可靠性和弹性的需求，现有行业倡议（如ETSI MEC、GSMA Operator Platform、CAMARA）显示出对集成边缘、云和网络资源编排框架的迫切需求

Method: 提出AIORA架构，基于3GPP边缘使能层和连接模型，采用多段虚拟连续体概念和嵌套AI驱动闭环，集成新工具和先进技术，实现边缘-云连续体的零接触管理和服务生命周期管理

Result: AIORA架构不仅与现有行业倡议保持一致，还通过多段虚拟连续体概念和实时优化的嵌套AI驱动闭环扩展了这些框架，能够无缝创建和管理多段异构环境中的服务

Conclusion: AIORA架构为6G系统提供了一种创新的AI原生解决方案，通过智能编排边缘-云连续体资源，满足未来6G服务和应用的复杂需求，代表了行业发展的新方向

Abstract: This paper elaborates on a novel AI-native architecture for emerging 6G systems harnessing open APIs, along with supporting mechanisms to empower intelligent and coordinated orchestration of edge-cloud continuum resources. The AIORA architecture facilitates a seamless creation, life-cycle management, and exposure of services in multi-segment heterogeneous environments. It integrates new breeds of tools and advanced technologies to enable zero-touch management of an edge-cloud continuum, building on top of the 3GPP Edge Enablement Layer and the respective connectivity models, allowing to cater to the high flexibility, availability, efficiency, reliability, and resilience needs of the future 6G services and applications. Several ongoing industry initiatives -- such as ETSI MEC for edge computing platforms, the GSMA Operator Platform for multi-operator service federation, and CAMARA for cross-operator API standardization -- demonstrate the growing momentum towards integrated frameworks where edge, cloud, and network resources can be seamlessly orchestrated. Our proposed AIORA architecture not only aligns with these initiatives but also extends them by leveraging a multi-segment virtual continuum concept and nested AI-driven closed loops for real-time optimization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques](https://arxiv.org/abs/2512.05169)
*Abdelmalik Moujahid,Fadi Dornaika*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了多视图聚类方法，将其分为七类主要策略，分析了各自的优缺点和实际挑战，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术面临计算约束、单视图学习算法局限性和处理多源大数据复杂性等挑战。多视图聚类作为无监督多视图学习的重要方法，能够弥补单视图方法的不足，提供更丰富的数据表示和有效的无监督学习解决方案。

Method: 论文采用系统性综述方法：1) 将多视图聚类方法系统分类为协同训练、协同正则化、子空间、深度学习、核方法、锚点法和图方法七类；2) 深入分析各类方法的优缺点和实际挑战；3) 整合了超过140篇基础与最新文献，比较了早期融合、晚期融合和联合学习等集成策略。

Result: 论文提供了多视图聚类领域的全面概览，包括：系统化的分类框架、各类方法的深入分析、实际应用案例（医疗健康、多媒体、社交网络分析）的调查研究，以及可操作的研究见解。

Conclusion: 这项研究填补了多视图聚类研究的现有空白，为领域发展提供了可操作的见解。多视图聚类具有丰富的语义特性，尽管存在固有复杂性，但其实际应用价值显著。未来研究方向包括新兴趋势、跨学科应用和可扩展性等挑战的解决。

Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.

</details>


### [35] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking方法，根据生物标志物的波动性自适应调整掩码概率，改进电子健康记录的自编码器预训练


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法通常采用均匀随机掩码，假设所有特征具有同等可预测性，但实际上实验室测试指标存在显著异质性：有些生物标志物（如钠）稳定，而有些（如乳酸）波动大且更难建模。临床上，波动性大的生物标志物常提示急性病理生理变化，需要更复杂的建模来捕捉其时间模式。

Method: 提出波动性感知预训练策略CV-Masking（变异系数掩码），根据每个特征的内在变异性自适应调整掩码概率。结合与临床工作流程对齐的仅值掩码目标，相比随机和基于方差的策略有系统性改进。

Result: 在大规模实验室测试面板上的实验表明，CV-Masking增强了重构能力，提高了下游预测性能，加速了收敛，产生了更稳健且具有临床意义的EHR表示。

Conclusion: CV-Masking通过考虑生物标志物的内在波动性，改进了电子健康记录的表示学习，为临床任务提供了更有效的预训练方法。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [36] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 系统评估临床时间序列建模中的分词策略，发现时间编码无显著益处，值特征重要性因任务而异，冻结预训练编码器优于可训练版本且参数更少。


<details>
  <summary>Details</summary>
Motivation: 当前对电子健康记录处理中分词策略有效性的公平比较有限，需要系统评估不同分词方法在临床时间序列建模中的表现。

Method: 在MIMIC-IV数据集上，使用基于Transformer的架构，通过控制性消融实验评估四种临床预测任务中的分词策略，包括时间编码、值特征和代码序列的对比。

Result: 显式时间编码对评估的下游任务无一致的统计显著益处；值特征重要性因任务而异（影响死亡率预测但不影响再入院预测）；冻结预训练代码编码器显著优于可训练版本且参数更少；更大的临床编码器在所有任务中提供一致改进。

Conclusion: 更简单、参数高效的方法在许多情况下能实现强大性能，但最优分词策略仍取决于具体任务；控制性评估实现了更公平的分词比较。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [37] [Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance](https://arxiv.org/abs/2512.05222)
*Yanhua Xu*

Main category: cs.LG

TL;DR: 结合预训练蛋白质语言模型与半监督学习，可在抗原性标签稀缺时保持高预测准确性，解决流感病毒抗原性评估中的标签瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 流感A病毒抗原性快速进化需要频繁更新疫苗，但传统的血凝抑制试验劳动密集且难以规模化，导致基因组数据远多于可用的表型标签，限制了传统监督模型的有效性。

Method: 评估两种半监督学习策略（自训练和标签传播），与全监督基线对比，使用四种PLM衍生嵌入（ESM-2、ProtVec、ProtT5、ProtBert）应用于血凝素序列。采用嵌套交叉验证框架模拟低标签场景（25%、50%、75%、100%标签可用性），覆盖四种IAV亚型（H1N1、H3N2、H5N1、H9N2）。

Result: 半监督学习在标签稀缺情况下持续提升性能。自训练与ProtVec结合产生最大相对增益，表明SSL可补偿低分辨率表示。ESM-2保持高度稳健，仅用25%标签数据即可达到F1分数0.82以上。H1N1和H9N2预测准确度高，而高度可变的H3N2亚型仍具挑战性，但SSL缓解了性能下降。

Conclusion: 整合蛋白质语言模型与半监督学习可解决抗原性标签瓶颈，更有效地利用未标记的监测序列，支持快速变异体优先级排序和及时疫苗株选择。

Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.

</details>


### [38] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出VaRDASS方法，通过分层采样减少无监督域适应中的方差，提高域差异估计的准确性


<details>
  <summary>Details</summary>
Motivation: 无监督域适应（UDA）在随机设置中域差异估计存在高方差问题，这会阻碍方法的理论优势，需要专门的方差减少技术

Method: 提出VaRDASS方法，针对相关性对齐和最大均值差异（MMD）两种差异度量，推导分层目标函数，并引入k-means风格的优化算法

Result: 在三个域偏移数据集上实验表明，该方法提高了差异估计准确性和目标域性能，并证明了MMD目标在特定假设下的理论最优性

Conclusion: VaRDASS是首个专门针对UDA的随机方差减少技术，通过分层采样有效减少域差异估计的方差，提升域适应性能

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [39] [MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System](https://arxiv.org/abs/2512.05234)
*Felix Mulitze,Herbert Woisetschläger,Hans Arno Jacobsen*

Main category: cs.LG

TL;DR: MAR-FL是一种新型的P2P联邦学习系统，通过迭代分组聚合大幅降低通信开销，同时保持对网络波动的鲁棒性，通信复杂度从O(N²)降至O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统与分布式机器学习的融合需要高效且鲁棒的联邦学习方法。现有的P2P FL方法存在通信复杂度过高的问题，限制了实际可扩展性，特别是在无线连接对等体和网络波动的情况下。

Method: MAR-FL采用迭代分组聚合机制，通过将节点分组进行分层聚合来减少通信开销，同时保持对不可靠客户端和网络波动的鲁棒性，并支持隐私计算集成。

Result: MAR-FL将通信复杂度从传统方法的O(N²)降低到O(N log N)，显著提高了可扩展性，特别是在聚合轮次中节点数量增加时仍能保持有效性。

Conclusion: MAR-FL为无线环境下的分布式机器学习提供了一个高效、可扩展且鲁棒的P2P联邦学习解决方案，解决了现有方法通信开销过大的问题。

Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.

</details>


### [40] [Edged Weisfeiler-Lehman Algorithm](https://arxiv.org/abs/2512.05238)
*Xiao Yue,Bo Liu,Feng Zhang,Guangzhi Qu*

Main category: cs.LG

TL;DR: 提出E-WL算法扩展1-WL以纳入边特征，并基于此构建EGIN模型，在12个边特征图数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的传播-聚合方法与1-WL算法类似，但1-WL算法无法利用边特征，这在某些领域存在改进空间。许多GNN模型也未能充分利用图数据的边特征。

Method: 提出E-WL算法扩展原始1-WL算法以纳入边特征，并基于此构建EGIN模型，专门用于利用边特征进行图学习。

Result: 在12个边特征基准图数据集上评估，EGIN模型在图形分类任务中总体上表现出优于现有基线模型的性能。

Conclusion: E-WL算法和EGIN模型成功解决了1-WL和许多GNN模型无法利用边特征的问题，在边特征图学习任务中表现出优越性能。

Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.

</details>


### [41] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: 提出多保真度学习框架，用量子求解器生成低保真解，再用稀疏经典训练数据校正到高保真精度，克服量子硬件限制


<details>
  <summary>Details</summary>
Motivation: 量子PDE求解器面临近端硬件限制：量子比特数限制空间分辨率，电路深度限制长时间积分精度，导致只能产生低保真解，无法满足实际科学计算需求

Method: 多保真度学习框架：先用量子求解器生成大量低保真解作为代理模型，然后通过多保真度神经网络架构学习校正映射，平衡线性和非线性变换，用稀疏经典高保真数据训练

Result: 在粘性Burgers方程和不可压缩Navier-Stokes流动等非线性PDE上成功校正粗量子预测，实现超出经典训练窗口的时间外推，预测精度可与经典方法竞争

Conclusion: 该框架在硬件受限的量子模拟与应用需求之间架起桥梁，为从当前量子设备中提取计算价值建立途径，推进近端量子计算在计算物理中的算法开发和实际部署

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [42] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 提出一种高效遗忘学习框架，通过识别对模型输出影响可忽略的训练数据子集，在遗忘前缩减数据集规模，实现显著计算节省（约50%）


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题日益突出，从训练模型中遗忘或移除特定数据点的能力变得愈发重要。现有遗忘方法通常平等对待遗忘集中的所有数据点，但并非所有数据点对模型学习都有显著影响

Method: 通过跨语言和视觉任务的比较分析，使用影响函数识别对模型输出影响可忽略的训练数据子集，基于此提出高效遗忘框架，在遗忘前缩减数据集规模

Result: 在真实世界实证案例中实现了显著的计算节省（最高约50%）

Conclusion: 挑战了传统遗忘方法平等对待所有数据点的做法，证明通过识别影响可忽略的数据子集，可以在保持模型性能的同时显著提升遗忘效率

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [43] [DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models](https://arxiv.org/abs/2512.05287)
*Ziqi Zhang*

Main category: cs.LG

TL;DR: 提出DMAGT模型，基于多层Transformer图神经网络预测药物-miRNA关联，在三个数据集上达到95.24% AUC，实验验证14/20预测关联


<details>
  <summary>Details</summary>
Motivation: 传统湿实验在探索药物与miRNA关联时存在效率和成本限制，需要计算模型来加速miRNA药物开发

Method: 使用Word2Vec嵌入药物分子结构和miRNA碱基结构特征，构建图表示药物-miRNA关联，采用图Transformer模型学习嵌入特征和关系结构

Result: 在ncDR、RNAInter、SM2miR三个数据集上达到最高95.24±0.05 AUC，优于其他方法；对5-氟尿嘧啶和奥沙利铂的预测中，20个最可能关联中有14个得到验证

Conclusion: DMAGT在预测药物-miRNA关联方面表现出色且稳定，为miRNA药物开发提供了新的捷径

Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.

</details>


### [44] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，将核希尔伯特空间与SHAP归因结合到Actor-Critic框架中，通过状态归因指导训练，提升效率、稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有Actor-Critic方法可解释性有限，且大多数可解释RL方法未能有效利用状态归因来辅助训练，通常平等对待所有状态特征，忽略了不同状态维度对奖励的异质性影响。

Method: 提出RSA2C算法：1) Actor在向量值再生核希尔伯特空间(RKHS)中实现，使用马氏距离加权的算子值核；2) Value Critic和Advantage Critic在标量RKHS中；3) 通过RKHS-SHAP计算状态归因，转换为马氏门控权重来调节Actor梯度和Advantage Critic目标；4) 使用稀疏化字典减少计算复杂度。

Result: 理论分析：推导了状态扰动下的全局非渐近收敛界，通过扰动误差项证明稳定性，通过收敛误差项证明效率。实验验证：在三个标准连续控制环境中，算法实现了效率、稳定性和可解释性。

Conclusion: RSA2C成功将状态归因集成到Actor-Critic框架中，通过RKHS-SHAP归因指导训练，在保持理论收敛保证的同时，显著提升了算法的效率、稳定性和可解释性。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [45] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: CFO框架通过流匹配直接学习PDE右侧项，无需通过ODE求解器反向传播，实现时间分辨率不变性，在长期稳定性和数据效率上优于自回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子代理使用自回归预测方案，在长期推演中会累积误差，且需要均匀时间离散化。现有连续时间方法（如神经ODE）计算负担重。需要一种既能学习连续时间PDE动力学，又避免计算负担的方法。

Method: CFO框架重新利用流匹配直接学习PDE右侧项，无需通过ODE求解器反向传播。对轨迹数据拟合时间样条，在节点处使用时间导数的有限差分估计来构建概率路径，其速度场近似真实PDE动力学。通过流匹配训练神经算子预测这些解析速度场。

Result: 在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO展示了优越的长期稳定性和显著的数据效率。仅使用25%不规则子采样时间点训练的CFO优于使用完整数据训练的自回归基线，相对误差减少高达87%。推理时仅需基线50%的函数评估。

Conclusion: CFO提供了一种时间分辨率不变的学习框架，能够处理任意非均匀时间网格的训练数据，并通过ODE积分在推理时实现任意时间分辨率查询，同时支持反向时间推理，在长期稳定性和数据效率方面显著优于传统自回归方法。

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [46] [Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.05306)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: SVGP KANs：将稀疏变分高斯过程与Kolmogorov-Arnold网络结合，实现可解释且具有不确定性量化的科学机器学习架构。


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov-Arnold网络缺乏系统的不确定性量化能力，而这对科学应用至关重要。需要开发既能保持可解释性又能进行贝叶斯推断的架构。

Method: 提出稀疏变分高斯过程推断与Kolmogorov-Arnold拓扑结构结合的框架，通过解析矩匹配在深度加性结构中传播不确定性，计算复杂度与样本量呈准线性关系。

Result: 通过三个案例研究展示了框架区分偶然不确定性与认知不确定性的能力：流体流动重建中的异方差测量噪声校准、平流-扩散动力学多步预测中的置信度退化量化、卷积自编码器中的分布外检测。

Conclusion: SVGP KANs是科学机器学习中具有不确定性感知能力的有前景架构，兼具可解释性和可扩展的贝叶斯推断能力。

Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.

</details>


### [47] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 研究评估SOTA模型区分人类与LLM生成科学想法的能力，发现连续改写后检测性能下降25.4%，加入研究问题上下文可提升2.97%，简化改写风格对检测挑战最大


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为研究代理的依赖增加，区分LLM与人类生成的想法对于理解LLM研究能力的认知差异至关重要。虽然LLM生成文本检测已有广泛研究，但区分人类与LLM生成的科学想法仍是未探索领域

Method: 系统评估最先进机器学习模型区分人类与LLM生成想法的能力，特别关注连续改写阶段后的检测效果，并研究加入研究问题作为上下文信息的影响

Result: SOTA模型在来源归因方面面临挑战，经过五次连续改写后检测性能平均下降25.4%；加入研究问题上下文可将检测性能提升最多2.97%；当想法被改写成简化的非专家风格时，检测算法困难最大，这是导致可区分LLM特征消失的主要原因

Conclusion: 区分人类与LLM生成的科学想法具有挑战性，特别是在多次改写后。上下文信息能略微改善检测，但简化改写风格会显著削弱检测能力，这对理解LLM作为研究代理的认知特性具有重要意义

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [48] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预报模型FourCastNetv2对输入噪声的鲁棒性，通过注入高斯噪声到飓风初始条件和完全随机初始条件来评估模型输出稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预报模型对输入噪声和不确定性的鲁棒性对于极端天气事件（如飓风）的预报可靠性至关重要，需要了解模型在不同噪声水平下的表现。

Method: 进行两个实验：1) 在飓风Florence的ERA5初始条件中注入不同水平的高斯噪声，观察轨迹和强度预测变化；2) 使用完全随机初始条件启动模型，观察模型对无意义输入的反应。

Result: FCNv2在低到中等噪声下能准确保持飓风特征；高噪声下仍能维持基本轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预报。

Conclusion: FCNv2表现出良好的鲁棒性，倾向于生成稳定平滑的输出，但存在低估风暴强度的系统性偏差；该方法简单且可移植到其他数据驱动的AI天气预报模型。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [49] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 提出新的联邦优化模型，量化通信和本地计算成本，并基于SAGA开发RG-SAGA算法，在非凸优化中实现最佳通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法使用不同的客户端选择策略（随机采样、全客户端通信或混合方案），这些策略在实际中产生不同的通信成本，但现有比较指标未能区分这些差异。

Method: 1) 建立量化通信和本地计算复杂度的联邦优化模型；2) 基于SAGA开发RG-SAGA梯度估计器，利用函数相似性；3) 引入递归梯度技术改进条件无偏梯度估计器的误差界；4) 设计基于不精确复合梯度方法的算法，包含精心构造的梯度估计器和辅助子问题求解过程。

Result: 提出的新算法在非凸联邦优化中实现了已知最佳的通信和本地计算复杂度，RG-SAGA相比原始SAGA具有改进的误差界。

Conclusion: 通过量化通信成本的新模型和改进的梯度估计技术，实现了联邦优化中通信效率和计算效率的平衡，为不同客户端选择策略提供了公平比较框架。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [50] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM验证过滤错误轨迹，并重新表述子查询来处理检索失败，从而提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务中，现有基于训练的方法仍受LLM幻觉和错误推理路径影响，导致性能受限。

Method: 1) 使用蒙特卡洛树搜索生成训练路径轨迹；2) 通过子答案召回和LLM作为裁判验证来过滤错误和冗长轨迹，提升训练数据质量；3) 重新表述子查询以处理检索失败情况。

Result: PATHFINDER在公开基准数据集上提升了多跳问答的性能。

Conclusion: 通过生成高质量训练轨迹、过滤错误路径和优化检索策略，PATHFINDER能有效改善多跳问答系统的性能。

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [51] [Interaction Tensor Shap](https://arxiv.org/abs/2512.05338)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出IT SHAP方法，将高阶Shapley交互表示为张量网络收缩，在多项式时间内计算高阶特征交互，解决了STII指数级计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得更深、维度更高，理解特征如何影响预测变得困难。现有Shapley方法无法高效计算高阶交互：STII需要指数级枚举，MST只限于一阶效应。需要同时保持STII的公理精确性并避免指数计算复杂度的方法。

Method: 提出Interaction Tensor SHAP (IT SHAP)，将Shapley Taylor Interaction Index (STII) 重新表述为值张量和权重张量的收缩。假设权重张量具有多项式TT秩的有限状态张量链表示，在TT结构化模型和分布张量下，将STII的指数复杂度Θ(4^n)降低到NC2并行时间。

Result: IT SHAP能够在多项式时间和多对数深度下计算高阶Shapley交互，为高维模型中的主效应和高阶交互提供了统一、公理化和计算可行的表述。

Conclusion: IT SHAP为可扩展的交互感知可解释AI奠定了基础，使之前因组合结构而无法进行交互分析的大型黑盒模型能够进行交互分析。

Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.

</details>


### [52] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: Roblox Guard 1.0是基于Llama-3.1-8B-Instruct的指令微调LLM，通过输入输出双重审核增强LLM系统安全性，并发布了RobloxGuard-Eval评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在训练后进行了安全对齐，但仍可能生成不当输出，存在用户风险，因此需要跨输入输出的鲁棒安全防护机制。

Method: 基于Llama-3.1-8B-Instruct进行指令微调，使用合成和开源安全数据集混合训练，通过思维链推理和输入反转增强上下文理解，采用LLM管道提升审核能力。

Result: 模型能泛化到未见过的安全分类体系，在域外安全基准上表现优异，同时发布了RobloxGuard-Eval可扩展安全分类评估基准。

Conclusion: Roblox Guard 1.0为LLM系统提供了全面的输入输出安全防护框架，通过指令微调和系统评估基准提升了LLM安全审核能力。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [53] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出针对硬件代码生成的LLM遗忘框架，通过语法保持遗忘和细粒度选择损失，有效移除问题知识而不损害代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在硬件代码生成中存在可靠性问题，包括记忆专有IP、基准污染和不安全编码模式，需要专门的方法来降低这些风险。

Method: 结合语法保持遗忘策略（保护硬件代码结构完整性）和细粒度floor感知选择损失（精确高效移除问题知识）的遗忘框架。

Result: 支持3倍大的遗忘集，通常只需单次训练周期，同时保持RTL代码的语法正确性和功能完整性。

Conclusion: 该工作为可靠的LLM辅助硬件设计开辟了新途径，实现了有效遗忘而不损害代码生成能力。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [54] [Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation](https://arxiv.org/abs/2512.05367)
*Mariia Karabin,Isaac Armstrong,Leo Beck,Paulina Apanel,Markus Eisenbach,David B. Mitzi,Hanna Terletska,Hendrik Heinz*

Main category: cs.LG

TL;DR: 提出机器学习框架预测杂化金属卤化物结构维度，使用化学特征工程和类别不平衡处理技术提升预测准确率


<details>
  <summary>Details</summary>
Motivation: 杂化金属卤化物（包括有机-无机钙钛矿）的结构维度预测对材料设计至关重要，但现有数据集存在严重的类别不平衡问题（0D、1D、2D、3D类别样本分布不均），这给预测建模带来重大挑战

Method: 1. 使用合成少数类过采样技术（SMOTE）将数据集从494个扩充到1336个以缓解类别不平衡；2. 开发基于相互作用的描述符；3. 采用多阶段工作流程，结合特征选择、模型堆叠和性能优化来提升维度预测准确性

Result: 该方法显著提高了少数类别的F1分数，在所有维度类别上都实现了稳健的交叉验证性能

Conclusion: 提出的机器学习框架通过化学特征工程和先进的类别不平衡处理技术，有效解决了杂化金属卤化物结构维度预测中的类别不平衡问题，为材料设计提供了可靠的工具

Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.

</details>


### [55] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏的文本特征子集来解决高维文本数据中的因果推断问题，特别是处理重叠性假设违反和估计不稳定的挑战。


<details>
  <summary>Details</summary>
Motivation: 高维文本数据在因果推断中虽然能编码丰富上下文信息，但会导致重叠性假设违反、极端倾向得分、权重不稳定和方差膨胀等问题，需要专门的方法来处理这些挑战。

Method: 提出混淆感知的标记合理化（CATR）框架，使用残差独立性诊断来选择稀疏的必要标记子集，保留足够的混淆信息以保证无混淆性，同时丢弃无关文本。

Result: 在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法产生了更准确、稳定和可解释的因果效应估计。

Conclusion: CATR通过选择稀疏的文本特征子集，有效缓解了观测层面的重叠性假设违反，稳定了下游因果效应估计器，为文本数据因果推断提供了实用解决方案。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [56] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究基于CorrDiff扩散模型框架，将统计降尺度方法应用于中国区域天气预测，从25km全球网格预报生成3km高分辨率预报，在多个变量上优于传统区域模型。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。统计降尺度方法通过建立低分辨率与高分辨率历史数据间的统计关系来解决这一问题。深度学习特别是扩散模型为这一任务提供了强大工具。

Method: 采用基于扩散模型的CorrDiff降尺度框架，扩展应用到近20倍大的中国区域，不仅考虑地表变量，还包含六个气压层的高层变量。添加全局残差连接提高精度，将模型应用于CMA-GFS和SFF的25km全球网格预报，生成3km分辨率预报。

Result: 实验结果表明，该方法降尺度后的预报在目标变量的MAE上普遍优于CMA-MESO区域模型的直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成更精细的细节，相比确定性回归模型产生更真实的预测。

Conclusion: 基于CorrDiff的扩散模型降尺度方法能够有效生成中国区域的高分辨率天气预报，在多个变量上超越传统区域模型，展示了生成模型在捕捉精细尺度细节方面的优势。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [57] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编程（通过自然语言与AI助手交互生成代码）是终端用户开发的可行范式，可能补充甚至取代传统的低代码/无代码平台。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编程是否可行，以及它是否能补充或取代现有的低代码/无代码平台。随着生成式AI和大型语言模型的发展，终端用户可能通过自然语言提示直接生成和优化代码，这为终端用户开发带来了新的可能性。

Method: 通过案例研究，让非程序员与AI助手交互来开发一个基本的Web应用。研究分析了参与者的表现、完成时间和他们对AI辅助终端用户编程的看法。

Result: 大多数研究参与者成功地在合理时间内完成了任务，并支持AI辅助终端用户编程作为终端用户开发的可行方法。

Conclusion: AI辅助终端用户编程是终端用户开发的可行范式，具有灵活性、适用性广、开发速度快、可重用性好和减少供应商锁定等优势，可能在未来补充甚至取代低代码/无代码平台。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [58] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新靶点上的泛化能力，发现常用基准测试无法反映真实挑战，探索大规模自监督预训练和简单测试数据利用方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中日益重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。虽然许多评分函数在标准基准测试上表现良好，但它们在训练数据之外的泛化能力仍然是一个重大挑战。

Method: 评估最先进评分函数在模拟有限已知结构和实验亲和力测量靶点的数据集划分上的泛化能力。研究大规模自监督预训练是否能弥补泛化差距，并探索利用有限测试靶点数据改进评分函数性能的简单方法。

Result: 分析显示常用基准测试不能反映泛化到新靶点的真实挑战。提供了大规模自监督预训练潜力的初步证据，并探讨了利用有限测试数据改进性能的方法。

Conclusion: 研究结果强调需要更严格的评估协议，并为设计具有扩展到新蛋白质靶点预测能力的评分函数提供实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [59] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 提出MineROI-Net，一个基于Transformer的模型，用于预测比特币挖矿硬件购买的盈利性，帮助矿工在波动的市场中做出更明智的投资决策。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件采购面临市场波动、技术快速过时和协议驱动的收入周期等挑战，但缺乏关于何时购买新ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件采购问题构建为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出MineROI-Net，一个开源的基于Transformer的架构，旨在捕捉挖矿盈利性的多尺度时间模式。

Result: 在2015-2024年发布的20种ASIC矿机数据上评估，MineROI-Net在多种市场环境下表现优于LSTM和TSLANet基线，达到83.7%的准确率和83.1%的宏F1分数。模型在经济相关性方面表现强劲，检测无盈利时期的精确度为93.6%，盈利时期的精确度为98.5%。

Conclusion: MineROI-Net为挖矿硬件采购时机提供了实用的数据驱动工具，可能降低资本密集型挖矿操作的财务风险。模型已开源供业界使用。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [60] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD：一种反射式进化编排器，将LLM推理与反馈对齐的架构搜索有效结合，实现高性能、可部署的神经架构设计


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的神经架构设计面临挑战：token级设计循环是离散且不可微分的，无法通过反馈平滑指导架构改进；这些方法容易陷入冗余结构的模式崩溃或向不可行设计漂移

Method: 1）多轮多专家共识：将孤立的设计规则转化为有意义的架构线索；2）自适应反射探索：根据奖励方差调整探索程度，在反馈不确定时探索，稳定时细化；3）帕累托引导的进化选择：联合优化准确性、效率、延迟、置信度和结构多样性

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape数据集上实现最先进的性能；消融和迁移研究进一步验证了RevoNAD在实际可靠、可部署神经架构设计中的有效性

Conclusion: RevoNAD成功地将LLM推理与反馈对齐的架构搜索相结合，解决了现有LLM驱动神经架构设计中的关键问题，实现了高性能且可部署的架构设计

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [61] [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)
*Bozhi Dan,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: Triplet-GCN：一种基于图卷积网络的脓毒症早期预警模型，通过患者-特征-值三元组构建二分EHR图，在ICU环境中显著优于传统表格基线模型。


<details>
  <summary>Details</summary>
Motivation: ICU中脓毒症是导致患者发病和死亡的主要原因，但电子健康记录数据的复杂性、稀疏性和异质性阻碍了及时检测。传统表格模型难以有效处理这种结构化数据。

Method: 提出Triplet-GCN模型：1) 将每次就诊表示为患者-特征-值三元组；2) 构建二分EHR图；3) 使用图卷积网络学习患者嵌入；4) 通过轻量级多层感知机进行分类；5) 采用类型特定的预处理方法（数值变量中位数填补和标准化，二元特征效应编码，稀有分类属性众数填补和低维嵌入）。

Result: 在中国三家三级医院的回顾性多中心队列（N=648，70/30训练测试分割）中，Triplet-GCN在区分度和平衡误差指标上一致优于强表格基线（KNN、SVM、XGBoost、随机森林），具有更有利的敏感性-特异性权衡和改善的早期预警整体效用。

Conclusion: 将EHR编码为三元组并在患者-特征图上传播信息比特征独立模型产生更具信息量的患者表示，为可部署的脓毒症风险分层提供了一个简单、端到端的蓝图。

Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient-feature-value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train-test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity-specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient-feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.

</details>


### [62] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: TS-Hint：一个结合链式思维推理的时间序列基础模型框架，用于半导体制造中的材料去除率预测，能在有限数据下通过少样本学习直接从多元时间序列特征中学习。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法依赖从时间序列中提取静态特征来近似半导体制造过程（如化学机械抛光）的材料去除率，但这会导致时间动态信息的丢失，并且这些方法需要大量数据进行有效训练。

Method: 提出TS-Hint框架，这是一个时间序列基础模型，集成了链式思维推理，在训练过程中基于注意力机制数据和显著性数据提供注意力提示。

Result: 实验结果表明，该模型在有限数据设置下通过少样本学习表现出有效性，并且能够直接从多元时间序列特征中学习。

Conclusion: TS-Hint框架解决了现有方法在时间动态信息丢失和大量数据需求方面的局限性，为半导体制造过程监控提供了更高效的时间序列分析方法。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [63] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: IdealTSF框架利用非理想负样本来增强时间序列预测，通过预训练、训练和优化三阶段，结合正负样本提升模型性能


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中普遍存在缺失值和异常值等问题，传统方法主要关注特征提取或将这些问题数据作为正样本进行知识迁移，但未能充分利用非理想负样本的潜力来增强事件预测能力

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据中提取知识；2) 训练阶段将序列数据转化为理想正样本；3) 应用带有对抗扰动的负优化机制

Result: 大量实验表明，负样本数据能够显著释放基础注意力架构在时间序列预测中的潜力，特别是在噪声样本或低质量数据应用中表现优异

Conclusion: IdealTSF框架通过整合理想正样本和非理想负样本，特别适合处理噪声样本或低质量数据的时间序列预测任务，为深度学习在时间序列预测中的进一步发展提供了新思路

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [64] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 集成模型在表格分类任务中通过降低方差实现高精度和低过拟合，但在线性数据上优势有限，在非线性数据上提升显著，在噪声数据上需要正则化。


<details>
  <summary>Details</summary>
Motivation: 研究集成模型如何在保持高精度的同时控制过拟合，理解集成模型在表格分类任务中的泛化能力，为实际应用提供模型选择指导。

Method: 使用重复分层交叉验证和统计显著性检验，比较线性模型、单决策树和9种集成方法在四个表格分类任务（乳腺癌、心脏病、糖尿病、信用卡欺诈）上的表现。

Result: 集成模型通过平均或受控提升降低方差，实现高精度和低泛化差距；在线性数据上集成优势有限；在非线性数据上树基集成提升测试精度5-7个百分点，泛化差距低于3%；在噪声或不平衡数据上需要正则化。

Conclusion: 集成模型能有效平衡精度和过拟合，但效果取决于数据特性；数据集复杂度指标（线性度评分、Fisher比率、噪声估计）可预测集成效果；研究为实际表格应用提供了模型选择的实用指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [65] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构学习中的性能，发现置换对称嵌入是最具泛化性的量子机器学习模型


<details>
  <summary>Details</summary>
Motivation: 研究分子几何结构层次中不同对称性量子机器学习模型的性能差异，为几何数据集选择合适模型提供标准

Method: 使用两种分子数据集（线性LiH分子和三角锥形NH3分子），比较无对称性、旋转和置换等变性、图嵌入置换等变性量子模型，以经典等变模型为基准

Result: 图嵌入特征能有效提升几何数据集的训练性，置换对称嵌入是几何学习中最具泛化性的量子机器学习模型

Conclusion: 分子几何结构与模型性能差异揭示了模型选择标准，图嵌入是实现几何数据集更好训练性的有效途径，置换对称嵌入是最佳泛化模型

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [66] [Turbulence Regression](https://arxiv.org/abs/2512.05483)
*Yingang Fan,Binjie Ding,Baiyi Chen*

Main category: cs.LG

TL;DR: 提出基于离散化数据的NeuTucker分解模型，用于低空湍流预测，通过构建四维Tucker交互张量捕捉三维风场数据的潜在时空交互，在缺失观测估计中优于传统回归模型。


<details>
  <summary>Details</summary>
Motivation: 低空湍流复杂多变，传统方法仅使用风廓线雷达数据难以准确预测湍流状态，需要新方法处理连续稀疏的三维风场数据。

Method: 1) 离散化连续输入数据以适应NeuTucF等需要离散输入的模型；2) 构建四维Tucker交互张量表示不同高度和三维风速间的所有可能时空交互；3) 基于Tucker神经网络构建低秩Tucker分解模型。

Result: 在真实数据集的缺失观测估计中，离散化NeuTucF模型相比多种常见回归模型表现出更优越的性能。

Conclusion: 提出的离散化NeuTucker分解模型能有效处理三维风场数据，捕捉湍流的潜在时空交互模式，为低空湍流预测提供了更准确的方法。

Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.

</details>


### [67] [GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop](https://arxiv.org/abs/2512.05502)
*Omid Bazgir,Vineeth Manthapuri,Ilia Rattsev,Mohammad Jafarnejad*

Main category: cs.LG

TL;DR: GRASP是一个多智能体、图推理框架，通过人机交互界面将定量系统药理学模型编码为类型化生物知识图谱，并编译为可执行的MATLAB/SimBiology代码，同时保持单位、质量平衡和生理约束。


<details>
  <summary>Details</summary>
Motivation: 定量系统药理学建模对药物开发至关重要，但需要大量时间投入，限制了领域专家的效率。当前方法存在效率低下、易出错的问题，需要一种既能保持生物医学保真度又能提高可访问性的解决方案。

Method: 采用两阶段工作流：理解阶段（从遗留代码重建图谱）和行动阶段（约束检查、语言驱动的修改）。使用状态机进行迭代验证，通过广度优先参数对齐围绕新实体发现依赖量并建议生物合理的默认值，运行自动执行/诊断直至收敛。

Result: 在LLM作为评判的对比评估中，GRASP在生物合理性、数学正确性、结构保真度和代码质量方面优于SME引导的CoT和ToT基线（约9-10/10 vs. 5-7/10）。BFS对齐在依赖发现、单位和范围方面达到F1 = 0.95。

Conclusion: 图结构化的智能体工作流可以使QSP模型开发既易于访问又严谨，使领域专家能够用自然语言指定机制而不牺牲生物医学保真度，为药物开发提供高效可靠的建模工具。

Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.

</details>


### [68] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 提出CDEC和IDEC两种新方法，通过信度集和证据预测分布区间来处理分类任务中的不确定性量化，能够区分认知和偶然不确定性，在不确定性过高时拒绝分类，在可接受范围内提供具有概率保证的标签集合。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是人工智能领域的关键挑战，直接影响决策、风险评估和模型可靠性。现有方法在处理不确定性方面存在不足，特别是在区分认知不确定性和偶然不确定性方面。

Method: 提出两种新方法：CDEC使用信度集（概率的闭凸集），IDEC使用证据预测分布区间。两种方法都采用标准反向传播和基于证据理论的损失函数进行训练，能够避免对训练数据的过拟合，并系统评估认知不确定性和偶然不确定性。

Result: 在MNIST、CIFAR-10、CIFAR-100及其自然分布外偏移数据集上的实验表明，CDEC和IDEC实现了竞争性的预测准确率、在认知不确定性和总不确定性下的最先进分布外检测能力，以及紧致、校准良好的预测区域，在分布偏移时可靠扩展。消融实验显示CDEC仅需小规模集成就能获得稳定的不确定性估计。

Conclusion: CDEC和IDEC克服了先前工作的不足，扩展了当前证据深度学习文献，为分类任务中的不确定性量化提供了有效解决方案，能够在不确定性过高时拒绝分类，在可接受范围内提供具有概率保证的预测。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [69] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的增量分布核方法，通过动态核均值嵌入实现高精度实时检测，相比现有方法在保持精度的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布不断演变的情况下保持高检测精度，同时确保实时效率。现有方法难以同时满足这两个要求。

Method: 提出IDK-S（增量分布核流式异常检测），基于隔离分布核框架，采用轻量级增量更新机制，避免完全模型重训练，在核均值嵌入框架中创建动态表示。

Result: 在13个基准测试中，IDK-S实现了优越的检测精度，同时运行速度显著快于现有最先进方法，在许多情况下快一个数量级，且统计上等效于完全重训练模型。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的平衡问题，通过创新的增量分布核方法，在保持检测精度的同时大幅提升计算效率。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [70] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出了首个统一的稀疏字典学习理论框架，将多种方法（稀疏自编码器、转码器、交叉编码器）统一为单一优化问题，分析了优化景观，并首次理论解释了特征吸收、死亡神经元等经验现象。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，理解其内部表示和处理机制变得至关重要。现有稀疏字典学习方法虽然经验成功但缺乏理论理解，现有理论仅限于权重绑定的稀疏自编码器，需要为更广泛的SDL方法提供理论基础。

Method: 提出统一的理论框架，将多种SDL方法（稀疏自编码器、转码器、交叉编码器）建模为统一的优化问题，分析优化景观，理论解释经验现象，并通过受控实验验证理论结果。

Result: 建立了首个统一的SDL理论框架，展示了不同方法如何实例化该框架，提供了优化景观的严格分析，首次理论解释了特征吸收、死亡神经元和神经元重采样技术等经验现象。

Conclusion: 该工作为稀疏字典学习方法提供了首个统一的理论基础，填补了经验成功与理论理解之间的空白，有助于更好地理解和改进神经网络的可解释性方法。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [71] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出SCoNE方法解决多视图异常检测中邻居一致性表示问题，无需学习过程，实现O(N)时间复杂度，在准确率和效率上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1) 在不同视图密度变化区域难以保证捕获一致的邻居，导致检测准确率低；2) 学习过程计算复杂度为O(N²)，不适用于大规模数据集

Method: 提出SCoNE方法，具有两个独特特征：1) 直接使用多视图实例表示一致邻居，无需中间表示；2) 邻居具有数据依赖特性，在稀疏区域形成大邻域，在密集区域形成小邻域

Result: SCoNE在检测准确率上优于现有方法，在大规模数据集上运行速度比现有方法快几个数量级

Conclusion: SCoNE通过数据依赖的邻域表示解决了多视图异常检测中的邻居一致性问题，无需学习过程，实现了线性时间复杂度，在准确率和效率方面均有显著提升

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [72] [RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs](https://arxiv.org/abs/2512.05542)
*Jonathan Geuter,Gregor Kornhardt*

Main category: cs.LG

TL;DR: RoBoN是一种多模型推理方法，通过在线路由在多个LLM之间选择最佳响应，相比单模型best-of-n有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 传统best-of-n方法只使用单一模型生成响应，但不同LLM在不同任务上具有互补优势，需要利用多模型多样性来提升推理性能

Method: 提出RoBoN方法：给定M个模型，基于奖励模型和预测响应的一致性信号，逐个在线路由生成过程，无需额外训练，保持计算对等

Result: 在多个推理基准测试中，RoBoN在较大n值时始终优于单模型best-of-n，绝对准确率提升高达3.4%，也优于均匀多模型组合基线

Conclusion: 多模型多样性可以在推理时被利用来提升best-of-n性能，提供了一种简单、无需训练的多LLM测试时扩展路径

Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.

</details>


### [73] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出一种基于MARS和N-ball采样的新型可解释方法，相比LIME能更好地捕捉非线性局部边界，提高解释的忠实度


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，提供预测解释变得至关重要。LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确

Method: 使用多元自适应回归样条（MARS）建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布采样，而非像LIME那样重新加权样本

Result: 在三个UCI数据集上评估，相比基线方法，新方法能产生更忠实的解释，平均减少37%的均方根误差，显著提高了局部忠实度

Conclusion: 提出的方法通过MARS建模非线性边界和N-ball采样技术，有效提高了局部解释的忠实度，解决了LIME方法在非线性场景下的局限性

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [74] [Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection](https://arxiv.org/abs/2512.05567)
*Antoine Blais,Nicolas Couëllan*

Main category: cs.LG

TL;DR: 提出基于最优传输的半监督学习方法，使用Wasserstein距离作为图像相似度度量，通过标签传播机制处理标注数据稀缺问题，应用于GNSS多径干扰检测。


<details>
  <summary>Details</summary>
Motivation: 在图像数据标注稀缺的情况下，需要有效的半监督学习方法。传统方法可能无法充分利用未标注数据，而最优传输理论中的Wasserstein距离能更好地度量图像样本间的相似性。

Method: 基于隐式图传导的半监督学习方法，使用Wasserstein距离作为图像相似度度量，在深度卷积网络中实现标签传播机制。通过超参数控制半监督程度和对度量的敏感度。

Result: 在GNSS多径干扰检测实验中，特定超参数设置下，分类准确率显著优于全监督训练方法，证明了该方法在处理标注数据稀缺问题上的有效性。

Conclusion: 基于最优传输的半监督学习方法能有效利用Wasserstein距离度量图像相似性，在标注数据稀缺的情况下显著提升分类性能，特别适用于GNSS多径干扰检测等实际应用。

Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.

</details>


### [75] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出熵比裁剪机制，通过约束当前与先前策略的熵比来稳定强化学习训练，解决分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练使用强化学习，但离策略训练引入分布偏移，导致训练不稳定（策略熵波动、梯度不稳定）。PPO-Clip通过重要性裁剪缓解但忽略了动作的全局分布偏移。

Method: 提出熵比作为全局度量，量化策略探索的相对变化。基于此提出熵比裁剪机制，对熵比施加双向约束，稳定策略更新。将ERC集成到DAPO和GPPO算法中。

Result: 在多个基准测试中，ERC一致提升了性能表现。

Conclusion: ERC机制能有效稳定强化学习训练，通过全局分布层面的约束改善训练稳定性，并弥补了PPO-clip无法调节未采样动作概率偏移的不足。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [76] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 该研究探讨如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）的规模，发现遵循μP的学习率缩放规则并结合分块和显式谱归一化可改善迁移效果，而计算最优缩放中权重衰减应随宽度反比缩放，最终在Llama架构模型上实现1.3-1.4倍于AdamW的加速。


<details>
  <summary>Details</summary>
Motivation: 近期基于矩阵级预条件的深度学习优化器在小规模实验中显示出优于AdamW的加速潜力，但验证和复现结果不一。为理解这些优化器在大规模场景下的有效性，需要研究如何通过超参数迁移来扩展预条件优化器的规模。

Method: 研究学习率和权重衰减如何随模型宽度和深度缩放，涵盖Shampoo、SOAP、Muon等多种优化器，并考虑分块和嫁接等常用技术的影响。应用μP缩放规则，分析有限宽度偏差问题，提出通过分块和显式谱归一化来缓解学习率漂移。

Result: 遵循μP的学习率缩放规则可改善迁移，但仍有显著有限宽度偏差导致最优学习率漂移，分块和显式谱归一化可缓解此问题。计算最优缩放中，权重衰减按1/宽度缩放近乎最优。应用这些规则后，Muon和Shampoo在190M至1.4B参数的Llama架构语言模型训练中，分别实现1.4倍和1.3倍于AdamW的加速，而错误缩放下加速效果随规模增大迅速消失。

Conclusion: 研究最优超参数迁移对于在现实调优预算下可靠比较大规模优化器至关重要。正确的缩放规则能使预条件优化器在大规模训练中保持显著加速优势，而错误缩放会导致优势迅速消失。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [77] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一种灵活控制GNN社区检测中社区数量的框架，允许用户指定范围或精确数量


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定聚类数量，而GNN方法即使指定了期望数量也经常无法准确返回。现有方法要么需要精确知道真实聚类数，要么无法可靠控制输出数量。

Method: 提出一个灵活框架，允许用户指定社区数量的合理范围，并在训练过程中强制执行这些边界约束。如果用户需要精确的聚类数量，也可以指定并可靠返回。

Result: 该方法能够可靠地控制GNN发现的社区数量，既支持范围约束也支持精确数量要求，解决了GNN社区检测中数量控制不可靠的问题。

Conclusion: 该工作为GNN社区检测提供了灵活且原则性的社区数量控制方法，克服了现有方法在数量控制方面的局限性。

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [78] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 论文提出"模块化喷射"框架，用于评估回归和分类流水线中模块分解的唯一性，区分"幻象"和"可识别"两种机制


<details>
  <summary>Details</summary>
Motivation: 传统监督学习通过预测风险评估模型，但无法确定模型内部分解是否由数据和评估设计唯一确定。需要一种方法来评估模块分解的唯一性。

Method: 提出模块化喷射框架：给定任务流形、模块分解和模块级表示访问，估计经验喷射（描述模块对输入扰动的局部线性响应）。引入"幻象"和"可识别"机制概念，提出MoJet算法进行喷射估计和幻象诊断。

Result: 在双模块线性回归流水线中证明喷射可识别性定理：在温和的秩假设和模块级喷射访问下，内部分解是唯一确定的。而仅基于风险的评估则允许大量实现相同输入-输出映射的幻象分解。

Conclusion: 模块化喷射框架提供了超越预测风险的模型评估方法，能够识别模块分解的唯一性，有助于理解模型内部结构和避免幻象分解的误导。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [79] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由方法，通过在预训练时对选定梯度进行零掩码，将目标知识隔离到特定参数子集中，从而更鲁棒地处理标签噪声，实现更好的知识保留/遗忘权衡。


<details>
  <summary>Details</summary>
Motivation: 现有数据过滤方法面临大规模标注成本高和标签噪声问题，即使少量误标内容也可能导致危险能力产生。需要一种在预训练时能更鲁棒处理标签噪声的安全缓解方法。

Method: 提出SGTM（选择性梯度掩码），改进梯度路由技术，对选定梯度进行零掩码，使目标领域示例仅更新其专用参数，从而将目标知识局部化到特定参数子集中。

Result: 在双语数据集移除语言知识和维基百科移除生物学知识两个应用中，SGTM相比数据过滤和先前梯度路由方法，在存在标签错误时提供了更好的保留/遗忘权衡。对对抗性微调表现出强鲁棒性，需要7倍于RMU的微调步数才能恢复遗忘集性能。

Conclusion: SGTM为现有安全缓解措施提供了一个有前景的预训练时补充方法，特别是在标签噪声不可避免的场景中，能够更鲁棒地处理误标有害内容的风险。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [80] [Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks](https://arxiv.org/abs/2512.05680)
*Alexander Mattick,George Yammine,Georgios Kontes,Setareh Maghsudi,Christopher Mutschler*

Main category: cs.LG

TL;DR: 该论文提出了一种基于部分可观测马尔可夫决策过程（POMDP）的波束选择方法，将波束选择问题建模为在线搜索过程，优于传统的监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 在5G/6G网络中，大规模天线阵列的模拟波束成形面临挑战：大型码本、反射和阻塞效应使得最优波束选择困难。现有方法使用监督学习基于历史波束预测最优波束，但难以处理新轨迹和环境变化。

Method: 将波束选择问题建模为部分可观测马尔可夫决策过程（POMDP），将环境建模为码本本身。在每个时间步，基于不可观测最优波束的信念状态和先前探测的波束来选择候选波束，将问题框架化为定位移动最优波束的在线搜索过程。

Result: 该方法能够处理新的或不可预见的轨迹和物理环境变化，性能比先前工作提高了数个数量级。

Conclusion: 基于POMDP的在线搜索方法比传统的监督学习方法更有效地解决了波束选择问题，特别是在处理动态环境和未知轨迹方面具有显著优势。

Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.

</details>


### [81] [BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language](https://arxiv.org/abs/2512.05721)
*Nitin Priyadarshini Shankar,Vaibhav Singh,Sheetal Kalyani,Christian Maciocco*

Main category: cs.LG

TL;DR: BERTO是一个基于BERT的框架，用于蜂窝网络的流量预测和能耗优化，通过自然语言提示平衡节能与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络需要同时优化流量预测准确性和能耗效率，传统方法难以灵活平衡节能与性能的权衡。

Method: 基于Transformer架构，采用平衡损失函数和基于提示的自定义机制，通过自然语言输入指导模型处理预测不足和过度预测问题。

Result: 在真实数据集上，BERTO相比现有模型将MSE降低了4.13%，能够在1.4kW功率范围和9倍服务质量变化范围内灵活运行。

Conclusion: BERTO通过自然语言提示有效平衡了节能与性能的竞争目标，适合智能无线接入网络部署。

Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.

</details>


### [82] [Teaching Language Models Mechanistic Explainability Through Arrow-Pushing](https://arxiv.org/abs/2512.05722)
*Théo A. Neukomm,Zlatko Jončev,Philippe Schwaller*

Main category: cs.LG

TL;DR: 开发了基于箭头推演形式主义的化学反应机制预测框架，使用MechSMILES编码分子结构和电子流，在多个机制预测任务上取得高准确率，为计算机辅助合成规划提供机理基础。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划系统缺乏机理基础，无法提供化学反应机制的关键洞察。需要开发能够预测化学反应机制的计算框架，使合成规划更加可靠和可解释。

Method: 引入基于箭头推演形式主义的计算框架，开发MechSMILES文本格式编码分子结构和电子流，使用语言模型在四个复杂度递增的机制预测任务上进行训练，使用mech-USPTO-31k和FlowER等机理反应数据集。

Result: 模型在基础步骤预测上达到超过95%的top-3准确率，在mech-USPTO-31k数据集上超过73%，在FlowER数据集上达到93%的完整反应机制检索准确率。实现了三个关键应用：CASP系统后验证、全原子映射和催化剂感知反应模板提取。

Conclusion: 通过基于物理意义的电子移动进行预测，确保质量和电荷守恒，为更可解释和化学有效的计算合成规划提供了途径，同时提供了架构无关的机制预测基准框架。

Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.

</details>


### [83] [Towards agent-based-model informed neural networks](https://arxiv.org/abs/2512.05764)
*Nino Antulov-Fantulin*

Main category: cs.LG

TL;DR: 提出ABM-NNs框架，将基于代理模型的原则融入神经网络设计，确保学习到的动力学保持物理约束和结构特性，在三个案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程在建模复杂系统时存在局限，缺乏物理不变量（如能量）但需要强制执行其他约束（如质量守恒、网络局部性、有限理性）。需要设计能保持基于代理模型原则的神经网络。

Method: 引入基于代理模型信息的神经网络（ABM-NNs），利用受限图神经网络和层次分解来学习可解释、结构保持的动力学。

Result: 在三个案例中验证：1）广义Lotka-Volterra系统中从短轨迹恢复真实参数；2）图基SIR传染模型中超越现有图学习基线；3）真实世界宏观经济模型中学习耦合GDP动态并进行基于梯度的反事实分析。

Conclusion: ABM-NNs框架成功将基于代理模型的原则融入神经网络设计，能够学习保持结构约束的可解释动力学，在复杂系统建模中表现出色。

Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.

</details>


### [84] [Learnability Window in Gated Recurrent Neural Networks](https://arxiv.org/abs/2512.05790)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 该论文建立了理论框架，解释门控机制如何决定循环神经网络的"可学习窗口"——梯度信息保持统计可恢复的最大时间范围。研究发现可学习性由"有效学习率"控制，而非传统的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统分析强调雅可比乘积的数值稳定性，但作者发现稳定性不足以解释循环神经网络学习长程时间依赖的能力。需要建立新的理论框架来理解门控机制如何影响梯度信息的统计可恢复性。

Method: 通过一阶展开门控诱导的雅可比乘积，推导出每个时滞和每个神经元的有效学习率μ_{t,ℓ}。在重尾（α-稳定）梯度噪声假设下，分析检测时滞ℓ依赖所需的最小样本量N(ℓ)∝f(ℓ)^{-α}，其中f(ℓ)=‖μ_{t,ℓ}‖₁是有效学习率包络。

Result: 理论推导出可学习窗口H_N的显式公式，以及f(ℓ)对数、多项式和指数衰减的闭式标度律。预测更宽或更异质的门控谱产生更慢的f(ℓ)衰减和更大的可学习窗口，而更重的尾部噪声通过减慢统计集中来压缩H_N。

Conclusion: 有效学习率是控制门控循环网络何时以及多长时间能够学习长程时间依赖的基本量。该框架将门控诱导的时间尺度结构、梯度噪声和样本复杂度联系起来，为理解循环神经网络的学习能力提供了新视角。

Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $μ_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($α$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-α}$, where $f(\ell)=\|μ_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

</details>


### [85] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK SAEs可识别生物学相关特征但因果控制有限，Ordered SAEs能可靠实现生成控制但可解释性较差


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在抗体语言模型p-IgGen中的机制可解释性，探索如何有效控制模型生成

Method: 使用TopK和Ordered两种稀疏自编码器分析p-IgGen模型，比较它们在特征识别和生成控制方面的表现

Result: TopK SAEs能揭示生物学有意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAEs能可靠识别可控制特征，但激活模式更复杂、可解释性较差

Conclusion: TopK SAEs适合将潜在特征映射到概念，而Ordered SAEs在需要精确生成控制时更优，这推进了领域特定蛋白质语言模型的机制可解释性

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [86] [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)
*Zhengquan Luo,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出统一理论框架分析数据集蒸馏，揭示性能随样本数增长的缩放规律和随配置多样性线性增长的覆盖规律，证明不同匹配方法是可互换的替代目标。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏虽然实证进展迅速，但缺乏统一理论框架。现有方法基于不同的替代目标和优化假设，难以分析共同原理或提供通用保证。不清楚在训练配置（优化器、架构、数据增强）变化时，蒸馏数据如何保持全数据集的有效性。

Method: 提出配置-动态-误差分析统一理论框架，将主要数据集蒸馏方法重新表述为通用泛化误差视角。提供两个主要结果：1) 缩放规律（单配置上界）；2) 覆盖规律（所需蒸馏样本数与配置多样性线性增长）。

Result: 理论分析表明：1) 误差随蒸馏样本数增加而减少，解释了常见的性能饱和效应；2) 所需蒸馏样本数与配置多样性线性增长，具有可证明匹配的上界和下界；3) 各种匹配方法是可互换的替代目标，减少相同的泛化误差。

Conclusion: 统一分析揭示了数据集蒸馏的基本原理，为理论驱动设计紧凑、配置鲁棒的数据集蒸馏提供了基础，通过实验验证了推导的规律，推进了数据集蒸馏的理论基础。

Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

</details>


### [87] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文提供了超体积近似算法的完整数学和算法描述，填补了文献中的空白


<details>
  <summary>Details</summary>
Motivation: 超体积贝叶斯优化中，获取函数的优化计算成本高昂，主要源于超体积改进计算的开销。虽然超体积盒分解能应对频繁的精确改进计算，但其在最坏情况下具有超多项式内存复杂度。现有近似算法缺乏严格的算法描述。

Method: 提供Couckuyt等人（2012）提出的超体积近似算法的全面数学和算法细节描述

Result: 填补了文献中关于超体积近似算法严格描述的空白，提供了完整的数学框架和算法实现细节

Conclusion: 本文通过提供超体积近似算法的完整数学和算法描述，解决了多目标决策中贝叶斯优化的计算效率问题，为实际应用提供了可靠的工具

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [88] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种基于邻域引导的高效自回归集合Transformer，将分子图视为原子集合，通过自回归流模型学习图边界上可接受token的顺序无关分布，实现3D分子生成。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在3D分子结构生成中存在局限性：它们假设token顺序，但分子图中的下一个token预测应该对原子排列保持不变。先前工作通过使用规范顺序或焦点原子来回避这个问题，作者认为这是不必要的。

Method: 提出NEAT（Neighborhood-guided, Efficient, Autoregressive, Set Transformer），将分子图视为原子集合，使用自回归流模型学习图边界上可接受token的顺序无关分布，实现原子级排列不变性。

Result: NEAT在3D分子生成中接近最先进性能，具有高计算效率和原子级排列不变性，为可扩展分子设计建立了实用基础。

Conclusion: NEAT通过将分子图视为集合并学习顺序无关分布，解决了自回归模型中原子排列不变性问题，为高效、可扩展的3D分子生成提供了实用解决方案。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [89] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，使Transformer注意力稀疏化而不牺牲性能，可将注意力连接减少到约0.3%，同时保持原始预训练损失


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法主要关注计算效率，而本文探索将稀疏性作为结构先验，以揭示更有序、可解释的连接模式，并简化全局电路结构

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使Transformer注意力稀疏化，同时保留原始预训练能力

Result: 在高达10亿参数的模型上，可将注意力连接减少到约0.3%，保持原始预训练损失；任务特定电路涉及的组件（注意力头和MLP）大幅减少，连接边减少达100倍

Conclusion: Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可作为构建更结构化、可解释模型的指导原则

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [90] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: SNNs结合贝叶斯优化和惩罚性峰值准确率目标，在高频交易价格峰值预测中显著优于传统方法


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中的价格峰值具有重要风险与机会，但传统金融模型难以捕捉毫秒级时间结构，需要更适合处理离散事件和时序信息的模型

Method: 将高频股票数据转换为脉冲序列，评估三种架构：无监督STDP训练的SNN、具有显式抑制竞争的新型SNN、监督反向传播网络；使用贝叶斯优化和惩罚性峰值准确率目标进行超参数调优

Result: PSA优化的模型在模拟交易中始终优于SA优化的模型和基线；扩展SNN模型在简单回测中获得最高累计收益76.8%，显著超过监督替代方案的42.54%

Conclusion: 通过任务特定目标的鲁棒调优，脉冲网络在高频交易价格峰值预测中具有有效潜力，验证了生物启发模型在金融时序分析中的应用价值

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [91] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 提出基于机器学习的数据驱动方法预测蒸汽压，用于筛选和发现适用于太空环境的新型液体润滑剂


<details>
  <summary>Details</summary>
Motivation: 太空环境中移动机械组件需要液体润滑剂，但现有适用于真空条件的液体润滑剂种类有限且各有局限，限制了机械设计

Method: 采用数据驱动的机器学习方法，结合高通量分子动力学模拟和实验数据库数据训练模型，注重模型可解释性以识别化学结构与蒸汽压的关系

Result: 开发了能够预测蒸汽压的机器学习模型，基于模型洞察提出了几种有潜力的候选分子，可用于未来太空润滑剂应用

Conclusion: 机器学习方法能够有效预测润滑剂蒸汽压，为发现新型太空适用液体润滑剂提供了有效途径，有助于克服现有润滑剂的局限性

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [92] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出基于神经一致性的模型选择方法，仅需少量无标签目标域样本即可有效选择预训练模型检查点，在数据稀缺、分布外场景中显著优于现有基线


<details>
  <summary>Details</summary>
Motivation: 当前微调预训练视觉模型时，如何从大量训练检查点中选择最佳起点仍是一个开放问题，特别是在目标域数据稀缺、无标签且分布外的情况下，传统基于验证集的方法不可靠或不适用

Method: 提出神经一致性概念，通过分析模型在源域和目标域的激活统计特征，设计数据高效的模型选择方法，仅需少量无标签目标域样本即可评估模型适应性

Result: 在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，并在元学习设置中验证，相比基线方法显著提升了泛化性能

Conclusion: 神经一致性是一个强大的原则，不仅可用于模型选择，还可扩展到训练数据选择等任务，为数据稀缺场景下的模型适配提供了有效解决方案

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [93] [DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints](https://arxiv.org/abs/2512.05881)
*Rahul Golder,Bimol Nath Roy,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: DAE-HardNet：一种物理约束神经网络，通过可微分投影层同时学习函数及其导数，严格满足微分代数方程约束，相比传统PINNs实现物理损失数量级降低。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）通常以软约束方式最小化物理约束违反，难以严格满足包含微分算子的约束。数据驱动模型将原始函数视为黑盒，其导数只能在函数求值后获得，导致严格嵌入微分代数方程（DAEs）约束具有挑战性。

Method: 提出DAE-HardNet，通过可微分投影层将模型预测投影到约束流形上，同时学习函数及其导数，强制实施代数和微分约束。该方法包含主干神经网络和投影层，能够学习导数以改进约束学习。

Result: 相比多层感知机（MLPs）和PINNs，DAE-HardNet实现物理损失数量级降低，同时保持预测精度。能够学习导数，改进投影层前主干神经网络的约束学习。对于特定问题，可以绕过投影层实现更快推理。展示了在Lotka-Volterra捕食者-猎物系统和瞬态热传导等DAEs问题中的应用，以及参数估计能力。

Conclusion: DAE-HardNet提供了一种物理约束而非仅物理信息的方法，严格满足微分代数方程约束，在保持精度的同时显著降低物理损失，具有学习导数、改进约束学习和潜在推理加速的优势。

Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

</details>


### [94] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 提出基于LSTM的循环神经网络框架，用于估计分数泊松过程的参数，相比传统矩估计法将均方误差降低约55.3%，并在真实高频数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程能够建模具有记忆性和长程依赖的事件到达过程，但传统参数估计方法存在局限性。需要开发更有效的估计方法来处理现实世界中复杂的时间依赖关系。

Method: 使用长短期记忆网络构建循环神经网络框架，从事件到达时间间隔序列中估计分数泊松过程的关键参数μ和β，有效建模时间依赖关系。

Result: 在合成数据上，相比传统矩估计法将均方误差降低约55.3%；在真实数据（蒙哥马利县紧急呼叫记录和AAPL股票交易数据）上，LSTM能有效跟踪每日模式和参数变化。

Conclusion: 提出的LSTM框架能够有效估计分数泊松过程参数，在处理具有复杂时间依赖关系的真实世界数据方面表现出色，为事件到达建模提供了更准确的工具。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


### [95] [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 提出一种基于线性矩阵不等式框架的L-Lipschitz深度残差网络设计方法，通过LDL⊤分解扩展至任意非线性架构，在对抗鲁棒性和网络可验证性方面取得改进。


<details>
  <summary>Details</summary>
Motivation: 深度残差网络在计算机视觉任务中表现出色，但需要控制Lipschitz常数来增强对抗鲁棒性和网络可验证性。现有方法在构建Lipschitz约束网络方面存在局限性。

Method: 将ResNet架构重新表述为循环三对角LMI，推导网络参数的闭式约束以确保L-Lipschitz连续性。使用新的LDL⊤分解方法验证LMI可行性，将构造扩展到任意非线性架构。采用Cholesky分解进行高效参数化。

Result: 提出的LDL⊤公式是SDP基网络的紧松弛，保持完全表达能力，在121个UCI数据集上比SLL层获得3%-13%的准确率提升。实现了可证明的参数化方法，适用于对抗鲁棒性、认证训练和控制系统。

Conclusion: 该研究提供了一种严格的L-Lipschitz深度残差网络设计框架，通过LMI和LDL⊤分解方法扩展了Lipschitz约束网络的构造范围，为对抗鲁棒性和可验证神经网络设计提供了有效工具。

Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

</details>


### [96] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD：一种通过闭式解直接对注意力矩阵进行最优低秩分解的KV缓存压缩方法，相比现有方法能更精确地保持注意力输出


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批处理大小的增长，Transformer大语言模型中的KV缓存成为主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或尝试联合嵌入查询和键，但这些方法都忽略了注意力机制本质上依赖于查询和键的内积这一事实。

Method: 提出KQ-SVD方法，通过闭式解直接对注意力矩阵进行最优低秩分解。该方法针对冗余的真正来源，在压缩下能更高保真度地保持注意力输出。

Result: 在LLaMA和Mistral模型上的广泛评估表明，KQ-SVD方法在投影质量方面始终优于现有方法。

Conclusion: 直接对注意力矩阵进行低秩分解比单独压缩键或联合嵌入查询和键更有效，KQ-SVD提供了一种简单且计算高效的方法来解决KV缓存的内存瓶颈问题。

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [97] [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)
*Neil G. Marchant,Andrew C. Cullen,Feng Liu,Sarah M. Erfani*

Main category: cs.LG

TL;DR: 本文发现现有代理损失函数对分歧差异度量的不一致性，提出理论边界分析并设计新的可证明一致的损失函数，在对抗条件下提升估计准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在真实世界部署时因分布偏移而失效，分歧差异度量可用于处理分布偏移问题，但现有优化方法使用非可微的0-1损失，需要代理损失函数。然而现有代理损失函数存在贝叶斯不一致性缺陷，可能导致无法最大化真实分歧差异。

Method: 首先理论分析现有代理损失函数的最优性差距，提供上下界理论结果。基于此理论指导，提出新的分歧损失函数，与交叉熵配对使用，构建可证明一致的代理损失函数来估计分歧差异。

Result: 理论证明新方法具有一致性保证。在多样化基准测试中的实证评估表明，相比现有方法，新方法能提供更准确和鲁棒的分歧差异估计，特别是在具有挑战性的对抗条件下表现更优。

Conclusion: 本文揭示了现有分歧差异代理损失函数的基本缺陷，提出了理论边界分析和新的可证明一致的损失函数，为分布偏移下的模型鲁棒性评估和训练提供了更可靠的工具。

Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.

</details>


### [98] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文提出使用机器学习模型为美国年度商业调查(ABS)生成合成公共使用微数据样本(PUMS)，以解决商业数据去匿名化风险，并通过质量指标和实证复制验证合成数据的真实性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升和大数据可用性增加，传统匿名化商业数据面临重新识别风险，可能违反调查受访者的保密承诺。商业数据去匿名化挑战尤为突出，因为企业缺乏匿名性且某些行业在特定地理区域容易被识别。

Method: 使用机器学习模型构建基于年度商业调查(ABS)的合成PUMS，生成保留关键统计特征但不包含任何真实个体或企业记录的数据。论文还开发了2007年企业主调查的合成PUMS作为验证案例。

Result: 通过复制《小企业经济学》上发表的高影响力分析进行计量经济学验证，证明合成数据与真实数据具有高度相似性。虽然ABS PUMS仍在完善且结果保密，但验证案例展示了合成数据的可行性。

Conclusion: 合成数据方法能够有效保护商业调查受访者的隐私，同时为研究人员提供有价值的分析数据。该方法为ABS等商业调查数据的公共使用提供了可行的解决方案，并讨论了潜在的应用场景。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


### [99] [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950)
*Zalish Mahmud,Anantaa Kotal,Aritran Piplai*

Main category: cs.LG

TL;DR: Impugan：基于条件生成对抗网络的缺失值填补方法，能处理非线性、多模态关系，在异构数据集整合任务中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在缺失值，传统填补方法（如回归、期望最大化、多重填补）基于线性和独立性假设，难以处理复杂异构数据，导致有偏或过度平滑的估计

Method: 提出Impugan，一种条件生成对抗网络（cGAN），使用完整样本训练学习缺失变量对观测变量的依赖关系。生成器从可用特征重建缺失条目，判别器通过区分真实与填补数据来确保真实性

Result: 在基准数据集和多源整合任务中，Impugan相比领先基线方法，地球移动距离（EMD）降低达82%，互信息偏差（MI）降低达70%

Conclusion: 对抗训练的生成模型为填补和整合不完整、异构数据提供了可扩展且原则性的方法，能捕捉传统方法无法表示的非线性和多模态关系

Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025

</details>


### [100] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: 提出MaxShapley算法，用于生成式搜索引擎中内容提供商的公平归因与补偿，相比传统Shapley值计算复杂度从指数级降低到线性级


<details>
  <summary>Details</summary>
Motivation: 生成式搜索引擎基于大语言模型正在取代传统搜索，改变了信息提供商的补偿方式。为了维持这个生态系统，需要公平的机制来归因和补偿内容提供商对其生成答案的贡献。

Method: 引入MaxShapley算法，这是著名Shapley值的一个特例。它利用可分解的最大和效用函数来计算归因，文档数量的计算复杂度从指数级降低到线性级。

Result: 在三个多跳QA数据集（HotPotQA、MuSiQUE、MS MARCO）上评估，MaxShapley在归因质量上与精确Shapley计算相当，同时显著减少资源消耗。例如，在相同归因准确度下，相比先前最先进方法减少高达8倍的资源消耗。

Conclusion: MaxShapley为生成式搜索管道中的公平归因提供了高效解决方案，能够以线性计算复杂度实现与Shapley值相当的归因质量，为内容提供商的公平补偿机制奠定基础。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [101] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 该论文提出使用α-散度优化LLM，解决RL训练导致的多样性损失问题，在定理证明基准上实现覆盖率和精度的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在微调LLM时会导致显著的多样性损失，这是因为RL隐式优化了"模式寻求"的Reverse KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（过滤错误答案但保留正确答案的相对概率），使用α-散度族近似该目标分布，通过插值在模式寻求和质量覆盖散度之间实现精度-多样性权衡的直接控制。

Result: 在Lean定理证明基准上，该方法在覆盖率-精度帕累托前沿上达到最先进性能，在覆盖率轴上优于所有先前方法。

Conclusion: 通过α-散度族显式控制精度-多样性权衡，可以有效解决RL训练导致的多样性损失问题，在定理证明等推理任务中实现更好的性能平衡。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [102] [BalLOT: Balanced $k$-means clustering with optimal transport](https://arxiv.org/abs/2512.05926)
*Wenyan Luo,Dustin G. Mixon*

Main category: stat.ML

TL;DR: BalLOT：一种基于最优传输的平衡k-means聚类方法，通过交替最小化实现快速有效的聚类，具有理论保证和数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决平衡k-means聚类这一基础问题，传统方法在处理平衡约束时存在效率问题，需要开发快速有效的解决方案。

Method: 提出BalLOT方法，将最优传输与交替最小化相结合，通过最优传输来处理平衡约束，实现高效的聚类分配。

Result: 数值实验显示BalLOT快速有效；理论证明：1) 对通用数据产生整数耦合；2) 在随机球模型下实现精确和部分恢复；3) 初始化方案能一步恢复聚类。

Conclusion: BalLOT为平衡k-means聚类提供了理论保证的快速解决方案，结合最优传输和交替最小化，在理论和实践上都有良好表现。

Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.

</details>


### [103] [How to Tame Your LLM: Semantic Collapse in Continuous Systems](https://arxiv.org/abs/2512.05162)
*C. M. Wyss*

Main category: stat.ML

TL;DR: 提出连续状态机理论，将大语言模型形式化为平滑动力系统，证明语义特征定理：转移算子的主导特征函数诱导出有限个谱盆地，解释离散符号语义如何从连续计算中涌现。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型中离散符号语义如何从连续的激活流形中涌现，为语义动力学提供统一的理论框架，连接连续计算与离散逻辑结构。

Method: 将大语言模型形式化为连续状态机（平滑动力系统），定义转移算子P，在紧致性、遍历性、有界雅可比等正则性假设下，证明P是紧算子且有离散谱，建立语义特征定理。

Result: 证明语义特征定理：转移算子的主导特征函数诱导出有限个谱盆地，每个盆地可在实数上的o-极小结构中定义，谱可聚合性与逻辑可驯性一致，连续激活流形坍缩为有限可解释本体。

Conclusion: 为语义动力学提供统一理论框架，解释离散语义从连续计算中的涌现机制，并将结果扩展到随机和绝热（时间非齐次）设置，证明缓慢漂移核保持紧致性、谱相干性和盆地结构。

Abstract: We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,μ) \to L^2(M,μ)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.

</details>


### [104] [One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow](https://arxiv.org/abs/2512.05251)
*Pascal Jutras-Dube,Jiaru Zhang,Ziran Wang,Ruqi Zhang*

Main category: stat.ML

TL;DR: 提出一步扩散采样器，通过状态空间一致性损失学习步长条件ODE，实现一步采样高质量样本，同时引入确定性流重要性权重和体积一致性正则化，实现稳定证据估计。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法通常需要大量迭代步骤才能生成高质量样本，导致计算成本高昂。扩散采样器在少步数情况下标准ELBO估计会退化，因为常见的离散积分器导致前向/后向转移核不匹配。

Method: 1. 学习步长条件ODE，通过状态空间一致性损失使一大步复制多个小步的轨迹；2. 推导确定性流重要性权重用于ELBO估计，无需后向核；3. 引入体积一致性正则化校准DF，对齐不同步长分辨率下沿流的累积体积变化。

Result: 在具有挑战性的合成和贝叶斯基准测试中，该方法以数量级更少的网络评估实现了具有竞争力的样本质量，同时保持了稳健的ELBO估计。

Conclusion: 提出的一步扩散采样器能够在仅一步或少数几步内同时实现高质量采样和稳定证据估计，显著降低了计算成本。

Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.

</details>


### [105] [Symmetric Linear Dynamical Systems are Learnable from Few Observations](https://arxiv.org/abs/2512.05337)
*Minh Vu,Andrey Y. Lokhov,Marc Vuffray*

Main category: stat.ML

TL;DR: 提出一种基于矩估计的新方法，仅需O(log N)个观测即可高精度估计N维随机线性动态系统的对称动态矩阵参数，无论矩阵稀疏与否


<details>
  <summary>Details</summary>
Motivation: 解决从单条轨迹（无论完全观测还是部分观测）学习随机线性动态系统参数的问题，特别是在高维N较大时，传统方法需要大量观测数据

Method: 基于矩估计方法，不依赖问题特定的正则化，能够处理对称动态矩阵的恢复，适用于稀疏和稠密矩阵

Result: 新估计器仅需T=O(log N)个观测即可实现小的最大元素误差，显著优于传统方法，且不依赖矩阵稀疏性假设

Conclusion: 该方法为结构发现等应用提供了高效参数估计工具，特别适用于高维动态系统，仅需对数级观测即可获得精确参数估计

Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.

</details>


### [106] [Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data](https://arxiv.org/abs/2512.05456)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ML

TL;DR: 使用预测数据做推断（IPD）存在统计挑战，高预测准确性不能保证下游推断的有效性，问题可归结为偏差和方差问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML工具更易获取且数据收集面临新障碍（成本上升、调查响应率下降），研究者越来越多地使用预训练算法的预测来替代缺失或未观测数据，但标准推断工具可能导致错误结果。

Method: 分析使用预测数据推断（IPD）的统计挑战，将问题归结为偏差（预测系统性改变估计量或扭曲变量关系）和方差（忽略预测模型不确定性和真实数据内在变异性）两类问题。

Result: 高预测准确性不能保证下游推断的有效性，所有IPD问题都可归结为偏差和方差问题，需要专门方法处理预测数据的不确定性。

Conclusion: 需要开发透明且统计原则的方法来使用预测数据，回顾了现有IPD方法并指出未来研究方向，强调预测数据在科学研究中需要谨慎处理。

Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.

</details>


### [107] [Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches](https://arxiv.org/abs/2512.05611)
*Aurélien Pion,Emmanuel Vazquez*

Main category: stat.ML

TL;DR: 本文研究高斯过程预测分布在插值设置下的校准问题，提出了两种方法：cps-gp和bcr-gp，用于改进预测分布的设计边际校准性能。


<details>
  <summary>Details</summary>
Motivation: 高斯过程预测分布在插值设置下的校准性能需要改进，特别是在设计边际视角下，需要确保预测区间覆盖率和概率校准的准确性。

Method: 提出了两种方法：1) cps-gp：将保形预测系统适应于GP插值，使用标准化留一残差；2) bcr-gp：保留GP后验均值，用拟合交叉验证标准化残差的广义正态模型替换高斯残差，并通过贝叶斯选择规则控制离散度和尾部行为。

Result: 在基准函数上的数值实验表明，cps-gp和bcr-gp在校准指标（覆盖率、Kolmogorov-Smirnov、积分绝对误差）和准确性方面表现良好，优于Jackknife+ for GPs和完全保形高斯过程方法。

Conclusion: cps-gp和bcr-gp方法能够提供具有有限样本边际校准的预测分布，适合顺序设计应用，同时保持平滑的预测分布特性。

Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure μ, we formalize μ-coverage for central intervals and μ-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.

</details>


### [108] [Consequences of Kernel Regularity for Bandit Optimization](https://arxiv.org/abs/2512.05957)
*Madison Lee,Tara Javidi*

Main category: stat.ML

TL;DR: 论文研究了核函数正则性与RKHS函数bandit优化算法性能的关系，通过谱分析统一了全局核方法与局部平滑方法，推导了多种核函数的显式遗憾界，并分析了混合算法LP-GP-UCB。


<details>
  <summary>Details</summary>
Motivation: 传统RKHS方法依赖全局核回归器，而平滑方法利用局部近似，这两种视角之间的关系尚未被充分理解。研究旨在通过核的谱特性揭示这两种方法的深层联系，为bandit优化提供统一分析框架。

Method: 通过分析Matérn、平方指数、有理二次、γ-指数、分段多项式和Dirichlet核的傅里叶谱特性，将谱衰减率与算法性能联系起来。对于核化bandit算法，谱衰减决定了最大信息增益的上界；对于平滑方法，相同的衰减率建立了Hölder空间嵌入和Besov空间范数等价性。还分析了结合两种方法的LP-GP-UCB算法。

Result: 为每个核族推导了显式遗憾界，在某些情况下获得了新结果，在其他情况下提供了改进分析。混合算法LP-GP-UCB虽然不总是优于专门方法，但在多个核族上达到了阶最优性。

Conclusion: 核的谱特性为统一分析全局核方法和局部平滑方法提供了框架，揭示了它们之间的深层联系。混合方法在不同核族上都能达到最优性能，为bandit优化提供了灵活有效的解决方案。

Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Matérn, square-exponential, rational-quadratic, $γ$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish Hölder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [109] [Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction](https://arxiv.org/abs/2512.05508)
*Yash Choudhary,Preeti Rao,Pushpak Bhattacharyya*

Main category: cs.SD

TL;DR: 该研究提出了一种利用LLM提取歌词特征的多模态音乐流行度预测方法，在SpotGenTrack数据集上显著超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有音乐流行度预测研究主要关注音频特征、社交元数据或模型架构，而歌词的作用尚未得到充分探索。准确预测音乐流行度对艺术家、制作人和流媒体平台都有重要价值。

Method: 开发了自动化流水线，使用LLM提取高维歌词嵌入，捕捉语义、句法和序列信息。构建了HitMusicLyricNet多模态架构，整合音频、歌词和社交元数据，预测0-100范围内的流行度分数。

Result: 在包含超过10万首曲目的SpotGenTrack数据集上，方法在MAE和MSE指标上分别比现有基线提升了9%和20%。消融实验证实了LLM驱动的歌词特征流水线（LyricsAENet）的贡献。

Conclusion: 歌词在音乐流行度预测中具有重要价值，LLM提取的密集歌词表示能显著提升预测性能，为音乐产业提供了新的技术方案。

Abstract: Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [110] [On Planar Straight-Line Dominance Drawings](https://arxiv.org/abs/2512.05225)
*Patrizio Angelini,Michael A. Bekos,Giuseppe Di Battista,Fabrizio Frati,Luca Grilli,Giacomo Ortali*

Main category: cs.CG

TL;DR: 研究st-平面图是否存在平面直线支配绘制的问题，发现存在困难，但确定了几个特定类别的st-平面图总是存在这种绘制。


<details>
  <summary>Details</summary>
Motivation: 自90年代以来一直存在的问题：是否每个st-平面图都允许平面直线支配绘制？这个问题在计算几何和图绘制领域具有重要意义。

Method: 通过证明存在性限制来展示问题的难度：1) 具有指定y坐标的平面直线支配绘制不一定存在；2) 不能总是通过收缩-绘制-扩展的归纳方法构建。同时识别了几个总是存在这种绘制的st-平面图类别。

Result: 证明了平面直线支配绘制存在性的困难，但确定了几个总是存在这种绘制的st-平面图类别：1) 每个堆叠操作引入两条进入新顶点边的st-平面3-树；2) 每个顶点都与汇点相邻的st-平面图；3) 没有面左边界是单边的st-平面图；4) 具有跨度最多为2的层次化的st-平面图。

Conclusion: 平面直线支配绘制问题比向上平面直线绘制更困难，但某些特定结构的st-平面图总是存在这种绘制，为理解该问题提供了重要进展。

Abstract: We study the following question, which has been considered since the 90's: Does every $st$-planar graph admit a planar straight-line dominance drawing? We show concrete evidence for the difficulty of this question, by proving that, unlike upward planar straight-line drawings, planar straight-line dominance drawings with prescribed $y$-coordinates do not always exist and planar straight-line dominance drawings cannot always be constructed via a contract-draw-expand inductive approach. We also show several classes of $st$-planar graphs that always admit a planar straight-line dominance drawing. These include $st$-planar $3$-trees in which every stacking operation introduces two edges incoming into the new vertex, $st$-planar graphs in which every vertex is adjacent to the sink, $st$-planar graphs in which no face has the left boundary that is a single edge, and $st$-planar graphs that have a leveling with span at most two.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [111] [Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective](https://arxiv.org/abs/2512.05267)
*Osvaldo Simeone,Yaniv Romano*

Main category: cs.IT

TL;DR: 这篇综述论文探讨了在数据有限场景下的人工智能系统，通过量化认知不确定性和数据增强两种互补方法来应对数据稀缺挑战，采用信息论视角分析数据稀缺的影响。


<details>
  <summary>Details</summary>
Motivation: 在机器人、电信、医疗等特定应用场景中，人工智能系统常面临训练数据有限的问题，导致认知不确定性（可减少的不确定性），这从根本上限制了预测性能。

Method: 论文采用两种互补方法：1）通过广义贝叶斯学习框架量化认知不确定性；2）通过合成数据增强缓解数据稀缺。同时探讨了信息论泛化边界、有限样本统计保证方法（如保形预测）以及结合有限标记数据和丰富模型预测/合成数据的方法。

Result: 论文系统性地回顾了处理数据稀缺问题的多种形式化方法，包括广义贝叶斯学习、信息论泛化边界理论、保形预测等有限样本保证方法，以及数据增强技术，为数据有限场景下的AI系统提供了全面的方法论框架。

Conclusion: 通过信息论视角，论文强调了信息度量在量化数据稀缺影响中的核心作用，为数据有限场景下的AI系统提供了理论和方法论基础，展示了如何通过量化不确定性和数据增强来应对数据稀缺挑战。

Abstract: In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [112] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

TL;DR: 本文提出CoT-Recipe方法，通过调节元训练中CoT和非CoT示例的比例，显著提升LLM在缺乏预训练知识的新任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT提示结合少样本上下文学习能显著提升LLM的推理能力，但在预训练知识不足的新任务上，传统的CoT-ICL方法效果有限。研究发现，元训练中过多包含CoT示例反而会降低性能，特别是在CoT监督有限的情况下。

Method: 提出CoT-Recipe方法，这是一种正式调节元训练序列中CoT和非CoT示例比例的方法。该方法在CoT-ICL Lab框架下进行控制实验，通过精心调节示例组合来优化模型学习新抽象推理任务的能力。

Result: CoT-Recipe方法能显著提升transformer模型在新任务上的准确率，即使上下文没有CoT示例，准确率也能提升高达300%。在预训练LLM（Qwen2.5系列）的符号推理任务上，准确率提升达130%。

Conclusion: 通过CoT-Recipe方法调节元训练中CoT和非CoT示例的比例，能有效提升LLM在缺乏预训练知识的新任务上的推理性能，为解决CoT-ICL在新任务上的局限性提供了有效方案。

Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [113] [LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning](https://arxiv.org/abs/2512.05325)
*Ömer Faruk Akgül,Yusuf Hakan Kalaycı,Rajgopal Kannan,Willie Neiswanger,Viktor Prasanna*

Main category: cs.CL

TL;DR: LYNX是一种在线提前退出机制，利用模型自身隐藏状态意识进行置信度控制的停止决策，在保持准确性的同时大幅减少推理计算量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中经常"过度思考"：在已有足够信息给出正确答案后仍继续推理，浪费推理计算资源并可能损害准确性。现有方法要么需要额外采样和启发式方法，要么依赖辅助验证模型，缺乏正式保证。

Method: LYNX将退出决策附加到自然出现的推理线索（如"hmm"、"wait"）上，在这些线索标记处使用隐藏状态训练轻量级探针，通过强制退出的监督学习，并使用分割共形预测包装得分以获得对提前退出的分布无关控制。

Result: 在1.5B到32B参数的三个模型系列上，单个数学训练的探针产生强大的准确性-效率权衡：GSM8K上减少40-65%标记；MATH-500上准确性提高12点，减少35-60%标记；AIME 2024上恢复基线准确性，节省50%以上标记；非数学任务CommonsenseQA上零样本转移，准确性略有提高，减少70%标记。

Conclusion: LYNX相比最先进的提前退出方法提供竞争性或更优的帕累托前沿，同时保持完全在线，推理时无需代理模型，并提供明确的用户可调置信度保证。

Abstract: Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.

</details>


### [114] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

TL;DR: 该研究针对"粉红泥浆新闻"（低质量自动生成的地方新闻）提出检测方法，发现大语言模型可显著降低现有检测系统性能，并设计了抗攻击的鲁棒学习框架。


<details>
  <summary>Details</summary>
Motivation: 地方新闻作为2800万美国人的重要可靠信息来源，正面临"粉红泥浆新闻"（自动生成的伪地方报道）的威胁。需要精细分析其语言、风格和词汇特征来检测这些欺骗性内容。

Method: 1. 对粉红泥浆内容进行全面研究，揭示其区分性模式；2. 基于这些洞察提出检测策略；3. 特别关注大语言模型作为新的对抗性攻击向量；4. 设计专门抵抗LLM对抗攻击的鲁棒学习框架。

Result: 研究发现：1. 即使是消费者可访问的大语言模型也能显著削弱现有检测系统，使F1分数下降高达40%；2. 提出的鲁棒学习框架能抵抗LLM对抗攻击，性能提升高达27%。

Conclusion: 粉红泥浆新闻检测面临大语言模型的新威胁，需要专门设计的鲁棒学习框架来应对不断演变的自动化粉红泥浆新闻环境，保护地方新闻生态系统的完整性。

Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [115] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: 该论文介绍了MusWikiDB音乐向量数据库和ArtistMus基准测试，用于评估检索增强生成在音乐问答任务中的效果，结果显示RAG显著提升了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在音乐相关推理方面效果有限，因为预训练数据中音乐知识稀疏。虽然音乐信息检索和计算音乐学已经探索了结构化多模态理解，但缺乏基于艺术家元数据或历史背景的事实性和上下文音乐问答资源。

Method: 1. 构建MusWikiDB：包含144K音乐相关维基百科页面的320万段落向量数据库
2. 创建ArtistMus基准：包含500位多样化艺术家的1000个问题，附带流派、出道年份等元数据
3. 使用检索增强生成方法进行音乐问答评估
4. 进行RAG风格微调以提升事实回忆和上下文推理能力

Result: 1. RAG显著提升事实准确性：开源模型提升高达56.8个百分点（如Qwen3 8B从35.0提升到91.8）
2. RAG微调进一步提升事实回忆和上下文推理能力
3. MusWikiDB相比通用维基百科语料库准确率提升约6个百分点，检索速度提升40%
4. 开源模型性能接近专有模型

Conclusion: MusWikiDB和ArtistMus资源为音乐信息检索和领域特定问答研究提供了基础，建立了在音乐等文化丰富领域中检索增强推理的框架，推动了音乐相关人工智能研究的发展。

Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [116] [ProbeWalk: Fast Estimation of Biharmonic Distance on Graphs via Probe-Driven Random Walks](https://arxiv.org/abs/2512.05460)
*Dehong Zheng,Zhongzhi Zhang*

Main category: cs.SI

TL;DR: 本文提出了一种基于探针驱动随机游走的算法，将双调和距离计算的时间复杂度从O(L⁵/ε_abs²)改进到O(L³/ε²)，实现了10-1000倍的加速。


<details>
  <summary>Details</summary>
Motivation: 双调和距离是图上的重要度量，广泛应用于网络中心性、图聚类和机器学习等领域。现有算法计算成本高昂，特别是对于现实世界中L值很大的网络（如L≥10³），需要更高效的算法。

Method: 采用探针驱动随机游走方法，通过相对误差保证而非绝对误差保证来改进算法，因为双调和距离在不同节点对之间变化幅度很大。

Result: 将时间复杂度从O(L⁵/ε_abs²)降低到O(L³/ε²)，在现实网络实验中实现了10-1000倍的每查询加速，并能扩展到数千万节点的图。

Conclusion: 提出的探针驱动随机游走算法显著提高了双调和距离计算的效率，通过降低L的依赖从五次方到三次方，在实际应用中实现了数量级的性能提升。

Abstract: The biharmonic distance is a fundamental metric on graphs that measures the dissimilarity between two nodes, capturing both local and global structures. It has found applications across various fields, including network centrality, graph clustering, and machine learning. These applications typically require efficient evaluation of pairwise biharmonic distances. However, existing algorithms remain computationally expensive. The state-of-the-art method attains an absolute-error guarantee epsilon_abs with time complexity O(L^5 / epsilon_abs^2), where L denotes the truncation length. In this work, we improve the complexity to O(L^3 / epsilon^2) under a relative-error guarantee epsilon via probe-driven random walks. We provide a relative-error guarantee rather than an absolute-error guarantee because biharmonic distances vary by orders of magnitude across node pairs. Since L is often very large in real-world networks (for example, L >= 10^3), reducing the L-dependence from the fifth to the third power yields substantial gains. Extensive experiments on real-world networks show that our method delivers 10x-1000x per-query speedups at matched relative error over strong baselines and scales to graphs with tens of millions of nodes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [117] [Please Don't Kill My Vibe: Empowering Agents with Data Flow Control](https://arxiv.org/abs/2512.05374)
*Charlie Summers,Haneen Mohammed,Eugene Wu*

Main category: cs.CR

TL;DR: 提出为LLM代理系统引入数据流控制机制，以解决代理行为带来的策略违规、流程腐败和安全漏洞风险


<details>
  <summary>Details</summary>
Motivation: LLM代理在执行复杂状态任务时存在重大风险，包括策略违规、流程腐败和安全漏洞，这些风险源于缺乏对代理行为产生的不良数据流的可见性和管理机制。当前代理工作流以临时方式执行这些策略，类似于数据库管理系统出现前应用程序需要自行处理数据验证和访问控制的情况。

Method: 提出数据流控制概念，开发DBMS的可移植DFC实例，并规划更广泛的代理生态系统DFC研究议程。借鉴数据库管理系统将数据验证和访问控制从应用程序层转移到系统层的思路。

Result: 论文描述了DBMS可移植DFC实例的早期开发工作，为代理生态系统中的数据流控制提供了初步实现方案。

Conclusion: 系统应原生支持数据流控制并强制执行DFC策略，就像数据库管理系统将数据验证和访问控制从应用程序层转移到系统层一样，这将使代理开发者从这些担忧中解放出来，推动LLM代理的可靠发展。

Abstract: The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.

</details>


### [118] [Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification](https://arxiv.org/abs/2512.05288)
*Feijiang Han*

Main category: cs.CR

TL;DR: 首次系统研究自动化WebShell家族分类，通过动态函数调用追踪和行为分析，结合LLM生成变体增强数据集，评估多种表示方法建立基准


<details>
  <summary>Details</summary>
Motivation: WebShell恶意软件对医疗、金融等关键基础设施构成重大威胁，当前研究主要停留在被动检测阶段，需要转向深度分析和主动防御。自动化WebShell家族分类能帮助理解攻击者战术并实现精准快速响应，但目前仍依赖缓慢的人工专家分析，缺乏系统研究

Method: 1) 提取动态函数调用追踪以捕获抗加密和混淆的内在行为特征；2) 使用大语言模型合成新变体增强数据集的规模和多样性；3) 将追踪数据抽象为序列、图和树结构；4) 全面评估多种表示方法：经典序列嵌入(CBOW、GloVe)、Transformer模型(BERT、SimCSE)、结构感知算法(图核、图编辑距离、Graph2Vec、各种图神经网络)

Result: 在四个真实世界、家族标注的数据集上进行监督和无监督设置的广泛实验，建立了稳健的基准，并为WebShell家族分类任务提供了数据抽象、表示模型和学习范式的最有效组合的实用见解

Conclusion: 该研究填补了WebShell家族分类自动化领域的空白，通过系统评估多种方法建立了基准，为从被动检测转向主动防御提供了技术基础，有助于理解攻击者战术并实现精准快速响应

Abstract: Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [119] [Robust forecast aggregation via additional queries](https://arxiv.org/abs/2512.05271)
*Rafael Frongillo,Mary Monroe,Eric Neyman,Bo Waggoner*

Main category: cs.GT

TL;DR: 本文提出了一种新的鲁棒预测聚合框架，通过结构化查询从专家处获取更丰富的信息，解决了传统方法中无法超越随机专家性能的限制，实现了最优聚合并建立了准确性与查询复杂度之间的紧致权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统预测聚合方法存在严重的局限性，即使在自然假设下，任何专家预测的聚合都无法超越随机选择专家的性能（Neyman和Roughgarden，2022）。这种不可能性结果促使研究者寻求更强大的聚合框架。

Method: 引入了一个更通用的框架，允许主体通过结构化查询从专家处获取更丰富的信息。该框架确保专家会如实报告其底层信念，并定义了查询难度的复杂度概念。在专家信号独立但重叠的一般模型下进行研究。

Result: 在最坏情况下实现了最优聚合，每个复杂度度量都受限于代理数量n。建立了准确性与查询复杂度之间的紧致权衡：聚合误差随查询数量线性减少，当"推理阶数"和与查询相关的代理数量为ω(√n)时，误差消失。

Conclusion: 适度的专家查询空间扩展显著增强了鲁棒预测聚合的能力，新的查询框架有望在该领域开辟富有成果的研究方向。

Abstract: We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022).
  In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the "order of reasoning" and number of agents relevant to a query is $ω(\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [120] [Designing an Optimal Sensor Network via Minimizing Information Loss](https://arxiv.org/abs/2512.05940)
*Daniel Waxman,Fernando Llorente,Katia Lamer,Petar M. Djurić*

Main category: stat.ME

TL;DR: 提出一种基于物理模拟和贝叶斯实验设计的新型传感器布置方法，用于监测时空过程，通过稀疏变分推理最小化信息损失


<details>
  <summary>Details</summary>
Motivation: 传统实验设计方法很少利用现代计算科学产生的大规模物理模拟数据，需要开发能够整合这些模拟数据并考虑时空维度的传感器布置方法

Method: 提出基于模型的传感器布置准则和高效优化算法，结合物理模拟和贝叶斯实验设计原理，使用稀疏变分推理和可分离高斯-马尔可夫先验

Result: 在凤凰城气温监测案例研究中，该方法优于随机或准随机采样，特别是在传感器数量有限的情况下表现更佳

Conclusion: 该框架为复杂建模工具和实际部署提供了实用考虑和启示，展示了整合物理模拟与实验设计的潜力

Abstract: Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [121] [Bayesian Optimization and Convolutional Neural Networks for Zernike-Based Wavefront Correction in High Harmonic Generation](https://arxiv.org/abs/2512.05127)
*Guilherme Grancho D. Fernandes,Duarte Alexandrino,Eduardo Silva,João Matias,Joaquim Pereira*

Main category: physics.optics

TL;DR: 使用机器学习方法优化高次谐波产生系统中的像差校正，比较贝叶斯优化和卷积神经网络方法，CNN在测试数据上达到80.39%的准确率。


<details>
  <summary>Details</summary>
Motivation: 高次谐波产生（HHG）能够产生可调谐、高能量、相干、超短脉冲的极紫外到软X射线辐射，在凝聚态物理、高能量密度等离子体和阿秒科学中有重要应用。然而，HHG所需的高功率激光系统中的光学像差会降低光束质量和效率，因此需要有效的像差校正方法。

Method: 采用机器学习方法优化空间光调制器的像差校正。实现并比较了两种方法：贝叶斯优化和卷积神经网络（CNN）。CNN用于预测用于波前校正的泽尼克多项式系数的最优值。

Result: CNN方法在测试数据上取得了80.39%的准确率，表现出良好的性能。这表明机器学习方法在高次谐波产生系统的自动像差校正中具有应用潜力。

Conclusion: 机器学习方法，特别是卷积神经网络，能够有效优化高次谐波产生系统中的像差校正，为自动化波前校正提供了有前景的技术途径。

Abstract: High harmonic generation (HHG) is a nonlinear process that enables table-top generation of tunable, high-energy, coherent, ultrashort radiation pulses in the extreme ultraviolet (EUV) to soft X-ray range. These pulses find applications in photoemission spectroscopy in condensed matter physics, pump-probe spectroscopy for high-energy-density plasmas, and attosecond science. However, optical aberrations in the high-power laser systems required for HHG degrade beam quality and reduce efficiency. We present a machine learning approach to optimize aberration correction using a spatial light modulator. We implemented and compared Bayesian optimization and convolutional neural network (CNN) methods to predict optimal Zernike polynomial coefficients for wavefront correction. Our CNN achieved promising results with 80.39% accuracy on test data, demonstrating the potential for automated aberration correction in HHG systems.

</details>


### [122] [FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability](https://arxiv.org/abs/2512.05361)
*Ziheng Guo,Fang Wu,Maoxiong Zhao,Chaoqun Fang,Yang Bu*

Main category: physics.optics

TL;DR: FieldSeer I：一种几何感知的世界模型，用于从2D TE波导中的部分观测预测电磁场动态，支持几何编辑而无需重新同化


<details>
  <summary>Details</summary>
Motivation: 为光子设计开发交互式数字孪生，需要能够从部分观测预测电磁场动态并支持几何编辑的模型

Method: 基于几何感知的世界模型，同化短前缀观测场，以标量源动作和结构/材料图为条件，在物理域生成闭环推演，在对称对数域训练确保数值稳定性

Result: 在可重现的FDTD基准测试（200个独特模拟，按结构划分）中，FieldSeer I在三个实际设置中比GRU和确定性基线获得更高的后缀保真度

Conclusion: 几何条件化的世界模型为光子设计的交互式数字孪生提供了实用路径

Abstract: We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models](https://arxiv.org/abs/2512.05134)
*Zihao Wu*

Main category: cs.CV

TL;DR: InvarDiff是一种无需训练的扩散模型加速方法，通过利用确定性采样中的特征不变性，实现2-3倍的端到端加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量结果，但迭代采样过程导致推理速度缓慢。作者观察到确定性采样中存在特征不变性，希望利用这种不变性来减少冗余计算。

Method: 通过少量确定性运行计算每个时间步、每个层、每个模块的二进制缓存计划矩阵，使用重采样校正避免连续缓存时的漂移。基于分位数的变化度量决定哪些模块在哪些步骤可以重用而非重新计算。同样的不变性准则应用于步骤尺度，实现跨时间步缓存。

Result: 在DiT和FLUX模型上，InvarDiff实现了2-3倍的端到端加速，对标准质量指标影响最小。视觉质量几乎与完整计算没有退化。

Conclusion: InvarDiff通过利用扩散模型中的特征不变性，提供了一种有效的训练免费加速方法，在保持生成质量的同时显著提升推理速度。

Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.

</details>


### [124] [Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models](https://arxiv.org/abs/2512.05139)
*Yang Xiang,Jingwen Zhong,Yige Yan,Petros Koutrakis,Eric Garshick,Meredith Franklin*

Main category: cs.CV

TL;DR: 提出迁移学习生成式降尺度框架，结合轻量U-Net迁移编码器和扩散模型，从50km粗分辨率数据重建7km细分辨率卫星图像，在亚洲区域取得优异性能


<details>
  <summary>Details</summary>
Motivation: 解决从粗分辨率卫星图像重建细分辨率图像的挑战，特别是当高分辨率训练数据有限时，为环境暴露评估和长期环境监测提供改进方案

Method: 结合轻量U-Net迁移编码器和扩散生成模型：1）先在长时间序列粗分辨率数据上预训练U-Net学习时空表征；2）冻结编码器并迁移到更大的降尺度模型中作为物理有意义的潜在特征；3）使用MERRA-2（50km）作为低分辨率源域，GEOS-5 Nature Run（7km）作为高分辨率目标域；4）将亚洲研究区域划分为两个子区域和四个季节进行处理

Result: 模型在季节性区域划分中表现出色（R2 = 0.65到0.94），优于确定性U-Net、变分自编码器和先前的迁移学习基线。通过半变异函数、ACF/PACF和基于滞后的RMSE/R2评估显示，预测的降尺度图像保持了物理一致的空间变异性和时间自相关性，能够在G5NR记录之外实现稳定的自回归重建

Conclusion: 迁移增强的扩散模型为有限训练周期内长时间序列粗分辨率图像的降尺度提供了稳健且物理一致的解决方案，对改进环境暴露评估和长期环境监测具有重要意义

Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.

</details>


### [125] [Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models](https://arxiv.org/abs/2512.05198)
*Rowan Bradbury,Dazhi Zhong*

Main category: cs.CV

TL;DR: 提出像素等效潜在合成(PELC)原则，开发DecFormer实现高质量潜在空间图像合成，解决传统线性插值导致的接缝伪影和颜色偏移问题


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中的潜在修复主要依赖在降采样掩码下的VAE潜在线性插值，这种方法无法实现像素等效合成，导致掩码边缘出现大伪影、全局质量下降和颜色偏移

Method: 提出像素等效潜在合成(PELC)原则，开发DecFormer——一个770万参数的Transformer，预测每通道混合权重和离流形残差校正，实现掩码一致的潜在融合

Result: 在FLUX.1系列上，DecFormer恢复了全局颜色一致性、软掩码支持、锐利边界和高保真掩码，边缘误差指标比标准掩码插值降低达53%；作为修复先验，轻量级LoRA配合DecFormer达到与完全微调的FLUX.1-Fill相当的保真度

Conclusion: PELC是像素等效潜在编辑的通用方法，DecFormer与现有扩散管道兼容，无需主干微调，仅增加0.07%参数和3.5%计算开销，显著提升潜在空间图像合成质量

Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.

</details>


### [126] [PoolNet: Deep Learning for 2D to 3D Video Process Validation](https://arxiv.org/abs/2512.05362)
*Sanchit Kaul,Joseph Luna,Shray Arora*

Main category: cs.CV

TL;DR: PoolNet是一个深度学习框架，用于验证图像数据是否适合SfM处理，能快速区分可处理与不可处理的场景，显著减少传统SfM算法所需时间。


<details>
  <summary>Details</summary>
Motivation: 传统SfM处理序列和非序列图像数据耗时且计算昂贵，且大量公开数据因相机姿态变化不足、场景元素遮挡和噪声数据等问题不适合处理。

Method: 提出PoolNet深度学习框架，进行帧级和场景级的验证，评估野外数据是否适合SfM处理。

Result: 模型成功区分适合SfM处理的场景与不适合的场景，同时显著减少了获取SfM数据所需时间，优于现有算法。

Conclusion: PoolNet提供了一种高效验证图像数据是否适合SfM处理的方法，解决了传统方法耗时和数据质量问题。

Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.

</details>


### [127] [Moving object detection from multi-depth images with an attention-enhanced CNN](https://arxiv.org/abs/2512.05415)
*Masato Shibukawa,Fumi Yoshida,Toshifumi Yanagisawa,Takashi Ito,Hirohisa Kurosaki,Makoto Yoshikawa,Kohki Kamiya,Ji-an Jiang,Wesley Fraser,JJ Kavelaars,Susan Benecchi,Anne Verbiscer,Akira Hatakeyama,Hosei O,Naoya Ozaki*

Main category: cs.CV

TL;DR: 提出一种集成卷积块注意力模块的多输入卷积神经网络，用于太阳系移动天体检测，将人工验证工作量减少99%以上


<details>
  <summary>Details</summary>
Motivation: 太阳系移动天体检测面临的主要挑战是区分真实天体信号与噪声，传统依赖人工验证导致巨大工作量，需要减少人工干预

Method: 提出集成卷积块注意力模块的多输入卷积神经网络，采用多输入架构同时处理多张堆叠图像，注意力模块聚焦空间和通道维度的关键特征

Result: 在约2000张观测图像数据集上评估，准确率达99%，AUC>0.99，通过调整检测阈值可将人工验证工作量减少99%以上

Conclusion: 该方法显著提升了移动天体检测性能，极大减少了人工验证需求，实现了高效可靠的自动化检测

Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.

</details>


### [128] [DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.05515)
*Yuhua Wen,Qifei Li,Yingying Zhou,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CV

TL;DR: DashFusion提出双流对齐与分层瓶颈融合框架，解决多模态情感分析中的对齐和融合问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临对齐和融合两大挑战：对齐需要跨模态的时序和语义同步，融合需要整合对齐后的特征。现有方法往往单独处理这两个问题，导致性能受限。

Method: 提出DashFusion框架：1) 双流对齐模块通过时序对齐（跨模态注意力建立帧级对应）和语义对齐（对比学习确保特征空间一致性）；2) 监督对比学习利用标签信息优化模态特征；3) 分层瓶颈融合通过压缩瓶颈token逐步整合多模态信息。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上达到最先进性能，消融实验验证了对齐和融合技术的有效性。

Conclusion: DashFusion通过统一的框架同时解决对齐和融合问题，在性能和计算效率之间取得平衡，为多模态情感分析提供了有效解决方案。

Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.

</details>


### [129] [InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem](https://arxiv.org/abs/2512.05672)
*Yeobin Hong,Suhyeon Lee,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: InverseCrafter：一种高效的4D视频生成方法，将任务重新定义为潜在空间中的修复问题，避免了对预训练视频扩散模型的微调需求


<details>
  <summary>Details</summary>
Motivation: 现有可控4D视频生成方法通常需要微调预训练视频扩散模型，这计算成本高、需要大规模数据集和架构修改，且容易导致模型遗忘原始生成先验

Method: 提出InverseCrafter，一种高效的修复逆求解器，将4D生成任务重新定义为潜在空间中的修复问题。核心是设计一种机制，将像素空间退化算子编码为连续的多通道潜在掩码，从而绕过重复VAE操作和反向传播的昂贵瓶颈

Result: 在相机控制任务中实现了可比的新视角生成和更优的测量一致性，计算开销接近零；在通用视频修复和编辑方面表现出色

Conclusion: InverseCrafter提供了一种高效、计算成本低的4D视频生成方法，避免了传统微调方法的缺点，在保持生成质量的同时显著降低了计算需求

Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.

</details>


### [130] [Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth](https://arxiv.org/abs/2512.05783)
*Maryam Yousefi,Soodeh Bakhshandeh*

Main category: cs.CV

TL;DR: 提出使用离散拉普拉斯算子进行曲率正则化，在仅有5%深度测量时实现比标准变分自编码器高18.1%的重建精度，挑战了几何深度学习中多约束组合的假设。


<details>
  <summary>Details</summary>
Motivation: 当深度传感器仅提供5%所需测量时，重建完整3D场景变得困难。自动驾驶车辆和机器人无法容忍稀疏重建引入的几何误差。需要一种有效的方法来处理极稀疏的深度数据。

Method: 通过离散拉普拉斯算子进行曲率正则化，该方法提供稳定的梯度和噪声抑制，仅增加15%的训练开销且推理成本为零。挑战了几何深度学习中多约束组合的假设。

Result: 在极稀疏深度数据（5%测量）下，重建精度比标准变分自编码器提高18.1%。单个精心设计的正则化项不仅匹配甚至超过复杂多项公式的效果。

Conclusion: 单个精心设计的正则化项可以超越复杂的多约束组合，离散拉普拉斯算子为稀疏3D重建提供了高效解决方案，代码和模型已开源。

Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.

</details>


### [131] [NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction](https://arxiv.org/abs/2512.05920)
*Jiawen Yang,Yihui Cao,Xuanyu Tian,Yuyao Zhang,Hongjiang Wei*

Main category: cs.CV

TL;DR: NICE使用隐式神经表示进行颅面解剖重建和正颌手术结果预测，通过区域特定的SDF解码器和变形解码器，显著提高了唇部和下巴等关键区域的预测精度。


<details>
  <summary>Details</summary>
Motivation: 正颌手术用于矫正牙颌面骨骼畸形，但术后面部外观预测困难，因为骨骼移动与面部软组织之间存在复杂的非线性相互作用。现有方法要么计算效率低，要么无法充分捕捉这些复杂相互作用。

Method: 提出神经隐式颅面模型(NICE)，包含形状模块和手术模块。形状模块使用区域特定的隐式符号距离函数(SDF)解码器重建面部表面、上颌骨和下颌骨；手术模块使用区域特定的变形解码器，由共享的手术潜在代码驱动，输出点位移场来建模手术结果。

Result: 大量实验表明，NICE优于当前最先进方法，显著提高了唇部和下巴等关键面部区域的预测精度，同时稳健地保持了解剖完整性。

Conclusion: NICE为增强正颌手术规划和患者咨询提供了一个临床可行的工具，能够准确预测术后面部外观。

Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [132] [STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings](https://arxiv.org/abs/2512.05245)
*Mehmet Efe Akça,Gökçe Uludoğan,Arzucan Özgür,İnci M. Baytaş*

Main category: q-bio.BM

TL;DR: STAR-GO是一个基于Transformer的框架，通过联合建模GO术语的语义和结构特征来增强零样本蛋白质功能预测，整合文本定义和本体图结构学习统一的GO表示。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能预测对于阐明分子机制和推进生物与治疗发现至关重要，但实验注释远远落后于蛋白质序列数据的快速增长。现有计算模型通常偏重一种模态（语义或结构），限制了泛化能力，特别是对新出现的GO术语，且随着本体演化，先前训练的模型会过时。

Method: STAR-GO整合GO术语的文本定义和本体图结构来学习统一的GO表示，按层次顺序处理这些表示以从通用术语向特定术语传播信息，然后将这些表示与蛋白质序列嵌入对齐以捕获序列-功能关系。

Result: STAR-GO实现了最先进的性能和优越的零样本泛化能力，证明了整合语义和结构对于稳健且适应性强的蛋白质功能预测的有效性。

Conclusion: STAR-GO展示了联合建模GO术语语义和结构特征对于增强蛋白质功能预测，特别是零样本泛化能力的重要性，为应对GO术语不断演化的挑战提供了解决方案。

Abstract: Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [133] [Over-the-Air Semantic Alignment with Stacked Intelligent Metasurfaces](https://arxiv.org/abs/2512.05657)
*Mario Edoardo Pandolfo,Kyriakos Stylianopoulos,George C. Alexandropoulos,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: 提出首个基于堆叠智能超表面的空中语义对齐框架，直接在波域实现潜在空间对齐，降低设备计算负担


<details>
  <summary>Details</summary>
Motivation: 异构收发器模型产生的潜在表示不匹配会降低语义通信系统性能，现有方法依赖额外数字处理增加设备复杂度

Method: 将SIM建模为可训练线性算子，模拟监督线性对齐器和零样本Parseval框架均衡器，开发基于梯度的优化程序定制超表面传递函数

Result: SIM能准确复现监督和零样本语义均衡器，在高信噪比下达到90%任务准确率，在低信噪比下保持强鲁棒性

Conclusion: SIM框架实现了波域语义对齐，显著降低设备计算复杂度，为异构语义通信系统提供有效解决方案

Abstract: Semantic communication systems aim to transmit task-relevant information between devices capable of artificial intelligence, but their performance can degrade when heterogeneous transmitter-receiver models produce misaligned latent representations. Existing semantic alignment methods typically rely on additional digital processing at the transmitter or receiver, increasing overall device complexity. In this work, we introduce the first over-the-air semantic alignment framework based on stacked intelligent metasurfaces (SIM), which enables latent-space alignment directly in the wave domain, reducing substantially the computational burden at the device level. We model SIMs as trainable linear operators capable of emulating both supervised linear aligners and zero-shot Parseval-frame-based equalizers. To realize these operators physically, we develop a gradient-based optimization procedure that tailors the metasurface transfer function to a desired semantic mapping. Experiments with heterogeneous vision transformer (ViT) encoders show that SIMs can accurately reproduce both supervised and zero-shot semantic equalizers, achieving up to 90% task accuracy in regimes with high signal-to-noise ratio (SNR), while maintaining strong robustness even at low SNR values.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [134] [Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms](https://arxiv.org/abs/2512.05967)
*Francesco Granata,Francesco Poggi,Misael Mongiovì*

Main category: cs.IR

TL;DR: 该研究提出了一种增强的RAG架构，通过集成实体链接的事实信号来提高意大利语教育问答系统的准确性，在特定领域环境中，基于互惠排名融合的混合方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，RAG架构因其能够将语言生成基于可靠知识源而受到关注。然而，仅基于语义相似性的RAG系统在专业领域中常常无法确保事实准确性，因为术语歧义会影响检索相关性。特别是在教育问答系统中，需要提高事实准确性。

Method: 提出增强的RAG架构，集成基于Wikidata的实体链接模块，并实现三种重排序策略来结合语义和实体信息：1) 混合分数加权模型；2) 互惠排名融合；3) 交叉编码器重排序器。在两个基准上进行实验：自定义学术数据集和标准SQuAD-it数据集。

Result: 在特定领域环境中，基于互惠排名融合的混合方案显著优于基线和交叉编码器方法；而在通用领域数据集上，交叉编码器取得最佳结果。这证实了领域不匹配效应的存在。

Conclusion: 研究强调了领域适应和混合排名策略对于增强检索增强生成的事实精度和可靠性的重要性，展示了实体感知RAG系统在教育环境中的潜力，有助于开发自适应且可靠的AI辅导工具。

Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [135] [Machine-learning-enabled interpretation of tribological deformation patterns in large-scale MD data](https://arxiv.org/abs/2512.05818)
*Hendrik J. Ehrich,Marvin C. May,Stefan J. Eder*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种数据驱动的工作流程，利用无监督和监督学习自动解释分子动力学模拟中的高维摩擦变形数据，通过自动编码器压缩图像特征，结合CNN-MLP模型预测变形模式，验证准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟已成为探索原子尺度摩擦变形模式的重要工具，但将高维数据转化为可解释的变形模式图仍然是一个资源密集且主要依赖人工的过程，需要自动化解决方案。

Method: 1. 使用自动编码器将CuNi合金模拟的晶粒取向彩色计算断层图像压缩为32维全局特征向量；2. 将学习到的表征与模拟元数据（成分、载荷、时间、温度、空间位置）结合；3. 训练CNN-MLP模型预测主导变形模式；4. 采用排除整个空间区域的评估策略验证泛化能力。

Result: 1. 即使经过强压缩，重建图像仍保留了晶界、堆垛层错、孪晶和部分晶格旋转等关键微观结构特征；2. 模型在验证数据上达到约96%的预测准确率；3. 排除训练区域评估策略证明了良好的泛化能力。

Conclusion: 该方法证明机器学习可以自动识别和分类结构图像中的关键摩擦变形特征，为构建完全自动化的数据驱动摩擦机制图迈出第一步，最终可能减少大规模分子动力学模拟的需求。

Abstract: Molecular dynamics (MD) simulations have become indispensable for exploring tribological deformation patterns at the atomic scale. However, transforming the resulting high-dimensional data into interpretable deformation pattern maps remains a resource-intensive and largely manual process. In this work, we introduce a data-driven workflow that automates this interpretation step using unsupervised and supervised learning. Grain-orientation-colored computational tomograph pictures obtained from CuNi alloy simulations were first compressed through an autoencoder to a 32-dimensional global feature vector. Despite this strong compression, the reconstructed images retained the essential microstructural motifs: grain boundaries, stacking faults, twins, and partial lattice rotations, while omitting only the finest defects. The learned representations were then combined with simulation metadata (composition, load, time, temperature, and spatial position) to train a CNN-MLP model to predict the dominant deformation pattern. The resulting model achieves a prediction accuracy of approximately 96% on validation data. A refined evaluation strategy, in which an entire spatial region containing distinct grains was excluded from training, provides a more robust measure of generalization. The approach demonstrates that essential tribological deformation signatures can be automatically identified and classified from structural images using Machine Learning. This proof of concept constitutes a first step towards fully automated, data-driven construction of tribological mechanism maps and, ultimately, toward predictive modeling frameworks that may reduce the need for large-scale MD simulation campaigns.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [136] [Continuous-Time Homeostatic Dynamics for Reentrant Inference Models](https://arxiv.org/abs/2512.05158)
*Byung Gyu Chae*

Main category: math.DS

TL;DR: 本文提出快速权重稳态再入网络(FHRN)，将其建模为连续时间神经ODE系统，揭示其作为规范调节的再入动力学过程，通过全局径向稳态实现稳定有界吸引子。


<details>
  <summary>Details</summary>
Motivation: 传统连续时间递归神经网络或液体神经网络通过固定递归或神经元局部时间适应实现稳定性，本文旨在建立一种新的自参照神经动力学类别，通过群体级增益调节实现递归但有界的计算。

Method: 从离散再入规则出发，推导出耦合系统，展示网络如何将快速联想记忆与全局径向稳态耦合。通过雅可比谱分析识别反射机制，证明再入诱导稳定振荡轨迹而非发散或崩溃。

Result: FHRN动力学允许由能量泛函控制的有界吸引子，形成环状流形。在反射机制中，再入产生稳定振荡轨迹。网络通过群体级增益调节而非固定递归或神经元局部时间适应实现稳定性。

Conclusion: 再入网络是一类独特的自参照神经动力学，支持递归但有界的计算，为神经计算提供了新的理论框架。

Abstract: We formulate the Fast-Weights Homeostatic Reentry Network (FHRN) as a continuous-time neural-ODE system, revealing its role as a norm-regulated reentrant dynamical process. Starting from the discrete reentry rule $x_t = x_t^{(\mathrm{ex})} + γ\, W_r\, g(\|y_{t-1}\|)\, y_{t-1}$, we derive the coupled system $\dot{y}=-y+f(W_ry;\,x,\,A)+g_{\mathrm{h}}(y)$ showing that the network couples fast associative memory with global radial homeostasis. The dynamics admit bounded attractors governed by an energy functional, yielding a ring-like manifold. A Jacobian spectral analysis identifies a \emph{reflective regime} in which reentry induces stable oscillatory trajectories rather than divergence or collapse. Unlike continuous-time recurrent neural networks or liquid neural networks, FHRN achieves stability through population-level gain modulation rather than fixed recurrence or neuron-local time adaptation. These results establish the reentry network as a distinct class of self-referential neural dynamics supporting recursive yet bounded computation.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [137] [SSDLabeler: Realistic semi-synthetic data generation for multi-label artifact classification in EEG](https://arxiv.org/abs/2512.05500)
*Taketo Akama,Akima Connelly,Shun Minamikawa,Natalia Polouliakh*

Main category: q-bio.NC

TL;DR: SSDLabeler：通过ICA分解真实EEG、RMS/PSD标准验证伪迹、将多种伪迹类型重新注入干净数据，生成逼真的半合成数据，用于训练多标签伪迹分类器，在原始EEG上取得更好性能。


<details>
  <summary>Details</summary>
Motivation: EEG记录固有地受到眼动、肌肉和环境噪声等伪迹污染，传统ICA方法存在局限性。伪迹分类在稳定性和透明度方面有优势，但需要大量手动标注，且无法覆盖真实EEG的多样性。现有半合成数据方法通常只注入单一伪迹类型或需要单独记录的伪迹信号，缺乏真实感和适用性。

Method: 提出SSDLabeler框架：1）使用ICA分解真实EEG；2）基于RMS和PSD标准进行epoch级伪迹验证；3）将多种伪迹类型重新注入干净数据，生成逼真的标注半合成数据；4）用于训练多标签伪迹分类器。

Result: 相比先前的半合成数据和原始EEG训练方法，SSDLabeler训练的伪迹分类器在多样条件下的原始EEG上取得了更高的准确率，能够捕捉真实EEG中伪迹的共现和复杂性。

Conclusion: SSDLabeler为伪迹处理提供了可扩展的基础，通过生成逼真的半合成数据解决了训练数据限制问题，提高了伪迹分类器在真实EEG场景中的性能。

Abstract: EEG recordings are inherently contaminated by artifacts such as ocular, muscular, and environmental noise, which obscure neural activity and complicate preprocessing. Artifact classification offers advantages in stability and transparency, providing a viable alternative to ICA-based methods that enable flexible use alongside human inspections and across various applications. However, artifact classification is limited by its training data as it requires extensive manual labeling, which cannot fully cover the diversity of real-world EEG. Semi-synthetic data (SSD) methods have been proposed to address this limitation, but prior approaches typically injected single artifact types using ICA components or required separately recorded artifact signals, reducing both the realism of the generated data and the applicability of the method. To overcome these issues, we introduce SSDLabeler, a framework that generates realistic, annotated SSDs by decomposing real EEG with ICA, epoch-level artifact verification using RMS and PSD criteria, and reinjecting multiple artifact types into clean data. When applied to train a multi-label artifact classifier, it improved accuracy on raw EEG across diverse conditions compared to prior SSD and raw EEG training, establishing a scalable foundation for artifact handling that captures the co-occurrence and complexity of real EEG.

</details>


### [138] [Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening](https://arxiv.org/abs/2512.05528)
*Taketo Akama,Zhuohao Zhang,Tsukasa Nagashima,Takagi Yutaka,Shun Minamikawa,Natalia Polouliakh*

Main category: q-bio.NC

TL;DR: 首次使用四通道消费级EEG设备解码自然音乐聆听中的选择性注意力，在真实录音室制作的歌曲中实现跨受试者的音乐注意力解码，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 音乐作为普遍存在的艺术形式，缺乏客观工具来量化自然聆听体验中的注意力和感知焦点。视觉艺术已有眼动追踪研究专家与新手差异，但听觉艺术的类似方法尚未充分发展。

Method: 使用仅四电极的轻量级消费级EEG设备，分析受试者在聆听真实录音室制作歌曲时的神经响应，在最小化参与者负担和保持音乐体验真实性的条件下进行解码。

Result: 音乐注意力不仅能在新歌曲中解码，还能跨新受试者解码，性能优于现有方法。消费级设备能可靠捕捉信号，神经解码在现实场景中具有可行性。

Conclusion: 该方法为教育、个性化音乐技术和治疗干预等应用铺平道路，证明在真实世界条件下使用消费级设备进行音乐注意力解码的可行性。

Abstract: Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [139] [EXR: An Interactive Immersive EHR Visualization in Extended Reality](https://arxiv.org/abs/2512.05438)
*Benoit Marteau,Shaun Q. Y. Tan,Jieru Li,Andrew Hornback,Yishan Zhong,Shaunna Wang,Christian Lowson,Jason Woloff,Joshua M. Pahys,Steven W. Hwang,Coleman Hilton,May D. Wang*

Main category: cs.HC

TL;DR: 开发了一个用于电子健康记录沉浸式可视化的扩展现实平台，将结构化与非结构化患者数据整合到共享3D环境中，支持实时协作和AI增强分析。


<details>
  <summary>Details</summary>
Motivation: 传统2D电子健康记录界面限制了数据探索和临床协作，需要更直观、沉浸式的可视化工具来支持下一代临床决策支持系统。

Method: 构建模块化XR平台，集成FHIR标准的EHR数据、容积医学影像和AI生成的分割结果，使用合成EHR数据集和CT衍生的脊柱模型进行演示。

Result: 成功实现了将患者数据可视化到共享3D环境中的平台，支持直观探索和实时协作，展示了AI增强的医疗数据集成能力。

Conclusion: 这种集成的XR解决方案可以为下一代临床决策支持工具奠定基础，使先进数据基础设施在交互式、空间丰富的环境中直接可访问。

Abstract: This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN结合变分量子电路与Rainbow DQN，在人力资源分配问题上超越经典方法，减少26.8%完工时间


<details>
  <summary>Details</summary>
Motivation: 资源分配是NP难问题，经典深度强化学习方法受限于函数逼近器的表达能力，需要量子计算的优势来提升性能

Method: 提出变分量子Rainbow DQN（VQR-DQN），将环拓扑变分量子电路与Rainbow DQN结合，利用量子叠加和纠缠特性，将人力资源分配问题建模为马尔可夫决策过程

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少26.8%的归一化完工时间，相比Double DQN和经典Rainbow DQN提升4.9-13.4%

Conclusion: 量子增强的深度强化学习在大规模资源分配中具有潜力，电路表达能力、纠缠与策略质量之间存在理论联系

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [141] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型的任务忠实度：语义忠实度（SF）和语义熵产生（SEP）。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，需要新的无监督度量方法来量化模型输出与任务要求的对齐程度。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文到答案的转换。将QCA三元组建模为共享主题的概率分布，使用转移矩阵Q和A分别编码查询目标和实际结果，通过KL散度优化计算语义忠实度。

Result: 提出了SF和SEP两种度量方法，SF通过KL散度量化忠实度，SEP基于热力学原理度量答案生成中的熵产生。高忠实度通常对应低熵产生，两种度量可单独或联合用于LLM评估和幻觉控制。

Conclusion: 该框架为LLM忠实度评估提供了理论基础和实用工具，在SEC 10-K文件摘要任务中展示了应用潜力，有助于改善模型可靠性和减少幻觉。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [142] [KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books](https://arxiv.org/abs/2512.05734)
*Jinfeng Zhong,Emmanuel Bacry,Agathe Guilloux,Jean-François Muzy*

Main category: cs.AI

TL;DR: KANFormer：结合Dilated Causal CNN、Transformer和Kolmogorov-Arnold Networks的深度学习模型，用于预测限价单的成交时间，整合市场级和代理级信息，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅依赖限价订单簿的快照序列，未能有效整合与LOB动态相关的代理行为以及订单在队列中的位置信息，限制了成交概率预测的准确性。

Method: 结合Dilated Causal Convolutional网络和Transformer编码器，通过Kolmogorov-Arnold Networks增强非线性逼近能力，整合市场级和代理级信息（包括LOB动态相关的代理行为和订单队列位置）。

Result: 在CAC 40指数期货数据上评估，KANFormer在校准指标（右删失对数似然、集成Brier分数）和区分指标（C指数、时间依赖性AUC）上均优于现有工作，并通过SHAP分析特征重要性。

Conclusion: 结合丰富的市场信号和表达能力强的神经架构能够实现准确且可解释的成交概率预测，KANFormer展示了这种整合方法的优势。

Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.

</details>


### [143] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署抗干扰雷达，相比进化算法速度提升约7000倍，覆盖效果相当


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有进化算法方法耗时且易陷入局部最优，需要更高效的解决方案

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并设计新的奖励格式

Result: 方法达到与进化算法相当的覆盖效果，同时部署速度提升约7000倍，消融实验验证了FARDA各组件必要性

Conclusion: FARDA框架通过深度强化学习有效解决了雷达快速部署问题，显著提升了部署效率，为抗干扰雷达部署提供了新方案

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [144] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLM只是模式匹配器，无法实现推理"的批评，提出推理的关键在于系统2协调层，而非模式库本身。作者提出了UCCT理论和MACI架构来实现这一协调层。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型能否实现AGI的批评，特别是认为LLM只是"模式匹配器"而无法进行真正推理的观点，作者认为这种批评找错了瓶颈所在。真正的瓶颈不是模式库本身，而是缺乏一个能够选择、约束和绑定这些模式的系统2协调层。

Method: 提出了UCCT（语义锚定理论），将推理建模为由有效支持(ρ_d)、表征不匹配(d_r)和自适应锚定预算(γ log k)控制的相变过程。基于此理论设计了MACI架构，包含三个核心组件：诱饵（行为调节辩论）、过滤（苏格拉底式判断）和持久性（事务性记忆）。

Result: 通过理论框架和架构设计，展示了如何将常见的反对意见重新解释为可测试的协调失败，论证了实现AGI的路径应该通过LLM而非绕过它们。

Conclusion: LLM不是AGI的死胡同，而是实现AGI的必要系统1底层。关键在于添加系统2协调层来选择和约束模式匹配，UCCT理论和MACI架构为实现这一目标提供了具体路径。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [145] [NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM](https://arxiv.org/abs/2512.05224)
*Miguel de Oliveira Guerreiro*

Main category: cs.PL

TL;DR: NVLang是一个静态类型函数式语言，为BEAM虚拟机提供全面的类型安全，通过代数数据类型编码actor消息协议，在编译时强制执行协议一致性，消除消息传递错误。


<details>
  <summary>Details</summary>
Motivation: Erlang/OTP等actor系统虽然可靠，但缺乏消息协议的静态保证，消息在运行时进行模式匹配，协议违规只能在生产环境中发现，导致潜在的系统故障。

Method: 使用代数数据类型编码actor消息协议，每个actor声明其消息词汇表的和类型；引入类型化进程标识符(Pid[T])编码actor期望的协议，以及类型化期货(Future[T])提供类型安全的请求-回复模式；扩展Hindley-Milner类型推断来跟踪消息协议。

Result: NVLang在编译时消除了一整类消息传递错误，同时保持了与动态类型替代方案相媲美的简洁语法；实现编译到Core Erlang，与现有Erlang生态系统无缝互操作；形式化类型系统并提供类型健全性证明草图。

Conclusion: NVLang为BEAM虚拟机带来了全面的类型安全，同时保留了actor模型的简单性和强大功能，通过静态类型系统确保良好类型的程序不会发送违反actor协议的消息。

Abstract: Actor-based systems like Erlang/OTP power critical infrastructure -- from telecommunications to messaging platforms -- handling millions of concurrent connections with legendary reliability. Yet these systems lack static guarantees about message protocols: processes communicate by sending arbitrary messages that pattern-matched at runtime, deferring protocol violations to production failures.
  We present NVLang, a statically typed functional language that brings comprehensive type safety to the BEAM virtual machine while preserving actor model's simplicity and power. NVLang's central contribution that algebraic data types (ADTs) naturally encode actor message protocols: each actor declares the sum type representing its message vocabulary, and the type system enforces protocol conformance at compile time. We introduce typed process identifiers (Pid[T]) that encode the protocol an actor expects, and typed futures (Future[T]) that provide type-safe request-reply patterns.
  By extending Hindley-Milner type inference to track message protocols, NVLang eliminates an entire class of message-passing errors while maintaining clean syntax that rivals dynamically typed alternatives. Our implementation compiles to Core Erlang, enabling seamless interoperability with the existing Erlang ecosystem. We formalize the type system and provide proof sketches for type soundness, demonstrating that well-typed NVLang programs cannot send messages that violate actor protocols.

</details>


### [146] [Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware](https://arxiv.org/abs/2512.05516)
*Pawel K. Radtke,Tobias Weinzierl*

Main category: cs.PL

TL;DR: 评估粒子模拟代码中AoS到SoA转换在不同GPU平台上的性能，结合降精度数据布局，通过编译器注解实现CPU/GPU转换编排


<details>
  <summary>Details</summary>
Motivation: 研究在GPU平台上对粒子模拟代码进行AoS到SoA转换的性能影响，特别是结合降精度数据布局。需要解决转换应该在CPU还是GPU执行，以及数据应该预先传输到加速器还是让加速器按需转换的问题。

Method: 引入编译器注解来促进AoS到SoA转换，并让程序员能够结合GPU卸载来编排这些转换。在不同GPU平台（Nvidia G200和AMD MI300A）上评估粒子模拟代码的性能。

Result: Nvidia G200平台在某些计算内核上获得约2.6倍加速，而AMD MI300A表现出更稳健的性能但收益较小。编译器技术适用于广泛的拉格朗日代码及其他应用。

Conclusion: 通过编译器注解实现的AoS到SoA转换结合降精度布局，在不同GPU平台上都能带来性能提升，为拉格朗日代码优化提供了有效的编译器支持方法。

Abstract: This study evaluates AoS-to-SoA transformations over reduced-precision data layouts for a particle simulation code on several GPU platforms: We hypothesize that SoA fits particularly well to SIMT, while AoS is the preferred storage format for many Lagrangian codes. Reduced-precision (below IEEE accuracy) is an established tool to address bandwidth constraints, although it remains unclear whether AoS and precision conversions should execute on a CPU or be deployed to a GPU if the compute kernel itself must run on an accelerator. On modern superchips where CPUs and GPUs share (logically) one data space, it is also unclear whether it is advantageous to stream data to the accelerator prior to the calculation, or whether we should let the accelerator transform data on demand, i.e.~work in-place logically. We therefore introduce compiler annotations to facilitate such conversions and to give the programmer the option to orchestrate the conversions in combination with GPU offloading. For some of our compute kernels of interest, Nvidia's G200 platforms yield a speedup of around 2.6 while AMD's MI300A exhibits more robust performance yet profits less. We assume that our compiler-based techniques are applicable to a wide variety of Lagrangian codes and beyond.

</details>


### [147] [Compiling Away the Overhead of Race Detection](https://arxiv.org/abs/2512.05555)
*Alexey Paznikov,Andrey Kogutenko,Yaroslav Osipov,Michael Schwarz,Umang Mathur*

Main category: cs.PL

TL;DR: 通过静态编译器分析识别并消除动态数据竞争检测中的冗余检测，显著降低运行时开销，平均加速1.34倍


<details>
  <summary>Details</summary>
Motivation: 动态数据竞争检测器的高运行时开销限制了其应用，主要原因是普遍的内存访问检测中有大量冗余检测

Method: 提出一套过程间静态分析，基于内存访问模式、同步和线程创建来消除可证明无竞争的访问检测；引入基于支配关系的消除分析来识别冗余检测

Result: 在LLVM中实现5种静态分析并与ThreadSanitizer集成，在真实应用中平均加速1.34倍，峰值加速达2.5倍，编译时间增加可忽略

Conclusion: 静态编译器优化能显著降低动态数据竞争检测的开销，已被ThreadSanitizer维护者接受并正在上游化

Abstract: Dynamic data race detectors are indispensable for flagging concurrency errors in software, but their high runtime overhead limits their adoption. This overhead stems primarily from pervasive instrumentation of memory accesses - a significant fraction of which is redundant. We addresses this inefficiency through a static, compiler-integrated approach that identifies and eliminates redundant instrumentation, drastically reducing the runtime cost of dynamic data race detectors. We introduce a suite of interprocedural static analyses reasoning about memory access patterns, synchronization, and thread creation to eliminate instrumentation for provably race-free accesses and show that the completeness properties of the data race detector are preserved. We further observe that many inserted checks flag a race if and only if a preceding check has already flagged an equivalent race for the same memory location - albeit potentially at a different access. We characterize this notion of equivalence and show that, when limiting reporting to at least one representative for each equivalence class, a further class of redundant checks can be eliminated. We identify such accesses using a novel dominance-based elimination analysis. Based on these two insights, we have implemented five static analyses within the LLVM, integrated with the instrumentation pass of the race detector ThreadSanitizer. Our experimental evaluation on a diverse suite of real-world applications demonstrates that our approach significantly reduces race detection overhead, achieving a geomean speedup of 1.34x, with peak speedups reaching 2.5x under high thread contention. This performance is achieved with a negligible increase in compilation time and, being fully automatic, places no additional burden on developers. Our optimizations have been accepted by the ThreadSanitizer maintainers and are in the process of being upstreamed.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [148] [Comparing the latent features of universal machine-learning interatomic potentials](https://arxiv.org/abs/2512.05717)
*Sofiia Chorna,Davide Tisi,Cesare Malosso,Wei Bin How,Michele Ceriotti,Sanggyu Chong*

Main category: physics.chem-ph

TL;DR: 该研究系统分析了不同通用机器学习原子间势（uMLIPs）的潜在特征信息内容，发现不同模型以显著不同的方式编码化学空间，微调后仍保留预训练偏差，并提出通过累积量压缩原子级特征为结构级特征的方法。


<details>
  <summary>Details</summary>
Motivation: 近年来开发的通用机器学习原子间势（uMLIPs）能够在广泛化学结构和组成范围内近似基态势能面，这些模型能够将大量化学信息压缩到描述性潜在特征中。然而，不同uMLIPs学习到的内容尚未得到系统分析。

Method: 使用特征重构误差作为度量标准，定量评估不同uMLIPs潜在特征的相对信息内容，观察训练集和训练协议选择如何影响趋势。同时探讨如何通过渐进累积量的连接将原子级特征压缩为全局结构级特征。

Result: 发现不同uMLIPs以显著不同的方式编码化学空间，存在大量跨模型特征重构误差。相同架构变体的趋势取决于数据集、目标和训练协议。微调uMLIPs时，潜在特征仍保留强烈的预训练偏差。

Conclusion: 不同uMLIPs学习到的化学空间表示存在显著差异，训练策略对特征表示有重要影响。通过累积量压缩原子级特征为结构级特征的方法能够有效捕捉系统内原子环境的变异性信息。

Abstract: The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors as metrics, and observing how the trends are affected by the choice of training set and training protocol. We find that the uMLIPs encode chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.

</details>
