<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.CY](#cs.CY) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.MA](#cs.MA) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CL](#cs.CL) [Total: 6]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations](https://arxiv.org/abs/2508.09238)
*Hyunsung Kim,Hoyoung Choi,Sangwoo Seo,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.DB

TL;DR: ELASTIC是一种仅使用跟踪数据特征的同步框架，解决了足球事件与跟踪数据同步的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足球中事件与跟踪数据的同步因手动记录的时间戳不准确而困难，现有方法依赖易出错的事件位置标注。

Method: 提出ELASTIC框架，仅利用跟踪数据特征，明确检测传球类事件的结束时间，并区分主次要事件。

Result: 在三个Eredivisie比赛的2134个事件上测试，ELASTIC显著优于现有同步器。

Conclusion: ELASTIC通过减少误差传递和提升同步完整性，有效解决了数据同步问题。

Abstract: The integration of event and tracking data has become essential for advanced
analysis in soccer. However, synchronizing these two modalities remains a
significant challenge due to temporal and spatial inaccuracies in manually
recorded event timestamps. Existing synchronizers typically rely on annotated
event locations, which themselves are prone to spatial errors and thus can
distort synchronization results. To address this issue, we propose ELASTIC
(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only
uses features derived from tracking data. ELASTIC also explicitly detects the
end times of pass-like events and separates the detection of major and minor
events, which improves the completeness of the synchronized output and reduces
error cascade across events. We annotated the ground truth timestamps of 2,134
events from three Eredivisie matches to measure the synchronization accuracy,
and the experimental results demonstrate that ELASTIC outperforms existing
synchronizers by a large margin.

</details>


### [2] [LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation](https://arxiv.org/abs/2508.09594)
*Fei Teng,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: LLMLog提出了一种基于多轮标注和自适应上下文学习的框架，用于提高日志模板生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有日志模板生成方法因依赖手工规则或特定训练集模式而准确性低，LLMs在处理复杂日志时也表现不佳。

Method: 提出基于编辑距离的相似性度量，选择最具代表性的未标注日志进行标注，并设计自适应上下文选择策略。

Result: 在16个数据集上的实验表明，LLMLog优于现有方法。

Conclusion: LLMLog通过多轮标注和自适应学习显著提升了日志模板生成的准确性。

Abstract: Modern computing systems, such as HDFS and Spark, produce vast quantities of
logs that developers use for tasks like anomaly detection and error analysis.
To simplify log analysis, template generation methods have been proposed to
standardize log formats, transforming unstructured data into structured
templates. Existing heuristic-based methods and neural network-based methods
suffer from low accuracy problems due to the reliance on handcrafted heuristics
or specific log patterns in training sets. Recently, large language models
(LLMs) have shown great potential in log template generation. However, they
often struggle with ambiguous, complex, or highly specific log content, which
can lead to errors in generating accurate templates. To address these
challenges, we propose LLMLog, a multi-round annotation framework with adaptive
in-context learning. We first propose an edit-distance-based similarity metric
to evaluate log similarity. Then, we introduce a method to select the most
informative $k$ unlabeled logs for annotation by considering both the
representativeness of the logs and the confidence of LLM predictions.
Additionally, we design an adaptive context selection strategy that adaptively
selects labeled logs to ensure comprehensive keyword coverage for unlabeled
logs. These labeled logs serve as the context for LLMs to better understand the
unlabeled logs, thereby enhancing the accuracy of template generation.
Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms
the state-of-the-art approaches.

</details>


### [3] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: 论文提出了一种名为CoDe的新方法，通过覆盖设计和张量分解技术，高效且准确地解决了基数估计问题。


<details>
  <summary>Details</summary>
Motivation: 基数估计是数据库管理中的关键任务，但现有技术要么精度低，要么延迟高，因此需要同时实现高速度和准确性。

Method: CoDe利用覆盖设计将表划分为多个重叠的小段，并通过张量分解建模数据分布，结合创新算法选择最佳分布组合。

Result: 实验表明，CoDe在基数估计中实现了最先进的精度和效率，半数以上查询达到绝对准确。

Conclusion: CoDe通过多模型近似分布，显著提升了基数估计的性能，为数据库系统提供了高效解决方案。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [4] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: 论文提出了一种针对图查询歧义的分类法，并引入AmbiGraph-Eval基准评估LLMs处理歧义查询的能力，发现现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现实中的图查询常存在歧义，而LLMs在处理这类问题时表现不足，需要系统评估和改进。

Method: 提出包含三种主要类型的图查询歧义分类法，并开发AmbiGraph-Eval基准，评估9种代表性LLMs。

Result: 即使顶级LLMs在处理歧义图查询时也表现不佳，揭示了歧义处理的关键缺口。

Conclusion: 研究强调了歧义处理的重要性，并呼吁未来开发专门的解决方案。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: 论文提出了一种静态检测分布式机器学习模型中错误的方法，通过检查模型细化（即能否从分布式模型的输出重建顺序模型的输出），并在GraphGuard中实现。


<details>
  <summary>Details</summary>
Motivation: 由于大型模型需要多GPU分布式训练，程序员在将顺序模型转换为分布式模型时可能引入错误，导致输出不一致。

Method: 使用迭代重写技术证明模型细化，并在GraphGuard中实现该方法。

Result: 方法可扩展到GPT和Llama-3等大型模型，并提供可操作的错误定位输出。

Conclusion: GraphGuard能有效识别分布式模型中的错误，并帮助定位问题。

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [6] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: HierMoE通过拓扑感知技术（令牌去重和专家交换）优化MoE模型的训练，显著提升通信效率和负载均衡。


<details>
  <summary>Details</summary>
Motivation: MoE模型在分布式GPU集群中存在通信和负载不均衡问题，阻碍了扩展性。

Method: 提出HierMoE系统，采用令牌去重减少通信流量，专家交换平衡GPU负载，并建立理论模型优化策略。

Result: 实验显示，HierMoE在通信和端到端训练速度上显著优于现有系统。

Conclusion: HierMoE有效解决了MoE模型的扩展性问题，提升了训练效率。

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


### [7] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: 论文提出了一种针对HPE Slingshot网络的扩展，使其适用于多租户的HPC-Cloud融合部署，基于Kubernetes实现安全、低开销的容器级访问。


<details>
  <summary>Details</summary>
Motivation: HPC-Cloud融合计算需要满足多租户隔离和高性能需求，但现有Slingshot软件栈仅支持单租户模式，无法适应融合部署的安全需求。

Method: 设计并实现了一个基于Kubernetes的Slingshot扩展，提供容器级、多租户的RDMA网络访问能力。

Result: 扩展实现了安全的多租户访问，且性能开销极低。

Conclusion: 该扩展成功解决了Slingshot在多租户环境中的适用性问题，为HPC-Cloud融合部署提供了可行方案。

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [An improved local search based algorithm for $k^-$-star partition](https://arxiv.org/abs/2508.09361)
*Mingyang Gong,Guohui Lin,Brendan Mumey*

Main category: cs.DS

TL;DR: 本文提出了一种改进的近似算法，用于解决$k^-$-star划分问题，时间复杂度为$O(|V|^3)$，近似比为$\frac k2 - \frac {k-2}{8k-14}$。


<details>
  <summary>Details</summary>
Motivation: 研究$k^-$-star划分问题，旨在找到覆盖图中所有顶点的最小顶点不相交星集，每个星最多包含$k$个顶点。

Method: 算法从具有最少1-星的划分开始，通过识别关键顶点，并迭代应用三种局部搜索操作来优化解。

Result: 提出的算法在时间复杂度$O(|V|^3)$内实现了改进的近似比。

Conclusion: 通过关键顶点和局部搜索操作的结合，算法在保证效率的同时提高了近似性能。

Abstract: We study the $k^-$-star partition problem that aims to find a minimum
collection of vertex-disjoint stars, each having at most $k$ vertices to cover
all vertices in a simple undirected graph $G = (V, E)$. Our main contribution
is an improved $O(|V|^3)$-time $(\frac k2 - \frac {k-2}{8k-14})$-approximation
algorithm.
  Our algorithm starts with a $k^-$-star partition with the least $1$-stars and
a key idea is to distinguish critical vertices, each of which is either in a
$2$-star or is the center of a $3$-star in the current solution. Our algorithm
iteratively updates the solution by three local search operations so that the
vertices in each star in the final solution produced cannot be adjacent to too
many critical vertices. We present an amortization scheme to prove the
approximation ratio in which the critical vertices are allowed to receive more
tokens from the optimal solution.

</details>


### [9] [A Classical Quadratic Speedup for Planted $k$XOR](https://arxiv.org/abs/2508.09422)
*Meghal Gupta,William He,Ryan O'Donnell,Noah G. Singer*

Main category: cs.DS

TL;DR: 本文设计了一种新的经典算法，在大型常数k的情况下，比之前的最佳算法快二次方，从而将量子加速从四次方降低到二次方。


<details>
  <summary>Details</summary>
Motivation: 研究量子算法在噪声种植kXOR问题上的优势，并探索经典算法是否能缩小与量子算法的性能差距。

Method: 结合亚线性时间算法（生日悖论）和多项式反集中工具，设计新算法。

Result: 新算法在大型常数k的情况下实现了二次加速，且适用于半随机情况。

Conclusion: 经典算法可以显著缩小与量子算法的性能差距，但仍保留量子算法的空间优势。

Abstract: A recent work of Schmidhuber et al (QIP, SODA, & Phys. Rev. X 2025) exhibited
a quantum algorithm for the noisy planted $k$XOR problem running quartically
faster than all known classical algorithms. In this work, we design a new
classical algorithm that is quadratically faster than the best previous one, in
the case of large constant $k$. Thus for such $k$, the quantum speedup of
Schmidhuber et al. becomes only quadratic (though it retains a space
advantage). Our algorithm, which also works in the semirandom case, combines
tools from sublinear-time algorithms (essentially, the birthday paradox) and
polynomial anticoncentration.

</details>


### [10] [Retroactive Monotonic Priority Queues via Range Searching](https://arxiv.org/abs/2508.09892)
*Lucas Castro,Rosiane de Freitas*

Main category: cs.DS

TL;DR: 论文研究了单调优先队列的限制变体，提出了一种完全回溯的单调优先队列，其操作时间为O(log m + T(m))，并进一步设计了一种操作时间为O(log m log log m)的队列。


<details>
  <summary>Details</summary>
Motivation: 目前完全回溯优先队列的操作时间较高（O(log² m log log m)），而标准和非完全回溯队列仅为O(log m)，因此探索是否能实现O(log m)的完全回溯队列。

Method: 将单调优先队列的最小值查找问题转化为范围搜索问题，并设计基于特定范围搜索数据结构的完全回溯单调优先队列。

Result: 提出了两种完全回溯单调优先队列，操作时间分别为O(log m + T(m))和O(log m log log m)。

Conclusion: 通过将问题转化为范围搜索，实现了更高效的完全回溯单调优先队列，但仍未达到O(log m)的目标。

Abstract: The best known fully retroactive priority queue costs $O(\log^2 m \log \log
m)$ time per operation, where $m$ is the number of operations performed on the
data structure. In contrast, standard (non-retroactive) and partially
retroactive priority queues cost $O(\log m)$ time per operation. So far, it is
unknown whether this $O(\log m)$ bound can be achieved for fully retroactive
priority queues.
  In this work, we study a restricted variant of priority queues known as
monotonic priority queues. We show that finding the minimum in a retroactive
monotonic priority queue is a special case of the range-searching problem. We
design a fully retroactive monotonic priority queue with a cost of $O(\log m +
T(m))$ time per operation, where $T(m)$ is the maximum between the query and
the update time of a specific range-searching data structure with $m$ elements.
Finally, we design a fully retroactive monotonic priority queue that costs
$O(\log m \log \log m)$ time per operation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 研究探讨如何利用大型语言模型（LLMs）通过实时、上下文感知的反馈提升软件工程课程中的代码重构教学。


<details>
  <summary>Details</summary>
Motivation: 代码重构能提升代码质量，但在复杂真实代码库中教学困难，传统方法反馈有限且不一致。

Method: 将LLM辅助重构整合到课程项目中，使用结构化提示帮助学生识别和解决代码异味（如长方法、低内聚）。

Result: 通过学生反馈和代码质量改进分析评估，发现LLMs能弥合理论与实践学习的差距。

Conclusion: LLMs有助于学生更深入理解可维护性和重构原则。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [12] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: PIPLUP是一种新型的统计型日志解析器，挑战了语义型解析器更优的普遍观点，通过数据不敏感参数实现高准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有统计型日志解析器在准确性和通用性上不足，而语义型解析器依赖外部知识，成本高且隐私性差。PIPLUP旨在提供高效、低成本且隐私友好的解决方案。

Method: PIPLUP消除了对常量令牌位置的预设，采用数据不敏感参数，实现“即插即用”的日志解析。

Result: PIPLUP在大型开源日志数据集上表现优异，优于统计型解析器Drain及其变体，并与最佳无监督语义型解析器LUNAR竞争。

Conclusion: PIPLUP是一种简单、高效且实用的日志解析方法，特别适用于成本与隐私敏感的场景。

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [13] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种三阶段方法，通过意图推断和交互式优化提升LLM在无注释代码库中的函数生成性能。


<details>
  <summary>Details</summary>
Motivation: 现实代码库中常缺少显式注释，导致LLM性能下降，需解决无注释时的功能推断问题。

Method: 分三阶段：1) 意图推断，分析代码上下文；2) 交互式优化，开发者参与意图修正；3) 生成目标函数。

Result: 在DevEval和ComplexCodeEval上，方法显著提升LLM性能，相对增益超20%。

Conclusion: 三阶段框架有效解决了无注释代码的意图推断问题，交互优化进一步提升了生成准确性。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [14] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne是一种基于LLM的代理，通过模块化设计将自然语言转换为结构化SRS，生成更准确和结构化的文档。


<details>
  <summary>Details</summary>
Motivation: 手动编写SRS耗时且易产生歧义，现有自动化方法依赖人工分析，LLM方法存在幻觉和可控性不足的问题。

Method: ReqInOne将SRS生成分解为摘要、需求提取和需求分类三个任务，每个任务使用定制提示模板。

Result: ReqInOne生成的SRS比GPT-4方法和初级工程师更准确和结构化，需求分类组件性能优于现有模型。

Conclusion: 模块化设计使ReqInOne在SRS生成中表现优异，需求分类组件达到或超越现有技术。

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [15] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: DeputyDev是一个AI驱动的代码审查助手，旨在解决软件开发中的低效问题，显著减少审查时间。


<details>
  <summary>Details</summary>
Motivation: 代码审查过程低效，包括耗时长、反馈不一致和质量不稳定。TATA 1mg的数据显示PR处理周期长，影响开发效率。

Method: 开发了DeputyDev的PR审查功能，提供自动化、上下文感知的代码审查，并通过双盲A/B实验验证效果。

Result: 实验结果显示，每PR审查时间减少23.09%，每行代码审查时间减少40.13%。

Conclusion: DeputyDev成功提升了审查效率，并已作为SaaS解决方案推广至外部公司。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [16] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: 自闭症个体在ICT领域（如软件开发、测试和网络安全）有显著贡献，但面临职场障碍。研究通过系统综述提出18个成功因素，分为四类，为教育机构、雇主和工具开发者提供包容性建议。


<details>
  <summary>Details</summary>
Motivation: 基于神经多样性运动的伦理框架，研究旨在解决自闭症个体在软件工程领域从教育到职场包容的全面路径缺失问题。

Method: 对30项研究进行系统综述，识别出18个成功因素，分为四类主题。

Result: 提出四类成功因素（软件工程教育、职业培训、工作环境、工具与技术），并给出具体包容性建议。

Conclusion: 研究为提升自闭症个体在软件工程领域的包容性提供了基于证据的建议，强调环境设计的重要性。

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [17] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec是一个结合LLM和RAG技术的框架，用于自动化推荐替代库，并通过上下文学习从提交消息中提取迁移意图以提高准确性。LibEval作为评估基准，包含2,888条迁移记录。实验评估了10种LLM的效果，并分析了框架的关键组件、提示策略和失败案例。


<details>
  <summary>Details</summary>
Motivation: 自动化库迁移推荐的需求，结合LLM和RAG技术提升推荐准确性。

Method: 提出LibRec框架，结合LLM和RAG技术，利用上下文学习提取迁移意图。

Result: 基于LibEval基准，评估了10种LLM的效果，分析了框架组件、提示策略和失败案例。

Conclusion: LibRec框架在库迁移推荐任务中表现出色，结合LLM和RAG技术显著提升了推荐准确性。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [18] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: 论文提出两种基于二分图的新启发式方法（Minimum Coverage和Maximum Coverage），用于近似计算软件项目的bus-factor，显著优于传统方法，并在大规模图上验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: bus-factor是衡量项目风险的重要指标，但精确计算其NP-Hard特性使其难以扩展到大系统，因此需要高效的近似方法。

Method: 将软件项目建模为开发者与任务的二分图，提出两种基于迭代图剥离的启发式方法。

Result: 在1000多个合成图上验证，新方法比传统方法更准确且可扩展到百万级节点图。

Conclusion: 新启发式方法高效、准确且鲁棒，已开源以支持研究和应用。

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [19] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文探讨了使用大型语言模型（LLMs）分类代码审查评论的潜力，发现其性能优于现有的深度学习方法，尤其在低频类别上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督学习的代码评论分类方法需要大量人工标注，限制了其可扩展性。

Method: 利用LLMs对17类代码审查评论进行分类，并与现有深度学习方法对比。

Result: LLMs在分类性能上优于现有方法，尤其在低频类别上表现更佳。

Conclusion: LLMs为代码审查分析提供了可扩展的解决方案，有望提升审查效率。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [20] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: 本文通过实证研究分析了Go语言中CGO的使用情况，揭示了其分布、模式、目的及关键问题，并提出了改进工具链的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管FFI在多语言软件开发中提升效率，但CGO作为Go的新兴FFI，其独特风险尚未被充分研究。

Method: 研究了920个开源Go项目，开发了CGOAnalyzer工具，分析CGO的使用特征和问题。

Result: 发现11.3%的项目使用CGO，识别了4种主要用途、15种模式和19类问题，提出了临时解决方案并提交了工具链改进提案。

Conclusion: 研究为开发者和Go团队提供了宝贵见解，提升了开发效率和工具链的稳健性。

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: WAAN框架通过嵌入轻量级TinyML代理，实现意图感知和主动切换，解决了6G网络中传统切换机制的局限性。


<details>
  <summary>Details</summary>
Motivation: 6G网络中传统反应式切换机制在移动边缘计算和自主代理服务场景中表现不足。

Method: WAAN框架利用TinyML代理作为自主协商实体，跨异构边缘节点实现意图传播和网络适配，并引入半稳定会合点确保连续性。

Result: 通过多模态环境控制案例研究，证明了WAAN在移动性下维持用户体验的有效性。

Conclusion: 文章讨论了WAAN部署和演进中的关键挑战与未来机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [22] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: 论文提出了一种语义感知的主动LLM编排框架（SP-LLM），用于优化动态车载边缘计算环境中的任务卸载和资源分配。


<details>
  <summary>Details</summary>
Motivation: 当前车载边缘计算管理系统在动态环境中表现不佳，因为它们基于静态优化目标和当前网络状态做出反应性决策。

Method: 通过将传统数字孪生（DT）升级为预测性数字孪生（pDT），并结合大型语言模型（LLM）作为认知编排器，实现前瞻性决策。

Result: SP-LLM在可扩展性、鲁棒性和适应性方面显著优于现有方法。

Conclusion: 该框架能够将人类意图转化为最优网络行为，为更智能、自主和目标驱动的车载网络铺平道路。

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [23] [Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs](https://arxiv.org/abs/2508.09150)
*Pietro Piscione,Leonardo Lossi,Maziar Nekovee,Chathura Galkandage,Phil O Connor,Simon Davies*

Main category: cs.NI

TL;DR: 论文展示了一个概念验证（PoC），将5G高级网络功能与通用API框架（CAPIF）集成，以支持汽车应用的增强连接。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过5G和CAPIF的集成，提升汽车应用的网络性能和动态服务质量（QoS）调整能力。

Method: 利用3GPP网络暴露功能（NEF）API，通过CAPIF实现网络性能的持续监控和QoS的动态调整，并将流量重定向至边缘以优化延迟和资源利用。

Result: PoC成功展示了动态QoS调整和流量边缘化的效果，提升了视频流服务的性能。

Conclusion: 该研究证明了5G与CAPIF集成的可行性，为汽车应用的网络优化提供了有效解决方案。

Abstract: This paper presents the design and implementation of a Proof of Concept (PoC)
that demonstrates how 5G Advanced Network Functions can be integrated with the
Common API Framework (CAPIF) to support enhanced connectivity for automotive
applications. The PoC shows the continuous monitoring of the mobile network
performance and the on-demand and dynamic adaptation of Quality of Service
(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using
standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,
traffic flows are redirected to the edge to improve latency and optimize
network resource utilization.

</details>


### [24] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 本文提出了一种基于整数线性规划（ILP）的模型放置算法，用于优化预训练MoE LLM在多服务器集群中的部署，以减少网络流量。


<details>
  <summary>Details</summary>
Motivation: 预训练的MoE LLM在推理阶段仅激活部分专家，导致负载不均衡，因此需要高效的模型放置算法以优化集群利用率。

Method: 提出了一种整数线性规划（ILP）方法，考虑网络拓扑结构，以最小化预期传输次数为目标，优化专家放置。

Result: 实验表明，ILP方法在小规模（DeepSeekMoE~16B）和大规模（DeepSeek-R1~671B）模型上均能减少网络流量。

Conclusion: ILP方法为MoE LLM的高效部署提供了一种有效的解决方案，显著降低了网络传输需求。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [25] [Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission](https://arxiv.org/abs/2508.09151)
*Chang Wu,Yuang Chen,Yiyuan Chen,Fengqian Guo,Xiaowei Qin,Hancheng Lu*

Main category: cs.NI

TL;DR: 论文提出了一种基于生理信号的VR流媒体QoE建模与优化框架，利用EEG、ECG和皮肤活动信号动态调整分辨率，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有QoE模型和传输方案未能充分解决VR流媒体中分辨率突变对用户体验的影响。

Method: 结合生理信号（EEG、ECG、皮肤活动）和深度强化学习（DRL），动态分配无线资源并调整分辨率。

Result: 分辨率提升88.7%，切换减少81.0%。

Conclusion: 生理信号驱动的策略有效提升了VR流媒体的QoE，展示了边缘AI在沉浸式媒体服务中的潜力。

Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly
impair the quality-of-experience (QoE) of users, particularly during
transitions from high to low resolutions. Existing QoE models and transmission
schemes inadequately address the perceptual impact of these shifts. To bridge
this gap, this article proposes, for the first time, an innovative
physiological signal-driven QoE modeling and optimization framework that fully
leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin
activity signals. This framework precisely captures the temporal dynamics of
physiological responses and resolution changes in VR streaming, enabling
accurate quantification of resolution upgrades' benefits and downgrades'
impacts. Integrated the proposed QoE framework into the radio access network
(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission
strategies have been implemented to allocate radio resources dynamically, which
mitigates short-term channel fluctuations and adjusts frame resolution in
response to channel variations caused by user mobility. By prioritizing
long-term resolution while minimizing abrupt transitions, the proposed solution
achieves an 88.7\% improvement in resolution and an 81.0\% reduction in
handover over the baseline. Experimental results demonstrate the effectiveness
of this physiological signal-driven strategy, underscoring the promise of edge
AI in immersive media services.

</details>


### [26] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 本文提出了一种基于AI/ML的故障分析引擎，用于自动分类5G核心网中的PCAP文件故障，显著减少人工分析时间并提高效率。


<details>
  <summary>Details</summary>
Motivation: 随着5G技术的发展，确保核心网流量性能至关重要。传统方法依赖人工分析PCAP文件，耗时且低效。

Method: 利用自然语言处理技术分析网络流量，结合生成式AI（基于LLM）提供故障修复建议，并参考3GPP标准等文档解释错误。

Result: 在80-20的训练测试集划分下，模型对PCAP文件的分类准确率较高。

Conclusion: 该引擎显著提升了故障分析效率，未来可扩展至4G网络和其他数据类型。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [27] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran SRB是一个基于代理的市场，通过三个自治AI分支（立法、行政、司法）协调多方利益，显著提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决下一代移动网络中多方服务所有者目标冲突的问题，克服现有网络切片控制器的僵化性。

Method: 采用三分支AI架构（立法、行政、司法）和代理协商机制，结合多目标优化器达成共识。

Result: 在5G测试中，eMBB切片吞吐量提升37%，URLLC切片延迟降低73%，PRB使用节省8.3%。

Conclusion: Agoran为6G网络提供了一种灵活、以利益相关者为中心的标准路径。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [28] [WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking](https://arxiv.org/abs/2508.09166)
*Wei Guo,Shunsei Yamagishi,Lei Jing*

Main category: cs.NI

TL;DR: WPTrack提出了一种结合Wi-Fi和压力鞋垫数据的单目标跟踪系统，解决了现有Wi-Fi跟踪技术中初始位置获取和方向估计的盲点问题。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展，室内定位对智能家居、行为监测和老年护理至关重要。现有Wi-Fi跟踪技术通常需要多设备支持，而单设备方案存在初始位置获取和方向估计的挑战。

Method: WPTrack通过单Wi-Fi链路采集CSI数据，结合90个鞋垫传感器的压力数据，计算相位差、多普勒速度和步行速度，提出CSI-压力融合模型以实现精确跟踪。

Result: 仿真显示初始定位精度为0.02 cm至42.55 cm，实验数据中的轨迹跟踪结果与实际轨迹高度吻合。

Conclusion: WPTrack通过融合Wi-Fi和压力数据，有效解决了单设备跟踪的挑战，实现了高精度室内定位。

Abstract: As the Internet of Things (IoT) continues to evolve, indoor location has
become a critical element for enabling smart homes, behavioral monitoring, and
elderly care. Existing WiFi-based human tracking solutions typically require
specialized equipment or multiple Wi-Fi links, a limitation in most indoor
settings where only a single pair of Wi-Fi devices is usually available.
However, despite efforts to implement human tracking using one Wi-Fi link,
significant challenges remain, such as difficulties in acquiring initial
positions and blind spots in DFS estimation of tangent direction. To address
these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure
Insoles Fusion System for Single Target Tracking. WPTrack collects Channel
State Information (CSI) from a single Wi-Fi link and pressure data from 90
insole sensors. The phase difference and Doppler velocity are computed from the
CSI, while the pressure sensor data is used to calculate walking velocity.
Then, we propose the CSI-pressure fusion model, integrating CSI and pressure
data to accurately determine initial positions and facilitate precise human
tracking. The simulation results show that the initial position localization
accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results
obtained from experimental data collected in a real-world environment closely
align with the actual trajectory.

</details>


### [29] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: webMCP是一种客户端标准，通过在网页中嵌入结构化交互元数据，显著提升了AI代理理解网页的效率，降低了计算开销和用户成本。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理需要大量处理才能理解网页，导致交互缓慢且昂贵，webMCP旨在解决这一问题。

Method: webMCP直接在网页中嵌入结构化交互元数据，为AI代理提供明确的页面元素与用户动作映射，减少对完整HTML文档的处理。

Result: 在1890个真实API调用中，webMCP将处理需求降低67.6%，任务成功率保持在97.9%，用户成本减少34-63%，响应时间更快。

Conclusion: webMCP无需服务器端修改，可部署于现有网站，显著提升了AI辅助网页交互的效率和可持续性。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [30] [Camel: Energy-Aware LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2508.09173)
*Hao Xu,Long Peng,Shezheng Song,Xiaodong Liu,Ma Jun,Shasha Li,Jie Yu,Xiaoguang Mao*

Main category: cs.NI

TL;DR: 论文提出了一种LLM推理能量管理框架，通过优化GPU频率和批量大小，平衡延迟和能耗，在边缘设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要部署在云端，面临网络延迟、隐私和带宽限制等问题，边缘设备部署成为研究重点，但需解决能耗与延迟的平衡。

Method: 提出框架优化GPU频率和批量大小，通过探索-利用策略搜索最优配置，并在NVIDIA Jetson AGX Orin平台上实现。

Result: 实验表明，框架比默认配置减少12.4%-29.9%的EDP，更好地平衡了能耗与延迟。

Conclusion: 该框架为边缘设备上的LLM推理提供了有效的能耗与延迟优化方案。

Abstract: Most Large Language Models (LLMs) are currently deployed in the cloud, with
users relying on internet connectivity for access. However, this paradigm faces
challenges such as network latency, privacy concerns, and bandwidth limits.
Thus, deploying LLMs on edge devices has become an important research focus. In
edge inference, request latency is critical as high latency can impair
real-time tasks. At the same time, edge devices usually have limited battery
capacity, making energy consumption another major concern. Balancing energy
consumption and inference latency is essential. To address this, we propose an
LLM inference energy management framework that optimizes GPU frequency and
batch size to balance latency and energy consumption. By effectively managing
the exploration-exploitation dilemma in configuration search, the framework
finds the optimal settings. The framework was implemented on the NVIDIA Jetson
AGX Orin platform, and a series of experimental validations were conducted.
Results demonstrate that, compared to the default configuration, our framework
reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance
between energy consumption and latency.

</details>


### [31] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: HiSTM是一种结合双空间编码器和Mamba时序模块的模型，用于蜂窝流量预测，显著提升了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对网络规划和资源分配至关重要，但现有模型在准确性和效率之间存在权衡。

Method: HiSTM采用选择性状态空间方法和注意力机制，捕捉时空模式。

Result: 在真实数据集上，HiSTM比STN基线MAE提升29.4%，参数减少94%。

Conclusion: HiSTM在不同数据集和长时间预测中表现优异。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [32] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: MX-AI是一个端到端的AI系统，用于6G无线接入网络（RAN），通过自然语言意图实现观察和控制功能，性能接近人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 未来6G网络需要AI原生支持，MX-AI旨在通过自主代理实现网络的观察、推理和重新配置。

Method: MX-AI基于OpenAirInterface和FlexRIC构建了一个5G Open RAN测试平台，并在SMO层部署了由大型语言模型（LLM）驱动的代理图。

Result: 在50个实际查询中，MX-AI的平均回答质量为4.1/5.0，决策准确率达100%，端到端延迟仅为8.8秒。

Conclusion: MX-AI验证了AI原生RAN的实用性，并公开了代理图、提示和评估工具以推动开放研究。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [33] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种动态资源感知的协作优化框架CoMoE，用于在移动边缘计算环境中高效部署Mixture-of-Experts（MoE）模型，显著降低内存使用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型在移动边缘环境中的部署面临内存占用大和动态专家激活模式的挑战，需要一种优化框架来解决这些问题。

Method: CoMoE通过联合优化专家聚合粒度和卸载策略，结合实时设备资源状态、网络条件和输入特征，提出自适应调度机制。

Result: 实验表明，CoMoE内存使用减少约70%，推理延迟降低10.5%，并成功将大型MoE模型部署到资源受限设备。

Conclusion: CoMoE为移动边缘环境中的MoE模型部署提供了高效解决方案，显著提升了资源利用率和性能稳定性。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [34] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: 论文提出NEFMind框架，利用高效参数微调的开源大语言模型（LLMs）解决5G服务架构中API管理的复杂性，显著降低通信开销并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现代电信中基于服务的架构导致网络功能（NFs）和API数量激增，服务发现和管理变得复杂。

Method: NEFMind框架包含三个核心组件：从NEF API规范生成合成数据集、通过量化低秩适应优化模型、使用GPT-4 Ref Score和BertScore评估性能。

Result: 实验表明，该方法在5G API管理中通信开销减少85%，Phi-2模型的API调用识别准确率达98-100%。

Conclusion: 验证了高效参数微调的LLM策略在管理下一代电信网络复杂API生态系统中的有效性。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [35] [On-Device Multimodal Federated Learning for Efficient Jamming Detection](https://arxiv.org/abs/2508.09369)
*Ioannis Panitsas,Iason Ofeidis,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了一种基于多模态联邦学习的轻量级框架，用于设备端的干扰检测与分类，显著提升了检测精度并降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 无线网络易受干扰攻击，现有检测方法多为单模态、集中式且资源消耗大，难以扩展和部署。

Method: 采用多模态联邦学习框架，结合频谱图和跨层网络KPI，通过双编码器架构和融合模块实现隐私保护训练与推理。

Result: 检测精度比现有单模态方法高15%，通信轮次减少60%，资源消耗低，在异构数据分布下表现稳健。

Conclusion: 该框架在干扰检测中表现出高效、隐私保护和强鲁棒性，适用于实际部署。

Abstract: Wireless networks face severe vulnerabilities from jamming attacks, which can
significantly disrupt communication. Existing detection approaches are often
unimodal, rely on centralized processing, and demand substantial computational
resources, hindering scalability, efficiency, and deployment feasibility. To
address these challenges, we introduce a multimodal Federated Learning (FL)
framework for on-device jamming detection and classification that integrates
spectrograms with cross-layer network Key Performance Indicators (KPIs) through
a lightweight dual-encoder architecture equipped with a fusion module and a
multimodal projection head. This design enables privacy-preserving training and
inference by ensuring that only model parameters are exchanged, while raw data
remains on the device. The framework is implemented and evaluated on a wireless
experimental testbed using, to the best of our knowledge, the first
over-the-air multimodal dataset with synchronized benign and three distinct
jamming scenarios. Results show that our approach surpasses state-of-the-art
unimodal baselines by up to 15% in detection accuracy, achieves convergence
with 60% fewer communication rounds, and maintains low resource usage. Its
benefits are most evident under heterogeneous data distributions across
devices, where it exhibits strong robustness and reliability.

</details>


### [36] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: 该论文提出了一套量化网络负载和流量影响的指标，通过百分位数和样本分布分析数据，并引入利用率评分指标。采用改进的Shapley值方法评估流量对网络的影响，比较了11种指标，发现其中三种能有效捕捉网络状态变化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决网络性能评估中因流量波动带来的挑战，特别是峰值数据速率对网络资源的影响。

Method: 通过百分位数和样本分布分析链路与流量数据，引入利用率评分指标，并采用改进的Shapley值方法评估流量贡献。

Result: 比较了11种指标，发现其中三种能有效捕捉网络状态变化，且易于维护。

Conclusion: 提出的方法为未来研究提供了框架，可扩展和优化用于评估流量对网络性能影响的指标集。

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [37] [Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment](https://arxiv.org/abs/2508.09582)
*Wafaa B. M. Fadlelmula,Sanaa Hamid Mohamed,Taisir E. H. El-Gorashi,Jaafar M. H. Elmirghani*

Main category: cs.NI

TL;DR: 本文提出了一种基于可见光通信（VLC）的室内雾计算资源连接方案，并通过被动光网络（PON）架构优化资源分配，显著降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 解决室内雾计算资源的高效连接问题，同时降低能耗。

Method: 开发了混合整数线性规划（MILP）模型，优化计算资源分配，并评估了不同工作负载和用户分布下的性能。

Result: 相比现有架构，能耗降低82%；相比集中式云处理，能效提升93%。

Conclusion: 提出的架构显著提升了能效，并通过任务拆分、动态带宽分配和多建筑资源共享进一步优化性能。

Abstract: In this paper, we consider the use of visible light communication (VLC) to
provide connectivity to indoor fog computing resources and propose an
energy-efficient passive optical network (PON)-based backhaul architecture to
support the VLC system. We develop a mixed-integer linear programming (MILP)
model to optimize the allocation of computing resources over the proposed
architecture, aiming to minimize processing and networking power consumption.
We evaluate the performance of the proposed architecture under varying workload
demands and user distributions. Comparative analysis against a backhaul
architecture that is based on the state-of-the-art spine-and-leaf (S&L) network
design demonstrates total power savings of up to 82%. Further comparison with
centralized cloud processing shows improvements in energy efficiency of up to
93%. Additionally, we examine the improvements in energy efficiency obtained by
splitting tasks among multiple processing nodes and propose enhancements to the
architecture including dynamic bandwidth allocation, increased wavelength
bandwidth and improved connectivity within rooms to alleviate networking
bottlenecks. Furthermore, we introduce an inter-building architecture that
leverages resources from neighboring buildings to support high-demand
scenarios.

</details>


### [38] [Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling](https://arxiv.org/abs/2508.09620)
*Michel Rottleuthner,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 论文研究了如何通过动态电压频率调整（DVFS）优化低功耗无线节点的能耗，实验显示节能效果显著。


<details>
  <summary>Details</summary>
Motivation: 受限物联网（IoT）设备的硬件性能差异大，通过降低时钟频率可最小化能耗延迟积（EDP），从而延长电池寿命。

Method: 将DVFS集成到RIOT IoT操作系统中，分析其在常见网络任务中的节能效果，包括CSMA/CA和时分多址（time slotting）两种MAC操作模式。

Result: 实验显示，MAC操作节能24%至52%，加密CoAP通信节能高达37%。

Conclusion: DVFS应被集成到未来IoT设备中，以在最优频率下执行任务，显著延长电池寿命。

Abstract: Minimizing energy consumption of low-power wireless nodes is a persistent
challenge from the constrained Internet of Things (IoT). In this paper, we
start from the observation that constrained IoT devices have largely different
hardware (im-)balances than full-scale machines. We find that the performance
gap between MCU and network throughput on constrained devices enables minimal
energy delay product (EDP) for IoT networking at largely reduced clock
frequencies. We analyze the potentials by integrating dynamic voltage and
frequency scaling (DVFS) into the RIOT IoT operating system and show that the
DVFS reconfiguration overhead stays below the energy saved for a single,
downscaled MAC operation. Backed by these findings, we systematically
investigate how DVFS further improves energy-efficiency for common networking
tasks -- in addition to duty-cycling. We measure IoT communication scenarios
between real-world systems and analyze two MAC operating modes -- CSMA/CA and
time slotting -- in combination with different CoAP transactions, payload
sizes, as well as DTLS transport encryption. Our experiments reveal energy
savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP
communication. These results shall encourage research and system design work to
integrate DVFS in future IoT devices for performing tasks at their optimal
frequencies and thereby significantly extending battery lifetimes.

</details>


### [39] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: ANCHOR是一个无监督异常检测解决方案，用于全球漫游平台的IoT连接服务，旨在通过分析被动信令流量，提前识别潜在问题客户，实现主动问题解决。


<details>
  <summary>Details</summary>
Motivation: IoT服务依赖多实体通信路径，现有平台通常被动应对问题，导致服务质量下降。ANCHOR旨在通过主动检测异常提升服务可靠性。

Method: 结合统计规则、机器学习和深度学习模型，基于被动信令流量设计无监督异常检测方案，并与运营团队合作评估。

Result: 在运营平台上成功部署并评估，能够有效识别潜在问题客户，提升服务可靠性。

Conclusion: ANCHOR为IoT连接服务提供了一种有效的主动异常检测方法，显著改善了服务质量和可靠性。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>


### [40] [Route Planning and Online Routing for Quantum Key Distribution Networks](https://arxiv.org/abs/2508.09735)
*Jorge López,Charalampos Chatzinakis,Marc Cartigny*

Main category: cs.NI

TL;DR: 论文提出了一种基于二次规划（QP）的模型来解决量子密钥分发（QKD）网络中的路由问题，并分析了最短路径和最短最宽路径策略的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的最短路径算法在QKD网络中表现不佳，且资源稀缺导致需求难以满足，需要新的路由解决方案。

Method: 将路由问题建模为二次规划问题，并分析了最短路径和最短最宽路径策略的性能。

Result: 最短路径策略在线路由中表现差，而最短最宽路径策略的竞争比至少为1/2。

Conclusion: 提出的QP模型和最短最宽路径策略能有效解决QKD网络中的路由问题。

Abstract: Quantum Key Distribution (QKD) networks harness the principles of quantum
physics in order to securely transmit cryptographic key material, providing
physical guarantees. These networks require traditional management and
operational components, such as routing information through the network
elements. However, due to the limitations on capacity and the particularities
of information handling in these networks, traditional shortest paths
algorithms for routing perform poorly on both route planning and online
routing, which is counterintuitive. Moreover, due to the scarce resources in
such networks, often the expressed demand cannot be met by any assignment of
routes. To address both the route planning problem and the need for fair
automated suggestions in infeasible cases, we propose to model this problem as
a Quadratic Programming (QP) problem. For the online routing problem, we
showcase that the shortest (available) paths routing strategy performs poorly
in the online setting. Furthermore, we prove that the widest shortest path
routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$,
efficiently addressing both routing modes in QKD networks.

</details>


### [41] [The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges](https://arxiv.org/abs/2508.09756)
*Mauro De Sanctis*

Main category: cs.NI

TL;DR: 本文提出了一种基于异构无线通信信号的“大规模无线人体感知”范式，旨在通过时间和空间域的信号多样性提升感知能力。


<details>
  <summary>Details</summary>
Motivation: 利用无线信号的多样性，提高人体感知的准确性和服务可用性。

Method: 结合设备无关和设备相关的无线感知方法，利用时间、频率和空间域的信号多样性。

Result: 提出了一种多技术、多方法的射频接收器架构，支持特征提取功能。

Conclusion: 讨论了架构解决方案和挑战，为未来该范式的发展提供方向。

Abstract: This article is a position paper which introduces the paradigm of ``Massive
Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing
based on a plethora of heterogeneous wireless communication signals. More
specifically, we aim to exploit signal diversity in the time, frequency, and
space domains using opportunistically both device-free and device-based
wireless sensing approaches, with the objective of enhancing human sensing
capabilities in terms of accuracy and service availability over different
environments. The enabling element of this concept is the massive wireless
human sensing edge device, that is, an embedded system acting as a
multi-technology and multi-approach RF receiver with feature extraction
functionality, located within the monitoring area or at its borders. In this
framework, architecture solutions and challenges are discussed to lead the
future development of this new paradigm.

</details>


### [42] [An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks](https://arxiv.org/abs/2508.09769)
*Simon Egger,Robin Laidig,Heiko Geppert,Lucas Haug,Jona Herrmann,Frank Dürr,Christian Becker*

Main category: cs.NI

TL;DR: 论文提出了一种(m,k)-firm Elevation Policy，用于在不稳定网络条件下维持弱硬实时保证，增强5G-TSN网络的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 5G与TSN的集成在实时通信中存在延迟特性与理想模型不匹配的问题，影响实时保证的鲁棒性。

Method: 通过动态优先级驱动方案，提升m/k连续延迟帧的优先级，作为主时间驱动调度的补充。

Result: 评估表明，该策略在维持控制质量的同时，仅引入少量开销。

Conclusion: (m,k)-firm Elevation Policy是一种轻量级后备机制，能在不稳定网络条件下提供有意义的实时保证。

Abstract: Current standardization efforts are advancing the integration of 5G and
Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical
industrial applications that require real-time communication. However, there
remains a fundamental disconnect between the probabilistic 5G delay
characteristics and the often idealistic delay models used to synthesize 5G-TSN
network configurations. For time-driven schedules in particular, any delay
outlier unforeseen during schedule synthesis can jeopardize the robustness of
their real-time guarantees. To address this challenge, we present the
(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time
guarantees during unstable network conditions that do not match the expected
delay characteristics. It augments the primary time-driven schedule with a
dynamic priority-driven scheme to elevate the priority of m out of k
consecutive frames if they are delayed. Our evaluations demonstrate that weakly
hard real-time guarantees are essential to uphold the quality of control within
a networked control system. At the same time, only a small overhead is imposed
when the primary schedule can provide stronger quality of service guarantees.
Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight
fallback mechanism to serve applications with meaningful guarantees during
unstable network conditions.

</details>


### [43] [A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study](https://arxiv.org/abs/2508.09839)
*Muhammad Asad Ullah,Luca Borgianni,Heikki Kokkinen,Antti Anttonen,Stefano Giordano*

Main category: cs.NI

TL;DR: 本文通过实测分析了Starlink在飞行中的性能，填补了相关研究的空白，重点关注吞吐量和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着航空公司开始提供Starlink机上互联网服务，需要深入评估其性能以满足航空用户需求。

Method: 在波罗的海和太平洋上空进行飞行实测，测量吞吐量和延迟，并分析影响因素。

Result: 单用户设备的下行和上行中位数吞吐量分别为64 Mbps和24 Mbps；高度影响性能，下降阶段上行吞吐量降至20 Mbps；RTT受地面站位置和星间链路影响。

Conclusion: Starlink在飞行中表现稳定，但性能受高度和链路因素影响，需进一步优化。

Abstract: Starlink delivers Internet services to users across terrestrial, maritime,
and aviation domains. The prior works have studied its performance at fixed
sites and in-motion vehicles, while an in-depth analysis of in-flight
performance remains absent. With major airlines now offering Starlink Internet
onboard, there is a growing need to evaluate and improve its performance for
aviation users. This paper addresses this shortcoming by conducting in-flight
measurements over the Baltic Sea and the Pacific Ocean. Our measurement results
show that a single user device experiences median throughputs of 64 Mbps and 24
Mbps for the downlink and uplink, respectively. The median uplink throughput is
approximately 33 Mbps when the aircraft maintains an altitude above 17,000
feet. However, a significant reduction in uplink performance is observed during
the aircraft descent phase, with the median throughput dropping to around 20
Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the
location of the ground station being pinged and the use of inter-satellite
links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over
the Pacific Ocean and investigate factors influencing RTT, hypothesizing that
ISLs routing, data queuing at satellites, and feeder link congestion contribute
to deviations from theoretical values. For comparative analysis, we evaluate
the Starlink ground terminal and in-flight connectivity performance from the
perspectives of a residential user and an airline passenger, respectively.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于特征标记化的Transformer模型，用于实时预测飞机到达时间（ETA），在准确性和效率上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 实时ETA预测对航空到达管理至关重要，尤其是在跑道排序中，需要高效率和准确性。

Method: 利用特征标记化将原始输入映射到潜在空间，并通过Transformer的多头自注意力机制捕捉关键信息，避免了复杂的特征工程。模型支持并行计算，每秒更新一次ETA预测。

Result: 在新加坡樟宜机场的实验中，该方法比XGBoost模型准确率提高7%，计算时间仅需39%，且单次推理时间仅51.7微秒。

Conclusion: 该方法高效且准确，适用于实时到达管理系统。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [45] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本文整合了15个数据集，构建了一个包含2510名受试者的大型糖尿病数据库，用于研究低血糖事件，并分析了数据质量与血糖与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决糖尿病研究中大型数据集缺乏的问题，以提升低血糖预测和管理的准确性。

Method: 方法包括系统整合15个数据集，构建两个子数据库（人口统计数据和心率数据），并进行数据质量评估与相关性分析。

Result: 结果包括构建了一个包含149百万次测量的数据库，发现数据不平衡和缺失值是主要挑战，并揭示了血糖与心率在低血糖前15至55分钟的相关性。

Conclusion: 结论是该数据库为糖尿病研究提供了重要资源，同时数据质量和相关性分析为未来研究提供了方向。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [46] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN框架通过模态感知的动态噪声编辑，细粒度地抑制噪声并保留关键信息，MoLAN+在情感分析中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感分析中因无关或误导性信息导致的问题，避免传统方法因全局处理而丢失关键信息。

Method: 提出MoLAN框架，将模态特征分块并动态分配去噪强度，MoLAN+基于此框架实现。

Result: 在多个模型和数据集上验证了MoLAN的广泛有效性，MoLAN+达到最优性能。

Conclusion: MoLAN框架灵活且高效，MoLAN+在多模态情感分析中表现卓越。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [47] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM transformer的上下文学习（ICL）理论，用于优化WiFi 7中的信道访问。通过预收集碰撞阈值数据示例和查询碰撞案例，设计了一个transformer-based ICL优化器，生成预测的竞争窗口阈值（CWT）。实验表明，该方法在未知节点密度下具有快速收敛和接近最优的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的二进制指数退避方案在动态信道环境下性能较差，而基于模型的优化方法因节点密度估计不准确导致吞吐量损失。本文旨在解决这一问题。

Method: 设计了一个transformer-based ICL优化器，通过预收集碰撞阈值数据示例和查询碰撞案例作为输入提示，生成预测的CWT。并开发了高效的训练算法。

Result: 实验证明，该方法在未知节点密度下能快速收敛，且吞吐量接近最优，优于现有基于模型和深度强化学习的方法。

Conclusion: 提出的transformer-based ICL优化器在动态信道环境下显著提升了吞吐量性能，且对输入数据的容错性强。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [48] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 论文提出了一种有限选择性预测模型（PLS），研究预测误差的最优解，并引入复杂度度量来提供实例依赖的误差界限。


<details>
  <summary>Details</summary>
Motivation: 研究在预测窗口受限的情况下，如何优化预测误差，弥补现有方法对任意时间预测的依赖。

Method: 提出PLS模型，通过实例分析和平均案例分析研究最优预测误差，并引入复杂度度量。

Result: 复杂度度量提供了实例依赖的误差界限，且在随机生成的PLS实例中高概率匹配。

Conclusion: PLS模型为有限选择性预测提供了理论支持，复杂度度量是有效的工具。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [49] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一个2.6B参数的基础模型，旨在平衡高性能与计算效率，通过创新的架构改进（如Differential Attention和PolyNorm激活函数）提升长文本理解、减少幻觉并增强上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决新兴研究团队在开发高性能且计算高效的基础LLM时的挑战，推动先进LLM技术的普及。

Method: 引入创新的架构组件（如Differential Attention和PolyNorm），并通过大量实验验证最优架构。

Result: Motif-2.6B在多项基准测试中表现优于或媲美同类先进模型，展示了其高效性、可扩展性和实际应用价值。

Conclusion: Motif-2.6B为高效、可扩展且强大的基础LLM提供了重要进展，为未来研究和部署奠定了坚实基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [50] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 研究表明，简单密集层可以替代复杂序列混合器，在时间序列分析中表现相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 质疑复杂序列混合器（如注意力机制）的必要性，探索其性能是否源于其他因素。

Method: 提出JustDense，用密集层替换序列混合器，基于MatrixMixer框架进行实验。

Result: 在29个基准测试中，密集层表现与复杂混合器相当或更优。

Conclusion: 挑战了“更深更复杂架构必然更好”的假设，简化模型可能更有效。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [51] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI是一种深度学习框架，通过I-G变换和2SRI技术解决社交网络中同伴效应的反馈和未观测混杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时处理同伴效应的反馈和未观测混杂，导致估计不准确。

Method: 结合I-G变换和2SRI技术，分两阶段训练神经网络，利用对抗性去偏消除混杂信号。

Result: DIG2RSI在半合成和真实数据集上优于现有方法，证明了其有效性。

Conclusion: DIG2RSI能准确估计同伴效应，解决了反馈和混杂问题。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [52] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: AdaPO是一种在线强化学习框架，通过自适应调整训练目标和动态奖励机制，解决了多目标优化中的奖励黑客问题，显著提升了模型的推理和自我评估能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在多轮对话中缺乏自我评估能力，现有强化学习方法因固定奖励机制易导致奖励黑客和模型崩溃。

Method: 提出AdaPO框架，包含自适应奖励模型（ARM）和奖励感知动态KL正则化机制，实时调整训练目标和奖励策略。

Result: 在8个基准测试和多种模型上的实验表明，AdaPO显著提升了直接推理和自我评估能力。

Conclusion: AdaPO通过自适应机制有效解决了奖励黑客问题，为LMMs的自我改进提供了新方向。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [53] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出了一种微调流匹配生成模型的框架，用于强制执行物理约束并解决科学系统中的逆问题。


<details>
  <summary>Details</summary>
Motivation: 解决低保真或观测数据训练的生成模型在物理一致性上的不足，同时处理逆问题中的未知物理输入。

Method: 采用可微分的后训练程序，最小化控制偏微分方程的弱形式残差，并引入可学习的潜在参数预测器进行联合优化。

Result: 模型能够生成物理有效的场解，并准确估计隐藏参数，验证了在PDE约束和参数恢复上的改进。

Conclusion: 该方法将生成建模与科学推断结合，为物理系统的仿真增强发现和数据高效建模开辟了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [54] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出了一种多目标强化学习框架，通过对抗优化实现轨迹生成与评估的闭环协同进化，解决了当前方法中迭代优化不足和标量化偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动驾驶决策中无法实现类似人类的迭代优化，且多目标偏好被简化为标量奖励，导致关键权衡丢失。

Method: EvaDrive采用分层生成器和可训练的多目标评估器，通过对抗游戏实现多轮迭代优化，并结合自回归意图建模和扩散模型提升时空灵活性。

Result: 在NAVSIM和Bench2Drive基准测试中表现优异，分别达到94.9 PDMS和64.96 Driving Score，优于现有方法。

Conclusion: EvaDrive提供了一种无标量化的轨迹优化方法，通过动态权重生成多样化驾驶风格，实现了类人的闭环迭代决策。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [55] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习和物理模型的Physics-Guided Memory Network (PgMN)，用于解决建筑能耗预测中历史数据不足或无数据的问题。


<details>
  <summary>Details</summary>
Motivation: 建筑能耗预测对资源管理和可持续发展至关重要，但深度学习依赖历史数据，而物理模型需要大量参数和时间。PgMN旨在结合两者优势。

Method: PgMN包含并行投影层处理不完整输入、记忆单元处理持续偏差、记忆经验模块扩展预测范围。

Result: PgMN在短期能耗预测中表现优异，适用于新建筑、数据缺失、稀疏历史数据等场景。

Conclusion: PgMN为动态建筑环境中的能耗预测提供了有效解决方案，扩展了模型在数据不足或无数据场景的适用性。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [56] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 论文提出了一种基于自编码器和定制化windowSHAP算法的无监督可解释AI框架，用于实时检测和表征核反应堆中的重放攻击。


<details>
  <summary>Details</summary>
Motivation: 新一代核反应堆依赖数字化系统，数据完整性对安全至关重要。现有方法依赖合成数据或线性模型，无法满足实际需求。

Method: 结合自编码器和windowSHAP算法，开发无监督可解释AI框架，检测并表征重放攻击。

Result: 在PUR-1核反应堆的真实数据测试中，框架检测和表征重放攻击的准确率达95%以上。

Conclusion: 该框架为核反应堆数据安全提供了高效且可解释的解决方案。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [57] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为ASL的新方案，将混合精度概念应用于随机计算神经网络，通过理论模型和实验验证，显著降低了能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）在资源受限场景（如物联网）中是一种高效低功耗的神经网络实现方式，但混合精度实现的进一步优化尚未探索。

Method: 引入ASL方案，基于算子范数理论模型分析截断噪声的累积传播，并通过随机森林回归进行敏感性分析。提出粗粒度和细粒度两种截断策略。

Result: 在32nm工艺下合成的SC MLP上，ASL方案可降低60%以上的能耗和延迟，且精度损失可忽略。

Conclusion: ASL方案在物联网应用中具有可行性，并突显了混合精度截断在SC设计中的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [58] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的人口合成方法，用于估计人口的联合分布，能够恢复缺失的采样零值并最小化结构零值，优于VAE和GAN方法。


<details>
  <summary>Details</summary>
Motivation: 人口合成是代理建模（ABM）中的关键任务，但高维属性导致调查数据稀疏，难以准确建模。现有深度生成模型能生成采样零值，但会引入结构零值。

Method: 采用扩散模型估计人口的联合分布，旨在恢复采样零值并减少结构零值。

Result: 实验表明，该方法在边际分布相似性、可行性和多样性方面优于VAE和GAN方法。

Conclusion: 提出的扩散模型方法在人口合成中实现了可行性与多样性的更好平衡。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [59] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过自适应块缺失表示学习，解决了不同ECG布局导致的信号异步和部分缺失问题，显著提升了心律失常识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 不同医院使用的ECG布局差异导致信号异步和部分缺失，现有模型难以处理。

Method: 提出PatchECG框架，基于掩码训练策略，自适应学习关键块，并利用导联间的协作依赖关系。

Result: 在PTB-XL数据集和真实ECG数据上表现优异，AUROC达0.835，且在外部验证中诊断房颤的AUROC为0.778。

Conclusion: PatchECG在多种ECG布局下表现稳定，优于经典方法和当前最优模型，具有临床应用潜力。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [60] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 论文介绍了SVG-1M数据集和SVGen模型，用于从自然语言生成SVG代码，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将创意转化为精确的矢量图形耗时，需高效工具支持。

Method: 构建SVG-1M数据集，提出SVGen模型，结合课程学习和强化学习优化。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVGen为矢量图形生成提供了高效解决方案，数据集和模型已开源。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [61] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态映射，解决预训练大型多模态模型（LMMs）中的模态鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 预训练和微调成本高昂，且LMMs存在模态鸿沟（文本与视觉表征不对齐），传统方法需要大量领域数据，不实用。

Method: 采用线性映射的RAG方法，高效计算跨模态映射；推理时通过映射检索训练集中的文本描述，结合指令生成新描述；引入迭代技术优化映射。

Result: 在两个基准多模态数据集上实验结果显示显著改进。

Conclusion: 提出的方法有效解决了模态鸿沟问题，且无需昂贵训练，具有实用性。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [62] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP是一种针对非独立同分布（non-IID）数据的联邦学习方法，通过随机特征流形补全和类原型对齐提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID数据（如医学影像）中因特征分布差异导致的模型收敛和性能问题。

Method: 使用随机特征流形补全和类原型对齐，优化客户端间的特征流形一致性。

Result: 在医学影像和多域自然图像数据集上表现优于现有联邦学习算法。

Conclusion: FedMP有效提升了非IID数据下的联邦学习性能，并分析了流形维度、通信效率和隐私影响。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [63] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: 本文提出了一种动态量化训练（DQT）框架，通过嵌套整数表示和自定义整数运算，实现了高效的动态量化，避免了传统方法的浮点运算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有动态量化方法需要昂贵的浮点运算来切换精度，破坏了整数硬件范式并限制了性能提升。

Method: DQT采用嵌套整数表示和整数运算，通过低成本位移操作实现动态精度切换。

Result: 在ImageNet上，4位动态ResNet50达到77.00% top-1准确率，优于现有静态和动态方法。

Conclusion: DQT为高效自适应AI开辟了新方向，显著降低了计算成本。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [64] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: 提出了一种名为scAGC的单细胞聚类方法，通过自适应学习细胞图和对比学习优化特征表示，解决了传统方法在高维和稀疏数据中的挑战。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的高维性和稀疏性导致传统聚类方法效果不佳，现有图神经网络方法依赖静态图结构且对噪声敏感，无法捕捉长尾分布。

Method: scAGC结合拓扑自适应图自编码器和可微Gumbel-Softmax采样策略，动态优化图结构，并引入ZINB损失和对比学习目标以提高鲁棒性和稳定性。

Result: 在9个真实数据集上，scAGC在NMI和ARI指标上均优于其他方法，表现最佳。

Conclusion: scAGC通过自适应图学习和对比指导，显著提升了单细胞聚类性能，为相关研究提供了有效工具。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [65] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于诚实拍卖的长期客户选择联邦学习方案（LCSFLA），以解决非独立同分布数据和资源浪费问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非独立同分布数据和资源浪费问题影响模型收敛和准确性，传统客户选择方法存在信息不对称和低效问题。

Method: 提出LCSFLA方案，结合长期数据质量评估和拍卖机制，激励客户参与并确保信息真实性。

Result: 实验证明LCSFLA能有效缓解非独立同分布数据导致的性能下降。

Conclusion: LCSFLA通过长期评估和拍卖机制，解决了联邦学习中的客户选择问题，提升了模型性能。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [66] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 综述探讨了呼吸分析的接触式与非接触式方法，重点介绍了机器学习和深度学习的最新进展，并分析了其应用、挑战及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸监测方法在舒适性和实用性上存在局限，需要探索更高效的非接触式技术。

Method: 比较接触式与非接触式方法，分析机器学习/深度学习在呼吸分析中的应用，包括数据预处理、特征提取和分类技术。

Result: 非接触式方法（如Wi-Fi CSI和声学传感）能实现高精度无创监测，适用于多用户场景和疾病检测。

Conclusion: 综述为呼吸分析的未来创新提供了框架，强调需解决数据稀缺、隐私等问题，并探索新兴技术如联邦学习和可解释AI。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [67] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种细粒度安全神经元（FGSN）方法，通过无训练持续投影技术减少微调LLM的安全风险，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能破坏原始对齐机制并引入安全风险，现有防御方法未能全面考虑安全层与细粒度神经元的交互。

Method: 提出FGSN方法，结合多尺度安全层与神经元交互，定位稀疏且精确的安全神经元，并通过投影技术提升安全性。

Result: 实验表明，FGSN显著降低有害性评分和攻击成功率，且仅需少量参数修改。

Conclusion: FGSN通过任务特异性多维度优化机制，实现了持续防御能力，并能应对未知安全威胁。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [68] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一个基于LLM的框架，通过语言符号表示实现上下文感知的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 解决历史数值序列与上下文特征（如非结构化文本数据）融合的挑战，提升预测准确性。

Method: 使用离散标记器将连续数值序列转换为时间标记，通过预训练LLM嵌入共享表示空间，并优化生成目标。

Result: 在多样化真实数据集上的实验验证了TokenCast的有效性和泛化能力。

Conclusion: TokenCast成功整合了数值和文本数据，为时间序列预测提供了新思路。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [69] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出了一种名为离散扩散强迫（D2F）的策略，通过改造扩散大语言模型（dLLMs）为自回归-扩散混合范式，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的开源dLLMs在推理速度上未能超越类似规模的自回归（AR）LLMs，因此需要一种更高效的解码方法。

Method: D2F通过块级自回归生成和跨块并行解码，结合非对称蒸馏和流水线并行解码算法，实现了高效推理。

Result: D2F dLLMs在GSM8K上的推理速度比LLaMA3和Qwen2.5快2.5倍以上，比LLaDA和Dream快50倍以上，同时保持输出质量。

Conclusion: D2F是一种简单有效的策略，成功突破了dLLMs推理速度的瓶颈，为高效文本生成提供了新思路。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [70] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL是一种多目标表示学习方法，通过结合句子嵌入和多标签分类网络，提升了生成模型对复杂文本指令的响应能力，实验显示可控性提升了13.8%。


<details>
  <summary>Details</summary>
Motivation: 现有IPCGRL方法难以充分利用文本输入的丰富表达力，尤其在多目标指令下可控性受限。

Method: 提出MIPCGRL，结合句子嵌入和多标签分类网络，训练多目标嵌入空间。

Result: 实验结果表明，MIPCGRL在多目标指令下的可控性提升了13.8%。

Conclusion: MIPCGRL能够处理复杂指令，实现更具表达力和灵活性的内容生成。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [71] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的框架，用于在分散式系统中自动选择最优的推理加速方法，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模模型（如LLMs）部署成本高，分散式系统可解决可扩展性和数据安全问题，但需高效推理加速方案。

Method: 通过元学习框架，从历史性能数据中学习，自动选择适合不同任务特点的加速策略。

Result: 该框架在效率和性能上优于传统方法，为分散式AI系统提供了更经济和可行的解决方案。

Conclusion: 元学习框架为分散式AI系统中的推理加速提供了有效路径，推动了更民主和经济的人工智能发展。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [72] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 论文提出了一种新的营销框架ADT4Coupons，用于优化在线平台的优惠券分发策略，以提升长期收入。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能有效利用平台与用户之间的复杂序列交互，导致性能瓶颈。

Method: 提出ADT4Coupons框架，整合通用场景、序列建模和高效迭代更新，直接设计长期收入优化的分发策略。

Result: 在真实工业数据集、公开数据集和合成数据集上的实验证明了该框架的优越性。

Conclusion: ADT4Coupons能有效提升优惠券分发的长期收入，适用于多样化的实际营销场景。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [73] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 论文介绍了Construction Safety Dataset (CSDataset)，一个综合多层级数据集，用于解决建筑安全研究中数据不足的问题，支持机器学习和语言模型分析。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集数量有限且多样性不足，难以支持深入分析。

Method: 引入CSDataset，整合OSHA的结构化属性和非结构化叙述，并进行初步基准测试和跨层级分析。

Result: 研究发现投诉驱动的检查与后续事故概率降低17.3%相关。

Conclusion: CSDataset为建筑安全研究提供了新工具，未来可进一步优化分析。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [74] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种分层联邦微调框架，用于动态车联网场景中的资源感知和移动弹性学习，通过LoRA和UCB-DUAL算法优化能效和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决车联网系统中由于客户端移动性、资源异构性和间歇性连接导致的多任务适应效率低和延迟高的问题。

Method: 采用分层联邦微调框架，结合LoRA和UCB-DUAL算法，实现资源感知和移动弹性学习。

Result: 实验表明，该方法在延迟和准确性上均优于基线，延迟降低24%，平均准确性提高2.5%。

Conclusion: 该框架为动态车联网场景下的高效多任务适应提供了可行方案。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [75] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于混合专家架构的量化推理框架，通过动态路由输入数据到最合适的量化专家模型，缓解单一量化模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会导致精度下降。MoQE旨在通过结合多个量化变体作为专家模型，提升量化模型的性能。

Method: MoQE结合全精度模型的多个量化变体作为专门的量化专家，并基于输入数据特征动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型，适用于CV和NLP任务。

Result: 在ResNet、LLaMA和Qwen模型家族及多个基准数据集上的实验表明，MoQE性能接近SOTA量化模型，且未显著增加推理延迟。

Conclusion: MoQE通过动态路由和专家模型结合，有效缓解量化模型的性能下降，为资源受限设备上的深度学习应用提供了高效解决方案。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [76] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 该论文提出了一种基于可微分转移模块的修复算法，用于优化微LED制造中的选择性转移过程，显著减少了转移步骤和规划时间。


<details>
  <summary>Details</summary>
Motivation: 微LED制造中的选择性转移需要高效的计算模型来优化XY平台的运动，并适应不同的优化目标。

Method: 提出了一种基于可微分转移模块的修复算法，支持梯度优化，避免了手工特征提取，训练速度更快。

Result: 实验显示，该方法在2000x2000阵列上减少了50%的转移步骤，规划时间低于2分钟。

Conclusion: 该方法为微LED修复提供了高效、灵活的解决方案，适用于AR/VR和下一代显示器的制造。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [77] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec提出了一种分层自适应网络，通过动态选择层和权重合并机制，提升预训练模型在测试时对分布变化的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法因单维线性分类层无法应对复杂分布变化的问题。

Method: 采用分层组织层结构，动态选择最优层，合并权重至其他层，并通过线性层一致性防止噪声批次干扰。

Result: 在多种挑战性场景和目标数据集上表现优异，提升了鲁棒性、处理不确定性和异常值能力。

Conclusion: Hi-Vec显著提升了现有方法的适应性，适用于复杂分布变化场景。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [78] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: GSMT是一种混合模型，结合图注意力网络和序列到序列RNN，通过任务校正器优化公交轨迹预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的发展中地区，仅依赖GPS数据预测公交轨迹具有挑战性，需要更高效的模型。

Method: GSMT整合GAT和RNN，利用任务校正器聚类历史轨迹并优化预测，通过两阶段方法实现多节点轨迹预测。

Result: 在吉隆坡的真实数据集上，GSMT在短期和长期轨迹预测任务中表现优异。

Conclusion: GSMT通过混合模型和任务校正器，显著提升了复杂城市环境中公交轨迹预测的准确性。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [79] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种结合量子启发图神经网络（QI-GNN）与集成模型的新方法，用于区块链网络中的反洗钱（AML）交易检测，通过引入CP分解层提升性能，F2分数达74.8%。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域快速发展，区块链网络中的非法交易检测需求迫切，需要创新解决方案。

Method: 结合QI-GNN与集成模型（QBoost或随机森林），并在图神经网络中引入CP分解层以高效处理复杂数据结构。

Result: 在检测欺诈交易中，F2分数达到74.8%，优于传统机器学习方法。

Conclusion: 量子启发算法结合结构优化在金融安全领域具有潜力，值得进一步探索和推广。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [80] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型的原型估计框架，用于零样本和小样本表格数据学习，无需训练分类器或微调模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在表格数据建模中潜力巨大，但在零样本和小样本场景下有效利用仍具挑战性。

Method: 通过任务和特征描述生成特征值，构建零样本原型，并融合小样本数据增强性能。

Result: 实验证明该方法在零样本和小样本表格学习中表现有效。

Conclusion: 该方法提供了一种可扩展且鲁棒的框架，克服了基于示例提示的限制。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [81] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 研究提出了一种基于局部场电位（LFPs）的深度学习模型，用于单次试验气味检测，验证了嗅觉球信号的有效性，并显著优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 当前气味检测传感器在复杂混合物中表现不佳，非侵入性记录缺乏单次试验可靠性，因此需要开发一种通用系统。

Method: 使用互补的一维卷积网络（ResCNN和AttentionCNN）解码嗅觉球多通道LFPs信号。

Result: 在七只清醒小鼠的2,349次试验中，模型平均准确率达86.6%，F1分数81.0%，AUC为0.9247，显著优于以往方法。

Conclusion: 研究证实了从LFPs中实现单次气味检测的可行性，并展示了深度学习在理解嗅觉表征中的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [82] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拓扑结构的方法来评估图神经网络（GNN）中的信息过度压缩（over-squashing）问题，并通过实验验证了重连（rewiring）策略在不同数据集上的效果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在多个领域表现出色，但消息传递机制导致长距离信息过度压缩，限制了模型表达能力。目前缺乏直接的度量方法评估这一问题，因此需要一种严谨的评估工具。

Method: 提出了一种基于节点对间互敏感度衰减率的拓扑方法，并扩展为四种图级统计量。结合因果设计，量化了重连策略对信息压缩的影响。

Result: 实验表明，图分类数据集普遍存在信息过度压缩问题，重连能有效缓解，但效果因数据集和方法而异；节点分类中信息压缩问题较轻，重连可能适得其反。

Conclusion: 重连策略在信息压缩严重且适度调整时最有效，过度重连或应用于轻度压缩的图可能有害。论文提供的诊断工具可帮助实践者在训练前判断重连是否有效。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [83] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 该论文研究了协作多智能体强化学习（c-MARL）在现实约束条件下的新漏洞，提出了一种高效的对抗扰动生成算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于训练时攻击或不现实场景，而该论文旨在探索更实际条件下c-MARL的脆弱性。

Method: 假设攻击者仅能收集并扰动部署智能体的观测数据，提出简单高效的对抗扰动算法。

Result: 在3个基准和22个环境中验证了算法的有效性，且仅需1,000样本，远少于先前方法。

Conclusion: 该算法在多样环境和算法中均表现出高效性，为c-MARL的安全性提供了新见解。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [84] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出了一种基于模式的知识组件（KC）自动发现框架，通过变分自编码器和注意力机制提取学生代码中的模式，形成可解释的KC，显著提升了知识追踪性能。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育中个性化学习需要准确建模学生知识，但现有KC提取方法因代码结构多变和概念交互复杂而难以解释和自动化。

Method: 使用变分自编码器生成代表性代码模式，结合注意力机制识别重要模式，聚类形成模式化KC，并通过学习曲线分析和深度知识追踪评估。

Result: 实验显示该方法能生成有意义的学习轨迹，并在知识追踪预测性能上优于传统方法。

Conclusion: 该框架为CS教育中的知识建模提供了自动化、可扩展且可解释的解决方案。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [85] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 数据集蒸馏将大数据集压缩为小型合成数据集，使在合成数据集上的学习近似于原始数据集。该方法可应用于强化学习任务，并将其转化为监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过数据集蒸馏技术压缩复杂任务（如强化学习），并将其转化为其他学习模式（如监督学习），以提高学习效率和通用性。

Method: 提出了一种基于近端策略优化的元学习方法，用于蒸馏多维度任务（如经典cart-pole问题、MuJoCo环境和Atari游戏）。

Result: 成功将复杂强化学习环境压缩为一步监督学习任务，并验证了该方法在不同学习架构中的通用性。

Conclusion: 数据集蒸馏技术能够高效压缩复杂任务，并实现跨学习模式的转换，展示了其在多领域的应用潜力。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [86] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出了一种结合联邦学习和区块链技术的去中心化天气预测框架，以提高隐私性、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前集中式天气预测系统存在安全漏洞、可扩展性差和单点故障问题，亟需改进。

Method: 整合联邦学习（保护隐私）和区块链技术（透明验证），引入基于信誉的投票机制和IPFS存储。

Result: 实验表明该方法提高了预测准确性、系统弹性和可扩展性。

Conclusion: 该框架适合在现实安全关键环境中部署。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [87] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种精确验证方法GNNev，用于评估图神经网络（GNNs）在对抗攻击下的鲁棒性，支持多种聚合函数，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: GNNs在关键应用中易受对抗攻击，现有方法对常用聚合函数的支持不足，因此需要一种更全面的验证方法。

Method: 通过约束求解和边界收紧技术，迭代解决松弛约束问题，支持sum、max和mean聚合函数。

Result: 在多个数据集（Cora、CiteSeer、Amazon、Yelp）上验证了GNNev的有效性，性能优于现有工具。

Conclusion: GNNev为GNNs的对抗鲁棒性提供了可靠验证工具，尤其在sum聚合任务中表现突出。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [88] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出一种基于权重大小的突触修剪方法，模拟生物大脑的修剪机制，逐步移除低重要性连接，显著提升时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生物大脑通过突触修剪优化效率，而人工神经网络中的dropout随机失活神经元，缺乏活动依赖性。本文旨在提出一种更接近生物机制的修剪方法。

Method: 在训练过程中，基于权重的绝对值计算重要性，采用立方调度逐步增加全局稀疏性，定期永久移除低重要性权重，同时保持梯度流动。

Result: 在多个时间序列预测模型和数据集上表现优异，显著降低平均绝对误差（金融预测中最高降低20%），在部分Transformer模型中降低52%。

Conclusion: 动态修剪机制结合权重消除与渐进稀疏化，性能优于传统dropout，尤其适用于金融时间序列预测，易于集成到多种架构中。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [89] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是一个基于几何的推荐框架，利用Ricci曲率和流分析动态金融图中的因果关系，提升推荐系统的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何方法（Ricci曲率和流）量化金融图中的局部压力，追踪冲击传播，从而改进金融决策支持系统。

Method: 通过动态金融图建模股票、宏观经济指标和新闻的交互，利用离散Ricci曲率量化局部压力，Ricci流追踪冲击传播，曲率梯度揭示因果子结构。

Result: 初步结果显示，在S&P 500数据和FinBERT情感分析下，RicciFlowRec在合成扰动下表现出更好的鲁棒性和可解释性。

Conclusion: RicciFlowRec是首个将几何流推理应用于金融决策支持的推荐系统，未来计划扩展至投资组合优化和收益预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [90] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 论文提出通过限制稀疏自编码器（SAEs）的训练领域（如医学文本），可以提升特征重建的保真度和可解释性，减少线性残差，并捕捉更有意义的领域特定特征。


<details>
  <summary>Details</summary>
Motivation: 传统SAEs在广泛数据分布上训练，导致固定预算只能捕捉高频通用模式，产生线性残差和特征碎片化问题。

Method: 在特定领域（医学文本）训练JumpReLU SAEs，使用195k临床问答数据，分析Gemma-2模型的第20层激活。

Result: 领域限制的SAEs解释更多方差（20%），提高损失恢复，减少线性残差，特征与临床概念对齐。

Conclusion: 领域限制可解决广泛领域SAEs的局限性，提供更完整和可解释的潜在分解，质疑通用SAEs的扩展需求。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [91] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 研究发现，通过文本到图像模型生成的图像可以检测痴呆症，准确率达75%，并揭示了语言中哪些部分对检测有贡献。


<details>
  <summary>Details</summary>
Motivation: 探索文本到图像模型是否能将病理语音（如痴呆相关语言）与生成图像对齐，并解释这种对齐的机制。

Method: 使用ADReSS数据集，分析模型生成的图像与痴呆相关语言的关联，并应用可解释性方法。

Result: 仅通过生成图像即可实现痴呆检测，准确率为75%。

Conclusion: 文本到图像模型在病理语音对齐方面具有潜力，为痴呆检测提供了新方法。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [92] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的金融风险评估框架，通过特征注意力和时序建模实现跨机构联合建模，保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构金融风险分析中的数据隐私和协作建模挑战。

Method: 采用联邦学习，结合特征注意力机制和时序建模，使用差分隐私和噪声注入保护参数，分布式优化策略训练本地子模型，中央服务器聚合生成全局模型。

Result: 在通信效率、模型准确性、系统性风险检测和跨市场泛化方面优于传统集中式方法和现有联邦学习变体。

Conclusion: 该方法在敏感金融环境中具有强大的建模能力和实用价值，同时保护数据主权，为智能金融风险分析提供了安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [93] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，用于分布式后端服务系统，解决了复杂结构依赖、行为演化和无标签数据等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统中异常检测的实际挑战，如复杂的结构依赖、多样化的行为演化和缺乏标签数据。

Method: 构建动态图表示服务调用关系，使用图卷积提取高阶结构特征，Transformer建模节点时间行为，融合结构和行为特征为统一异常向量，非线性映射计算异常分数。

Result: 在真实云监控数据上实验，表现优于现有模型，能更好地捕捉异常传播路径和动态行为序列。

Conclusion: 该方法具有强表达力和稳定性，适合实际部署。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [94] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种新的元学习算法，结合梯度匹配和锐度感知最小化，通过双层优化框架提升模型适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在有限训练数据下跨任务泛化的问题，适用于少样本学习和快速适应场景。

Method: 结合梯度匹配与锐度感知最小化，采用双层优化框架，并辅以PAC-Bayes理论和收敛性分析。

Result: 在基准数据集上表现优于现有方法，准确率和泛化能力更优。

Conclusion: DGS-MAML在少样本学习和快速适应场景中具有显著优势，代码已开源。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [95] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 本文提出了隐式超图神经网络（IHGNN），通过非线性固定点方程解决表示问题，避免了传统超图神经网络的深度限制，提升了长距离依赖捕获能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多交互是群体性的（如多作者论文或用户联合参与项目），传统超图神经网络依赖固定层数的显式消息传递，限制了长距离依赖捕获且训练不稳定。

Method: 引入IHGNN，通过非线性固定点方程计算表示，避免了深度架构；开发了收敛性可证明的训练方案，分析了模型的过平滑条件和表达能力，并推导了超图上的转导泛化界。

Result: 在引用基准测试中，IHGNN在准确性和鲁棒性上均优于传统图/超图神经网络基线，且对随机初始化和超参数变化具有强鲁棒性。

Conclusion: IHGNN为高阶关系学习提供了稳定、高效且泛化能力强的解决方案，具有实际应用价值。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [96] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA是一种新算法，用于识别高速公路系统中导致其他部分拥堵的区域，通过时间序列分析和概率模型提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决交通拥堵问题，通过识别拥堵源头更高效地分配资源。

Method: 利用时间序列数据，开发新算法，包括事件检测、概率模型和二元分类器。

Result: 在洛杉矶地区195个传感器数据上测试，表现优于现有方法。

Conclusion: NEXICA在准确性和计算速度上优于现有技术，可用于实际交通管理。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [97] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT框架首次将对比学习和生成方法结合，通过联合优化解决各自局限性，在时间序列数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习和生成方法在时间序列数据中的互补潜力，解决各自的高类内相似性和大数据依赖问题。

Method: 提出Contrastive Generative Time series框架（CoGenT），通过联合对比-生成优化统一两种范式。

Result: 在六个数据集上表现优于单独方法，F1分数提升最高达59.2%和14.27%。

Conclusion: CoGenT为时间序列领域的混合自监督学习奠定了基础，兼具判别力和生成鲁棒性。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [98] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种名为FGCRN的新型开放集故障诊断模型，通过多尺度深度卷积、双向门控循环单元和时间注意力机制捕捉特征，结合无监督学习构建细粒度特征表示，有效识别未知故障。


<details>
  <summary>Details</summary>
Motivation: 在多重模式过程中，同一健康状态的样本常呈现多簇分布，难以构建紧凑准确的决策边界，需开发能识别未知故障的诊断系统。

Method: 结合多尺度深度卷积、双向门控循环单元和时间注意力机制捕捉特征，设计基于距离的损失函数增强类内紧凑性，利用无监督学习构建细粒度特征表示，并采用极值理论建模样本特征与细粒度表示的距离。

Result: 实验证明该方法性能优越。

Conclusion: FGCRN模型能有效识别未知故障，为开放集故障诊断提供了新解决方案。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [99] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS是一种新型Meta-NAS框架，通过图建模和混合搜索策略（全局贝叶斯优化与局部梯度上升）提升任务感知架构的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Meta-NAS方法存在泛化性差、搜索空间有限或计算成本高的问题，限制了实际应用。

Method: GraB-NAS将神经网络架构建模为图，采用混合搜索策略（全局贝叶斯优化+局部梯度上升）生成新架构。

Result: 实验表明，GraB-NAS在性能和泛化能力上优于现有Meta-NAS方法，并能超越预定义搜索空间。

Conclusion: GraB-NAS通过混合搜索策略显著提升了Meta-NAS的效能，为任务感知架构设计提供了新方向。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [100] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: DeepFeatIoT是一种新型深度学习模型，通过结合局部和全局特征、随机卷积核特征及大语言模型特征，显著提升物联网时间序列数据的分类效果。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器数据存在元数据丢失、数据源异构、采样频率不一致等问题，导致原始数据难以解析，影响智能系统效果。

Method: 提出DeepFeatIoT模型，融合学习与非学习特征，包括局部/全局特征、随机卷积核特征及大语言模型特征。

Result: 在多个真实物联网数据集上表现优于现有基准模型，尤其在标记数据有限的情况下。

Conclusion: DeepFeatIoT有望推动物联网分析进步，支持下一代智能系统发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [101] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP是一种基于扩展图理论的N:M结构化剪枝方法，有效减少LLMs的模型大小和计算需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）规模扩大，部署这些模型的计算和内存挑战日益严重，亟需开发更高效的模型变体。

Method: 利用扩展图理论指导N:M结构化剪枝设计，确保剪枝网络中信息流动，保留模型功能。

Result: 实验表明，EGGS-PTP不仅显著加速并节省内存，还在多种LLMs上优于现有结构化剪枝技术的准确性。

Conclusion: EGGS-PTP为LLMs的高效部署提供了一种有效解决方案，兼具性能和准确性优势。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [102] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune通过细粒度神经元动态调制，同时优化LLM的安全性和实用性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐方法存在鲁棒性不足、频繁拒绝良性查询及实用性下降等问题，亟需改进。

Method: 提出NeuronTune框架，通过归因识别安全与实用神经元，并利用元学习动态调整其激活强度。

Result: 实验表明，NeuronTune在安全性和实用性上均显著优于现有技术。

Conclusion: NeuronTune为LLM的安全与实用平衡提供了高效解决方案。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [103] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出了一种联邦持续学习框架，通过轻量级本地模型动态适应新任务并提升大模型性能，解决了基础模型在持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在联邦持续学习中表现不佳，无法有效利用本地私有数据且容易遗忘先前知识，而小模型则更适合资源受限的环境。

Method: 提出协作框架，包括小模型持续微调防止遗忘，以及一对一蒸馏实现异构本地知识的个性化融合。

Result: 实验证明该框架性能优越，即使客户端使用异构小模型。

Conclusion: 该框架成功解决了基础模型在联邦持续学习中的挑战，提升了性能。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [104] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: MiCo框架是一个用于边缘AI应用的混合精度量化（MPQ）探索和部署框架，旨在通过优化算法搜索最佳量化方案，同时满足延迟约束。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ方案探索算法在灵活性和效率上受限，且缺乏对后训练量化和量化感知训练结果的复杂影响的理解，同时缺少端到端的MPQ模型优化和部署框架。

Method: 提出MiCo框架，采用新型优化算法搜索最优量化方案，构建硬件感知延迟模型以快速探索，并支持从PyTorch MPQ模型直接部署到裸机C代码。

Result: 框架实现了端到端加速，同时最小化精度损失。

Conclusion: MiCo为边缘AI应用提供了一种高效、灵活的MPQ解决方案。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [105] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: CGAD是一种基于因果图的异常检测框架，用于公共基础设施系统中的网络攻击检测，通过因果分析和图结构比较显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 随着针对关键基础设施的网络攻击日益复杂，传统方法在分布偏移和类别不平衡问题上表现不佳，亟需更鲁棒的异常检测策略。

Method: CGAD采用两阶段监督框架：1) 使用动态贝叶斯网络学习因果不变图结构；2) 通过因果图比较检测异常。

Result: CGAD在非平稳和不平衡时间序列环境中表现优异，F1和ROC-AUC分数显著优于基线方法。

Conclusion: CGAD通过揭示因果结构，不仅提高了检测精度，还重新定义了异常检测的鲁棒性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [106] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: Gauss-Tin是一种结合回放策略与高斯混合模型的新方法，旨在通过优化样本选择和指导生成来减轻大语言模型（LLMs）的灾难性遗忘问题，实验结果显示其性能比传统方法提升6%。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是LLMs面临的主要挑战，即在学习新知识时遗忘旧知识。为解决这一问题，研究探索了持续学习策略，尤其是回放技术。

Method: Gauss-Tin结合回放策略与高斯混合模型，优化样本选择，并通过指导生成强化过去的学习。

Result: 实验表明，Gauss-Tin在保留指标上比传统方法提升6%，有效缓解灾难性遗忘。

Conclusion: Gauss-Tin展示了混合模型在动态学习环境中增强LLMs鲁棒性和适应性的潜力。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [107] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络（GNN）的统一、可解释框架，用于预测业务流程监控（PBPM）中的未来事件，解决了现有方法在局部建模、时间相关性和语义表达上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的PBPM模型在捕捉时间相关性和语义信息方面表现不足，需要一种更全面且可解释的解决方案。

Method: 结合前缀图卷积网络（GCN）和全迹图注意力网络（GAT），引入时间衰减注意力机制和边特征嵌入语义，提升模型性能。

Result: 在五个基准测试中，模型表现出色，无需针对每个数据集调整即可达到高准确率。

Conclusion: 该框架通过改进架构、时间和语义表达，为PBPM中的事件预测提供了鲁棒、通用且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [108] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G利用LLMs生成合成数据解决稀有事件分类的冷启动问题，通过半监督标签传播扩展数据集，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，尤其是稀有事件，阻碍了有效机器学习模型的训练。

Method: 提出SYNAPSE-G，利用LLMs生成合成数据作为种子，通过半监督标签传播扩展数据集，最终训练分类器。

Result: 在SST2和MHS数据集上，SYNAPSE-G在发现正标签方面优于基线方法。

Conclusion: SYNAPSE-G通过合成数据和半监督学习有效解决了稀有事件分类问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [109] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: EGI（边缘通用智能）通过世界模型实现边缘计算的自主感知、推理和行动，填补了无线边缘领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 探索世界模型在边缘计算中的应用，以提升自主AI系统的能力。

Method: 分析世界模型的架构基础（如潜在表示学习、动态建模和基于想象的规划）及其在EGI场景中的应用。

Result: 展示了世界模型在车辆网络、无人机网络等场景中的优化潜力，并探讨了与基础模型和数字孪生的协同作用。

Conclusion: 提出了安全保证、高效训练等开放挑战，为下一代智能边缘系统提供了理论和实践指导。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [110] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果能力的GDCC框架，用于强化学习中的高效环境探索，通过测量因果能力并识别关键点作为子目标，显著提升了探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以在复杂场景中测量动作与状态转移之间的因果关系，限制了强化学习代理的高效探索能力。

Method: 提出GDCC框架，首先测量状态空间中的因果能力，然后通过蒙特卡洛方法识别关键点作为子目标，优化探索策略。

Result: 实验表明，高因果能力的状态与预期子目标一致，GDCC在多目标任务中显著提升了成功率。

Conclusion: GDCC框架通过因果能力和子目标引导，实现了更高效和有目的的环境探索。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [111] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 论文提出了一种物理和几何感知的时空谱图神经算子（πG-Sp²GNO），用于高效求解偏微分方程（PDEs），在复杂几何和有限标注数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 高效准确地求解PDEs是科学与工程中的核心挑战，尤其是在复杂几何和有限标注数据的情况下。

Method: 改进现有的Sp²GNO，引入几何感知能力，并结合物理信息，提出了一种混合物理损失函数，用于时间相关问题。

Result: 在多个基准测试中，该方法优于当前最先进的物理信息神经算子算法。

Conclusion: πG-Sp²GNO在复杂几何和时间相关问题中表现出色，为PDEs求解提供了高效且准确的解决方案。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [112] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG是一个多模态因果推理框架，通过结合变量语义和历史时间序列数据，提升时间序列建模的预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽略变量语义信息，而TimeMKG旨在利用变量名称和描述中的领域知识，实现更鲁棒和可解释的建模。

Method: TimeMKG使用大语言模型解析变量语义，构建多变量知识图谱，并通过双模态编码器分别建模语义提示和统计模式，最后通过跨模态注意力融合表示。

Result: 实验表明，引入变量级知识显著提升了预测性能和泛化能力。

Conclusion: TimeMKG通过融合语义和统计信息，为时间序列建模提供了更高效和可解释的解决方案。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [113] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的统计框架，用于分析蛋白质热稳定性数据，克服了现有热蛋白质组分析（TPP）方法的关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有TPP方法假设熔解曲线为S形，并受限于经验零分布，导致仅能识别约5%的数据。Thermal Tracks旨在解决这一问题，提供更灵活的分析方法。

Method: 采用高斯过程（GP）模型和平方指数核，灵活建模任意熔解曲线形状，并通过核先验生成无偏零分布。

Result: Thermal Tracks能有效分析蛋白质热稳定性的全局扰动，如通路抑制、基因修饰或环境压力，尤其适用于非传统熔解曲线的蛋白质。

Conclusion: Thermal Tracks是一个免费、灵活的工具，适用于蛋白质组范围内的热稳定性研究。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [114] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究了非对称低秩矩阵补全问题，通过梯度下降方法解决无约束非凸优化问题，发现无需正则化项也能保证收敛。


<details>
  <summary>Details</summary>
Motivation: 探索梯度下降方法在非对称低秩矩阵补全中的表现，特别是去除正则化项对收敛性能的影响。

Method: 采用谱初始化的普通梯度下降方法，结合留一技术进行理论分析。

Result: 证明该方法具有高概率线性收敛率，且计算成本更低。

Conclusion: 梯度下降具有隐式正则化特性，无需显式正则化项即可高效完成矩阵补全。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [115] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 提出了一种基于算子理论的时态锚定框架，结合漂移映射和事件索引块，最终通过仿射投影实现。


<details>
  <summary>Details</summary>
Motivation: 为解决嵌入空间中的时态锚定问题，提供一种统一的算子理论框架。

Method: 使用漂移映射与事件索引块结合仿射投影，并证明相关定理（如收缩引理、收敛定理）。

Result: 证明了软注意力层的Lipschitz性质，并给出了层收缩的充分条件。

Conclusion: 框架具有严格的数学基础，适用于注意力层等场景，且计算等价性得到验证。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [116] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出了一种动态连接掩码（DCM）机制，用于增强MLPs和KANs对噪声标签的鲁棒性，通过自适应掩码不重要边减少梯度误差，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中噪声标签不可避免，现有研究多关注鲁棒损失函数和样本选择，对模型架构正则化的探索较少。

Method: 基于KANs的稀疏正则化，提出DCM机制，动态评估并掩码不重要边，减少梯度误差。

Result: 在合成和真实数据集上实验表明，DCM优于现有方法，且首次发现KANs在噪声场景中优于MLPs。

Conclusion: DCM机制有效提升模型对噪声标签的鲁棒性，可与其他噪声鲁棒方法结合，KANs在噪声场景中表现优异。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [117] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出GraphTreeGen（GTG），一种基于子树的生成框架，用于高效、准确地合成脑连接组，解决了现有模型在局部结构、节点属性依赖、边权重预测和计算效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 脑连接组的获取成本高且耗时，现有生成模型在局部结构保留、节点属性依赖、边权重预测和计算效率方面存在不足，需要一种更高效的生成方法。

Method: GTG将连接组分解为熵引导的k跳子树，通过共享GCN编码局部结构，利用二分消息传递层融合子树嵌入与全局节点特征，双分支解码器联合预测边存在和权重。

Result: GTG在自监督任务中优于现有基线，在监督任务中表现竞争性，提供更高的结构保真度和更精确的权重预测，同时内存消耗更低。

Conclusion: GTG为脑连接组生成提供了一种高效、准确的解决方案，其模块化设计支持扩展到超分辨率和跨模态合成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [118] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLM）处理临床笔记的方法，以改进概念瓶颈模型（CBM）在急性呼吸窘迫综合征（ARDS）识别中的性能，实现了10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 利用公开临床数据集研究疾病异质性和个性化治疗，但这些数据集通常不完整且缺乏关键标签。现有AI工具在解释性上存在局限，而CBM虽能提供可解释的概念，但性能受限。

Method: 通过LLM处理临床笔记生成额外概念，结合CBM提升性能，并减少信息泄漏和对虚假捷径的依赖。

Result: 方法在ARDS识别任务中实现了10%的性能提升，并学习到更全面的概念。

Conclusion: 结合LLM的CBM方法显著提升了性能，同时增强了模型的可解释性和对ARDS的表征能力。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [119] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: GenCO框架通过生成模型与多实例奖励学习结合，优化电商广告创意组合选择，显著提升广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独评估创意元素，无法高效探索大量组合空间，需改进以提升广告效果。

Method: 采用两阶段架构：生成模型生成多样化创意组合，强化学习优化选择；多实例学习模型将组合级反馈（如点击）归因于单个元素。

Result: 在电商平台部署后显著增加广告收入，并发布工业数据集促进研究。

Conclusion: GenCO框架有效解决创意组合选择问题，具有实际应用价值和研究意义。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [120] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出了一种名为Hereditary Knowledge Transfer（HKT）的方法，通过模块化和选择性特征转移优化小型可部署模型。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在性能和效率之间的权衡问题，提出一种受生物学启发的知识继承框架。

Method: 采用提取、转移和混合（ETM）三阶段过程，结合遗传注意力机制（GA）选择性集成特征。

Result: 在多个视觉任务中表现优于传统蒸馏方法，同时保持模型紧凑性。

Conclusion: HKT为资源受限环境提供了一种通用、可解释且可扩展的高性能神经网络部署方案。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [121] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 该研究开发了一种机器学习流程，通过纵向数据预测衰老轨迹，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 预测个体衰老轨迹是预防医学和生物信息学的核心挑战，现有模型难以捕捉衰老的动态特性。

Method: 利用纵向队列数据（2019-2020和2021-2022），通过设计捕捉生物标志物变化率的新特征，训练LightGBM模型。

Result: 模型在预测后续时间点年龄时表现优异（男性R²=0.515，女性R²=0.498），显著优于线性模型和其他树模型。

Conclusion: 动态健康轨迹比静态健康快照更能预测生物年龄，为临床动态跟踪和个性化干预提供了新工具。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [122] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: 本文探讨了如何将μTransfer技术应用于MoE架构，提出了μP方法，并验证了其在模型宽度和学习率优化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 近年来，LLMs和MoE架构分别成为重要技术，但二者的结合尚未被研究。本文旨在填补这一空白。

Method: 提出了μ-参数化（μP）方法，为MoE架构中的路由器和专家模块提供理论保证，并通过实验验证其效果。

Result: 实验验证了μP方法的有效性，并研究了专家数量和粒度对最优学习率的影响。

Conclusion: μP方法为MoE架构提供了理论支持，并展示了其在超参数优化中的潜力。

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [123] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出TriForecaster框架，通过混合专家（MoE）和多任务学习（MTL）解决多区域电力负荷预测（MRELF）问题，平均预测误差降低22.4%。


<details>
  <summary>Details</summary>
Motivation: 智能电网和电表的普及提供了更详细的负荷数据，但不同区域的负荷预测面临区域、上下文和时间变化的挑战。

Method: 提出TriForecaster框架，包含RegionMixer和CTSpecializer层，动态协调专家模型在区域、上下文和时间维度的合作与专业化。

Result: 在四个真实数据集上，TriForecaster平均预测误差降低22.4%，并在中国东部17个城市成功部署。

Conclusion: TriForecaster在多区域电力负荷预测中表现出色，具有灵活性和广泛适用性。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [124] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H是一种快速训练范式，结合闭式权重计算和梯度优化少量合成输入、软标签及隐藏激活。


<details>
  <summary>Details</summary>
Motivation: 旨在通过减少可训练参数数量并利用闭式解加速训练过程，同时保持高准确性。

Method: 通过闭式解计算权重矩阵，并用Adam优化原型（合成输入、软标签和隐藏激活），将可训练自由度从权重空间转移到数据/激活空间。

Result: 在MNIST和Fashion-MNIST上分别达到97.8%和89.3%的测试准确率，训练时间仅需3.9-4.5秒。

Conclusion: 该方法在准确性、速度和模型大小之间取得了良好平衡，优于ELM、随机特征岭回归和浅层MLP。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [125] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出了一种优化Matérn核时间高斯过程超参数的方法，基于递归贝叶斯估计，性能优于边际似然最大化和哈密顿蒙特卡洛采样。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在概率数值中很重要，但现有超参数优化方法效率不足。

Method: 将超参数优化问题转化为自回归模型参数的递归贝叶斯估计。

Result: 在运行时间和均方根误差上优于边际似然最大化和哈密顿蒙特卡洛采样。

Conclusion: 该方法在效率和准确性上表现优越，适用于高斯过程回归。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [126] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 该研究利用机器学习分析跳远比赛中的生物力学特征，识别关键特征及其组合效应，发现男女运动员的关键技术差异。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以明确分析生物力学特征与运动员表现的关系，现代数据分析和机器学习为此提供了新工具。

Method: 使用分位数回归建模生物力学特征与跳远成绩的关系，并结合SHAP、PDP和ICE图解释模型。

Result: 男性运动员的关键特征是起跳前支撑腿膝盖角度（>169°），女性运动员的关键特征是着陆姿势和助跑技术。

Conclusion: 研究为分析运动表现特征提供了框架，特别关注顶级赛事中的关键因素。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [127] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，解释了大语言模型（LLMs）在上下文学习（ICL）中如何通过向量算术完成事实回忆任务，并证明了其强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLMs在ICL中存在潜在任务向量并通过向量算术解决问题，但缺乏理论解释。本文旨在填补这一空白。

Method: 基于层次概念建模，提出优化理论，分析非线性残差变换器通过梯度下降和交叉熵损失完成事实回忆任务的机制。

Result: 理论证明了0-1损失的收敛性，并展示了模型对概念重组和分布变化的强鲁棒性。

Conclusion: 研究揭示了变换器优于静态嵌入模型的优势，并通过实验验证了理论结果。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [128] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: RankList是一种新颖的列表式偏好学习框架，扩展了RankNet，通过全局排名一致性建模和跳过式比较提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RankNet等成对框架在全局排名一致性上的局限性。

Method: 提出RankList框架，结合局部和非局部排名约束，使用log-sum-exp近似提高训练效率，并引入跳过式比较。

Result: 在多个基准数据集（MSP-Podcast、IEMOCAP等）上，RankList在Kendall's Tau和排名准确性上优于基线方法。

Conclusion: RankList为主观学习场景提供了一种统一且可扩展的偏好建模方法。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [129] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: FedShard是一种联邦学习遗忘算法，首次同时保证效率公平性和性能公平性，解决了收敛、遗忘效率和公平性之间的权衡问题，并提出了新的公平性评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前联邦遗忘研究主要关注效率和有效性，而忽略了去中心化客户在遗忘过程中的效率公平性和性能公平性。

Method: 提出FedShard算法，自适应解决收敛、遗忘效率和公平性之间的困境，并设计两种新指标量化公平性。

Result: FedShard在遗忘性能和效率上均表现公平，能抵御级联离开和投毒攻击，实验显示其遗忘速度比从头训练快1.3-6.2倍，比现有方法快4.9倍。

Conclusion: FedShard首次实现了联邦遗忘中的双重公平性，为实际应用提供了更平衡的解决方案。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [130] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种数据高效蒸馏框架（DED），通过优化推理蒸馏的帕累托前沿，实现了在少量精选数据下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过扩展语料库和多阶段训练提升了推理能力，但推理扩展定律仍增加了计算成本。DED旨在解决这一问题，提供高效且实用的推理能力提升途径。

Method: DED框架包括三个关键点：(1) 通过综合比较选择最优教师模型；(2) 使用精选小语料库平衡领域内外性能；(3) 多样化的推理轨迹提升学生模型的鲁棒性。

Result: 在数学推理（AIME 2024/2025, MATH-500）和代码生成（LiveCodeBench）任务中，仅用0.8k精选数据即达到最先进性能。

Conclusion: DED通过超越表面难度、标记长度或教师模型能力的因素，提供了一种高效且实用的推理能力提升方法。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [131] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 现代人工神经网络（ANN）在土壤属性预测中表现优于传统方法，尤其是TabPFN模型。


<details>
  <summary>Details</summary>
Motivation: 验证现代ANN在田间尺度土壤预测建模（PSM）中的适用性，挑战传统机器学习的主导地位。

Method: 通过31个田间数据集评估多种ANN架构（如TabM、RealMLP、FT-Transformer等）与传统方法（如随机森林、偏最小二乘回归）的性能。

Result: 现代ANN在多数任务中表现更优，TabPFN表现最稳健。

Conclusion: 推荐现代ANN（尤其是TabPFN）作为田间尺度PSM的新标准工具。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [132] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 论文研究了如何确定数据集中是否存在异常，提出了一个基于数据集大小、污染率和算法常数的下限条件。


<details>
  <summary>Details</summary>
Motivation: 异常检测中确认异常存在的基础问题尚未充分研究，本文旨在填补这一空白。

Method: 通过超过三百万次统计测试，分析数据集大小、污染率与算法常数α_algo的关系。

Result: 发现N ≥ α_algo/ν²是确认异常存在的样本数量下限，揭示了异常稀有性的极限。

Conclusion: 研究为异常检测提供了理论支持，明确了确认异常存在的条件限制。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [133] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 论文提出四种策略（ReDP、CorDP、IC-DP、RouteDP）来提升大语言模型（LLMs）在上下文辅助预测任务中的零样本能力，展示了比直接提示更优的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过直接提示利用LLMs进行上下文辅助预测，未能充分发挥其潜力。

Method: 提出四种策略：ReDP（增强可解释性）、CorDP（优化现有预测）、IC-DP（嵌入历史示例）、RouteDP（任务路由优化）。

Result: 在CiK基准测试中，这些策略在不同规模和家族的LLMs上均优于直接提示方法。

Conclusion: 这些简单而有效的策略为LLM在上下文辅助预测中的进一步改进提供了方向。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [134] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 论文提出了一种名为原型扩散模型（PDM）的方法，通过在扩散过程中直接集成原型学习，实现了高效且自适应的视觉条件生成，无需外部存储。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在高质量图像生成方面表现优异，但计算成本高，尤其是迭代去噪过程。现有方法如RDM虽提升效率，但依赖外部存储和检索基础设施，且缺乏训练适应性。

Method: PDM通过对比学习从干净图像特征中构建动态紧凑视觉原型集，这些原型通过语义对齐引导去噪步骤。

Result: 实验表明，PDM在保持高质量生成的同时，显著降低了计算和存储开销。

Conclusion: PDM为扩散模型提供了一种可扩展的替代方案，解决了检索条件方法的局限性。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [135] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种新型未训练的循环神经网络ResRMN，结合线性记忆库和非线性库，通过残差正交连接增强长期输入传播。实验证明其优于传统RC模型。


<details>
  <summary>Details</summary>
Motivation: 传统RC模型在长期输入传播方面存在不足，ResRMN通过残差正交连接改进这一问题。

Method: ResRMN结合线性记忆库和非线性库，利用残差正交连接优化时间维度上的传播，并通过线性稳定性分析研究动态。

Result: 在时间序列和像素级1-D分类任务中，ResRMN表现优于传统RC模型。

Conclusion: ResRMN通过残差正交连接显著提升了长期输入传播能力，为RC模型提供了新的优化方向。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [136] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少推理计算开销，同时保留测试时扩展的优势。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展虽然提升了模型性能，但显著增加了计算时间，限制了其实际应用。

Method: 提出噪声超网络，通过调制初始输入噪声来优化生成模型，学习奖励倾斜的分布。

Result: 方法在显著降低计算成本的同时，恢复了大部分测试时优化的质量提升。

Conclusion: 噪声超网络是一种高效且理论支持的方法，可在减少计算开销的同时保持模型性能。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [137] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 论文提出了一种动态混合专家（DyMoE）方法，用于解决图增量学习中的灾难性遗忘问题，通过定制正则化损失和稀疏MoE设计提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法在增量学习场景中会出现灾难性遗忘问题，且现有方法未考虑不同时间点获取的知识对新任务学习的贡献差异。

Method: 提出动态混合专家（DyMoE）方法，通过新增专家网络和定制正则化损失优化学习，并引入稀疏MoE设计降低计算成本。

Result: 模型在类增量学习任务中相对基线方法提升了4.92%的准确率。

Conclusion: DyMoE方法有效解决了灾难性遗忘问题，并通过稀疏设计提升了计算效率。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [138] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: 本文通过混合方法调查了AI开发人员的伦理认知与实践，强调角色敏感性和多方协作的重要性。


<details>
  <summary>Details</summary>
Motivation: AI技术的快速发展引发伦理担忧，需制定指南和法规以降低风险。

Method: 采用混合方法（统计与定性分析），调查414名来自43国的AI开发人员。

Result: 不同角色、地区对AI伦理的熟悉度和实践经验存在差异，需角色敏感的协作方案。

Conclusion: 呼吁定制化解决方案和未来研究方向，以推动伦理意识在AI实践中的应用。

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [139] [DeepWKB: Learning WKB Expansions of Invariant Distributions for Stochastic Systems](https://arxiv.org/abs/2508.09529)
*Yao Li,Yicheng Liu,Shirou Wang*

Main category: math.DS

TL;DR: DeepWKB是一种新的深度学习方法，用于估计随机扰动系统的稳态分布，通过WKB近似计算准势和归一化因子，适用于高维系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声强度较小时难以准确估计稳态分布，DeepWKB旨在解决这一问题。

Method: 结合蒙特卡洛数据和偏微分方程，分别计算准势和归一化因子，实现稳态分布的近似。

Result: DeepWKB在小噪声强度下有效，适用于高维系统，为复杂系统的罕见事件分析提供了新工具。

Conclusion: DeepWKB为解决高维随机系统的稳态分布问题提供了可扩展且灵活的方法。

Abstract: This paper introduces a novel deep learning method, called DeepWKB, for
estimating the invariant distribution of randomly perturbed systems via its
Wentzel-Kramers-Brillouin (WKB) approximation $u_\epsilon(x) = Q(\epsilon)^{-1}
Z_\epsilon(x) \exp\{-V(x)/\epsilon\}$, where $V$ is known as the
quasi-potential, $\epsilon$ denotes the noise strength, and $Q(\epsilon)$ is
the normalization factor. By utilizing both Monte Carlo data and the partial
differential equations satisfied by $V$ and $Z_\epsilon$, the DeepWKB method
computes $V$ and $Z_\epsilon$ separately. This enables an approximation of the
invariant distribution in the singular regime where $\epsilon$ is sufficiently
small, which remains a significant challenge for most existing methods.
Moreover, the DeepWKB method is applicable to higher-dimensional stochastic
systems whose deterministic counterparts admit non-trivial attractors. In
particular, it provides a scalable and flexible alternative for computing the
quasi-potential, which plays a key role in the analysis of rare events,
metastability, and the stochastic stability of complex systems.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [140] [Deep Generative Models for Discrete Genotype Simulation](https://arxiv.org/abs/2508.09212)
*Sihan Xie,Thierry Tribout,Didier Boichard,Blaise Hanczar,Julien Chiquet,Eric Barrey*

Main category: q-bio.GN

TL;DR: 该研究探讨了生成离散基因型数据的深度生成模型，比较了VAE、扩散模型和GAN，并在大规模数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基因型数据生成的挑战，同时保护隐私并提高数据可访问性。

Method: 开发并评估了VAE、扩散模型和GAN，针对离散基因型数据进行了适应性调整。

Result: 模型能有效捕捉遗传模式并保持基因型-表型关联。

Conclusion: 提供了模型比较和未来研究的实用指南，代码已开源。

Abstract: Deep generative models open new avenues for simulating realistic genomic data
while preserving privacy and addressing data accessibility constraints. While
previous studies have primarily focused on generating gene expression or
haplotype data, this study explores generating genotype data in both
unconditioned and phenotype-conditioned settings, which is inherently more
challenging due to the discrete nature of genotype data. In this work, we
developed and evaluated commonly used generative models, including Variational
Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks
(GANs), and proposed adaptation tailored to discrete genotype data. We
conducted extensive experiments on large-scale datasets, including all
chromosomes from cow and multiple chromosomes from human. Model performance was
assessed using a well-established set of metrics drawn from both deep learning
and quantitative genetics literature. Our results show that these models can
effectively capture genetic patterns and preserve genotype-phenotype
association. Our findings provide a comprehensive comparison of these models
and offer practical guidelines for future research in genotype simulation. We
have made our code publicly available at
https://github.com/SihanXXX/DiscreteGenoGen.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [141] [Objective Soups: Multilingual Multi-Task Modeling for Speech Processing](https://arxiv.org/abs/2508.09228)
*A F M Saif,Lisha Chen,Xiaodong Cui,Songtao Lu,Brian Kingsbury,Tianyi Chen*

Main category: eess.AS

TL;DR: 论文探讨了多任务语音处理中目标冲突问题，提出了分层多目标优化方法，实验证明其优于传统优化方法。


<details>
  <summary>Details</summary>
Motivation: 多任务语音处理中，任务目标冲突导致优化困难，传统多目标优化方法在任务增多时效果下降。

Method: 研究了三种多目标优化配方，引入轻量级层选择机制以减少计算开销。

Result: 实验表明，分层优化方法在语音识别和翻译任务上表现更优。

Conclusion: 分层多目标优化是构建高效多任务语音处理模型的有效方法。

Abstract: Training a single model for multilingual, multi-task speech processing (MSP)
is severely hampered by conflicting objectives between tasks like speech
recognition and translation. While multi-objective optimization (MOO) aims to
align gradient updates, its effectiveness diminishes as the number of tasks
grows, making it difficult to find a common descent direction. This raises a
fundamental question: should highly conflicting objectives be optimized jointly
or separated into a hierarchical structure? To address this question, this
paper investigates three multi-objective MSP formulations, which we refer to as
\textbf{objective soup recipes}. These formulations apply multi-objective
optimization at different optimization levels to mitigate potential conflicts
among all objectives. To ensure efficiency, we introduce a lightweight
layer-selection mechanism that computes the conflict-avoiding gradient using
only the most problematic layers, minimizing computational and memory overhead.
Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a
bi-level recipe separating recognition and translation tasks consistently
outperforms standard flat optimization. Our work demonstrates that hierarchical
MOO is a more effective and scalable approach for building state-of-the-art MSP
models. Our code has been released at
https://github.com/afmsaif/Objective_Soups.

</details>


### [142] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: Fake-Mamba是一种基于双向Mamba的实时深度伪造语音检测方法，通过结合XLSR前端和三种高效编码器，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成技术的进步，深度伪造语音的安全威胁加剧，推动了实时检测方法的研究。

Method: Fake-Mamba整合了XLSR前端和双向Mamba，并提出了三种高效编码器（TransBiMamba、ConBiMamba、PN-BiMamba），以捕捉局部和全局特征。

Result: 在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中，Fake-Mamba的EER分别为0.97%、1.74%和5.85%，优于现有SOTA模型。

Conclusion: Fake-Mamba展示了强大的泛化能力和实时推理性能，具有实际应用潜力。

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [143] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 论文提出了一种独立模型ProMode，用于从文本生成韵律特征（如F0和能量），并应用于TTS等下游任务。通过部分掩码的声学特征和文本输入，模型生成固定长度的潜在韵律嵌入，并在掩码区域预测声学特征。实验表明，该方法在F0和能量预测上优于现有风格编码器，且在TTS系统中表现出更高的韵律偏好。


<details>
  <summary>Details</summary>
Motivation: 韵律包含丰富的情感和语义信息，以及个体特征。现有方法在韵律建模上仍有改进空间，因此需要一种独立模型来更好地生成韵律特征。

Method: ProMode模型通过部分掩码的声学特征和文本输入生成固定长度的韵律嵌入，解码器利用这些嵌入和未掩码文本预测掩码区域的声学特征。

Result: 在GigaSpeech数据集上训练后，模型在F0和能量预测上优于现有方法，且在TTS系统中表现出更高的韵律偏好。

Conclusion: ProMode模型在韵律建模任务中表现出潜力，尤其在需要高质量韵律生成的下游应用中。

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


### [144] [Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning](https://arxiv.org/abs/2508.09803)
*Carlos Franzreb,Arnab Das,Tim Polzehl,Sebastian Möller*

Main category: eess.AS

TL;DR: 论文提出了一种改进的隐私评估方法，通过添加目标分类器来更准确地衡量说话人匿名化中的隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 当前说话人匿名化的隐私评估在使用同性别目标选择算法（TSA）时高估了隐私保护效果，忽略了匿名化语音中同时包含源说话人和目标说话人信息的问题。

Method: 提出添加目标分类器来评估目标说话人信息的影响，并通过对抗学习消除这种影响。

Result: 实验证明该方法对多种匿名化器有效，尤其是在使用同性别TSA时，能提供更可靠的评估。

Conclusion: 该方法显著改进了隐私评估的准确性，特别是在同性别TSA场景下。

Abstract: The current privacy evaluation for speaker anonymization often overestimates
privacy when a same-gender target selection algorithm (TSA) is used, although
this TSA leaks the speaker's gender and should hence be more vulnerable. We
hypothesize that this occurs because the evaluation does not account for the
fact that anonymized speech contains information from both the source and
target speakers. To address this, we propose to add a target classifier that
measures the influence of target speaker information in the evaluation, which
can also be removed with adversarial learning. Experiments demonstrate that
this approach is effective for multiple anonymizers, particularly when using a
same-gender TSA, leading to a more reliable assessment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [145] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: 扩展OWASP多代理系统威胁建模指南，填补现有分类在LLM驱动多代理架构中的安全漏洞，提出新威胁类别和评估策略。


<details>
  <summary>Details</summary>
Motivation: 现有OWASP指南在多代理系统（尤其是LLM驱动架构）中存在安全漏洞，需补充以应对实际部署中的新挑战。

Method: 分析现有威胁模型的不足，提出新威胁类别（如推理链崩溃、度量过拟合等），并设计评估策略（如鲁棒性测试、协调评估）。

Result: 扩展了OWASP框架，增强了对复杂多代理系统的安全覆盖，提升了实际部署中的安全性和弹性。

Conclusion: 通过补充新威胁类别和评估方法，提升了多代理系统的安全性和适应性，适用于更复杂的实际场景。

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>


### [146] [Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective](https://arxiv.org/abs/2508.09541)
*Gang Chen,Guoxin Wang,Anton van Beek,Zhenjun Ming,Yan Yan*

Main category: cs.MA

TL;DR: 本文研究了多智能体自组织系统（MASOS）中依赖层次结构的动态涌现，通过多智能体强化学习（MARL）分析任务执行中的依赖关系及其演化。


<details>
  <summary>Details</summary>
Motivation: MASOS的自组织特性导致其涌现行为具有不可预测性，本文旨在理解任务执行中依赖层次结构的动态形成及其影响因素。

Method: 采用多智能体强化学习（MARL）训练MASOS完成协作推箱任务，通过计算智能体动作对其他智能体状态的梯度量化依赖关系，并分析层次结构的涌现。

Result: 结果表明，依赖层次结构随任务需求动态演化，且其形成受任务环境和网络初始化条件影响。层次结构源于智能体的“天赋”与“努力”在“环境”中的动态交互。

Conclusion: 依赖层次结构是智能体为共同目标自然涌现的结果，而非预设规则或参数的产物，其动态性为MASOS的设计提供了新视角。

Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [147] [A pseudo-inverse of a line graph](https://arxiv.org/abs/2508.09412)
*Sevvandi Kandanaarachchi,Philip Kilby,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 论文研究了线图的小扰动情况下如何恢复对应的根图，提出了一种线性整数规划方法，并通过谱范数证明了其伪逆操作的合理性。


<details>
  <summary>Details</summary>
Motivation: 线图变换通常不可逆，本文旨在解决线图扰动后恢复根图的问题。

Method: 提出一种线性整数规划方法，通过最小化线图的边编辑数来找到对应的根图。

Result: 理论证明和实验验证表明，该方法在Erdős-Rényi图上有效。

Conclusion: 该方法为线图的伪逆操作提供了理论和实践支持。

Abstract: Line graphs are an alternative representation of graphs where each vertex of
the original (root) graph becomes an edge. However not all graphs have a
corresponding root graph, hence the transformation from graphs to line graphs
is not invertible. We investigate the case when there is a small perturbation
in the space of line graphs, and try to recover the corresponding root graph,
essentially defining the inverse of the line graph operation. We propose a
linear integer program that edits the smallest number of edges in the line
graph, that allow a root graph to be found. We use the spectral norm to
theoretically prove that such a pseudo-inverse operation is well behaved.
Illustrative empirical experiments on Erd\H{o}s-R\'enyi graphs show that our
theoretical results work in practice.

</details>


### [148] [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](https://arxiv.org/abs/2508.09623)
*Akshay Thakur,Sawan Kumar,Matthew Zahr,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 论文提出了一种可扩展的概率数值方法，通过随机对偶下降算法和聚类主动学习策略，显著降低了计算成本，适用于大规模或高维PDE问题。


<details>
  <summary>Details</summary>
Motivation: 传统概率数值方法在求解PDE时计算成本高，难以处理大规模或高维问题，因此需要一种更高效的方法。

Method: 结合随机对偶下降算法（降低计算复杂度）和聚类主动学习策略（自适应选择采样点），实现高效求解。

Result: 提出的方法在二维、三维稳态椭圆问题和时空抛物PDE中表现良好，显著提升了计算效率。

Conclusion: 该方法为大规模PDE问题提供了一种高效且可扩展的概率数值解决方案。

Abstract: Solving partial differential equations (PDEs) within the framework of
probabilistic numerics offers a principled approach to quantifying epistemic
uncertainty arising from discretization. By leveraging Gaussian process
regression and imposing the governing PDE as a constraint at a finite set of
collocation points, probabilistic numerics delivers mesh-free solutions at
arbitrary locations. However, the high computational cost, which scales
cubically with the number of collocation points, remains a critical bottleneck,
particularly for large-scale or high-dimensional problems. We propose a
scalable enhancement to this paradigm through two key innovations. First, we
develop a stochastic dual descent algorithm that reduces the per-iteration
complexity from cubic to linear in the number of collocation points, enabling
tractable inference. Second, we exploit a clustering-based active learning
strategy that adaptively selects collocation points to maximize information
gain while minimizing computational expense. Together, these contributions
result in an $h$-adaptive probabilistic solver that can scale to a large number
of collocation points. We demonstrate the efficacy of the proposed solver on
benchmark PDEs, including two- and three-dimensional steady-state elliptic
problems, as well as a time-dependent parabolic PDE formulated in a space-time
setting.

</details>


### [149] [Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA](https://arxiv.org/abs/2508.09721)
*Yuan-Hao Wei,Fu-Hao Deng,Lin-Yong Cui,Yan-Jie Sun*

Main category: stat.ML

TL;DR: SKR-VAE提出了一种更高效的方法，通过结构化核回归在VAE中实现解耦，避免了高斯过程的高计算成本，同时保持了ICA性能。


<details>
  <summary>Details</summary>
Motivation: 提升生成模型的可解释性和可控性，同时解决高斯过程在VAE中计算成本高的问题。

Method: 利用结构化核回归替代高斯过程，对VAE的潜在变量进行解耦，避免核矩阵求逆的高计算负担。

Result: SKR-VAE在保持ICA性能的同时，显著提高了计算效率，降低了计算负担。

Conclusion: SKR-VAE是一种高效且可解释的生成模型改进方法，适用于大规模数据集。

Abstract: The interpretability of generative models is considered a key factor in
demonstrating their effectiveness and controllability. The generated data are
believed to be determined by latent variables that are not directly observable.
Therefore, disentangling, decoupling, decomposing, causal inference, or
performing Independent Component Analysis (ICA) in the latent variable space
helps uncover the independent factors that influence the attributes or features
affecting the generated outputs, thereby enhancing the interpretability of
generative models. As a generative model, Variational Autoencoders (VAEs)
combine with variational Bayesian inference algorithms. Using VAEs, the inverse
process of ICA can be equivalently framed as a variational inference process.
In some studies, Gaussian processes (GPs) have been introduced as priors for
each dimension of latent variables in VAEs, structuring and separating each
dimension from temporal or spatial perspectives, and encouraging different
dimensions to control various attributes of the generated data. However, GPs
impose a significant computational burden, resulting in substantial resource
consumption when handling large datasets. Essentially, GPs model different
temporal or spatial structures through various kernel functions. Structuring
the priors of latent variables via kernel functions-so that different kernel
functions model the correlations among sequence points within different latent
dimensions-is at the core of achieving disentanglement in VAEs. The proposed
Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more
efficient way, avoiding the costly kernel matrix inversion required in GPs.
This research demonstrates that, while maintaining ICA performance, SKR-VAE
achieves greater computational efficiency and significantly reduced
computational burden compared to GP-VAE.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [150] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY是一种基于深度学习的物理层认证方案，用于5G网络，通过隐写术在无线I/Q传输中嵌入签名，实现高精度设备认证。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在无线网络中存在效率和安全问题，物理层认证（PLA）利用通信介质的固有特性提供高效认证。深度学习的发展使PLA更准确可靠。

Method: VeriPHY通过高斯混合模型生成伪随机签名，并将其嵌入5G网络的I/Q传输中，利用深度神经网络识别和认证设备。

Result: VeriPHY在签名更新频率为20毫秒时，识别精度达93%-100%，误报率低，推理时间为28毫秒。其隐写模式能保持93%以上的检测准确率。

Conclusion: VeriPHY为5G网络提供了一种高效、高精度的物理层认证方案，结合深度学习和隐写术，显著提升了设备认证的安全性和可靠性。

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


### [151] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: 提出了一种新的社交传感器云身份克隆检测方法，结合相似身份检测和加密认证协议，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近年来社交传感器云身份克隆事件频发，现有方法性能不足且缺乏大规模真实数据集验证。

Method: 1) 使用弱监督深度森林模型检测相似身份；2) 设计加密认证协议验证身份来源。

Result: 在大规模真实数据集上验证了方法的可行性和优越性能。

Conclusion: 该方法有效解决了身份克隆检测问题，性能优于现有技术。

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [152] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: 论文提出了一种结合关键词特征与传统特征的新方法，用于增强机器学习算法检测钓鱼URL的能力，实验表明该方法显著降低了分类错误率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益增多，导致用户敏感信息泄露和财务损失，早期检测钓鱼URL至关重要。

Method: 提出了一种结合关键词特征与传统特征的新方法，应用于多种传统机器学习算法。

Result: 该方法在大数据集上平均减少30%的分类错误，对小数据集效果更显著，最佳准确率达99.68%。

Conclusion: 该方法有效提升了钓鱼URL检测的准确性，且不依赖第三方服务提供的信息。

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [153] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种基于多任务学习框架的个性化产品搜索排序优化模型，结合表格与非表格数据，利用TinyBERT和新型采样技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化产品搜索排序中混合数据类型处理的挑战，并优化模型性能。

Method: 整合表格与非表格数据，使用预训练TinyBERT生成语义嵌入，结合新型采样技术和多任务学习框架。

Result: 实验表明，该方法显著优于基线模型（如XGBoost、TabNet等），并通过消融研究验证了各模块的有效性。

Conclusion: 该方法在个性化产品搜索排序中表现优异，证明了多任务学习与高级嵌入技术的结合优势。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [154] [Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data](https://arxiv.org/abs/2508.09243)
*Sebastian Kot*

Main category: econ.GN

TL;DR: 论文通过主成分分析（PCA）研究现代重商主义，揭示其与全球化范式的差异，并利用文本分析量化保护主义和技术主权等动态。


<details>
  <summary>Details</summary>
Motivation: 探讨现代重商主义对全球化范式的颠覆性影响，提供数据驱动的量化分析方法。

Method: 使用PCA分析768维SBERT生成的新闻语义嵌入，提取与保护主义和技术主权相关的潜在因子。

Result: 通过主成分载荷识别关键语义特征，提升分类性能和预测准确性。

Conclusion: 该方法为高维文本分析提供了可扩展的框架，用于追踪重商主义动态。

Abstract: This paper examines Modern Mercantilism, characterized by rising economic
nationalism, strategic technological decoupling, and geopolitical
fragmentation, as a disruptive shift from the post-1945 globalization paradigm.
It applies Principal Component Analysis (PCA) to 768-dimensional
SBERT-generated semantic embeddings of curated news articles to extract
orthogonal latent factors that discriminate binary event outcomes linked to
protectionism, technological sovereignty, and bloc realignments. Analysis of
principal component loadings identifies key semantic features driving
classification performance, enhancing interpretability and predictive accuracy.
This methodology provides a scalable, data-driven framework for quantitatively
tracking emergent mercantilist dynamics through high-dimensional text analytics

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [155] [Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery](https://arxiv.org/abs/2508.09183)
*Farzan Moosavi,Bilal Farooq*

Main category: quant-ph

TL;DR: 论文提出了一种结合强化学习和参数化量子电路的框架，用于解决大规模CPDPTW问题，展示了量子计算在优化问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 经典方法在大规模组合优化问题中难以处理，量子计算为解决此类问题提供了新途径。

Method: 设计了基于强化学习和参数化量子电路的框架，提出了问题特定的量子编码电路，并对比了PPO和QSVT方法。

Result: 数值实验表明，所提方法在解决方案规模和训练复杂度上优于对比方法，同时考虑了现实约束。

Conclusion: 量子计算结合强化学习为解决大规模CPDPTW问题提供了高效且可扩展的方案。

Abstract: Quantum computation has demonstrated a promising alternative to solving the
NP-hard combinatorial problems. Specifically, when it comes to optimization,
classical approaches become intractable to account for large-scale solutions.
Specifically, we investigate quantum computing to solve the large-scale
Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this
regard, a Reinforcement Learning (RL) framework augmented with a Parametrized
Quantum Circuit (PQC) is designed to minimize the travel time in a realistic
last-mile on-demand delivery. A novel problem-specific encoding quantum circuit
with an entangling and variational layer is proposed. Moreover, Proximal Policy
Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are
designed for comparison through numerical experiments, highlighting the
superiority of the proposed method in terms of the scale of the solution and
training complexity while incorporating the real-world constraints.

</details>


### [156] [Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks](https://arxiv.org/abs/2508.09209)
*Kun Ming Goh*

Main category: quant-ph

TL;DR: 该研究探讨了混合量子-经典生成对抗网络（HQCGANs）的可行性，通过量子电路生成潜在向量，并与经典GAN性能对比。结果显示7-qubit HQCGAN表现接近经典GAN，验证了在NISQ时代量子电路作为潜在先验的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索量子电路在生成对抗网络中的应用潜力，特别是在当前量子硬件限制下，是否能提升生成模型性能。

Method: 采用混合量子-经典GAN架构，量子生成器通过参数化量子电路生成潜在向量，经典判别器进行判别。对比了经典GAN与3、5、7-qubit HQCGAN的性能。

Result: 经典GAN表现最佳，但7-qubit HQCGAN在后期表现接近，3-qubit模型因收敛限制表现较差。量子采样开销仅导致训练时间适度增加。

Conclusion: 研究表明，在NISQ时代，量子电路可作为GAN的潜在先验，为生成模型提供了新的可能性。

Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm
for producing high-fidelity data samples, yet their performance is constrained
by the quality of latent representations, typically sampled from classical
noise distributions. This study investigates hybrid quantum-classical GANs
(HQCGANs) in which a quantum generator, implemented via parameterised quantum
circuits, produces latent vectors for a classical discriminator. We evaluate a
classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using
Qiskit's AerSimulator with realistic noise models to emulate near-term quantum
devices. The binary MNIST dataset (digits 0 and 1) is used to align with the
low-dimensional latent spaces imposed by current quantum hardware. Models are
trained for 150 epochs and assessed with Frechet Inception Distance (FID) and
Kernel Inception Distance (KID). Results show that while the classical GAN
achieved the best scores, the 7-qubit HQCGAN produced competitive performance,
narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier
convergence limitations. Efficiency analysis indicates only moderate training
time increases despite quantum sampling overhead. These findings validate the
feasibility of noisy quantum circuits as latent priors in GAN architectures,
highlighting their potential to enhance generative modelling within the
constraints of the noisy intermediate-scale quantum (NISQ) era.

</details>


### [157] [On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators](https://arxiv.org/abs/2508.09844)
*Jasmin Frkatovic,Akash Malemath,Ivan Kankeu,Yannick Werner,Matthias Tschöpe,Vitor Fortes Rey,Sungho Suh,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: QGANs在图像生成任务中表现有限，难以泛化到不同数据集，仅能收敛于训练数据的平均表示。纯态输出时，生成器与目标数据分布之间的保真度限制了判别器质量。


<details>
  <summary>Details</summary>
Motivation: 研究量子生成对抗网络（QGANs）在图像生成任务中的能力，探索其局限性及其对相关量子生成模型的广泛影响。

Method: 通过数值测试当前主要架构，分析纯态输出时的理论下限。

Result: QGANs泛化能力有限，生成器输出与目标分布的保真度限制了判别器质量。

Conclusion: 现有量子生成模型在泛化能力上存在根本性挑战，结果对相关量子模型具有广泛意义。

Abstract: We investigate the capabilities of Quantum Generative Adversarial Networks
(QGANs) in image generations tasks. Our analysis centers on fully quantum
implementations of both the generator and discriminator. Through extensive
numerical testing of current main architectures, we find that QGANs struggle to
generalize across datasets, converging on merely the average representation of
the training data. When the output of the generator is a pure-state, we
analytically derive a lower bound for the discriminator quality given by the
fidelity between the pure-state output of the generator and the target data
distribution, thereby providing a theoretical explanation for the limitations
observed in current models. Our findings reveal fundamental challenges in the
generalization capabilities of existing quantum generative models. While our
analysis focuses on QGANs, the results carry broader implications for the
performance of related quantum generative models.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [158] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD是一个基于深度学习的端到端图像数据处理框架，用于快速识别血细胞聚集体，解决了传统方法的数据存储和处理难题。


<details>
  <summary>Details</summary>
Motivation: 传统流式细胞仪无法有效识别血细胞聚集体，而定量相位成像流式细胞术的数据存储和处理问题限制了临床应用。

Method: RT-HAD结合物理一致的全息重建和检测技术，将每个血细胞表示为图结构以识别聚集体。

Result: RT-HAD能在1.5分钟内实时处理30GB图像数据，血小板聚集体检测错误率为8.9%。

Conclusion: RT-HAD解决了即时诊断中的大数据挑战，显著提升了无标记功能诊断的效率。

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


### [159] [Exploring Molecular Odor Taxonomies for Structure-based Odor Predictions using Machine Learning](https://arxiv.org/abs/2508.09217)
*Akshay Sajan,Stijn Sluis,Reza Haydarlou,Sanne Abeln,Pasquale Lisena,Raphael Troncy,Caro Verbeek,Inger Leemans,Halima Mouhib*

Main category: q-bio.QM

TL;DR: 论文提出通过专家和数据驱动的气味分类法改进机器学习模型对气味预测的性能，并评估了两种分类法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决从分子结构预测气味的挑战，尤其是气味空间理解的不足和结构-气味关系的复杂性。

Method: 使用基于语义和感知相似性的专家分类法，以及基于气味描述符共现模式的数据驱动分类法，改进机器学习模型。

Result: 两种分类法均提升了预测性能，优于随机分组，数据驱动分类法有助于评估专家分类法并理解气味空间。

Conclusion: 提供了两种分类法和完整数据集，为未来气味分子基础研究奠定基础。

Abstract: One of the key challenges to predict odor from molecular structure is
unarguably our limited understanding of the odor space and the complexity of
the underlying structure-odor relationships. Here, we show that the predictive
performance of machine learning models for structure-based odor predictions can
be improved using both, an expert and a data-driven odor taxonomy. The expert
taxonomy is based on semantic and perceptual similarities, while the
data-driven taxonomy is based on clustering co-occurrence patterns of odor
descriptors directly from the prepared dataset. Both taxonomies improve the
predictions of different machine learning models and outperform random
groupings of descriptors that do not reflect existing relations between odor
descriptors. We assess the quality of both taxonomies through their predictive
performance across different odor classes and perform an in-depth error
analysis highlighting the complexity of odor-structure relationships and
identifying potential inconsistencies within the taxonomies by showcasing pear
odorants used in perfumery. The data-driven taxonomy allows us to critically
evaluate our expert taxonomy and better understand the molecular odor space.
Both taxonomies as well as a full dataset are made available to the community,
providing a stepping stone for a future community-driven exploration of the
molecular basis of smell. In addition, we provide a detailed multi-layer expert
taxonomy including a total of 777 different descriptors from the Pyrfume
repository.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [160] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: 论文提出了PETLP框架，整合GDPR、版权法和平台条款，为社交媒体数据研究提供统一合规指导。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能整合不同法规领域，导致研究人员缺乏统一指导。

Method: 引入PETLP框架，将法律保障嵌入ETL流程，动态更新数据保护影响评估。

Result: 通过Reddit分析，揭示研究机构与商业实体在数据提取权利上的差异，以及GDPR的普遍适用性。

Conclusion: PETLP通过结构化合规决策和简化数据管理计划，帮助研究人员应对法规复杂性。

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [161] [Classifying Cool Dwarfs: Comprehensive Spectral Typing of Field and Peculiar Dwarfs Using Machine Learning](https://arxiv.org/abs/2508.09370)
*Tianxing Zhou,Christopher A. Theissen,S. Jean Feeser,William M. J. Best,Adam J. Burgasser,Kelle L. Cruz,Lexu Zhao*

Main category: astro-ph.SR

TL;DR: 论文探讨了机器学习在低质量恒星和褐矮星光谱分类中的应用，特别是针对M0-T9型矮星的低分辨率近红外光谱，比较了不同模型的性能，并分析了光谱区域对分类的重要性。


<details>
  <summary>Details</summary>
Motivation: 低质量恒星和褐矮星在恒星和亚恒星研究中具有重要意义，但目前分类仍依赖人工方法，机器学习为大规模光谱数据提供了自动化解决方案。

Method: 使用随机森林（RF）、支持向量机（SVM）和K近邻（KNN）模型，以分箱通量为输入特征，测试不同归一化方法及光谱区域的重要性。

Result: 最佳模型（KNN）在±1光谱类型内分类准确率为95.5±0.6%，表面重力和金属丰度子类分类准确率为89.5±0.9%。信噪比≥60时准确率≥95%。

Conclusion: 机器学习可有效分类低分辨率光谱，zy波段和FeH、TiO特征在分类中起关键作用。

Abstract: Low-mass stars and brown dwarfs -- spectral types (SpTs) M0 and later -- play
a significant role in studying stellar and substellar processes and
demographics, reaching down to planetary-mass objects. Currently, the
classification of these sources remains heavily reliant on visual inspection of
spectral features, equivalent width measurements, or narrow-/wide-band spectral
indices. Recent advances in machine learning (ML) methods offer automated
approaches for spectral typing, which are becoming increasingly important as
large spectroscopic surveys such as Gaia, SDSS, and SPHEREx generate datasets
containing millions of spectra. We investigate the application of ML in
spectral type classification on low-resolution (R $\sim$ 120) near-infrared
spectra of M0--T9 dwarfs obtained with the SpeX instrument on the NASA Infrared
Telescope Facility. We specifically aim to classify the gravity- and
metallicity-dependent subclasses for late-type dwarfs. We used binned fluxes as
input features and compared the efficacy of spectral type estimators built
using Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor
(KNN) models. We tested the influence of different normalizations and analyzed
the relative importance of different spectral regions for surface gravity and
metallicity subclass classification. Our best-performing model (using KNN)
classifies 95.5 $\pm$ 0.6% of sources to within $\pm$1 SpT, and assigns surface
gravity and metallicity subclasses with 89.5 $\pm$ 0.9% accuracy. We test the
dependence of signal-to-noise ratio on classification accuracy and find sources
with SNR $\gtrsim$ 60 have $\gtrsim$ 95% accuracy. We also find that zy-band
plays the most prominent role in the RF model, with FeH and TiO having the
highest feature importance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和行为树的新型框架，用于实现自然语言指令到机器人动作的转换，提升了人机交互的直观性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着智能机器人在人类环境中的普及，需要更直观、可靠且适应性强的人机交互（HRI）界面。传统方法要求用户适应界面或记忆命令，限制了在动态环境中的可用性。

Method: 通过将LLMs与行为树结合，开发了一个框架，支持自然语言指令的解析和执行，并利用领域特定插件实现模块化扩展。

Result: 实验表明，系统在真实场景中表现优异，认知到执行的准确率平均达94%。

Conclusion: 该框架为人机交互系统提供了实用且高效的解决方案，代码已开源。

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [163] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: 本文提出了一种通用行为克隆（GBC）框架，通过自适应数据管道、DAgger-MMPPO算法和开源平台，解决了人形机器人数据处理的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 人形机器人开发中数据处理和学习算法的碎片化问题阻碍了通用性，GBC旨在提供一个统一的端到端解决方案。

Method: GBC通过自适应数据管道（利用可微分IK网络）、DAgger-MMPPO算法和开源平台（基于Isaac Lab）实现人形机器人动作的通用学习。

Result: 在多种异构人形机器人上验证了GBC的优异性能和动作迁移能力。

Conclusion: GBC首次实现了真正通用的人形机器人控制器，为领域提供了实用的统一解决方案。

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [164] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D是一款用于交互式分析X射线CT图像中区域的软件工具，旨在改进相位识别、考虑部分体积效应、提高检测限和量化准确性，并统一跨学科的3D定量分析。


<details>
  <summary>Details</summary>
Motivation: X射线CT成像技术存在固有成像伪影（如束硬化和部分体积效应），导致用户需基于体素灰度值做出多项决策来分割和分类微观结构。ARI3D旨在解决这些问题，简化分析流程。

Method: 提出ARI3D软件工具，通过交互式分析3D X射线CT图像中的区域，协助用户完成分类和量化协议中的各个步骤。

Result: ARI3D能够改进相位识别、处理部分体积效应、提高检测限和量化准确性，并支持跨学科的统一3D定量分析。

Conclusion: ARI3D为X射线CT图像的定量分析提供了高效工具，解决了现有技术中的挑战，并具有广泛的适用性。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [165] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究比较了多种图像到图像转换模型，发现C-GAN在动漫角色草图着色任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决动漫行业中草图着色耗时且成本高的问题。

Method: 评估了Neural Style Transfer、C-GAN和CycleGAN等模型。

Result: C-GAN能生成接近人类创作的高质量、高分辨率图像。

Conclusion: C-GAN是动漫草图着色任务中最有效的模型。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [166] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种新颖的输入自适应导航方法，通过三种算法提升视觉与语言导航（VLN）模型的效率，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有输入自适应机制在减少计算量时性能下降严重，需解决这一问题。

Method: 引入三种自适应算法：选择性处理全景视图、基于重要性的自适应阈值和缓存机制。

Result: 在七个VLN基准测试中，计算量减少超过2倍，性能无明显下降。

Conclusion: 该方法有效提升了VLN模型的效率，适用于计算资源有限的场景。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [167] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: FusionEnsemble-Net是一种基于注意力的时空网络集成方法，通过动态融合视觉和运动数据提升手语识别精度，测试准确率达99.44%。


<details>
  <summary>Details</summary>
Motivation: 解决医疗沟通中复杂多模态手势识别的挑战。

Method: 提出FusionEnsemble-Net，同步处理RGB视频和雷达数据，通过四种时空网络和注意力融合模块动态融合特征，最终通过集成分类器输出结果。

Result: 在意大利手语数据集MultiMeDaLIS上达到99.44%的测试准确率，优于现有方法。

Conclusion: 基于注意力融合的多样化时空网络集成方法为复杂多模态手势识别提供了鲁棒且准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [168] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种双架构框架解决连续手语识别中的签名者独立性和未见句子问题，实验表明性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 连续手语识别面临签名者间差异大和未见句子泛化能力差的问题，传统方法效果不佳。

Method: 使用签名者不变Conformer和多尺度融合Transformer，分别处理签名者独立性和未见句子任务。

Result: 在Isharah-1000数据集上，签名者不变任务WER降至13.07%，未见句子任务WER为47.78%，性能优于现有方法。

Conclusion: 任务特定网络设计显著提升了连续手语识别性能，为后续研究设定了新基准。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [169] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 论文研究了医学图像分割中注释者间差异与皮肤病变恶性程度的关系，并利用多任务学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割存在注释者间差异，尤其是边界模糊的病变（如恶性皮肤病变），研究这些差异及其与恶性程度的关系对临床诊断有重要意义。

Method: 构建了IMA++数据集，分析注释者、恶性程度等因素对分割差异的影响，并利用多任务学习将注释者间一致性作为软特征。

Result: 发现注释者间一致性与恶性程度显著相关（p<0.001），并成功预测一致性（MAE=0.108），多任务学习使模型性能提升4.2%。

Conclusion: 注释者间一致性可作为临床特征提升模型性能，为医学图像分割提供了新思路。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [170] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD是一种新颖的知识蒸馏框架，通过逆向知识转移解决高光谱遥感中基础模型应用的挑战，显著提升了表示学习和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在高光谱遥感中直接应用存在光谱差异和数据稀缺的挑战，需要一种有效的方法来迁移学习表示。

Method: HyperKD采用基于特征的知识蒸馏策略，包括光谱范围对齐、空间特征引导掩码和增强损失函数，从教师模型向学生模型迁移知识。

Result: 实验表明，HyperKD显著提升了表示学习效果，增强了重建保真度，并在土地覆盖分类、作物类型识别等下游任务中表现更鲁棒。

Conclusion: HyperKD成功解决了高光谱遥感中的逆向域适应问题，展示了知识蒸馏框架在遥感分析中的潜力。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [171] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种基于局部曲率特征的快速、准确的分子对接方法，通过整合几何信息改进了现有深度学习方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在分子对接中忽略了关键的几何信息，导致口袋定位和结合构象不准确。

Method: CWFBind结合局部曲率描述符和度感知权重机制，改进了特征提取和消息传递过程，并采用动态半径策略和增强损失函数解决类别不平衡问题。

Result: CWFBind在多个对接基准测试中表现出色，实现了准确性和效率的平衡。

Conclusion: CWFBind通过几何信息的整合和优化策略，显著提升了分子对接的准确性。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [172] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 论文提出了一种结合ProGAN和SAGAN的GAN变体，用于生成高质量、高分辨率的印度手语图像，并在IS和FID指标上优于传统ProGAN。


<details>
  <summary>Details</summary>
Motivation: 手语是听障人士的重要交流方式，但目前手语生成领域仍需探索。现有模型在分辨率和细节上存在不足，需要改进。

Method: 结合Progressive Growing GAN和Self-Attention GAN的优点，开发了一种新的GAN变体，用于生成特征丰富、高分辨率的印度手语图像。

Result: 新模型在IS和FID指标上分别提升了3.2和30.12，并发布了包含印度手语字母、数字和129个单词的高质量数据集。

Conclusion: 提出的模型在手语图像生成上表现优异，为听障人士的交流提供了更好的工具。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [173] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL是一个利用语义引导数据压缩的新型框架，通过结构修剪胸部X光片并转换为图表示，显著减少数据大小，同时保持高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像数据的快速增长在资源受限的临床环境中带来了存储和传输挑战。

Method: 利用生成式视觉语言模型的交叉注意力分数对胸部X光片进行结构修剪，生成压缩的图表示，并与临床报告的知识图融合。

Result: 在MIMIC-CXR和CheXpert Plus数据集上，NEURAL实现了93.4-97.7%的数据压缩，诊断性能AUC为0.88-0.95，优于基线模型。

Conclusion: NEURAL解决了数据大小与临床效用之间的权衡，支持高效工作流程和远程放射学，且性能不降低。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [174] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 提出了一种基于sheaf的框架，用于融合MRI和组织病理学数据，解决了现有方法在多模态数据融合中的局限性，并在数据缺失情况下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤的分子亚型分类需要侵入性组织提取，现有多模态方法在保留跨模态结构信息和处理不完整数据方面存在不足。

Method: 采用sheaf-based框架，实现MRI和组织病理学数据的结构感知和一致性融合。

Result: 模型优于基线方法，在数据不完整或缺失情况下表现鲁棒。

Conclusion: 该框架为快速诊断的虚拟活检工具开发提供了支持。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [175] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: TRACE框架通过将3D点建模为刚性粒子，直接学习其平移旋转动力学系统，无需人工标签即可建模复杂动态3D场景的物理运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖物理模型或标签，难以学习复杂运动物理。TRACE旨在无标签条件下建模3D场景的几何、外观和物理信息。

Method: 将3D点视为刚性粒子，学习其平移旋转动力学系统，显式估计物理参数以控制粒子运动。

Result: 在多个动态数据集上表现优异，尤其在未来帧外推任务中优于基线方法。

Conclusion: TRACE不仅能建模复杂物理运动，还能通过聚类物理参数轻松分割多物体或部件。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [176] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RayletDF的新方法，用于从点云或3D高斯中重建3D表面，通过射线距离场直接预测表面点。


<details>
  <summary>Details</summary>
Motivation: 现有基于坐标的方法在渲染显式表面时计算量大，需要更高效的解决方案。

Method: 采用三个关键模块：射线特征提取器、射线距离场预测器和多射线混合器，以提取几何特征、预测距离并聚合结果。

Result: 在多个公开数据集上表现优异，具有出色的泛化能力，能在未见数据集上单次前向传播重建表面。

Conclusion: RayletDF方法高效且泛化能力强，适用于3D表面重建。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [177] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: Stable Diffusion模型可通过自注意力层改造实现视觉上下文学习（V-ICL），无需额外训练即可适应多种视觉任务。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用现成的Stable Diffusion模型实现视觉上下文学习，避免复杂训练和数据限制。

Method: 在Stable Diffusion的自注意力层中引入上下文重计算机制，显式结合查询与示例提示。

Result: 模型在六种视觉任务中表现优异，如前景分割任务在Pascal-5i数据集上mIoU提升8.9%。

Conclusion: 该方法无需微调即可高效适应多任务，并通过集成多提示进一步提升性能。

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [178] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board是一个无需训练的框架，用于从自然语言生成富有表现力的故事板。它通过轻量级一致性框架（包括潜在面板锚定和互惠注意力值混合）增强连贯性，无需架构更改或微调。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注主题身份，忽略了视觉叙事的关键方面（如空间构图、背景演变和叙事节奏）。

Method: 引入轻量级一致性框架，结合潜在面板锚定和互惠注意力值混合，利用现成语言模型将自由故事转换为面板级提示。

Result: 在Rich Storyboard Benchmark上表现优异，生成的故事板在动态性、连贯性和叙事吸引力上优于现有基线。

Conclusion: Story2Board能够生成视觉多样且一致的故事板，为视觉叙事提供了更有效的工具。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [179] [Distributed Diamond Formation of Sliding Squares](https://arxiv.org/abs/2508.09638)
*Irina Kostitsyna,David Liedtke,Christian Scheideler*

Main category: cs.CG

TL;DR: 提出一种分布式算法，使方形模块机器人能够从任意连接配置重构为钻石形状，保持连通性且仅需最小假设。


<details>
  <summary>Details</summary>
Motivation: 研究自重构机器人系统，解决现有方法对滑动方形移动集的限制问题。

Method: 使用未修改的移动集，提出顺序和并行算法，基于局部连通性决策。

Result: 顺序算法时间复杂度为O(n²)，并行变体显著加速，第二变体平均线性时间。

Conclusion: 算法在保持连通性和最小假设下高效完成重构，并行变体性能优越。

Abstract: The sliding square model is a widely used abstraction for studying
self-reconfigurable robotic systems, where modules are square-shaped robots
that move by sliding or rotating over one another. In this paper, we propose a
novel distributed algorithm that allows a group of modules to reconfigure into
a diamond shape, starting from an arbitrary side-connected configuration. It is
connectivity-preserving and operates under minimal assumptions: one leader
module, common chirality, constant memory per module, and visibility and
communication restricted to immediate neighbors. Unlike prior work, which
relaxes the original sliding square move-set, our approach uses the unmodified
move-set, addressing the additional challenge of handling locked
configurations. Our algorithm is sequential in nature and operates with a
worst-case time complexity of $\mathcal{O}(n^2)$ rounds, which is optimal for
sequential algorithms. To improve runtime, we introduce two parallel variants
of the algorithm. Both rely on a spanning tree data structure, allowing modules
to make decisions based on local connectivity. Our experimental results show a
significant speedup for the first variant, and linear average runtime for the
second variant, which is worst-case optimal for parallel algorithms.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [180] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: RadioMamba是一种混合Mamba-UNet架构，用于解决无线电地图构建中的精度与效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线电地图构建方法存在精度与效率的权衡问题，无法同时满足实时性和准确性需求。

Method: 采用Mamba-卷积块，Mamba分支以线性复杂度捕获全局空间依赖，卷积分支提取局部特征，实现全局与局部特征的结合。

Result: RadioMamba在精度上优于现有方法（包括扩散模型），速度快20倍，且仅需2.9%的模型参数。

Conclusion: RadioMamba通过提升精度和效率，为下一代无线系统的实时智能优化提供了可行方案。

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


### [181] [3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator](https://arxiv.org/abs/2508.09708)
*Thomas Fehrenbach,Luis Omar Ortiz Abrego,Cornelius Hellge,Thomas Schierl,Jörg Ott*

Main category: eess.SP

TL;DR: 论文研究了V2X通信中的群组调度（Mode 2d）对车辆编队行驶的无线通信性能提升效果。


<details>
  <summary>Details</summary>
Motivation: 车辆编队行驶（platooning）是V2X通信的重要应用，但其对无线通信的高可靠性和低延迟提出了挑战。

Method: 采用分布式和预定的资源分配方案（Mode 2d），车辆群组从配置的资源池中选择资源，无需网络辅助。

Result: 仿真结果表明，该方案能满足编队行驶对可靠性、低延迟和数据速率的要求。

Conclusion: 群组调度（Mode 2d）是一种有效的资源分配方案，适用于V2X通信中的车辆编队行驶。

Abstract: Vehicle-to-everything (V2X) communication is a key technology for enabling
intelligent transportation systems (ITS) that can improve road safety, traffic
efficiency, and environmental sustainability. Among the various V2X
applications, platooning is one of the most promising ones, as it allows a
group of vehicles to travel closely together at high speeds, reducing fuel
consumption and emissions. However, it poses significant challenges for
wireless communication, such as high reliability and low latency. In this
paper, we evaluate the benefits of group scheduling, also referred to as Mode
2d, which is based on a distributed and scheduled resource allocation scheme
that allows the group of cars to select resources from a configured pool
without network assistance. We evaluated the scheme through simulations, and
the results show that this approach can meet the reliability, low latency, and
data rate requirements for platooning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 论文提出Columbo方法，显著提升表格列名缩写扩展的准确性，优于现有最佳方法4-29%。


<details>
  <summary>Details</summary>
Motivation: 解决表格列名缩写扩展问题，因现有方法在合成数据和准确性评估上存在不足。

Method: 引入4个真实数据集，提出新准确性评估方法，开发基于LLM的Columbo解决方案。

Result: Columbo在5个数据集上显著优于现有最佳方法NameGuess。

Conclusion: Columbo在实际应用中表现优异，已在环境科学数据门户EDI中投入使用。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [183] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 论文探讨了通过调整解码温度提升语言模型多样性的方法，发现降低温度可提高质量（精确度），而提高温度未必能提升覆盖率（召回率）。作者提出基于精确度-召回率框架重新设计损失函数，以优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型多样性的提升是一个重要但具挑战性的目标，传统方法（如调整解码温度）效果有限，需探索更有效的方法。

Method: 通过分析温度调整对模型性能的影响，提出基于精确度-召回率框架的损失函数设计方法。

Result: 新方法在精确度与召回率之间实现了更好的平衡，优于传统的负对数似然训练结合温度调整。

Conclusion: 研究为语言模型提供了更灵活和鲁棒的优化路径，强调了损失函数设计的重要性。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [184] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO通过训练时采样更多组并基于长度和奖励效率过滤响应，有效减少语言模型为追求准确性而过度延长回答的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在强化学习中倾向于通过增加回答长度来提升准确性，导致许多冗余内容。

Method: 引入GFPO方法，训练时采样更多组，基于长度和奖励效率过滤响应，并动态分配资源到难题。

Result: 在多个基准测试中，GFPO将长度膨胀减少46-85%，同时保持准确性。

Conclusion: GFPO证明增加训练计算可直接减少测试计算，实现高效推理。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [185] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一种多维度评估框架，用于系统比较大型语言模型（LLM）的对齐技术，涵盖检测、质量、计算效率和鲁棒性四个维度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的普及，确保其输出符合人类价值观和安全标准变得至关重要，但目前缺乏统一的评估框架来系统比较不同对齐方法。

Method: 提出一个多维度评估框架，从对齐检测、对齐质量、计算效率和鲁棒性四个维度评估主流对齐技术。

Result: 实验表明，该框架能有效识别当前先进模型的优势和局限性，为未来研究提供方向。

Conclusion: 该框架为系统评估和比较LLM对齐技术提供了实用工具，有助于指导部署决策和未来研究。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [186] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 研究比较了通用、医学和领域特定分词器在放射学报告摘要任务中的表现，发现领域特定分词器在性能和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索分词器对语言模型在放射学文本生成质量中的影响，填补该领域的研究空白。

Method: 系统比较通用、医学和领域特定分词器在三种成像模态上的表现，并研究有无预训练的影响。

Result: 医学和领域特定分词器在从头训练时表现优于通用分词器，预训练部分缩小了性能差距，领域特定分词器表现最优且计算效率更高。

Conclusion: 适应临床领域的分词器能提升性能并降低计算需求，有助于研究和实际医疗应用。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [187] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出了一种基于神经上下文老虎机的算法，用于动态选择适合不同子任务的大语言模型（LLM）序列，以优化任务完成效果和成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，如何高效选择适合特定子任务的LLM序列成为关键问题，尤其是在复杂任务中，单个LLM可能无法胜任。

Method: 采用神经上下文老虎机算法，在线训练神经网络模型，动态学习每个子任务的LLM选择策略。

Result: 在电信问答和医疗诊断预测数据集上验证了算法的有效性，优于其他LLM选择方法。

Conclusion: 该算法能够动态优化LLM序列选择，适应复杂任务需求，提高任务成功率和成本效率。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [188] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 提出了一种新颖的联邦学习方法，利用模型不确定性进行聚合和预测不确定性进行推理，实现了跨多样腹部CT数据集的通用分割。


<details>
  <summary>Details</summary>
Motivation: 解决不同CT数据集因扫描器和设置差异导致的标签不统一问题，同时保护患者隐私。

Method: 利用随机小批量梯度下降的噪声估计模型权重分布，采用贝叶斯逆方差聚合方案，并通过传播模型权重的不确定性量化预测不确定性。

Result: 实验表明，该方法在联邦聚合质量和不确定性加权推理方面优于现有基线。

Conclusion: 该方法有效提升了跨数据集分割的通用性和临床决策的可靠性。

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [189] [A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.09271)
*Reihaneh Hassanzadeh,Anees Abrol,Hamid Reza Hassanzadeh,Vince D. Calhoun*

Main category: eess.IV

TL;DR: 提出了一种生成对抗网络方法，用于从现有模态重建缺失的神经影像数据，提高了阿尔茨海默病分类准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据分析能提高脑部疾病诊断准确性，但数据缺失是主要挑战。传统方法可能降低准确性或引入偏差，需要更有效的解决方案。

Method: 使用生成对抗网络（GAN）从T1加权结构磁共振成像和功能网络连接中重建缺失模态，保留疾病模式。

Result: 与传统方法相比，生成填补方法在阿尔茨海默病与认知正常组的分类准确性上提高了9%。

Conclusion: 生成对抗网络是解决神经影像数据缺失问题的有效方法，能显著提升诊断准确性。

Abstract: Multimodal data analysis can lead to more accurate diagnoses of brain
disorders due to the complementary information that each modality adds.
However, a major challenge of using multimodal datasets in the neuroimaging
field is incomplete data, where some of the modalities are missing for certain
subjects. Hence, effective strategies are needed for completing the data.
Traditional methods, such as subsampling or zero-filling, may reduce the
accuracy of predictions or introduce unintended biases. In contrast, advanced
methods such as generative models have emerged as promising solutions without
these limitations. In this study, we proposed a generative adversarial network
method designed to reconstruct missing modalities from existing ones while
preserving the disease patterns. We used T1-weighted structural magnetic
resonance imaging and functional network connectivity as two modalities. Our
findings showed a 9% improvement in the classification accuracy for Alzheimer's
disease versus cognitive normal groups when using our generative imputation
method compared to the traditional approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [190] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用先前任务的紧凑表格Q值作为可转移知识库，提升学习效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在DRL中，VFI面临状态-动作空间连续、神经网络近似噪声和存储过去模型不切实际的挑战，DQInit旨在解决这些问题。

Method: DQInit通过基于已知度的机制，将转移的值软性整合到未探索区域，并逐步转向代理的学习估计，避免固定时间衰减的限制。

Result: 实验表明，DQInit在多个连续控制任务中显著提高了早期学习效率、稳定性和整体性能。

Conclusion: DQInit为DRL中的知识转移提供了新视角，仅依赖值估计而非策略或演示，结合了跳启动RL和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>
