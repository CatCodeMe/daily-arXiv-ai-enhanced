{"id": "2506.21811", "pdf": "https://arxiv.org/pdf/2506.21811", "abs": "https://arxiv.org/abs/2506.21811", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "title": "Revisiting Graph Analytics Benchmark", "categories": ["cs.DB", "cs.GR"], "comment": null, "summary": "The rise of graph analytics platforms has led to the development of various\nbenchmarks for evaluating and comparing platform performance. However, existing\nbenchmarks often fall short of fully assessing performance due to limitations\nin core algorithm selection, data generation processes (and the corresponding\nsynthetic datasets), as well as the neglect of API usability evaluation. To\naddress these shortcomings, we propose a novel graph analytics benchmark.\nFirst, we select eight core algorithms by extensively reviewing both academic\nand industrial settings. Second, we design an efficient and flexible data\ngenerator and produce eight new synthetic datasets as the default datasets for\nour benchmark. Lastly, we introduce a multi-level large language model\n(LLM)-based framework for API usability evaluation-the first of its kind in\ngraph analytics benchmarks. We conduct comprehensive experimental evaluations\non existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and\nG-thinker). The experimental results demonstrate the superiority of our\nproposed benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u6838\u5fc3\u7b97\u6cd5\u9009\u62e9\u3001\u6570\u636e\u751f\u6210\u548cAPI\u53ef\u7528\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u5728\u6838\u5fc3\u7b97\u6cd5\u9009\u62e9\u3001\u6570\u636e\u751f\u6210\u548cAPI\u53ef\u7528\u6027\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5e73\u53f0\u6027\u80fd\u3002", "method": "1. \u901a\u8fc7\u5e7f\u6cdb\u8c03\u7814\u9009\u62e9\u516b\u79cd\u6838\u5fc3\u7b97\u6cd5\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u7075\u6d3b\u7684\u6570\u636e\u751f\u6210\u5668\u5e76\u751f\u6210\u516b\u79cd\u65b0\u5408\u6210\u6570\u636e\u96c6\uff1b3. \u5f15\u5165\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5c42\u6b21API\u53ef\u7528\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5e73\u53f0\uff08\u5982GraphX\u3001PowerGraph\u7b49\uff09\u3002", "conclusion": "\u65b0\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u5728\u7b97\u6cd5\u9009\u62e9\u3001\u6570\u636e\u751f\u6210\u548cAPI\u53ef\u7528\u6027\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3a\u56fe\u5206\u6790\u5e73\u53f0\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.21901", "pdf": "https://arxiv.org/pdf/2506.21901", "abs": "https://arxiv.org/abs/2506.21901", "authors": ["James Pan", "Guoliang Li"], "title": "A Survey of LLM Inference Systems", "categories": ["cs.DB"], "comment": "25 pages", "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6280\u672f\uff0c\u5305\u62ec\u8bf7\u6c42\u5904\u7406\u3001\u6a21\u578b\u4f18\u5316\u3001\u5185\u5b58\u7ba1\u7406\u7b49\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u8fd9\u4e9b\u6280\u672f\u7ed3\u5408\u4ee5\u6784\u5efa\u5355\u526f\u672c\u548c\u591a\u526f\u672c\u63a8\u7406\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u72ec\u7279\u7684\u81ea\u56de\u5f52\u7279\u6027\u5bf9\u63a8\u7406\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u51fa\u4e86\u65b0\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u6280\u672f\u4ee5\u652f\u6301\u9ad8\u8d1f\u8f7d\u548c\u9ad8\u901f\u5ea6\u7684\u5de5\u4f5c\u6d41\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bf7\u6c42\u5904\u7406\u7b97\u6cd5\u3001\u6a21\u578b\u4f18\u5316\u6280\u672f\uff08\u5982\u5185\u6838\u8bbe\u8ba1\u3001\u6279\u5904\u7406\u548c\u8c03\u5ea6\uff09\u4ee5\u53ca\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\uff08\u5982\u5206\u9875\u5185\u5b58\u3001\u91cf\u5316\u548c\u7f13\u5b58\u6301\u4e45\u5316\uff09\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6280\u672f\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u4f9d\u8d56\u4e8e\u8d1f\u8f7d\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u673a\u5236\u548c\u6210\u672c\u964d\u4f4e\uff0c\u4ee5\u514b\u670d\u81ea\u56de\u5f52\u751f\u6210\u7684\u6311\u6218\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u6784\u5efa\u9ad8\u6548LLM\u63a8\u7406\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5269\u4f59\u6311\u6218\uff0c\u5982\u8d44\u6e90\u5206\u914d\u548c\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u3002"}}
{"id": "2506.21593", "pdf": "https://arxiv.org/pdf/2506.21593", "abs": "https://arxiv.org/abs/2506.21593", "authors": ["Abu Hanif Muhammad Syarubany", "Chang Dong Yoo"], "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications", "categories": ["cs.IR", "cs.DB"], "comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers", "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.", "AI": {"tldr": "PentaRAG\u662f\u4e00\u4e2a\u4e94\u5c42\u6a21\u5757\uff0c\u901a\u8fc7\u7f13\u5b58\u3001\u8bb0\u5fc6\u53ec\u56de\u548c\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u5c42\u4f18\u5316LLM\u7684\u67e5\u8be2\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f01\u4e1a\u90e8\u7f72LLM\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u53ef\u9884\u6d4b\u7684GPU\u6210\u672c\uff0c\u4f20\u7edfRAG\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u9700\u6c42\u3002", "method": "PentaRAG\u91c7\u7528\u4e94\u5c42\u8def\u7531\u7b56\u7565\uff0c\u5305\u62ec\u56fa\u5b9a\u952e\u503c\u7f13\u5b58\u3001\u8bed\u4e49\u7f13\u5b58\u3001\u8bb0\u5fc6\u53ec\u56de\u6a21\u5f0f\u3001\u81ea\u9002\u5e94\u4f1a\u8bdd\u5185\u5b58\u548c\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u5c42\u3002", "result": "\u5728TriviaQA\u9886\u57df\uff0cPentaRAG\u63d0\u5347\u7b54\u6848\u76f8\u4f3c\u5ea68%\u548c\u4e8b\u5b9e\u6b63\u786e\u602716%\uff0c\u5e73\u5747GPU\u65f6\u95f4\u51cf\u5c1150%\uff0c\u5ef6\u8fdf\u964d\u81f3\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u5206\u5c42\u8def\u7531\u7b56\u7565\u53ef\u5728\u751f\u4ea7\u7ea7RAG\u7cfb\u7edf\u4e2d\u540c\u65f6\u5b9e\u73b0\u65b0\u9c9c\u5ea6\u3001\u901f\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2506.22199", "pdf": "https://arxiv.org/pdf/2506.22199", "abs": "https://arxiv.org/abs/2506.22199", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "REDELEX: A Framework for Relational Deep Learning Exploration", "categories": ["cs.LG", "cs.DB"], "comment": "Accepted to ECMLPKDD 2025 at Porto, Portugal", "summary": "Relational databases (RDBs) are widely regarded as the gold standard for\nstoring structured information. Consequently, predictive tasks leveraging this\ndata format hold significant application promise. Recently, Relational Deep\nLearning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized\nas graph structures, enabling the application of various graph neural\narchitectures to effectively address these tasks. However, given its novelty,\nthere is a lack of analysis into the relationships between the performance of\nvarious RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for\nevaluating RDL models of varying complexity on the most diverse collection of\nover 70 RDBs, which we make available to the community. Benchmarked alongside\nkey representatives of classic methods, we confirm the generally superior\nperformance of RDL while providing insights into the main factors shaping\nperformance, including model complexity, database sizes and their structural\nproperties.", "AI": {"tldr": "REDELEX\u6846\u67b6\u8bc4\u4f30\u4e86\u4e0d\u540c\u590d\u6742\u5ea6\u7684RDL\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0RDL\u6574\u4f53\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76RDL\u6a21\u578b\u6027\u80fd\u4e0e\u5e95\u5c42RDB\u7279\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faREDELEX\u6846\u67b6\uff0c\u8bc4\u4f30\u591a\u79cdRDL\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "RDL\u6027\u80fd\u666e\u904d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u3001\u6570\u636e\u5e93\u5927\u5c0f\u548c\u7ed3\u6784\u7279\u6027\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "REDELEX\u4e3aRDL\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u5173\u952e\u56e0\u7d20\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2506.21634", "pdf": "https://arxiv.org/pdf/2506.21634", "abs": "https://arxiv.org/abs/2506.21634", "authors": ["Lutz Prechelt", "Lloyd Montgomery", "Julian Frattini", "Franz Zieris"], "title": "How (Not) To Write a Software Engineering Abstract", "categories": ["cs.SE", "D.2.0; A.m; K.m"], "comment": "16 pages, 11 figures, 2 tables", "summary": "Background: Abstracts are a particularly valuable element in a software\nengineering research article. However, not all abstracts are as informative as\nthey could be. Objective: Characterize the structure of abstracts in\nhigh-quality software engineering venues. Observe and quantify deficiencies.\nSuggest guidelines for writing informative abstracts. Methods: Use qualitative\nopen coding to derive concepts that explain relevant properties of abstracts.\nIdentify the archetypical structure of abstracts. Use quantitative content\nanalysis to objectively characterize abstract structure of a sample of 362\nabstracts from five presumably high-quality venues. Use exploratory data\nanalysis to find recurring issues in abstracts. Compare the archetypical\nstructure to actual structures. Infer guidelines for producing informative\nabstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,\nprovide background, objective, method, result, and conclusion information. For\nstructured abstracts, the ratio is twice as big. Only 4% of the abstracts are\nproper, i.e., they also have good readability (Flesch-Kincaid score) and have\nno informativeness gaps, understandability gaps, nor highly ambiguous\nsentences. Conclusions: (1) Even in top venues, a large majority of abstracts\nare far from ideal. (2) Structured abstracts tend to be better than\nunstructured ones. (3) Artifact-centric works need a different structured\nformat. (4) The community should start requiring conclusions that generalize,\nwhich currently are often missing in abstracts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u4f1a\u8bae\u6458\u8981\u4e2d\u4ec5\u670929%\u5b8c\u6574\uff0c\u7ed3\u6784\u5316\u6458\u8981\u8868\u73b0\u66f4\u597d\uff0c\u5efa\u8bae\u6539\u8fdb\u6458\u8981\u5199\u4f5c\u6307\u5357\u3002", "motivation": "\u5206\u6790\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u4f1a\u8bae\u6458\u8981\u7684\u7ed3\u6784\uff0c\u53d1\u73b0\u5176\u4e0d\u8db3\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u5f00\u653e\u7f16\u7801\u548c\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\uff0c\u5206\u6790362\u7bc7\u6458\u8981\u7684\u7ed3\u6784\u548c\u95ee\u9898\u3002", "result": "\u4ec529%\u6458\u8981\u5b8c\u6574\uff0c\u7ed3\u6784\u5316\u6458\u8981\u8868\u73b0\u66f4\u4f18\uff0c4%\u6458\u8981\u5b8c\u5168\u7b26\u5408\u6807\u51c6\u3002", "conclusion": "\u591a\u6570\u6458\u8981\u4e0d\u7406\u60f3\uff0c\u7ed3\u6784\u5316\u6458\u8981\u66f4\u4f18\uff0c\u9700\u6539\u8fdb\u7ed3\u8bba\u90e8\u5206\u5e76\u63a8\u5e7f\u65b0\u683c\u5f0f\u3002"}}
{"id": "2506.21998", "pdf": "https://arxiv.org/pdf/2506.21998", "abs": "https://arxiv.org/abs/2506.21998", "authors": ["R\u00e9my Raes", "Olivier Ruas", "Adrien Luxey-Bitri", "Romain Rouvoy"], "title": "INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User Privacy at the Edge", "categories": ["cs.DS"], "comment": null, "summary": "Data streams produced by mobile devices, such as smartphones, offer highly\nvaluable sources of information to build ubiquitous services. Such data streams\nare generally uploaded and centralized to be processed by third parties,\npotentially exposing sensitive personal information. In this context, existing\nprotection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),\nhave been investigated. Alas, none of them have actually been implemented, nor\ndeployed in real-life, in mobile devices to enforce user privacy at the edge.\nMoreover, the diversity of embedded sensors and the resulting data deluge makes\nit impractical to provision such services directly on mobiles, due to their\nconstrained storage capacity, communication bandwidth and processing power.\nThis article reports on the FLI technique, which leverages a piece-wise linear\napproximation technique to capture compact representations of data streams in\nmobile devices. Beyond the FLI storage layer, we introduce Divide \\& Stay, a\nnew privacy preservation technique to execute Points of Interest (POIs)\ninference. Finally, we deploy both of them on Android and iOS as the INTACT\nframework, making a concrete step towards enforcing privacy and trust in\nubiquitous computing systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFLI\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6355\u83b7\u6570\u636e\u6d41\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e86Divide & Stay\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u6700\u7ec8\u5728Android\u548ciOS\u4e0a\u90e8\u7f72\u4e3aINTACT\u6846\u67b6\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4ea7\u751f\u7684\u6570\u636e\u6d41\u5177\u6709\u9ad8\u4ef7\u503c\uff0c\u4f46\u96c6\u4e2d\u5904\u7406\u53ef\u80fd\u66b4\u9732\u654f\u611f\u4fe1\u606f\u3002\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u672a\u5728\u5b9e\u9645\u79fb\u52a8\u8bbe\u5907\u4e2d\u90e8\u7f72\uff0c\u4e14\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u6570\u636e\u6d41\u3002", "method": "\u91c7\u7528FLI\u6280\u672f\u8fdb\u884c\u6570\u636e\u6d41\u7d27\u51d1\u8868\u793a\uff0c\u7ed3\u5408Divide & Stay\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u6267\u884cPOI\u63a8\u65ad\u3002", "result": "\u6210\u529f\u5728Android\u548ciOS\u4e0a\u90e8\u7f72INTACT\u6846\u67b6\uff0c\u4e3a\u8fb9\u7f18\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FLI\u548cDivide & Stay\u6280\u672f\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0cINTACT\u6846\u67b6\u8fc8\u51fa\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2506.21655", "pdf": "https://arxiv.org/pdf/2506.21655", "abs": "https://arxiv.org/abs/2506.21655", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAsymmetric Policy Optimization (APO)\u65b9\u6cd5\uff0c\u7ed3\u5408DADS\u548cSTCR\u6280\u672f\uff0c\u89e3\u51b3\u4e86MLLMs\u4e2d\u590d\u6742\u63a8\u7406\u548c\u8fc7\u601d\u8003\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5e94\u7528\u53c8\u5bb9\u6613\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u601d\u8003\u95ee\u9898\u3002", "method": "\u63d0\u51faAPO\u65b9\u6cd5\uff0c\u5c06\u6837\u672c\u5206\u4e3a\u6b63\u8d1f\u4e24\u7ec4\uff1a\u6b63\u6837\u672c\u91c7\u7528DADS\u52a8\u6001\u8c03\u6574KL\u6563\u5ea6\u6743\u91cd\uff1b\u8d1f\u6837\u672c\u91c7\u7528STCR\u60e9\u7f5a\u8fc7\u957f\u54cd\u5e94\u3002", "result": "View-R1-3B\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53477%\uff0c\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684MLLMs\uff0c\u4e14\u901a\u7528\u4efb\u52a1\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "DADS\u548cSTCR\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.22033", "pdf": "https://arxiv.org/pdf/2506.22033", "abs": "https://arxiv.org/abs/2506.22033", "authors": ["Yongchao He", "Bohan Zhao", "Zheng Cao"], "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference", "categories": ["cs.DC"], "comment": null, "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.", "AI": {"tldr": "SiPipe\u662f\u4e00\u79cd\u5f02\u6784\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5229\u7528\u672a\u5145\u5206\u5229\u7528\u7684CPU\u8d44\u6e90\u6765\u5378\u8f7d\u8f85\u52a9\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u541e\u5410\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u589e\u52a0\uff0c\u6d41\u6c34\u7ebf\u5e76\u884c\uff08PP\uff09\u5728\u591aGPU\u90e8\u7f72\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5b58\u5728\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5982\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u9636\u6bb5\u95f4/\u9636\u6bb5\u5185\u6c14\u6ce1\uff0c\u9650\u5236\u4e86\u6d41\u6c34\u7ebf\u9971\u548c\u3002", "method": "SiPipe\u91c7\u7528\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1aCPU\u91c7\u6837\u3001\u4ee4\u724c\u5b89\u5168\u6267\u884c\u6a21\u578b\u548c\u7ed3\u6784\u611f\u77e5\u4f20\u8f93\uff0c\u4ee5\u51cf\u8f7b\u6d41\u6c34\u7ebf\u6c14\u6ce1\u5e76\u63d0\u9ad8\u6267\u884c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSiPipe\u5728\u76f8\u540cPP\u914d\u7f6e\u4e0b\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848vLLM\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.1\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u300143%\u7684\u6bcf\u4ee4\u724c\u5ef6\u8fdf\u964d\u4f4e\u548c23%\u7684\u5e73\u5747GPU\u5229\u7528\u7387\u63d0\u5347\u3002", "conclusion": "SiPipe\u901a\u8fc7\u4f18\u5316\u5f02\u6784\u8d44\u6e90\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u548c\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2506.21933", "pdf": "https://arxiv.org/pdf/2506.21933", "abs": "https://arxiv.org/abs/2506.21933", "authors": ["Yifan Xue", "Ruihuai Liang", "Bo Yang", "Xuelin Cao", "Zhiwen Yu", "M\u00e9rouane Debbah", "Chau Yuen"], "title": "Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "With the rapid development of the low-altitude economy, air-ground integrated\nmulti-access edge computing (MEC) systems are facing increasing demands for\nreal-time and intelligent task scheduling. In such systems, task offloading and\nresource allocation encounter multiple challenges, including node\nheterogeneity, unstable communication links, and dynamic task variations. To\naddress these issues, this paper constructs a three-layer heterogeneous MEC\nsystem architecture for low-altitude economic networks, encompassing aerial and\nground users as well as edge servers. The system is systematically modeled from\nthe perspectives of communication channels, computational costs, and constraint\nconditions, and the joint optimization problem of offloading decisions and\nresource allocation is uniformly abstracted into a graph-structured modeling\ntask. On this basis, we propose a graph attention diffusion-based solution\ngenerator (GADSG). This method integrates the contextual awareness of graph\nattention networks with the solution distribution learning capability of\ndiffusion models, enabling joint modeling and optimization of discrete\noffloading variables and continuous resource allocation variables within a\nhigh-dimensional latent space. We construct multiple simulation datasets with\nvarying scales and topologies. Extensive experiments demonstrate that the\nproposed GADSG model significantly outperforms existing baseline methods in\nterms of optimization performance, robustness, and generalization across task\nstructures, showing strong potential for efficient task scheduling in dynamic\nand complex low-altitude economic network environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u6269\u6563\u7684\u89e3\u51b3\u65b9\u6848\u751f\u6210\u5668\uff08GADSG\uff09\uff0c\u7528\u4e8e\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u5b9e\u65f6\u548c\u667a\u80fd\u4efb\u52a1\u8c03\u5ea6\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u9762\u4e34\u8282\u70b9\u5f02\u6784\u6027\u3001\u901a\u4fe1\u94fe\u8def\u4e0d\u7a33\u5b9a\u548c\u4efb\u52a1\u52a8\u6001\u53d8\u5316\u7b49\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e09\u5c42\u5f02\u6784MEC\u7cfb\u7edf\u67b6\u6784\uff0c\u63d0\u51faGADSG\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u6269\u6563\u6a21\u578b\u7684\u89e3\u5206\u5e03\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGADSG\u5728\u4f18\u5316\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GADSG\u5728\u52a8\u6001\u590d\u6742\u7684\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u4efb\u52a1\u8c03\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21654", "pdf": "https://arxiv.org/pdf/2506.21654", "abs": "https://arxiv.org/abs/2506.21654", "authors": ["Wolfgang Bangerth"], "title": "Experience converting a large mathematical software package written in C++ to C++20 modules", "categories": ["cs.SE", "cs.MS"], "comment": null, "summary": "Mathematical software has traditionally been built in the form of \"packages\"\nthat build on each other. A substantial fraction of these packages is written\nin C++ and, as a consequence, the interface of a package is described in the\nform of header files that downstream packages and applications can then\n#include. C++ has inherited this approach towards exporting interfaces from C,\nbut the approach is clunky, unreliable, and slow. As a consequence, C++20 has\nintroduced a \"module\" system in which packages explicitly export declarations\nand code that compilers then store in machine-readable form and that downstream\nusers can \"import\" -- a system in line with what many other programming\nlanguages have used for decades.\n  Herein, I explore how one can convert large mathematical software packages\nwritten in C++ to this system, using the deal.II finite element library with\nits around 800,000 lines of code as an example. I describe an approach that\nallows providing both header-based and module-based interfaces from the same\ncode base, discuss the challenges one encounters, and how modules actually work\nin practice in a variety of technical and human metrics. The results show that\nwith a non-trivial, but also not prohibitive effort, the conversion to modules\nis possible, resulting in a reduction in compile time for the converted library\nitself; on the other hand, for downstream projects, compile times show no clear\ntrend. I end with thoughts about long-term strategies for converting the entire\necosystem of mathematical software over the coming years or decades.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5927\u578b\u6570\u5b66\u8f6f\u4ef6\u5305\u4ece\u4f20\u7edf\u7684C++\u5934\u6587\u4ef6\u63a5\u53e3\u8f6c\u6362\u4e3aC++20\u7684\u6a21\u5757\u7cfb\u7edf\uff0c\u4ee5deal.II\u6709\u9650\u5143\u5e93\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u8f6c\u6362\u7684\u53ef\u884c\u6027\u548c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684C++\u5934\u6587\u4ef6\u63a5\u53e3\u65b9\u5f0f\u5b58\u5728\u7b28\u91cd\u3001\u4e0d\u53ef\u9760\u548c\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0cC++20\u5f15\u5165\u7684\u6a21\u5757\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5141\u8bb8\u5728\u540c\u4e00\u4ee3\u7801\u5e93\u4e2d\u540c\u65f6\u63d0\u4f9b\u57fa\u4e8e\u5934\u6587\u4ef6\u548c\u6a21\u5757\u7684\u63a5\u53e3\uff0c\u5e76\u8ba8\u8bba\u4e86\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6280\u672f\u53ca\u4eba\u4e3a\u56e0\u7d20\u3002", "result": "\u8f6c\u6362\u5230\u6a21\u5757\u7cfb\u7edf\u662f\u53ef\u884c\u7684\uff0c\u80fd\u51cf\u5c11\u5e93\u81ea\u8eab\u7684\u7f16\u8bd1\u65f6\u95f4\uff0c\u4f46\u5bf9\u4e0b\u6e38\u9879\u76ee\u7684\u7f16\u8bd1\u65f6\u95f4\u5f71\u54cd\u4e0d\u660e\u663e\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u8f6c\u6362\u7684\u53ef\u884c\u6027\u548c\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u51e0\u5e74\u6216\u51e0\u5341\u5e74\u5185\u5c06\u6574\u4e2a\u6570\u5b66\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u8f6c\u6362\u4e3a\u6a21\u5757\u7cfb\u7edf\u7684\u957f\u671f\u7b56\u7565\u3002"}}
{"id": "2506.22010", "pdf": "https://arxiv.org/pdf/2506.22010", "abs": "https://arxiv.org/abs/2506.22010", "authors": ["Matthias Bentert", "Fedor V. Fomin", "Petr A. Golovach", "Laure Morelle"], "title": "Fault-Tolerant Matroid Bases", "categories": ["cs.DS", "cs.DM"], "comment": "An extended abstract of this paper appears in the proceedings of ESA\n  2025", "summary": "We investigate the problem of constructing fault-tolerant bases in matroids.\nGiven a matroid M and a redundancy parameter k, a k-fault-tolerant basis is a\nminimum-size set of elements such that, even after the removal of any k\nelements, the remaining subset still spans the entire ground set. Since\nmatroids generalize linear independence across structures such as vector\nspaces, graphs, and set systems, this problem unifies and extends several\nfault-tolerant concepts appearing in prior research.\n  Our main contribution is a fixed-parameter tractable (FPT) algorithm for the\nk-fault-tolerant basis problem, parameterized by both k and the rank r of the\nmatroid. This two-variable parameterization by k + r is shown to be tight in\nthe following sense. On the one hand, the problem is already NP-hard for k=1.\nOn the other hand, it is Para-NP-hard for r \\geq 3 and polynomial-time solvable\nfor r \\leq 2.", "AI": {"tldr": "\u7814\u7a76\u5728\u62df\u9635\u4e2d\u6784\u5efa\u5bb9\u9519\u57fa\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff08FPT\uff09\u7b97\u6cd5\uff0c\u53c2\u6570\u5316\u4e3ak\u548c\u62df\u9635\u7684\u79e9r\u3002", "motivation": "\u62df\u9635\u80fd\u591f\u63a8\u5e7f\u7ebf\u6027\u72ec\u7acb\u6027\u5230\u5411\u91cf\u7a7a\u95f4\u3001\u56fe\u7b49\u7ed3\u6784\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u5bb9\u9519\u57fa\u95ee\u9898\u53ef\u4ee5\u7edf\u4e00\u548c\u6269\u5c55\u5148\u524d\u7814\u7a76\u4e2d\u7684\u591a\u4e2a\u5bb9\u9519\u6982\u5ff5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff08FPT\uff09\u7b97\u6cd5\uff0c\u53c2\u6570\u5316\u4e3ak\u548c\u62df\u9635\u7684\u79e9r\u3002", "result": "\u95ee\u9898\u5728k=1\u65f6\u5df2\u7ecf\u662fNP\u96be\uff0c\u800c\u5bf9\u4e8er\u22653\u662fPara-NP\u96be\uff0c\u5bf9\u4e8er\u22642\u5219\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u53c2\u6570k\u548cr\u4e0b\u662f\u7d27\u7684\uff0c\u4e3a\u62df\u9635\u5bb9\u9519\u57fa\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21683", "pdf": "https://arxiv.org/pdf/2506.21683", "abs": "https://arxiv.org/abs/2506.21683", "authors": ["Xihong Su", "Jia Lin Hau", "Gersi Doko", "Kishan Panaganti", "Marek Petrik"], "title": "Risk-Averse Total-Reward Reinforcement Learning", "categories": ["cs.LG"], "comment": "The paper is under review now", "summary": "Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising\nframework for modeling and solving undiscounted infinite-horizon objectives.\nExisting model-based algorithms for risk measures like the entropic risk\nmeasure (ERM) and entropic value-at-risk (EVaR) are effective in small\nproblems, but require full access to transition probabilities. We propose a\nQ-learning algorithm to compute the optimal stationary policy for total-reward\nERM and EVaR objectives with strong convergence and performance guarantees. The\nalgorithm and its optimality are made possible by ERM's dynamic consistency and\nelicitability. Our numerical results on tabular domains demonstrate quick and\nreliable convergence of the proposed Q-learning algorithm to the optimal\nrisk-averse value function.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u98ce\u9669\u538c\u6076\u603b\u5956\u52b1MDP\u95ee\u9898\uff0c\u9002\u7528\u4e8eERM\u548cEVaR\u76ee\u6807\uff0c\u5177\u6709\u5f3a\u6536\u655b\u6027\u548c\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u7b97\u6cd5\u9700\u8981\u5b8c\u5168\u8bbf\u95ee\u8f6c\u79fb\u6982\u7387\uff0c\u9650\u5236\u4e86\u5728\u5c0f\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528ERM\u7684\u52a8\u6001\u4e00\u81f4\u6027\u548c\u53ef\u6fc0\u53d1\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u8868\u683c\u57df\u4e2d\u5feb\u901f\u53ef\u9760\u5730\u6536\u655b\u5230\u6700\u4f18\u98ce\u9669\u538c\u6076\u503c\u51fd\u6570\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u98ce\u9669\u538c\u6076\u603b\u5956\u52b1MDP\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22035", "pdf": "https://arxiv.org/pdf/2506.22035", "abs": "https://arxiv.org/abs/2506.22035", "authors": ["Qiqi GU", "Chenpeng Wu", "Heng Shi", "Jianguo Yao"], "title": "SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap", "categories": ["cs.DC"], "comment": null, "summary": "Stencil computation, a pivotal numerical method in science and engineering,\niteratively updates grid points using weighted neighbor contributions and\nexhibits strong parallelism for multi-core processors. Current optimization\ntechniques targeting conducting stencil computation on tensor core accelerators\nincur substantial overheads due to redundant zero-padding during the\ntransformation to matrix multiplication. To address this, we introduce a sparse\ncomputation paradigm that eliminates inefficiencies by exploiting specialized\nhardware units.\n  This paper exploits the sparsity in these matrices as a feature and presents\nSPTCStencil, a high-performance stencil computation system accelerated by\nSparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for\nacceleration beyond deep learning domains. First, Our approach generalizes an\nefficient transformation of stencil computation into matrix multiplications and\nspecializes this conversion for SpTC compatibility through a novel\nsparsification strategy. Furthermore, SPTCStencil incorporates a\nhigh-performance GPU kernel with systematic optimizations designed to maximize\nefficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil\n5.46$\\times$ and Tensor Core-based approaches by 2.00$\\times$ on average.", "AI": {"tldr": "SPTCStencil\u5229\u7528\u7a00\u758f\u5f20\u91cf\u6838\u5fc3\u4f18\u5316\u6a21\u677f\u8ba1\u7b97\uff0c\u6d88\u9664\u5197\u4f59\u96f6\u586b\u5145\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u5f53\u524d\u5728\u5f20\u91cf\u6838\u5fc3\u52a0\u901f\u5668\u4e0a\u4f18\u5316\u6a21\u677f\u8ba1\u7b97\u7684\u65b9\u6cd5\u56e0\u5197\u4f59\u96f6\u586b\u5145\u5bfc\u81f4\u9ad8\u5f00\u9500\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u8ba1\u7b97\u8303\u5f0f\uff0c\u5c06\u6a21\u677f\u8ba1\u7b97\u9ad8\u6548\u8f6c\u6362\u4e3a\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e13\u7528GPU\u5185\u6838\u3002", "result": "SPTCStencil\u5e73\u5747\u6027\u80fd\u63d0\u53475.46\u500d\uff0c\u4f18\u4e8e\u4f20\u7edf\u5f20\u91cf\u6838\u5fc3\u65b9\u6cd52\u500d\u3002", "conclusion": "SPTCStencil\u9996\u6b21\u5c06\u7a00\u758f\u5f20\u91cf\u6838\u5fc3\u5e94\u7528\u4e8e\u975e\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u6a21\u677f\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.22148", "pdf": "https://arxiv.org/pdf/2506.22148", "abs": "https://arxiv.org/abs/2506.22148", "authors": ["Joshua Goulton", "Milena Radenkovic"], "title": "Resilient Communication For Avalanche Response in Infrastructure-Limited Environments", "categories": ["cs.NI"], "comment": null, "summary": "Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining\ncommunication in infrastructure limited environments, such as those encountered\nduring natural disasters. This paper investigates the viability of leveraging\nan existing national transport system - the Swiss rail network - as a data mule\nbackbone for disseminating critical avalanche alerts. Using The Opportunistic\nNetwork Environment (ONE) simulator, we model the entire Swiss rail network and\nconduct a rigorous comparative analysis of two seminal DTN routing protocols:\nEpidemic and PROPHET. Experiments are performed in two distinct scenarios:\nalerts originating from dense urban centres and from sparse, remote mountainous\nregions. Our results demonstrate that the rail network provides robust\nconnectivity for opportunistic communication in both environments thus\nvalidating the integration of DTN principles in remote scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u745e\u58eb\u94c1\u8def\u7f51\u7edc\u4f5c\u4e3a\u6570\u636e\u9aa8\u5e72\u7f51\u4f20\u64ad\u5173\u952e\u96ea\u5d29\u8b66\u62a5\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DTN\u534f\u8bae\u5728\u5bc6\u96c6\u57ce\u5e02\u548c\u504f\u8fdc\u5c71\u533a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u81ea\u7136\u707e\u5bb3\u73af\u5883\u4e2d\uff0c\u5ef6\u8fdf\u5bb9\u5fcd\u7f51\u7edc\uff08DTN\uff09\u4e3a\u7ef4\u6301\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5229\u7528\u73b0\u6709\u56fd\u5bb6\u8fd0\u8f93\u7cfb\u7edf\uff08\u745e\u58eb\u94c1\u8def\u7f51\u7edc\uff09\u4f5c\u4e3a\u6570\u636e\u9aa8\u5e72\u7f51\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528The Opportunistic Network Environment (ONE)\u6a21\u62df\u5668\uff0c\u5bf9\u745e\u58eb\u94c1\u8def\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u6bd4\u8f83\u4e24\u79cdDTN\u8def\u7531\u534f\u8bae\uff08Epidemic\u548cPROPHET\uff09\u5728\u5bc6\u96c6\u57ce\u5e02\u548c\u504f\u8fdc\u5c71\u533a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u94c1\u8def\u7f51\u7edc\u5728\u4e24\u79cd\u73af\u5883\u4e2d\u5747\u80fd\u63d0\u4f9b\u7a33\u5065\u7684\u901a\u4fe1\u8fde\u63a5\uff0c\u9a8c\u8bc1\u4e86DTN\u5728\u504f\u8fdc\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u745e\u58eb\u94c1\u8def\u7f51\u7edc\u53ef\u4f5c\u4e3aDTN\u7684\u6709\u6548\u9aa8\u5e72\u7f51\uff0c\u652f\u6301\u5173\u952e\u8b66\u62a5\u7684\u4f20\u64ad\uff0c\u5c24\u5176\u5728\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u504f\u8fdc\u5730\u533a\u3002"}}
{"id": "2506.21693", "pdf": "https://arxiv.org/pdf/2506.21693", "abs": "https://arxiv.org/abs/2506.21693", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Fredrik T\u00f6rner", "Christian Berger"], "title": "The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation", "categories": ["cs.SE", "cs.RO"], "comment": "Accepted for publication in the Journal of Systems and Software (JSS)", "summary": "Developing autonomous driving (AD) systems is challenging due to the\ncomplexity of the systems and the need to assure their safe and reliable\noperation. The widely adopted approach of DevOps seems promising to support the\ncontinuous technological progress in AI and the demand for fast reaction to\nincidents, which necessitate continuous development, deployment, and\nmonitoring. We present a systematic literature review meant to identify,\nanalyse, and synthesise a broad range of existing literature related to usage\nof DevOps in autonomous driving development. Our results provide a structured\noverview of challenges and solutions, arising from applying DevOps to\nsafety-related AI-enabled functions. Our results indicate that there are still\nseveral open topics to be addressed to enable safe DevOps for the development\nof safe AD.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86DevOps\u5728\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4ecd\u9700\u89e3\u51b3\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u590d\u6742\u4e14\u9700\u786e\u4fdd\u5b89\u5168\u53ef\u9760\uff0cDevOps\u65b9\u6cd5\u56e0\u5176\u652f\u6301\u5feb\u901f\u54cd\u5e94\u548c\u6301\u7eed\u5f00\u53d1\u800c\u663e\u5f97\u6709\u524d\u666f\u3002", "method": "\u8fdb\u884c\u4e86\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u3001\u5206\u6790\u5e76\u7efc\u5408\u4e86DevOps\u5728\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e2d\u7684\u76f8\u5173\u6587\u732e\u3002", "result": "\u63d0\u4f9b\u4e86\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u7684\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u76f8\u5173AI\u529f\u80fd\u4e2dDevOps\u5e94\u7528\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\uff0c\u5b9e\u73b0\u5b89\u5168\u7684DevOps\u4ee5\u5f00\u53d1\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ecd\u9700\u89e3\u51b3\u591a\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2506.22127", "pdf": "https://arxiv.org/pdf/2506.22127", "abs": "https://arxiv.org/abs/2506.22127", "authors": ["V\u00e1clav Bla\u017eej", "Andreas Emil Feldmann", "Foivos Fioravantes", "Pawe\u0142 Rz\u0105\u017cewski", "Ond\u0159ej Such\u00fd"], "title": "Parameterized Complexity of Directed Traveling Salesman Problem", "categories": ["cs.DS", "68Q27"], "comment": null, "summary": "The Directed Traveling Salesman Problem (DTSP) is a variant of the classical\nTraveling Salesman Problem in which the edges in the graph are directed and a\nvertex and edge can be visited multiple times. The goal is to find a directed\nclosed walk of minimum length (or total weight) that visits every vertex of the\ngiven graph at least once. In a yet more general version, Directed Waypoint\nRouting Problem (DWRP), some vertices are marked as terminals and we are only\nrequired to visit all terminals. Furthermore, each edge has its capacity\nbounding the number of times this edge can be used by a solution.\n  While both problems (and many other variants of TSP) were extensively\ninvestigated, mostly from the approximation point of view, there are\nsurprisingly few results concerning the parameterized complexity. Our starting\npoint is the result of Marx et al. [APPROX/RANDOM 2016] who proved that DTSP is\nW[1]-hard parameterized by distance to pathwidth 3. In this paper we aim to\ninitiate the systematic complexity study of variants of DTSP with respect to\nvarious, mostly structural, parameters.\n  We show that DWRP is FPT parameterized by the solution size, the feedback\nedge number, and the vertex integrity of the underlying undirected graph.\nFurthermore, the problem is XP parameterized by treewidth. On the complexity\nside, we show that the problem is W[1]-hard parameterized by the distance to\nconstant treedepth.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6709\u5411\u65c5\u884c\u5546\u95ee\u9898\uff08DTSP\uff09\u53ca\u5176\u53d8\u4f53\u6709\u5411\u8def\u5f84\u8def\u7531\u95ee\u9898\uff08DWRP\uff09\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\uff0c\u5c55\u793a\u4e86DWRP\u5728\u591a\u79cd\u7ed3\u6784\u53c2\u6570\u4e0b\u7684\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u6027\uff08FPT\uff09\u548cXP\u590d\u6742\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u67d0\u4e9b\u53c2\u6570\u4e0b\u7684W[1]-\u56f0\u96be\u6027\u3002", "motivation": "\u5c3d\u7ba1DTSP\u53ca\u5176\u53d8\u4f53\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5173\u4e8e\u5176\u53c2\u6570\u5316\u590d\u6742\u6027\u7684\u7ed3\u679c\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7684\u7ed3\u6784\u53c2\u6570\uff08\u5982\u89e3\u7684\u5927\u5c0f\u3001\u53cd\u9988\u8fb9\u6570\u3001\u9876\u70b9\u5b8c\u6574\u6027\u7b49\uff09\uff0c\u7814\u7a76\u4e86DWRP\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\u3002", "result": "\u8bc1\u660e\u4e86DWRP\u5728\u89e3\u7684\u5927\u5c0f\u3001\u53cd\u9988\u8fb9\u6570\u548c\u9876\u70b9\u5b8c\u6574\u6027\u53c2\u6570\u4e0b\u662fFPT\u7684\uff0c\u5728\u6811\u5bbd\u53c2\u6570\u4e0b\u662fXP\u7684\uff0c\u800c\u5728\u6052\u5b9a\u6811\u6df1\u8ddd\u79bb\u53c2\u6570\u4e0b\u662fW[1]-\u56f0\u96be\u7684\u3002", "conclusion": "\u672c\u6587\u4e3aDTSP\u53ca\u5176\u53d8\u4f53\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7ed3\u679c\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.21695", "pdf": "https://arxiv.org/pdf/2506.21695", "abs": "https://arxiv.org/abs/2506.21695", "authors": ["Oron Nir", "Jay Tenenbaum", "Ariel Shamir"], "title": "Unimodal Strategies in Density-Based Clustering", "categories": ["cs.LG"], "comment": null, "summary": "Density-based clustering methods often surpass centroid-based counterparts,\nwhen addressing data with noise or arbitrary data distributions common in\nreal-world problems. In this study, we reveal a key property intrinsic to\ndensity-based clustering methods regarding the relation between the number of\nclusters and the neighborhood radius of core points - we empirically show that\nit is nearly unimodal, and support this claim theoretically in a specific\nsetting. We leverage this property to devise new strategies for finding\nappropriate values for the radius more efficiently based on the Ternary Search\nalgorithm. This is especially important for large scale data that is\nhigh-dimensional, where parameter tuning is computationally intensive. We\nvalidate our methodology through extensive applications across a range of\nhigh-dimensional, large-scale NLP, Audio, and Computer Vision tasks,\ndemonstrating its practical effectiveness and robustness. This work not only\noffers a significant advancement in parameter control for density-based\nclustering but also broadens the understanding regarding the relations between\ntheir guiding parameters. Our code is available at\nhttps://github.com/oronnir/UnimodalStrategies.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5bc6\u5ea6\u805a\u7c7b\u65b9\u6cd5\u4e2d\u6838\u5fc3\u70b9\u90bb\u57df\u534a\u5f84\u4e0e\u7c07\u6570\u4e4b\u95f4\u7684\u5355\u5cf0\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4e09\u5206\u641c\u7d22\u7684\u9ad8\u6548\u53c2\u6570\u8c03\u4f18\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u5927\u89c4\u6a21\u6570\u636e\u3002", "motivation": "\u5bc6\u5ea6\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u566a\u58f0\u6216\u590d\u6742\u5206\u5e03\u6570\u636e\u65f6\u4f18\u4e8e\u57fa\u4e8e\u8d28\u5fc3\u7684\u65b9\u6cd5\uff0c\u4f46\u53c2\u6570\u8c03\u4f18\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u66f4\u9ad8\u6548\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u90bb\u57df\u534a\u5f84\u4e0e\u7c07\u6570\u7684\u5355\u5cf0\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u4e09\u5206\u641c\u7d22\u7b97\u6cd5\u8bbe\u8ba1\u9ad8\u6548\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u7ef4\u5927\u89c4\u6a21NLP\u3001\u97f3\u9891\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u6539\u8fdb\u4e86\u5bc6\u5ea6\u805a\u7c7b\u7684\u53c2\u6570\u63a7\u5236\uff0c\u8fd8\u6df1\u5316\u4e86\u5bf9\u53c2\u6570\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u3002"}}
{"id": "2506.22169", "pdf": "https://arxiv.org/pdf/2506.22169", "abs": "https://arxiv.org/abs/2506.22169", "authors": ["Zheng Zhang", "Donglin Yang", "Xiaobo Zhou", "Dazhao Cheng"], "title": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators", "categories": ["cs.DC", "cs.PL"], "comment": "12 pages, accepted at SC 2024", "summary": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility.", "AI": {"tldr": "MCFuser\u662f\u4e00\u4e2a\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u878d\u5408\u5185\u5b58\u5bc6\u96c6\u578b\u8ba1\u7b97\u5bc6\u96c6\u578b\uff08MBCI\uff09\u7b97\u5b50\u94fe\uff0c\u901a\u8fc7\u9ad8\u7ea7\u5206\u5757\u8868\u8fbe\u5f0f\u548cDAG\u5206\u6790\u4f18\u5316\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8c03\u4f18\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u591a\u8ba1\u7b97\u5bc6\u96c6\u578b\u7b97\u5b50\u878d\u5408\u56e0\u8ba1\u7b97\u541e\u5410\u9971\u548c\u800c\u5931\u8d25\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u52a8\u6001\u5f20\u91cf\u7ef4\u5ea6\u5bfc\u81f4\u7684\u5185\u5b58\u9650\u5236\u548c\u8c03\u4f18\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u9ad8\u7ea7\u5206\u5757\u8868\u8fbe\u5f0f\u5b9a\u4e49\u641c\u7d22\u7a7a\u95f4\uff0c\u7ed3\u5408DAG\u5206\u6790\u6d88\u9664\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u901a\u8fc7\u526a\u679d\u641c\u7d22\u7a7a\u95f4\u548c\u542f\u53d1\u5f0f\u641c\u7d22\u52a0\u901f\u8c03\u4f18\u3002", "result": "\u5728NVIDIA A100\u548cRTX3080 GPU\u4e0a\uff0cMCFuser\u6bd4Ansor\u7b49\u7f16\u8bd1\u5668\u6027\u80fd\u63d0\u53475.9\u500d\uff0c\u8c03\u4f18\u65f6\u95f4\u51cf\u5c1170\u500d\u4ee5\u4e0a\u3002", "conclusion": "MCFuser\u901a\u8fc7\u9ad8\u6548\u7684\u5185\u6838\u878d\u5408\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u5bc6\u96c6\u578b\u8ba1\u7b97\u5bc6\u96c6\u578b\u7b97\u5b50\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8c03\u4f18\u65f6\u95f4\u3002"}}
{"id": "2506.22223", "pdf": "https://arxiv.org/pdf/2506.22223", "abs": "https://arxiv.org/abs/2506.22223", "authors": ["Felipe Valle Quiroz", "Johan Elfing", "Joel P\u00e5lsson", "Elena Haller", "Oscar Amador Molina"], "title": "V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles", "categories": ["cs.NI"], "comment": "Accepted into FAST-zero'25: 8th International Symposium on Future\n  Active Safety Technology toward zero traffic accidents", "summary": "This paper introduces a novel intention-sharing mechanism for Electrically\nPower-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing\nthe ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete\npredicted trajectory points with a compact elliptical geographical area\nrepresentation derived via quadratic polynomial fitting and Least Squares\nMethod (LSM). This approach encodes trajectory predictions with fixed-size data\npayloads, independent of the number of forecasted points, enabling\nhigher-frequency transmissions and improved network reliability. Simulation\nresults demonstrate superior inter-packet gap (IPG) performance compared to\nstandard ETSI VAMs, particularly under constrained communication conditions. A\nphysical experiment validates the feasibility of real-time deployment on\nembedded systems. The method supports scalable, low-latency intention sharing,\ncontributing to cooperative perception and enhanced safety for vulnerable road\nusers in connected and automated mobility ecosystems. Finally, we discuss the\nviability of LSM and open the door to other methods for prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u7535\u52a8\u52a9\u529b\u81ea\u884c\u8f66\u610f\u56fe\u5171\u4eab\u673a\u5236\uff0c\u901a\u8fc7\u692d\u5706\u5730\u7406\u533a\u57df\u8868\u793a\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f18\u5316\u8f68\u8ff9\u9884\u6d4b\uff0c\u63d0\u5347\u4e86V2X\u901a\u4fe1\u4e2d\u7684\u7f51\u7edc\u53ef\u9760\u6027\u548c\u4f20\u8f93\u9891\u7387\u3002", "motivation": "\u63d0\u5347\u7535\u52a8\u52a9\u529b\u81ea\u884c\u8f66\u5728V2X\u901a\u4fe1\u4e2d\u7684\u610f\u56fe\u5171\u4eab\u6548\u7387\uff0c\u589e\u5f3a\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u4e8c\u6b21\u591a\u9879\u5f0f\u62df\u5408\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff0c\u5c06\u79bb\u6563\u8f68\u8ff9\u9884\u6d4b\u70b9\u66ff\u6362\u4e3a\u7d27\u51d1\u7684\u692d\u5706\u5730\u7406\u533a\u57df\u8868\u793a\uff0c\u56fa\u5b9a\u6570\u636e\u8d1f\u8f7d\u5927\u5c0f\u3002", "result": "\u4eff\u771f\u663e\u793a\u5728\u53d7\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4\u6807\u51c6ETSI VAM\u534f\u8bae\uff0c\u5177\u6709\u66f4\u4f18\u7684\u5305\u95f4\u9694\u6027\u80fd\uff1b\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5b9e\u65f6\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u610f\u56fe\u5171\u4eab\uff0c\u4e3a\u8054\u7f51\u548c\u81ea\u52a8\u5316\u51fa\u884c\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f31\u52bf\u9053\u8def\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.21703", "pdf": "https://arxiv.org/pdf/2506.21703", "abs": "https://arxiv.org/abs/2506.21703", "authors": ["Victoria Jackson", "Susannah Liu", "Andre van der Hoek"], "title": "Using Generative AI in Software Design Education: An Experience Report", "categories": ["cs.SE"], "comment": "12 pages, 1 figure", "summary": "With the rapid adoption of Generative AI (GenAI) tools, software engineering\neducators have grappled with how best to incorporate them into the classroom.\nWhile some research discusses the use of GenAI in the context of learning to\ncode, there is little research that explores the use of GenAI in the classroom\nfor other areas of software development. This paper provides an experience\nreport on introducing GenAI into an undergraduate software design class.\nStudents were required to use GenAI (in the form of ChatGPT) to help complete a\nteam-based assignment. The data collected consisted of the ChatGPT conversation\nlogs and students' reflections on using ChatGPT for the assignment.\nSubsequently, qualitative analysis was undertaken on the data. Students\nidentified numerous ways ChatGPT helped them in their design process while\nrecognizing the need to critique the response before incorporating it into\ntheir design. At the same time, we identified several key lessons for educators\nin how to deploy GenAI in a software design class effectively. Based on our\nexperience, we believe students can benefit from using GenAI in software design\neducation as it helps them design and learn about the strengths and weaknesses\nof GenAI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u672c\u79d1\u8f6f\u4ef6\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u5f15\u5165\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u7684\u7ecf\u9a8c\uff0c\u5206\u6790\u4e86\u5b66\u751f\u4f7f\u7528AI\u5b8c\u6210\u56e2\u961f\u4f5c\u4e1a\u7684\u6548\u679c\u53ca\u5176\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u5feb\u901f\u666e\u53ca\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u8005\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5c06\u5176\u6709\u6548\u878d\u5165\u8bfe\u5802\uff0c\u5c24\u5176\u662f\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u7b49\u975e\u7f16\u7801\u9886\u57df\u3002", "method": "\u5b66\u751f\u5728\u56e2\u961f\u4f5c\u4e1a\u4e2d\u4f7f\u7528ChatGPT\u8f85\u52a9\u8bbe\u8ba1\uff0c\u6536\u96c6\u5bf9\u8bdd\u65e5\u5fd7\u548c\u53cd\u601d\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5b66\u751f\u53d1\u73b0ChatGPT\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u5e2e\u52a9\uff0c\u4f46\u4e5f\u610f\u8bc6\u5230\u9700\u6279\u5224\u6027\u8bc4\u4f30\u5176\u56de\u7b54\uff1b\u540c\u65f6\uff0c\u7814\u7a76\u603b\u7ed3\u4e86\u6559\u80b2\u8005\u6709\u6548\u90e8\u7f72AI\u7684\u5173\u952e\u7ecf\u9a8c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u65e2\u80fd\u8f85\u52a9\u8bbe\u8ba1\uff0c\u53c8\u80fd\u5e2e\u52a9\u5b66\u751f\u4e86\u89e3AI\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002"}}
{"id": "2506.22261", "pdf": "https://arxiv.org/pdf/2506.22261", "abs": "https://arxiv.org/abs/2506.22261", "authors": ["Yael Kirkpatrick", "Virginia Vassilevska Williams"], "title": "Shortest Paths in Multimode Graphs", "categories": ["cs.DS"], "comment": null, "summary": "In this work we study shortest path problems in multimode graphs, a\ngeneralization of the min-distance measure introduced by Abboud, Vassilevska W.\nand Wang in [SODA'16]. A multimode shortest path is the shortest path using one\nof multiple `modes' of transportation that cannot be combined. This represents\nreal-world scenarios where different modes are not combinable, such as flights\noperated by different airlines. More precisely, a $k$-multimode graph is a\ncollection of $k$ graphs on the same vertex set and the $k$-mode distance\nbetween two vertices is defined as the minimum among the distances computed in\neach individual graph.\n  We focus on approximating fundamental graph parameters on these graphs,\nspecifically diameter and radius. In undirected multimode graphs we first show\nan elegant linear time 3-approximation algorithm for 2-mode diameter. We then\nextend this idea into a general subroutine that can be used as a part of any\n$\\alpha$-approximation, and use it to construct a 2 and 2.5 approximation\nalgorithm for 2-mode diameter. For undirected radius, we introduce a general\nscheme that can compute a 3-approximation of the $k$-mode radius for any $k$.\nIn the directed case we develop novel techniques to construct a linear time\nalgorithm to determine whether the diameter is finite.\n  We also develop many conditional fine-grained lower bounds for various\nmultimode diameter and radius approximation problems. We are able to show that\nmany of our algorithms are tight under popular fine-grained complexity\nhypotheses, including our linear time 3-approximation for $3$-mode undirected\ndiameter and radius. As part of this effort we propose the first extension to\nthe Hitting Set Hypothesis [SODA'16], which we call the $\\ell$-Hitting Set\nHypothesis. We use this hypothesis to prove the first parameterized lower bound\ntradeoff for radius approximation algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u5f0f\u56fe\u4e2d\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8fd1\u4f3c\u7b97\u6cd5\u548c\u6761\u4ef6\u7cbe\u7ec6\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u67d0\u4e9b\u7b97\u6cd5\u7684\u7d27\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u5f0f\u56fe\u4e2d\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u540c\u4ea4\u901a\u6a21\u5f0f\u4e0d\u53ef\u7ec4\u5408\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u65f6\u95f43-\u8fd1\u4f3c\u7b97\u6cd5\u7528\u4e8e2\u6a21\u5f0f\u76f4\u5f84\uff0c\u5e76\u6269\u5c55\u4e3a\u901a\u7528\u5b50\u7a0b\u5e8f\uff1b\u5f00\u53d1\u4e863-\u8fd1\u4f3ck\u6a21\u5f0f\u534a\u5f84\u7684\u901a\u7528\u65b9\u6848\uff1b\u9488\u5bf9\u6709\u5411\u56fe\u8bbe\u8ba1\u4e86\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u7d27\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u2113-Hitting Set\u5047\u8bbe\u4ee5\u652f\u6301\u53c2\u6570\u5316\u4e0b\u754c\u3002", "conclusion": "\u672c\u6587\u5728\u591a\u6a21\u5f0f\u56fe\u53c2\u6570\u8fd1\u4f3c\u7b97\u6cd5\u548c\u590d\u6742\u6027\u7406\u8bba\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.21714", "pdf": "https://arxiv.org/pdf/2506.21714", "abs": "https://arxiv.org/abs/2506.21714", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "categories": ["cs.LG", "cs.CV"], "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a7\u5236\u8d28\u91cf-\u590d\u6742\u5ea6\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u65f6\u95f4\u6b65\u957f\u548c\u795e\u7ecf\u7f51\u7edc\u957f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u6c42\u89e3ODE\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8fde\u63a5Transformer\u67b6\u6784\u4e2d\u7684\u5757\u6765\u6c42\u89e3\u5185\u90e8\u79bb\u6563ODE\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u65f6\u95f4\u548c\u957f\u5ea6\u4e00\u81f4\u6027\u9879\u3002", "result": "\u5728CelebA-HQ\u548cImageNet\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5ef6\u8fdf\u51cf\u5c11\u4e863\u500d\uff0cFID\u5206\u6570\u63d0\u9ad8\u4e863.5\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u548c\u957f\u5ea6\u7ef4\u5ea6\u4e0a\u5747\u5177\u6709\u7075\u6d3b\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.22171", "pdf": "https://arxiv.org/pdf/2506.22171", "abs": "https://arxiv.org/abs/2506.22171", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "title": "Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance", "categories": ["cs.DC"], "comment": "8 pages, submitted to WI IAT 2025", "summary": "Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure\nthe ledger yet cannot measure validator trustworthiness, allowing subtle\nmisconduct that is especially damaging in decentralized-finance (DeFi)\nsettings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)\ngives each action a layered utility score -- covering motivation and outcome,\n(ii) adapts validator weights using recent scores, and (iii) applies\ndecentralized verification with proportional slashing. The reward design is\nincentive-compatible, yielding a Nash equilibrium in which honest behavior\nmaximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,\nreputation-weighted validation) show that PoB cuts fraud acceptance by more\nthan 90%, demotes malicious validators within two rounds, and improves proposer\nfairness versus standard PoS, all with no more than a 5% throughput overhead.\nBy linking consensus influence to verifiably trustworthy conduct, PoB offers a\nscalable, regulation-friendly foundation for secure and fair blockchain\ngovernance in financial applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProof-of-Behavior (PoB)\u7684\u5171\u8bc6\u6a21\u578b\uff0c\u901a\u8fc7\u884c\u4e3a\u8bc4\u5206\u548c\u52a8\u6001\u6743\u91cd\u8c03\u6574\u6765\u63d0\u5347\u533a\u5757\u94fe\u7684\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u533a\u5757\u94fe\u534f\u8bae\uff08\u5982PoW\u548cPoS\uff09\u65e0\u6cd5\u8861\u91cf\u9a8c\u8bc1\u8005\u7684\u53ef\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u5728DeFi\u73af\u5883\u4e2d\u5b58\u5728\u6f5c\u5728\u5371\u5bb3\u7684\u5fae\u5999\u4e0d\u5f53\u884c\u4e3a\u3002", "method": "PoB\u901a\u8fc7\u5206\u5c42\u884c\u4e3a\u8bc4\u5206\uff08\u52a8\u673a\u548c\u7ed3\u679c\uff09\u3001\u52a8\u6001\u8c03\u6574\u9a8c\u8bc1\u8005\u6743\u91cd\u4ee5\u53ca\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\u4e0e\u6bd4\u4f8b\u60e9\u7f5a\u673a\u5236\u6765\u5b9e\u73b0\u5171\u8bc6\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cPoB\u5c06\u6b3a\u8bc8\u63a5\u53d7\u7387\u964d\u4f4e90%\u4ee5\u4e0a\uff0c\u4e24\u8f6e\u5185\u964d\u7ea7\u6076\u610f\u9a8c\u8bc1\u8005\uff0c\u5e76\u63d0\u5347\u63d0\u6848\u516c\u5e73\u6027\uff0c\u541e\u5410\u91cf\u5f00\u9500\u4e0d\u8d85\u8fc75%\u3002", "conclusion": "PoB\u901a\u8fc7\u5c06\u5171\u8bc6\u5f71\u54cd\u529b\u4e0e\u53ef\u9a8c\u8bc1\u7684\u53ef\u4fe1\u884c\u4e3a\u5173\u8054\uff0c\u4e3a\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u533a\u5757\u94fe\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5408\u89c4\u7684\u57fa\u7840\u3002"}}
{"id": "2506.22260", "pdf": "https://arxiv.org/pdf/2506.22260", "abs": "https://arxiv.org/abs/2506.22260", "authors": ["Douglas Dziedzorm Agbeve", "Andrey Belogaev", "Jeroen Famaey"], "title": "Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3", "categories": ["cs.NI"], "comment": null, "summary": "Wi-Fi networks have long relied on the Enhanced Distributed Channel Access\n(EDCA) mechanism, allowing stations to compete for transmission opportunities.\nHowever, as networks become denser and emerging applications demand lower\nlatency and higher reliability, the limitations of EDCA such as overhead due to\ncontention and collisions have become more pronounced. To address these\nchallenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been\nintroduced in Wi-Fi, enabling more efficient channel utilization through\nscheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal\nFrequency Division Multiple Random Access (UORA), a hybrid mechanism that\ncombines both scheduled and random access, balancing efficiency and\nresponsiveness in resource allocation. Despite significant research on UORA,\nmost studies rely on custom simulators that are not publicly available,\nlimiting reproducibility and preventing validation of the presented results.\nThe only known open-source UORA implementation in the ns-3 simulator exhibits\nkey limitations, such as usage of the same trigger frame (TF) to schedule\nresources for buffer status reports and data transmissions, and lack of\nsignaling for UORA configuration. In this paper, we present a fully\nstandard-compliant and open source UORA implementation that is compatible with\nns-3 version 3.38, addressing these limitations to improve resource allocation\nefficiency and adaptability. This implementation enables more accurate and\nflexible evaluation of UORA, fostering future research on Wi-Fi resource\nallocation strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u7b26\u5408\u6807\u51c6\u7684\u5f00\u6e90UORA\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ns-3\u6a21\u62df\u5668\u4e2dUORA\u5b9e\u73b0\u7684\u5173\u952e\u9650\u5236\uff0c\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u968f\u7740Wi-Fi\u7f51\u7edc\u5bc6\u5ea6\u589e\u52a0\u548c\u65b0\u5174\u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u9760\u6027\u7684\u9700\u6c42\uff0cEDCA\u673a\u5236\u7684\u5c40\u9650\u6027\u65e5\u76ca\u660e\u663e\uff0c\u800c\u73b0\u6709UORA\u7814\u7a76\u591a\u4f9d\u8d56\u4e0d\u516c\u5f00\u7684\u6a21\u62df\u5668\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u548c\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u7b26\u5408\u6807\u51c6\u4e14\u5f00\u6e90\u7684UORA\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86ns-3\u6a21\u62df\u5668\u4e2d\u73b0\u6709UORA\u5b9e\u73b0\u7684\u5173\u952e\u9650\u5236\uff0c\u5982\u8d44\u6e90\u8c03\u5ea6\u548c\u914d\u7f6e\u4fe1\u53f7\u95ee\u9898\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u7075\u6d3b\u7684UORA\u8d44\u6e90\u5206\u914d\uff0c\u4e3a\u672a\u6765Wi-Fi\u8d44\u6e90\u5206\u914d\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u5f00\u6e90UORA\u5b9e\u73b0\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86Wi-Fi\u8d44\u6e90\u5206\u914d\u7b56\u7565\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.22037", "pdf": "https://arxiv.org/pdf/2506.22037", "abs": "https://arxiv.org/abs/2506.22037", "authors": ["Jiawei Li", "Zan Liang", "Guoxin Wang", "Jinzhi Lu", "Yan Yan", "Shouxuan Wu", "Hao Wang"], "title": "KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering", "categories": ["cs.SE"], "comment": "12 pages, 9 figures, submitted to the 15th international Complex\n  Systems Design & Management (CSD&M) conference", "summary": "Model reconstruction is a method used to drive the development of complex\nsystem development processes in model-based systems engineering. Currently,\nduring the iterative design process of a system, there is a lack of an\neffective method to manage changes in development requirements, such as\ndevelopment cycle requirements and cost requirements, and to realize the\nreconstruction of the system development process model. To address these\nissues, this paper proposes a model reconstruction method to support the\ndevelopment process model. Firstly, the KARMA language, based on the GOPPRR-E\nmetamodeling method, is utilized to uniformly formalize the process models\nconstructed based on different modeling languages. Secondly, a model\nreconstruction framework is introduced. This framework takes a structured\ndevelopment requirements based natural language as input, employs natural\nlanguage processing techniques to analyze the development requirements text,\nand extracts structural and optimization constraint information. Then, after\nstructural reorganization and algorithm optimization, a development process\nmodel that meets the development requirements is obtained. Finally, as a case\nstudy, the development process of the aircraft onboard maintenance system is\nreconstructed. The results demonstrate that this method can significantly\nenhance the design efficiency of the development process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKARMA\u8bed\u8a00\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6a21\u578b\u91cd\u6784\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u7cfb\u7edf\u5f00\u53d1\u8fc7\u7a0b\u6a21\u578b\u7684\u7ba1\u7406\u548c\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u7cfb\u7edf\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9700\u6c42\u53d8\u66f4\u7ba1\u7406\u4e0d\u8db3\u548c\u6a21\u578b\u91cd\u6784\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528KARMA\u8bed\u8a00\u7edf\u4e00\u5f62\u5f0f\u5316\u8fc7\u7a0b\u6a21\u578b\uff0c\u5f15\u5165\u6a21\u578b\u91cd\u6784\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u9700\u6c42\u6587\u672c\u5e76\u63d0\u53d6\u7ea6\u675f\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u98de\u673a\u673a\u8f7d\u7ef4\u62a4\u7cfb\u7edf\u7684\u6848\u4f8b\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u8fc7\u7a0b\u7684\u8bbe\u8ba1\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u652f\u6301\u4e86\u5f00\u53d1\u8fc7\u7a0b\u6a21\u578b\u7684\u52a8\u6001\u91cd\u6784\u548c\u4f18\u5316\u3002"}}
{"id": "2506.22281", "pdf": "https://arxiv.org/pdf/2506.22281", "abs": "https://arxiv.org/abs/2506.22281", "authors": ["L\u00e1szl\u00f3 Kozma", "Junqi Tan"], "title": "Faster exponential algorithms for cut problems via geometric data structures", "categories": ["cs.DS", "cs.CG"], "comment": "10 pages; to be presented at ESA 2025", "summary": "For many hard computational problems, simple algorithms that run in time $2^n\n\\cdot n^{O(1)}$ arise, say, from enumerating all subsets of a size-$n$ set.\nFinding (exponentially) faster algorithms is a natural goal that has driven\nmuch of the field of exact exponential algorithms (e.g., see Fomin and Kratsch,\n2010). In this paper we obtain algorithms with running time $O(1.9999977^n)$ on\ninput graphs with $n$ vertices, for the following well-studied problems:\n  - $d$-Cut: find a proper cut in which no vertex has more than $d$ neighbors\non the other side of the cut;\n  - Internal Partition: find a proper cut in which every vertex has at least as\nmany neighbors on its side of the cut as on the other side; and\n  - ($\\alpha,\\beta$)-Domination: given intervals $\\alpha,\\beta \\subseteq\n[0,n]$, find a subset $S$ of the vertices, so that for every vertex $v \\in S$\nthe number of neighbors of $v$ in $S$ is from $\\alpha$ and for every vertex $v\n\\notin S$, the number of neighbors of $v$ in $S$ is from $\\beta$.\n  Our algorithms are exceedingly simple, combining the split and list technique\n(Horowitz and Sahni, 1974; Williams, 2005) with a tool from computational\ngeometry: orthogonal range searching in the moderate dimensional regime (Chan,\n2017). Our technique is applicable to the decision, optimization and counting\nversions of these problems and easily extends to various generalizations with\nmore fine-grained, vertex-specific constraints, as well as to directed,\nbalanced, and other variants. Algorithms with running times of the form $c^n$,\nfor $c<2$, were known for the first problem only for constant $d$, and for the\nthird problem for certain special cases of $\\alpha$ and $\\beta$; for the second\nproblem we are not aware of such results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u51e0\u4e2a\u7ecf\u5178\u56fe\u95ee\u9898\u7684\u6307\u6570\u7ea7\u66f4\u5feb\u7b97\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a$O(1.9999977^n)$\uff0c\u7ed3\u5408\u4e86\u5206\u5217\u6280\u672f\u548c\u8ba1\u7b97\u51e0\u4f55\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7b97\u6cd5\u8fd0\u884c\u65f6\u95f4$2^n \\cdot n^{O(1)}$\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u7cbe\u786e\u6307\u6570\u7b97\u6cd5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u5206\u5217\u6280\u672f\uff08split and list\uff09\u548c\u6b63\u4ea4\u8303\u56f4\u641c\u7d22\uff08computational geometry\u5de5\u5177\uff09\uff0c\u9002\u7528\u4e8e\u51b3\u7b56\u3001\u4f18\u5316\u548c\u8ba1\u6570\u95ee\u9898\u3002", "result": "\u5b9e\u73b0\u4e86$O(1.9999977^n)$\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u89e3\u51b3\u4e86$d$-Cut\u3001Internal Partition\u548c($\\alpha,\\beta$)-Domination\u95ee\u9898\u3002", "conclusion": "\u7b97\u6cd5\u7b80\u5355\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u53d8\u4f53\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u586b\u8865\u4e86\u90e8\u5206\u95ee\u9898\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2506.21718", "pdf": "https://arxiv.org/pdf/2506.21718", "abs": "https://arxiv.org/abs/2506.21718", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u590d\u6742\u7cfb\u7edf\u6570\u636e\u4e2d\u7684\u6307\u6807\u7ed3\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7cfb\u7edf\u6570\u636e\uff08\u5982\u914d\u7f6e\u6587\u4ef6\u6216\u7cfb\u7edf\u65e5\u5fd7\uff09\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u752860M\u53c2\u6570\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u8bad\u7ec3\uff0c\u8fdb\u884c\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u3002", "result": "\u5728Google\u7684Borg\u96c6\u7fa4\u8c03\u5ea6\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u76840.99\u7b49\u7ea7\u76f8\u5173\u6027\uff08\u5e73\u57470.9\uff09\uff0cMSE\u6bd4\u8868\u683c\u65b9\u6cd5\u4f4e100\u500d\uff0c\u5e76\u80fd\u8f7b\u677e\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7cfb\u7edf\u7ed3\u679c\u7684\u901a\u7528\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5f3a\u8c03\u4e86\u7f16\u7801\u5668\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.22175", "pdf": "https://arxiv.org/pdf/2506.22175", "abs": "https://arxiv.org/abs/2506.22175", "authors": ["Zheng Zhang", "Donglin Yang", "Yaqi Xia", "Liang Ding", "Dacheng Tao", "Xiaobo Zhou", "Dazhao Cheng"], "title": "MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism", "categories": ["cs.DC"], "comment": "11 pages, accepted at IPDPS 2023", "summary": "Recently, Mixture-of-Experts (MoE) has become one of the most popular\ntechniques to scale pre-trained models to extraordinarily large sizes. Dynamic\nactivation of experts allows for conditional computation, increasing the number\nof parameters of neural networks, which is critical for absorbing the vast\namounts of knowledge available in many deep learning areas. However, despite\nthe existing system and algorithm optimizations, there are significant\nchallenges to be tackled when it comes to the inefficiencies of communication\nand memory consumption.\n  In this paper, we present the design and implementation of MPipeMoE, a\nhigh-performance library that accelerates MoE training with adaptive and\nmemory-efficient pipeline parallelism. Inspired by that the MoE training\nprocedure can be divided into multiple independent sub-stages, we design\nadaptive pipeline parallelism with an online algorithm to configure the\ngranularity of the pipelining. Further, we analyze the memory footprint\nbreakdown of MoE training and identify that activations and temporary buffers\nare the primary contributors to the overall memory footprint. Toward memory\nefficiency, we propose memory reusing strategies to reduce memory requirements\nby eliminating memory redundancies, and develop an adaptive selection component\nto determine the optimal strategy that considers both hardware capacities and\nmodel characteristics at runtime. We implement MPipeMoE upon PyTorch and\nevaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA\nDGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up\nto 2.8x speedup and reduces memory footprint by up to 47% in training large\nmodels.", "AI": {"tldr": "MPipeMoE\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u5e93\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u548c\u5185\u5b58\u9ad8\u6548\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u52a0\u901fMoE\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e862.8\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c47%\u7684\u5185\u5b58\u5360\u7528\u51cf\u5c11\u3002", "motivation": "\u5c3d\u7ba1MoE\u6280\u672f\u5728\u6269\u5c55\u9884\u8bad\u7ec3\u6a21\u578b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u901a\u4fe1\u548c\u5185\u5b58\u6d88\u8017\u7684\u4f4e\u6548\u6027\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u5185\u5b58\u91cd\u7528\u7b56\u7565\uff0c\u7ed3\u5408\u786c\u4ef6\u548c\u6a21\u578b\u7279\u6027\u52a8\u6001\u4f18\u5316\u3002", "result": "\u57288\u53f0NVIDIA DGX A100\u670d\u52a1\u5668\u4e0a\u6d4b\u8bd5\uff0cMPipeMoE\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb2.8\u500d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1147%\u3002", "conclusion": "MPipeMoE\u6709\u6548\u89e3\u51b3\u4e86MoE\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u548c\u5185\u5b58\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22359", "pdf": "https://arxiv.org/pdf/2506.22359", "abs": "https://arxiv.org/abs/2506.22359", "authors": ["Viswanath Kumarskandpriya", "Abdulhalim Dandoush", "Abbas Bradai", "Ali Belgacem"], "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7535\u4fe1\u548c\u7f51\u7edc\u9886\u57df\uff0c\u5927\u578b\u6982\u5ff5\u6a21\u578b\uff08LCMs\uff09\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u9002\u5408\u89e3\u51b3\u590d\u6742\u3001\u591a\u5c42\u6b21\u7684\u7535\u4fe1\u95ee\u9898\u3002", "motivation": "\u7535\u4fe1\u9886\u57df\u9762\u4e34\u590d\u6742\u3001\u591a\u5c42\u6b21\u548c\u591a\u7ba1\u7406\u57df\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u73b0\u6709LLMs\u56e0\u5904\u7406\u65b9\u5f0f\u53d7\u9650\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4f7f\u7528LCMs\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u62bd\u8c61\u548c\u53cc\u66f2\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u6765\u89e3\u51b3\u7535\u4fe1\u95ee\u9898\u3002", "result": "LCMs\u5728\u5185\u5b58\u6548\u7387\u3001\u8de8\u5c42\u5173\u8054\u548c\u591a\u6a21\u6001\u96c6\u6210\u65b9\u9762\u4f18\u4e8eLLMs\u3002", "conclusion": "\u91c7\u7528LCMs\u662f\u5b9e\u73b0\u7a33\u5065\u4e14\u9ad8\u6548AI\u9a71\u52a8\u7535\u4fe1\u7ba1\u7406\u7684\u5fc5\u8981\u8fdb\u5316\u6b65\u9aa4\u3002"}}
{"id": "2506.22185", "pdf": "https://arxiv.org/pdf/2506.22185", "abs": "https://arxiv.org/abs/2506.22185", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMAPE-K\u548c\u4ee3\u7406AI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5fae\u670d\u52a1\u7684\u81ea\u4e3b\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4fee\u590d\uff0c\u63d0\u5347\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u5fae\u670d\u52a1\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u5e26\u6765\u5b89\u5168\u548c\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5a01\u80c1\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "method": "\u57fa\u4e8eMAPE-K\u6846\u67b6\uff0c\u5229\u7528\u4ee3\u7406AI\u5b9e\u73b0\u81ea\u4e3b\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4fee\u590d\u3002", "result": "\u63d0\u4f9b\u5b9e\u7528\u3001\u884c\u4e1a\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u7cfb\u7edf\u7a33\u5b9a\u6027\u3001\u51cf\u5c11\u505c\u673a\u65f6\u95f4\uff0c\u5e76\u76d1\u63a7\u6027\u80fd\u3001\u5f39\u6027\u7b49\u8d28\u91cf\u5c5e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u5b9a\u5236\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u5b9e\u8df5\uff0c\u63d0\u5347\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2506.21926", "pdf": "https://arxiv.org/pdf/2506.21926", "abs": "https://arxiv.org/abs/2506.21926", "authors": ["Anastasiia Tkachenko", "Haitao Wang"], "title": "Computing Maximum Cliques in Unit Disk Graphs", "categories": ["cs.CG", "cs.DS"], "comment": "To appear in CCCG 2025", "summary": "Given a set $P$ of $n$ points in the plane, the unit-disk graph $G(P)$ is a\ngraph with $P$ as its vertex set such that two points of $P$ have an edge if\ntheir Euclidean distance is at most $1$. We consider the problem of computing a\nmaximum clique in $G(P)$. The previously best algorithm for the problem runs in\n$O(n^{7/3+o(1)})$ time. We show that the problem can be solved in $O(n \\log n +\nn K^{4/3+o(1)})$ time, where $K$ is the maximum clique size. The algorithm is\nfaster than the previous one when $K=o(n)$. In addition, if $P$ is in convex\nposition, we give a randomized algorithm that runs in $O(n^{15/7+o(1)})=\nO(n^{2.143})$ worst-case time and the algorithm can compute a maximum clique\nwith high probability. For points in convex position, one special case we solve\nis when a point in the maximum clique is given; we present an $O(n^2\\log n)$\ntime (deterministic) algorithm for this special case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u5e73\u9762\u70b9\u96c6\u7684\u5355\u4f4d\u5706\u56fe\u4e2d\u7684\u6700\u5927\u56e2\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6700\u5927\u56e2\u89c4\u6a21\u8f83\u5c0f\u65f6\u3002\u5bf9\u4e8e\u51f8\u5305\u70b9\u96c6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u968f\u673a\u5316\u7b97\u6cd5\u548c\u786e\u5b9a\u6027\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5355\u4f4d\u5706\u56fe\u4e2d\u6700\u5927\u56e2\u8ba1\u7b97\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u6700\u5927\u56e2\u89c4\u6a21\u8f83\u5c0f\u65f6\u548c\u70b9\u96c6\u4e3a\u51f8\u5305\u65f6\u7684\u7279\u6b8a\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n log n + n K^{4/3+o(1)})\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u70b9\u96c6\uff1b\u5bf9\u4e8e\u51f8\u5305\u70b9\u96c6\uff0c\u8bbe\u8ba1\u4e86\u968f\u673a\u5316\u7b97\u6cd5\u548c\u786e\u5b9a\u6027\u7b97\u6cd5\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u6700\u5927\u56e2\u89c4\u6a21\u8f83\u5c0f\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u51f8\u5305\u70b9\u96c6\u7684\u968f\u673a\u5316\u7b97\u6cd5\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n^{2.143})\uff0c\u786e\u5b9a\u6027\u7b97\u6cd5\u4e3aO(n^2 log n)\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u4f4d\u5706\u56fe\u4e2d\u6700\u5927\u56e2\u8ba1\u7b97\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u4e3a\u76f8\u5173\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21744", "pdf": "https://arxiv.org/pdf/2506.21744", "abs": "https://arxiv.org/abs/2506.21744", "authors": ["Biying Zhou", "Nanyu Luo", "Feng Ji"], "title": "Federated Item Response Theory Models", "categories": ["cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Item Response Theory (IRT) models have been widely used to estimate\nrespondents' latent abilities and calibrate items' difficulty. Traditional IRT\nestimation requires all individual raw response data to be centralized in one\nplace, thus potentially causing privacy issues. Federated learning is an\nemerging field in computer science and machine learning with added features of\nprivacy protection and distributed computing. To integrate the advances from\nfederated learning with modern psychometrics, we propose a novel framework,\nFederated Item Response Theory (IRT), to enable estimating traditional IRT\nmodels with additional privacy, allowing estimation in a distributed manner\nwithout losing estimation accuracy.\n  Our numerical experiments confirm that FedIRT achieves statistical accuracy\nsimilar to standard IRT estimation using popular R packages, while offering\ncritical advantages: privacy protection and reduced communication costs. We\nalso validate FedIRT's utility through a real-world exam dataset, demonstrating\nits effectiveness in realistic educational contexts. This new framework extends\nIRT's applicability to distributed settings, such as multi-school assessments,\nwithout sacrificing accuracy or security. To support practical adoption, we\nprovide an open-ource R package, FedIRT, implementing the framework for the\ntwo-parameter logistic (2PL) and partial credit models (PCM).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedIRT\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u8054\u90a6\u5b66\u4e60\u4e0eIRT\u7ed3\u5408\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f30\u8ba1\uff0c\u4fdd\u62a4\u9690\u79c1\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfIRT\u9700\u8981\u96c6\u4e2d\u6240\u6709\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff1b\u8054\u90a6\u5b66\u4e60\u80fd\u4fdd\u62a4\u9690\u79c1\u5e76\u652f\u6301\u5206\u5e03\u5f0f\u8ba1\u7b97\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4e0eIRT\uff0c\u63d0\u51faFedIRT\u6846\u67b6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFedIRT\u4e0e\u4f20\u7edfIRT\u51c6\u786e\u6027\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "FedIRT\u6269\u5c55\u4e86IRT\u7684\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u573a\u666f\uff0c\u5982\u591a\u6821\u8bc4\u4f30\uff0c\u4e14\u5f00\u6e90\u5b9e\u73b0\u652f\u6301\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.22267", "pdf": "https://arxiv.org/pdf/2506.22267", "abs": "https://arxiv.org/abs/2506.22267", "authors": ["Junaid Ahmed Khan", "Hiari Pizzini Cavagna", "Andrea Proia", "Andrea Bartolini"], "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need", "categories": ["cs.DC"], "comment": "11 pages", "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u77e5\u8bc6\u56fe\u8c31\uff08VKG\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7aef\u5230\u7aefODA\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u4e2d\u5fc3\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8ba1\u7b97\u6548\u7387\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u67e5\u8be2\u65e0\u6a21\u5f0f\u7684NoSQL\u6570\u636e\u5e93\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u652f\u6301\u5b9e\u65f6\u6570\u636e\u5206\u6790\u3002", "method": "\u5229\u7528VKG\u751f\u6210\u8fd0\u884c\u65f6\u67e5\u8be2\u7279\u5b9a\u7684\u56fe\u8c31\uff0c\u7ed3\u5408LLM\u751f\u6210SPARQL\u67e5\u8be2\uff0c\u4f18\u5316\u4e86VKG\u6784\u5efa\u548cLLM\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u7cfb\u7edf\u67e5\u8be2\u51c6\u786e\u7387\u8fbe\u523092.5%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u4ece20.36\u79d2\u964d\u81f33.03\u79d2\uff0cVKG\u5927\u5c0f\u63a7\u5236\u5728179 MiB\u4ee5\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86ODA\u7cfb\u7edf\u7684\u5b9e\u65f6\u4ea4\u4e92\u80fd\u529b\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.22052", "pdf": "https://arxiv.org/pdf/2506.22052", "abs": "https://arxiv.org/abs/2506.22052", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "title": "Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "V2X communication has become crucial for enhancing road safety, especially\nfor Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the\nincreasing number of devices communicating on the same channels will lead to\nsignificant channel load. To address this issue this study evaluates the\neffectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),\nfocusing specifically on cyclists. The objective of RM is to minimize the\ntransmission of redundant information. We conducted a simulation study using a\nurban scenario with a high bicycle density based on traffic data from Hannover,\nGermany. This study assessed the impact of RM on channel load, measured by\nChannel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in\nsimulation. To evaluate the accuracy and reliability of the RM mechanisms, we\nanalyzed the actual differences in position, speed, and heading between the ego\nVRU and the VRU, which was assumed to be redundant. Our findings indicate that\nwhile RM can reduce channel congestion, it also leads to a decrease in VPR. The\nanalysis of actual differences revealed that the RM mechanism standardized by\nETSI often uses outdated information, leading to significant discrepancies in\nposition, speed, and heading, which could result in dangerous situations. To\naddress these limitations, we propose an adapted RM mechanism that improves the\nbalance between reducing channel load and maintaining VRU awareness. The\nadapted approach shows a significant reduction in maximum CBR and a less\nsignificant decrease in VPR compared to the standardized RM. Moreover, it\ndemonstrates better performance in the actual differences in position, speed,\nand heading, thereby enhancing overall safety. Our results highlight the need\nfor further research to optimize RM techniques and ensure they effectively\nenhance V2X communication without compromising the safety of VRUs.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5197\u4f59\u7f13\u89e3\uff08RM\uff09\u5bf9V2X\u901a\u4fe1\u4e2d\u81ea\u884c\u8f66\u624b\u611f\u77e5\u6d88\u606f\uff08VAM\uff09\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6807\u51c6\u5316RM\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u8fc7\u65f6\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u5e73\u8861\u4fe1\u9053\u8d1f\u8f7d\u548c\u5b89\u5168\u6027\u3002", "motivation": "V2X\u901a\u4fe1\u5bf9\u63d0\u5347\u9053\u8def\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bbe\u5907\u589e\u591a\u5bfc\u81f4\u4fe1\u9053\u8d1f\u8f7d\u589e\u52a0\uff0c\u9700\u8bc4\u4f30RM\u5bf9VAM\u7684\u6548\u679c\u4ee5\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\u3002", "method": "\u57fa\u4e8e\u5fb7\u56fd\u6c49\u8bfa\u5a01\u4ea4\u901a\u6570\u636e\u7684\u9ad8\u5bc6\u5ea6\u81ea\u884c\u8f66\u573a\u666f\u6a21\u62df\uff0c\u5206\u6790RM\u5bf9\u4fe1\u9053\u8d1f\u8f7d\uff08CBR\uff09\u548c\u5b89\u5168\u6027\uff08VPR\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u822a\u5411\u7684\u5b9e\u9645\u5dee\u5f02\u3002", "result": "RM\u51cf\u5c11\u4fe1\u9053\u62e5\u5835\u4f46\u964d\u4f4eVPR\uff1b\u6807\u51c6\u5316RM\u4f7f\u7528\u8fc7\u65f6\u4fe1\u606f\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u964d\u4f4eCBR\u4e14VPR\u4e0b\u964d\u8f83\u5c11\uff0c\u63d0\u5347\u5b89\u5168\u6027\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u4f18\u5316RM\u6280\u672f\uff0c\u786e\u4fddV2X\u901a\u4fe1\u6548\u7387\u540c\u65f6\u4e0d\u727a\u7272VRU\u5b89\u5168\uff0c\u6539\u8fdb\u65b9\u6cd5\u5c55\u793a\u66f4\u597d\u5e73\u8861\u3002"}}
{"id": "2506.22370", "pdf": "https://arxiv.org/pdf/2506.22370", "abs": "https://arxiv.org/abs/2506.22370", "authors": ["Carolina Carreira", "\u00c1lvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b66\u751f\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u89e3\u51b3\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4efb\u52a1\u65f6\u7684\u8868\u73b0\u548c\u7b56\u7565\uff0c\u53d1\u73b0\u4f7f\u7528ChatGPT\u7684\u5b66\u751f\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6548\u679c\u4e0e\u63d0\u793a\u8d28\u91cf\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u8ba4\u77e5\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u9a8c\u8bc1\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u8ba9\u7855\u58eb\u751f\u5728\u5f62\u5f0f\u5316\u65b9\u6cd5\u8bfe\u7a0b\u4e2d\u5b8c\u6210Dafny\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e00\u7ec4\u4f7f\u7528ChatGPT\uff0c\u53e6\u4e00\u7ec4\u4e0d\u4f7f\u7528\uff0c\u5e76\u8bb0\u5f55\u4e92\u52a8\u65e5\u5fd7\u3002", "result": "\u4f7f\u7528ChatGPT\u7684\u5b66\u751f\u8868\u73b0\u663e\u8457\u66f4\u597d\uff0c\u4f46\u8868\u73b0\u63d0\u5347\u4e0e\u63d0\u793a\u8d28\u91cf\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u5efa\u8bae\u5728\u5f62\u5f0f\u5316\u65b9\u6cd5\u8bfe\u7a0b\u4e2d\u66f4\u6709\u6548\u5730\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8bbe\u8ba1\u80fd\u4fc3\u8fdb\u5b66\u4e60\u800c\u975e\u66ff\u4ee3\u7684LLM\u611f\u77e5\u6311\u6218\u3002"}}
{"id": "2506.21771", "pdf": "https://arxiv.org/pdf/2506.21771", "abs": "https://arxiv.org/abs/2506.21771", "authors": ["John Wesley Hostetter", "Min Chi"], "title": "Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks", "categories": ["cs.LG", "cs.NE"], "comment": "45 pages", "summary": "Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function\napproximations that perform as well as conventional neural architectures, but\ntheir knowledge is expressed as linguistic IF-THEN rules. Despite these\nadvantages, their systematic design process remains a challenge. Existing work\nwill often sequentially build NFNs by inefficiently isolating parametric and\nstructural identification, leading to a premature commitment to brittle and\nsubpar architecture. We propose a novel application-independent approach called\ngradient-based neuroplastic adaptation for the concurrent optimization of NFNs'\nparameters and structure. By recognizing that NFNs' parameters and structure\nshould be optimized simultaneously as they are deeply conjoined, settings\npreviously unapproachable for NFNs are now accessible, such as the online\nreinforcement learning of NFNs for vision-based tasks. The effectiveness of\nconcurrently optimizing NFNs is empirically shown as it is trained by online\nreinforcement learning to proficiently play challenging scenarios from a\nvision-based video game called DOOM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u795e\u7ecf\u53ef\u5851\u6027\u9002\u5e94\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u795e\u7ecf\u6a21\u7cca\u7f51\u7edc\u7684\u53c2\u6570\u548c\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u53c2\u6570\u4e0e\u7ed3\u6784\u5206\u79bb\u4f18\u5316\u7684\u95ee\u9898\uff0c\u5e76\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u795e\u7ecf\u6a21\u7cca\u7f51\u7edc\uff08NFNs\uff09\u5177\u6709\u900f\u660e\u6027\u548c\u7b26\u53f7\u5316\u8868\u8fbe\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u7cfb\u7edf\u5316\u8bbe\u8ba1\u8fc7\u7a0b\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u53c2\u6570\u548c\u7ed3\u6784\u4f18\u5316\u5206\u79bb\uff0c\u5bfc\u81f4\u67b6\u6784\u8106\u5f31\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u5e94\u7528\u7684\u68af\u5ea6\u57fa\u795e\u7ecf\u53ef\u5851\u6027\u9002\u5e94\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316NFNs\u7684\u53c2\u6570\u548c\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5f97NFNs\u80fd\u591f\u5e94\u7528\u4e8e\u4e4b\u524d\u96be\u4ee5\u5904\u7406\u7684\u573a\u666f\uff0c\u5982\u57fa\u4e8e\u89c6\u89c9\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u5728DOOM\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u540c\u65f6\u4f18\u5316\u53c2\u6570\u548c\u7ed3\u6784\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NFNs\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22180", "pdf": "https://arxiv.org/pdf/2506.22180", "abs": "https://arxiv.org/abs/2506.22180", "authors": ["\u00d6nder G\u00fcrcan"], "title": "Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study", "categories": ["cs.CR", "cs.DC"], "comment": "23 pages, 5 figures, 2 tables", "summary": "The industrial market continuously needs reliable solutions to secure\nautonomous systems. Especially as these systems become more complex and\ninterconnected, reliable security solutions are becoming increasingly\nimportant. One promising solution to tackle this challenge is using smart\ncontracts designed to meet contractual conditions, avoid malicious errors,\nsecure exchanges, and minimize the need for reliable intermediaries. However,\nsmart contracts are immutable. Moreover, there are different smart contract\nexecution architectures (namely Order-Execute and Execute-Order-Validate) that\nhave different throughputs. In this study, we developed an evaluation model for\nassessing the security of reliable smart contract execution. We then developed\na realistic smart contract enabled IoT energy case study. Finally, we simulate\nthe developed case study to evaluate several smart contract security\nvulnerabilities reported in the literature. Our results show that the\nExecute-Order-Validate architecture is more promising regarding reliability and\nsecurity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u667a\u80fd\u5408\u7ea6\u6267\u884c\u5b89\u5168\u6027\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7269\u8054\u7f51\u80fd\u6e90\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86Execute-Order-Validate\u67b6\u6784\u5728\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u590d\u6742\u6027\u548c\u4e92\u8054\u6027\u7684\u589e\u52a0\uff0c\u53ef\u9760\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u9700\u6c42\u65e5\u76ca\u8feb\u5207\uff0c\u667a\u80fd\u5408\u7ea6\u56e0\u5176\u80fd\u591f\u6ee1\u8db3\u5408\u540c\u6761\u4ef6\u3001\u907f\u514d\u6076\u610f\u9519\u8bef\u7b49\u7279\u70b9\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u8bc4\u4f30\u667a\u80fd\u5408\u7ea6\u6267\u884c\u5b89\u5168\u6027\u7684\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u5408\u7ea6\u7684\u7269\u8054\u7f51\u80fd\u6e90\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u591a\u79cd\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cExecute-Order-Validate\u67b6\u6784\u5728\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u667a\u80fd\u5408\u7ea6\u5728\u81ea\u4e3b\u7cfb\u7edf\u5b89\u5168\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662fExecute-Order-Validate\u67b6\u6784\u66f4\u9002\u7528\u4e8e\u9ad8\u5b89\u5168\u9700\u6c42\u573a\u666f\u3002"}}
{"id": "2506.22390", "pdf": "https://arxiv.org/pdf/2506.22390", "abs": "https://arxiv.org/abs/2506.22390", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub", "categories": ["cs.SE"], "comment": null, "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.", "AI": {"tldr": "\u5206\u6790\u4e86686\u4e2a\u5f00\u53d1\u8005\u4e0eChatGPT\u7684\u5bf9\u8bdd\uff0c\u53d1\u73b062%\u7684\u5bf9\u8bdd\u5bf9\u89e3\u51b3\u95ee\u9898\u6709\u5e2e\u52a9\u3002ChatGPT\u5728\u4ee3\u7801\u751f\u6210\u548c\u5de5\u5177\u63a8\u8350\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u4ee3\u7801\u89e3\u91ca\u4e0a\u8f83\u5dee\u3002\u6709\u6548\u7684\u5bf9\u8bdd\u901a\u5e38\u66f4\u77ed\u3001\u66f4\u6613\u8bfb\u4e14\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u5f00\u53d1\u8005\u4e0eChatGPT\u5bf9\u8bdd\u7684\u6709\u6548\u6027\uff0c\u4ee5\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u6548\u7387\u3002", "method": "\u5206\u6790686\u4e2aGitHub\u95ee\u9898\u7ebf\u7a0b\u4e2d\u7684\u5bf9\u8bdd\uff0c\u5206\u7c7b\u4efb\u52a1\u7c7b\u578b\uff0c\u8bc4\u4f30\u5bf9\u8bdd\u3001\u9879\u76ee\u548c\u95ee\u9898\u76f8\u5173\u6307\u6807\u3002", "result": "62%\u7684\u5bf9\u8bdd\u6709\u5e2e\u52a9\uff0cChatGPT\u64c5\u957f\u4ee3\u7801\u751f\u6210\u548c\u5de5\u5177\u63a8\u8350\uff0c\u4f46\u5bf9\u4ee3\u7801\u89e3\u91ca\u8868\u73b0\u4e0d\u4f73\u3002\u6709\u6548\u5bf9\u8bdd\u66f4\u77ed\u3001\u66f4\u6613\u8bfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u6307\u5bfc\u5f00\u53d1\u8005\u4f18\u5316\u4ea4\u4e92\u7b56\u7565\uff0c\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\uff0c\u5e76\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2506.21782", "pdf": "https://arxiv.org/pdf/2506.21782", "abs": "https://arxiv.org/abs/2506.21782", "authors": ["Aditya Narendra", "Dmitry Makarov", "Aleksandr Panov"], "title": "M3PO: Massively Multi-Task Model-Based Policy Optimization", "categories": ["cs.LG", "cs.RO"], "comment": "6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version,\n  including appendix and implementation details", "summary": "We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a\nscalable model-based reinforcement learning (MBRL) framework designed to\naddress sample inefficiency in single-task settings and poor generalization in\nmulti-task domains. Existing model-based approaches like DreamerV3 rely on\npixel-level generative models that neglect control-centric representations,\nwhile model-free methods such as PPO suffer from high sample complexity and\nweak exploration. M3PO integrates an implicit world model, trained to predict\ntask outcomes without observation reconstruction, with a hybrid exploration\nstrategy that combines model-based planning and model-free uncertainty-driven\nbonuses. This eliminates the bias-variance trade-off in prior methods by using\ndiscrepancies between model-based and model-free value estimates to guide\nexploration, while maintaining stable policy updates through a trust-region\noptimizer. M3PO provides an efficient and robust alternative to existing\nmodel-based policy optimization approaches and achieves state-of-the-art\nperformance across multiple benchmarks.", "AI": {"tldr": "M3PO\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5355\u4efb\u52a1\u6837\u672c\u6548\u7387\u4f4e\u548c\u591a\u4efb\u52a1\u6cdb\u5316\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982DreamerV3\uff09\u5ffd\u7565\u63a7\u5236\u4e2d\u5fc3\u8868\u793a\uff0c\u4ee5\u53ca\u65e0\u6a21\u578b\u65b9\u6cd5\uff08\u5982PPO\uff09\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u548c\u63a2\u7d22\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u9690\u5f0f\u4e16\u754c\u6a21\u578b\uff08\u9884\u6d4b\u4efb\u52a1\u7ed3\u679c\u800c\u975e\u89c2\u6d4b\u91cd\u5efa\uff09\u548c\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff08\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u548c\u65e0\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5956\u52b1\uff09\uff0c\u5229\u7528\u57fa\u4e8e\u6a21\u578b\u4e0e\u65e0\u6a21\u578b\u4ef7\u503c\u4f30\u8ba1\u7684\u5dee\u5f02\u6307\u5bfc\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u4fe1\u4efb\u533a\u57df\u4f18\u5316\u5668\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "M3PO\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.22323", "pdf": "https://arxiv.org/pdf/2506.22323", "abs": "https://arxiv.org/abs/2506.22323", "authors": ["Alessio Di Santo"], "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "comment": null, "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.", "AI": {"tldr": "\u9488\u5bf9\u62c9\u4e01\u7f8e\u6d32\uff08\u5c24\u5176\u662f\u5df4\u897f\uff09\u7684\u590d\u6742\u6076\u610f\u90ae\u4ef6\u6d3b\u52a8\uff0c\u901a\u8fc7\u9493\u9c7c\u90ae\u4ef6\u8bf1\u4f7f\u7528\u6237\u6267\u884c\u6076\u610fMSI\u6587\u4ef6\uff0c\u5229\u7528DLL\u4fa7\u52a0\u8f7d\u7ed5\u8fc7\u5b89\u5168\u9632\u5fa1\uff0c\u90e8\u7f72BlotchyQuasar\uff08QuasarRAT\u53d8\u79cd\uff09\u7a83\u53d6\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u63ed\u793a\u9488\u5bf9\u62c9\u4e01\u7f8e\u6d32\u7684\u6076\u610f\u653b\u51fb\u6d3b\u52a8\uff0c\u5206\u6790\u5176\u6280\u672f\u624b\u6bb5\u548c\u6f5c\u5728\u5371\u5bb3\u3002", "method": "\u5229\u7528\u9493\u9c7c\u90ae\u4ef6\u548cDLL\u4fa7\u52a0\u8f7d\u6280\u672f\uff0c\u90e8\u7f72BlotchyQuasar\u6076\u610f\u8f6f\u4ef6\uff0c\u7a83\u53d6\u6d4f\u89c8\u5668\u51ed\u8bc1\u548c\u94f6\u884c\u4fe1\u606f\u3002", "result": "\u6076\u610f\u8f6f\u4ef6\u6210\u529f\u7a83\u53d6\u6570\u636e\u5e76\u5efa\u7acb\u6301\u4e45\u6027\uff0c\u4f46\u56e0\u5f00\u53d1\u4ed3\u4fc3\u5b58\u5728\u7f3a\u9677\u3002", "conclusion": "\u8be5\u6d3b\u52a8\u5a01\u80c1\u4e25\u91cd\uff0c\u9700\u52a0\u5f3a\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u3002"}}
{"id": "2506.21788", "pdf": "https://arxiv.org/pdf/2506.21788", "abs": "https://arxiv.org/abs/2506.21788", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "comment": "15 pages, 4 figures, 2 tables", "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5e76\u884c\u5316\u7684\u56fe\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7GPU\u52a0\u901f\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9ad8\u6548\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u591a\u6e90\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u7684\u9884\u8bad\u7ec3\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u591a\u6e90\u3001\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u5904\u7406\u6311\u6218\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u5728\u672a\u63a2\u7d22\u5316\u5b66\u533a\u57df\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u5171\u4eab\u6d88\u606f\u4f20\u9012\u5c42\u5904\u7406\u8f93\u5165\u539f\u5b50\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u89e3\u7801\u5934\u9884\u6d4b\u6570\u636e\u7279\u5b9a\u8f93\u51fa\uff1b\u63d0\u51fa\u591a\u4efb\u52a1\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u5229\u7528GPU\u52a0\u901f\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\u3002", "result": "\u5728\u8d85\u8fc72400\u4e07\u4e2a\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u4e09\u79cd\u9ad8\u5ea6\u5f02\u6784\u7684\u8d85\u7ea7\u8ba1\u7b97\u673a\u67b6\u6784\u4e0a\u5c55\u793a\u4e86\u9ad8\u6548\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u9ad8\u6548\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2506.21797", "pdf": "https://arxiv.org/pdf/2506.21797", "abs": "https://arxiv.org/abs/2506.21797", "authors": ["Peihao Wang", "Zhangyang Wang"], "title": "Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning", "categories": ["cs.LG"], "comment": "International Conference on Neuro-symbolic Systems (NeuS), 2025", "summary": "We develop a theoretical framework that explains how discrete symbolic\nstructures can emerge naturally from continuous neural network training\ndynamics. By lifting neural parameters to a measure space and modeling training\nas Wasserstein gradient flow, we show that under geometric constraints, such as\ngroup invariance, the parameter measure $\\mu_t$ undergoes two concurrent\nphenomena: (1) a decoupling of the gradient flow into independent optimization\ntrajectories over some potential functions, and (2) a progressive contraction\non the degree of freedom. These potentials encode algebraic constraints\nrelevant to the task and act as ring homomorphisms under a commutative\nsemi-ring structure on the measure space. As training progresses, the network\ntransitions from a high-dimensional exploration to compositional\nrepresentations that comply with algebraic operations and exhibit a lower\ndegree of freedom. We further establish data scaling laws for realizing\nsymbolic tasks, linking representational capacity to the group invariance that\nfacilitates symbolic solutions. This framework charts a principled foundation\nfor understanding and designing neurosymbolic systems that integrate continuous\nlearning with discrete algebraic reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u79bb\u6563\u7b26\u53f7\u7ed3\u6784\u5982\u4f55\u4ece\u8fde\u7eed\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002\u901a\u8fc7\u5c06\u795e\u7ecf\u53c2\u6570\u63d0\u5347\u5230\u6d4b\u5ea6\u7a7a\u95f4\u5e76\u5efa\u6a21\u8bad\u7ec3\u4e3aWasserstein\u68af\u5ea6\u6d41\uff0c\u63ed\u793a\u4e86\u5728\u51e0\u4f55\u7ea6\u675f\u4e0b\u53c2\u6570\u6d4b\u5ea6\u7684\u4e24\u79cd\u73b0\u8c61\uff1a\u68af\u5ea6\u6d41\u89e3\u8026\u548c\u81ea\u7531\u5ea6\u6536\u7f29\u3002", "motivation": "\u63a2\u7d22\u8fde\u7eed\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5982\u4f55\u81ea\u7136\u5f62\u6210\u79bb\u6563\u7b26\u53f7\u7ed3\u6784\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u7406\u89e3\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u795e\u7ecf\u53c2\u6570\u63d0\u5347\u5230\u6d4b\u5ea6\u7a7a\u95f4\uff0c\u5efa\u6a21\u8bad\u7ec3\u4e3aWasserstein\u68af\u5ea6\u6d41\uff0c\u5206\u6790\u51e0\u4f55\u7ea6\u675f\u4e0b\u7684\u53c2\u6570\u6d4b\u5ea6\u53d8\u5316\u3002", "result": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7f51\u7edc\u4ece\u9ad8\u7ef4\u63a2\u7d22\u8fc7\u6e21\u5230\u7b26\u5408\u4ee3\u6570\u8fd0\u7b97\u7684\u4f4e\u81ea\u7531\u5ea6\u7ec4\u5408\u8868\u793a\uff0c\u5e76\u5efa\u7acb\u4e86\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6574\u5408\u8fde\u7eed\u5b66\u4e60\u4e0e\u79bb\u6563\u4ee3\u6570\u63a8\u7406\u7684\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.21833", "pdf": "https://arxiv.org/pdf/2506.21833", "abs": "https://arxiv.org/abs/2506.21833", "authors": ["Kunjal Panchal", "Sunav Choudhary", "Yuriy Brun", "Hui Guan"], "title": "The Cost of Avoiding Backpropagation", "categories": ["cs.LG"], "comment": null, "summary": "Forward-mode automatic differentiation (FmAD) and zero-order (ZO)\noptimization have been proposed as memory-efficient alternatives to\nbackpropagation (BP) for gradient computation, especially in low-resource\nsettings. However, their practical benefits remain unclear due to two key gaps:\na lack of comparison against memory-efficient BP variants, such as activation\ncheckpointing, and a lack of a unified theoretical analysis. This work presents\na comprehensive theoretical and empirical comparison of BP, FmAD, and ZO\nmethods. Our theoretical analysis shows that while FmAD, and ZO can reduce\nmemory usage, they incur significant costs in accuracy, convergence speed, and\ncomputation compared to BP with checkpointing. These drawbacks worsen with\nlarger models or constrained perturbation budgets. Empirical experiments on\nlarge language and vision-language models show that BP with checkpointing\noutperforms FmAD and ZO variants, including those enhanced with variance\nreduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and\n3.8x fewer computations at comparable memory usage. Our results highlight\nfundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as\nthe most effective strategy for model training under memory-constrained\nsettings. Our code is available at\nhttps://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u524d\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08FmAD\uff09\u548c\u96f6\u9636\u4f18\u5316\uff08ZO\uff09\u4e0e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u53ca\u5176\u5185\u5b58\u9ad8\u6548\u53d8\u4f53\uff08\u5982\u6fc0\u6d3b\u68c0\u67e5\u70b9\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0BP\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8eFmAD\u548cZO\u3002", "motivation": "\u7814\u7a76FmAD\u548cZO\u4f5c\u4e3aBP\u7684\u66ff\u4ee3\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u5185\u5b58\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u4e0e\u5185\u5b58\u9ad8\u6548BP\u53d8\u4f53\u6bd4\u8f83\u548c\u7edf\u4e00\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u6bd4\u8f83BP\u3001FmAD\u548cZO\u5728\u5185\u5b58\u4f7f\u7528\u3001\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBP\u5728\u5185\u5b58\u4f7f\u7528\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u6027\u6bd4FmAD\u548cZO\u9ad831.1%\uff0c\u6536\u655b\u901f\u5ea6\u5feb34.8%\uff0c\u8ba1\u7b97\u91cf\u5c113.8\u500d\u3002", "conclusion": "BP\u7ed3\u5408\u68c0\u67e5\u70b9\u662f\u6700\u6709\u6548\u7684\u5185\u5b58\u53d7\u9650\u8bad\u7ec3\u7b56\u7565\uff0c\u800cFmAD\u548cZO\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u7f3a\u9677\u3002"}}
{"id": "2506.21844", "pdf": "https://arxiv.org/pdf/2506.21844", "abs": "https://arxiv.org/abs/2506.21844", "authors": ["Jun Ohkubo"], "title": "Koopman operator-based discussion on partial observation in stochastic systems", "categories": ["cs.LG"], "comment": "23 pages, 5 figures", "summary": "It is sometimes difficult to achieve a complete observation for a full set of\nobservables, and partial observations are necessary. For deterministic systems,\nthe Mori-Zwanzig formalism provides a theoretical framework for handling\npartial observations. Recently, data-driven algorithms based on the Koopman\noperator theory have made significant progress, and there is a discussion to\nconnect the Mori-Zwanzig formalism with the Koopman operator theory. In this\nwork, we discuss the effects of partial observation in stochastic systems using\nthe Koopman operator theory. The discussion clarifies the importance of\ndistinguishing the state space and the function space in stochastic systems.\nEven in stochastic systems, the delay embedding technique is beneficial for\npartial observation, and several numerical experiments showed a power-law\nbehavior of the accuracy for the amplitude of the additive noise. We also\ndiscuss the relation between the exponent of the power-law behavior and the\neffects of partial observation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u968f\u673a\u7cfb\u7edf\u4e2d\u4f7f\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5904\u7406\u90e8\u5206\u89c2\u6d4b\u7684\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u72b6\u6001\u7a7a\u95f4\u4e0e\u51fd\u6570\u7a7a\u95f4\u7684\u533a\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u7684\u4f18\u52bf\u3002", "motivation": "\u5728\u968f\u673a\u7cfb\u7edf\u4e2d\uff0c\u90e8\u5206\u89c2\u6d4b\u662f\u5e38\u89c1\u7684\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u3002Mori-Zwanzig\u5f62\u5f0f\u4e3b\u4e49\u548cKoopman\u7b97\u5b50\u7406\u8bba\u7684\u7ed3\u5408\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u91c7\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5206\u6790\u968f\u673a\u7cfb\u7edf\u4e2d\u7684\u90e8\u5206\u89c2\u6d4b\uff0c\u7ed3\u5408\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u52a0\u6027\u566a\u58f0\u7684\u5e45\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8\u4e86\u5e42\u5f8b\u6307\u6570\u4e0e\u90e8\u5206\u89c2\u6d4b\u6548\u679c\u7684\u5173\u7cfb\u3002", "conclusion": "\u5728\u968f\u673a\u7cfb\u7edf\u4e2d\uff0c\u533a\u5206\u72b6\u6001\u7a7a\u95f4\u4e0e\u51fd\u6570\u7a7a\u95f4\u81f3\u5173\u91cd\u8981\uff0c\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u5bf9\u90e8\u5206\u89c2\u6d4b\u6709\u6548\uff0c\u5e42\u5f8b\u884c\u4e3a\u63ed\u793a\u4e86\u566a\u58f0\u4e0e\u7cbe\u5ea6\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.21872", "pdf": "https://arxiv.org/pdf/2506.21872", "abs": "https://arxiv.org/abs/2506.21872", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "title": "A Survey of Continual Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "This work has been submitted to the IEEE TPAMI", "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u7684\u6838\u5fc3\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u52a8\u6001\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002CRL\u65e8\u5728\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u548c\u77e5\u8bc6\u4fdd\u7559\u6765\u63d0\u5347RL\u7684\u9002\u7528\u6027\u3002", "method": "\u8bba\u6587\u9996\u5148\u56de\u987e\u4e86\u73b0\u6709\u5de5\u4f5c\uff0c\u5206\u6790\u4e86\u5176\u6307\u6807\u3001\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u573a\u666f\u8bbe\u7f6e\uff1b\u5176\u6b21\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u5b58\u50a8\u548c\u8f6c\u79fb\u7684CRL\u65b9\u6cd5\u5206\u7c7b\u6cd5\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u6cd5\u548c\u5206\u6790\uff0c\u8bba\u6587\u603b\u7ed3\u4e86CRL\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "CRL\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u51b3\u5176\u6311\u6218\u4ee5\u63d0\u5347RL\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.21899", "pdf": "https://arxiv.org/pdf/2506.21899", "abs": "https://arxiv.org/abs/2506.21899", "authors": ["Amara Zuffer", "Michael Burke", "Mehrtash Harandi"], "title": "Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review", "categories": ["cs.LG"], "comment": "65 pages, 9 figures", "summary": "The diversity of tasks and dynamic nature of reinforcement learning (RL)\nrequire RL agents to be able to learn sequentially and continuously, a learning\nparadigm known as continuous reinforcement learning. This survey reviews how\ncontinual learning transforms RL agents into dynamic continual learners. This\nenables RL agents to acquire and retain useful and reusable knowledge\nseamlessly. The paper delves into fundamental aspects of continual\nreinforcement learning, exploring key concepts, significant challenges, and\nnovel methodologies. Special emphasis is placed on recent advancements in\ncontinual reinforcement learning within robotics, along with a succinct\noverview of evaluation environments utilized in prominent research,\nfacilitating accessibility for newcomers to the field. The review concludes\nwith a discussion on limitations and promising future directions, providing\nvaluable insights for researchers and practitioners alike.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u7684\u5173\u952e\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u673a\u5728\u4e8e\u4f7fRL\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u548c\u4fdd\u7559\u77e5\u8bc6\uff0c\u9002\u5e94\u591a\u6837\u5316\u548c\u52a8\u6001\u7684\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u7efc\u8ff0\u4e86CRL\u7684\u57fa\u672c\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b0\u5174\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u673a\u5668\u4eba\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u8bc4\u4f30\u73af\u5883\u3002", "result": "\u603b\u7ed3\u4e86CRL\u7684\u73b0\u72b6\uff0c\u5305\u62ec\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u3002", "conclusion": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u4e2a\u5145\u6ee1\u6f5c\u529b\u7684\u9886\u57df\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6311\u6218\u4ee5\u63a8\u52a8\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21900", "pdf": "https://arxiv.org/pdf/2506.21900", "abs": "https://arxiv.org/abs/2506.21900", "authors": ["Sheng Yun", "Jianhua Pei", "Ping Wang"], "title": "TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments", "categories": ["cs.LG", "eess.IV"], "comment": null, "summary": "The evolution toward 6G networks demands a fundamental shift from bit-centric\ntransmission to semantic-aware communication that emphasizes task-relevant\ninformation. This work introduces TOAST (Task-Oriented Adaptive Semantic\nTransmission), a unified framework designed to address the core challenge of\nmulti-task optimization in dynamic wireless environments through three\ncomplementary components. First, we formulate adaptive task balancing as a\nMarkov decision process, employing deep reinforcement learning to dynamically\nadjust the trade-off between image reconstruction fidelity and semantic\nclassification accuracy based on real-time channel conditions. Second, we\nintegrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our\nSwin Transformer-based joint source-channel coding architecture, enabling\nparameter-efficient fine-tuning that dramatically reduces adaptation overhead\nwhile maintaining full performance across diverse channel impairments including\nAdditive White Gaussian Noise (AWGN), fading, phase noise, and impulse\ninterference. Third, we incorporate an Elucidating diffusion model that\noperates in the latent space to restore features corrupted by channel noises,\nproviding substantial quality improvements compared to baseline approaches.\nExtensive experiments across multiple datasets demonstrate that TOAST achieves\nsuperior performance compared to baseline approaches, with significant\nimprovements in both classification accuracy and reconstruction quality at low\nSignal-to-Noise Ratio (SNR) conditions while maintaining robust performance\nacross all tested scenarios.", "AI": {"tldr": "TOAST\u662f\u4e00\u4e2a\u9762\u54116G\u7f51\u7edc\u7684\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5e73\u8861\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u566a\u58f0\u6062\u590d\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u4f18\u5316\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u4ece\u6bd4\u7279\u4f20\u8f93\u8f6c\u5411\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\uff0c\u4ee5\u5f3a\u8c03\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u89e3\u51b3\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u591a\u4efb\u52a1\u4f18\u5316\u6311\u6218\u3002", "method": "TOAST\u7ed3\u5408\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u5e73\u8861\u3001\u57fa\u4e8eSwin Transformer\u7684\u4f4e\u79e9\u9002\u5e94\u673a\u5236\u548c\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u9002\u5e94\u4e0d\u540c\u4fe1\u9053\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTOAST\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "TOAST\u4e3a6G\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21937", "pdf": "https://arxiv.org/pdf/2506.21937", "abs": "https://arxiv.org/abs/2506.21937", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification", "categories": ["cs.LG"], "comment": null, "summary": "We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain\ntumor classification using MRI images. Trained on a dataset of 7,576 scans\ncovering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC\nintegrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized\nvia AdamW and a composite loss blending cross-entropy and attention\nconsistency.\n  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical\nbaseline (86.72%). It delivers higher precision and F1-scores, especially for\nglioma detection. t-SNE projections reveal enhanced feature separability in\nquantum space, and confusion matrices show lower misclassification. Attention\nmap analysis (Jaccard Index) confirms more accurate and focused tumor\nlocalization at high-confidence thresholds.\n  These results highlight the promise of quantum-enhanced models in medical\nimaging, advancing both diagnostic accuracy and interpretability for clinical\nbrain tumor assessment.", "AI": {"tldr": "HQCM-EBTC\u662f\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u7528\u4e8eMRI\u56fe\u50cf\u7684\u81ea\u52a8\u8111\u80bf\u7624\u5206\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63d0\u9ad8\u8111\u80bf\u7624\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a2\u7d22\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7ed3\u54085\u91cf\u5b50\u6bd4\u7279\u3001\u6df1\u5ea6\u4e3a2\u7684\u91cf\u5b50\u5c42\u4e0e5\u4e2a\u5e76\u884c\u7535\u8def\uff0c\u4f7f\u7528AdamW\u4f18\u5316\u5668\u548c\u6df7\u5408\u635f\u5931\u51fd\u6570\uff08\u4ea4\u53c9\u71b5\u548c\u6ce8\u610f\u529b\u4e00\u81f4\u6027\uff09\u3002", "result": "\u57287,576\u5f20\u626b\u63cf\u56fe\u50cf\u4e0a\u8fbe\u523096.48%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u768486.72%\uff0c\u5c24\u5176\u5728\u80f6\u8d28\u7624\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.21940", "pdf": "https://arxiv.org/pdf/2506.21940", "abs": "https://arxiv.org/abs/2506.21940", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus", "categories": ["cs.LG"], "comment": null, "summary": "Variational Quantum Algorithms (VQAs) offer potential for near-term quantum\nadvantage but face challenges from barren plateaus, where gradients vanish, and\npoorly conditioned optimization landscapes. We introduce GuiderNet, a\nmeta-learning framework that conditions Parameterized Quantum Circuits (PQCs)\nusing data-dependent parameter shifts aimed at minimizing the log condition\nnumber of the Fubini-Study metric tensor. Implemented as a classical neural\nnetwork, GuiderNet is meta-trained to guide PQC parameters into geometrically\nfavorable regions and is embedded within hybrid quantum-classical pipelines to\nsteer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces\ncumulative training loss by over 5x, improves test accuracy from 75.3% to\n98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also\nsuppresses gradient explosion and stabilizes parameter updates, enabling\nsmoother and more robust optimization. These results demonstrate that geometric\nmeta-conditioning can mitigate barren plateaus and ill-conditioning, providing\na scalable approach to enhance trainability and generalization in quantum\nmachine learning.", "AI": {"tldr": "GuiderNet\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u51e0\u4f55\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQAs\uff09\u5728\u8fd1\u671f\u91cf\u5b50\u4f18\u52bf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u68af\u5ea6\u6d88\u5931\u548c\u4f18\u5316\u6761\u4ef6\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165GuiderNet\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u7684\u53c2\u6570\u8c03\u6574\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u51e0\u4f55\u6761\u4ef6\u3002", "result": "\u5728\u7cd6\u5c3f\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGuiderNet\u5c06\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece75.3%\u63d0\u5347\u81f398.6%\uff0c\u5e76\u663e\u8457\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "\u51e0\u4f55\u5143\u6761\u4ef6\u5316\u53ef\u7f13\u89e3\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u548c\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u53ef\u8bad\u7ec3\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.21952", "pdf": "https://arxiv.org/pdf/2506.21952", "abs": "https://arxiv.org/abs/2506.21952", "authors": ["Yangyang Wan", "Haotian Wang", "Xuhui Yu", "Jiageng Chen", "Xinyu Fan", "Zuyuan He"], "title": "Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications", "categories": ["cs.LG", "physics.app-ph", "physics.optics"], "comment": null, "summary": "Distributed acoustic sensing (DAS) has attracted considerable attention\nacross various fields and artificial intelligence (AI) technology plays an\nimportant role in DAS applications to realize event recognition and denoising.\nExisting AI models require real-world data (RWD), whether labeled or not, for\ntraining, which is contradictory to the fact of limited available event data in\nreal-world scenarios. Here, a physics-informed DAS neural network paradigm is\nproposed, which does not need real-world events data for training. By\nphysically modeling target events and the constraints of real world and DAS\nsystem, physical functions are derived to train a generative network for\ngeneration of DAS events data. DAS debackground net is trained by using the\ngenerated DAS events data to eliminate background noise in DAS data. The\neffectiveness of the proposed paradigm is verified in event identification\napplication based on a public dataset of DAS spatiotemporal data and in belt\nconveyor fault monitoring application based on DAS time-frequency data, and\nachieved comparable or better performance than data-driven networks trained\nwith RWD. Owing to the introduction of physical information and capability of\nbackground noise removal, the paradigm demonstrates generalization in same\napplication on different sites. A fault diagnosis accuracy of 91.8% is achieved\nin belt conveyor field with networks which transferred from simulation test\nsite without any fault events data of test site and field for training. The\nproposed paradigm is a prospective solution to address significant obstacles of\ndata acquisition and intense noise in practical DAS applications and explore\nmore potential fields for DAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684DAS\u795e\u7ecf\u7f51\u7edc\u8303\u5f0f\uff0c\u65e0\u9700\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210\u6570\u636e\uff0c\u5e76\u5728\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3DAS\u5e94\u7528\u4e2d\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u53bb\u566a\u548c\u4e8b\u4ef6\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u751f\u6210DAS\u4e8b\u4ef6\u6570\u636e\uff0c\u8bad\u7ec3\u751f\u6210\u7f51\u7edc\u548c\u53bb\u80cc\u666f\u7f51\u7edc\uff0c\u5e94\u7528\u4e8e\u4e8b\u4ef6\u8bc6\u522b\u548c\u6545\u969c\u76d1\u6d4b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u6a21\u578b\uff0c\u6545\u969c\u8bca\u65ad\u51c6\u786e\u7387\u8fbe91.8%\u3002", "conclusion": "\u8be5\u8303\u5f0f\u4e3a\u89e3\u51b3DAS\u6570\u636e\u83b7\u53d6\u548c\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86DAS\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21956", "pdf": "https://arxiv.org/pdf/2506.21956", "abs": "https://arxiv.org/abs/2506.21956", "authors": ["Hao Jiang", "Yongxiang Tang", "Yanxiang Zeng", "Pengjia Yuan", "Yanhua Cheng", "Teng Sha", "Xialong Liu", "Peng Jiang"], "title": "Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement", "categories": ["cs.LG"], "comment": null, "summary": "In the realm of online advertising, advertisers partake in ad auctions to\nobtain advertising slots, frequently taking advantage of auto-bidding tools\nprovided by demand-side platforms. To improve the automation of these bidding\nsystems, we adopt generative models, namely the Decision Transformer (DT), to\ntackle the difficulties inherent in automated bidding. Applying the Decision\nTransformer to the auto-bidding task enables a unified approach to sequential\nmodeling, which efficiently overcomes short-sightedness by capturing long-term\ndependencies between past bidding actions and user behavior. Nevertheless,\nconventional DT has certain drawbacks: (1) DT necessitates a preset\nreturn-to-go (RTG) value before generating actions, which is not inherently\nproduced; (2) The policy learned by DT is restricted by its training data,\nwhich is consists of mixed-quality trajectories. To address these challenges,\nwe introduce the R* Decision Transformer (R* DT), developed in a three-step\nprocess: (1) R DT: Similar to traditional DT, R DT stores actions based on\nstate and RTG value, as well as memorizing the RTG for a given state using the\ntraining set; (2) R^ DT: We forecast the highest value (within the training\nset) of RTG for a given state, deriving a suboptimal policy based on the\ncurrent state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,\nwe generate trajectories and select those with high rewards (using a simulator)\nto augment our training dataset. This data enhancement has been shown to\nimprove the RTG of trajectories in the training data and gradually leads the\nsuboptimal policy towards optimality. Comprehensive tests on a publicly\navailable bidding dataset validate the R* DT's efficacy and highlight its\nsuperiority when dealing with mixed-quality trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u51b3\u7b56\u53d8\u6362\u5668\uff08R* DT\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u7ade\u4ef7\u4efb\u52a1\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4f18\u5316RTG\u503c\uff0c\u63d0\u5347\u4e86\u7ade\u4ef7\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5e7f\u544a\u7ade\u4ef7\u4e2d\uff0c\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u5b58\u5728RTG\u503c\u9884\u8bbe\u548c\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u7ade\u4ef7\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faR* DT\uff0c\u5206\u4e09\u6b65\uff1aR DT\u5b58\u50a8\u72b6\u6001\u548cRTG\u503c\uff1bR^ DT\u9884\u6d4b\u6700\u4f18RTG\u503c\uff1bR* DT\u901a\u8fc7\u751f\u6210\u9ad8\u5956\u52b1\u8f68\u8ff9\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u7ade\u4ef7\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86R* DT\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u6df7\u5408\u8d28\u91cf\u8f68\u8ff9\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "R* DT\u901a\u8fc7\u4f18\u5316RTG\u503c\u548c\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u7ade\u4ef7\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21976", "pdf": "https://arxiv.org/pdf/2506.21976", "abs": "https://arxiv.org/abs/2506.21976", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "comment": "Accepted to CVPR 2025", "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "AI": {"tldr": "\u63d0\u51faCitySim\u613f\u666f\uff0c\u901a\u8fc7SceneDiffuser++\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u57ce\u5e02\u89c4\u6a21\u4ea4\u901a\u6a21\u62df\uff0c\u6574\u5408\u573a\u666f\u751f\u6210\u3001\u884c\u4e3a\u5efa\u6a21\u7b49\u6280\u672f\uff0c\u9a8c\u8bc1\u5176\u5728\u957f\u65f6\u6a21\u62df\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4ea4\u901a\u6a21\u62df\u4e2d\u624b\u52a8\u9a7e\u9a76\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u6a21\u62df\u57ce\u5e02\u73af\u5883\uff0c\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u7684\u6d4b\u8bd5\u4e0e\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faSceneDiffuser++\uff0c\u9996\u4e2a\u57fa\u4e8e\u5355\u4e00\u635f\u5931\u51fd\u6570\u7684\u7aef\u5230\u7aef\u751f\u6210\u4e16\u754c\u6a21\u578b\uff0c\u6574\u5408\u573a\u666f\u751f\u6210\u3001\u52a8\u6001\u4ee3\u7406\u884c\u4e3a\u5efa\u6a21\u7b49\u6280\u672f\u3002", "result": "\u5728\u6269\u5c55\u7248Waymo Open Motion Dataset\u4e0a\u9a8c\u8bc1\u4e86SceneDiffuser++\u7684\u57ce\u5e02\u89c4\u6a21\u6a21\u62df\u80fd\u529b\u53ca\u957f\u65f6\u6a21\u62df\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SceneDiffuser++\u4e3a\u57ce\u5e02\u89c4\u6a21\u4ea4\u901a\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u771f\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u52a8\u6001\u573a\u666f\u751f\u6210\u7b49\u6280\u672f\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.21997", "pdf": "https://arxiv.org/pdf/2506.21997", "abs": "https://arxiv.org/abs/2506.21997", "authors": ["Rafael Sojo", "Javier D\u00edaz-Rozo", "Concha Bielza", "Pedro Larra\u00f1aga"], "title": "Binned semiparametric Bayesian networks", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "comment": null, "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u534a\u53c2\u6570\u6a21\u578b\uff0c\u5229\u7528\u6570\u636e\u5206\u7bb1\u964d\u4f4e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u8fc7\u7a00\u758f\u5f20\u91cf\u548c\u9650\u5236\u7236\u8282\u70b9\u6570\u91cf\u89e3\u51b3\u7ef4\u5ea6\u707e\u96be\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4e0e\u672a\u5206\u7bb1\u6a21\u578b\u76f8\u5f53\u4f46\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u89e3\u51b3\u975e\u53c2\u6570\u5206\u5e03\u4e2d\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u5206\u7bb1\u6a21\u578b\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff08\u7a00\u758f\u5206\u7bb1\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5085\u91cc\u53f6\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff09\uff0c\u5229\u7528\u7a00\u758f\u5f20\u91cf\u548c\u9650\u5236\u7236\u8282\u70b9\u6570\u91cf\u4f18\u5316\u8ba1\u7b97\u3002", "result": "\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u7ed3\u6784\u5b66\u4e60\u548c\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u4e0a\u4e0e\u672a\u5206\u7bb1\u6a21\u578b\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.22004", "pdf": "https://arxiv.org/pdf/2506.22004", "abs": "https://arxiv.org/abs/2506.22004", "authors": ["Mohammad Sabbaqi", "Riccardo Taormina", "Elvin Isufi"], "title": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u4e86\u56fe\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u548c\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u53c2\u6570\u5b66\u4e60\u548c\u72b6\u6001\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u56fe\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u56fe-\u65f6\u95f4\u6a21\u5f0f\u7684\u8054\u5408\u5efa\u6a21\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u6c34\u7f51\u3001\u7ecf\u6d4e\u5b66\u548c\u7f51\u7edc\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u3002", "method": "\u4f7f\u7528\u56fe\u8bf1\u5bfc\u7684\u53c2\u6570\u91cf\u5316\u6a21\u578b\u6784\u5efa\u72b6\u6001\u548c\u89c2\u6d4b\u65b9\u7a0b\uff0c\u72b6\u6001\u65b9\u7a0b\u57fa\u4e8e\u56fe\u4e0a\u7684\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u89c2\u6d4b\u6a21\u578b\u4e3a\u56fe\u6ee4\u6ce2\u91c7\u6837\u7248\u672c\u3002", "result": "\u63d0\u51fa\u4e86\u7406\u8bba\u53ef\u8ffd\u8e2a\u7684\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u548c\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u53c2\u6570\u5b66\u4e60\u548c\u72b6\u6001\u8ddf\u8e2a\u3002", "conclusion": "\u6a21\u578b\u5728\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u4e0b\u6709\u6548\uff0c\u9002\u7528\u4e8e\u9884\u6d4b\u548c\u63d2\u8865\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.22008", "pdf": "https://arxiv.org/pdf/2506.22008", "abs": "https://arxiv.org/abs/2506.22008", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gissl\u00e9n"], "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTROFI\u7684\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u5373\u53ef\u6709\u6548\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u5e38\u9700\u8981\u9884\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u6765\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u89c6\u9891\u6e38\u620f\u5f00\u53d1\uff09\u5956\u52b1\u51fd\u6570\u53ef\u80fd\u4e0d\u53ef\u7528\u3002", "method": "TROFI\u901a\u8fc7\u4ece\u4eba\u7c7b\u504f\u597d\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u7528\u5176\u6807\u8bb0\u539f\u59cb\u6570\u636e\u96c6\uff0c\u4ece\u800c\u8bad\u7ec3\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6700\u4f18\u8f68\u8ff9\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTROFI\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f7f\u7528\u771f\u5b9e\u5956\u52b1\u51fd\u6570\u5b66\u4e60\u7b56\u7565\u7684\u6548\u679c\u76f8\u5f53\u3002\u57283D\u6e38\u620f\u73af\u5883\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5956\u52b1\u51fd\u6570\u7684\u8bbe\u8ba1\u5bf9\u7b56\u7565\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u826f\u597d\u7684\u5956\u52b1\u51fd\u6570\u80fd\u786e\u4fdd\u4ef7\u503c\u51fd\u6570\u4e0e\u5b9e\u9645\u672a\u6765\u5956\u52b1\u5bf9\u9f50\u3002"}}
{"id": "2506.22036", "pdf": "https://arxiv.org/pdf/2506.22036", "abs": "https://arxiv.org/abs/2506.22036", "authors": ["Ying Zhang", "Yu Zhao", "Xuhui Sui", "Baohang Zhou", "Xiangrui Cai", "Li Shen", "Xiaojie Yuan", "Dacheng Tao"], "title": "Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion", "categories": ["cs.LG", "cs.MM"], "comment": "Submitted to the IEEE for possible publication", "summary": "With the increasing multimodal knowledge privatization requirements,\nmultimodal knowledge graphs in different institutes are usually decentralized,\nlacking of effective collaboration system with both stronger reasoning ability\nand transmission safety guarantees. In this paper, we propose the Federated\nMultimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over\nfederated MKGs for better predicting the missing links in clients without\nsharing sensitive knowledge. We propose a framework named MMFeD3-HidE for\naddressing multimodal uncertain unavailability and multimodal client\nheterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed\nHyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete\nmultimodal distributions from incomplete entity embeddings constrained by\navailable modalities. (2) Among clients, our proposed Multimodal FeDerated Dual\nDistillation (MMFeD3) transfers knowledge mutually between clients and the\nserver with logit and feature distillation to improve both global convergence\nand semantic consistency. We propose a FedMKGC benchmark for a comprehensive\nevaluation, consisting of a general FedMKGC backbone named MMFedE, datasets\nwith heterogeneous multimodal information, and three groups of constructed\nbaselines. Experiments conducted on our benchmark validate the effectiveness,\nsemantic consistency, and convergence robustness of MMFeD3-HidE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8054\u90a6\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\uff08FedMKGC\uff09\uff0c\u5e76\u63d0\u51fa\u4e86MMFeD3-HidE\u6846\u67b6\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u6311\u6218\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u77e5\u8bc6\u79c1\u6709\u5316\u9700\u6c42\u7684\u589e\u52a0\uff0c\u5206\u6563\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u6709\u6548\u7684\u534f\u4f5c\u7cfb\u7edf\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u8f93\u5b89\u5168\u4fdd\u969c\u3002", "method": "\u63d0\u51fa\u4e86HidE\u6a21\u578b\u7528\u4e8e\u5ba2\u6237\u7aef\u5185\u6062\u590d\u5b8c\u6574\u591a\u6a21\u6001\u5206\u5e03\uff0c\u4ee5\u53caMMFeD3\u65b9\u6cd5\u7528\u4e8e\u5ba2\u6237\u7aef\u95f4\u77e5\u8bc6\u4e92\u4f20\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MMFeD3-HidE\u7684\u6709\u6548\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6536\u655b\u9c81\u68d2\u6027\u3002", "conclusion": "MMFeD3-HidE\u4e3a\u8054\u90a6\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22039", "pdf": "https://arxiv.org/pdf/2506.22039", "abs": "https://arxiv.org/abs/2506.22039", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "AI": {"tldr": "UniCA\u6846\u67b6\u901a\u8fc7\u5c06\u5f02\u6784\u534f\u53d8\u91cf\u8f6c\u5316\u4e3a\u540c\u6784\u8868\u793a\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\uff0c\u6269\u5c55\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u9002\u7528\u6027\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709TSFMs\u4e3b\u8981\u9488\u5bf9\u5b9e\u503c\u5e8f\u5217\uff0c\u96be\u4ee5\u5904\u7406\u5305\u542b\u5f02\u6784\u534f\u53d8\u91cf\uff08\u5982\u5206\u7c7b\u53d8\u91cf\u3001\u56fe\u50cf\u3001\u6587\u672c\uff09\u7684\u9884\u6d4b\u4efb\u52a1\u3002UniCA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "UniCA\u901a\u8fc7\u534f\u53d8\u91cf\u540c\u8d28\u5316\u5c06\u5f02\u6784\u6570\u636e\u8f6c\u5316\u4e3a\u540c\u6784\u5e8f\u5217\u8868\u793a\uff0c\u5e76\u91c7\u7528\u7edf\u4e00\u7684\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u3002", "result": "\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u534f\u53d8\u91cf\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniCA\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniCA\u5c55\u793a\u4e86\u534f\u53d8\u91cf\u611f\u77e5TSFM\u9002\u5e94\u5728\u73b0\u5b9e\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22049", "pdf": "https://arxiv.org/pdf/2506.22049", "abs": "https://arxiv.org/abs/2506.22049", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPAS\u7684\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3Pre-LN Transformer\u4e2d\u6fc0\u6d3b\u65b9\u5dee\u6307\u6570\u589e\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7f29\u653e\u4e2d\u95f4\u6fc0\u6d3b\u503c\u4f46\u4fdd\u6301\u68af\u5ea6\u4e0d\u53d8\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Pre-LN Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u6fc0\u6d3b\u65b9\u5dee\u6307\u6570\u589e\u957f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6df1\u5c42\u5b66\u4e60\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51faGPAS\u6280\u672f\uff0c\u901a\u8fc7\u7f29\u653e\u4e2d\u95f4\u6fc0\u6d3b\u503c\u4f46\u4fdd\u6301\u68af\u5ea6\u4e0d\u53d8\uff0c\u907f\u514d\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u572871M\u52301B\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cGPAS\u5747\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u9002\u7528\u4e8e\u5176\u4ed6\u67b6\u6784\u5982Sandwich-LN\u548cDeepNorm\u3002", "conclusion": "GPAS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u63d0\u5347Pre-LN\u53ca\u5176\u4ed6\u67b6\u6784\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6027\u80fd\u3002"}}
{"id": "2506.22055", "pdf": "https://arxiv.org/pdf/2506.22055", "abs": "https://arxiv.org/abs/2506.22055", "authors": ["Mehul Gautam"], "title": "crypto price prediction using lstm+xgboost", "categories": ["cs.LG"], "comment": null, "summary": "The volatility and complex dynamics of cryptocurrency markets present unique\nchallenges for accurate price forecasting. This research proposes a hybrid deep\nlearning and machine learning model that integrates Long Short-Term Memory\n(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency\nprice prediction. The LSTM component captures temporal dependencies in\nhistorical price data, while XGBoost enhances prediction by modeling nonlinear\nrelationships with auxiliary features such as sentiment scores and\nmacroeconomic indicators. The model is evaluated on historical datasets of\nBitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and\nlocalized exchange data. Comparative analysis using Mean Absolute Percentage\nError (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)\ndemonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone\nmodels and traditional forecasting methods. This study underscores the\npotential of hybrid architectures in financial forecasting and provides\ninsights into model adaptability across different cryptocurrencies and market\ncontexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cXGBoost\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u9884\u6d4b\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u6ce2\u52a8\u6027\u548c\u590d\u6742\u6027\u5bf9\u4ef7\u683c\u9884\u6d4b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u6574\u5408LSTM\uff08\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff09\u548cXGBoost\uff08\u5efa\u6a21\u975e\u7ebf\u6027\u5173\u7cfb\uff09\uff0c\u7ed3\u5408\u5386\u53f2\u4ef7\u683c\u6570\u636e\u548c\u8f85\u52a9\u7279\u5f81\uff08\u5982\u60c5\u611f\u5206\u6570\u548c\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\uff09\u3002", "result": "\u5728\u6bd4\u7279\u5e01\u3001\u4ee5\u592a\u574a\u7b49\u52a0\u5bc6\u8d27\u5e01\u6570\u636e\u96c6\u4e0a\uff0cLSTM+XGBoost\u6df7\u5408\u6a21\u578b\u5728MAPE\u548cMinMax RMSE\u6307\u6807\u4e0a\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u4e0d\u540c\u52a0\u5bc6\u8d27\u5e01\u548c\u5e02\u573a\u73af\u5883\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2506.22084", "pdf": "https://arxiv.org/pdf/2506.22084", "abs": "https://arxiv.org/abs/2506.22084", "authors": ["Chaitanya K. Joshi"], "title": "Transformers are Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86Transformer\u67b6\u6784\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u6307\u51faTransformer\u53ef\u89c6\u4e3a\u5728\u5b8c\u5168\u8fde\u63a5\u56fe\u4e0a\u64cd\u4f5c\u7684\u6d88\u606f\u4f20\u9012GNN\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u786c\u4ef6\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u63a2\u8ba8Transformer\u4e0eGNN\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63ed\u793aTransformer\u5728\u786c\u4ef6\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5c06Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u91ca\u4e3a\u6d88\u606f\u4f20\u9012GNN\uff0c\u5206\u6790\u5176\u5728\u5b8c\u5168\u8fde\u63a5\u56fe\u4e0a\u7684\u64cd\u4f5c\u3002", "result": "Transformer\u662f\u4e00\u79cd\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u96c6\u5408\u5904\u7406\u7f51\u7edc\uff0c\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u8f93\u5165\u5143\u7d20\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "Transformer\u662f\u5f53\u524d\u786c\u4ef6\u73af\u5883\u4e0b\u9ad8\u6548\u7684GNN\u5b9e\u73b0\uff0c\u5c55\u73b0\u4e86\u786c\u4ef6\u4f18\u52bf\u5bf9\u7b97\u6cd5\u9009\u62e9\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.22095", "pdf": "https://arxiv.org/pdf/2506.22095", "abs": "https://arxiv.org/abs/2506.22095", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Bal\u00e1zs Kulcs\u00e1r"], "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 5 Figures", "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u795e\u7ecf\u65b9\u6cd5\u89e3\u51b3\u591a\u76ee\u6807\u591a\u56fe\u8def\u7531\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u591a\u56fe\u73af\u5883\u4e0b\u7684\u591a\u76ee\u6807\u8def\u7531\u95ee\u9898\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u7b2c\u4e00\u79cd\u65b9\u6cd5\u76f4\u63a5\u5728\u591a\u56fe\u4e0a\u81ea\u56de\u5f52\u9009\u62e9\u8fb9\uff1b\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5148\u526a\u679d\u4e3a\u7b80\u5355\u56fe\u518d\u6784\u5efa\u8def\u5f84\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728TSP\u548cCVRP\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u591a\u56fe\u8def\u7531\u4e2d\u6709\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2506.22096", "pdf": "https://arxiv.org/pdf/2506.22096", "abs": "https://arxiv.org/abs/2506.22096", "authors": ["Tin Lai", "Farnaz Farid", "Yueyang Kuan", "Xintian Zhang"], "title": "Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments", "categories": ["cs.LG"], "comment": null, "summary": "Detecting heavy metal pollution in soils and seaports is vital for regional\nenvironmental monitoring. The Pollution Load Index (PLI), an international\nstandard, is commonly used to assess heavy metal containment. However, the\nconventional PLI assessment involves laborious procedures and data analysis of\nsediment samples. To address this challenge, we propose a deep-learning-based\nmodel that simplifies the heavy metal assessment process. Our model tackles the\nissue of data scarcity in the water-sediment domain, which is traditionally\nplagued by challenges in data collection and varying standards across nations.\nBy leveraging transfer learning, we develop an accurate quantitative assessment\nmethod for predicting PLI. Our approach allows the transfer of learned features\nacross domains with different sets of features. We evaluate our model using\ndata from six major ports in New South Wales, Australia: Port Yamba, Port\nNewcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results\ndemonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute\nPercentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared\nto other models. Our model performance is up to 2 orders of magnitude than\nother baseline models. Our proposed model offers an innovative, accessible, and\ncost-effective approach to predicting water quality, benefiting marine life\nconservation, aquaculture, and industrial pollution monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7b80\u5316\u91cd\u91d1\u5c5e\u6c61\u67d3\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u6fb3\u5927\u5229\u4e9a\u516d\u4e2a\u6e2f\u53e3\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edfPLI\u8bc4\u4f30\u65b9\u6cd5\u7e41\u7410\u4e14\u6570\u636e\u6536\u96c6\u56f0\u96be\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u91cd\u91d1\u5c5e\u6c61\u67d3\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8de8\u9886\u57df\u8f6c\u79fb\u5b66\u4e60\u7279\u5f81\uff0c\u9884\u6d4bPLI\u3002", "result": "\u6a21\u578b\u5728\u516d\u4e2a\u6e2f\u53e3\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0cMAE\u548cMAPE\u5206\u522b\u4e3a0.5\u548c0.03\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u6c34\u8d28\u9884\u6d4b\u63d0\u4f9b\u4e86\u521b\u65b0\u3001\u6613\u7528\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u5bf9\u73af\u5883\u4fdd\u62a4\u548c\u5de5\u4e1a\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.22129", "pdf": "https://arxiv.org/pdf/2506.22129", "abs": "https://arxiv.org/abs/2506.22129", "authors": ["Anurag Panda", "Gaurav Kumar Yadav"], "title": "Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models", "categories": ["cs.LG"], "comment": "3rd International Conference on Applied Mathematics in Science and\n  Engineering", "summary": "In the aftermath of major earthquakes, evaluating structural and\ninfrastructural damage is vital for coordinating post-disaster response\nefforts. This includes assessing damage's extent and spatial distribution to\nprioritize rescue operations and resource allocation. Accurately estimating\ndamage grades to buildings post-earthquake is paramount for effective response\nand recovery, given the significant impact on lives and properties,\nunderscoring the urgency of streamlining relief fund allocation processes.\nPrevious studies have shown the effectiveness of multi-class classification,\nespecially XGBoost, along with other machine learning models and ensembling\nmethods, incorporating regularization to address class imbalance. One\nconsequence of class imbalance is that it may give rise to skewed models that\nundervalue minority classes and give preference to the majority class. This\nresearch deals with the problem of class imbalance with the help of the\nsynthetic minority oversampling technique (SMOTE). We delve into multiple\nmulti-class classification machine learning, deep learning models, and\nensembling methods to forecast structural damage grades. The study elucidates\nperformance determinants through comprehensive feature manipulation experiments\nand diverse training approaches. It identifies key factors contributing to\nseismic vulnerability while evaluating model performance using techniques like\nthe confusion matrix further to enhance understanding of the effectiveness of\nearthquake damage prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5730\u9707\u540e\u5efa\u7b51\u7ed3\u6784\u635f\u574f\u7b49\u7ea7\u7684\u591a\u5206\u7c7b\u9884\u6d4b\uff0c\u901a\u8fc7SMOTE\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5730\u9707\u540e\u51c6\u786e\u8bc4\u4f30\u5efa\u7b51\u635f\u574f\u7b49\u7ea7\u5bf9\u6551\u63f4\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u9884\u6d4b\u6548\u679c\u3002", "method": "\u4f7f\u7528SMOTE\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u7ed3\u5408\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982XGBoost\u548c\u96c6\u6210\u65b9\u6cd5\uff09\u8fdb\u884c\u591a\u5206\u7c7b\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u5b9e\u9a8c\u548c\u6df7\u6dc6\u77e9\u9635\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0SMOTE\u80fd\u6709\u6548\u6539\u5584\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u8bc6\u522b\u51fa\u5f71\u54cd\u5730\u9707\u8106\u5f31\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5730\u9707\u540e\u5efa\u7b51\u635f\u574f\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u6551\u63f4\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2506.22186", "pdf": "https://arxiv.org/pdf/2506.22186", "abs": "https://arxiv.org/abs/2506.22186", "authors": ["Kaikai Zheng", "Dawei Shi", "Yang Shi", "Long Wang"], "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems", "categories": ["cs.LG"], "comment": null, "summary": "Thompson sampling (TS) is an effective method to explore parametric\nuncertainties and can therefore be used for active learning-based controller\ndesign. However, TS relies on finite parametric representations, which limits\nits applicability to more general spaces, which are more commonly encountered\nin control system design. To address this issue, this work pro poses a\nparameterization method for control law learning using reproducing kernel\nHilbert spaces and designs a data-driven active learning control approach.\nSpecifically, the proposed method treats the control law as an element in a\nfunction space, allowing the design of control laws without imposing\nrestrictions on the system structure or the form of the controller. A TS\nframework is proposed in this work to explore potential optimal control laws,\nand the convergence guarantees are further provided for the learning process.\nTheoretical analysis shows that the proposed method learns the relationship\nbetween control laws and closed-loop performance metrics at an exponential\nrate, and the upper bound of control regret is also derived. Numerical\nexperiments on controlling unknown nonlinear systems validate the effectiveness\nof the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u5f8b\u5b66\u4e60\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6570\u636e\u9a71\u52a8\u7684\u4e3b\u52a8\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "Thompson\u91c7\u6837\uff08TS\uff09\u4f9d\u8d56\u4e8e\u6709\u9650\u7684\u53c2\u6570\u8868\u793a\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u4e00\u822c\u7684\u63a7\u5236\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u63a7\u5236\u5f8b\u89c6\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u5143\u7d20\uff0c\u5229\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5e76\u8bbe\u8ba1TS\u6846\u67b6\u63a2\u7d22\u6700\u4f18\u63a7\u5236\u5f8b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u6307\u6570\u901f\u7387\u5b66\u4e60\u63a7\u5236\u5f8b\u4e0e\u95ed\u73af\u6027\u80fd\u6307\u6807\u7684\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfc\u4e86\u63a7\u5236\u9057\u61be\u7684\u4e0a\u754c\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u9650\u5236\u7cfb\u7edf\u7ed3\u6784\u6216\u63a7\u5236\u5668\u5f62\u5f0f\u7684\u60c5\u51b5\u4e0b\u8bbe\u8ba1\u63a7\u5236\u5f8b\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.22189", "pdf": "https://arxiv.org/pdf/2506.22189", "abs": "https://arxiv.org/abs/2506.22189", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sj\u00f6lund"], "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6a21\u5757\u5316\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u548c\u5de5\u5177\u8c03\u7528\u4e0e\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22LLM\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u6a21\u5757\u5316\uff08\u5982LLM\u7684\u53ef\u66ff\u6362\u6027\uff09\u8fd9\u4e00\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\u3002", "method": "\u6bd4\u8f83\u4e0d\u540cLLM\uff08\u5982Claude-3.5-Sonnet\u3001GPT-4o\u7b49\uff09\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u5de5\u5177\u8c03\u7528\u4e0e\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\u7684\u6548\u679c\uff0c\u4f7f\u7528LLM-as-a-judge\u8bc4\u5206\u3002", "result": "Claude-3.5-Sonnet\u3001Claude-3.7-Sonnet\u548cGPT-4o\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\u5e73\u5747\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u7ed3\u679c\u9ad8\u5ea6\u4f9d\u8d56\u95ee\u9898\u548c\u6a21\u578b\u3002", "conclusion": "\u6a21\u5757\u5316\u9700\u7ed3\u5408\u63d0\u793a\u8bcd\u91cd\u65b0\u8bbe\u8ba1\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22190", "pdf": "https://arxiv.org/pdf/2506.22190", "abs": "https://arxiv.org/abs/2506.22190", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "title": "dreaMLearning: Data Compression Assisted Machine Learning", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "comment": "18 pages, 11 figures", "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "AI": {"tldr": "dreaMLearning\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u6570\u636e\u76f4\u63a5\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u9700\u6c42\u95ee\u9898\u3002", "method": "\u57fa\u4e8eEntroGeDe\u7684\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u4efb\u52a1\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u53478.8\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1110\u500d\uff0c\u5b58\u50a8\u8282\u770142%\uff0c\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "dreaMLearning\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2506.22200", "pdf": "https://arxiv.org/pdf/2506.22200", "abs": "https://arxiv.org/abs/2506.22200", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "AI": {"tldr": "EFRame\u6846\u67b6\u901a\u8fc7\u63a2\u7d22\u3001\u8fc7\u6ee4\u548c\u56de\u653e\u673a\u5236\u589e\u5f3aGRPO\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "GRPO\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u63a2\u7d22\u4e0d\u8db3\u3001\u6837\u672c\u6548\u7387\u4f4e\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "EFRame\u901a\u8fc7\u989d\u5916\u63a2\u7d22\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3001\u5728\u7ebf\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\u548c\u5229\u7528\u7ecf\u9a8c\u56de\u653e\uff0c\u6784\u5efa\u5b8c\u6574\u7a33\u5b9a\u7684\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEFRame\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u89e3\u9501\u4e86GRPO\u65e0\u6cd5\u5b9e\u73b0\u7684\u66f4\u6df1\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "EFRame\u4e0d\u4ec5\u4f18\u5316\u4e86GRPO\u7684\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u8bad\u7ec3\u6837\u672c\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\uff0c\u63a8\u52a8\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22253", "pdf": "https://arxiv.org/pdf/2506.22253", "abs": "https://arxiv.org/abs/2506.22253", "authors": ["Shunta Nonaga", "Koji Tabata", "Yuta Mizuno", "Tamiki Komatsuzaki"], "title": "Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence", "categories": ["cs.LG"], "comment": null, "summary": "Decision making under uncertain environments in the maximization of expected\nreward while minimizing its risk is one of the ubiquitous problems in many\nsubjects. Here, we introduce a novel problem setting in stochastic bandit\noptimization that jointly addresses two critical aspects of decision-making:\nmaximizing expected reward and minimizing associated uncertainty, quantified\nvia the mean-variance(MV) criterion. Unlike traditional bandit formulations\nthat focus solely on expected returns, our objective is to efficiently and\naccurately identify the Pareto-optimal set of arms that strikes the best\ntrade-off between expected performance and risk. We propose a unified\nmeta-algorithmic framework capable of operating under both fixed-confidence and\nfixed-budget regimes, achieved through adaptive design of confidence intervals\ntailored to each scenario using the same sample exploration strategy. We\nprovide theoretical guarantees on the correctness of the returned solutions in\nboth settings. To complement this theoretical analysis, we conduct extensive\nempirical evaluations across synthetic benchmarks, demonstrating that our\napproach outperforms existing methods in terms of both accuracy and sample\nefficiency, highlighting its broad applicability to risk-aware decision-making\ntasks in uncertain environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u95ee\u9898\u8bbe\u7f6e\uff0c\u65e8\u5728\u540c\u65f6\u6700\u5927\u5316\u9884\u671f\u5956\u52b1\u548c\u6700\u5c0f\u5316\u98ce\u9669\uff08\u901a\u8fc7\u5747\u503c-\u65b9\u5dee\u51c6\u5219\u8861\u91cf\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5143\u7b97\u6cd5\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u4e24\u79cd\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u8001\u864e\u673a\u95ee\u9898\u4ec5\u5173\u6ce8\u9884\u671f\u56de\u62a5\uff0c\u800c\u5ffd\u7565\u4e86\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9884\u671f\u5956\u52b1\u548c\u98ce\u9669\uff0c\u4e3a\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5143\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bbe\u8ba1\u7684\u7f6e\u4fe1\u533a\u95f4\u548c\u76f8\u540c\u7684\u6837\u672c\u63a2\u7d22\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u4e24\u79cd\u573a\u666f\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.22255", "pdf": "https://arxiv.org/pdf/2506.22255", "abs": "https://arxiv.org/abs/2506.22255", "authors": ["Maciej Stefaniak", "Micha\u0142 Krutul", "Jan Ma\u0142a\u015bnicki", "Maciej Pi\u00f3ro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6295\u5f71\u538b\u7f29\u201d\u7684\u65b0\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u6295\u5f71\u6a21\u5757\u51cf\u5c11\u6a21\u578b\u6743\u91cd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u53c2\u6570\uff0c\u6700\u7ec8\u751f\u6210\u4e00\u4e2a\u66f4\u5c0f\u7684\u6807\u51c6Transformer\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u63a8\u7406\u65f6\u95f4\u548c\u8ba1\u7b97\u9700\u6c42\u4e5f\u968f\u4e4b\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8fd9\u4e9b\u5f00\u9500\u3002", "method": "\u8bad\u7ec3\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u6295\u5f71\u6743\u91cd\uff0c\u4fdd\u7559\u539f\u59cb\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6295\u5f71\u5408\u5e76\u4e3a\u4f4e\u7ef4\u4e58\u79ef\u77e9\u9635\uff0c\u4ece\u800c\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6295\u5f71\u538b\u7f29\u5728\u9ad8\u8d28\u91cf\u6a21\u578b\u4e0a\u4f18\u4e8e\u786c\u526a\u679d\u548c\u518d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u968ftoken\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "\u6295\u5f71\u538b\u7f29\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u5e76\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2506.22295", "pdf": "https://arxiv.org/pdf/2506.22295", "abs": "https://arxiv.org/abs/2506.22295", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Score-Based Model for Low-Rank Tensor Recovery", "categories": ["cs.LG"], "comment": null, "summary": "Low-rank tensor decompositions (TDs) provide an effective framework for\nmultiway data analysis. Traditional TD methods rely on predefined structural\nassumptions, such as CP or Tucker decompositions. From a probabilistic\nperspective, these can be viewed as using Dirac delta distributions to model\nthe relationships between shared factors and the low-rank tensor. However, such\nprior knowledge is rarely available in practical scenarios, particularly\nregarding the optimal rank structure and contraction rules. The optimization\nprocedures based on fixed contraction rules are complex, and approximations\nmade during these processes often lead to accuracy loss. To address this issue,\nwe propose a score-based model that eliminates the need for predefined\nstructural or distributional assumptions, enabling the learning of\ncompatibility between tensors and shared factors. Specifically, a neural\nnetwork is designed to learn the energy function, which is optimized via score\nmatching to capture the gradient of the joint log-probability of tensor entries\nand shared factors. Our method allows for modeling structures and distributions\nbeyond the Dirac delta assumption. Moreover, integrating the block coordinate\ndescent (BCD) algorithm with the proposed smooth regularization enables the\nmodel to perform both tensor completion and denoising. Experimental results\ndemonstrate significant performance improvements across various tensor types,\nincluding sparse and continuous-time tensors, as well as visual data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u5339\u914d\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u7ed3\u6784\u6216\u5206\u5e03\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u7ed3\u6784\u5047\u8bbe\uff08\u5982CP\u6216Tucker\u5206\u89e3\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5f80\u5f80\u4e0d\u9002\u7528\uff0c\u4e14\u4f18\u5316\u8fc7\u7a0b\u590d\u6742\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u4f18\u5316\uff0c\u6355\u6349\u5f20\u91cf\u6761\u76ee\u548c\u5171\u4eab\u56e0\u5b50\u7684\u8054\u5408\u5bf9\u6570\u6982\u7387\u68af\u5ea6\uff0c\u5e76\u7ed3\u5408\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\uff08BCD\uff09\u548c\u5e73\u6ed1\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u5f20\u91cf\u3001\u8fde\u7eed\u65f6\u95f4\u5f20\u91cf\u548c\u89c6\u89c9\u6570\u636e\u7b49\u591a\u79cd\u5f20\u91cf\u7c7b\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u9884\u5b9a\u4e49\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u5f20\u91cf\u5206\u89e3\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.22299", "pdf": "https://arxiv.org/pdf/2506.22299", "abs": "https://arxiv.org/abs/2506.22299", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "icmr", "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "AI": {"tldr": "CoATA\u662f\u4e00\u4e2a\u53cc\u901a\u9053GNN\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u589e\u5f3a\u62d3\u6251\u548c\u5c5e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u56fe\u4e2d\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u56fe\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u4e25\u91cd\u5f71\u54cd\u4e86GNN\u7684\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4e00\u7ef4\u5ea6\u7684\u589e\u5f3a\uff0c\u5ffd\u7565\u4e86\u62d3\u6251\u548c\u5c5e\u6027\u4e4b\u95f4\u7684\u6df1\u5c42\u4ea4\u4e92\u3002", "method": "CoATA\u901a\u8fc7\u53cc\u901a\u9053\u8bbe\u8ba1\uff0c\u5148\u4f20\u64ad\u7ed3\u6784\u4fe1\u53f7\u589e\u5f3a\u8282\u70b9\u5c5e\u6027\uff0c\u518d\u901a\u8fc7\u8282\u70b9-\u5c5e\u6027\u4e8c\u5206\u56fe\u4f18\u5316\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u76f8\u4e92\u6821\u6b63\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoATA\u4f18\u4e8e\u5341\u4e00\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u4e86\u62d3\u6251\u4e0e\u5c5e\u6027\u7684\u534f\u540c\u5173\u7cfb\u3002", "conclusion": "CoATA\u901a\u8fc7\u8054\u5408\u589e\u5f3a\u62d3\u6251\u548c\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u5728\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u56fe\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22301", "pdf": "https://arxiv.org/pdf/2506.22301", "abs": "https://arxiv.org/abs/2506.22301", "authors": ["Takumi Okuo", "Shinnosuke Matsuo", "Shota Harada", "Kiyohito Tanaka", "Ryoma Bise"], "title": "Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling", "categories": ["cs.LG"], "comment": "Accepted at IJCNN2025", "summary": "Domain shift is a significant challenge in machine learning, particularly in\nmedical applications where data distributions differ across institutions due to\nvariations in data collection practices, equipment, and procedures. This can\ndegrade performance when models trained on source domain data are applied to\nthe target domain. Domain adaptation methods have been widely studied to\naddress this issue, but most struggle when class proportions between the source\nand target domains differ. In this paper, we propose a weakly-supervised domain\nadaptation method that leverages class proportion information from the target\ndomain, which is often accessible in medical datasets through prior knowledge\nor statistical reports. Our method assigns pseudo-labels to the unlabeled\ntarget data based on class proportion (called proportion-constrained\npseudo-labeling), improving performance without the need for additional\nannotations. Experiments on two endoscopic datasets demonstrate that our method\noutperforms semi-supervised domain adaptation techniques, even when 5% of the\ntarget domain is labeled. Additionally, the experimental results with noisy\nproportion labels highlight the robustness of our method, further demonstrating\nits effectiveness in real-world application scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u57df\u7684\u7c7b\u522b\u6bd4\u4f8b\u4fe1\u606f\uff0c\u901a\u8fc7\u6bd4\u4f8b\u7ea6\u675f\u4f2a\u6807\u8bb0\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u9886\u57df\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6e90\u57df\u548c\u76ee\u6807\u57df\u7c7b\u522b\u6bd4\u4f8b\u4e0d\u540c\u65f6\u3002", "method": "\u57fa\u4e8e\u76ee\u6807\u57df\u7c7b\u522b\u6bd4\u4f8b\u4fe1\u606f\uff0c\u5bf9\u672a\u6807\u8bb0\u76ee\u6807\u6570\u636e\u5206\u914d\u4f2a\u6807\u7b7e\uff08\u6bd4\u4f8b\u7ea6\u675f\u4f2a\u6807\u8bb0\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u534a\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5373\u4f7f\u76ee\u6807\u57df\u4ec5\u67095%\u6807\u8bb0\u6570\u636e\uff1b\u5bf9\u566a\u58f0\u6bd4\u4f8b\u6807\u7b7e\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u6709\u6548\u4e14\u9c81\u68d2\u3002"}}
{"id": "2506.22304", "pdf": "https://arxiv.org/pdf/2506.22304", "abs": "https://arxiv.org/abs/2506.22304", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "AI": {"tldr": "Koopman-enhanced Conditional Flow Matching (CFM) \u901a\u8fc7\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\uff0c\u52a0\u901fCFM\u91c7\u6837\u5e76\u63d0\u4f9b\u751f\u6210\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7ed3\u6784\u5316\u7684\u751f\u6210\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edfCFM\u91c7\u6837\u4f9d\u8d56\u975e\u7ebf\u6027ODE\u6570\u503c\u89e3\uff0c\u8ba1\u7b97\u6602\u8d35\u4e14\u96be\u4ee5\u89e3\u91ca\uff1b\u73b0\u6709\u65b9\u6cd5\u867d\u63d0\u5347\u901f\u5ea6\u4f46\u672a\u80fd\u63ed\u793a\u751f\u6210\u8fc7\u7a0b\u7684\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u89e3\u7801\u5668\u81ea\u7531\u7684Koopman-CFM\u67b6\u6784\uff0c\u5b66\u4e60\u7ebf\u6027\u52a8\u6001\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u77e9\u9635\u6307\u6570\u5b9e\u73b0\u4e00\u6b65\u95ed\u5f0f\u91c7\u6837\u3002", "result": "\u57282D\u6570\u636e\u96c6\u548cMNIST\u7b49\u57fa\u51c6\u4e0a\u663e\u8457\u52a0\u901f\u91c7\u6837\uff0cKoopman\u751f\u6210\u5668\u7684\u8c31\u7279\u6027\u63d0\u4f9b\u5206\u6790\u5de5\u5177\uff08\u5982\u65f6\u95f4\u7f29\u653e\u3001\u6a21\u5f0f\u7a33\u5b9a\u6027\uff09\u3002", "conclusion": "Koopman\u589e\u5f3a\u7684CFM\u7ed3\u5408\u91c7\u6837\u6548\u7387\u4e0e\u89e3\u6790\u7ed3\u6784\uff0c\u4e3a\u5feb\u901f\u53ef\u89e3\u91ca\u751f\u6210\u5efa\u6a21\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2506.22331", "pdf": "https://arxiv.org/pdf/2506.22331", "abs": "https://arxiv.org/abs/2506.22331", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "title": "Less Greedy Equivalence Search", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": "35 total pages. 14 figures", "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "AI": {"tldr": "LGES\u662fGES\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u66f4\u9488\u5bf9\u6027\u7684\u641c\u7d22\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u652f\u6301\u5148\u9a8c\u5047\u8bbe\u548c\u6570\u636e\u4fee\u6b63\u3002", "motivation": "\u89e3\u51b3GES\u7b97\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u6837\u672c\u51c6\u786e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4fee\u6539\u8d2a\u5a6a\u6b65\u9aa4\uff0c\u907f\u514d\u5728\u6761\u4ef6\u72ec\u7acb\u53d8\u91cf\u95f4\u63d2\u5165\u8fb9\uff0c\u5e76\u652f\u6301\u5148\u9a8c\u5047\u8bbe\u548c\u6570\u636e\u4fee\u6b63\u3002", "result": "LGES\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eGES\uff0c\u4e14\u80fd\u5904\u7406\u9519\u8bef\u5148\u9a8c\u5047\u8bbe\u3002", "conclusion": "LGES\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u89c2\u6d4b\u548c\u5e72\u9884\u6570\u636e\u3002"}}
{"id": "2506.22342", "pdf": "https://arxiv.org/pdf/2506.22342", "abs": "https://arxiv.org/abs/2506.22342", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 6 figures", "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d41\u884c\u75c5\u9884\u6d4b\u548c\u4f20\u64ad\u673a\u5236\u5efa\u6a21\uff0c\u540c\u65f6\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u5728\u6d41\u884c\u75c5\u5b66\u548c\u516c\u5171\u536b\u751f\u5206\u6790\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u90e8\u5206\u6570\u636e\u654f\u611f\uff0c\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u3002\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u56e0\u5176\u5f3a\u4fdd\u969c\u6210\u4e3a\u6807\u51c6\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\uff0c\u6574\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6d41\u884c\u75c5\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528\u591a\u6570\u636e\u96c6\uff08\u5305\u62ecDP\u4fdd\u62a4\u7684\u6570\u636e\uff09\u8fdb\u884c\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d41\u884c\u75c5\u9884\u6d4b\u548c\u4f20\u64ad\u673a\u5236\u5efa\u6a21\uff0c\u6574\u5408\u4e86\u591a\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\uff09\u3002", "result": "\u901a\u8fc7\u5408\u6210\u91d1\u878d\u6570\u636e\u96c6\uff08\u5e26DP\u4fdd\u62a4\uff09\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u5728\u9884\u6d4b\u548c\u5efa\u6a21\u4e2d\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\uff0c\u5229\u7528\u591a\u6570\u636e\u96c6\uff08\u5305\u62ecDP\u4fdd\u62a4\u7684\u6570\u636e\uff09\u8fdb\u884c\u6d41\u884c\u75c5\u5206\u6790\uff0c\u4e3a\u654f\u611f\u6570\u636e\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22365", "pdf": "https://arxiv.org/pdf/2506.22365", "abs": "https://arxiv.org/abs/2506.22365", "authors": ["Tao Li", "Haozhe Lei", "Mingsheng Yin", "Yaqi Hu"], "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation", "categories": ["cs.LG", "cs.RO"], "comment": "Spotlight paper at Reinforcement Learning Conference 2025, Workshop\n  on Inductive Biases in Reinforcement Learning", "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b26\u53f7\u5316\u65b9\u6cd5PiPRL\uff0c\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faPiPRL\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u5316\u7f16\u7a0b\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u7b56\u7565\u3002", "result": "PiPRL\u5728\u5ba4\u5185\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u7eaf\u7b26\u53f7\u6216\u795e\u7ecf\u7b56\u7565\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1126%\u3002", "conclusion": "PiPRL\u901a\u8fc7\u7b26\u53f7\u5316\u65b9\u6cd5\u6709\u6548\u6574\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.22374", "pdf": "https://arxiv.org/pdf/2506.22374", "abs": "https://arxiv.org/abs/2506.22374", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 9 figures", "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "AI": {"tldr": "\u63d0\u51faSheaf-DMFL\u6846\u67b6\uff0c\u5229\u7528\u5c42\u7406\u8bba\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Sheaf-DMFL-Att\u589e\u5f3a\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355\u6a21\u6001\u6570\u636e\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSheaf-DMFL\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u7406\u8bba\u5efa\u6a21\u5ba2\u6237\u7aef\u4efb\u52a1\u5c42\u95f4\u7684\u76f8\u5173\u6027\uff1bSheaf-DMFL-Att\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u591a\u6a21\u6001\u76f8\u5173\u6027\u3002", "result": "\u5728\u94fe\u8def\u963b\u585e\u9884\u6d4b\u548c\u6beb\u7c73\u6ce2\u6ce2\u675f\u6210\u5f62\u7b49\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "Sheaf-DMFL\u6846\u67b6\u5728\u591a\u6a21\u6001\u5f02\u6784\u901a\u4fe1\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.22376", "pdf": "https://arxiv.org/pdf/2506.22376", "abs": "https://arxiv.org/abs/2506.22376", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "title": "Probabilistic Optimality for Inference-time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u63a8\u7406\u65f6\u6269\u5c55\u7684\u6700\u4f18\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u52a8\u6001\u786e\u5b9a\u6700\u4f18\u6837\u672c\u6570\u7684\u7b97\u6cd5OptScale\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u91c7\u6837\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u5e76\u884c\u91c7\u6837\u7b56\u7565\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u65e0\u6cd5\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u590d\u6742\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u6982\u7387\u6846\u67b6\uff0c\u5047\u8bbe\u5e76\u884c\u6837\u672c\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u63a8\u5bfc\u7406\u8bba\u4e0b\u9650\uff0c\u5f00\u53d1\u52a8\u6001\u786e\u5b9a\u6837\u672c\u6570\u7684\u7b97\u6cd5OptScale\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptScale\u663e\u8457\u51cf\u5c11\u91c7\u6837\u5f00\u9500\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u4e3a\u63a8\u7406\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.22389", "pdf": "https://arxiv.org/pdf/2506.22389", "abs": "https://arxiv.org/abs/2506.22389", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "title": "Towards Distributed Neural Architectures", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": "36 pages, 25 figures", "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u795e\u7ecf\u67b6\u6784\uff08DNA\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u548c\u53c2\u6570\u5171\u4eab\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u63a2\u7d22\u8ba1\u7b97\u548c\u53c2\u6570\u5206\u914d\u7684\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u5305\u542b\u591a\u79cd\u6a21\u5757\uff08\u5982Transformer\u3001MLP\u3001\u6ce8\u610f\u529b\u7b49\uff09\u7684\u539f\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b66\u4e60\u52a8\u6001\u8def\u7531\u548c\u8ba1\u7b97\u6a21\u5f0f\u3002", "result": "DNA\u5728\u6027\u80fd\u4e0a\u4e0e\u5bc6\u96c6\u57fa\u7ebf\u76f8\u5f53\uff0c\u5e76\u80fd\u5b66\u4e60\u8ba1\u7b97\u6548\u7387\u548c\u53c2\u6570\u5171\u4eab\uff1b\u8def\u5f84\u5206\u5e03\u7b26\u5408\u5e42\u5f8b\uff0c\u6a21\u5757\u8868\u73b0\u51fa\u4e13\u4e1a\u5316\u3002", "conclusion": "DNA\u5c55\u793a\u4e86\u52a8\u6001\u8ba1\u7b97\u548c\u53c2\u6570\u5206\u914d\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6548\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22393", "pdf": "https://arxiv.org/pdf/2506.22393", "abs": "https://arxiv.org/abs/2506.22393", "authors": ["YongKyung Oh", "Alex Bui"], "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8de8\u9886\u57df\u9002\u5e94\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5177\u6709\u590d\u6742\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u8fd9\u4e9b\u7279\u6027\u3002", "method": "\u91c7\u7528\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6574\u5408\u65f6\u5e8f\u6a21\u5f0f\u3001\u5bfc\u6570\u52a8\u6001\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u72ec\u7acb\u7f16\u7801\u5668\u548c\u5206\u5c42\u878d\u5408\u673a\u5236\u5b66\u4e60\u8de8\u9886\u57df\u53ef\u8fc1\u79fb\u7684\u7279\u5f81\u4e0d\u53d8\u8868\u793a\u3002", "result": "\u5728EEG\u3001ECG\u548cEMG\u7b49\u591a\u6837\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22401", "pdf": "https://arxiv.org/pdf/2506.22401", "abs": "https://arxiv.org/abs/2506.22401", "authors": ["Tong Yang", "Bo Dai", "Lin Xiao", "Yuejie Chi"], "title": "Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Online reinforcement learning (RL) with complex function approximations such\nas transformers and deep neural networks plays a significant role in the modern\npractice of artificial intelligence. Despite its popularity and importance,\nbalancing the fundamental trade-off between exploration and exploitation\nremains a long-standing challenge; in particular, we are still in lack of\nefficient and practical schemes that are backed by theoretical performance\nguarantees. Motivated by recent developments in exploration via optimistic\nregularization, this paper provides an interpretation of the principle of\noptimism through the lens of primal-dual optimization. From this fresh\nperspective, we set forth a new value-incentivized actor-critic (VAC) method,\nwhich optimizes a single easy-to-optimize objective integrating exploration and\nexploitation -- it promotes state-action and policy estimates that are both\nconsistent with collected data transitions and result in higher value\nfunctions. Theoretically, the proposed VAC method has near-optimal regret\nguarantees under linear Markov decision processes (MDPs) in both finite-horizon\nand infinite-horizon settings, which can be extended to the general function\napproximation setting under appropriate assumptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ef7\u503c\u6fc0\u52b1\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08VAC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u59cb-\u5bf9\u5076\u4f18\u5316\u89c6\u89d2\u89e3\u91ca\u4e50\u89c2\u539f\u5219\uff0c\u6574\u5408\u63a2\u7d22\u4e0e\u5f00\u53d1\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u7f3a\u4e4f\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e50\u89c2\u6b63\u5219\u5316\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u63d0\u51faVAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u76ee\u6807\u4f18\u5316\u6574\u5408\u63a2\u7d22\u4e0e\u5f00\u53d1\u3002", "result": "VAC\u65b9\u6cd5\u5728\u6709\u9650\u548c\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u7684\u7ebf\u6027MDP\u4e2d\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u4e00\u822c\u51fd\u6570\u903c\u8fd1\u3002", "conclusion": "VAC\u65b9\u6cd5\u4e3a\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22423", "pdf": "https://arxiv.org/pdf/2506.22423", "abs": "https://arxiv.org/abs/2506.22423", "authors": ["Pritam Dash", "Ethan Chan", "Nathan P. Lawrence", "Karthik Pattabiraman"], "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks", "categories": ["cs.LG", "cs.CR", "cs.RO"], "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,\nnavigation, and control. However, these sensors are susceptible to physical\nattacks, such as GPS spoofing, that can corrupt state estimates and lead to\nunsafe behavior. While reinforcement learning (RL) offers adaptive control\ncapabilities, existing safe RL methods are ineffective against such attacks. We\npresent ARMOR (Adaptive Robust Manipulation-Optimized State Representations),\nan attack-resilient, model-free RL controller that enables robust UAV operation\nunder adversarial sensor manipulation. Instead of relying on raw sensor\nobservations, ARMOR learns a robust latent representation of the UAV's physical\nstate via a two-stage training framework. In the first stage, a teacher\nencoder, trained with privileged attack information, generates attack-aware\nlatent states for RL policy training. In the second stage, a student encoder is\ntrained via supervised learning to approximate the teacher's latent states\nusing only historical sensor data, enabling real-world deployment without\nprivileged information. Our experiments show that ARMOR outperforms\nconventional methods, ensuring UAV safety. Additionally, ARMOR improves\ngeneralization to unseen attacks and reduces training cost by eliminating the\nneed for iterative adversarial training.", "AI": {"tldr": "ARMOR\u662f\u4e00\u79cd\u9488\u5bf9\u65e0\u4eba\u673a\u4f20\u611f\u5668\u653b\u51fb\u7684\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5b66\u4e60\u9c81\u68d2\u72b6\u6001\u8868\u793a\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5b89\u5168\u3002", "motivation": "\u65e0\u4eba\u673a\u4f20\u611f\u5668\u6613\u53d7\u7269\u7406\u653b\u51fb\uff08\u5982GPS\u6b3a\u9a97\uff09\uff0c\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6b64\u65e0\u6548\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u63a7\u5236\u5668\u3002", "method": "ARMOR\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u6559\u5e08\u7f16\u7801\u5668\u751f\u6210\u653b\u51fb\u611f\u77e5\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5b66\u751f\u7f16\u7801\u5668\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8fd1\u4f3c\u6559\u5e08\u72b6\u6001\uff0c\u4ec5\u4f7f\u7528\u5386\u53f2\u4f20\u611f\u5668\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eARMOR\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u5b89\u5168\uff0c\u63d0\u9ad8\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "ARMOR\u4e3a\u65e0\u4eba\u673a\u5728\u5bf9\u6297\u6027\u4f20\u611f\u5668\u653b\u51fb\u4e0b\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22427", "pdf": "https://arxiv.org/pdf/2506.22427", "abs": "https://arxiv.org/abs/2506.22427", "authors": ["Randeep Bhatia", "Nikos Papadis", "Murali Kodialam", "TV Lakshman", "Sayak Chakrabarty"], "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 4 figures", "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.", "AI": {"tldr": "CLoVE\u662f\u4e00\u79cd\u7528\u4e8e\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u635f\u5931\u5411\u91cf\u5d4c\u5165\u8bc6\u522b\u5ba2\u6237\u7fa4\uff0c\u65e0\u9700\u521d\u59cb\u5316\u6700\u4f18\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u76d1\u7763\u548c\u65e0\u76d1\u7763\u4efb\u52a1\u3002", "motivation": "\u5728CFL\u4e2d\uff0c\u5ba2\u6237\u57fa\u4e8e\u6570\u636e\u5206\u5e03\u81ea\u7136\u5206\u7ec4\uff0c\u4f46\u8bc6\u522b\u8fd9\u4e9b\u7fa4\u7ec4\u5177\u6709\u6311\u6218\u6027\u3002CLoVE\u65e8\u5728\u901a\u8fc7\u635f\u5931\u6a21\u5f0f\u5dee\u5f02\u51c6\u786e\u533a\u5206\u5ba2\u6237\u7fa4\u3002", "method": "CLoVE\u5229\u7528\u5ba2\u6237\u6570\u636e\u4e0a\u7684\u6a21\u578b\u635f\u5931\u751f\u6210\u5d4c\u5165\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u79bb\u4e0d\u540c\u7fa4\u7ec4\u7684\u5ba2\u6237\uff0c\u5e76\u4f18\u5316\u7fa4\u7ec4\u7279\u5b9a\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660eCLoVE\u80fd\u9ad8\u6982\u7387\u51c6\u786e\u6062\u590d\u7fa4\u7ec4\uff0c\u5e76\u5728\u7ebf\u6027\u8bbe\u7f6e\u4e2d\u6307\u6570\u7ea7\u5feb\u901f\u6536\u655b\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CLoVE\u5728\u7fa4\u7ec4\u6062\u590d\u548c\u6a21\u578b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709CFL\u548cPFL\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2109.05721", "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faADL\u548cAAM\u65b9\u6cd5\u89e3\u51b3\u4eba\u8138\u5bf9\u9f50\u4e2d\u7684\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7ADNet\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u7814\u7a76\u4eba\u8138\u5bf9\u9f50\u4e2d\u8bef\u5dee\u5206\u5e03\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u8bef\u5dee\u6cbf\u5207\u7ebf\u65b9\u5411\u5206\u5e03\uff0c\u4e0e\u6a21\u7cca\u6807\u6ce8\u4efb\u52a1\u76f8\u5173\u3002", "method": "\u63d0\u51faADL\uff08\u5404\u5411\u5f02\u6027\u65b9\u5411\u635f\u5931\uff09\u548cAAM\uff08\u5404\u5411\u5f02\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5750\u6807\u548c\u70ed\u56fe\u56de\u5f52\u3002", "result": "ADNet\u5728300W\u3001WFLW\u548cCOFW\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "ADL\u548cAAM\u4e92\u8865\u5b66\u4e60\u4eba\u8138\u7ed3\u6784\u548c\u7eb9\u7406\u7ec6\u8282\uff0cADNet\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2212.09525", "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u5730\u6807\u6570\u636e\u96c6\uff08\u5982300W\u548cWFLW\uff09\u589e\u5f3a\u5730\u6807\u5bc6\u5ea6\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5bc6\u96c6\u4eba\u8138\u5bf9\u9f50\u3002", "motivation": "\u5bc6\u96c6\u4eba\u8138\u5730\u6807\u5728\u7f8e\u5bb9\u533b\u5b66\u548c\u9762\u90e8\u7f8e\u5316\u7b49\u573a\u666f\u4e2d\u9700\u6c42\u9ad8\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u7a00\u758f\u5bf9\u9f50\u3002", "method": "\u5229\u7528\u7a00\u758f\u5730\u6807\u5b66\u4e60\u7ec6\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u591a\u4e2a\u64cd\u4f5c\u7b26\u5b9e\u73b0\u5bc6\u96c6\u5730\u6807\u751f\u6210\u3002", "result": "\u5728\u5bc6\u96c6300W\u6d4b\u8bd5\u96c6\u53ca\u539f\u59cb\u7a00\u758f\u6d4b\u8bd5\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u6210\u672c\u5373\u53ef\u63d0\u5347\u5730\u6807\u5bc6\u5ea6\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2412.15194", "pdf": "https://arxiv.org/pdf/2412.15194", "abs": "https://arxiv.org/abs/2412.15194", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MMLU-CF\uff0c\u4e00\u4e2a\u65e0\u6c61\u67d3\u4e14\u66f4\u5177\u6311\u6218\u6027\u7684\u591a\u9009\u9898\u57fa\u51c6\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u9009\u9898\u6570\u636e\u96c6\uff08\u5982MMLU\uff09\u56e0\u5f00\u6e90\u6027\u548c\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u5e7f\u6cdb\uff0c\u5bfc\u81f4\u57fa\u51c6\u6c61\u67d3\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u4ece\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\u83b7\u53d6\u6570\u636e\u5e76\u8bbe\u8ba1\u4e09\u6761\u53bb\u6c61\u67d3\u89c4\u5219\uff0c\u907f\u514d\u65e0\u610f\u6570\u636e\u6cc4\u6f0f\uff1b\u5c06\u57fa\u51c6\u5206\u4e3a\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6076\u610f\u6570\u636e\u6cc4\u6f0f\u3002", "result": "\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08\u5982GPT-4o\u76845-shot\u5f97\u5206\u4e3a73.4%\uff09\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MMLU-CF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e25\u683c\u4e14\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.21545", "pdf": "https://arxiv.org/pdf/2506.21545", "abs": "https://arxiv.org/abs/2506.21545", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "title": "Data Efficacy for Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6570\u636e\u6548\u80fd\uff08Data Efficacy\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u7684\u7ec4\u7ec7\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86DELT\u8303\u5f0f\uff0c\u5305\u542b\u6570\u636e\u8bc4\u5206\u3001\u9009\u62e9\u548c\u6392\u5e8f\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u6548\u7387\uff08\u5982\u6570\u636e\u7b5b\u9009\u548c\u91c7\u6837\uff09\uff0c\u800c\u6570\u636e\u7ec4\u7ec7\u4f18\u5316\uff08\u6570\u636e\u6548\u80fd\uff09\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faDELT\u8303\u5f0f\uff0c\u5305\u542b\u6570\u636e\u8bc4\u5206\uff08\u5982LQS\uff09\u3001\u6570\u636e\u9009\u62e9\u548c\u6570\u636e\u6392\u5e8f\uff08\u5982FO\uff09\u3002LQS\u4ece\u68af\u5ea6\u4e00\u81f4\u6027\u89d2\u5ea6\u8bc4\u4f30\u6570\u636e\u6837\u672c\u7684\u5b66\u4e60\u6027\u548c\u8d28\u91cf\uff0cFO\u89e3\u51b3\u6a21\u578b\u9057\u5fd8\u548c\u6570\u636e\u5206\u5e03\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDELT\u80fd\u4e0d\u540c\u7a0b\u5ea6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u6570\u636e\u89c4\u6a21\u6216\u6a21\u578b\u5927\u5c0f\u3002LQS\u4e0eFO\u7ec4\u5408\u6548\u679c\u6700\u4f73\uff0c\u6570\u636e\u6548\u80fd\u4e0e\u6570\u636e\u6548\u7387\u53ef\u540c\u65f6\u5b9e\u73b0\u3002", "conclusion": "\u6570\u636e\u6548\u80fd\u662f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u4e00\u4e2a\u6f5c\u529b\u5de8\u5927\u7684\u57fa\u7840\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.21558", "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter M\u00fchlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBTF\u7684\u201c\u8fc7\u53bb\u9884\u6d4b\u201d\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u5df2\u77e5\u7ed3\u679c\u7684\u95ee\u9898\u548c\u5927\u91cf\u76f8\u5173\u7f51\u9875\u6570\u636e\u6a21\u62df\u771f\u5b9e\u9884\u6d4b\u73af\u5883\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u73b0\u5b9e\u3001\u5c01\u95ed\u4e14\u53ef\u91cd\u590d\u7684LLM\u9884\u6d4b\u8bc4\u4f30\u73af\u5883\uff0cBTF\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "BTF\u4f7f\u7528\u5df2\u77e5\u7ed3\u679c\u7684\u6570\u767e\u4e2a\u9ad8\u8d28\u91cf\u95ee\u9898\u53ca\u5927\u91cf\u76f8\u5173\u7f51\u9875\u6570\u636e\uff0c\u6a21\u62df\u771f\u5b9e\u9884\u6d4b\u573a\u666f\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540cLLM\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBTF\u80fd\u4ea7\u751f\u4e0e\u5b9e\u65f6\u4e92\u8054\u7f51\u9884\u6d4b\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u5e76\u80fd\u8ffd\u8e2aLLM\u9884\u6d4b\u80fd\u529b\u7684\u6301\u7eed\u8fdb\u6b65\u3002", "conclusion": "BTF\u662f\u4e00\u4e2a\u52a8\u6001\u57fa\u51c6\uff0c\u5c06\u6301\u7eed\u66f4\u65b0\u95ee\u9898\u4ee5\u9002\u914d\u8bad\u7ec3\u6570\u636e\u7684\u53d8\u5316\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.21565", "pdf": "https://arxiv.org/pdf/2506.21565", "abs": "https://arxiv.org/abs/2506.21565", "authors": ["Takato Ueno", "Keito Inoshita"], "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e5\u672c\u4f20\u7edf\u6c9f\u901a\u65b9\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff08KCS+IBC\uff09\uff0c\u7528\u4e8e\u60c5\u611f\u5206\u6790\u4e2d\u7684\u504f\u89c1\u7f13\u89e3\u3001\u53ef\u89e3\u91ca\u6027\u63d0\u5347\u548c\u6982\u7387\u9884\u6d4b\u3002", "motivation": "\u53d7\u65e5\u672c\u4f20\u7edf\u6c9f\u901a\u65b9\u5f0f\uff08\u5982\u56de\u89a7\u677f\u548c\u4e95\u7aef\u4f1a\u8bdd\uff09\u542f\u53d1\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u66f4\u5e73\u8861\u548c\u591a\u6837\u5316\u7684\u60c5\u611f\u5206\u6790\u9884\u6d4b\u3002", "method": "\u6574\u5408\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a0\u5165\u975e\u6b63\u5f0f\u5bf9\u8bdd\u73af\u8282\uff0c\u5e76\u5f15\u5165\u6982\u7387\u60c5\u611f\u9884\u6d4b\u3002", "result": "KCS\u4e0e\u5355\u4e00LLM\u7684\u51c6\u786e\u7387\u76f8\u5f53\uff0c\u800cKCS+IBC\u5728\u63a8\u7406\u540e\u671f\u8868\u73b0\u51fa\u71b5\u7684\u6301\u7eed\u964d\u4f4e\u548c\u65b9\u5dee\u7684\u9010\u6e10\u589e\u52a0\uff0c\u8868\u660e\u5176\u80fd\u5e73\u8861\u9884\u6d4b\u7684\u805a\u5408\u4e0e\u591a\u6837\u6027\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u91cf\u5316\u8fd9\u4e9b\u7279\u6027\u5bf9\u504f\u89c1\u4fee\u6b63\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u3002"}}
{"id": "2506.21566", "pdf": "https://arxiv.org/pdf/2506.21566", "abs": "https://arxiv.org/abs/2506.21566", "authors": ["Arwa Arif"], "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, 8 Pages", "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u53cd\u5411\u7ffb\u8bd1\uff08BT\uff09\u5728\u82f1\u8bed-\u53e4\u5409\u62c9\u7279\u8bed\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u5e76\u672a\u63d0\u5347\u6027\u80fd\uff0c\u751a\u81f3\u7565\u6709\u4e0b\u964d\u3002", "motivation": "\u53cd\u5411\u7ffb\u8bd1\u5728\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u9ad8\u8d28\u91cf\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528MBART50\u6a21\u578b\uff0c\u57fa\u4e8e5\u4e07\u53e5\u5bf9\u7684\u9ad8\u8d28\u91cf\u5e73\u884c\u8bed\u6599\u5e93\u8bad\u7ec3\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u52a0\u5165\u53cd\u5411\u7ffb\u8bd1\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u3002", "result": "\u5408\u6210\u6570\u636e\u672a\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u7565\u6709\u4e0b\u964d\uff0c\u901a\u8fc7\u591a\u79cd\u6307\u6807\u9a8c\u8bc1\u3002", "conclusion": "\u53cd\u5411\u7ffb\u8bd1\u5728\u67d0\u4e9b\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u53ef\u80fd\u6536\u76ca\u9012\u51cf\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.21567", "pdf": "https://arxiv.org/pdf/2506.21567", "abs": "https://arxiv.org/abs/2506.21567", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86BioPars\uff0c\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u80fd\u529b\u7684\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6ce2\u65af\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u9996\u6b21\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLMs\u5728\u751f\u7269\u4fe1\u606f\u5b66\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6ce2\u65af\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165BIOPARS-BENCH\u6570\u636e\u96c6\u548cBioParsQA\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u6bd4\u8f83ChatGPT\u3001Llama\u548cGalactica\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793aBioPars\u5728ROUGE-L\u3001BERTScore\u3001MoverScore\u548cBLEURT\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5982GPT-4 1.0\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLMs\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0cBioPars\u4e3a\u6ce2\u65af\u533b\u5b66\u95ee\u7b54\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21570", "pdf": "https://arxiv.org/pdf/2506.21570", "abs": "https://arxiv.org/abs/2506.21570", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "AI": {"tldr": "\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u6570\u636e\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u8bbe\u8ba1\u9009\u62e9\u5bf9\u9a8c\u8bc1\u635f\u5931\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u5730\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0a\u6e38\u540e\u8bad\u7ec3\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u8bcd\u5668\u548c\u8bed\u8a00\u6a21\u578b\u4e3b\u5e72\u5927\u5c0f\u7b49\u8bbe\u8ba1\u9009\u62e9\uff0c\u8bc4\u4f30\u5176\u5bf9\u9a8c\u8bc1\u635f\u5931\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u9a8c\u8bc1\u635f\u5931\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u6301\u7eed\u4e0b\u964d\uff0c\u800c\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u5df2\u6536\u655b\uff0c\u5f62\u6210\u975e\u96f6\u8fc1\u79fb\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u7406\u89e3\u9ad8\u6548\u8ba1\u7b97\u8bad\u7ec3\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e3a\u7814\u7a76\u6a21\u6001\u65e0\u5173\u7684\u6570\u636e\u5206\u5e03\u7279\u6027\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2506.21573", "pdf": "https://arxiv.org/pdf/2506.21573", "abs": "https://arxiv.org/abs/2506.21573", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ed1\u76d2\u548c\u767d\u76d2\u6a21\u578b\u4f18\u52bf\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u7ea6\u675f\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u9ed1\u76d2\u6a21\u578b\u6210\u672c\u9ad8\uff0c\u767d\u76d2\u6a21\u578b\u8d44\u6e90\u9700\u6c42\u5927\u4e14\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7ed3\u5408\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "\u878d\u5408\u9ed1\u76d2\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u521d\u59cb\u5316\u6307\u4ee4\u548c\u767d\u76d2\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u7ea6\u675f\u5b9e\u73b0\u4f18\u5316\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21581", "pdf": "https://arxiv.org/pdf/2506.21581", "abs": "https://arxiv.org/abs/2506.21581", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8bc4\u4f30\u57fa\u51c6\u7684\u7279\u6027\u53ef\u80fd\u626d\u66f2\u68c0\u7d22\u6a21\u578b\u4e2d\u9886\u57df\u9002\u5e94\u7684\u771f\u5b9e\u6548\u76ca\uff0c\u5bfc\u81f4\u8bef\u5bfc\u6027\u8bc4\u4f30\u3002\u901a\u8fc7\u73af\u5883\u76d1\u7ba1\u6587\u4ef6\u68c0\u7d22\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u4e0d\u540c\u8bed\u4e49\u7ed3\u6784\u7684\u57fa\u51c6\u5bf9\u9886\u57df\u9002\u5e94\u6548\u679c\u7684\u8bc4\u4ef7\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u63ed\u793a\u8bc4\u4f30\u57fa\u51c6\u7684\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u9886\u57df\u9002\u5e94\u6548\u679c\u7684\u611f\u77e5\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\uff0c\u907f\u514d\u56e0\u57fa\u51c6\u7279\u6027\u5bfc\u81f4\u7684\u8bef\u5bfc\u6027\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u4f7f\u7528ColBERTv2\u6a21\u578b\u5728\u73af\u5883\u76d1\u7ba1\u6587\u4ef6\uff08EIS\uff09\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u4e24\u4e2a\u8bed\u4e49\u7ed3\u6784\u4e0d\u540c\u7684\u57fa\u51c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002\u901a\u8fc7\u4e3b\u9898\u591a\u6837\u6027\u6307\u6807\u6bd4\u8f83\u57fa\u51c6\u5dee\u5f02\u3002", "result": "\u5728\u4e3b\u9898\u8fb9\u754c\u6e05\u6670\u7684\u57fa\u51c6\u4e0a\uff0c\u9886\u57df\u9002\u5e94\u4ec5\u5e26\u67650.61%\u7684NDCG\u589e\u76ca\uff1b\u800c\u5728\u8bed\u4e49\u91cd\u53e0\u7684\u57fa\u51c6\u4e0a\uff0c\u589e\u76ca\u9ad8\u8fbe2.22%\uff0c\u5dee\u5f02\u8fbe3.6\u500d\u3002\u9ad8\u589e\u76ca\u57fa\u51c6\u7684\u4e3b\u9898\u591a\u6837\u6027\u66f4\u9ad8\u3002", "conclusion": "\u8bc4\u4f30\u57fa\u51c6\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u6548\u679c\u8bc4\u4f30\u3002\u8bed\u4e49\u91cd\u53e0\u7684\u57fa\u51c6\u66f4\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5bf9\u9886\u57df\u9002\u5e94\u7684\u8bc4\u4f30\u66f4\u51c6\u786e\u3002"}}
{"id": "2506.21585", "pdf": "https://arxiv.org/pdf/2506.21585", "abs": "https://arxiv.org/abs/2506.21585", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff08\u76f4\u63a5\u63d0\u53d6\u548c\u95f4\u63a5\u63d0\u53d6\uff09\u4ece\u98df\u54c1\u4ea7\u54c1\u9875\u9762\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u6027\u80fd\uff0c\u95f4\u63a5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u7565\u4f4e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u751f\u6210\u5f0fAI\u548cLLM\u81ea\u52a8\u5316\u4ece\u7f51\u9875\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u98df\u54c1\u4ea7\u54c1\u9875\u9762\u4e0a\u3002", "method": "\u5bf9\u6bd4\u4e86\u76f4\u63a5\u63d0\u53d6\u548c\u901a\u8fc7\u751f\u6210\u51fd\u6570\u95f4\u63a5\u63d0\u53d6\u4e24\u79cdLLM\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u3002", "result": "\u95f4\u63a5\u63d0\u53d6\u65b9\u6cd5\u51c6\u786e\u7387\u7565\u4f4e\uff0896.48%\uff09\uff0c\u4f46\u51cf\u5c11\u4e8695.82%\u7684LLM\u8c03\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u95f4\u63a5\u63d0\u53d6\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u6a21\u677f\u7f51\u9875\u7684\u5927\u89c4\u6a21\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21590", "pdf": "https://arxiv.org/pdf/2506.21590", "abs": "https://arxiv.org/abs/2506.21590", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u793a\u4e00\u81f4\u6027\uff08RC\uff09\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408LLM\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff0c\u7ed3\u5408\u5185\u90e8\u6fc0\u6d3b\u4e00\u81f4\u6027\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u63d0\u793a\u548c\u91c7\u6837\u7b56\u7565\u4fee\u6539\uff0cRC\u65e8\u5728\u7b80\u5316\u5e76\u63d0\u5347\u7b54\u6848\u805a\u5408\u6548\u679c\u3002", "method": "\u5229\u7528\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\uff08\u5bc6\u96c6\u6216\u7a00\u758f\uff09\u7684\u4e00\u81f4\u6027\uff0c\u5bf9\u5019\u9009\u7b54\u6848\u8fdb\u884c\u52a0\u6743\u805a\u5408\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u6e90LLM\u548c\u56db\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u63d0\u5347\u8fbe4%\u3002", "conclusion": "RC\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u67e5\u8be2\uff0c\u4ec5\u9700\u8f7b\u91cf\u8ba1\u7b97\u5373\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2506.21599", "pdf": "https://arxiv.org/pdf/2506.21599", "abs": "https://arxiv.org/abs/2506.21599", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barth\u00e9lemy", "Tomasz Bednarz", "Flora D. Salim"], "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRefine-POI\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u89e3\u51b3LLM\u5728POI\u63a8\u8350\u4e2dSFT\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684POI\u63a8\u8350\u65b9\u6cd5\u4e2d\uff0cSFT\u65b9\u6cd5\u56e0\u6570\u636e\u4e0e\u4efb\u52a1\u4e0d\u5339\u914d\u800c\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faRefine-POI\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u8350\u9a71\u52a8\u5956\u52b1\uff0c\u5229\u7528\u5355\u4e00\u771f\u5b9ePOI\u751f\u6210top-k\u63a8\u8350\u5217\u8868\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRefine-POI\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u7684top-k\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "Refine-POI\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u6709\u6548\u89e3\u51b3\u4e86SFT\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2506.21603", "pdf": "https://arxiv.org/pdf/2506.21603", "abs": "https://arxiv.org/abs/2506.21603", "authors": ["Yenisel Plasencia-Cala\u00f1a"], "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u7cfb\u7edf\u7684\u4eba\u672c\u64cd\u4f5c\u5316\uff0c\u6bd4\u8f83\u4e86\u673a\u5668\u5b66\u4e60\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4f18\u52a3\uff0c\u91cd\u70b9\u5173\u6ce8\u504f\u5dee\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u51c6\u786e\u6027\uff0c\u63a2\u7d22AES\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4eba\u672c\u9700\u6c42\uff0c\u5982\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u3002", "method": "\u6bd4\u8f83\u4e86\u673a\u5668\u5b66\u4e60\u4e0eLLMs\u5728AES\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u7ef4\u5ea6\uff08\u5982\u504f\u5dee\u3001\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff09\u7684\u5dee\u5f02\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eLLMs\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u8f83\u5dee\uff1bLLMs\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\uff0c\u4f46\u4e24\u8005\u5728\u504f\u5dee\u548c\u8fb9\u7f18\u5206\u6570\u9c81\u68d2\u6027\u4e0a\u5747\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540cAES\u65b9\u6cd5\u7684\u6311\u6218\u4e0e\u6743\u8861\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u4fe1\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.21604", "pdf": "https://arxiv.org/pdf/2506.21604", "abs": "https://arxiv.org/abs/2506.21604", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u91cf\u5316\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u751f\u6210AI\u7684\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f01\u4e1a\u6587\u6863\u667a\u80fd\u4e2d\u7684VisualRAG\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u751f\u6210AI\u8bc4\u4f30\u6846\u67b6\u96be\u4ee5\u5efa\u7acb\u53ef\u4fe1\u5ea6\uff0c\u963b\u788d\u4e86\u4f01\u4e1a\u91c7\u7528\uff0c\u800c\u53ef\u9760\u6027\u5bf9\u4f01\u4e1a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u6280\u672f\u6307\u6807\u4e0e\u7528\u6237\u4fe1\u4efb\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u6807\u9898\u3001OCR\uff09\u7684\u52a0\u6743\u65b9\u6cd5\u3002", "result": "\u6700\u4f18\u6a21\u6001\u6743\u91cd\uff0830%\u6587\u672c\u300115%\u56fe\u50cf\u300125%\u6807\u9898\u300130%OCR\uff09\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u6027\u80fd\u63d0\u534757.3%\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f01\u4e1aAI\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u91cf\u5316\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8d1f\u8d23\u4efbAI\u7684\u90e8\u7f72\u3002"}}
{"id": "2506.21607", "pdf": "https://arxiv.org/pdf/2506.21607", "abs": "https://arxiv.org/abs/2506.21607", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "AI": {"tldr": "CORE-KG\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6cd5\u5f8b\u6587\u672c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u7c7b\u578b\u611f\u77e5\u7684\u5171\u6307\u6d88\u89e3\u548c\u9886\u57df\u5f15\u5bfc\u7684\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\uff0c\u663e\u8457\u51cf\u5c11\u8282\u70b9\u91cd\u590d\u548c\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u5728\u5904\u7406\u6cd5\u5f8b\u6587\u672c\u65f6\u5b58\u5728\u5171\u6307\u6d88\u89e3\u4e0d\u8db3\u548c\u566a\u58f0\u95ee\u9898\uff0cCORE-KG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6d41\u7a0b\uff1a\u7c7b\u578b\u611f\u77e5\u7684\u5171\u6307\u6d88\u89e3\u548c\u9886\u57df\u5f15\u5bfc\u7684\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684GraphRAG\u6846\u67b6\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\uff0cCORE-KG\u51cf\u5c11\u8282\u70b9\u91cd\u590d33.28%\uff0c\u566a\u58f038.37%\uff0c\u751f\u6210\u66f4\u6e05\u6670\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "CORE-KG\u4e3a\u5206\u6790\u590d\u6742\u72af\u7f6a\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.21611", "pdf": "https://arxiv.org/pdf/2506.21611", "abs": "https://arxiv.org/abs/2506.21611", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "title": "Does Multimodality Lead to Better Time Series Forecasting?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u6587\u672c\u4fe1\u606f\u7684\u4f5c\u7528\u53ca\u5176\u9002\u7528\u6761\u4ef6\uff0c\u53d1\u73b0\u5176\u6548\u679c\u56e0\u6a21\u578b\u548c\u6570\u636e\u7279\u6027\u800c\u5f02\u3002", "motivation": "\u63a2\u7d22\u5728\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u6587\u672c\u4fe1\u606f\u662f\u5426\u53ca\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e8614\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4e24\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5bf9\u9f50\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6548\u679c\u5e76\u975e\u666e\u904d\u9002\u7528\uff0c\u5176\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8e\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u7279\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u9884\u6d4b\u4e2d\u6587\u672c\u4fe1\u606f\u9002\u7528\u7684\u5177\u4f53\u6761\u4ef6\uff0c\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.21623", "pdf": "https://arxiv.org/pdf/2506.21623", "abs": "https://arxiv.org/abs/2506.21623", "authors": ["Peiheng Gao", "Chen Yang", "Ning Sun", "Ri\u010dardas Zitikis"], "title": "Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine learning (ML) has significantly advanced text classification by\nenabling automated understanding and categorization of complex, unstructured\ntextual data. However, accurately capturing nuanced linguistic patterns and\ncontextual variations inherent in natural language, particularly within\nconsumer complaints, remains a challenge. This study addresses these issues by\nincorporating human-experience-trained algorithms that effectively recognize\nsubtle semantic differences crucial for assessing consumer relief eligibility.\nFurthermore, we propose integrating synthetic data generation methods that\nutilize expert evaluations of generative adversarial networks and are refined\nthrough expert annotations. By combining expert-trained classifiers with\nhigh-quality synthetic data, our research seeks to significantly enhance\nmachine learning classifier performance, reduce dataset acquisition costs, and\nimprove overall evaluation metrics and robustness in text classification tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u4eba\u7c7b\u7ecf\u9a8c\u8bad\u7ec3\u7684\u7b97\u6cd5\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u6d88\u8d39\u8005\u6295\u8bc9\u4e2d\u7684\u8bed\u4e49\u5dee\u5f02\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u4e2d\u7ec6\u5fae\u8bed\u4e49\u5dee\u5f02\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u6355\u6349\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6d88\u8d39\u8005\u6295\u8bc9\u9886\u57df\u3002", "method": "\u7ed3\u5408\u4eba\u7c7b\u7ecf\u9a8c\u8bad\u7ec3\u7684\u7b97\u6cd5\u548c\u4e13\u5bb6\u8bc4\u4f30\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u4f18\u5316\u3002", "result": "\u9884\u671f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u964d\u4f4e\u6570\u636e\u96c6\u83b7\u53d6\u6210\u672c\uff0c\u5e76\u6539\u5584\u8bc4\u4f30\u6307\u6807\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u548c\u7ecf\u6d4e\u6027\u3002"}}
{"id": "2506.21624", "pdf": "https://arxiv.org/pdf/2506.21624", "abs": "https://arxiv.org/abs/2506.21624", "authors": ["Bla\u017e \u0160krlj", "Yonatan Karni", "Grega Ga\u0161per\u0161i\u010d", "Bla\u017e Mramor", "Yulia Stolin", "Martin Jakomin", "Jasna Urban\u010di\u010d", "Yuval Dishi", "Natalia Silberstein", "Ophir Friedler", "Assaf Klein"], "title": "DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": "AdKDD 25", "summary": "The Deep and Cross architecture (DCNv2) is a robust production baseline and\nis integral to numerous real-life recommender systems. Its inherent efficiency\nand ability to model interactions often result in models that are both simpler\nand highly competitive compared to more computationally demanding alternatives,\nsuch as Deep FFMs. In this work, we introduce three significant algorithmic\nimprovements to the DCNv2 architecture, detailing their formulation and\nbehavior at scale. The enhanced architecture we refer to as DCN^2 is actively\nused in a live recommender system, processing over 0.5 billion predictions per\nsecond across diverse use cases where it out-performed DCNv2, both offline and\nonline (ab tests). These improvements effectively address key limitations\nobserved in the DCNv2, including information loss in Cross layers, implicit\nmanagement of collisions through learnable lookup-level weights, and explicit\nmodeling of pairwise similarities with a custom layer that emulates FFMs'\nbehavior. The superior performance of DCN^2 is also demonstrated on four\npublicly available benchmark data sets.", "AI": {"tldr": "DCN^2\u662f\u5bf9DCNv2\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u4e09\u79cd\u7b97\u6cd5\u4f18\u5316\u89e3\u51b3\u4e86DCNv2\u7684\u5173\u952e\u9650\u5236\uff0c\u5e76\u5728\u5b9e\u9645\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "DCNv2\u867d\u7136\u9ad8\u6548\u4e14\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4ecd\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u78b0\u649e\u7ba1\u7406\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u7b97\u6cd5\u6539\u8fdb\uff1a\u89e3\u51b3Cross\u5c42\u4fe1\u606f\u4e22\u5931\u3001\u53ef\u5b66\u4e60\u67e5\u627e\u7ea7\u6743\u91cd\u7ba1\u7406\u78b0\u649e\u3001\u81ea\u5b9a\u4e49\u5c42\u6a21\u62dfFFM\u884c\u4e3a\u3002", "result": "DCN^2\u5728\u79bb\u7ebf\u3001\u5728\u7ebf\u6d4b\u8bd5\u53ca\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8eDCNv2\uff0c\u5904\u7406\u80fd\u529b\u8fbe\u6bcf\u79d25\u4ebf\u9884\u6d4b\u3002", "conclusion": "DCN^2\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u6210\u4e3a\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21628", "pdf": "https://arxiv.org/pdf/2506.21628", "abs": "https://arxiv.org/abs/2506.21628", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "title": "Ark: An Open-source Python-based Framework for Robot Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "AI": {"tldr": "ARK\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u4ee5Python\u4e3a\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u673a\u5668\u4eba\u8f6f\u4ef6\u5f00\u53d1\uff0c\u63d0\u4f9b\u7c7b\u4f3cGym\u7684\u63a5\u53e3\uff0c\u652f\u6301\u4eff\u771f\u4e0e\u7269\u7406\u673a\u5668\u4eba\u5207\u6362\uff0c\u5e76\u96c6\u6210ROS\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8f6f\u4ef6\u6808\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3001\u5de5\u5177\u5206\u6563\u4e14\u786c\u4ef6\u96c6\u6210\u590d\u6742\uff0c\u4e0eAI\u9886\u57df\u7684Python\u751f\u6001\u7cfb\u7edf\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0cARK\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "ARK\u63d0\u4f9bGym\u98ce\u683c\u7684\u73af\u5883\u63a5\u53e3\uff0c\u652f\u6301\u6570\u636e\u6536\u96c6\u3001\u9884\u5904\u7406\u548c\u7b56\u7565\u8bad\u7ec3\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5e76\u9644\u5e26\u53ef\u91cd\u7528\u6a21\u5757\u548cROS\u4e92\u64cd\u4f5c\u6027\u3002", "result": "ARK\u5b9e\u73b0\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u786c\u4ef6\u65e0\u7f1d\u5207\u6362\u548c\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5f00\u53d1\u7684\u5165\u95e8\u95e8\u69db\u3002", "conclusion": "ARK\u901a\u8fc7\u7edf\u4e00Python\u751f\u6001\u7cfb\u7edf\u4e0b\u7684\u673a\u5668\u4eba\u548cAI\u5b9e\u8df5\uff0c\u52a0\u901f\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u7814\u7a76\u548c\u5546\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2506.21630", "pdf": "https://arxiv.org/pdf/2506.21630", "abs": "https://arxiv.org/abs/2506.21630", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "8 pages, 9 figures, 2025 IJCNN", "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u53ef\u901a\u884c\u8def\u5f84\u68c0\u6d4b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u65b0\u6570\u636e\u96c6TOMD\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u57ce\u5e02\u6216\u5bbd\u9614\u7684\u8d8a\u91ce\u573a\u666f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u72ed\u7a84\u5c0f\u5f84\u7c7b\u8d8a\u91ce\u73af\u5883\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165TOMD\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u591a\u5c3a\u5ea6\u6570\u636e\u878d\u5408\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u878d\u5408\u7b56\u7565\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u4e14\u5149\u7167\u5bf9\u5206\u5272\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "TOMD\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.21638", "pdf": "https://arxiv.org/pdf/2506.21638", "abs": "https://arxiv.org/abs/2506.21638", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "title": "IRanker: Towards Ranking Foundation Model", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6392\u5e8f\u57fa\u7840\u6a21\u578bIRanker\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8fed\u4ee3\u89e3\u7801\u89e3\u51b3\u6392\u5e8f\u4efb\u52a1\u7f3a\u4e4f\u660e\u786e\u76d1\u7763\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u6392\u5e8f\u4efb\u52a1\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u4e14\u76d1\u7763\u4fe1\u53f7\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u6392\u5e8f\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faIRanker\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u8fed\u4ee3\u89e3\u7801\u9010\u6b65\u5254\u9664\u5019\u9009\u6c60\u4e2d\u6700\u5dee\u9009\u9879\uff0c\u51cf\u5c11\u8f93\u51fa\u7ec4\u5408\u7a7a\u95f4\u3002", "result": "IRanker-3B\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "IRanker\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8fed\u4ee3\u89e3\u7801\u6709\u6548\u89e3\u51b3\u4e86\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.21681", "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan \u00c7apuk", "Andrew Bond", "Muhammed Burak K\u0131z\u0131l", "Emir G\u00f6\u00e7en", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360\u00b0 Panorama Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "TanDiT\u662f\u4e00\u79cd\u751f\u6210\u5168\u666f\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u6269\u6563\u6a21\u578b\u751f\u6210\u5207\u7ebf\u5e73\u9762\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5168\u666f\u751f\u6210\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u548c\u4e00\u81f4\u6027\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u51e0\u4f55\u5931\u771f\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTanDiT\u65b9\u6cd5\uff0c\u5229\u7528\u7edf\u4e00\u6269\u6563\u6a21\u578b\u751f\u6210\u5207\u7ebf\u5e73\u9762\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u540e\u5904\u7406\u6b65\u9aa4\u589e\u5f3a\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTanDiT\u80fd\u6709\u6548\u6cdb\u5316\uff0c\u5904\u7406\u590d\u6742\u6587\u672c\u63d0\u793a\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\u50cf\u3002", "conclusion": "TanDiT\u4e3a\u5168\u666f\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21686", "pdf": "https://arxiv.org/pdf/2506.21686", "abs": "https://arxiv.org/abs/2506.21686", "authors": ["Swastika Kundu", "Autoshi Ibrahim", "Mithila Rahman", "Tanvir Ahmed"], "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing.", "AI": {"tldr": "ANUBHUTI\u662f\u4e00\u4e2a\u5305\u542b2000\u53e5\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u60c5\u611f\u5206\u6790\uff0c\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u65b9\u8a00\u7684\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u8bed\u8a00\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\uff0c\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u56db\u79cd\u65b9\u8a00\u7684\u53e5\u5b50\uff0c\u91c7\u7528\u53cc\u91cd\u6807\u6ce8\u65b9\u6848\uff1a\u4e3b\u9898\u5206\u7c7b\u548c\u60c5\u611f\u6807\u6ce8\uff0c\u5e76\u7531\u4e13\u5bb6\u7ffb\u8bd1\u548c\u6807\u6ce8\u3002", "result": "\u6570\u636e\u96c6\u8d28\u91cf\u9ad8\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u68c0\u9a8c\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u3002", "conclusion": "ANUBHUTI\u4e3a\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.21720", "pdf": "https://arxiv.org/pdf/2506.21720", "abs": "https://arxiv.org/abs/2506.21720", "authors": ["Thorsten Buss", "Frank Gaede", "Gregor Kasieczka", "Anatolii Korol", "Katja Kr\u00fcger", "Peter McKeown", "Martina Mozzanica"], "title": "CaloHadronic: a diffusion model for the generation of hadronic showers", "categories": ["physics.ins-det", "cs.LG", "hep-ex", "hep-ph", "physics.data-an"], "comment": null, "summary": "Simulating showers of particles in highly-granular calorimeters is a key\nfrontier in the application of machine learning to particle physics. Achieving\nhigh accuracy and speed with generative machine learning models can enable them\nto augment traditional simulations and alleviate a major computing constraint.\nRecent developments have shown how diffusion based generative shower simulation\napproaches that do not rely on a fixed structure, but instead generate\ngeometry-independent point clouds, are very efficient. We present a\ntransformer-based extension to previous architectures which were developed for\nsimulating electromagnetic showers in the highly granular electromagnetic\ncalorimeter of the International Large Detector, ILD. The attention mechanism\nnow allows us to generate complex hadronic showers with more pronounced\nsubstructure across both the electromagnetic and hadronic calorimeters. This is\nthe first time that machine learning methods are used to holistically generate\nshowers across the electromagnetic and hadronic calorimeter in highly granular\nimaging calorimeter systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u9ad8\u7c92\u5ea6\u91cf\u80fd\u5668\u4e2d\u7535\u78c1\u548c\u5f3a\u5b50\u7c07\u5c04\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u4e24\u79cd\u91cf\u80fd\u5668\u7684\u6574\u4f53\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u63d0\u5347\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u7f13\u89e3\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408Transformer\u67b6\u6784\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u751f\u6210\u51e0\u4f55\u65e0\u5173\u7684\u70b9\u4e91\u6570\u636e\u3002", "result": "\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5177\u6709\u590d\u6742\u5b50\u7ed3\u6784\u7684\u5f3a\u5b50\u7c07\u5c04\uff0c\u9002\u7528\u4e8e\u7535\u78c1\u548c\u5f3a\u5b50\u91cf\u80fd\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u7535\u78c1\u548c\u5f3a\u5b50\u91cf\u80fd\u5668\u7684\u6574\u4f53\u7c07\u5c04\u751f\u6210\uff0c\u4e3a\u7c92\u5b50\u7269\u7406\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.21732", "pdf": "https://arxiv.org/pdf/2506.21732", "abs": "https://arxiv.org/abs/2506.21732", "authors": ["Ameya Salvi", "Venkat Krovi"], "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u89c9\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6ed1\u79fb\u8f6c\u5411\u8f66\u8f86\u5728\u52a8\u6001\u64cd\u4f5c\u4e2d\u7f3a\u4e4f\u51c6\u786e\u5206\u6790\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6a21\u4eff\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u8fdb\u884c\u8f6f\u4ef6\u6a21\u62df\u3001\u786c\u4ef6\u8bc4\u4f30\u548c\u6d88\u878d\u7814\u7a76\u3002", "result": "\u5728\u5f53\u4ee3\u6587\u732e\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21734", "pdf": "https://arxiv.org/pdf/2506.21734", "abs": "https://arxiv.org/abs/2506.21734", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "title": "Hierarchical Reasoning Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "AI": {"tldr": "HRM\u662f\u4e00\u79cd\u65b0\u578b\u9012\u5f52\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u5904\u7406\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u6216\u663e\u5f0f\u76d1\u7763\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\u3001\u9ad8\u6570\u636e\u9700\u6c42\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u53d7\u4eba\u7c7b\u5927\u8111\u5206\u5c42\u5904\u7406\u542f\u53d1\u3002", "method": "\u63d0\u51faHRM\uff0c\u5305\u542b\u9ad8\u4f4e\u7ea7\u9012\u5f52\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u62bd\u8c61\u89c4\u5212\u548c\u8be6\u7ec6\u8ba1\u7b97\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u63a8\u7406\u3002", "result": "\u4ec5\u752827M\u53c2\u6570\u548c1000\u6837\u672c\uff0cHRM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u72ec\u3001\u8ff7\u5bab\u8def\u5f84\uff09\u548cARC\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HRM\u4e3a\u901a\u7528\u8ba1\u7b97\u548c\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u5728\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2506.21739", "pdf": "https://arxiv.org/pdf/2506.21739", "abs": "https://arxiv.org/abs/2506.21739", "authors": ["Felipe Rog\u00e9rio Pimentel", "Rafael Gustavo Alves"], "title": "Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19", "categories": ["stat.ML", "cs.LG", "math.OC", "92B05, 92-10, 65K05, 37M99, 49"], "comment": "14 pages, 3 figures, 3 tables, and 2 algorithms", "summary": "Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use\nthe Finite Impulse Response (FIR) linear system filtering method to track and\npredict the number of people infected and recovered from COVID-19, in a\npandemic context in which there was still no vaccine and the only way to avoid\ncontagion was isolation. To estimate the coefficients of these FIR filters,\nChen et al. used machine learning methods through a classical optimization\nproblem with regularization (ridge regression). These estimated coefficients\nare called ridge coefficients. The epidemic mathematical model adopted by these\nresearchers to formulate the FIR filters is the time-dependent discrete SIR. In\nthis paper, we propose a small modification to the algorithm of Chen et al. to\nobtain the ridge coefficients. We then used this modified algorithm to track\nand predict the number of people infected and recovered from COVID-19 in the\nstate of Minas Gerais/Brazil, within a prediction window, during the initial\nperiod of the pandemic. We also compare the predicted data with the respective\nreal data to check how good the approximation is. In the modified algorithm, we\nset values for the FIR filter orders and for the regularization parameters,\nboth different from the respective values defined by Chen et al. in their\nalgorithm. In this context, the numerical results obtained by the modified\nalgorithm in some simulations present better approximation errors compared to\nthe respective approximation errors presented by the algorithm of Chen et al.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684FIR\u6ee4\u6ce2\u5668\u7b97\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4bCOVID-19\u611f\u67d3\u548c\u5eb7\u590d\u4eba\u6570\uff0c\u5e76\u5728\u5df4\u897f\u7c73\u7eb3\u65af\u5409\u62c9\u65af\u5dde\u7684\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u539f\u7b97\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u75ab\u82d7\u5c1a\u672a\u666e\u53ca\u7684\u75ab\u60c5\u521d\u671f\uff0c\u901a\u8fc7\u6539\u8fdb\u7b97\u6cd5\u63d0\u9ad8\u5bf9\u611f\u67d3\u548c\u5eb7\u590d\u4eba\u6570\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528FIR\u7ebf\u6027\u7cfb\u7edf\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u4f9d\u8d56\u79bb\u6563SIR\u6a21\u578b\uff0c\u901a\u8fc7\u4fee\u6539Chen\u7b49\u4eba\u7684\u7b97\u6cd5\uff08\u8c03\u6574\u6ee4\u6ce2\u5668\u9636\u6570\u548c\u6b63\u5219\u5316\u53c2\u6570\uff09\u6765\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u6539\u8fdb\u7b97\u6cd5\u5728\u67d0\u4e9b\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u6bd4\u539f\u7b97\u6cd5\u66f4\u4f4e\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "conclusion": "\u6539\u8fdb\u7684\u7b97\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u75ab\u60c5\u6570\u636e\uff0c\u4e3a\u75ab\u60c5\u9632\u63a7\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2506.21741", "pdf": "https://arxiv.org/pdf/2506.21741", "abs": "https://arxiv.org/abs/2506.21741", "authors": ["Benjamin Sterling", "Chad Gueli", "M\u00f3nica F. Bugallo"], "title": "Critically-Damped Higher-Order Langevin Dynamics", "categories": ["stat.ML", "cs.LG"], "comment": "12 pages", "summary": "Denoising Diffusion Probabilistic Models represent an entirely new class of\ngenerative AI methods that have yet to be fully explored. Critical damping has\nbeen successfully introduced in Critically-Damped Langevin Dynamics (CLD) and\nCritically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been\napplied to dynamics of arbitrary order. The proposed line of work generalizes\nHigher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion\nmethod, by introducing the concept of critical damping from systems analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e34\u754c\u963b\u5c3c\u5f15\u5165\u9ad8\u9636Langevin\u52a8\u529b\u5b66\uff08HOLD\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u751f\u6210AI\u7684\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u4e34\u754c\u963b\u5c3c\u5728\u4efb\u610f\u9636\u52a8\u529b\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u7684\u6269\u6563\u65b9\u6cd5\u3002", "method": "\u5c06\u4e34\u754c\u963b\u5c3c\u6982\u5ff5\u4ece\u7cfb\u7edf\u5206\u6790\u5f15\u5165\u9ad8\u9636Langevin\u52a8\u529b\u5b66\uff08HOLD\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86HOLD\u7684\u80fd\u529b\u3002", "conclusion": "\u4e34\u754c\u963b\u5c3c\u7684\u5e94\u7528\u4e3a\u9ad8\u9636\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.21743", "pdf": "https://arxiv.org/pdf/2506.21743", "abs": "https://arxiv.org/abs/2506.21743", "authors": ["Jinpai Zhao", "Albert Cerrone", "Eirik Valseth", "Leendert Westerink", "Clint Dawson"], "title": "Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "Storm surge forecasting plays a crucial role in coastal disaster\npreparedness, yet existing machine learning approaches often suffer from\nlimited spatial resolution, reliance on coastal station data, and poor\ngeneralization. Moreover, many prior models operate directly on unstructured\nspatial data, making them incompatible with modern deep learning architectures.\nIn this work, we introduce a novel approach that projects unstructured water\nelevation fields onto structured Red Green Blue (RGB)-encoded image\nrepresentations, enabling the application of Convolutional Long Short Term\nMemory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our\nmodel further integrates ground-truth wind fields as dynamic conditioning\nsignals and topo-bathymetry as a static input, capturing physically meaningful\ndrivers of surge evolution. Evaluated on a large-scale dataset of synthetic\nstorms in the Gulf of Mexico, our method demonstrates robust 48-hour\nforecasting performance across multiple regions along the Texas coast and\nexhibits strong spatial extensibility to other coastal areas. By combining\nstructured representation, physically grounded forcings, and scalable deep\nlearning, this study advances the frontier of storm surge forecasting in\nusability, adaptability, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u975e\u7ed3\u6784\u5316\u6c34\u4f4d\u6570\u636e\u8f6c\u6362\u4e3aRGB\u56fe\u50cf\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408ConvLSTM\u7f51\u7edc\u548c\u7269\u7406\u9a71\u52a8\u56e0\u7d20\uff0c\u63d0\u5347\u4e86\u98ce\u66b4\u6f6e\u9884\u6d4b\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u98ce\u66b4\u6f6e\u9884\u6d4b\u4e2d\u5b58\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u4f9d\u8d56\u6d77\u5cb8\u7ad9\u70b9\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u4e14\u4e0e\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e0d\u517c\u5bb9\u3002", "method": "\u5c06\u975e\u7ed3\u6784\u5316\u6c34\u4f4d\u6570\u636e\u6295\u5f71\u4e3aRGB\u56fe\u50cf\u8868\u793a\uff0c\u7ed3\u5408ConvLSTM\u7f51\u7edc\uff0c\u5e76\u6574\u5408\u98ce\u573a\u548c\u5730\u5f62\u6570\u636e\u4f5c\u4e3a\u52a8\u6001\u548c\u9759\u6001\u8f93\u5165\u3002", "result": "\u5728\u58a8\u897f\u54e5\u6e7e\u5408\u6210\u98ce\u66b4\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u572848\u5c0f\u65f6\u9884\u6d4b\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u5177\u6709\u7a7a\u95f4\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u3001\u7269\u7406\u9a71\u52a8\u548c\u53ef\u6269\u5c55\u6df1\u5ea6\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u98ce\u66b4\u6f6e\u9884\u6d4b\u7684\u5b9e\u7528\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.21748", "pdf": "https://arxiv.org/pdf/2506.21748", "abs": "https://arxiv.org/abs/2506.21748", "authors": ["Liav Hen", "Erez Yosef", "Dan Raviv", "Raja Giryes", "Jacob Scheuer"], "title": "Inverse Design of Diffractive Metasurfaces Using Diffusion Models", "categories": ["physics.optics", "cs.CV", "cs.LG"], "comment": null, "summary": "Metasurfaces are ultra-thin optical elements composed of engineered\nsub-wavelength structures that enable precise control of light. Their inverse\ndesign - determining a geometry that yields a desired optical response - is\nchallenging due to the complex, nonlinear relationship between structure and\noptical properties. This often requires expert tuning, is prone to local\nminima, and involves significant computational overhead. In this work, we\naddress these challenges by integrating the generative capabilities of\ndiffusion models into computational design workflows. Using an RCWA simulator,\nwe generate training data consisting of metasurface geometries and their\ncorresponding far-field scattering patterns. We then train a conditional\ndiffusion model to predict meta-atom geometry and height from a target spatial\npower distribution at a specified wavelength, sampled from a continuous\nsupported band. Once trained, the model can generate metasurfaces with low\nerror, either directly using RCWA-guided posterior sampling or by serving as an\ninitializer for traditional optimization methods. We demonstrate our approach\non the design of a spatially uniform intensity splitter and a polarization beam\nsplitter, both produced with low error in under 30 minutes. To support further\nresearch in data-driven metasurface design, we publicly release our code and\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u8868\u9762\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u3001\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u8d85\u8868\u9762\u7684\u9006\u5411\u8bbe\u8ba1\u56e0\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u7ed3\u6784-\u5149\u5b66\u5173\u7cfb\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u5bb6\u8c03\u53c2\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7RCWA\u6a21\u62df\u5668\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4ee5\u6839\u636e\u76ee\u6807\u7a7a\u95f4\u529f\u7387\u5206\u5e03\u9884\u6d4b\u8d85\u8868\u9762\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u80fd\u5feb\u901f\u751f\u6210\u4f4e\u8bef\u5dee\u8d85\u8868\u9762\uff0c\u5982\u5747\u5300\u5f3a\u5ea6\u5206\u675f\u5668\u548c\u504f\u632f\u5206\u675f\u5668\uff0c\u8bbe\u8ba1\u65f6\u95f4\u5c11\u4e8e30\u5206\u949f\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u8d85\u8868\u9762\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.21757", "pdf": "https://arxiv.org/pdf/2506.21757", "abs": "https://arxiv.org/abs/2506.21757", "authors": ["Tianrong Chen", "Huangjie Zheng", "David Berthelot", "Jiatao Gu", "Josh Susskind", "Shuangfei Zhai"], "title": "TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Diffusion models have demonstrated exceptional capabilities in generating\nhigh-fidelity images but typically suffer from inefficient sampling. Many\nsolver designs and noise scheduling strategies have been proposed to\ndramatically improve sampling speeds. In this paper, we introduce a new\nsampling method that is up to $186\\%$ faster than the current state of the art\nsolver for comparative FID on ImageNet512. This new sampling method is\ntraining-free and uses an ordinary differential equation (ODE) solver. The key\nto our method resides in using higher-dimensional initial noise, allowing to\nproduce more detailed samples with less function evaluations from existing\npretrained diffusion models. In addition, by design our solver allows to\ncontrol the level of detail through a simple hyper-parameter at no extra\ncomputational cost. We present how our approach leverages momentum dynamics by\nestablishing a fundamental equivalence between momentum diffusion models and\nconventional diffusion models with respect to their training paradigms.\nMoreover, we observe the use of higher-dimensional noise naturally exhibits\ncharacteristics similar to stochastic differential equations (SDEs). Finally,\nwe demonstrate strong performances on a set of representative pretrained\ndiffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover\nmodels in both pixel and latent spaces, as well as class and text conditional\nsettings. The code is available at https://github.com/apple/ml-tada.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u6bd4\u73b0\u6709\u6700\u5feb\u91c7\u6837\u65b9\u6cd5\u5feb186%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u9ad8\u7ef4\u521d\u59cb\u566a\u58f0\u548cODE\u6c42\u89e3\u5668\u5b9e\u73b0\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u91c7\u6837\u901f\u5ea6\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u9ad8\u7ef4\u521d\u59cb\u566a\u58f0\u548cODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u52a8\u91cf\u52a8\u529b\u5b66\u63a7\u5236\u7ec6\u8282\u6c34\u5e73\u3002", "result": "\u5728ImageNet512\u4e0a\uff0c\u91c7\u6837\u901f\u5ea6\u63d0\u5347186%\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u91c7\u6837\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\uff0c\u4e14\u80fd\u7075\u6d3b\u63a7\u5236\u7ec6\u8282\u3002"}}
{"id": "2506.21770", "pdf": "https://arxiv.org/pdf/2506.21770", "abs": "https://arxiv.org/abs/2506.21770", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEfficientNet-B0\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u89c6\u7f51\u819c\u773c\u5e95\u56fe\u50cf\u4e2d\u68c0\u6d4b\u9752\u5149\u773c\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u7b80\u5355\u9884\u5904\u7406\u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u9752\u5149\u773c\u662f\u5bfc\u81f4\u4e0d\u53ef\u9006\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u6cbb\u7597\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5177\u6709\u4fb5\u5165\u6027\u4e14\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u3002", "method": "\u4f7f\u7528EfficientNet-B0\u67b6\u6784\uff0c\u901a\u8fc7ACRIMA\u3001ORIGA\u548cRIM-ONE\u6570\u636e\u96c6\u8fdb\u884c\u987a\u5e8f\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b80\u5355\u9884\u5904\u7406\u6bd4\u590d\u6742\u589e\u5f3a\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684AUC-ROC\uff0c\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5224\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u4e3a\u9752\u5149\u773c\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u6f5c\u5728\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.21772", "pdf": "https://arxiv.org/pdf/2506.21772", "abs": "https://arxiv.org/abs/2506.21772", "authors": ["No\u00e9 Lallouet", "Tristan Cazenave", "Cyrille Enderli", "St\u00e9phanie Gourdin"], "title": "Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Recent research works establish deep neural networks as high performing tools\nfor radar target detection, especially on challenging environments (presence of\nclutter or interferences, multi-target scenarii...). However, the usually large\ncomputational complexity of these networks is one of the factors preventing\nthem from being widely implemented in embedded radar systems. We propose to\ninvestigate novel neural architecture search (NAS) methods, based on\nMonte-Carlo Tree Search (MCTS), for finding neural networks achieving the\nrequired detection performance and striving towards a lower computational\ncomplexity. We evaluate the searched architectures on endoclutter radar\nsignals, in order to compare their respective performance metrics and\ngeneralization properties. A novel network satisfying the required detection\nprobability while being significantly lighter than the expert-designed baseline\nis proposed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u63d0\u5347\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u5d4c\u5165\u5f0f\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u5bfb\u627e\u6ee1\u8db3\u68c0\u6d4b\u6027\u80fd\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u4f4e\u7684\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u96f7\u8fbe\u4fe1\u53f7\u4e0a\u8bc4\u4f30\u641c\u7d22\u5230\u7684\u67b6\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7f51\u7edc\uff0c\u5176\u68c0\u6d4b\u6027\u80fd\u6ee1\u8db3\u8981\u6c42\u4e14\u663e\u8457\u8f7b\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u8bbe\u8ba1\u51fa\u9ad8\u6027\u80fd\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4e3a\u5d4c\u5165\u5f0f\u96f7\u8fbe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21795", "pdf": "https://arxiv.org/pdf/2506.21795", "abs": "https://arxiv.org/abs/2506.21795", "authors": ["Reem Alothman", "Hafida Benhidour", "Said Kerrache"], "title": "Offensive Language Detection on Social Media Using XLNet", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eXLNet\u7684\u81ea\u52a8\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u5192\u72af\u6027\u8bed\u8a00\u7684\u6a21\u578b\uff0c\u5e76\u4e0eBERT\u8fdb\u884c\u4e86\u6027\u80fd\u6bd4\u8f83\u3002\u5b9e\u9a8c\u8868\u660e\uff0cXLNet\u5728\u68c0\u6d4b\u5192\u72af\u6027\u5185\u5bb9\u548c\u5206\u7c7b\u5192\u72af\u7c7b\u578b\u4e0a\u4f18\u4e8eBERT\uff0c\u800cBERT\u5728\u8bc6\u522b\u5192\u72af\u76ee\u6807\u4e0a\u7a0d\u4f18\u3002\u6b64\u5916\uff0c\u8fc7\u91c7\u6837\u548c\u6b20\u91c7\u6837\u7b56\u7565\u80fd\u6709\u6548\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u6587\u672c\u4ea4\u6d41\u7684\u666e\u53ca\u5bfc\u81f4\u5192\u72af\u6027\u5185\u5bb9\u589e\u52a0\uff0c\u624b\u52a8\u5ba1\u6838\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528XLNet\u548cBERT\u6a21\u578b\uff0c\u57fa\u4e8eOLID\u6570\u636e\u96c6\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u5e76\u91c7\u7528\u8fc7\u91c7\u6837\u548c\u6b20\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "XLNet\u5728\u68c0\u6d4b\u5192\u72af\u6027\u5185\u5bb9\u548c\u5206\u7c7b\u5192\u72af\u7c7b\u578b\u4e0a\u4f18\u4e8eBERT\uff0cBERT\u5728\u8bc6\u522b\u5192\u72af\u76ee\u6807\u4e0a\u8868\u73b0\u7a0d\u597d\u3002\u8fc7\u91c7\u6837\u548c\u6b20\u91c7\u6837\u7b56\u7565\u6709\u6548\u3002", "conclusion": "XLNet\u548c\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\u5728\u6784\u5efa\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.21802", "pdf": "https://arxiv.org/pdf/2506.21802", "abs": "https://arxiv.org/abs/2506.21802", "authors": ["Johan Hallberg Szabadv\u00e1ry", "Tuwe L\u00f6fstr\u00f6m", "Ulf Johansson", "Cecilia S\u00f6nstr\u00f6d", "Ernst Ahlberg", "Lars Carlsson"], "title": "Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": "20 pages, 3 figures", "summary": "Machine learning (ML) models always make a prediction, even when they are\nlikely to be wrong. This causes problems in practical applications, as we do\nnot know if we should trust a prediction. ML with reject option addresses this\nissue by abstaining from making a prediction if it is likely to be incorrect.\nIn this work, we formalise the approach to ML with reject option in binary\nclassification, deriving theoretical guarantees on the resulting error rate.\nThis is achieved through conformal prediction (CP), which produce prediction\nsets with distribution-free validity guarantees. In binary classification, CP\ncan output prediction sets containing exactly one, two or no labels. By\naccepting only the singleton predictions, we turn CP into a binary classifier\nwith reject option.\n  Here, CP is formally put in the framework of predicting with reject option.\nWe state and prove the resulting error rate, and give finite sample estimates.\nNumerical examples provide illustrations of derived error rate through several\ndifferent conformal prediction settings, ranging from full conformal prediction\nto offline batch inductive conformal prediction. The former has a direct link\nto sharp validity guarantees, whereas the latter is more fuzzy in terms of\nvalidity guarantees but can be used in practice. Error-reject curves illustrate\nthe trade-off between error rate and reject rate, and can serve to aid a user\nto set an acceptable error rate or reject rate in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u7684\u4e8c\u5143\u5206\u7c7b\u62d2\u7edd\u9009\u9879\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u9519\u8bef\u7387\u4e0e\u62d2\u7edd\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u4e0d\u53ef\u9760\u65f6\u4ecd\u5f3a\u5236\u8f93\u51fa\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u4fe1\u4efb\u7684\u62d2\u7edd\u9009\u9879\u673a\u5236\u3002", "method": "\u5229\u7528\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u751f\u6210\u9884\u6d4b\u96c6\uff0c\u901a\u8fc7\u4ec5\u63a5\u53d7\u5355\u4f8b\u9884\u6d4b\u5c06\u5176\u8f6c\u5316\u4e3a\u5e26\u62d2\u7edd\u9009\u9879\u7684\u4e8c\u5143\u5206\u7c7b\u5668\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u9519\u8bef\u7387\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u4e0d\u540cCP\u8bbe\u7f6e\u4e0b\u7684\u9519\u8bef\u7387\u4e0e\u62d2\u7edd\u7387\u5173\u7cfb\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u4e3a\u4e8c\u5143\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u62d2\u7edd\u9009\u9879\u65b9\u6cd5\uff0c\u9519\u8bef-\u62d2\u7edd\u66f2\u7ebf\u5e2e\u52a9\u7528\u6237\u5728\u5b9e\u9645\u4e2d\u8bbe\u7f6e\u53ef\u63a5\u53d7\u7684\u53c2\u6570\u3002"}}
{"id": "2506.21803", "pdf": "https://arxiv.org/pdf/2506.21803", "abs": "https://arxiv.org/abs/2506.21803", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP.", "AI": {"tldr": "MELP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5c3a\u5ea6ECG-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u76d1\u7763\u4eceECG-\u6587\u672c\u5bf9\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfECG\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u8017\u65f6\u8017\u529b\uff1b\u81ea\u76d1\u7763\u5b66\u4e60\u867d\u80fd\u63d0\u53d6ECG\u8868\u5f81\uff0c\u4f46\u672a\u80fd\u6355\u6349\u5176\u591a\u5c3a\u5ea6\u7279\u6027\u3002", "method": "MELP\u7ed3\u5408\u5fc3\u810f\u5b66\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u548c\u4e09\u5c42\u6b21\u8de8\u6a21\u6001\u76d1\u7763\uff08token\u3001beat\u3001rhythm\uff09\uff0c\u5bf9\u9f50ECG\u4fe1\u53f7\u4e0e\u6587\u672c\u62a5\u544a\u3002", "result": "\u5728\u591a\u4e2aECG\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\uff0cMELP\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "MELP\u901a\u8fc7\u591a\u5c3a\u5ea6\u76d1\u7763\u663e\u8457\u63d0\u5347ECG\u8868\u5f81\u5b66\u4e60\uff0c\u4e3a\u4e34\u5e8aECG\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21813", "pdf": "https://arxiv.org/pdf/2506.21813", "abs": "https://arxiv.org/abs/2506.21813", "authors": ["Felix Holm", "G\u00f6zde \u00dcnver", "Ghazal Ghazaei", "Nassir Navab"], "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u767d\u5185\u969c\u624b\u672f\u573a\u666f\u56fe\u6570\u636e\u96c6\uff08CAT-SG\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6807\u6ce8\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u548c\u65f6\u5e8f\u4f9d\u8d56\uff0c\u4e3a\u624b\u672f\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5168\u9762\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u65b0\u6a21\u578bCatSGG\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u5173\u6ce8\u624b\u672f\u5206\u6790\u7684\u5b64\u7acb\u65b9\u9762\uff08\u5982\u5de5\u5177\u68c0\u6d4b\u6216\u9636\u6bb5\u5206\u5272\uff09\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u4f53\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u5168\u9762\u6355\u6349\u3002", "method": "\u5f15\u5165CAT-SG\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u548c\u65f6\u5e8f\u4f9d\u8d56\uff0c\u5e76\u63d0\u51fa\u573a\u666f\u56fe\u751f\u6210\u6a21\u578bCatSGG\u3002", "result": "CAT-SG\u63d0\u4f9b\u4e86\u624b\u672f\u5de5\u4f5c\u6d41\u7684\u5168\u9762\u89c6\u56fe\uff0cCatSGG\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u624b\u672f\u8868\u793a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CAT-SG\u6570\u636e\u96c6\u548cCatSGG\u6a21\u578b\u4e3aAI\u9a71\u52a8\u7684\u4e34\u5e8a\u5b9e\u8df5\uff08\u5982\u624b\u672f\u57f9\u8bad\u548c\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\uff09\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21815", "pdf": "https://arxiv.org/pdf/2506.21815", "abs": "https://arxiv.org/abs/2506.21815", "authors": ["Augustine Twumasi", "Prokash Chandra Roy", "Zixun Li", "Soumya Shouvik Bhattacharjee", "Zhengtao Gan"], "title": "Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning", "categories": ["cs.CE", "cs.LG", "math.OC"], "comment": null, "summary": "Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing\ntechnology for producing intricate metal components with exceptional accuracy.\nA key challenge in L-PBF is the formation of complex microstructures affecting\nproduct quality. We propose a physics-guided, machine-learning approach to\noptimize scan paths for desired microstructure outcomes, such as equiaxed\ngrains. We utilized a phase-field method (PFM) to model crystalline grain\nstructure evolution. To reduce computational costs, we trained a surrogate\nmachine learning model, a 3D U-Net convolutional neural network, using\nsingle-track phase-field simulations with various laser powers to predict\ncrystalline grain orientations based on initial microstructure and thermal\nhistory. We investigated three scanning strategies across various hatch\nspacings within a square domain, achieving a two-orders-of-magnitude speedup\nusing the surrogate model. To reduce trial and error in designing laser scan\ntoolpaths, we used deep reinforcement learning (DRL) to generate optimized scan\npaths for target microstructure. Results from three cases demonstrate the DRL\napproach's effectiveness. We integrated the surrogate 3D U-Net model into our\nDRL environment to accelerate the reinforcement learning training process. The\nreward function minimizes both aspect ratio and grain volume of the predicted\nmicrostructure from the agent's scan path. The reinforcement learning algorithm\nwas benchmarked against conventional zigzag approach for smaller and larger\ndomains, showing machine learning methods' potential to enhance microstructure\ncontrol and computational efficiency in L-PBF optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u6fc0\u5149\u7c89\u672b\u5e8a\u7194\u878d\uff08L-PBF\uff09\u7684\u626b\u63cf\u8def\u5f84\uff0c\u4ee5\u5b9e\u73b0\u76ee\u6807\u5fae\u89c2\u7ed3\u6784\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "L-PBF\u6280\u672f\u4e2d\u590d\u6742\u5fae\u89c2\u7ed3\u6784\u7684\u5f62\u6210\u5f71\u54cd\u4ea7\u54c1\u8d28\u91cf\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u8bd5\u9519\u3002", "method": "\u7ed3\u5408\u76f8\u573a\u6cd5\uff08PFM\uff09\u548c3D U-Net\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4f18\u5316\u626b\u63cf\u8def\u5f84\u3002", "result": "\u4ee3\u7406\u6a21\u578b\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0cDRL\u65b9\u6cd5\u5728\u4e09\u79cd\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u952f\u9f7f\u5f62\u626b\u63cf\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728L-PBF\u4f18\u5316\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u63a7\u5236\u5fae\u89c2\u7ed3\u6784\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21826", "pdf": "https://arxiv.org/pdf/2506.21826", "abs": "https://arxiv.org/abs/2506.21826", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, accepted at ICDAR2025", "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u5c11\u6837\u672c\u5386\u53f2\u5730\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u4f5c\u4e3a\u4e30\u5bcc\u7684\u5386\u53f2\u8d44\u6e90\uff0c\u5176\u591a\u6837\u5316\u7684\u89c6\u89c9\u8868\u73b0\u548c\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u7ed9\u81ea\u52a8\u5316\u5904\u7406\u5e26\u6765\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u5d4c\u5165\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u5206\u5272\u3002", "result": "\u5728Siegfried\u6570\u636e\u96c6\u4e0a\uff0c10-shot\u548c5-shot\u573a\u666f\u4e0b\u5206\u522b\u63d0\u53475%\u548c13%\u7684mIoU\uff1b\u5728ICDAR 2021\u6570\u636e\u96c6\u4e0a\u8fbe\u523067.3%\u7684PQ\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6781\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u63a8\u52a8\u5386\u53f2\u5730\u56fe\u7684\u81ea\u52a8\u5316\u5904\u7406\u3002"}}
{"id": "2506.21828", "pdf": "https://arxiv.org/pdf/2506.21828", "abs": "https://arxiv.org/abs/2506.21828", "authors": ["Weitao Tang", "Johann Vargas-Calixto", "Nasim Katebi", "Robert Galinsky", "Gari D. Clifford", "Faezeh Marzbanrad"], "title": "Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "comment": "Review article, 17 pages, 1 figure, 5 tables, submitted to Sleep\n  (under review)", "summary": "Fetal sleep is a relatively underexplored yet vital aspect of prenatal\nneurodevelopment. Understanding fetal sleep patterns could provide insights\ninto early brain maturation and help clinicians detect signs of neurological\ncompromise that arise due to fetal hypoxia or fetal growth restriction. This\nreview synthesizes over eight decades of research on the physiological\ncharacteristics, ontogeny, and regulation of fetal sleep. We compare\nsleep-state patterns in humans and large animal models, highlighting\nspecies-specific differences and the presence of sleep-state analogs. We review\nboth invasive techniques in animals and non-invasive modalities in humans.\nComputational methods for sleep-state classification are also examined,\nincluding rule-based approaches (with and without clustering-based\npreprocessing) and state-of-the-art deep learning techniques. Finally, we\ndiscuss how intrauterine conditions such as hypoxia and fetal growth\nrestriction can disrupt fetal sleep. This review provides a comprehensive\nfoundation for the development of objective, multimodal, and non-invasive fetal\nsleep monitoring technologies to support early diagnosis and intervention in\nprenatal care.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u80ce\u513f\u7761\u7720\u7684\u751f\u7406\u7279\u5f81\u3001\u53d1\u80b2\u89c4\u5f8b\u53ca\u8c03\u63a7\u673a\u5236\uff0c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u52a8\u7269\u6a21\u578b\u7684\u7761\u7720\u6a21\u5f0f\uff0c\u63a2\u8ba8\u4e86\u4fb5\u5165\u6027\u548c\u975e\u4fb5\u5165\u6027\u76d1\u6d4b\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u5bab\u5185\u73af\u5883\u5bf9\u80ce\u513f\u7761\u7720\u7684\u5f71\u54cd\u3002", "motivation": "\u80ce\u513f\u7761\u7720\u5bf9\u65e9\u671f\u795e\u7ecf\u53d1\u80b2\u81f3\u5173\u91cd\u8981\uff0c\u4e86\u89e3\u5176\u6a21\u5f0f\u6709\u52a9\u4e8e\u53d1\u73b0\u7f3a\u6c27\u6216\u751f\u957f\u53d7\u9650\u7b49\u795e\u7ecf\u95ee\u9898\u3002", "method": "\u7efc\u5408\u4e8680\u591a\u5e74\u7684\u7814\u7a76\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u52a8\u7269\u6a21\u578b\u7684\u7761\u7720\u6a21\u5f0f\uff0c\u5206\u6790\u4fb5\u5165\u6027\u548c\u975e\u4fb5\u5165\u6027\u6280\u672f\uff0c\u4ee5\u53ca\u8ba1\u7b97\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u548c\u89c4\u5219\u5206\u7c7b\uff09\u3002", "result": "\u63ed\u793a\u4e86\u7269\u79cd\u95f4\u7761\u7720\u6a21\u5f0f\u7684\u5dee\u5f02\u53ca\u5bab\u5185\u73af\u5883\uff08\u5982\u7f3a\u6c27\uff09\u5bf9\u80ce\u513f\u7761\u7720\u7684\u5e72\u6270\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u975e\u4fb5\u5165\u6027\u80ce\u513f\u7761\u7720\u76d1\u6d4b\u6280\u672f\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u65e9\u671f\u8bca\u65ad\u548c\u5e72\u9884\u3002"}}
{"id": "2506.21842", "pdf": "https://arxiv.org/pdf/2506.21842", "abs": "https://arxiv.org/abs/2506.21842", "authors": ["Archisman Ghosh", "Satwik Kundu", "Swaroop Ghosh"], "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses", "categories": ["quant-ph", "cs.CR", "cs.LG"], "comment": "23 pages, 5 figures", "summary": "Quantum Machine Learning (QML) integrates quantum computing with classical\nmachine learning, primarily to solve classification, regression and generative\ntasks. However, its rapid development raises critical security challenges in\nthe Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines\nadversarial threats unique to QML systems, focusing on vulnerabilities in\ncloud-based deployments, hybrid architectures, and quantum generative models.\nKey attack vectors include model stealing via transpilation or output\nextraction, data poisoning through quantum-specific perturbations, reverse\nengineering of proprietary variational quantum circuits, and backdoor attacks.\nAdversaries exploit noise-prone quantum hardware and insufficiently secured\nQML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,\nand functionality. Defense mechanisms leverage quantum properties to counter\nthese threats. Noise signatures from training hardware act as non-invasive\nwatermarks, while hardware-aware obfuscation techniques and ensemble strategies\ndisrupt cloning attempts. Emerging solutions also adapt classical adversarial\ntraining and differential privacy to quantum settings, addressing\nvulnerabilities in quantum neural networks and generative architectures.\nHowever, securing QML requires addressing open challenges such as balancing\nnoise levels for reliability and security, mitigating cross-platform attacks,\nand developing quantum-classical trust frameworks. This chapter summarizes\nrecent advances in attacks and defenses, offering a roadmap for researchers and\npractitioners to build robust, trustworthy QML systems resilient to evolving\nadversarial landscapes.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff0c\u4f46\u9762\u4e34NISQ\u65f6\u4ee3\u7684\u72ec\u7279\u5b89\u5168\u6311\u6218\u3002\u672c\u7ae0\u63a2\u8ba8QML\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u5a01\u80c1\u53ca\u9632\u5fa1\u673a\u5236\u3002", "motivation": "QML\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u5b89\u5168\u62c5\u5fe7\uff0c\u5c24\u5176\u5728\u4e91\u90e8\u7f72\u548c\u6df7\u5408\u67b6\u6784\u4e2d\uff0c\u9700\u7814\u7a76\u5176\u72ec\u7279\u6f0f\u6d1e\u53ca\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u5206\u6790QML\u7684\u5bf9\u6297\u5a01\u80c1\uff08\u5982\u6a21\u578b\u7a83\u53d6\u3001\u6570\u636e\u6295\u6bd2\uff09\u53ca\u9632\u5fa1\u673a\u5236\uff08\u5982\u566a\u58f0\u6c34\u5370\u3001\u786c\u4ef6\u611f\u77e5\u6df7\u6dc6\uff09\u3002", "result": "\u63d0\u51fa\u9632\u5fa1\u7b56\u7565\uff08\u5982\u91cf\u5b50\u5bf9\u6297\u8bad\u7ec3\u3001\u5dee\u5206\u9690\u79c1\uff09\uff0c\u4f46\u9700\u89e3\u51b3\u566a\u58f0\u5e73\u8861\u7b49\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u4e3a\u6784\u5efa\u9c81\u68d2QML\u7cfb\u7edf\u63d0\u4f9b\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u5bf9\u6297\u6027\u5a01\u80c1\u4e0e\u9632\u5fa1\u7684\u5e73\u8861\u3002"}}
{"id": "2506.21849", "pdf": "https://arxiv.org/pdf/2506.21849", "abs": "https://arxiv.org/abs/2506.21849", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u751f\u6210\u4e00\u81f4\u6027\u7684\u5047\u8bbe\uff08\u4e00\u81f4\u6027\u5047\u8bbe\uff09\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u9700\u8981\u9ad8\u7528\u6237\u4fe1\u4efb\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4f30\u8ba1LLM\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\u56e0\u5176\u4ec5\u9700\u6a21\u578bAPI\u8bbf\u95ee\u7684\u5b9e\u7528\u6027\u800c\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u4e2a\u6570\u5b66\u9648\u8ff0\u53ca\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u5f62\u5f0f\u5316\u4e86\u4e00\u81f4\u6027\u5047\u8bbe\uff0c\u5e76\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c3\u4e2a\u4efb\u52a1\uff08\u95ee\u7b54\u3001\u6587\u672c\u6458\u8981\u548c\u6587\u672c\u5230SQL\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e00\u81f4\u6027\u5047\u8bbe\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u666e\u904d\u6210\u7acb\uff0c\u5176\u4e2d\u201cSim-Any\u201d\u5047\u8bbe\u6700\u5177\u53ef\u64cd\u4f5c\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65e0\u9700\u6570\u636e\u7684\u9ed1\u76d2UQ\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4e00\u81f4\u6027\u5047\u8bbe\u7684\u5b9e\u8bc1\u89c2\u5bdf\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aLLM\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.21857", "pdf": "https://arxiv.org/pdf/2506.21857", "abs": "https://arxiv.org/abs/2506.21857", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u548c\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u8de8\u75be\u75c5\u75c5\u7406\u4efb\u52a1\u7684\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46WSI\u4e0eST\u6570\u636e\u7684\u5168\u9762\u6574\u5408\u4ecd\u5b58\u5728\u7a7a\u767d\u3002", "method": "SPADE\u91c7\u7528\u6df7\u5408\u6570\u636e\u4e13\u5bb6\u6280\u672f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7279\u5f81\u7a7a\u95f4\u805a\u7c7b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6574\u5408WSI\u548c\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u3002", "result": "\u572814\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cSPADE\u8868\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u5c11\u6837\u672c\u6027\u80fd\u3002", "conclusion": "SPADE\u8bc1\u660e\u4e86\u5c06\u5f62\u6001\u5b66\u548c\u5206\u5b50\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.21884", "pdf": "https://arxiv.org/pdf/2506.21884", "abs": "https://arxiv.org/abs/2506.21884", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chac\u00f3n", "Bernard Ghanem"], "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "comment": "Paper accepted at ICCV 2025 main conference", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "AI": {"tldr": "UnMix-NeRF\u7ed3\u5408\u5149\u8c31\u5206\u89e3\u4e0eNeRF\uff0c\u5b9e\u73b0\u9ad8\u5149\u8c31\u65b0\u89c6\u89d2\u5408\u6210\u548c\u65e0\u76d1\u7763\u6750\u6599\u5206\u5272\uff0c\u63d0\u5347\u6750\u6599\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709NeRF\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56RGB\u6570\u636e\uff0c\u7f3a\u4e4f\u6750\u6599\u5c5e\u6027\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u3001AR\u7b49\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u5149\u8c31\u53cd\u5c04\u7684\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\u6210\u5206\uff0c\u5b66\u4e60\u5168\u5c40\u7aef\u5143\u5b57\u5178\u548c\u70b9\u7ea7\u4e30\u5ea6\u5206\u5e03\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u6750\u6599\u805a\u7c7b\u548c\u573a\u666f\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUnMix-NeRF\u5728\u5149\u8c31\u91cd\u5efa\u548c\u6750\u6599\u5206\u5272\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UnMix-NeRF\u4e3a\u6750\u6599\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u652f\u6301\u7075\u6d3b\u7684\u573a\u666f\u7f16\u8f91\u3002"}}
{"id": "2506.21887", "pdf": "https://arxiv.org/pdf/2506.21887", "abs": "https://arxiv.org/abs/2506.21887", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "AI": {"tldr": "Active-MoSH\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u8f6f\u786c\u8fb9\u754c\u548c\u504f\u597d\u5b66\u4e60\uff0c\u63d0\u5347\u51b3\u7b56\u8005\u4fe1\u4efb\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u9ad8\u540e\u679c\u51b3\u7b56\u4e2d\u591a\u76ee\u6807\u4f18\u5316\u7684\u6311\u6218\uff0c\u5982\u8d44\u6e90\u5bc6\u96c6\u8bc4\u4f30\u548c\u51b3\u7b56\u8005\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\uff08\u8f6f\u786c\u8fb9\u754c\u4e0e\u504f\u597d\u5b66\u4e60\uff09\u548c\u5168\u5c40\uff08\u591a\u76ee\u6807\u654f\u611f\u6027\u5206\u6790\uff09\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e3b\u52a8\u91c7\u6837\u4f18\u5316\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u6536\u655b\u6027\u3001\u4fe1\u4efb\u548c\u504f\u597d\u8868\u8fbe\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "Active-MoSH\u6709\u6548\u652f\u6301\u51b3\u7b56\u8005\uff0c\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2506.21894", "pdf": "https://arxiv.org/pdf/2506.21894", "abs": "https://arxiv.org/abs/2506.21894", "authors": ["Rafael Oliveira", "Xuesong Wang", "Kian Ming A. Chai", "Edwin V. Bonilla"], "title": "Thompson Sampling in Function Spaces via Neural Operators", "categories": ["stat.ML", "cs.LG"], "comment": "Under review", "summary": "We propose an extension of Thompson sampling to optimization problems over\nfunction spaces where the objective is a known functional of an unknown\noperator's output. We assume that functional evaluations are inexpensive, while\nqueries to the operator (such as running a high-fidelity simulator) are costly.\nOur algorithm employs a sample-then-optimize approach using neural operator\nsurrogates. This strategy avoids explicit uncertainty quantification by\ntreating trained neural operators as approximate samples from a Gaussian\nprocess. We provide novel theoretical convergence guarantees, based on Gaussian\nprocesses in the infinite-dimensional setting, under minimal assumptions. We\nbenchmark our method against existing baselines on functional optimization\ntasks involving partial differential equations and other nonlinear\noperator-driven phenomena, demonstrating improved sample efficiency and\ncompetitive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eThompson\u91c7\u6837\u7684\u51fd\u6570\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7b97\u5b50\u66ff\u4ee3\u9ad8\u6210\u672c\u64cd\u4f5c\uff0c\u65e0\u9700\u663e\u5f0f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u51fd\u6570\u7a7a\u95f4\u4e2d\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u76ee\u6807\u51fd\u6570\u662f\u672a\u77e5\u7b97\u5b50\u8f93\u51fa\u7684\u5df2\u77e5\u6cdb\u51fd\uff0c\u4e14\u7b97\u5b50\u67e5\u8be2\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u6837\u672c\u540e\u4f18\u5316\u7b56\u7565\uff0c\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u66ff\u4ee3\u9ad8\u65af\u8fc7\u7a0b\u6837\u672c\uff0c\u907f\u514d\u663e\u5f0f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u504f\u5fae\u5206\u65b9\u7a0b\u7b49\u975e\u7ebf\u6027\u7b97\u5b50\u9a71\u52a8\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51fd\u6570\u4f18\u5316\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2506.21946", "pdf": "https://arxiv.org/pdf/2506.21946", "abs": "https://arxiv.org/abs/2506.21946", "authors": ["Till Wenke"], "title": "Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling", "categories": ["cs.CY", "cs.LG"], "comment": null, "summary": "Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded\nsystematic study due to its informal nature. This paper presents and analyzes\nthe largest known structured dataset of hitchhiking rides, comprising over\n63,000 entries collected over nearly two decades through platforms associated\nwith hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced\ncontributions, the dataset captures key spatiotemporal and strategic aspects of\nhitchhiking. This work documents the dataset's origins, evolution, and\ncommunity-driven maintenance, highlighting its Europe-centric distribution,\nseasonal patterns, and reliance on a small number of highly active\ncontributors. Through exploratory analyses, I examine waiting times, user\nbehavior, and comment metadata, shedding light on the lived realities of\nhitchhikers. While the dataset has inherent biases and limitations - such as\ndemographic skew and unverifiable entries it offers a rare and valuable window\ninto an alternative form of mobility. I conclude by outlining future directions\nfor enriching the dataset and advancing research on hitchhiking as both a\ntransportation practice and cultural phenomenon.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6700\u5927\u7684\u642d\u4fbf\u8f66\u6570\u636e\u96c6\uff0863,000+\u6761\u76ee\uff09\uff0c\u63ed\u793a\u4e86\u5176\u65f6\u7a7a\u7279\u5f81\u3001\u5b63\u8282\u6027\u6a21\u5f0f\u53ca\u7528\u6237\u884c\u4e3a\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u642d\u4fbf\u8f66\u4f5c\u4e3a\u4e00\u79cd\u975e\u6b63\u5f0f\u7684\u51fa\u884c\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528hitchwiki.org\u548chitchmap.com\u5e73\u53f0\u6536\u96c6\u7684\u4f17\u5305\u6570\u636e\uff0c\u8fdb\u884c\u65f6\u7a7a\u548c\u7b56\u7565\u5206\u6790\u3002", "result": "\u6570\u636e\u96c6\u63ed\u793a\u4e86\u6b27\u6d32\u4e2d\u5fc3\u7684\u5206\u5e03\u3001\u5b63\u8282\u6027\u6a21\u5f0f\u53ca\u6d3b\u8dc3\u8d21\u732e\u8005\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u7b49\u5f85\u65f6\u95f4\u548c\u7528\u6237\u884c\u4e3a\u7684\u7279\u70b9\u3002", "conclusion": "\u5c3d\u7ba1\u6570\u636e\u96c6\u5b58\u5728\u504f\u5dee\uff0c\u4f46\u4ecd\u4e3a\u7814\u7a76\u642d\u4fbf\u8f66\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4e30\u5bcc\u6570\u636e\u5e76\u6df1\u5316\u7814\u7a76\u3002"}}
{"id": "2506.21967", "pdf": "https://arxiv.org/pdf/2506.21967", "abs": "https://arxiv.org/abs/2506.21967", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.", "AI": {"tldr": "\u7814\u7a76\u6307\u51fa\u5f53\u524d\u5de5\u5177\u96c6\u6210LLM\u4ee3\u7406\u7684\u8bc4\u4f30\u591a\u5173\u6ce8\u7aef\u5230\u7aef\u5de5\u5177\u4f7f\u7528\uff0c\u800c\u5ffd\u89c6\u5176\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u53d1\u73b0\u4ee3\u7406\u5728\u5404\u9636\u6bb5\u6613\u51fa\u9519\uff0c\u5f00\u6e90\u6a21\u578b\u4ee3\u7406\u66f4\u8106\u5f31\uff0c\u589e\u5927\u6a21\u578b\u89c4\u6a21\u672a\u5fc5\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5ffd\u89c6\u4ee3\u7406\u7a33\u5b9a\u6027\uff0c\u5f71\u54cd\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u7814\u7a76\u4ee3\u7406\u5728\u5de5\u5177\u8c03\u7528\u5168\u8fc7\u7a0b\u4e2d\u7684\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4ee3\u7406\u5728\u8bfb\u53d6\u5de5\u5177\u6587\u6863\u3001\u9009\u62e9\u5de5\u5177\u4e0e\u751f\u6210\u53c2\u6570\u3001\u5904\u7406\u5de5\u5177\u54cd\u5e94\u7b49\u9636\u6bb5\u7684\u9519\u8bef\u6613\u53d1\u6027\u3002", "result": "\u4ee3\u7406\u5728\u5404\u9636\u6bb5\u5747\u6613\u51fa\u9519\uff0c\u5f00\u6e90\u6a21\u578b\u4ee3\u7406\u66f4\u8106\u5f31\uff1b\u589e\u5927\u6a21\u578b\u89c4\u6a21\u672a\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u653b\u51fb\u98ce\u9669\u3002", "conclusion": "\u5f3a\u8c03\u8bc4\u4f30\u4ee3\u7406\u7a33\u5b9a\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765LLM\u5f00\u53d1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u6d1e\u89c1\u3002"}}
{"id": "2506.21972", "pdf": "https://arxiv.org/pdf/2506.21972", "abs": "https://arxiv.org/abs/2506.21972", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6df7\u5408\u653b\u51fb\u65b9\u6cd5\uff08GCG + PAIR\u548cGCG + WordGame\uff09\uff0c\u7ed3\u5408\u4ee4\u724c\u7ea7\u548c\u63d0\u793a\u7ea7\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u7a81\u7834\u4e86\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PTLMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5b89\u5168\u6027\u4ecd\u5b58\u5728\u6f0f\u6d1e\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff08\u4ee4\u724c\u7ea7\u548c\u63d0\u793a\u7ea7\uff09\u5404\u6709\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6df7\u5408\u653b\u51fb\u65b9\u6cd5\uff1aGCG + PAIR\u548cGCG + WordGame\uff0c\u7ed3\u5408\u4ee4\u724c\u7ea7\u548c\u63d0\u793a\u7ea7\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728Vicuna\u548cLlama\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "GCG + PAIR\u5728Llama-3\u4e0a\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8fbe\u523091.6%\uff0c\u663e\u8457\u9ad8\u4e8ePAIR\u768458.4%\uff1bGCG + WordGame\u5728\u9ad8\u4e25\u683c\u8bc4\u4f30\u4e0b\u4ecd\u4fdd\u630180%\u4ee5\u4e0a\u7684ASR\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u7a81\u7834\u9ad8\u7ea7\u9632\u5fa1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u7684\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u9632\u5fa1\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u5168\u9762\u7684\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2506.21990", "pdf": "https://arxiv.org/pdf/2506.21990", "abs": "https://arxiv.org/abs/2506.21990", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u548c\u6807\u51c6\u5316\u65b9\u6848\u63d0\u5347Whisper\u6a21\u578b\u5728\u9a7e\u9a76\u8231\u5bf9\u8bdd\u8f6c\u5f55\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5c06\u8bcd\u9519\u8bef\u7387\u4ece68.49%\u964d\u81f326.26%\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9a7e\u9a76\u8231\u7b49\u7279\u5b9a\u9886\u57df\u7684\u8f6c\u5f55\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u6536\u96c6\u9a7e\u9a76\u8231\u6a21\u62df\u5668\u548c\u98de\u884c\u5458\u8bbf\u8c08\u5f55\u97f3\uff0c\u624b\u52a8\u6807\u6ce8\u6570\u636e\uff1b\u63d0\u51fa\u591a\u79cd\u6807\u51c6\u5316\u65b9\u6848\uff1b\u91c7\u7528LoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u540e\u7684Whisper Large\u6a21\u578b\u7ed3\u5408\u6807\u51c6\u5316\u65b9\u6848\uff0c\u8bcd\u9519\u8bef\u7387\u4ece68.49%\u964d\u81f326.26%\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u6807\u51c6\u5316\u548c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86Whisper\u6a21\u578b\u5728\u9a7e\u9a76\u8231\u5bf9\u8bdd\u8f6c\u5f55\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22101", "pdf": "https://arxiv.org/pdf/2506.22101", "abs": "https://arxiv.org/abs/2506.22101", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": "Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo\n  will be made publicly available soon", "summary": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "AI": {"tldr": "TPM\u6539\u8fdbADNet\uff0c\u901a\u8fc7\u7ed1\u5b9a\u539f\u578b\u4f4d\u7f6e\u548c\u591a\u539f\u578b\u6269\u5c55\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3ADNet\u5728\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u5355\u539f\u578b\u4f9d\u8d56\u3001\u4e8c\u5206\u7c7b\u95ee\u9898\u548c\u56fa\u5b9a\u9608\u503c\u3002", "method": "\u63d0\u51faTied Prototype Model (TPM)\uff0c\u7ed1\u5b9a\u524d\u666f\u548c\u80cc\u666f\u5206\u5e03\u7684\u539f\u578b\u4f4d\u7f6e\uff0c\u652f\u6301\u591a\u539f\u578b\u548c\u591a\u7c7b\u5206\u5272\uff0c\u5e76\u5229\u7528\u7c7b\u5148\u9a8c\u5b9a\u4e49\u81ea\u9002\u5e94\u9608\u503c\u3002", "result": "TPM\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u591a\u539f\u578b\u548c\u591a\u7c7b\u5206\u5272\u4efb\u52a1\u4e2d\u3002", "conclusion": "TPM\u4e3a\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u539f\u578b\u5efa\u6a21\u89c6\u89d2\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.22105", "pdf": "https://arxiv.org/pdf/2506.22105", "abs": "https://arxiv.org/abs/2506.22105", "authors": ["David Demitri Africa"], "title": "Identifying a Circuit for Verb Conjugation in GPT-2", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u4eceGPT-2 Small\u4e2d\u5206\u79bb\u51fa\u8d1f\u8d23\u4e3b\u8c13\u4e00\u81f4\u7684\u5b50\u7f51\u7edc\uff0c\u5e76\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22GPT-2 Small\u4e2d\u4e3b\u8c13\u4e00\u81f4\u4efb\u52a1\u7684\u5185\u90e8\u673a\u5236\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5b9e\u73b0\u8fd9\u4e00\u529f\u80fd\u3002", "method": "\u4f7f\u7528\u6027\u80fd\u9a8c\u8bc1\u3001\u81ea\u52a8\u7535\u8def\u53d1\u73b0\uff08\u76f4\u63a5\u8def\u5f84\u4fee\u8865\uff09\u548c\u76f4\u63a5\u5bf9\u6570\u5f52\u56e0\u6280\u672f\uff0c\u5206\u79bb\u51fa\u5019\u9009\u7535\u8def\u3002", "result": "\u4ec5\u9700\u5c11\u91cf\u7f51\u7edc\u7ec4\u4ef6\u5373\u53ef\u5b8c\u6210\u57fa\u7840\u4efb\u52a1\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u9700\u8981\u66f4\u591a\u7ec4\u4ef6\u3002", "conclusion": "\u4e3b\u8c13\u4e00\u81f4\u529f\u80fd\u7531\u7279\u5b9a\u5b50\u7f51\u7edc\u5b9e\u73b0\uff0c\u4e14\u5176\u590d\u6742\u6027\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u3002"}}
{"id": "2506.22146", "pdf": "https://arxiv.org/pdf/2506.22146", "abs": "https://arxiv.org/abs/2506.22146", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u8f93\u5165\u7684\u4f4e\u7ea7\u7a7a\u95f4\u7ed3\u6784\u548c\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u5b58\u5728\u7ed1\u5b9a\u95ee\u9898\uff0c\u96be\u4ee5\u53ef\u9760\u5730\u5c06\u611f\u77e5\u7279\u5f81\u4e0e\u89c6\u89c9\u5bf9\u8c61\u5173\u8054\uff0c\u5bfc\u81f4\u5728\u8ba1\u6570\u3001\u89c6\u89c9\u641c\u7d22\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u6dfb\u52a0\u4f4e\u7ea7\u7a7a\u95f4\u7ed3\u6784\uff08\u5982\u6c34\u5e73\u7ebf\uff09\u5e76\u7ed3\u5408\u6587\u672c\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u611f\u77e5\u7684\u5e8f\u5217\u89e3\u6790\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1a\u89c6\u89c9\u641c\u7d22\u51c6\u786e\u7387\u63d0\u9ad825.00%\uff0c\u8ba1\u6570\u51c6\u786e\u7387\u63d0\u9ad826.83%\uff0c\u573a\u666f\u63cf\u8ff0\u7f16\u8f91\u8ddd\u79bb\u8bef\u5dee\u51cf\u5c110.32\uff0c\u7a7a\u95f4\u5173\u7cfb\u4efb\u52a1\u6027\u80fd\u63d0\u53479.50%\u3002", "conclusion": "\u4f4e\u7ea7\u7684\u89c6\u89c9\u7ed3\u6784\u8c03\u6574\u662f\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u6709\u6548\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u65b9\u5411\u3002"}}
{"id": "2506.22174", "pdf": "https://arxiv.org/pdf/2506.22174", "abs": "https://arxiv.org/abs/2506.22174", "authors": ["Bavo Lesy", "Siemen Herremans", "Robin Kerstens", "Jan Steckel", "Walter Daems", "Siegfried Mercelis", "Ali Anwar"], "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research", "categories": ["cs.RO", "cs.LG"], "comment": "14 Pages, 11 Figures", "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.", "AI": {"tldr": "ASVSim\u662f\u4e00\u4e2a\u5f00\u6e90\u4eff\u771f\u6846\u67b6\uff0c\u4e13\u4e3a\u5185\u6cb3\u548c\u6e2f\u53e3\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u822a\u8fd0\u7814\u7a76\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u8239\u8236\u52a8\u529b\u5b66\u548c\u6d77\u6d0b\u4f20\u611f\u5668\u6a21\u62df\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USVs\uff09\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5f00\u6e90\u3001\u9ad8\u4fdd\u771f\u4eff\u771f\u6846\u67b6\u548c\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u822a\u8fd0\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eCosys-AirSim\u5f00\u53d1ASVSim\uff0c\u7ed3\u5408\u8239\u8236\u52a8\u529b\u5b66\u548c\u4f20\u611f\u5668\u6a21\u62df\uff08\u5982\u96f7\u8fbe\u548c\u6444\u50cf\u5934\uff09\uff0c\u652f\u6301\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "ASVSim\u4e3a\u81ea\u4e3b\u5bfc\u822a\u7b97\u6cd5\u5f00\u53d1\u548c\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u5e73\u53f0\uff0c\u652f\u6301\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u3002", "conclusion": "ASVSim\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6d77\u6d0b\u5de5\u7a0b\u9886\u57df\u7684\u81ea\u4e3b\u5bfc\u822a\u7814\u7a76\u3002"}}
{"id": "2506.22204", "pdf": "https://arxiv.org/pdf/2506.22204", "abs": "https://arxiv.org/abs/2506.22204", "authors": ["Gurjeet Sangra Singh", "Maciej Falkiewicz", "Alexandros Kalousis"], "title": "Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport", "categories": ["stat.ML", "cs.LG"], "comment": "Workshop paper at ICLR 2025 (XAI4Science Workshop)", "summary": "Physics phenomena are often described by ordinary and/or partial differential\nequations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,\nmany real-world systems are described only approximately with missing or\nunknown terms in the equations. This makes the distribution of the physics\nmodel differ from the true data-generating process (DGP). Using limited and\nunpaired data between DGP observations and the imperfect model simulations, we\ninvestigate this particular setting by completing the known-physics model,\ncombining theory-driven models and data-driven to describe the shifted\ndistribution involved in the DGP. We present a novel hybrid generative model\napproach combining deep grey-box modelling with Optimal Transport (OT) methods\nto enhance incomplete physics models. Our method implements OT maps in data\nspace while maintaining minimal source distribution distortion, demonstrating\nsuperior performance in resolving the unpaired problem and ensuring correct\nusage of physics parameters. Unlike black-box alternatives, our approach\nleverages physics-based inductive biases to accurately learn system dynamics\nwhile preserving interpretability through its domain knowledge foundation.\nExperimental results validate our method's effectiveness in both generation\ntasks and model transparency, offering detailed insights into learned physics\ndynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u7070\u7bb1\u5efa\u6a21\u4e0e\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u65b9\u6cd5\u7684\u6df7\u5408\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u8865\u5168\u4e0d\u5b8c\u6574\u7269\u7406\u6a21\u578b\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u4e0e\u6a21\u578b\u6a21\u62df\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u5e38\u56e0\u7269\u7406\u6a21\u578b\u4e0d\u5b8c\u6574\u800c\u4e0e\u771f\u5b9e\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff08DGP\uff09\u5b58\u5728\u5206\u5e03\u5dee\u5f02\uff0c\u9700\u7ed3\u5408\u7406\u8bba\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u7070\u7bb1\u5efa\u6a21\u4e0eOT\u65b9\u6cd5\u7ed3\u5408\u7684\u6df7\u5408\u751f\u6210\u6a21\u578b\uff0c\u5728\u6570\u636e\u7a7a\u95f4\u5b9e\u73b0OT\u6620\u5c04\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6e90\u5206\u5e03\u5931\u771f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u548c\u6a21\u578b\u900f\u660e\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u51c6\u786e\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u672a\u914d\u5bf9\u6570\u636e\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.22228", "pdf": "https://arxiv.org/pdf/2506.22228", "abs": "https://arxiv.org/abs/2506.22228", "authors": ["Rong Ma", "Xi Li", "Jingyuan Hu", "Bin Yu"], "title": "Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings", "categories": ["stat.ML", "cs.LG", "q-bio.GN", "stat.AP"], "comment": null, "summary": "Single-cell sequencing is revolutionizing biology by enabling detailed\ninvestigations of cell-state transitions. Many biological processes unfold\nalong continuous trajectories, yet it remains challenging to extract smooth,\nlow-dimensional representations from inherently noisy, high-dimensional\nsingle-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,\nare widely used to embed high-dimensional single-cell data into low dimensions.\nBut they often introduce undesirable distortions, resulting in misleading\ninterpretations. Existing evaluation methods for NE algorithms primarily focus\non separating discrete cell types rather than capturing continuous cell-state\ntransitions, while dynamic modeling approaches rely on strong assumptions about\ncellular processes and specialized data. To address these challenges, we build\non the Predictability-Computability-Stability (PCS) framework for reliable and\nreproducible data-driven discoveries. First, we systematically evaluate popular\nNE algorithms through empirical analysis, simulation, and theory, and reveal\ntheir key shortcomings, such as artifacts and instability. We then introduce\nNESS, a principled and interpretable machine learning approach to improve NE\nrepresentations by leveraging algorithmic stability and to enable robust\ninference of smooth biological structures. NESS offers useful concepts,\nquantitative stability metrics, and efficient computational workflows to\nuncover developmental trajectories and cell-state transitions in single-cell\ndata. Finally, we apply NESS to six single-cell datasets, spanning pluripotent\nstem cell differentiation, organoid development, and multiple tissue-specific\nlineage trajectories. Across these diverse contexts, NESS consistently yields\nuseful biological insights, such as identification of transitional and stable\ncell states and quantification of transcriptional dynamics during development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNESS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5355\u7ec6\u80de\u6570\u636e\u7684\u4f4e\u7ef4\u5d4c\u5165\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u8fde\u7eed\u7ec6\u80de\u72b6\u6001\u8f6c\u6362\u65f6\u7684\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u5355\u7ec6\u80de\u6d4b\u5e8f\u6280\u672f\u867d\u80fd\u8be6\u7ec6\u7814\u7a76\u7ec6\u80de\u72b6\u6001\u8f6c\u6362\uff0c\u4f46\u73b0\u6709\u90bb\u57df\u5d4c\u5165\u7b97\u6cd5\uff08\u5982t-SNE\u548cUMAP\uff09\u5e38\u5f15\u5165\u5931\u771f\uff0c\u4e14\u8bc4\u4f30\u65b9\u6cd5\u591a\u5173\u6ce8\u79bb\u6563\u7ec6\u80de\u7c7b\u578b\u800c\u975e\u8fde\u7eed\u8f6c\u6362\u3002", "method": "\u57fa\u4e8ePCS\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u3001\u6a21\u62df\u548c\u7406\u8bba\u8bc4\u4f30\u73b0\u6709NE\u7b97\u6cd5\uff0c\u5e76\u63d0\u51faNESS\u65b9\u6cd5\uff0c\u5229\u7528\u7b97\u6cd5\u7a33\u5b9a\u6027\u6539\u8fdb\u8868\u793a\u5e76\u63a8\u65ad\u5e73\u6ed1\u751f\u7269\u7ed3\u6784\u3002", "result": "NESS\u5728\u516d\u79cd\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e2d\u4e00\u81f4\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u751f\u7269\u5b66\u89c1\u89e3\uff0c\u5982\u8bc6\u522b\u8fc7\u6e21\u548c\u7a33\u5b9a\u7ec6\u80de\u72b6\u6001\u53ca\u91cf\u5316\u53d1\u80b2\u4e2d\u7684\u8f6c\u5f55\u52a8\u6001\u3002", "conclusion": "NESS\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63ed\u793a\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u7684\u8fde\u7eed\u751f\u7269\u7ed3\u6784\u3002"}}
{"id": "2506.22236", "pdf": "https://arxiv.org/pdf/2506.22236", "abs": "https://arxiv.org/abs/2506.22236", "authors": ["Hanti Lin"], "title": "A Plea for History and Philosophy of Statistics and Machine Learning", "categories": ["stat.OT", "cs.LG"], "comment": null, "summary": "The integration of the history and philosophy of statistics was initiated at\nleast by Hacking (1965) and advanced by Mayo (1996), but it has not received\nsustained follow-up. Yet such integration is more urgent than ever, as the\nrecent success of artificial intelligence has been driven largely by machine\nlearning -- a field historically developed alongside statistics. Today, the\nboundary between statistics and machine learning is increasingly blurred. What\nwe now need is integration, twice over: of history and philosophy, and of the\nfield they engage -- statistics and machine learning. I present a case study of\na philosophical idea in machine learning (and in formal epistemology) whose\nroot can be traced back to an often under-appreciated insight in Neyman and\nPearson's 1936 work (a follow-up to their 1933 classic). This leads to the\narticulation of a foundational assumption -- largely implicit in, but shared\nby, the practices of frequentist statistics and machine learning -- which I\ncall achievabilism. Another integration also emerges at the level of\nmethodology, combining two ends of the philosophy of science spectrum: history\nand philosophy of science on the one hand, and formal epistemology on the other\nhand.", "AI": {"tldr": "\u8be5\u8bba\u6587\u547c\u5401\u5c06\u7edf\u8ba1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2\u548c\u54f2\u5b66\u8fdb\u884c\u53cc\u91cd\u6574\u5408\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u57fa\u7840\u5047\u8bbe\u2014\u2014achievableism\u3002", "motivation": "\u5f53\u524d\u7edf\u8ba1\u5b66\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u754c\u9650\u6a21\u7cca\uff0c\u9700\u8981\u6574\u5408\u5176\u5386\u53f2\u4e0e\u54f2\u5b66\uff0c\u4ee5\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u8ffd\u6eaf\u54f2\u5b66\u601d\u60f3\u5230Neyman\u548cPearson\u7684\u65e9\u671f\u5de5\u4f5c\uff0c\u5e76\u63ed\u793a\u57fa\u7840\u5047\u8bbe\u3002", "result": "\u63d0\u51fa\u4e86achievableism\u8fd9\u4e00\u57fa\u7840\u5047\u8bbe\uff0c\u5e76\u5c55\u793a\u4e86\u65b9\u6cd5\u8bba\u5c42\u9762\u7684\u6574\u5408\u3002", "conclusion": "\u53cc\u91cd\u6574\u5408\u5bf9\u7edf\u8ba1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u672a\u6765\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.22241", "pdf": "https://arxiv.org/pdf/2506.22241", "abs": "https://arxiv.org/abs/2506.22241", "authors": ["Matthias Tsch\u00f6pe", "Vitor Fortes Rey", "Sogo Pierre Sanon", "Paul Lukowicz", "Nikolaos Palaiodimopoulos", "Maximilian Kiefer-Emmanouilidis"], "title": "Boosting Classification with Quantum-Inspired Augmentations", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "comment": null, "summary": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u95e8\u6270\u52a8\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u6280\u672f\u5bf9\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u9690\u79c1\u8ba1\u7b97\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u6270\u52a8\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5c06\u5176\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u6280\u672f\u5e94\u7528\u4e8e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u968f\u673aBloch\u7403\u65cb\u8f6c\u4f5c\u4e3a\u91cf\u5b50\u542f\u53d1\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5176\u6548\u679c\u3002", "result": "\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff08Top-1\u51c6\u786e\u7387\u63d0\u9ad83%\uff0cTop-5\u51c6\u786e\u7387\u63d0\u9ad82.5%\uff0cF1\u5206\u6570\u4ece8%\u63d0\u5347\u81f312%\uff09\u3002", "conclusion": "\u91cf\u5b50\u6270\u52a8\u589e\u5f3a\u6280\u672f\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u6548\uff0c\u4f46\u5728\u9690\u79c1\u8ba1\u7b97\u65b9\u9762\u65e0\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.22271", "pdf": "https://arxiv.org/pdf/2506.22271", "abs": "https://arxiv.org/abs/2506.22271", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u6a21\u578b\u4e2d\u7684\u79e9\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u7684\u8f93\u51fa\u5c42\uff08KGE-MoS\uff09\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KGC\u6a21\u578b\u5728\u5b9e\u4f53\u6570\u91cf\u8fdc\u5927\u4e8e\u5d4c\u5165\u7ef4\u5ea6\u65f6\uff0c\u7531\u4e8e\u7ebf\u6027\u8f93\u51fa\u5c42\u7684\u79e9\u74f6\u9888\u9650\u5236\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5f71\u54cd\u4e86\u6392\u540d\u51c6\u786e\u6027\u548c\u5206\u6570\u5206\u5e03\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faKGE-MoS\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u7684\u8f93\u51fa\u5c42\uff0c\u4ee5\u6253\u7834\u79e9\u74f6\u9888\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKGE-MoS\u4ee5\u8f83\u4f4e\u53c2\u6570\u6210\u672c\u63d0\u5347\u4e86KGC\u6a21\u578b\u7684\u6027\u80fd\u548c\u6982\u7387\u62df\u5408\u5ea6\u3002", "conclusion": "KGE-MoS\u6709\u6548\u89e3\u51b3\u4e86\u79e9\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u4e86KGC\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2506.22309", "pdf": "https://arxiv.org/pdf/2506.22309", "abs": "https://arxiv.org/abs/2506.22309", "authors": ["Klara M. Gutekunst", "Dominik D\u00fcrrschnabel", "Johannes Hirth", "Gerd Stumme"], "title": "Conceptual Topic Aggregation", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u7684\u65b9\u6cd5FAT-CAT\uff0c\u7528\u4e8e\u6539\u8fdb\u4e3b\u9898\u5efa\u6a21\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89c6\u5316\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u9650\u5236\u4e86\u6570\u636e\u7ed3\u6784\u548c\u5185\u5bb9\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u91c7\u7528\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u6784\u5efa\u6982\u5ff5\u683c\uff0c\u5b9e\u73b0\u4e3b\u9898\u7684\u5c42\u6b21\u5316\u8868\u793a\u548c\u53ef\u89c6\u5316\u3002", "result": "\u5728ETYNTKE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFAT-CAT\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u66f4\u5177\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u8868\u793a\u3002", "conclusion": "FAT-CAT\u901a\u8fc7FCA\u63d0\u5347\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6570\u636e\u96c6\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.22335", "pdf": "https://arxiv.org/pdf/2506.22335", "abs": "https://arxiv.org/abs/2506.22335", "authors": ["Osama Ahmed", "Felix Tennie", "Luca Magri"], "title": "Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability", "categories": ["quant-ph", "cs.LG", "nlin.CD"], "comment": "28 pages, 12 figures", "summary": "We show that recurrent quantum reservoir computers (QRCs) and their\nrecurrence-free architectures (RF-QRCs) are robust tools for learning and\nforecasting chaotic dynamics from time-series data. First, we formulate and\ninterpret quantum reservoir computers as coupled dynamical systems, where the\nreservoir acts as a response system driven by training data; in other words,\nquantum reservoir computers are generalized-synchronization (GS) systems.\nSecond, we show that quantum reservoir computers can learn chaotic dynamics and\ntheir invariant properties, such as Lyapunov spectra, attractor dimensions, and\ngeometric properties such as the covariant Lyapunov vectors. This analysis is\nenabled by deriving the Jacobian of the quantum reservoir update. Third, by\nleveraging tools from generalized synchronization, we provide a method for\ndesigning robust quantum reservoir computers. We propose the criterion\n$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We\nanalytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we\nanalyze the effect of simulated noise. We find that dissipation from noise\nenhances the robustness of quantum reservoir computers. Numerical verifications\non systems of different dimensions support our conclusions. This work opens\nopportunities for designing robust quantum machines for chaotic time series\nforecasting on near-term quantum hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u91cf\u5b50\u50a8\u5907\u8ba1\u7b97\u5668\uff08QRCs\uff09\u53ca\u5176\u65e0\u9012\u5f52\u67b6\u6784\uff08RF-QRCs\uff09\u662f\u5b66\u4e60\u6df7\u6c8c\u52a8\u529b\u5b66\u7684\u6709\u6548\u5de5\u5177\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u50a8\u5907\u8ba1\u7b97\u5668\u5728\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63a2\u7d22\u5176\u9c81\u68d2\u6027\u548c\u8bbe\u8ba1\u51c6\u5219\u3002", "method": "\u5c06QRCs\u5efa\u6a21\u4e3a\u8026\u5408\u52a8\u529b\u7cfb\u7edf\uff0c\u63a8\u5bfc\u5176Jacobian\u77e9\u9635\uff0c\u63d0\u51faGS=ESP\u8bbe\u8ba1\u51c6\u5219\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "QRCs\u80fd\u5b66\u4e60\u6df7\u6c8c\u52a8\u529b\u5b66\u53ca\u5176\u4e0d\u53d8\u6027\u8d28\uff0c\u566a\u58f0\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0cRF-QRCs\u6ee1\u8db3GS=ESP\u51c6\u5219\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fd1\u91cf\u5b50\u786c\u4ef6\u4e0a\u7684\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u8bbe\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2506.22340", "pdf": "https://arxiv.org/pdf/2506.22340", "abs": "https://arxiv.org/abs/2506.22340", "authors": ["Yannick Werner", "Akash Malemath", "Mengxi Liu", "Vitor Fortes Rey", "Nikolaos Palaiodimopoulos", "Paul Lukowicz", "Maximilian Kiefer-Emmanouilidis"], "title": "QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks", "categories": ["quant-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold\nrepresentation theorem (KAR), have demonstrated promising capabilities in\nexpressing complex functions with fewer neurons. This is achieved by\nimplementing learnable parameters on the edges instead of on the nodes, unlike\ntraditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs\npotential in quantum machine learning has not yet been well explored. In this\nwork, we present an implementation of these KAN architectures in both hybrid\nand fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt\nthe KAN transfer using pre-trained residual functions, thereby exploiting the\nrepresentational power of parametrized quantum circuits. In the hybrid model we\ncombine classical KAN components with quantum subroutines, while the fully\nquantum version the entire architecture of the residual function is translated\nto a quantum model. We demonstrate the feasibility, interpretability and\nperformance of the proposed Quantum KAN (QuKAN) architecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov Arnold Networks (KANs)\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u7ecf\u5178\u548c\u91cf\u5b50\u7ec4\u4ef6\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u884c\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22KANs\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u5229\u7528\u5176\u8868\u8fbe\u590d\u6742\u51fd\u6570\u7684\u4f18\u52bf\u3002", "method": "\u5b9e\u73b0\u6df7\u5408\u548c\u5168\u91cf\u5b50\u5f62\u5f0f\u7684KAN\u67b6\u6784\uff0c\u4f7f\u7528Quantum Circuit Born Machine (QCBM)\u548c\u9884\u8bad\u7ec3\u6b8b\u5dee\u51fd\u6570\u3002", "result": "\u5c55\u793a\u4e86Quantum KAN (QuKAN)\u67b6\u6784\u7684\u53ef\u884c\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "conclusion": "QuKAN\u67b6\u6784\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u7ed3\u5408\u4e86KANs\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.22343", "pdf": "https://arxiv.org/pdf/2506.22343", "abs": "https://arxiv.org/abs/2506.22343", "authors": ["Xiang Li", "Garrett Wen", "Weiqing He", "Jiayuan Wu", "Qi Long", "Weijie J. Su"], "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "comment": null, "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6df7\u5408\u6765\u6e90\u6587\u672c\u4e2d\u6c34\u5370\u6bd4\u4f8b\u7684\u6700\u4f18\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5173\u952e\u7edf\u8ba1\u91cf\u7684\u9ad8\u6548\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u5b58\u5728\u6df7\u5408\u6765\u6e90\u6587\u672c\uff08\u4eba\u7c7b\u4e66\u5199\u4e0eLLM\u751f\u6210\u5185\u5bb9\u6df7\u5408\uff09\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6574\u7bc7\u6587\u672c\u7684\u6c34\u5370\u68c0\u6d4b\uff0c\u800c\u7f3a\u4e4f\u5bf9\u6df7\u5408\u6587\u672c\u4e2d\u6c34\u5370\u6bd4\u4f8b\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u57fa\u4e8e\u5173\u952e\u7edf\u8ba1\u91cf\u7684\u6df7\u5408\u6a21\u578b\u6bd4\u4f8b\u53c2\u6570\u4f30\u8ba1\uff0c\u9488\u5bf9\u8fde\u7eed\u5173\u952e\u7edf\u8ba1\u91cf\u7684\u6c34\u5370\u65b9\u6cd5\u63d0\u51fa\u9ad8\u6548\u4f30\u8ba1\u5668\uff0c\u5e76\u63a8\u5bfc\u6781\u5c0f\u6781\u5927\u4e0b\u754c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u4f30\u8ba1\u5668\u5728\u5408\u6210\u6570\u636e\u548c\u5f00\u6e90\u6a21\u578b\u751f\u6210\u7684\u6df7\u5408\u6587\u672c\u4e2d\u5747\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u5bf9\u4e8e\u4f7f\u7528\u8fde\u7eed\u5173\u952e\u7edf\u8ba1\u91cf\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u6c34\u5370\u6bd4\u4f8b\u53c2\u6570\u662f\u53ef\u8bc6\u522b\u7684\uff0c\u4e14\u6240\u63d0\u4f30\u8ba1\u5668\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.22360", "pdf": "https://arxiv.org/pdf/2506.22360", "abs": "https://arxiv.org/abs/2506.22360", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86CNN\uff08ResNet34\uff09\u548cViT\uff08ViT B16\uff09\u5728\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0ResNet34\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u7565\u4f18\uff0c\u4f46ViT B16\u5728\u566a\u58f0\u73af\u5883\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\uff08\u5982\u65e0\u4eba\u673a\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u9002\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5728GEN1\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\u5fae\u8c03ResNet34\u548cViT B16\uff0c\u5e76\u5728\u6807\u51c6\u6761\u4ef6\u548c\u6a21\u62df\u566a\u58f0\u4e0b\u8bc4\u4f30\u6027\u80fd\u3002", "result": "ResNet34\u548cViT B16\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a88%\u548c86%\uff0cViT B16\u5728\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e8b\u4ef6\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u6cd5\u53c2\u8003\uff0cViT B16\u7684\u9c81\u68d2\u6027\u5c24\u5176\u503c\u5f97\u5173\u6ce8\u3002"}}
{"id": "2506.22362", "pdf": "https://arxiv.org/pdf/2506.22362", "abs": "https://arxiv.org/abs/2506.22362", "authors": ["Yang Yang", "Yunpeng Li", "George Sung", "Shao-Fu Shih", "Craig Dooley", "Alessio Centazzo", "Ramanan Rajeswaran"], "title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding", "categories": ["eess.AS", "cs.LG"], "comment": null, "summary": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss.", "AI": {"tldr": "DiffSoundStream\u901a\u8fc7\u51cf\u5c11\u8bed\u4e49\u548c\u58f0\u5b66\u4ee4\u724c\u7684\u5197\u4f59\u4ee5\u53ca\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u975e\u6d41\u5f0f\u573a\u666f\u4e2d\u8bed\u97f3\u4ee4\u724c\u5316\u7684\u6548\u7387\u3002", "motivation": "\u8bed\u97f3\u751f\u6210\u4e2d\u4ee4\u724c\u5316\u65b9\u6cd5\u7684\u63a8\u7406\u901f\u5ea6\u53d7\u9650\u4e8e\u4ee4\u724c\u7387\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u901a\u8fc7\u8bed\u4e49\u4ee4\u724c\u8c03\u8282\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b2. \u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4ece\u8bed\u4e49\u548c\u7c97\u7c92\u5ea6\u58f0\u5b66\u4ee4\u724c\u5408\u6210\u9ad8\u8d28\u91cf\u6ce2\u5f62\u3002", "result": "\u572850\u4ee4\u724c/\u79d2\u4e0b\uff0cDiffSoundStream\u7684\u8bed\u97f3\u8d28\u91cf\u4e0e\u6807\u51c6SoundStream\u6a21\u578b\u5728100\u4ee4\u724c/\u79d2\u65f6\u76f8\u5f53\uff0c\u4e14\u4ec5\u97004\u6b65\u6269\u6563\u91c7\u6837\u5373\u53ef\u5b9e\u73b0\u6b65\u957f\u84b8\u998f\u3002", "conclusion": "DiffSoundStream\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u97f3\u4ee4\u724c\u5316\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u751f\u6210\u3002"}}
{"id": "2506.22419", "pdf": "https://arxiv.org/pdf/2506.22419", "abs": "https://arxiv.org/abs/2506.22419", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u81ea\u52a8\u5316LLM\u901f\u5ea6\u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u5728\u79d1\u5b66\u91cd\u73b0\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u524d\u6cbfLLM\u5373\u4f7f\u6709\u8be6\u7ec6\u63d0\u793a\u4e5f\u96be\u4ee5\u91cd\u73b0\u5df2\u77e5\u521b\u65b0\u3002", "motivation": "\u8bc4\u4f30AI\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u91cd\u73b0\u7ed3\u679c\u7684\u80fd\u529b\uff0c\u4ee5\u63a8\u52a8\u79d1\u5b66\u8fdb\u6b65\u3002", "method": "\u5229\u7528NanoGPT\u901f\u5ea6\u8fd0\u884c\u7ade\u8d5b\u7684\u6570\u636e\uff0c\u8bbe\u8ba119\u4e2a\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e0d\u540c\u683c\u5f0f\u7684\u63d0\u793a\uff0c\u6d4b\u8bd5LLM\u91cd\u73b0\u4ee3\u7801\u6539\u8fdb\u7684\u80fd\u529b\u3002", "result": "\u524d\u6cbfLLM\u5373\u4f7f\u7ed3\u5408\u5148\u8fdb\u6846\u67b6\uff0c\u4e5f\u96be\u4ee5\u91cd\u73b0\u5df2\u77e5\u521b\u65b0\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8861\u91cfLLM\u81ea\u52a8\u5316\u79d1\u5b66\u91cd\u73b0\u80fd\u529b\u63d0\u4f9b\u4e86\u7b80\u5355\u4e14\u672a\u9971\u548c\u7684\u6807\u51c6\uff0c\u662f\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\u7684\u5fc5\u8981\u6280\u80fd\u3002"}}
{"id": "2506.22429", "pdf": "https://arxiv.org/pdf/2506.22429", "abs": "https://arxiv.org/abs/2506.22429", "authors": ["David Holzm\u00fcller", "Max Sch\u00f6lpple"], "title": "Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "While the theory of deep learning has made some progress in recent years,\nmuch of it is limited to the ReLU activation function. In particular, while the\nneural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)\nhave given theoreticians tractable limiting cases of fully connected neural\nnetworks, their properties for most activation functions except for powers of\nthe ReLU function are poorly understood. Our main contribution is to provide a\nmore general characterization of the RKHS of these kernels for typical\nactivation functions whose only non-smoothness is at zero, such as SELU, ELU,\nor LeakyReLU. Our analysis also covers a broad set of special cases such as\nmissing biases, two-layer networks, or polynomial activations. Our results show\nthat a broad class of not infinitely smooth activations generate equivalent\nRKHSs at different network depths, while polynomial activations generate\nnon-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP\nsample paths, characterizing the smoothness of infinitely wide neural networks\nat initialization.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5178\u578b\u6fc0\u6d3b\u51fd\u6570\uff08\u5982SELU\u3001ELU\u3001LeakyReLU\uff09\u5728\u795e\u7ecf\u6b63\u5207\u6838\uff08NTK\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u9ad8\u65af\u8fc7\u7a0b\u6838\uff08NNGP\uff09\u4e2d\u7684RKHS\u7279\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7f51\u7edc\u6df1\u5ea6\u548c\u591a\u9879\u5f0f\u6fc0\u6d3b\u51fd\u6570\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u4e3b\u8981\u5c40\u9650\u4e8eReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u5bf9\u5176\u4ed6\u6fc0\u6d3b\u51fd\u6570\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5178\u578b\u6fc0\u6d3b\u51fd\u6570\u7684RKHS\u7279\u6027\uff0c\u7814\u7a76\u5176\u5728NTK\u548cNNGP\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6269\u5c55\u5230\u7279\u6b8a\u60c5\u51b5\uff08\u5982\u65e0\u504f\u7f6e\u3001\u4e24\u5c42\u7f51\u7edc\u3001\u591a\u9879\u5f0f\u6fc0\u6d3b\uff09\u3002", "result": "\u53d1\u73b0\u4e00\u7c7b\u975e\u65e0\u9650\u5e73\u6ed1\u7684\u6fc0\u6d3b\u51fd\u6570\u5728\u4e0d\u540c\u7f51\u7edc\u6df1\u5ea6\u4e0b\u751f\u6210\u7b49\u6548\u7684RKHS\uff0c\u800c\u591a\u9879\u5f0f\u6fc0\u6d3b\u51fd\u6570\u751f\u6210\u975e\u7b49\u6548\u7684RKHS\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e86NNGP\u6837\u672c\u8def\u5f84\u7684\u5e73\u6ed1\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u66f4\u5e7f\u6cdb\u6fc0\u6d3b\u51fd\u6570\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u5bf9\u7f51\u7edc\u884c\u4e3a\u7684\u5f71\u54cd\u3002"}}
