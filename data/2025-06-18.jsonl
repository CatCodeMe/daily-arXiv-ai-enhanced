{"id": "2506.13785", "pdf": "https://arxiv.org/pdf/2506.13785", "abs": "https://arxiv.org/abs/2506.13785", "authors": ["Patrick Sutanto", "Jonathan Kenrick", "Max Lorenz", "Joan Santoso"], "title": "LLM-Driven Data Generation and a Novel Soft Metric for Evaluating Text-to-SQL in Aviation MRO", "categories": ["cs.DB", "cs.IR"], "comment": null, "summary": "The application of Large Language Models (LLMs) to text-to-SQL tasks promises\nto democratize data access, particularly in critical industries like aviation\nMaintenance, Repair, and Operation (MRO). However, progress is hindered by two\nkey challenges: the rigidity of conventional evaluation metrics such as\nexecution accuracy, which offer coarse, binary feedback, and the scarcity of\ndomain-specific evaluation datasets. This paper addresses these gaps. To enable\nmore nuanced assessment, we introduce a novel F1-score-based 'soft' metric that\nquantifies the informational overlap between generated and ground-truth SQL\nresults. To address data scarcity, we propose an LLM-driven pipeline that\nsynthesizes realistic question-SQL pairs from database schemas. We demonstrate\nour contributions through an empirical evaluation on an authentic MRO database.\nOur experiments show that the proposed soft metric provides more insightful\nperformance analysis than strict accuracy, and our data generation technique is\neffective in creating a domain-specific benchmark. Together, these\ncontributions offer a robust framework for evaluating and advancing text-to-SQL\nsystems in specialized environments."}
{"id": "2506.14034", "pdf": "https://arxiv.org/pdf/2506.14034", "abs": "https://arxiv.org/abs/2506.14034", "authors": ["Brian Tsan", "Abylay Amanbayev", "Asoke Datta", "Florin Rusu"], "title": "Sketched Sum-Product Networks for Joins", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Sketches have shown high accuracy in multi-way join cardinality estimation, a\ncritical problem in cost-based query optimization. Accurately estimating the\ncardinality of a join operation -- analogous to its computational cost --\nallows the optimization of query execution costs in relational database\nsystems. However, although sketches have shown high efficacy in query\noptimization, they are typically constructed specifically for predefined\nselections in queries that are assumed to be given a priori, hindering their\napplicability to new queries. As a more general solution, we propose for\nSum-Product Networks to dynamically approximate sketches on-the-fly.\nSum-Product Networks can decompose and model multivariate distributions, such\nas relations, as linear combinations of multiple univariate distributions. By\nrepresenting these univariate distributions as sketches, Sum-Product Networks\ncan combine them element-wise to efficiently approximate the sketch of any\nquery selection. These approximate sketches can then be applied to join\ncardinality estimation. In particular, we implement the Fast-AGMS and Bound\nSketch methods, which have successfully been used in prior work, despite their\ncostly construction. By accurately approximating them instead, our work\nprovides a practical alternative to apply these sketches to query optimization."}
{"id": "2506.14707", "pdf": "https://arxiv.org/pdf/2506.14707", "abs": "https://arxiv.org/abs/2506.14707", "authors": ["Qian Xu", "Feng Zhang", "Chengxi Li", "Lei Cao", "Zheng Chen", "Jidong Zhai", "Xiaoyong Du"], "title": "HARMONY: A Scalable Distributed Vector Database for High-Throughput Approximate Nearest Neighbor Search", "categories": ["cs.DB"], "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) is essential for various\ndata-intensive applications, including recommendation systems, image retrieval,\nand machine learning. Scaling ANNS to handle billions of high-dimensional\nvectors on a single machine presents significant challenges in memory capacity\nand processing efficiency. To address these challenges, distributed vector\ndatabases leverage multiple nodes for the parallel storage and processing of\nvectors. However, existing solutions often suffer from load imbalance and high\ncommunication overhead, primarily due to traditional partition strategies that\nfail to effectively distribute the workload. In this paper, we introduce\nHarmony, a distributed ANNS system that employs a novel multi-granularity\npartition strategy, combining dimension-based and vector-based partition. This\nstrategy ensures a balanced distribution of computational load across all nodes\nwhile effectively minimizing communication costs. Furthermore, Harmony\nincorporates an early-stop pruning mechanism that leverages the monotonicity of\ndistance computations in dimension-based partition, resulting in significant\nreductions in both computational and communication overhead. We conducted\nextensive experiments on diverse real-world datasets, demonstrating that\nHarmony outperforms leading distributed vector databases, achieving 4.63 times\nthroughput on average in four nodes and 58% performance improvement over\ntraditional distribution for skewed workloads."}
{"id": "2506.13989", "pdf": "https://arxiv.org/pdf/2506.13989", "abs": "https://arxiv.org/abs/2506.13989", "authors": ["Johan Östman", "Edvin Callisen", "Anton Chen", "Kristiina Ausmees", "Emanuel Gårdh", "Jovan Zamac", "Jolanta Goldsteine", "Hugo Wefer", "Simon Whelan", "Markus Reimegård"], "title": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering", "categories": ["cs.SI", "cs.AI", "cs.DB", "cs.LG"], "comment": "21 figures, 22 pages", "summary": "Money laundering enables organized crime by allowing illicit funds to enter\nthe legitimate economy. Although trillions of dollars are laundered each year,\nonly a small fraction is ever uncovered. This stems from a range of factors,\nincluding deliberate evasion by launderers, the rarity of confirmed cases, and\nthe limited visibility each financial institution has into the global\ntransaction network. While several synthetic datasets are available, they fail\nto model the structural and behavioral complexity of real-world money\nlaundering. In particular, they often overlook partial observability, sparse\nand uncertain labels, strategic behavior, temporal dynamics, class imbalance,\nand network-level dependencies. To address these limitations, we present\nAMLGentex, an open-source suite for generating realistic, configurable\ntransaction data and benchmarking detection methods. It enables systematic\nevaluation of anti-money laundering (AML) systems in a controlled environment\nthat captures key real-world challenges. We demonstrate how the framework can\nbe used to rigorously evaluate methods under conditions that reflect the\ncomplexity of practical AML scenarios."}
{"id": "2506.13998", "pdf": "https://arxiv.org/pdf/2506.13998", "abs": "https://arxiv.org/abs/2506.13998", "authors": ["Michael Anoprenko", "Andrei Tonkikh", "Alexander Spiegelman", "Petr Kuznetsov"], "title": "DAGs for the Masses", "categories": ["cs.DC"], "comment": null, "summary": "A recent approach to building consensus protocols on top of Directed Acyclic\nGraphs (DAGs) shows much promise due to its simplicity and stable throughput.\nHowever, as each node in the DAG typically includes a linear number of\nreferences to the nodes in the previous round, prior DAG protocols only scale\nup to a certain point when the overhead of maintaining the graph becomes the\nbottleneck.\n  To enable large-scale deployments of DAG-based protocols, we propose a sparse\nDAG architecture, where each node includes only a constant number of references\nto random nodes in the previous round. We present a sparse version of Bullshark\n-- one of the most prominent DAG-based consensus protocols -- and demonstrate\nits improved scalability.\n  Remarkably, unlike other protocols that use random sampling to reduce\ncommunication complexity, we manage to avoid sacrificing resilience: the\nprotocol can tolerate up to $f<n/3$ Byzantine faults (where $n$ is the number\nof participants), same as its less scalable deterministic counterpart. The\nproposed ``sparse'' methodology can be applied to any protocol that maintains\ndisseminated system updates and causal relations between them in a graph-like\nstructure. Our simulations show that the considerable reduction of transmitted\nmetadata in sparse DAGs results in more efficient network utilization and\nbetter scalability."}
{"id": "2506.13982", "pdf": "https://arxiv.org/pdf/2506.13982", "abs": "https://arxiv.org/abs/2506.13982", "authors": ["Ewan Davies", "Ryan Job", "Maxine Kampbell", "Hannah Kim", "Hyojin Seo"], "title": "Spectral partitioning of graphs into compact, connected regions", "categories": ["cs.DS", "physics.soc-ph"], "comment": "18 pages", "summary": "We define and study a spectral recombination algorithm, SpecReCom, for\npartitioning a graph into a given number of connected parts. It is\nstraightforward to introduce additional constraints such as the requirement\nthat the weight (or number of vertices) in each part is approximately balanced,\nand we exemplify this by stating a variant, BalSpecReCom, of the SpecReCom\nalgorithm. We provide empirical evidence that the algorithm achieves more\ncompact partitions than alternatives such as RevReCom by studying a $56\\times\n56$ grid graph and a planar graph obtained from the state of Colorado."}
{"id": "2506.13822", "pdf": "https://arxiv.org/pdf/2506.13822", "abs": "https://arxiv.org/abs/2506.13822", "authors": ["Yang Liu"], "title": "The Trip to ZigBee Backscatter across a Decade, a Systematic Review", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "The field of backscatter communication has undergone a profound\ntransformation, evolving from a niche technology for radio-frequency\nidentification (RFID) into a sophisticated paradigm poised to enable a truly\nbattery-free Internet of Things (IoT). This evolution is built upon a deepening\nunderstanding of the fundamental principles governing these ultra-low-power\nlinks. Modern backscatter systems are no longer simple reflectors of continuous\nwaves but are increasingly designed to interact with complex, data-carrying\nambient signals from ubiquitous sources like WiFi, ZigBee, and cellular\nnetworks. This review systematically charts the journey of ambient backscatter,\nparticularly focusing on its interaction with ZigBee and other commodity\nwireless protocols over the last decade. We analyze the progression from\nfoundational proof-of-concept systems that established productive backscatter\nto modern high-throughput, concurrent, and cross-technology communication\narchitectures. Key advancements in fine-grained modulation, robust\nsynchronization, cross-technology physical layer emulation, and multi-tag\ncoordination are detailed. A comparative analysis of state-of-the-art systems\nhighlights the core trade-offs between performance metrics like data rate and\nrange, power consumption, and compatibility with commodity hardware. Finally,\nwe synthesize the primary challenges, including networking scalability,\nsecurity vulnerabilities, the near-far problem, and practical deployment\nhurdles, and outline future research directions, such as integration with\nReconfigurable Intelligent Surfaces (RIS) and 6G networks, that promise to\nfurther expand the capabilities of this transformative technology."}
{"id": "2506.13800", "pdf": "https://arxiv.org/pdf/2506.13800", "abs": "https://arxiv.org/abs/2506.13800", "authors": ["Abul Ehtesham", "Aditi Singh", "Saket Kumar"], "title": "Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Enhancing clinical decision support (CDS), reducing documentation burdens,\nand improving patient health literacy remain persistent challenges in digital\nhealth. This paper presents an open-source, agent-based framework that\nintegrates Large Language Models (LLMs) with HL7 FHIR data via the Model\nContext Protocol (MCP) for dynamic extraction and reasoning over electronic\nhealth records (EHRs). Built on the established MCP-FHIR implementation, the\nframework enables declarative access to diverse FHIR resources through\nJSON-based configurations, supporting real-time summarization, interpretation,\nand personalized communication across multiple user personas, including\nclinicians, caregivers, and patients. To ensure privacy and reproducibility,\nthe framework is evaluated using synthetic EHR data from the SMART Health IT\nsandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4\nstandard. Unlike traditional approaches that rely on hardcoded retrieval and\nstatic workflows, the proposed method delivers scalable, explainable, and\ninteroperable AI-powered EHR applications. The agentic architecture further\nsupports multiple FHIR formats, laying a robust foundation for advancing\npersonalized digital health solutions."}
{"id": "2506.13771", "pdf": "https://arxiv.org/pdf/2506.13771", "abs": "https://arxiv.org/abs/2506.13771", "authors": ["Banseok Lee", "Dongkyu Kim", "Youngcheon You", "Youngmin Kim"], "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. This establishes a superior size-performance trade-off, with\nkernel-level benchmarks indicating potential for a 5$\\times$ speedup compared\nto FP16. LittleBit paves the way for deploying powerful LLMs in\nresource-constrained environments."}
{"id": "2506.14630", "pdf": "https://arxiv.org/pdf/2506.14630", "abs": "https://arxiv.org/abs/2506.14630", "authors": ["Rúben Adão", "Zhongjie Wu", "Changjun Zhou", "Oana Balmau", "João Paulo", "Ricardo Macedo"], "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)", "categories": ["cs.DC", "cs.DB"], "comment": "This is an extended version of the full paper to appear in VLDB 2025", "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."}
{"id": "2506.14107", "pdf": "https://arxiv.org/pdf/2506.14107", "abs": "https://arxiv.org/abs/2506.14107", "authors": ["Jinwoo Hwang", "Daeun Kim", "Sangyeop Lee", "Yoonsung Kim", "Guseul Heo", "Hojoon Kim", "Yunseok Jeong", "Tadiwos Meaza", "Eunhyeok Park", "Jeongseob Ahn", "Jongse Park"], "title": "Déjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse", "categories": ["cs.DC", "cs.CV"], "comment": "Accepted to 2025 VLDB", "summary": "Recently, Video-Language Models (VideoLMs) have demonstrated remarkable\ncapabilities, offering significant potential for flexible and powerful video\nquery systems. These models typically rely on Vision Transformers (ViTs), which\nprocess video frames individually to extract visual embeddings. However,\ngenerating embeddings for large-scale videos requires ViT inferencing across\nnumerous frames, posing a major hurdle to real-world deployment and\nnecessitating solutions for integration into scalable video data management\nsystems. This paper introduces D\\'ej\\`a Vu, a video-language query engine that\naccelerates ViT-based VideoLMs by reusing computations across consecutive\nframes. At its core is ReuseViT, a modified ViT model specifically designed for\nVideoLM tasks, which learns to detect inter-frame reuse opportunities, striking\nan effective balance between accuracy and reuse. Although ReuseViT\nsignificantly reduces computation, these savings do not directly translate into\nperformance gains on GPUs. To overcome this, D\\'ej\\`a Vu integrates\nmemory-compute joint compaction techniques that convert the FLOP savings into\ntangible performance gains. Evaluations on three VideoLM tasks show that\nD\\'ej\\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error\nbound, dramatically enhancing the practicality of VideoLMs for large-scale\nvideo analytics."}
{"id": "2506.13991", "pdf": "https://arxiv.org/pdf/2506.13991", "abs": "https://arxiv.org/abs/2506.13991", "authors": ["Viktor Krapivensky"], "title": "glass: ordered set data structure for client-side order books", "categories": ["cs.DS"], "comment": null, "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."}
{"id": "2506.13934", "pdf": "https://arxiv.org/pdf/2506.13934", "abs": "https://arxiv.org/abs/2506.13934", "authors": ["Warren Scantlebury", "Milena Radenkovic"], "title": "Emerging Networks and Services in Developing Nations -- Barbados Use Case", "categories": ["cs.NI"], "comment": null, "summary": "This report aims to conduct an in-depth comparison of DTN\n(Delay/Disconnection Tolerant Network) performance and characteristics in the\ndeveloping country of Barbados versus two major UK cities Nottingham and\nLondon. We aim to detect any common patterns or deviations between the two\nregion areas and use the results of our network simulations to draw\nwell-founded conclusions on the reasons for these similarities and differences.\nIn the end we hope to be able to assimilate specific portions of the island to\nthese major cities in regard to DTN characteristics. We also want to\ninvestigate the viability of DTN use in the transport sector which has\nstruggled from a range of issues related to efficiency and finance, by\nrecording and analysing the same metrics for a DTN that consists of only buses.\nThis work is intended to serve as a bridge for expanding the breadth of\nresearch done on developed countries allowing other researchers to be able to\nmake well informed assumptions about how that research may apply to developing\nnations. It will consist of results that show graphical trends and analysis of\nwhy these trends might exist and how they apply to real world scenarios."}
{"id": "2506.13804", "pdf": "https://arxiv.org/pdf/2506.13804", "abs": "https://arxiv.org/abs/2506.13804", "authors": ["Edward McDaid", "Sarah McDaid"], "title": "Instruction and Solution Probabilities as Heuristics for Inductive Programming", "categories": ["cs.SE", "cs.AI", "D.1.2; D.3.3; F.1.1; F.3.1; F.3.3; I.2.1; I.2.2; I.2.4; I.2.5;\n  I.2.8; I.5.3"], "comment": "10 pages, 10 figures", "summary": "Instruction subsets (ISs) are heuristics that can shrink the size of the\ninductive programming (IP) search space by tens of orders of magnitude. Here,\nwe extend the IS approach by introducing instruction and solution probabilities\nas additional heuristics. Instruction probability reflects the expectation of\nan instruction occurring in a solution, based on the frequency of instruction\noccurrence in a large code sample. The solution probability for a partial or\ncomplete program is simply the product of all constituent instruction\nprobabilities, including duplicates. We treat the minimum solution\nprobabilities observed in code sample program units of different sizes as\nsolution probability thresholds. These thresholds are used to prune the search\nspace as partial solutions are constructed, thereby eliminating any branches\ncontaining unlikely combinations of instructions. The new approach has been\nevaluated using a large sample of human code. We tested two formulations of\ninstruction probability: one based on instruction occurrence across the entire\ncode sample and another that measured the distribution separately for each IS.\nOur results show that both variants produce substantial further reductions in\nthe IP search space size of up to tens of orders of magnitude, depending on\nsolution size. In combination with IS, reductions of over 100 orders of\nmagnitude can be achieved. We also carried out cross-validation testing to show\nthat the heuristics should work effectively with unseen code. The approach is\ndescribed and the results and some ideas for future work are discussed."}
{"id": "2506.13772", "pdf": "https://arxiv.org/pdf/2506.13772", "abs": "https://arxiv.org/abs/2506.13772", "authors": ["Zhenyan Lu", "Daliang Xu", "Dongqi Cai", "Zexi Li", "Wei Liu", "Fangming Liu", "Shangguang Wang", "Mengwei Xu"], "title": "MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are deployed on mobile devices to power killer\napplications such as intelligent assistants. LLMs pre-trained on general\ncorpora often hallucinate when handling personalized or unseen queries, leading\nto incorrect or outdated responses. Knowledge editing addresses this by\nidentifying and adjusting a small crucial portion of model weights, without\ncompromising the general knowledge. However, prior knowledge editing methods\nare impractical to run on local devices due to the resource-heavy\nbackpropagation (BP) needed for updates. We present MobiEdit, the first mobile\nknowledge editing framework that enables efficient LLM personalization on\ncommercial off-the-shelf (COTS) mobile devices. MobiEdit replaces\nfull-precision BP with quantized forward-only gradient estimation, thus\ncompatible with the energy-efficient mobile neural processing units (NPUs).\nMobiEdit replaces full-precision backpropagation with quantized forward-only\ngradient estimation, making it compatible with energy-efficient mobile NPUs. To\nfurther improve gradient estimation efficiency, we introduce two optimizations:\nan early stoping mechanism that adaptively terminates editing upon success and\na prefix cache that reuses computation across steps. Our approach enables\nreal-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile\ndevices with 7.6$\\times$ less memory, 14.7 $\\times$ less energy and 3.6$\\times$\nless latency compared to previous knowledge editing methods."}
{"id": "2506.14197", "pdf": "https://arxiv.org/pdf/2506.14197", "abs": "https://arxiv.org/abs/2506.14197", "authors": ["Dr Craig S Wright"], "title": "The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic Demonstration of Miner-Centric Propagation Topologies", "categories": ["cs.DC", "cs.CR", "cs.SI", "math.CO", "05C82, 68M10", "C.2.1; C.2.0; G.2.2"], "comment": "34 pages, 1 figures. Comprehensive technical treatment of Bitcoin\n  propagation topology. Submitted to arXiv for public dissemination and\n  archival reference", "summary": "This paper formally examines the network structure of Bitcoin CORE (BTC) and\nBitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that\nhome-hosted full nodes are incapable of participating in or influencing the\npropagation topology. Leveraging established models such as scale-free networks\nand small-world connectivity, we demonstrate that the propagation graph is\ndominated by a densely interconnected miner clique, while full nodes reside on\nthe periphery, excluded from all transaction-to-block inclusion paths. Using\nsimulation-backed metrics and eigenvalue centrality analysis, we confirm that\nfull nodes are neither critical nor operationally relevant for consensus\npropagation."}
{"id": "2506.14062", "pdf": "https://arxiv.org/pdf/2506.14062", "abs": "https://arxiv.org/abs/2506.14062", "authors": ["Lilith Orion Hafner", "Adriano Meligrana"], "title": "An Exact and Efficient Sampler for Dynamic Discrete Distributions", "categories": ["cs.DS", "math.PR", "stat.CO"], "comment": "8 pages, 5 figures", "summary": "Sampling from a dynamic discrete distribution involves sampling from a\ndynamic set of weighted elements, where elements can be added or removed at any\nstage of the sampling process. Although efficient for static sets, the Alias\nmethod becomes impractical in dynamic settings due to the need to reconstruct\nthe sampler after each update, which incurs a computational cost proportional\nto the size of the distribution, making it unsuitable for applications\nrequiring frequent insertions, deletions, or weight adjustments. To address\nthis limitation, different approaches have been studied, such as the Forest of\nTrees method and the BUcket Sampling (BUS) method. However, all previous\nmethods suffered from numerical issues which can bias the sampling process. In\nthis paper, we describe EBUS (Exact BUcket Sampling), the first exact algorithm\nwith $O(1)$ sampling and update cost. The sampler can be updated by base-$b$\nnumbers with bounded precision and exponent, and sample the distribution of its\nelements exactly and efficiently. We provide also a state of the art\nimplementation of the method using IEEE 64-bit floating point numbers which we\nempirically show to be more efficient than several implementations of previous\ninexact methods."}
{"id": "2506.14151", "pdf": "https://arxiv.org/pdf/2506.14151", "abs": "https://arxiv.org/abs/2506.14151", "authors": ["Chungang Lin", "Yilong Jiang", "Weiyao Zhang", "Xuying Meng", "Tianyu Zuo", "Yujun Zhang"], "title": "TraGe: A Generic Packet Representation for Traffic Classification Based on Header-Payload Differences", "categories": ["cs.NI"], "comment": "This paper has been accepted by IWQoS 2025 as a short paper", "summary": "Traffic classification has a significant impact on maintaining the Quality of\nService (QoS) of the network. Since traditional methods heavily rely on feature\nextraction and large scale labeled data, some recent pre-trained models manage\nto reduce the dependency by utilizing different pre-training tasks to train\ngeneric representations for network packets. However, existing pre-trained\nmodels typically adopt pre-training tasks developed for image or text data,\nwhich are not tailored to traffic data. As a result, the obtained traffic\nrepresentations fail to fully reflect the information contained in the traffic,\nand may even disrupt the protocol information. To address this, we propose\nTraGe, a novel generic packet representation model for traffic classification.\nBased on the differences between the header and payload-the two fundamental\ncomponents of a network packet-we perform differentiated pre-training according\nto the byte sequence variations (continuous in the header vs. discontinuous in\nthe payload). A dynamic masking strategy is further introduced to prevent\noverfitting to fixed byte positions. Once the generic packet representation is\nobtained, TraGe can be finetuned for diverse traffic classification tasks using\nlimited labeled data. Experimental results demonstrate that TraGe significantly\noutperforms state-of-the-art methods on two traffic classification tasks, with\nup to a 6.97% performance improvement. Moreover, TraGe exhibits superior\nrobustness under parameter fluctuations and variations in sampling\nconfigurations."}
{"id": "2506.13815", "pdf": "https://arxiv.org/pdf/2506.13815", "abs": "https://arxiv.org/abs/2506.13815", "authors": ["Shrinivass Arunachalam Balasubramanian"], "title": "Signal-First Architectures: Rethinking Front-End Reactivity", "categories": ["cs.SE"], "comment": "18 pages, 4 figures", "summary": "Modern front-end frameworks face escalating reactivity management challenges,\nincluding performance degradation from complex observable chains and\nunpredictable re-renders. This paper introduces Signal-First Architecture--a\nnovel paradigm where granular, dependency-tracked signals are the atomic unit\nof reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces\nreactive flows from explicit signal declarations, with derived values via\ncomputed() and side effects scoped to effect(). This model ensures\ndeterministic behavior by eliminating implicit subscriptions and optimizing\nreactive graph evaluation.\n  We present a comparative analysis of three Angular reactivity models: RxJS\nservice-based, NgRx global stores, and pure Signal-First implementations.\nThrough controlled benchmarking, including Chrome DevTools performance tracing,\nmemory heap snapshots, and Lighthouse audits, this study quantifies\nSignal-First advantages."}
{"id": "2506.13781", "pdf": "https://arxiv.org/pdf/2506.13781", "abs": "https://arxiv.org/abs/2506.13781", "authors": ["Pablo Ariño Fernández"], "title": "Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment", "categories": ["cs.LG", "cs.AI", "cs.DM", "90-04 (Primary), 90B35, 68T05 (Secondary)", "I.2.8; I.2.6; D.0; G.2.1; G.2.2"], "comment": "Bachelor's thesis, Universidad Polit\\'ecnica de Madrid, 2025. 150\n  pages, 23 figures", "summary": "The job shop scheduling problem is an NP-hard combinatorial optimization\nproblem relevant to manufacturing and timetabling. Traditional approaches use\npriority dispatching rules based on simple heuristics. Recent work has\nattempted to replace these with deep learning models, particularly graph neural\nnetworks (GNNs), that learn to assign priorities from data. However, training\nsuch models requires customizing numerous factors: graph representation, node\nfeatures, action space, and reward functions. The lack of modular libraries for\nexperimentation makes this research time-consuming. This work introduces\nJobShopLib, a modular library that allows customizing these factors and\ncreating new components with its reinforcement learning environment. We trained\nseveral dispatchers through imitation learning to demonstrate the environment's\nutility. One model outperformed various graph-based dispatchers using only\nindividual operation features, highlighting the importance of feature\ncustomization. Our GNN model achieved near state-of-the-art results on\nlarge-scale problems. These results suggest significant room for improvement in\ndeveloping such models. JobShopLib provides the necessary tools for future\nexperimentation."}
{"id": "2506.14237", "pdf": "https://arxiv.org/pdf/2506.14237", "abs": "https://arxiv.org/abs/2506.14237", "authors": ["Xiyu Zhao", "Qimei Cui", "Wei Ni", "Quan Z. Sheng", "Abbas Jamalipour", "Guoshun Nan", "Xiaofeng Tao", "Ping Zhang"], "title": "A Novel Indicator for Quantifying and Minimizing Information Utility Loss of Robot Teams", "categories": ["cs.DC", "cs.RO"], "comment": null, "summary": "The timely exchange of information among robots within a team is vital, but\nit can be constrained by limited wireless capacity. The inability to deliver\ninformation promptly can result in estimation errors that impact collaborative\nefforts among robots. In this paper, we propose a new metric termed Loss of\nInformation Utility (LoIU) to quantify the freshness and utility of information\ncritical for cooperation. The metric enables robots to prioritize information\ntransmissions within bandwidth constraints. We also propose the estimation of\nLoIU using belief distributions and accordingly optimize both transmission\nschedule and resource allocation strategy for device-to-device transmissions to\nminimize the time-average LoIU within a robot team. A semi-decentralized\nMulti-Agent Deep Deterministic Policy Gradient framework is developed, where\neach robot functions as an actor responsible for scheduling transmissions among\nits collaborators while a central critic periodically evaluates and refines the\nactors in response to mobility and interference. Simulations validate the\neffectiveness of our approach, demonstrating an enhancement of information\nfreshness and utility by 98%, compared to alternative methods."}
{"id": "2506.14564", "pdf": "https://arxiv.org/pdf/2506.14564", "abs": "https://arxiv.org/abs/2506.14564", "authors": ["Lukas Geis", "Alexander Leonhardt", "Johannes Meintrup", "Ulrich Meyer", "Manuel Penschuck"], "title": "Simpler, Better, Faster, Stronger: Revisiting a Successful Reduction Rule for Dominating Set", "categories": ["cs.DS"], "comment": null, "summary": "DominatingSet is a classical NP-complete problem and also known to be\nW[2]-hard. Thus, there is little hope for small kernels on general graphs.\nHowever, in practice, reduction rules to heuristically shrink instances are\nused. In this context, Rule1 by Alber et. al. is quite successful, yet at times\nsomewhat expensive to execute. We propose a linear time algorithm implementing\nand surpassing the original Rule1 formulation. Our discussions and proofs yield\ninteresting structural insights into the reduction rule and its interplay with\nthe DominatingSet problem. For instance, while the original formulation\nwarrants repeated invocations of an $\\mathcal{O}(n^3)$ time algorithm, we\nrecast it to allow a single search run in linear time. Then, we propose simple,\nbut practically significant, extensions to our algorithmic framework to prune\nthe graph even further. The algorithm is easy to implement and highly\npractical."}
{"id": "2506.14208", "pdf": "https://arxiv.org/pdf/2506.14208", "abs": "https://arxiv.org/abs/2506.14208", "authors": ["Cui Zhang", "Maoxin Ji", "Qiong Wu", "Pingyi Fan", "Qiang Fan"], "title": "Optimizing System Latency for Blockchain-Encrypted Edge Computing in Internet of Vehicles", "categories": ["cs.NI"], "comment": "This paper has been accepted by Computers, Materials & Continua", "summary": "As Internet of Vehicles (IoV) technology continues to advance, edge computing\nhas become an important tool for assisting vehicles in handling complex tasks.\nHowever, the process of offloading tasks to edge servers may expose vehicles to\nmalicious external attacks, resulting in information loss or even tampering,\nthereby creating serious security vulnerabilities. Blockchain technology can\nmaintain a shared ledger among servers. In the Raft consensus mechanism, as\nlong as more than half of the nodes remain operational, the system will not\ncollapse, effectively maintaining the system's robustness and security. To\nprotect vehicle information, we propose a security framework that integrates\nthe Raft consensus mechanism from blockchain technology with edge computing. To\naddress the additional latency introduced by blockchain, we derived a\ntheoretical formula for system delay and proposed a convex optimization\nsolution to minimize the system latency, ensuring that the system meets the\nrequirements for low latency and high reliability. Simulation results\ndemonstrate that the optimized data extraction rate significantly reduces\nsystem delay, with relatively stable variations in latency. Moreover, the\nproposed optimization solution based on this model can provide valuable\ninsights for enhancing security and efficiency in future network environments,\nsuch as 5G and next-generation smart city systems."}
{"id": "2506.13820", "pdf": "https://arxiv.org/pdf/2506.13820", "abs": "https://arxiv.org/abs/2506.13820", "authors": ["Shraddha Surana", "Ashwin Srinivasan", "Michael Bain"], "title": "Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The IPARC Challenge, inspired by ARC, provides controlled program synthesis\ntasks over synthetic images to evaluate automatic program construction,\nfocusing on sequence, selection, and iteration. This set of 600 tasks has\nresisted automated solutions. This paper presents a structured inductive\nprogramming approach with LLMs that successfully solves tasks across all IPARC\ncategories. The controlled nature of IPARC reveals insights into LLM-based code\ngeneration, including the importance of prior structuring, LLMs' ability to aid\nstructuring (requiring human refinement), the need to freeze correct code, the\nefficiency of code reuse, and how LLM-generated code can spark human\ncreativity. These findings suggest valuable mechanisms for human-LLM\ncollaboration in tackling complex program synthesis."}
{"id": "2506.13786", "pdf": "https://arxiv.org/pdf/2506.13786", "abs": "https://arxiv.org/abs/2506.13786", "authors": ["Vuong M. Ngo", "Tran Quang Vinh", "Patricia Kearney", "Mark Roantree"], "title": "Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "17th International Conference on Computational Collective\n  Intelligence, LNAI, Springer, 11 pages", "summary": "Diabetes is a chronic metabolic disease characterized by elevated blood\nglucose levels, leading to complications like heart disease, kidney failure,\nand nerve damage. Accurate state-level predictions are vital for effective\nhealthcare planning and targeted interventions, but in many cases, data for\nnecessary analyses are incomplete. This study begins with a data engineering\nprocess to integrate diabetes-related datasets from 2011 to 2021 to create a\ncomprehensive feature set. We then introduce an enhanced bagging ensemble\nregression model (EBMBag+) for time series forecasting to predict diabetes\nprevalence across U.S. cities. Several baseline models, including SVMReg,\nBDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our\nEBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved\nthe best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an\nR2 of 0.9."}
{"id": "2506.14610", "pdf": "https://arxiv.org/pdf/2506.14610", "abs": "https://arxiv.org/abs/2506.14610", "authors": ["C. Nicole Avans", "Alfredo A. Correa", "Sayan Ghosh", "Matthias Schimek", "Joseph Schuchart", "Anthony Skjellum", "Evan D. Suggs", "Tim Niklas Uhl"], "title": "Concepts for designing modern C++ interfaces for MPI", "categories": ["cs.DC"], "comment": null, "summary": "Since the C++ bindings were deleted in 2008, the Message Passing Interface\n(MPI) community has revived efforts in building high-level modern C++\ninterfaces. Such interfaces are either built to serve specific scientific\napplication needs (with limited coverage to the underlying MPI\nfunctionalities), or as an exercise in general-purpose programming model\nbuilding, with the hope that bespoke interfaces can be broadly adopted to\nconstruct a variety of distributed-memory scientific applications. However,\nwith the advent of modern C++-based heterogeneous programming models, GPUs and\nwidespread Machine Learning (ML) usage in contemporary scientific computing,\nthe role of prospective community-standardized high-level C++ interfaces to MPI\nis evolving. The success of such an interface clearly will depend on providing\nrobust abstractions and features adhering to the generic programming principles\nthat underpin the C++ programming language, without compromising on either\nperformance and portability, the core principles upon which MPI was founded.\nHowever, there is a tension between idiomatic C++ handling of types and\nlifetimes, and, MPI's loose interpretation of object lifetimes/ownership and\ninsistence on maintaining global states.\n  Instead of proposing \"yet another\" high-level C++ interface to MPI,\noverlooking or providing partial solutions to work around the key issues\nconcerning the dissonance between MPI semantics and idiomatic C++, this paper\nfocuses on the three fundamental aspects of a high-level interface: type\nsystem, object lifetimes and communication buffers, also identifying\ninconsistencies in the MPI specification. Presumptive solutions can be\nunrefined, and we hope the broader MPI and C++ communities will engage with us\nin productive exchange of ideas and concerns."}
{"id": "2506.14734", "pdf": "https://arxiv.org/pdf/2506.14734", "abs": "https://arxiv.org/abs/2506.14734", "authors": ["Ruben Becker", "Davide Cenzato", "Travis Gagie", "Sung-Hwan Kim", "Ragnar Groot Koerkamp", "Giovanni Manzini", "Nicola Prezza"], "title": "Compressing Suffix Trees by Path Decompositions", "categories": ["cs.DS"], "comment": "preliminary incomplete draft. Many details missing!", "summary": "In classic suffix trees, path compression works by replacing unary suffix\ntrie paths with pairs of pointers to $T$, which must be available in the form\nof some random access oracle at query time. In this paper, we revisit path\ncompression and show that a more careful choice of pointers leads to a new\nelegant, simple, and remarkably efficient way to compress the suffix tree. We\nbegin by observing that an alternative way to path-compress the suffix trie of\n$T$ is to decompose it into a set of (disjoint) node-to-leaf paths and then\nrepresent each path as a pointer $i$ to one of the string's suffixes $T[i,n]$.\nAt this point, we show that the array $A$ of such indices $i$, sorted by the\ncolexicographic order of the corresponding text prefixes $T[1,i]$, possesses\nthe following properties: (i) it supports \\emph{cache-efficient} pattern\nmatching queries via simple binary search on $A$ and random access on $T$, and\n(ii) it contains a number of entries being proportional to the size of the\n\\emph{compressed text}. Of particular interest is the path decomposition given\nby the colexicographic rank of $T$'s prefixes. The resulting index is smaller\nand orders of magnitude faster than the $r$-index on the task of locating all\noccurrences of a query pattern."}
{"id": "2506.14221", "pdf": "https://arxiv.org/pdf/2506.14221", "abs": "https://arxiv.org/abs/2506.14221", "authors": ["Rujia Zou", "Haipeng Zhang", "Karthik Sundaresan", "Zhensheng Jia", "Suresh Subramaniam"], "title": "A Novel Dynamic Bandwidth Allocation Design for 100G Coherent Passive Optical Network", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "With the rapid advancements in coherent Passive Optical Network (PON)\ntechnologies featuring 100G and higher data rates, this paper addresses the\nurgent requirement for sophisticated simulation and MAC layer development\nwithin the domain of coherent Time Division Multiplexing (TDM) PON and coherent\nTime and Frequency Division Multiplexing (TFDM) PON networks. The ever-growing\ndemand for latency-sensitive services and expanding user populations in\nnext-generation 100G and beyond coherent PONs, underscores the crucial need for\nlow-latency bandwidth management and efficient Dynamic Bandwidth Allocation\n(DBA) mechanisms. In this paper, we present a pioneering analysis of two\nestablished DBAs from the perspective of temporal misalignments. Subsequently,\na novel DBA algorithm tailored for coherent PONs featuring 100 Gbps data rate\nand up to 512 end-users is introduced, named the Hybrid-Switch DBA. This\ninnovative approach allows for adaptive switching of the DBA scheme in response\nto real-time traffic conditions. To the best of our knowledge, this paper\nrepresents the first attempt to address the misalignment problem of DBA and\nproposes a novel DBA solution for both TDM- and TFDM-based coherent PON\nnetworks. This research significantly contributes to the development of\ncoherent TDM PON and coherent TFDM PON networks by enhancing the efficiency of\nbandwidth allocation and addressing the challenges associated with\nmisalignments in DBA mechanisms. As optical access networks continue to evolve\nto meet the ever-increasing demands of modern communication services, the\nHybrid-Switch DBA algorithm presented in this paper offers a promising solution\nfor optimizing network performance and accommodating latency-sensitive\napplications."}
{"id": "2506.13821", "pdf": "https://arxiv.org/pdf/2506.13821", "abs": "https://arxiv.org/abs/2506.13821", "authors": ["Giovanni Bernardi", "Adrian Francalanza", "Marco Peressotti", "Mohammad Reza Mousavi"], "title": "Role, cost, and complexity of software in the real-world: a case for formal methods", "categories": ["cs.SE", "cs.CY"], "comment": null, "summary": "In this chapter we outline the role that software has in modern society,\nalong with the staggering costs of poor software quality. To lay this bare, we\nrecall the costs of some of the major software failures that happened during\nthe last~$40$ years. We argue that these costs justify researching, studying\nand applying formal software verification and in particular program analysis.\nThis position is supported by successful industrial experiences."}
{"id": "2506.13828", "pdf": "https://arxiv.org/pdf/2506.13828", "abs": "https://arxiv.org/abs/2506.13828", "authors": ["Abdullah Burkan Bereketoglu"], "title": "Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles", "categories": ["cs.LG", "cs.SY", "eess.SY", "physics.acc-ph"], "comment": "6 pages, 5 figures, 5 algorithms", "summary": "We propose a hybrid meta-learning framework for forecasting and anomaly\ndetection in nonlinear dynamical systems characterized by nonstationary and\nstochastic behavior. The approach integrates a physics-inspired simulator that\ncaptures nonlinear growth-relaxation dynamics with random perturbations,\nrepresentative of many complex physical, industrial, and cyber-physical\nsystems. We use CNN-LSTM architectures for spatio-temporal feature extraction,\nVariational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation\nForests for residual-based outlier detection in addition to a Dual-Stage\nAttention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of\nthe generated simulation data. To create composite anomaly forecasts, these\nmodels are combined using a meta-learner that combines forecasting outputs,\nreconstruction errors, and residual scores. The hybrid ensemble performs better\nthan standalone models in anomaly localization, generalization, and robustness\nto nonlinear deviations, according to simulation-based experiments. The\nframework provides a broad, data-driven approach to early defect identification\nand predictive monitoring in nonlinear systems, which may be applied to a\nvariety of scenarios where complete physical models might not be accessible."}
{"id": "2506.14630", "pdf": "https://arxiv.org/pdf/2506.14630", "abs": "https://arxiv.org/abs/2506.14630", "authors": ["Rúben Adão", "Zhongjie Wu", "Changjun Zhou", "Oana Balmau", "João Paulo", "Ricardo Macedo"], "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)", "categories": ["cs.DC", "cs.DB"], "comment": "This is an extended version of the full paper to appear in VLDB 2025", "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."}
{"id": "2506.14042", "pdf": "https://arxiv.org/pdf/2506.14042", "abs": "https://arxiv.org/abs/2506.14042", "authors": ["Bernardo Subercaseaux"], "title": "Asymptotically Smaller Encodings for Graph Problems and Scheduling", "categories": ["cs.LO", "cs.AI", "cs.DS"], "comment": null, "summary": "We show how several graph problems (e.g., vertex-cover, independent-set,\n$k$-coloring) can be encoded into CNF using only $O(|V|^2 / \\lg |V|)$ many\nclauses, as opposed to the $\\Omega(|V|^2)$ constraints used by standard\nencodings. This somewhat surprising result is a simple consequence of a result\nof Erd\\H{o}s, Chung, and Spencer (1983) about biclique coverings of graphs, and\nopens theoretical avenues to understand the success of \"Bounded Variable\nAddition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally,\nwe show a novel encoding for independent sets in some dense interval graphs\nusing only $O(|V| \\lg |V|)$ clauses (the direct encoding uses $\\Omega(|V|^2)$),\nwhich we have successfully applied to a string-compression encoding posed by\nBannai et al. (2022). As a direct byproduct, we obtain a reduction in the\nencoding size of a scheduling problem posed by Mayank and Modal (2020) from\n$O(NMT^2)$ to $O(NMT + M T^2 \\lg T)$, where $N$ is the number of tasks, $T$ the\ntotal timespan, and $M$ the number of machines."}
{"id": "2506.14057", "pdf": "https://arxiv.org/pdf/2506.14057", "abs": "https://arxiv.org/abs/2506.14057", "authors": ["Yash Vekaria", "Yohan Beugin", "Shaoor Munir", "Gunes Acar", "Nataliia Bielova", "Steven Englehardt", "Umar Iqbal", "Alexandros Kapravelos", "Pierre Laperdrix", "Nick Nikiforakis", "Jason Polakis", "Franziska Roesner", "Zubair Shafiq", "Sebastian Zimmeck"], "title": "SoK: Advances and Open Problems in Web Tracking", "categories": ["cs.CR", "cs.CY", "cs.NI", "K.5.2; H.4.3; K.4.1; H.3.4; H.3.5; C.2.3; C.2.4; D.4.6"], "comment": "Extended Version is available at:\n  https://github.com/privacysandstorm/sok-advances-open-problems-web-tracking", "summary": "Web tracking is a pervasive and opaque practice that enables personalized\nadvertising, retargeting, and conversion tracking. Over time, it has evolved\ninto a sophisticated and invasive ecosystem, employing increasingly complex\ntechniques to monitor and profile users across the web. The research community\nhas a long track record of analyzing new web tracking techniques, designing and\nevaluating the effectiveness of countermeasures, and assessing compliance with\nprivacy regulations. Despite a substantial body of work on web tracking, the\nliterature remains fragmented across distinctly scoped studies, making it\ndifficult to identify overarching trends, connect new but related techniques,\nand identify research gaps in the field. Today, web tracking is undergoing a\nonce-in-a-generation transformation, driven by fundamental shifts in the\nadvertising industry, the adoption of anti-tracking countermeasures by\nbrowsers, and the growing enforcement of emerging privacy regulations. This\nSystematization of Knowledge (SoK) aims to consolidate and synthesize this\nwide-ranging research, offering a comprehensive overview of the technical\nmechanisms, countermeasures, and regulations that shape the modern and rapidly\nevolving web tracking landscape. This SoK also highlights open challenges and\noutlines directions for future research, aiming to serve as a unified reference\nand introductory material for researchers, practitioners, and policymakers\nalike."}
{"id": "2506.13824", "pdf": "https://arxiv.org/pdf/2506.13824", "abs": "https://arxiv.org/abs/2506.13824", "authors": ["Jinyang Huang", "Xiachong Feng", "Qiguang Chen", "Hanjie Zhao", "Zihui Cheng", "Jiesong Bai", "Jingxuan Zhou", "Min Li", "Libo Qin"], "title": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios", "categories": ["cs.SE", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Code debugging is a crucial task in software engineering, which attracts\nincreasing attention. While remarkable success has been made in the era of\nlarge language models (LLMs), current research still focuses on the simple\nno-library or single-library setting, ignoring the complex multi-library\nscenario in real-world applications. To address this limitation, we make the\nfirst attempt to introduce MLDebugging (Multi-Library Debugging), a\ncomprehensive benchmark designed to assess debugging challenges within\nmulti-library Python code. Specifically, MLDebugging encompasses 126 distinct\nPython libraries, covering a wide range of multi-library code issues,\ncategorized into seven distinct types. Furthermore, we conduct a thorough\nevaluation of MLDebugging using both mainstream open-source and closed-source\nLLMs and highlight that current LLMs still struggle to correctly perform code\ndebugging across multi-library scenarios. We hope this work can uncover the\npotential of LLMs in multi-library debugging scenario and offer insights for\nfuture research."}
{"id": "2506.13831", "pdf": "https://arxiv.org/pdf/2506.13831", "abs": "https://arxiv.org/abs/2506.13831", "authors": ["Jitian Zhao", "Chenghui Li", "Frederic Sala", "Karl Rohe"], "title": "Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept-based approaches, which aim to identify human-understandable concepts\nwithin a model's internal representations, are a promising method for\ninterpreting embeddings from deep neural network models, such as CLIP. While\nthese approaches help explain model behavior, current methods lack statistical\nrigor, making it challenging to validate identified concepts and compare\ndifferent techniques. To address this challenge, we introduce a hypothesis\ntesting framework that quantifies rotation-sensitive structures within the CLIP\nembedding space. Once such structures are identified, we propose a post-hoc\nconcept decomposition method. Unlike existing approaches, it offers theoretical\nguarantees that discovered concepts represent robust, reproducible patterns\n(rather than method-specific artifacts) and outperforms other techniques in\nterms of reconstruction error. Empirically, we demonstrate that our\nconcept-based decomposition algorithm effectively balances reconstruction\naccuracy with concept interpretability and helps mitigate spurious cues in\ndata. Applied to a popular spurious correlation dataset, our method yields a\n22.6% increase in worst-group accuracy after removing spurious background\nconcepts."}
{"id": "2506.14743", "pdf": "https://arxiv.org/pdf/2506.14743", "abs": "https://arxiv.org/abs/2506.14743", "authors": ["Sergio Iserte", "Iker Martín-Álvarez", "Krzysztof Rojek", "José I. Aliaga", "Maribel Castillo", "Weronika Folwarska", "Antonio J. Peña"], "title": "Resource Optimization with MPI Process Malleability for Dynamic Workloads in HPC Clusters", "categories": ["cs.DC"], "comment": null, "summary": "Dynamic resource management is essential for optimizing computational\nefficiency in modern high-performance computing (HPC) environments,\nparticularly as systems scale. While research has demonstrated the benefits of\nmalleability in resource management systems (RMS), the adoption of such\ntechniques in production environments remains limited due to challenges in\nstandardization, interoperability, and usability. Addressing these gaps, this\npaper extends our prior work on the Dynamic Management of Resources (DMR)\nframework, which provides a modular and user-friendly approach to dynamic\nresource allocation. Building upon the original DMRlib reconfiguration runtime,\nthis work integrates new methodology from the Malleability Module (MaM) of the\nProteo framework, further enhancing reconfiguration capabilities with new\nspawning strategies and data redistribution methods. In this paper, we explore\nnew malleability strategies in HPC dynamic workloads, such as merging MPI\ncommunicators and asynchronous reconfigurations, which offer new opportunities\nfor dramatically reducing memory overhead. The proposed enhancements are\nrigorously evaluated on a world-class supercomputer, demonstrating improved\nresource utilization and workload efficiency. Results show that dynamic\nresource management can reduce the workload completion time by 40% and increase\nthe resource utilization by over 20%, compared to static resource allocation."}
{"id": "2506.14081", "pdf": "https://arxiv.org/pdf/2506.14081", "abs": "https://arxiv.org/abs/2506.14081", "authors": ["Marco Bressan", "Julian Brinkmann", "Holger Dell", "Marc Roth", "Philip Wellnitz"], "title": "The Complexity of Counting Small Sub-Hypergraphs", "categories": ["cs.CC", "cs.DS"], "comment": null, "summary": "Subgraph counting is a fundamental and well-studied problem whose\ncomputational complexity is well understood. Quite surprisingly, the hypergraph\nversion of subgraph counting has been almost ignored. In this work, we address\nthis gap by investigating the most basic sub-hypergraph counting problem: given\na (small) hypergraph $H$ and a (large) hypergraph $G$, compute the number of\nsub-hypergraphs of $G$ isomorphic to $H$. Formally, for a family $\\mathcal{H}$\nof hypergraphs, let #Sub($\\mathcal{H}$) be the restriction of the problem to $H\n\\in \\mathcal{H}$; the induced variant #IndSub($\\mathcal{H}$) is defined\nanalogously. Our main contribution is a complete classification of the\ncomplexity of these problems. Assuming the Exponential Time Hypothesis, we\nprove that #Sub($\\mathcal{H}$) is fixed-parameter tractable if and only if\n$\\mathcal{H}$ has bounded fractional co-independent edge-cover number, a novel\ngraph parameter we introduce. Moreover, #IndSub($\\mathcal{H}$) is\nfixed-parameter tractable if and only if $\\mathcal{H}$ has bounded fractional\nedge-cover number. Both results subsume pre-existing results for graphs as\nspecial cases. We also show that the fixed-parameter tractable cases of\n#Sub($\\mathcal{H}$) and #IndSub($\\mathcal{H}$) are unlikely to be in polynomial\ntime, unless respectively #P = P and Graph Isomorphism $\\in$ P. This shows a\nseparation with the special case of graphs, where the fixed-parameter tractable\ncases are known to actually be in polynomial time."}
{"id": "2506.14323", "pdf": "https://arxiv.org/pdf/2506.14323", "abs": "https://arxiv.org/abs/2506.14323", "authors": ["Ting-Han Chen", "Jeroen van der Ham-de Vos"], "title": "Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale", "categories": ["cs.CR", "cs.NI"], "comment": "18 pages, 1 figure", "summary": "Security researchers are interested in security vulnerabilities, but these\nsecurity vulnerabilities create risks for stakeholders. Coordinated\nVulnerability Disclosure has been an accepted best practice for many years in\ndisclosing newly discovered vulnerabilities. This practice has mostly worked,\nbut it can become challenging when there are many different parties involved.\n  There has also been research into known vulnerabilities, using datasets or\nactive scans to discover how many machines are still vulnerable. The ethical\nguidelines suggest that researchers also make an effort to notify the owners of\nthese machines. We posit that this differs from vulnerability disclosure, but\nrather the practice of vulnerability notification. This practice has some\nsimilarities with vulnerability disclosure but should be distinguished from it,\nproviding other challenges and requiring a different approach.\n  Based on our earlier disclosure experience and on prior work documenting\ntheir disclosure and notification operations, we provide a meta-review on\nvulnerability disclosure and notification to observe the shifts in strategies\nin recent years. We assess how researchers initiated their messaging and\nexamine the outcomes. We then compile the best practices for the existing\ndisclosure guidelines and for notification operations."}
{"id": "2506.13832", "pdf": "https://arxiv.org/pdf/2506.13832", "abs": "https://arxiv.org/abs/2506.13832", "authors": ["Hongda Zhu", "Yiwen Zhang", "Bing Zhao", "Jingzhe Ding", "Siyao Liu", "Tong Liu", "Dandan Wang", "Yanan Liu", "Zhaojian Li"], "title": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in front-end code\ngeneration. However, existing benchmarks exhibit several critical limitations:\nmany tasks are overly simplistic, test cases often lack rigor, and end-to-end\nvalidation is absent. These issues hinder the accurate assessment of model\nperformance. To address these challenges, we present FrontendBench, a benchmark\nco-developed by humans and LLMs. FrontendBench categorizes tasks based on code\nfunctionality and incorporates interactive test scenarios, enabling a more\ncomprehensive and practical evaluation of front-end code generation\ncapabilities. The benchmark comprises 148 meticulously crafted prompt-test case\npairs spanning five levels of web components, from basic UI elements to complex\ninteractive features. Each task reflects realistic front-end development\nchallenges. Furthermore, we introduce an automatic evaluation framework that\nexecutes generated code within a sandbox environment and assesses outcomes\nusing predefined test scripts. This framework achieves a 90.54% agreement rate\nwith expert human evaluations, demonstrating high reliability. We benchmark\nseveral state-of-the-art LLMs on FrontendBench and observe substantial\nperformance disparities in handling real-world front-end tasks. These results\nhighlight FrontendBench as a reliable and scalable benchmark, supporting\nconsistent multimodal evaluation and providing a robust foundation for future\nresearch in front-end code generation. Our data and code will be released soon."}
{"id": "2506.13834", "pdf": "https://arxiv.org/pdf/2506.13834", "abs": "https://arxiv.org/abs/2506.13834", "authors": ["Zhao Wei", "Chin Chun Ooi", "Abhishek Gupta", "Jian Cheng Wong", "Pao-Hsiung Chiu", "Sheares Xue Wen Toh", "Yew-Soon Ong"], "title": "Evolvable Conditional Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents an evolvable conditional diffusion method such that\nblack-box, non-differentiable multi-physics models, as are common in domains\nlike computational fluid dynamics and electromagnetics, can be effectively used\nfor guiding the generative process to facilitate autonomous scientific\ndiscovery. We formulate the guidance as an optimization problem where one\noptimizes for a desired fitness function through updates to the descriptive\nstatistic for the denoising distribution, and derive an evolution-guided\napproach from first principles through the lens of probabilistic evolution.\nInterestingly, the final derived update algorithm is analogous to the update as\nper common gradient-based guided diffusion models, but without ever having to\ncompute any derivatives. We validate our proposed evolvable diffusion algorithm\nin two AI for Science scenarios: the automated design of fluidic topology and\nmeta-surface. Results demonstrate that this method effectively generates\ndesigns that better satisfy specific optimization objectives without reliance\non differentiable proxies, providing an effective means of guidance-based\ndiffusion that can capitalize on the wealth of black-box, non-differentiable\nmulti-physics numerical models common across Science."}
{"id": "2506.13935", "pdf": "https://arxiv.org/pdf/2506.13935", "abs": "https://arxiv.org/abs/2506.13935", "authors": ["Vishesh Kumar Tanwar", "Soumik Sarkar", "Asheesh K. Singh", "Sajal K. Das"], "title": "ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture", "categories": ["cs.LG", "cs.DC", "cs.ET"], "comment": null, "summary": "To empower precision agriculture through distributed machine learning (DML),\nsplit learning (SL) has emerged as a promising paradigm, partitioning deep\nneural networks (DNNs) between edge devices and servers to reduce computational\nburdens and preserve data privacy. However, conventional SL frameworks'\none-split-fits-all strategy is a critical limitation in agricultural ecosystems\nwhere edge insect monitoring devices exhibit vast heterogeneity in\ncomputational power, energy constraints, and connectivity. This leads to\nstraggler bottlenecks, inefficient resource utilization, and compromised model\nperformance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement\nlearning (RL)-driven framework that dynamically tailors DNN split points for\neach device, optimizing efficiency without sacrificing accuracy. Specifically,\na Q-learning agent acts as an adaptive orchestrator, balancing workloads and\nlatency thresholds across devices to mitigate computational starvation or\noverload. By framing split layer selection as a finite-state Markov decision\nprocess, ReinDSplit convergence ensures that highly constrained devices\ncontribute meaningfully to model training over time. Evaluated on three insect\nclassification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit\nachieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit\npioneers a paradigm shift in SL by harmonizing RL for resource efficiency,\nprivacy, and scalability in heterogeneous environments."}
{"id": "2506.13932", "pdf": "https://arxiv.org/pdf/2506.13932", "abs": "https://arxiv.org/abs/2506.13932", "authors": ["Ira Ceka", "Saurabh Pujar", "Irene Manotas", "Gail Kaiser", "Baishakhi Ray", "Shyam Ramji"], "title": "How Does LLM Reasoning Work for Code? A Survey and a Call to Action", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rise of large language models (LLMs) has led to dramatic improvements\nacross a wide range of natural language tasks. These advancements have extended\ninto the domain of code, facilitating complex tasks such as code generation,\ntranslation, summarization, and repair. However, their utility for real-world\ndeployment in-the-wild has only recently been studied, particularly on software\nengineering (SWE) tasks such as GitHub issue resolution. In this study, we\nexamine the code reasoning techniques that underlie the ability to perform such\ntasks, and examine the paradigms used to drive their performance. Our\ncontributions in this paper are: (1) the first dedicated survey on code\nreasoning for code tasks, highlighting overarching strategies, hybrid and\nagentic approaches; (2) a taxonomy of various techniques used to drive code\nreasoning; (3) a comprehensive overview of performance on common benchmarks and\na showcase of new, under-explored benchmarks with high potential in SWE; (4) an\nexploration on how core properties of code can be used to explain different\nreasoning techniques; and (5) gaps and potentially under-explored areas for\nfuture research."}
{"id": "2506.13836", "pdf": "https://arxiv.org/pdf/2506.13836", "abs": "https://arxiv.org/abs/2506.13836", "authors": ["Dang Viet Anh Nguyen", "Carlos Lima Azevedo", "Tomer Toledo", "Filipe Rodrigues"], "title": "Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study", "categories": ["cs.LG", "cs.AI", "90B20, 90C39, 68T05, 62P30", "I.2.6; I.2.8; G.3; J.4"], "comment": "35 pages, 5 figures, 3 tables", "summary": "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a\npromising approach for improving urban mobility. However, its robustness under\nreal-world disruptions such as traffic incidents remains largely underexplored.\nIn this study, we introduce T-REX, an open-source, SUMO-based simulation\nframework for training and evaluating RL-TSC methods under dynamic, incident\nscenarios. T-REX models realistic network-level performance considering\ndrivers' probabilistic rerouting, speed adaptation, and contextual\nlane-changing, enabling the simulation of congestion propagation under\nincidents. To assess robustness, we propose a suite of metrics that extend\nbeyond conventional traffic efficiency measures. Through extensive experiments\nacross synthetic and real-world networks, we showcase T-REX for the evaluation\nof several state-of-the-art RL-TSC methods under multiple real-world deployment\nparadigms. Our findings show that while independent value-based and\ndecentralized pressure-based methods offer fast convergence and generalization\nin stable traffic conditions and homogeneous networks, their performance\ndegrades sharply under incident-driven distribution shifts. In contrast,\nhierarchical coordination methods tend to offer more stable and adaptable\nperformance in large-scale, irregular networks, benefiting from their\nstructured decision-making architecture. However, this comes with the trade-off\nof slower convergence and higher training complexity. These findings highlight\nthe need for robustness-aware design and evaluation in RL-TSC research. T-REX\ncontributes to this effort by providing an open, standardized and reproducible\nplatform for benchmarking RL methods under dynamic and disruptive traffic\nscenarios."}
{"id": "2506.14251", "pdf": "https://arxiv.org/pdf/2506.14251", "abs": "https://arxiv.org/abs/2506.14251", "authors": ["Xiyu Zhao", "Qimei Cui", "Weicai Li", "Wei Ni", "Ekram Hossain", "Quan Z. Sheng", "Xiaofeng Tao", "Ping Zhang"], "title": "Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a\nbalance between personalization and generalization by conducting federated\nlearning (FL) to guide personalized learning (PL). While FL is unaffected by\npersonalized model training, in Ditto, PL depends on the outcome of the FL.\nHowever, the clients' concern about their privacy and consequent perturbation\nof their local models can affect the convergence and (performance) fairness of\nPL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension\nof Ditto under the protection of differential privacy (DP), and analyzes the\ntrade-off among its privacy guarantee, model convergence, and performance\ndistribution fairness. We also analyze the convergence upper bound of the\npersonalized models under DP-Ditto and derive the optimal number of global\naggregations given a privacy budget. Further, we analyze the performance\nfairness of the personalized models, and reveal the feasibility of optimizing\nDP-Ditto jointly for convergence and fairness. Experiments validate our\nanalysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of\nthe state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by\nover 32.71% in fairness and 9.66% in accuracy."}
{"id": "2506.13977", "pdf": "https://arxiv.org/pdf/2506.13977", "abs": "https://arxiv.org/abs/2506.13977", "authors": ["Shiting Huang", "Zhen Fang", "Zehui Chen", "Siyu Yuan", "Junjie Ye", "Yu Zeng", "Lin Chen", "Qi Mao", "Feng Zhao"], "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\n\\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}."}
{"id": "2506.13838", "pdf": "https://arxiv.org/pdf/2506.13838", "abs": "https://arxiv.org/abs/2506.13838", "authors": ["Lorena Poenaru-Olaru", "June Sallou", "Luis Cruz", "Jan Rellermeyer", "Arie van Deursen"], "title": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "12 pages. Accepted at ICT4Sustainability 2025 conference", "summary": "The reliability of machine learning (ML) software systems is heavily\ninfluenced by changes in data over time. For that reason, ML systems require\nregular maintenance, typically based on model retraining. However, retraining\nrequires significant computational demand, which makes it energy-intensive and\nraises concerns about its environmental impact. To understand which retraining\ntechniques should be considered when designing sustainable ML applications, in\nthis work, we study the energy consumption of common retraining techniques.\nSince the accuracy of ML systems is also essential, we compare retraining\ntechniques in terms of both energy efficiency and accuracy. We showcase that\nretraining with only the most recent data, compared to all available data,\nreduces energy consumption by up to 25\\%, being a sustainable alternative to\nthe status quo. Furthermore, our findings show that retraining a model only\nwhen there is evidence that updates are necessary, rather than on a fixed\nschedule, can reduce energy consumption by up to 40\\%, provided a reliable data\nchange detector is in place. Our findings pave the way for better\nrecommendations for ML practitioners, guiding them toward more energy-efficient\nretraining techniques when designing sustainable ML software systems."}
{"id": "2506.14393", "pdf": "https://arxiv.org/pdf/2506.14393", "abs": "https://arxiv.org/abs/2506.14393", "authors": ["Kamil Tylinski", "Abylay Satybaldy", "Paolo Tasca"], "title": "Consensus Power Inequality: A Comparative Study of Blockchain Networks", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "The distribution of consensus power is a cornerstone of decentralisation,\ninfluencing the security, resilience, and fairness of blockchain networks while\nensuring equitable impact among participants. This study provides a rigorous\nevaluation of consensus power inequality across five prominent blockchain\nnetworks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data\ncollected from January 2022 to July 2024. Leveraging established economic\nmetrics, including the Gini coefficient and Theil index, the research\nquantitatively assesses how power is distributed among blockchain network\nparticipants. A robust dataset, capturing network-specific characteristics such\nas mining pools, staking patterns, and consensus nodes, forms the foundation of\nthe analysis, enabling meaningful comparisons across diverse architectures.\nThrough an in-depth comparative study, the paper identifies key disparities in\nconsensus power distribution. Hedera and Bitcoin demonstrate more balanced\npower distribution, aligning closely with the principles of decentralisation.\nEthereum and Cardano demonstrate moderate levels of inequality. However,\ncontrary to expectations, Ethereum has become more concentrated following its\ntransition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced\ncentralisation of power. Moreover, the findings highlight the structural and\noperational drivers of inequality, including economic barriers, governance\nmodels, and network effects, offering actionable insights for more equitable\nnetwork design. This study establishes a methodological framework for\nevaluating blockchain consensus power inequality, emphasising the importance of\ntargeted strategies to ensure fairer power distribution and enhancing the\nsustainability of decentralised systems. Future research will build on these\nfindings by integrating additional metrics and examining the influence of\nemerging consensus mechanisms."}
{"id": "2506.14055", "pdf": "https://arxiv.org/pdf/2506.14055", "abs": "https://arxiv.org/abs/2506.14055", "authors": ["Yutian Tang", "Hongchen Cao", "Yuxi Chen", "David Lo"], "title": "Characterising Bugs in Jupyter Platform", "categories": ["cs.SE"], "comment": null, "summary": "As a representative literate programming platform, Jupyter is widely adopted\nby developers, data analysts, and researchers for replication, data sharing,\ndocumentation, interactive data visualization, and more. Understanding the bugs\nin the Jupyter platform is essential for ensuring its correctness, security,\nand robustness. Previous studies focused on code reuse, restoration, and repair\nexecution environment for Jupyter notebooks. However, the bugs in Jupyter\nnotebooks' hosting platform Jupyter are not investigated. In this paper, we\ninvestigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified\ninto 11 root causes and 11 bug symptoms. We identify 14 major findings for\ndevelopers. More importantly, our study opens new directions in building tools\nfor detecting and fixing bugs in the Jupyter platform."}
{"id": "2506.13842", "pdf": "https://arxiv.org/pdf/2506.13842", "abs": "https://arxiv.org/abs/2506.13842", "authors": ["Yuanlong Wang", "Pengqi Wang", "Changchang Yin", "Ping Zhang"], "title": "SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors", "categories": ["cs.LG", "I.2.6; J.3; E.0"], "comment": "18 pages, 6 figures. To be published in SIGKDD 2025 Datasets and\n  Benchmarks Track", "summary": "Living environments play a vital role in the prevalence and progression of\ndiseases, and understanding their impact on patient's health status becomes\nincreasingly crucial for developing AI models. However, due to the lack of\nlong-term and fine-grained spatial and temporal data in public and population\nhealth studies, most existing studies fail to incorporate environmental data,\nlimiting the models' performance and real-world application. To address this\nshortage, we developed SatHealth, a novel dataset combining multimodal\nspatiotemporal data, including environmental data, satellite images,\nall-disease prevalences estimated from medical claims, and social determinants\nof health (SDoH) indicators. We conducted experiments under two use cases with\nSatHealth: regional public health modeling and personal disease risk\nprediction. Experimental results show that living environmental information can\nsignificantly improve AI models' performance and temporal-spatial\ngeneralizability on various tasks. Finally, we deploy a web-based application\nto provide an exploration tool for SatHealth and one-click access to both our\ndata and regional environmental embedding to facilitate plug-and-play\nutilization. SatHealth is now published with data in Ohio, and we will keep\nupdating SatHealth to cover the other parts of the US. With the web application\nand published code pipeline, our work provides valuable angles and resources to\ninclude environmental data in healthcare research and establishes a\nfoundational framework for future research in environmental health informatics."}
{"id": "2506.14718", "pdf": "https://arxiv.org/pdf/2506.14718", "abs": "https://arxiv.org/abs/2506.14718", "authors": ["Eric J. Korpela", "David P. Anderson", "Jeff Cobb", "Matt Lebofsky", "Wei Liu", "Dan Werthimer"], "title": "SETI@home: Data Acquisition and Front-End Processing", "categories": ["astro-ph.IM", "cs.DC", "physics.data-an"], "comment": "21 pages, 7 figures, 5 tables. Accepted to AJ", "summary": "SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project,\nlooking for technosignatures in data recorded at multiple observatories from\n1998 to 2020. Most radio SETI projects analyze data using dedicated processing\nhardware. SETI@home uses a different approach: time-domain data is distributed\nover the Internet to $\\gt 10^{5}$ volunteered home computers, which analyze it.\nThe large amount of computing power this affords ($\\sim 10^{15}$ floating-point\noperations per second (FPOP/s)) allows us to increase the sensitivity and\ngenerality of our search in three ways. We use coherent integration, a\ntechnique in which data is transformed so that the power of drifting signals is\nconfined to a single discrete Fourier transform (DFT) bin. We perform this\ncoherent search over 123 000 Doppler drift rates in the range ($\\pm$100 Hz\ns$^{-1}$). Second, we search for a variety of signal types, such as pulsed\nsignals and arbitrary repeated waveforms. The analysis uses a range of DFT\nsizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front\nend of SETI@home produces a set of detections that exceed thresholds in power\nand goodness of fit. We accumulated $\\sim 1.2\\times 10^{10}$ such detections.\nThe back end of SETI@home takes these detections, identifies and removes radio\nfrequency interference (RFI), and looks for groups of detections that are\nconsistent with extraterrestrial origin and that persist over long timescales.\nThis paper describes the front end of SETI@home and provides parameters for the\nprimary data source, the Arecibo Observatory; the back end and its results are\ndescribed in a companion paper."}
{"id": "2506.14129", "pdf": "https://arxiv.org/pdf/2506.14129", "abs": "https://arxiv.org/abs/2506.14129", "authors": ["Shuchang Wang", "Xiaopeng Qiu", "Yingxing Xue", "Yanfu Li", "Wei Yang"], "title": "A Quantum Annealing Approach for Solving Optimal Feature Selection and Next Release Problems", "categories": ["cs.SE", "quant-ph"], "comment": null, "summary": "Search-based software engineering (SBSE) addresses critical optimization\nchallenges in software engineering, including the next release problem (NRP)\nand feature selection problem (FSP). While traditional heuristic approaches and\ninteger linear programming (ILP) methods have demonstrated efficacy for small\nto medium-scale problems, their scalability to large-scale instances remains\nunknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling\nmulti-objective SBSE problems, leveraging the computational potential of\nquantum systems. We propose two QA-based algorithms tailored to different\nproblem scales. For small-scale problems, we reformulate multi-objective\noptimization (MOO) as single-objective optimization (SOO) using penalty-based\nmappings for quantum processing. For large-scale problems, we employ a\ndecomposition strategy guided by maximum energy impact (MEI), integrating QA\nwith a steepest descent method to enhance local search efficiency. Applied to\nNRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and\nthe ILP-based $\\epsilon$-constraint method. Experimental results reveal that\nwhile our methods produce fewer non-dominated solutions than\n$\\epsilon$-constraint, they achieve significant reductions in execution time.\nMoreover, compared to NSGA-II, our methods deliver more non-dominated solutions\nwith superior computational efficiency. These findings underscore the potential\nof QA in advancing scalable and efficient solutions for SBSE challenges."}
{"id": "2506.13862", "pdf": "https://arxiv.org/pdf/2506.13862", "abs": "https://arxiv.org/abs/2506.13862", "authors": ["Alena Shilova", "Alex Davey", "Brahim Driss", "Riad Akrour"], "title": "StaQ it! Growing neural networks for Policy Mirror Descent", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages, 12 figures", "summary": "In Reinforcement Learning (RL), regularization has emerged as a popular tool\nboth in theory and practice, typically based either on an entropy bonus or a\nKullback-Leibler divergence that constrains successive policies. In practice,\nthese approaches have been shown to improve exploration, robustness and\nstability, giving rise to popular Deep RL algorithms such as SAC and TRPO.\nPolicy Mirror Descent (PMD) is a theoretical framework that solves this general\nregularized policy optimization problem, however the closed-form solution\ninvolves the sum of all past Q-functions, which is intractable in practice. We\npropose and analyze PMD-like algorithms that only keep the last $M$ Q-functions\nin memory, and show that for finite and large enough $M$, a convergent\nalgorithm can be derived, introducing no error in the policy update, unlike\nprior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong\ntheoretical guarantees and is competitive with deep RL baselines, while\nexhibiting less performance oscillation, paving the way for fully stable deep\nRL algorithms and providing a testbed for experimentation with Policy Mirror\nDescent."}
{"id": "2506.14192", "pdf": "https://arxiv.org/pdf/2506.14192", "abs": "https://arxiv.org/abs/2506.14192", "authors": ["Shristi Shrestha", "Anas Mahmoud"], "title": "Mobile Application Review Summarization using Chain of Density Prompting", "categories": ["cs.SE"], "comment": "30 pages, 11 Figures, Automated Software Engineering Journal", "summary": "Mobile app users commonly rely on app store ratings and reviews to find apps\nthat suit their needs. However, the sheer volume of reviews available on app\nstores can lead to information overload, thus impeding users' ability to make\ninformed app selection decisions. To address this challenge, we leverage Large\nLanguage Models (LLMs) to summarize mobile app reviews. In particular, we use\nthe Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate\nabstractive, semantically dense, and easily interpretable summaries of mobile\napp reviews. The CoD prompt is engineered to iteratively extract salient\nentities from the source text and fuse them into a fixed-length summary. We\nevaluate the performance of our approach using a large dataset of mobile app\nreviews. We further conduct an empirical evaluation with 48 study participants\nto assess the readability of the generated summaries. Our results demonstrate\nthat adapting the CoD prompt to focus on app features improves its ability to\nextract key themes from user reviews and generate natural language summaries\ntailored for end-user consumption. The prompt also manages to maintain the\nreadability of the generated summaries while increasing their semantic density.\nOur work in this paper aims to improve mobile app users' experience by\nproviding an effective mechanism for summarizing important user feedback in the\nreview stream."}
{"id": "2506.13892", "pdf": "https://arxiv.org/pdf/2506.13892", "abs": "https://arxiv.org/abs/2506.13892", "authors": ["Samuel Beaussant", "Mehdi Mounsif"], "title": "Scaling Algorithm Distillation for Continuous Control with Mamba", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Algorithm Distillation (AD) was recently proposed as a new approach to\nperform In-Context Reinforcement Learning (ICRL) by modeling across-episodic\ntraining histories autoregressively with a causal transformer model. However,\ndue to practical limitations induced by the attention mechanism, experiments\nwere bottlenecked by the transformer's quadratic complexity and limited to\nsimple discrete environments with short time horizons. In this work, we propose\nleveraging the recently proposed Selective Structured State Space Sequence (S6)\nmodels, which achieved state-of-the-art (SOTA) performance on long-range\nsequence modeling while scaling linearly in sequence length. Through four\ncomplex and continuous Meta Reinforcement Learning environments, we demonstrate\nthe overall superiority of Mamba, a model built with S6 layers, over a\ntransformer model for AD. Additionally, we show that scaling AD to very long\ncontexts can improve ICRL performance and make it competitive even with a SOTA\nonline meta RL baseline."}
{"id": "2506.14232", "pdf": "https://arxiv.org/pdf/2506.14232", "abs": "https://arxiv.org/abs/2506.14232", "authors": ["Sonja M. Hyrynsalmi", "Mary Sanchez-Gordon", "Anna Szlavi", "Letizia Jaccheri"], "title": "The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity, and Inclusion in Software Engineering", "categories": ["cs.SE"], "comment": "FSE Companion '25", "summary": "Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top\npriority for leading software companies. However, in a short period, a wave of\nbacklash has led many firms to re-assess their DEI strategies. Responding to\nthis DEI backlash is crucial in academic research, especially because,\ncurrently, little scholarly research has been done on it. In this paper,\ntherefore, we have set forth the following research question (RQ): \"How have\nleading software companies changed their DEI strategies in recent years?\" Given\nthe novelty of the RQ and, consequently, the lack of scholarly research on it,\nwe are conducting a grey literature study, examining the current state of DEI\ninitiatives in 10 leading software companies. Based on our analysis, we have\nclassified companies into categories based on their shift in commitment to DEI.\nWe can identify that companies are indeed responding to the backlash by\nrethinking their strategy, either by reducing, increasing, or renaming their\nDEI initiatives. In contrast, some companies keep on with their DEI strategy,\nat least so far, despite the challenging political climate. To illustrate these\nchanges, we introduce the DEI Universe Map, a visual representation of software\nindustry trends in DEI commitment and actions."}
{"id": "2506.13903", "pdf": "https://arxiv.org/pdf/2506.13903", "abs": "https://arxiv.org/abs/2506.13903", "authors": ["Christel Sirocchi", "Damiano Verda"], "title": "Enhancing interpretability of rule-based classifiers through feature graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In domains where transparency and trustworthiness are crucial, such as\nhealthcare, rule-based systems are widely used and often preferred over\nblack-box models for decision support systems due to their inherent\ninterpretability. However, as rule-based models grow complex, discerning\ncrucial features, understanding their interactions, and comparing feature\ncontributions across different rule sets becomes challenging. To address this,\nwe propose a comprehensive framework for estimating feature contributions in\nrule-based systems, introducing a graph-based feature visualisation strategy, a\nnovel feature importance metric agnostic to rule-based predictors, and a\ndistance metric for comparing rule sets based on feature contributions. By\nexperimenting on two clinical datasets and four rule-based methods (decision\ntrees, logic learning machines, association rules, and neural networks with\nrule extraction), we showcase our method's capability to uncover novel insights\non the combined predictive value of clinical features, both at the dataset and\nclass-specific levels. These insights can aid in identifying new risk factors,\nsignature genes, and potential biomarkers, and determining the subset of\npatient information that should be prioritised to enhance diagnostic accuracy.\nComparative analysis of the proposed feature importance score with\nstate-of-the-art methods on 15 public benchmarks demonstrates competitive\nperformance and superior robustness. The method implementation is available on\nGitHub: https://github.com/ChristelSirocchi/rule-graph."}
{"id": "2506.14281", "pdf": "https://arxiv.org/pdf/2506.14281", "abs": "https://arxiv.org/abs/2506.14281", "authors": ["Ethem Utku Aktas", "Burak Tuzlutas", "Burak Yesiltas"], "title": "Designing a Custom Chaos Engineering Framework for Enhanced System Resilience at Softtech", "categories": ["cs.SE"], "comment": "EASE 2025 - Industry papers (4 pages)", "summary": "Chaos Engineering is a discipline which enhances software resilience by\nintroducing faults to observe and improve system behavior intentionally. This\npaper presents a design proposal for a customized Chaos Engineering framework\ntailored for Softtech, a leading software development company serving the\nfinancial sector. It outlines foundational concepts and activities for\nintroducing Chaos Engineering within Softtech, while considering financial\nsector regulations. Building on these principles, the framework aims to be\niterative and scalable, enabling development teams to progressively improve\ntheir practices. The study addresses two primary questions: how Softtech's\nunique infrastructure, business priorities, and organizational context shape\nthe customization of its Chaos Engineering framework and what key activities\nand components are necessary for creating an effective framework tailored to\nSofttech's needs."}
{"id": "2506.13906", "pdf": "https://arxiv.org/pdf/2506.13906", "abs": "https://arxiv.org/abs/2506.13906", "authors": ["Milad Ramezankhani", "Janak M. Patel", "Anirudh Deodhar", "Dagnachew Birru"], "title": "GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations", "categories": ["cs.LG"], "comment": null, "summary": "We present a novel graph-informed transformer operator (GITO) architecture\nfor learning complex partial differential equation systems defined on irregular\ngeometries and non-uniform meshes. GITO consists of two main modules: a hybrid\ngraph transformer (HGT) and a transformer neural operator (TNO). HGT leverages\na graph neural network (GNN) to encode local spatial relationships and a\ntransformer to capture long-range dependencies. A self-attention fusion layer\nintegrates the outputs of the GNN and transformer to enable more expressive\nfeature learning on graph-structured data. TNO module employs linear-complexity\ncross-attention and self-attention layers to map encoded input functions to\npredictions at arbitrary query locations, ensuring discretization invariance\nand enabling zero-shot super-resolution across any mesh. Empirical results on\nbenchmark PDE tasks demonstrate that GITO outperforms existing\ntransformer-based neural operators, paving the way for efficient, mesh-agnostic\nsurrogate solvers in engineering applications."}
{"id": "2506.14290", "pdf": "https://arxiv.org/pdf/2506.14290", "abs": "https://arxiv.org/abs/2506.14290", "authors": ["Daniele La Prova", "Emanuele Gentili", "Davide Falessi"], "title": "Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity Effects", "categories": ["cs.SE"], "comment": null, "summary": "The primary goal of bug prediction is to optimize testing efforts by focusing\non software fragments, i.e., classes, methods, commits (JIT), or lines of code,\nmost likely to be buggy. However, these predicted fragments already contain\nbugs. Thus, the current bug prediction approaches support fixing rather than\nprevention. The aim of this paper is to introduce and evaluate Ticket-Level\nPrediction (TLP), an approach to identify tickets that will introduce bugs once\nimplemented. We analyze TLP at three temporal points, each point represents a\nticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1)\nTLP accuracy increases as tickets progress towards the closed stage due to\nimproved feature reliability over time, and (2) the predictive power of\nfeatures changes across these temporal points. Our TLP approach leverages 72\nfeatures belonging to six different families: code, developer, external\ntemperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our\nTLP evaluation uses a sliding-window approach, balancing feature selection and\nthree machine-learning bug prediction classifiers on about 10,000 tickets of\ntwo Apache open-source projects. Our results show that TLP accuracy increases\nwith proximity, confirming the expected trade-off between early prediction and\naccuracy. Regarding the prediction power of feature families, no single feature\nfamily dominates across stages; developer-centric signals are most informative\nearly, whereas code and JIT metrics prevail near closure, and temperature-based\nfeatures provide complementary value throughout. Our findings complement and\nextend the literature on bug prediction at the class, method, or commit level\nby showing that defect prediction can be effectively moved upstream, offering\nopportunities for risk-aware ticket triaging and developer assignment before\nany code is written."}
{"id": "2506.13909", "pdf": "https://arxiv.org/pdf/2506.13909", "abs": "https://arxiv.org/abs/2506.13909", "authors": ["Xinyuan Tu", "Haocheng Zhang", "Tao Chengxu", "Zuyi Chen"], "title": "Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Few-shot learning (FSL) has shown promise in vision but remains largely\nunexplored for \\emph{industrial} time-series data, where annotating every new\ndefect is prohibitively expensive. We present a systematic FSL study on\nscrew-fastening process monitoring, using a 2\\,300-sample multivariate torque\ndataset that covers 16 uni- and multi-factorial defect types. Beyond\nbenchmarking, we introduce a \\textbf{label-aware episodic sampler} that\ncollapses multi-label sequences into multiple single-label tasks, keeping the\noutput dimensionality fixed while preserving combinatorial label information.\n  Two FSL paradigms are investigated: the metric-based \\emph{Prototypical\nNetwork} and the gradient-based \\emph{Model-Agnostic Meta-Learning} (MAML),\neach paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter\ntransformer \\emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime +\nPrototypical Network combination achieves a \\textbf{0.944 weighted F1} in the\nmulti-class regime and \\textbf{0.935} in the multi-label regime, outperforming\nfinetuned Moment by up to 5.3\\% while requiring two orders of magnitude fewer\nparameters and training time. Across all backbones, metric learning\nconsistently surpasses MAML, and our label-aware sampling yields an additional\n1.7\\% F1 over traditional class-based sampling.\n  These findings challenge the assumption that large foundation models are\nalways superior: when data are scarce, lightweight CNN architectures augmented\nwith simple metric learning not only converge faster but also generalize\nbetter. We release code, data splits and pre-trained weights to foster\nreproducible research and to catalyze the adoption of FSL in high-value\nmanufacturing inspection."}
{"id": "2506.14297", "pdf": "https://arxiv.org/pdf/2506.14297", "abs": "https://arxiv.org/abs/2506.14297", "authors": ["Victor Alves", "Carla Bezerra", "Ivan Machado", "Larissa Rocha", "Tássio Virgínio", "Publio Silva"], "title": "Quality Assessment of Python Tests Generated by Large Language Models", "categories": ["cs.SE"], "comment": "International Conference on Evaluation and Assessment in Software\n  Engineering (EASE), 2025 edition", "summary": "The manual generation of test scripts is a time-intensive, costly, and\nerror-prone process, indicating the value of automated solutions. Large\nLanguage Models (LLMs) have shown great promise in this domain, leveraging\ntheir extensive knowledge to produce test code more efficiently. This study\ninvestigates the quality of Python test code generated by three LLMs: GPT-4o,\nAmazon Q, and LLama 3.3. We evaluate the structural reliability of test suites\ngenerated under two distinct prompt contexts: Text2Code (T2C) and Code2Code\n(C2C). Our analysis includes the identification of errors and test smells, with\na focus on correlating these issues to inadequate design patterns. Our findings\nreveal that most test suites generated by the LLMs contained at least one error\nor test smell. Assertion errors were the most common, comprising 64% of all\nidentified errors, while the test smell Lack of Cohesion of Test Cases was the\nmost frequently detected (41%). Prompt context significantly influenced test\nquality; textual prompts with detailed instructions often yielded tests with\nfewer errors but a higher incidence of test smells. Among the evaluated LLMs,\nGPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C),\nwhereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For\ntest smells, Amazon Q had fewer detections in the C2C context (9%), while LLama\n3.3 performed best in the T2C context (10%). Additionally, we observed a strong\nrelationship between specific errors, such as assertion or indentation issues,\nand test case cohesion smells. These findings demonstrate opportunities for\nimproving the quality of test generation by LLMs and highlight the need for\nfuture research to explore optimized generation scenarios and better prompt\nengineering strategies."}
{"id": "2506.13911", "pdf": "https://arxiv.org/pdf/2506.13911", "abs": "https://arxiv.org/abs/2506.13911", "authors": ["Arie Soeteman", "Balder ten Cate"], "title": "Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization", "categories": ["cs.LG", "cs.AI", "cs.LO", "I.2.6; F.2.0"], "comment": "Submitted to NeurIPS 2025, 28 pages, 5 figures", "summary": "We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an\nexpressive extension of graph neural networks (GNNs) with hierarchical node\nindividualization, inspired by the Individualization-Refinement paradigm for\ngraph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy\nof increasingly expressive models that, in the limit, can distinguish graphs up\nto isomorphism. We provide a logical characterization of HEGNN node\nclassifiers, with and without subgraph restrictions, using graded hybrid logic.\nThis characterization enables us to relate the separating power of HEGNNs to\nthat of higher-order GNNs, GNNs enriched with local homomorphism count\nfeatures, and color refinement algorithms based on\nIndividualization-Refinement. Our experimental results confirm the practical\nfeasibility of HEGNNs and show benefits in comparison with traditional GNN\narchitectures, both with and without local homomorphism count features."}
{"id": "2506.14369", "pdf": "https://arxiv.org/pdf/2506.14369", "abs": "https://arxiv.org/abs/2506.14369", "authors": ["Maria Spichkova"], "title": "Agile and Student-Centred Teaching of Agile/Scrum Concepts", "categories": ["cs.SE"], "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "summary": "In this paper, we discuss our experience in designing and teaching a course\non Software Engineering Project Management, where the focus is on Agile/Scrum\ndevelopment and Requirement Engineering activities. The course has undergone\nfundamental changes since 2020 to make the teaching approach more\nstudent-centred and flexible. As many universities abandoned having\nface-to-face exams at the end of the semester, authentic assessments now play\nan even more important role than before. This makes assessment of students'\nwork even more challenging, especially if we are dealing with large cohorts of\nstudents. The complexity is not only in dealing with diversity in the student\ncohorts when elaborating the assessment tasks, but also in being able to\nprovide feedback and marks in a timely and fairly. We report our lessons\nlearned, which might provide useful insights for teaching Agile/Scrum concepts\nto undergraduate and postgraduate students. We also analyse what course\nstructure might be effective to support a blended learning approach, as well as\nwhat could be a reasonable structure of online assessments, to keep them both\nauthentic and scalable for large cohorts of students."}
{"id": "2506.13916", "pdf": "https://arxiv.org/pdf/2506.13916", "abs": "https://arxiv.org/abs/2506.13916", "authors": ["Isaias Banales", "Arturo Jaramillo", "Heli Ricalde Guerrero"], "title": "Branching Stein Variational Gradient Descent for sampling multimodal distributions", "categories": ["cs.LG", "stat.CO", "62F15, 65C05, 65C35"], "comment": null, "summary": "We propose a novel particle-based variational inference method designed to\nwork with multimodal distributions. Our approach, referred to as Branched Stein\nVariational Gradient Descent (BSVGD), extends the classical Stein Variational\nGradient Descent (SVGD) algorithm by incorporating a random branching mechanism\nthat encourages the exploration of the state space. In this work, a theoretical\nguarantee for the convergence in distribution is presented, as well as\nnumerical experiments to validate the suitability of our algorithm. Performance\ncomparisons between the BSVGD and the SVGD are presented using the Wasserstein\ndistance between samples and the corresponding computational times."}
{"id": "2506.14409", "pdf": "https://arxiv.org/pdf/2506.14409", "abs": "https://arxiv.org/abs/2506.14409", "authors": ["Rafael C. Lopes", "Danilo M. Ribeiro"], "title": "Defining the Game Producer: A Mapping of Key Characteristics and Differentiators of the Professional Behind Digital Game Production", "categories": ["cs.SE"], "comment": null, "summary": "Introduction: As digital games grow in complexity, the role of the Game\nProducer becomes increasingly relevant for aligning creative, technical, and\nbusiness dimensions. Objective: This study aimed to identify and map the main\ncharacteristics, skills, and competencies that define the Digital Game Producer\nprofile. Methodology: A qualitative investigation was conducted with 11\nsemi-structured interviews, analyzed through Grounded Theory to build\ncategories grounded in professional practice. Results: The study produced a\nstructured set of personal characteristics, practical skills, and strategic\ncompetencies considered essential for Game Producers. Communication,\nadaptability, and project management emerged as central elements across the\nsample. Conclusion: The resulting model offers a foundation for professional\ntraining, recruitment strategies, and future research on leadership roles in\ngame development."}
{"id": "2506.13923", "pdf": "https://arxiv.org/pdf/2506.13923", "abs": "https://arxiv.org/abs/2506.13923", "authors": ["Vaskar Nath", "Elaine Lau", "Anisha Gunjal", "Manasi Sharma", "Nikhil Baharte", "Sean Hendryx"], "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study the process through which reasoning models trained with\nreinforcement learning on verifiable rewards (RLVR) can learn to solve new\nproblems. We find that RLVR drives performance through two main means: (1) by\ncompressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models\nlearn to solve new problems that they previously could not solve even at high\n$k$. We find that while capability gain exists across model scales, learning to\nsolve new problems is primarily driven through self-distillation. We\ndemonstrate these findings across model scales ranging from 0.5B to 72B on\n>500,000 reasoning problems with prompts and verifiable final answers across\nmath, science, and code domains. We further show that we can significantly\nimprove pass@$k$ rates by leveraging natural language guidance for the model to\nconsider within context while still requiring the model to derive a solution\nchain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new\nclass of online training algorithms. $\\text{Guide}$ adaptively incorporates\nhints into the model's context on problems for which all rollouts were\ninitially incorrect and adjusts the importance sampling ratio for the\n\"off-policy\" trajectories in order to optimize the policy for contexts in which\nthe hints are no longer present. We describe variants of $\\text{Guide}$ for\nGRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter\nmodels improves generalization over its vanilla counterpart with up to 4$\\%$\nmacro-average improvement across math benchmarks. We include careful ablations\nto analyze $\\text{Guide}$'s components and theoretically analyze Guide's\nlearning efficiency."}
{"id": "2506.14535", "pdf": "https://arxiv.org/pdf/2506.14535", "abs": "https://arxiv.org/abs/2506.14535", "authors": ["José Manuel Suárez", "Luis Mariano Bibbó", "Joaquin Bogado", "Alejandro Fernandez"], "title": "Automatic Qiskit Code Refactoring Using Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "Submitted for review to \"Taller Latinoamericano de Ingenier\\'ia de\n  Software Cu\\'antico\" (https://www.ripaisc.net/call-for-papers-tlisc-2025/)", "summary": "As quantum software frameworks evolve, developers face increasing challenges\nin maintaining compatibility with rapidly changing APIs. In this work, we\npresent a novel methodology for refactoring Qiskit code using large language\nmodels (LLMs). We begin by extracting a taxonomy of migration scenarios from\nthe different sources of official Qiskit documentation (such as release notes),\ncapturing common patterns such as migration of functionality to different\nmodules and deprecated usage. This taxonomy, along with the original Python\nsource code, is provided as input to an LLM, which is then tasked with\nidentifying instances of migration scenarios in the code and suggesting\nappropriate refactoring solutions. Our approach is designed to address the\ncontext length limitations of current LLMs by structuring the input and\nreasoning process in a targeted, efficient manner. The results demonstrate that\nLLMs, when guided by domain-specific migration knowledge, can effectively\nassist in automating Qiskit code migration. This work contributes both a set of\nproven prompts and taxonomy for Qiskit code migration from earlier versions to\nversion 0.46 and a methodology to asses the capabilities of LLMs to assist in\nthe migration of quantum code."}
{"id": "2506.13935", "pdf": "https://arxiv.org/pdf/2506.13935", "abs": "https://arxiv.org/abs/2506.13935", "authors": ["Vishesh Kumar Tanwar", "Soumik Sarkar", "Asheesh K. Singh", "Sajal K. Das"], "title": "ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture", "categories": ["cs.LG", "cs.DC", "cs.ET"], "comment": null, "summary": "To empower precision agriculture through distributed machine learning (DML),\nsplit learning (SL) has emerged as a promising paradigm, partitioning deep\nneural networks (DNNs) between edge devices and servers to reduce computational\nburdens and preserve data privacy. However, conventional SL frameworks'\none-split-fits-all strategy is a critical limitation in agricultural ecosystems\nwhere edge insect monitoring devices exhibit vast heterogeneity in\ncomputational power, energy constraints, and connectivity. This leads to\nstraggler bottlenecks, inefficient resource utilization, and compromised model\nperformance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement\nlearning (RL)-driven framework that dynamically tailors DNN split points for\neach device, optimizing efficiency without sacrificing accuracy. Specifically,\na Q-learning agent acts as an adaptive orchestrator, balancing workloads and\nlatency thresholds across devices to mitigate computational starvation or\noverload. By framing split layer selection as a finite-state Markov decision\nprocess, ReinDSplit convergence ensures that highly constrained devices\ncontribute meaningfully to model training over time. Evaluated on three insect\nclassification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit\nachieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit\npioneers a paradigm shift in SL by harmonizing RL for resource efficiency,\nprivacy, and scalability in heterogeneous environments."}
{"id": "2506.14623", "pdf": "https://arxiv.org/pdf/2506.14623", "abs": "https://arxiv.org/abs/2506.14623", "authors": ["Aaron Conrardy", "Armen Sulejmani", "Cindy Guerlain", "Daniele Pagani", "David Hick", "Matteo Satta", "Jordi Cabot"], "title": "Low-code to fight climate change: the Climaborough project", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "This paper was presented in the Research Projects Track of the 19th\n  International Conference on Research Challenges in Information Science (RCIS\n  2025)", "summary": "The EU-funded Climaborough project supports European cities to achieve carbon\nneutrality by 2030. Eleven cities in nine countries will deploy in real\nconditions products and services fostering climate transition in their local\nenvironment. The Climaborough City Platform is being developed to monitor the\ncities' overall progress towards their climate goals by aggregating historic\nand real-time data and displaying the results in user-friendly dashboards that\nwill be used by non-technical experts to evaluate the effectiveness of local\nexperimental initiatives, identify those that yield significant impact, and\nassess the potential consequences of scaling them up to a broader level. In\nthis paper, we explain how we have put in place a low-code/no-code strategy in\nClimaborough in response to the project's aim to quickly deploy climate\ndashboards. A low-code strategy is used to accelerate the development of the\ndashboards. The dashboards embed a no-code philosophy that enables all types of\ncitizen profiles to configure and adapt the dashboard to their specific needs."}
{"id": "2506.13958", "pdf": "https://arxiv.org/pdf/2506.13958", "abs": "https://arxiv.org/abs/2506.13958", "authors": ["Leonardo Guiducci", "Antonio Rizzo", "Giovanna Maria Dimitri"], "title": "Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Elastic Decision Transformers (EDTs) have proved to be particularly\nsuccessful in offline reinforcement learning, offering a flexible framework\nthat unifies sequence modeling with decision-making under uncertainty. Recent\nresearch has shown that incorporating intrinsic motivation mechanisms into EDTs\nimproves performance across exploration tasks, yet the representational\nmechanisms underlying these improvements remain unexplored. In this paper, we\nintroduce a systematic post-hoc explainability framework to analyze how\nintrinsic motivation shapes learned embeddings in EDTs. Through statistical\nanalysis of embedding properties (including covariance structure, vector\nmagnitudes, and orthogonality), we reveal that different intrinsic motivation\nvariants create fundamentally different representational structures. Our\nanalysis demonstrates environment-specific correlation patterns between\nembedding metrics and performance that explain why intrinsic motivation\nimproves policy learning. These findings show that intrinsic motivation\noperates beyond simple exploration bonuses, acting as a representational prior\nthat shapes embedding geometry in biologically plausible ways, creating\nenvironment-specific organizational structures that facilitate better\ndecision-making."}
{"id": "2506.14627", "pdf": "https://arxiv.org/pdf/2506.14627", "abs": "https://arxiv.org/abs/2506.14627", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "ACM Survey Draft on Formalising Software Requirements with Large Language Models", "categories": ["cs.SE", "cs.AI", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "comment": "22 pages. 6 summary tables", "summary": "This draft is a working document, having a summary of nighty-four (94) papers\nwith additional sections on Traceability of Software Requirements (Section 4),\nFormal Methods and Its Tools (Section 5), Unifying Theories of Programming\n(UTP) and Theory of Institutions (Section 6). Please refer to abstract of\n[7,8]. Key difference of this draft from our recently anticipated ones with\nsimilar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:\n  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted\non 18th of March, 2025, it went through the light-weight blind review and\naccepted for poster presentation. Conference was held on 15th of May, 2025.\n  [8] is a nine page paper with additional nine pages of references and summary\ntables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April,\n2025. It went through rigorous review process. The uploaded version on\narXiv.org [8] is the improved one of the submission, after addressing the\nspecific suggestions to improve the paper."}
{"id": "2506.13972", "pdf": "https://arxiv.org/pdf/2506.13972", "abs": "https://arxiv.org/abs/2506.13972", "authors": ["Zhiqi Wang", "Chengyu Zhang", "Yuetian Chen", "Nathalie Baracaldo", "Swanand Kadhe", "Lei Yu"], "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble", "categories": ["cs.LG"], "comment": null, "summary": "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation."}
{"id": "2506.14640", "pdf": "https://arxiv.org/pdf/2506.14640", "abs": "https://arxiv.org/abs/2506.14640", "authors": ["Ina K. Schieferdecker"], "title": "Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey", "categories": ["cs.SE", "cs.AI", "D.2.5"], "comment": "15 pages, 7 figures, 1 table, 2 listings (will be presented at FMICS\n  2025)", "summary": "In industry, software testing is the primary method to verify and validate\nthe functionality, performance, security, usability, and so on, of\nsoftware-based systems. Test automation has gained increasing attention in\nindustry over the last decade, following decades of intense research into test\nautomation and model-based testing. However, designing, developing, maintaining\nand evolving test automation is a considerable effort. Meanwhile, AI's\nbreakthroughs in many engineering fields are opening up new perspectives for\nsoftware testing, for both manual and automated testing. This paper reviews\nrecent research on AI augmentation in software test automation, from no\nautomation to full automation. It also discusses new forms of testing made\npossible by AI. Based on this, the newly developed taxonomy, ai4st, is\npresented and used to classify recent research and identify open research\nquestions."}
{"id": "2506.13974", "pdf": "https://arxiv.org/pdf/2506.13974", "abs": "https://arxiv.org/abs/2506.13974", "authors": ["Michael Crawshaw", "Blake Woodworth", "Mingrui Liu"], "title": "Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous\nobjectives requires stepsizes $\\eta \\leq 1/K$ where $K$ is the communication\ninterval, which ensures monotonic decrease of the objective. In contrast, we\nanalyze Local Gradient Descent for logistic regression with separable,\nheterogeneous data using any stepsize $\\eta > 0$. With $R$ communication rounds\nand $M$ clients, we show convergence at a rate $\\mathcal{O}(1/\\eta K R)$ after\nan initial unstable phase lasting for $\\widetilde{\\mathcal{O}}(\\eta K M)$\nrounds. This improves upon the existing $\\mathcal{O}(1/R)$ rate for general\nsmooth, convex objectives. Our analysis parallels the single machine analysis\nof~\\cite{wu2024large} in which instability is caused by extremely large\nstepsizes, but in our setting another source of instability is large local\nupdates with heterogeneous objectives."}
{"id": "2506.14649", "pdf": "https://arxiv.org/pdf/2506.14649", "abs": "https://arxiv.org/abs/2506.14649", "authors": ["Yanzhen Zou", "Xianlin Zhao", "Xinglu Pan", "Bing Xie"], "title": "Issue Retrieval and Verification Enhanced Supplementary Code Comment Generation", "categories": ["cs.SE"], "comment": "12 pages, 8 figures", "summary": "Issue reports have been recognized to contain rich information for\nretrieval-augmented code comment generation. However, how to minimize\nhallucinations in the generated comments remains significant challenges. In\nthis paper, we propose IsComment, an issue-based LLM retrieval and verification\napproach for generating method's design rationale, usage directives, and so on\nas supplementary code comments. We first identify five main types of code\nsupplementary information that issue reports can provide through\ncode-comment-issue analysis. Next, we retrieve issue sentences containing these\ntypes of supplementary information and generate candidate code comments. To\nreduce hallucinations, we filter out those candidate comments that are\nirrelevant to the code or unverifiable by the issue report, making the code\ncomment generation results more reliable. Our experiments indicate that\ncompared with LLMs, IsComment increases the coverage of manual supplementary\ncomments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and\nfrom 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can\ngenerate richer and more useful supplementary code comments for programming\nunderstanding, which is quantitatively evaluated through the MESIA metric on\nboth methods with and without manual code comments."}
{"id": "2506.13981", "pdf": "https://arxiv.org/pdf/2506.13981", "abs": "https://arxiv.org/abs/2506.13981", "authors": ["Thanh Dan Bui"], "title": "HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.4; I.6.5; J.4"], "comment": null, "summary": "High-frequency stock price prediction is challenging due to non-stationarity,\nnoise, and volatility. To tackle these issues, we propose the Hybrid Attentive\nEnsemble Learning Transformer (HAELT), a deep learning framework combining a\nResNet-based noise-mitigation module, temporal self-attention for dynamic focus\non relevant history, and a hybrid LSTM-Transformer core that captures both\nlocal and long-range dependencies. These components are adaptively ensembled\nbased on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from\nJan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set,\neffectively identifying both upward and downward price movements. This\ndemonstrates HAELT's potential for robust, practical financial forecasting and\nalgorithmic trading."}
{"id": "2506.14683", "pdf": "https://arxiv.org/pdf/2506.14683", "abs": "https://arxiv.org/abs/2506.14683", "authors": ["Leonhard Applis", "Yuntong Zhang", "Shanchao Liang", "Nan Jiang", "Lin Tan", "Abhik Roychoudhury"], "title": "Unified Software Engineering agent as AI Software Engineer", "categories": ["cs.SE", "cs.AI"], "comment": "Leonhard Applis and Yuntong Zhang contributed equally to this work", "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future."}
{"id": "2506.13987", "pdf": "https://arxiv.org/pdf/2506.13987", "abs": "https://arxiv.org/abs/2506.13987", "authors": ["Md Abrar Jahin", "Adiba Abid", "M. F. Mridha"], "title": "Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems", "categories": ["cs.LG"], "comment": null, "summary": "Expert systems often operate in domains characterized by class-imbalanced\ntabular data, where detecting rare but critical instances is essential for\nsafety and reliability. While conventional approaches, such as cost-sensitive\nlearning, oversampling, and graph neural networks, provide partial solutions,\nthey suffer from drawbacks like overfitting, label noise, and poor\ngeneralization in low-density regions. To address these challenges, we propose\nQCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented\nwith k-nearest neighbor (kNN) guided dynamic mixup for robust classification\nunder imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum\nEntanglement-inspired layer that models complex feature interactions through\nsinusoidal transformations and gated attention, (ii) a sample-aware mixup\nstrategy that adaptively interpolates feature representations of semantically\nsimilar instances to enhance minority class representation, and (iii) a hybrid\nloss function that unifies focal reweighting, supervised contrastive learning,\ntriplet margin loss, and variance regularization to improve both intra-class\ncompactness and inter-class separability. Extensive experiments on 18\nreal-world imbalanced datasets (binary and multi-class) demonstrate that\nQCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep\nlearning, and GNN-based baselines in macro-F1 and recall, often by substantial\nmargins. Ablation studies further validate the critical role of each\narchitectural component. Our results establish QCL-MixNet as a new benchmark\nfor tabular imbalance handling in expert systems. Theoretical analyses\nreinforce its expressiveness, generalization, and optimization robustness."}
{"id": "2506.13817", "pdf": "https://arxiv.org/pdf/2506.13817", "abs": "https://arxiv.org/abs/2506.13817", "authors": ["Saleem A. Al Dajani", "Abel Sanchez", "John R. Williams"], "title": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG", "cs.SE", "q-bio.QM"], "comment": "4 pages, 5 figures, Accepted by ICML 2025 FM4LS\n  https://openreview.net/forum?id=zNjXOZxEYB . Workshop on Multi-modal\n  Foundation Models and Large Language Models for Life Sciences (FM4LS)}, July\n  2025", "summary": "Generative AI foundation models offer transformative potential for processing\nstructured biological data, particularly in single-cell RNA sequencing, where\ndatasets are rapidly scaling toward billions of cells. We propose the use of\nagentic foundation models with real-time web search to automate the labeling of\nexperimental data, achieving up to 82.5% accuracy. This addresses a key\nbottleneck in supervised learning for structured omics data by increasing\nannotation throughput without manual curation and human error. Our approach\nenables the development of virtual cell foundation models capable of downstream\ntasks such as cell-typing and perturbation prediction. As data volume grows,\nthese models may surpass human performance in labeling, paving the way for\nreliable inference in large-scale perturbation screens. This application\ndemonstrates domain-specific innovation in health monitoring and diagnostics,\naligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network."}
{"id": "2506.13992", "pdf": "https://arxiv.org/pdf/2506.13992", "abs": "https://arxiv.org/abs/2506.13992", "authors": ["An Luo", "Xun Xian", "Jin Du", "Fangqiao Tian", "Ganghua Wang", "Ming Zhong", "Shengchun Zhao", "Xuan Bi", "Zirui Liu", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "62-07, 62-08, 68T05, 68T07, 68T01, 68T50", "I.2.0; I.2.6; I.2.7; I.5.1; I.5.4; H.2.8; G.3"], "comment": null, "summary": "Large language models (LLMs) have advanced the automation of data science\nworkflows. Yet it remains unclear whether they can critically leverage external\ndomain knowledge as human data scientists do in practice. To answer this\nquestion, we introduce AssistedDS (Assisted Data Science), a benchmark designed\nto systematically evaluate how LLMs handle domain knowledge in tabular\nprediction tasks. AssistedDS features both synthetic datasets with explicitly\nknown generative mechanisms and real-world Kaggle competitions, each\naccompanied by curated bundles of helpful and adversarial documents. These\ndocuments provide domain-specific insights into data cleaning, feature\nengineering, and model selection. We assess state-of-the-art LLMs on their\nability to discern and apply beneficial versus harmful domain knowledge,\nevaluating submission validity, information recall, and predictive performance.\nOur results demonstrate three key findings: (1) LLMs frequently exhibit an\nuncritical adoption of provided information, significantly impairing their\npredictive performance when adversarial content is introduced, (2) helpful\nguidance is often insufficient to counteract the negative influence of\nadversarial information, and (3) in Kaggle datasets, LLMs often make errors in\nhandling time-series data, applying consistent feature engineering across\ndifferent folds, and interpreting categorical variables correctly. These\nfindings highlight a substantial gap in current models' ability to critically\nevaluate and leverage expert knowledge, underscoring an essential research\ndirection for developing more robust, knowledge-aware automated data science\nsystems."}
{"id": "2506.13838", "pdf": "https://arxiv.org/pdf/2506.13838", "abs": "https://arxiv.org/abs/2506.13838", "authors": ["Lorena Poenaru-Olaru", "June Sallou", "Luis Cruz", "Jan Rellermeyer", "Arie van Deursen"], "title": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "12 pages. Accepted at ICT4Sustainability 2025 conference", "summary": "The reliability of machine learning (ML) software systems is heavily\ninfluenced by changes in data over time. For that reason, ML systems require\nregular maintenance, typically based on model retraining. However, retraining\nrequires significant computational demand, which makes it energy-intensive and\nraises concerns about its environmental impact. To understand which retraining\ntechniques should be considered when designing sustainable ML applications, in\nthis work, we study the energy consumption of common retraining techniques.\nSince the accuracy of ML systems is also essential, we compare retraining\ntechniques in terms of both energy efficiency and accuracy. We showcase that\nretraining with only the most recent data, compared to all available data,\nreduces energy consumption by up to 25\\%, being a sustainable alternative to\nthe status quo. Furthermore, our findings show that retraining a model only\nwhen there is evidence that updates are necessary, rather than on a fixed\nschedule, can reduce energy consumption by up to 40\\%, provided a reliable data\nchange detector is in place. Our findings pave the way for better\nrecommendations for ML practitioners, guiding them toward more energy-efficient\nretraining techniques when designing sustainable ML software systems."}
{"id": "2506.13996", "pdf": "https://arxiv.org/pdf/2506.13996", "abs": "https://arxiv.org/abs/2506.13996", "authors": ["Stas Bekman", "Samyam Rajbhandari", "Michael Wyatt", "Jeff Rasley", "Tunji Ruwase", "Zhewei Yao", "Aurick Qiao", "Yuxiong He"], "title": "Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences", "categories": ["cs.LG"], "comment": "19 pages, 13 figures", "summary": "Long sequences are critical for applications like RAG, long document\nsummarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout,\nsupport max sequence length of up to 10 million tokens. However, outside of\nenterprise labs, long sequence training is challenging for the AI community\nwith limited system support in the open-source space.\n  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B\nmodel with sequence over 32K runs out of memory on a basic Hugging Face (HF)\nmodel due to two reasons: i) LLM training workloads are not optimized to fully\nleverage a single GPU memory, ii) existing solutions for leveraging multiple\nGPU memory are not easily available to HF models, making long sequence training\ninaccessible.\n  We address this with Arctic Long Sequence Training (ALST). It offers a\ncombination of attention-agnostic single GPU and multi-GPU memory\noptimizations, that enables it to support out-of-box training of multi-million\nsequence length for a wide variety of HF models.\n  ALST supports training Meta's Llama 8B model with 500K sequence length on a\nsingle H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node\ncluster, an increase of over 400x compared to the 32K baseline for the latter.\nALST is fully compatible with HF models and open-sourced via Deepspeed\nhttps://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic\nTraining\nhttps://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md."}
{"id": "2506.13933", "pdf": "https://arxiv.org/pdf/2506.13933", "abs": "https://arxiv.org/abs/2506.13933", "authors": ["Tobias Kerbl", "David Brecht", "Nils Gehrke", "Nijinshan Karunainayagam", "Niklas Krauss", "Florian Pfab", "Richard Taupitz", "Ines Trautmannsheimer", "Xiyan Su", "Maria-Magdalena Wolf", "Frank Diermeyer"], "title": "TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles", "categories": ["cs.RO", "cs.HC", "cs.SE"], "comment": null, "summary": "Teleoperation is a key enabler for future mobility, supporting Automated\nVehicles in rare and complex scenarios beyond the capabilities of their\nautomation. Despite ongoing research, no open source software currently\ncombines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance\nthrough high-level interaction with automated driving software modules, and\nintegration with a real-world vehicle for practical testing. To address this\ngap, we present a modular, open source teleoperation software stack that can\ninteract with an automated driving software, e.g., Autoware, enabling Remote\nAssistance and Remote Driving. The software featuresstandardized interfaces for\nseamless integration with various real-world and simulation platforms, while\nallowing for flexible design of the human-machine interface. The system is\ndesigned for modularity and ease of extension, serving as a foundation for\ncollaborative development on individual software components as well as\nrealistic testing and user studies. To demonstrate the applicability of our\nsoftware, we evaluated the latency and performance of different vehicle\nplatforms in simulation and real-world. The source code is available on GitHub"}
{"id": "2506.14002", "pdf": "https://arxiv.org/pdf/2506.14002", "abs": "https://arxiv.org/abs/2506.14002", "authors": ["Siyu Chen", "Heejune Sheen", "Xuyuan Xiong", "Tianhao Wang", "Zhuoran Yang"], "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML", "I.2.6"], "comment": "136 pages, 21 figures", "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically \\highlight{prove that this\nalgorithm correctly recovers all monosemantic features} when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\n\\highlight{demonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters}. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability."}
{"id": "2506.14426", "pdf": "https://arxiv.org/pdf/2506.14426", "abs": "https://arxiv.org/abs/2506.14426", "authors": ["Matt Luckcuck", "Angelo Ferrando", "Fatma Faruq"], "title": "Varanus: Runtime Verification for CSP", "categories": ["cs.LO", "cs.SE"], "comment": "Accepted at Towards Autonomous Robotic Systems (TAROS) 2025", "summary": "Autonomous systems are often used in changeable and unknown environments,\nwhere traditional verification may not be suitable. Runtime Verification (RV)\nchecks events performed by a system against a formal specification of its\nintended behaviour, making it highly suitable for ensuring that an autonomous\nsystem is obeying its specification at runtime. Communicating Sequential\nProcesses (CSP) is a process algebra usually used in static verification, which\ncaptures behaviour as a trace of events, making it useful for RV as well.\nFurther, CSP has more recently been used to specify autonomous and robotic\nsystems. Though CSP is supported by two extant model checkers, so far it has no\nRV tool. This paper presents Varanus, an RV tool that monitors a system against\nan oracle built from a CSP specification. This approach enables the reuse\nwithout modifications of a specification that was built, e.g during the\nsystem's design. We describe the tool, apply it to a simulated autonomous\nrobotic rover inspecting a nuclear waste store, empirically comparing its\nperformance to two other RV tools using different languages, and demonstrate\nhow it can detect violations of the specification. Varanus can synthesise a\nmonitor from a CSP process in roughly linear time, with respect to the number\nof states and transitions in the model; and checks each event in roughly\nconstant time."}
{"id": "2506.14003", "pdf": "https://arxiv.org/pdf/2506.14003", "abs": "https://arxiv.org/abs/2506.14003", "authors": ["Yiwei Chen", "Soumyadeep Pal", "Yimeng Zhang", "Qing Qu", "Sijia Liu"], "title": "Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs", "categories": ["cs.LG"], "comment": null, "summary": "Machine unlearning (MU) for large language models (LLMs), commonly referred\nto as LLM unlearning, seeks to remove specific undesirable data or knowledge\nfrom a trained model, while maintaining its performance on standard tasks.\nWhile unlearning plays a vital role in protecting data privacy, enforcing\ncopyright, and mitigating sociotechnical harms in LLMs, we identify a new\nvulnerability post-unlearning: unlearning trace detection. We discover that\nunlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces\nin both model behavior and internal representations. These traces can be\nidentified from output responses, even when prompted with forget-irrelevant\ninputs. Specifically, a simple supervised classifier can reliably determine\nwhether a model has undergone unlearning based solely on its textual outputs.\nFurther analysis shows that these traces are embedded in intermediate\nactivations and propagate nonlinearly to the final layer, forming\nlow-dimensional, learnable manifolds in activation space. Through extensive\nexperiments, we show that forget-relevant prompts enable over 90% accuracy in\ndetecting unlearning traces across all model sizes. Even with forget-irrelevant\ninputs, large LLMs maintain high detectability, demonstrating the broad\napplicability of unlearning trace detection. These findings reveal that\nunlearning leaves measurable signatures, introducing a new risk of\nreverse-engineering forgotten information when a model is identified as\nunlearned given an input query. Codes are available at [this\nURL](https://github.com/OPTML-Group/Unlearn-Trace)."}
{"id": "2506.14470", "pdf": "https://arxiv.org/pdf/2506.14470", "abs": "https://arxiv.org/abs/2506.14470", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "As one of the most detrimental code smells, code clones significantly\nincrease software maintenance costs and heighten vulnerability risks, making\ntheir detection a critical challenge in software engineering. Abstract Syntax\nTrees (ASTs) dominate deep learning-based code clone detection due to their\nprecise syntactic structure representation, but they inherently lack semantic\ndepth. Recent studies address this by enriching AST-based representations with\nsemantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs\n(DFGs). However, the effectiveness of various enriched AST-based\nrepresentations and their compatibility with different graph-based machine\nlearning techniques remains an open question, warranting further investigation\nto unlock their full potential in addressing the complexities of code clone\ndetection. In this paper, we present a comprehensive empirical study to\nrigorously evaluate the effectiveness of AST-based hybrid graph representations\nin Graph Neural Network (GNN)-based code clone detection. We systematically\ncompare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs\n(FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid\nrepresentations impact GNNs differently: while AST+CFG+DFG consistently\nenhances accuracy for convolution- and attention-based models (Graph\nConvolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST\nfrequently introduces structural complexity that harms performance. Notably,\nGMN outperforms others even with standard AST representations, highlighting its\nsuperior cross-code similarity detection and reducing the need for enriched\nstructures."}
{"id": "2506.14020", "pdf": "https://arxiv.org/pdf/2506.14020", "abs": "https://arxiv.org/abs/2506.14020", "authors": ["Keyue Jiang", "Jiahao Cui", "Xiaowen Dong", "Laura Toni"], "title": "Bures-Wasserstein Flow Matching for Graph Generation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Graph generation has emerged as a critical task in fields ranging from\nmolecule design to drug discovery. Contemporary approaches, notably diffusion\nand flow-based models, have achieved solid graph generative performance through\nconstructing a probability path that interpolates between a reference\ndistribution and the data distribution. However, these methods typically model\nthe evolution of individual nodes and edges independently and use linear\ninterpolations to build the path assuming that the data lie in Euclidean space.\nWe show that this is suboptimal given the intrinsic non-Euclidean structure and\ninterconnected patterns of graphs, and it poses risks to the sampling\nconvergence. To build a better probability path, we model the joint evolution\nof the nodes and edges by representing graphs as connected systems\nparameterized by Markov random fields (MRF). We then leverage the optimal\ntransport displacement between MRF objects to design the probability path for\ngraph generation. Based on this, we introduce BWFlow, a flow-matching framework\nfor graph generation that respects the underlying geometry of graphs and\nprovides smooth velocities in the probability path. The novel framework can be\nadapted to both continuous and discrete flow-matching algorithms. Experimental\nevaluations in plain graph generation and 2D/3D molecule generation validate\nthe effectiveness of BWFlow in graph generation with competitive performance,\nstable training, and guaranteed sampling convergence."}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch."}
{"id": "2506.14036", "pdf": "https://arxiv.org/pdf/2506.14036", "abs": "https://arxiv.org/abs/2506.14036", "authors": ["Tatthapong Srikitrungruang", "Sina Aghaee Dabaghan Fard", "Matthew Lemon", "Jaesung Lee", "Yuxiao Zhou"], "title": "Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data", "categories": ["cs.LG"], "comment": null, "summary": "Accurately estimating spatially heterogeneous elasticity parameters,\nparticularly Young's modulus and Poisson's ratio, from noisy displacement\nmeasurements remains significantly challenging in inverse elasticity problems.\nExisting inverse estimation techniques are often limited by instability,\npronounced sensitivity to measurement noise, and difficulty in recovering\nabsolute-scale Young's modulus. This work presents a novel Inverse Elasticity\nPhysics-Informed Neural Network (IE-PINN) specifically designed to robustly\nreconstruct heterogeneous distributions of elasticity parameters from noisy\ndisplacement data based on linear elasticity physics. IE-PINN integrates three\ndistinct neural network architectures dedicated to separately modeling\ndisplacement fields, strain fields, and elasticity distributions, thereby\nsignificantly enhancing stability and accuracy against measurement noise.\nAdditionally, a two-phase estimation strategy is introduced: the first phase\nrecovers relative spatial distributions of Young's modulus and Poisson's ratio,\nand the second phase calibrates the absolute scale of Young's modulus using\nimposed loading boundary conditions. Additional methodological innovations,\nincluding positional encoding, sine activation functions, and a sequential\npretraining protocol, further enhance the model's performance and robustness.\nExtensive numerical experiments demonstrate that IE-PINN effectively overcomes\ncritical limitations encountered by existing methods, delivering accurate\nabsolute-scale elasticity estimations even under severe noise conditions. This\nadvancement holds substantial potential for clinical imaging diagnostics and\nmechanical characterization, where measurements typically encounter substantial\nnoise."}
{"id": "2506.14038", "pdf": "https://arxiv.org/pdf/2506.14038", "abs": "https://arxiv.org/abs/2506.14038", "authors": ["Nabil Omi", "Siddhartha Sen", "Ali Farhadi"], "title": "Load Balancing Mixture of Experts with Similarity Preserving Routers", "categories": ["cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) models offer a scalable and efficient\narchitecture for training large neural networks by activating only a subset of\nparameters (\"experts\") for each input. A learned router computes a distribution\nover these experts, and assigns input tokens to a small subset. However,\nwithout auxiliary balancing mechanisms, routers often converge to using only a\nfew experts, severely limiting model capacity and degrading performance. Most\ncurrent load balancing mechanisms encourage a distribution over experts that\nresembles a roughly uniform distribution of experts per token. During training,\nthis can result in inconsistent routing behavior, resulting in the model\nspending its capacity to learn redundant knowledge. We address this by\nintroducing a novel load balancing loss that preserves token-wise relational\nstructure, encouraging consistent expert choices for similar inputs during\ntraining. Our experimental results show that applying our loss to the router\nresults in 36% faster convergence and lower redundancy compared to a popular\nload balancing loss."}
{"id": "2506.14054", "pdf": "https://arxiv.org/pdf/2506.14054", "abs": "https://arxiv.org/abs/2506.14054", "authors": ["Joshua Fan", "Haodi Xu", "Feng Tao", "Md Nasim", "Marc Grimson", "Yiqi Luo", "Carla P. Gomes"], "title": "Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, submitted to NeurIPS 2025", "summary": "Neural networks are a powerful tool for learning patterns from data. However,\nthey do not respect known scientific laws, nor can they reveal novel scientific\ninsights due to their black-box nature. In contrast, scientific reasoning\ndistills biological or physical principles from observations and controlled\nexperiments, and quantitatively interprets them with process-based models made\nof mathematical equations. Yet, process-based models rely on numerous free\nparameters that must be set in an ad-hoc manner, and thus often fit\nobservations poorly in cross-scale predictions. While prior work has embedded\nprocess-based models in conventional neural networks, discovering interpretable\nrelationships between parameters in process-based models and input features is\nstill a grand challenge for scientific discovery. We thus propose\nScientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent\nframework that combines interpretable neural and process-based reasoning. An\ninterpretable encoder predicts scientifically-meaningful latent parameters,\nwhich are then passed through a differentiable process-based decoder to predict\nlabeled output variables. ScIReN also uses a novel hard-sigmoid constraint\nlayer to restrict latent parameters to meaningful ranges defined by scientific\nprior knowledge, further enhancing its interpretability. While the embedded\nprocess-based model enforces established scientific knowledge, the encoder\nreveals new scientific mechanisms and relationships hidden in conventional\nblack-box models. We apply ScIReN on two tasks: simulating the flow of organic\ncarbon through soils, and modeling ecosystem respiration from plants. In both\ntasks, ScIReN outperforms black-box networks in predictive accuracy while\nproviding substantial scientific interpretability -- it can infer latent\nscientific mechanisms and their relationships with input features."}
{"id": "2506.14067", "pdf": "https://arxiv.org/pdf/2506.14067", "abs": "https://arxiv.org/abs/2506.14067", "authors": ["Minjae Lee", "Yoonjae Jung", "Sangdon Park"], "title": "A Regret Perspective on Online Selective Generation", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Large language generative models increasingly interact with humans, while\ntheir falsified responses raise concerns. To address this hallucination effect,\nselectively abstaining from answering, called selective generation, provides an\neffective way for generators to control the hallucination when it is unsure of\ntheir answers. However, as selective generators are interacting under\nnon-stochastic environments and having partial feedback from users on selective\ngeneration (e.g., thumbs up or down on the selected answer), learning methods\nfor selective generation under such practical setups are crucial but currently\nmissing. To address these limitations, we propose an online learning algorithm\nfor selective generation under partial feedback. In particular, as learning\nunder partial feedback is well-studied by multi-armed bandit problems, we\nreduce selective generation to bandits and provide a novel conversion lemma\nfrom bandits back to selective generation to leverage any known bandit\nalgorithms and theoretical properties. This mainly connects regret guarantees\nof bandits to false discovery rate (FDR) guarantees of selective generation for\ncontrolling hallucination. However, naively exploiting known bandit algorithms\nand their regret bounds suffers from slow convergence speed in practice due the\nnature of partial feedback. To overcome this, we exploit a unique structure of\narms in selective generation for feedback unlocking, i.e., unlocking unknown\nfeedback from observed feedback. We theoretically and empirically evaluate the\nefficacy of the proposed online selective generation algorithm under partial\nfeedback over diverse data environment setups, resulting in controlling a\ndesired FDR, while maintaining reasonable selection efficiency, i.e., the ratio\nof non-abstaining answers, compared to baselines."}
{"id": "2506.14074", "pdf": "https://arxiv.org/pdf/2506.14074", "abs": "https://arxiv.org/abs/2506.14074", "authors": ["Nathaniel Pinckney", "Chenhui Deng", "Chia-Tung Ho", "Yun-Da Tsai", "Mingjie Liu", "Wenfei Zhou", "Brucek Khailany", "Haoxing Ren"], "title": "Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification", "categories": ["cs.LG", "cs.AR"], "comment": "16 pages with appendix", "summary": "We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new\ndataset and infrastructure to advance LLM and agent research in hardware design\nand verification. CVDP includes 783 problems across 13 task categories,\ncovering RTL generation, verification, debugging, specification alignment, and\ntechnical Q&A authored by experienced hardware engineers. Problems are offered\nin both non-agentic and agentic formats. The benchmark introduces more\nrealistic and challenging contexts than prior work, with state-of-the-art\nmodels achieving no more than 34% pass@1 on code generation. Agentic\ntasks$\\unicode{x2013}$especially those involving RTL reuse and\nverification$\\unicode{x2013}$are particularly difficult. Evaluation uses\nopen-source tools and model scoring infrastructure, with comprehension tasks\nassessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in\ncurrent model capabilities, underscoring the need for continued research toward\nrobust, real-world hardware design automation."}
{"id": "2506.14087", "pdf": "https://arxiv.org/pdf/2506.14087", "abs": "https://arxiv.org/abs/2506.14087", "authors": ["Zhongzheng Qiao", "Chenghao Liu", "Yiming Zhang", "Ming Jin", "Quang Pham", "Qingsong Wen", "P. N. Suganthan", "Xudong Jiang", "Savitha Ramasamy"], "title": "Multi-Scale Finetuning for Encoder-based Time Series Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Time series foundation models (TSFMs) demonstrate impressive zero-shot\nperformance for time series forecasting. However, an important yet\nunderexplored challenge is how to effectively finetune TSFMs on specific\ndownstream tasks. While naive finetuning can yield performance gains, we argue\nthat it falls short of fully leveraging TSFMs' capabilities, often resulting in\noverfitting and suboptimal performance. Given the diverse temporal patterns\nacross sampling scales and the inherent multi-scale forecasting capabilities of\nTSFMs, we adopt a causal perspective to analyze finetuning process, through\nwhich we highlight the critical importance of explicitly modeling multiple\nscales and reveal the shortcomings of naive approaches. Focusing on\n\\textit{encoder-based} TSFMs, we propose \\textbf{M}ulti\\textbf{\\textsc{s}}cale\n\\textbf{\\textsc{f}}ine\\textbf{\\textsc{t}}uning (\\textbf{MSFT}), a simple yet\ngeneral framework that explicitly integrates multi-scale modeling into the\nfinetuning process. Experimental results on three different backbones (\\moirai,\n\\moment\\ and \\units) demonstrate that TSFMs finetuned with MSFT not only\noutperform naive and typical parameter efficient finetuning methods but also\nsurpass state-of-the-art deep learning methods."}
{"id": "2506.14095", "pdf": "https://arxiv.org/pdf/2506.14095", "abs": "https://arxiv.org/abs/2506.14095", "authors": ["Parikshit Ram", "Kenneth L. Clarkson", "Tim Klinger", "Shashanka Ubaru", "Alexander G. Gray"], "title": "Transformers Learn Faster with Semantic Focus", "categories": ["cs.LG"], "comment": null, "summary": "Various forms of sparse attention have been explored to mitigate the\nquadratic computational and memory cost of the attention mechanism in\ntransformers. We study sparse transformers not through a lens of efficiency but\nrather in terms of learnability and generalization. Empirically studying a\nrange of attention mechanisms, we find that input-dependent sparse attention\nmodels appear to converge faster and generalize better than standard attention\nmodels, while input-agnostic sparse attention models show no such benefits -- a\nphenomenon that is robust across architectural and optimization hyperparameter\nchoices. This can be interpreted as demonstrating that concentrating a model's\n\"semantic focus\" with respect to the tokens currently being considered (in the\nform of input-dependent sparse attention) accelerates learning. We develop a\ntheoretical characterization of the conditions that explain this behavior. We\nestablish a connection between the stability of the standard softmax and the\nloss function's Lipschitz properties, then show how sparsity affects the\nstability of the softmax and the subsequent convergence and generalization\nguarantees resulting from the attention mechanism. This allows us to\ntheoretically establish that input-agnostic sparse attention does not provide\nany benefits. We also characterize conditions when semantic focus\n(input-dependent sparse attention) can provide improved guarantees, and we\nvalidate that these conditions are in fact met in our empirical evaluations."}
{"id": "2506.14098", "pdf": "https://arxiv.org/pdf/2506.14098", "abs": "https://arxiv.org/abs/2506.14098", "authors": ["Ziyuan Tang", "Jie Chen"], "title": "Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A foundation model like GPT elicits many emergent abilities, owing to the\npre-training with broad inclusion of data and the use of the powerful\nTransformer architecture. While foundation models in natural languages are\nprevalent, can we build similar models for graphs? This paper describes an\napproach toward a graph foundation model that is pre-trained with diverse graph\ndatasets by adapting the Transformer backbone. A central challenge toward this\nend is how a sequence model encodes graphs of varying sizes and from different\ndomains. We propose representing a node as multiple random walks, such that the\nTransformer can extract node representations from sequences, which in turn form\nedge and graph representations. We develop a novel context prediction loss for\nthese random walks and theoretically analyze their expressive power in\ndistinguishing neighborhoods and graphs. We also demonstrate the pre-training\nof our model and its adaptation to downstream tasks, showcasing its potential\nas a foundation for processing and reasoning with graph-structured data."}
{"id": "2506.14113", "pdf": "https://arxiv.org/pdf/2506.14113", "abs": "https://arxiv.org/abs/2506.14113", "authors": ["Yitian Zhang", "Liheng Ma", "Antonios Valkanas", "Boris N. Oreshkin", "Mark Coates"], "title": "SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Koopman operator theory provides a framework for nonlinear dynamical system\nanalysis and time-series forecasting by mapping dynamics to a space of\nreal-valued measurement functions, enabling a linear operator representation.\nDespite the advantage of linearity, the operator is generally\ninfinite-dimensional. Therefore, the objective is to learn measurement\nfunctions that yield a tractable finite-dimensional Koopman operator\napproximation. In this work, we establish a connection between Koopman operator\napproximation and linear Recurrent Neural Networks (RNNs), which have recently\ndemonstrated remarkable success in sequence modeling. We show that by\nconsidering an extended state consisting of lagged observations, we can\nestablish an equivalence between a structured Koopman operator and linear RNN\nupdates. Building on this connection, we present SKOLR, which integrates a\nlearnable spectral decomposition of the input signal with a multilayer\nperceptron (MLP) as the measurement functions and implements a structured\nKoopman operator via a highly parallel linear RNN stack. Numerical experiments\non various forecasting benchmarks and dynamical systems show that this\nstreamlined, Koopman-theory-based design delivers exceptional performance."}
{"id": "2506.14114", "pdf": "https://arxiv.org/pdf/2506.14114", "abs": "https://arxiv.org/abs/2506.14114", "authors": ["Khushnood Abbas", "Ruizhe Hou", "Zhou Wengang", "Dong Shi", "Niu Ling", "Satyaki Nan", "Alireza Abbasi"], "title": "Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization", "categories": ["cs.LG"], "comment": "ACM single column 633 pages", "summary": "Graph Neural Networks (GNNs) became useful for learning on non-Euclidean\ndata. However, their best performance depends on choosing the right model\narchitecture and the training objective, also called the loss function.\nResearchers have studied these parts separately, but a large-scale evaluation\nhas not looked at how GNN models and many loss functions work together across\ndifferent tasks. To fix this, we ran a thorough study - it included seven\nwell-known GNN architectures. We also used a large group of 30 single plus\nmixed loss functions. The study looked at both inductive and transductive\nsettings. Our evaluation spanned three distinct real-world datasets, assessing\nperformance in both inductive and transductive settings using 21 comprehensive\nevaluation metrics. From these extensive results (detailed in supplementary\ninformation 1 \\& 2), we meticulously analyzed the top ten model-loss\ncombinations for each metric based on their average rank. Our findings reveal\nthat, especially for the inductive case: 1) Hybrid loss functions generally\nyield superior and more robust performance compared to single loss functions,\nindicating the benefit of multi-objective optimization. 2) The GIN architecture\nalways showed the highest-level average performance, especially with\nCross-Entropy loss. 3) Although some combinations had overall lower average\nranks, models such as GAT, particularly with certain hybrid losses,\ndemonstrated incredible specialized strengths, maximizing the most top-1\nresults among the individual metrics, emphasizing subtle strengths for\nparticular task demands. 4) On the other hand, the MPNN architecture typically\nlagged behind the scenarios it was tested against."}
{"id": "2506.14122", "pdf": "https://arxiv.org/pdf/2506.14122", "abs": "https://arxiv.org/abs/2506.14122", "authors": ["Tianming Zhang", "Renbo Zhang", "Zhengyi Yang", "Yunjun Gao", "Bin Cao", "Jing Fan"], "title": "CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs", "categories": ["cs.LG", "cs.AI", "I.2.6; G.2.2; I.5.1"], "comment": null, "summary": "Temporal Betweenness Centrality (TBC) measures how often a node appears on\noptimal temporal paths, reflecting its importance in temporal networks.\nHowever, exact computation is highly expensive, and real-world TBC\ndistributions are extremely imbalanced. The severe imbalance leads\nlearning-based models to overfit to zero-centrality nodes, resulting in\ninaccurate TBC predictions and failure to identify truly central nodes.\nExisting graph neural network (GNN) methods either fail to handle such\nimbalance or ignore temporal dependencies altogether. To address these issues,\nwe propose a scalable and inductive contrastive learning-based GNN (CLGNN) for\naccurate and efficient TBC prediction. CLGNN builds an instance graph to\npreserve path validity and temporal order, then encodes structural and temporal\nfeatures using dual aggregation, i.e., mean and edge-to-node multi-head\nattention mechanisms, enhanced by temporal path count and time encodings. A\nstability-based clustering-guided contrastive module (KContrastNet) is\nintroduced to separate high-, median-, and low-centrality nodes in\nrepresentation space, mitigating class imbalance, while a regression module\n(ValueNet) estimates TBC values. CLGNN also supports multiple optimal path\ndefinitions to accommodate diverse temporal semantics. Extensive experiments\ndemonstrate the effectiveness and efficiency of CLGNN across diverse\nbenchmarks. CLGNN achieves up to a 663.7~$\\times$ speedup compared to\nstate-of-the-art exact TBC computation methods. It outperforms leading static\nGNN baselines with up to 31.4~$\\times$ lower MAE and 16.7~$\\times$ higher\nSpearman correlation, and surpasses state-of-the-art temporal GNNs with up to\n5.7~$\\times$ lower MAE and 3.9~$\\times$ higher Spearman correlation."}
{"id": "2506.14126", "pdf": "https://arxiv.org/pdf/2506.14126", "abs": "https://arxiv.org/abs/2506.14126", "authors": ["Stefan Horoi", "Guy Wolf", "Eugene Belilovsky", "Gintare Karolina Dziugaite"], "title": "Less is More: Undertraining Experts Improves Model Upcycling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern deep learning is increasingly characterized by the use of open-weight\nfoundation models that can be fine-tuned on specialized datasets. This has led\nto a proliferation of expert models and adapters, often shared via platforms\nlike HuggingFace and AdapterHub. To leverage these resources, numerous model\nupcycling methods have emerged, enabling the reuse of fine-tuned models in\nmulti-task systems. A natural pipeline has thus formed to harness the benefits\nof transfer learning and amortize sunk training costs: models are pre-trained\non general data, fine-tuned on specific tasks, and then upcycled into more\ngeneral-purpose systems. A prevailing assumption is that improvements at one\nstage of this pipeline propagate downstream, leading to gains at subsequent\nsteps. In this work, we challenge that assumption by examining how expert\nfine-tuning affects model upcycling. We show that long fine-tuning of experts\nthat optimizes for their individual performance leads to degraded merging\nperformance, both for fully fine-tuned and LoRA-adapted models, and to worse\ndownstream results when LoRA adapters are upcycled into MoE layers. We trace\nthis degradation to the memorization of a small set of difficult examples that\ndominate late fine-tuning steps and are subsequently forgotten during merging.\nFinally, we demonstrate that a task-dependent aggressive early stopping\nstrategy can significantly improve upcycling performance."}
{"id": "2506.14143", "pdf": "https://arxiv.org/pdf/2506.14143", "abs": "https://arxiv.org/abs/2506.14143", "authors": ["Hayden McTavish", "Zachery Boner", "Jon Donnelly", "Margo Seltzer", "Cynthia Rudin"], "title": "Leveraging Predictive Equivalence in Decision Trees", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Decision trees are widely used for interpretable machine learning due to\ntheir clearly structured reasoning process. However, this structure belies a\nchallenge we refer to as predictive equivalence: a given tree's decision\nboundary can be represented by many different decision trees. The presence of\nmodels with identical decision boundaries but different evaluation processes\nmakes model selection challenging. The models will have different variable\nimportance and behave differently in the presence of missing values, but most\noptimization procedures will arbitrarily choose one such model to return. We\npresent a boolean logical representation of decision trees that does not\nexhibit predictive equivalence and is faithful to the underlying decision\nboundary. We apply our representation to several downstream machine learning\ntasks. Using our representation, we show that decision trees are surprisingly\nrobust to test-time missingness of feature values; we address predictive\nequivalence's impact on quantifying variable importance; and we present an\nalgorithm to optimize the cost of reaching predictions."}
{"id": "2506.14162", "pdf": "https://arxiv.org/pdf/2506.14162", "abs": "https://arxiv.org/abs/2506.14162", "authors": ["Amirhossein Rajabpour", "Kiarash Aghakasiri", "Sandra Zilles", "Levi H. S. Lelis"], "title": "Common Benchmarks Undervalue the Generalization Power of Programmatic Policies", "categories": ["cs.LG"], "comment": "17 pages, 5 figures", "summary": "Algorithms for learning programmatic representations for sequential\ndecision-making problems are often evaluated on out-of-distribution (OOD)\nproblems, with the common conclusion that programmatic policies generalize\nbetter than neural policies on OOD problems. In this position paper, we argue\nthat commonly used benchmarks undervalue the generalization capabilities of\nprogrammatic representations. We analyze the experiments of four papers from\nthe literature and show that neural policies, which were shown not to\ngeneralize, can generalize as effectively as programmatic policies on OOD\nproblems. This is achieved with simple changes in the neural policies training\npipeline. Namely, we show that simpler neural architectures with the same type\nof sparse observation used with programmatic policies can help attain OOD\ngeneralization. Another modification we have shown to be effective is the use\nof reward functions that allow for safer policies (e.g., agents that drive\nslowly can generalize better). Also, we argue for creating benchmark problems\nhighlighting concepts needed for OOD generalization that may challenge neural\npolicies but align with programmatic representations, such as tasks requiring\nalgorithmic constructs like stacks."}
{"id": "2506.14164", "pdf": "https://arxiv.org/pdf/2506.14164", "abs": "https://arxiv.org/abs/2506.14164", "authors": ["Hanzhong Cao"], "title": "Light Aircraft Game : Basic Implementation and training results analysis", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "This paper investigates multi-agent reinforcement learning (MARL) in a\npartially observable, cooperative-competitive combat environment known as LAG.\nWe describe the environment's setup, including agent actions, hierarchical\ncontrols, and reward design across different combat modes such as No Weapon and\nShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy\nhierarchical variant of PPO, and HASAC, an off-policy method based on soft\nactor-critic. We analyze their training stability, reward progression, and\ninter-agent coordination capabilities. Experimental results show that HASAC\nperforms well in simpler coordination tasks without weapons, while HAPPO\ndemonstrates stronger adaptability in more dynamic and expressive scenarios\ninvolving missile combat. These findings provide insights into the trade-offs\nbetween on-policy and off-policy methods in multi-agent settings."}
{"id": "2506.14167", "pdf": "https://arxiv.org/pdf/2506.14167", "abs": "https://arxiv.org/abs/2506.14167", "authors": ["Prithvi Raj"], "title": "Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model", "categories": ["cs.LG"], "comment": null, "summary": "We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling\nby reinterpreting its inner functions as a Markov Kernel between probability\nspaces via inverse transform sampling. We present a generative model that is\ninterpretable, easy to design, and efficient. Our approach couples a\nKolmogorov-Arnold Network generator with independent energy-based priors,\ntrained via Maximum Likelihood. Inverse sampling enables fast inference, while\nprior knowledge can be incorporated before training to better align priors with\nposteriors, thereby improving learning efficiency and sample quality. The\nlearned prior is also recoverable and visualizable post-training, offering an\nempirical Bayes perspective. To address inflexibility and mitigate\nprior-posterior mismatch, we introduce scalable extensions based on mixture\ndistributions and Langevin Monte Carlo methods, admitting a trade-off between\nflexibility and training efficiency. Our contributions connect classical\nrepresentation theorems with modern probabilistic modeling, while balancing\ntraining stability, inference speed, and the quality and diversity of\ngenerations."}
{"id": "2506.14194", "pdf": "https://arxiv.org/pdf/2506.14194", "abs": "https://arxiv.org/abs/2506.14194", "authors": ["Sudeepta Mondal", "Zhuolin Jiang", "Ganesh Sundaramoorthi"], "title": "A Variational Information Theoretic Approach to Out-of-Distribution Detection", "categories": ["cs.LG"], "comment": null, "summary": "We present a theory for the construction of out-of-distribution (OOD)\ndetection features for neural networks. We introduce random features for OOD\nthrough a novel information-theoretic loss functional consisting of two terms,\nthe first based on the KL divergence separates resulting in-distribution (ID)\nand OOD feature distributions and the second term is the Information\nBottleneck, which favors compressed features that retain the OOD information.\nWe formulate a variational procedure to optimize the loss and obtain OOD\nfeatures. Based on assumptions on OOD distributions, one can recover properties\nof existing OOD features, i.e., shaping functions. Furthermore, we show that\nour theory can predict a new shaping function that out-performs existing ones\non OOD benchmarks. Our theory provides a general framework for constructing a\nvariety of new features with clear explainability."}
{"id": "2506.14202", "pdf": "https://arxiv.org/pdf/2506.14202", "abs": "https://arxiv.org/abs/2506.14202", "authors": ["Makoto Shing", "Takuya Akiba"], "title": "DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To appear at TTODLer-FM Workshop of the 42nd International Conference\n  on Machine Learning", "summary": "Training large neural networks with end-to-end backpropagation creates\nsignificant memory bottlenecks, limiting accessibility to state-of-the-art AI\nresearch. We propose $\\textit{DiffusionBlocks}$, a novel training framework\nthat interprets neural network blocks as performing denoising operations in a\ncontinuous-time diffusion process. By partitioning the network into\nindependently trainable blocks and optimizing noise level assignments based on\nequal cumulative probability mass, our approach achieves significant memory\nefficiency while maintaining competitive performance compared to traditional\nbackpropagation in generative tasks. Experiments on image generation and\nlanguage modeling tasks demonstrate memory reduction proportional to the number\nof blocks while achieving superior performance. DiffusionBlocks provides a\npromising pathway for democratizing access to large-scale neural network\ntraining with limited computational resources."}
{"id": "2506.14217", "pdf": "https://arxiv.org/pdf/2506.14217", "abs": "https://arxiv.org/abs/2506.14217", "authors": ["Dipesh Tharu Mahato", "Rohan Poudel", "Pramod Dhungana"], "title": "TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 6 tables, 6 figures", "summary": "Deep neural networks often achieve high accuracy, but ensuring their\nreliability under adversarial and distributional shifts remains a pressing\nchallenge. We propose TriGuard, a unified safety evaluation framework that\ncombines (1) formal robustness verification, (2) attribution entropy to\nquantify saliency concentration, and (3) a novel Attribution Drift Score\nmeasuring explanation stability. TriGuard reveals critical mismatches between\nmodel accuracy and interpretability: verified models can still exhibit unstable\nreasoning, and attribution-based signals provide complementary safety insights\nbeyond adversarial accuracy. Extensive experiments across three datasets and\nfive architectures show how TriGuard uncovers subtle fragilities in neural\nreasoning. We further demonstrate that entropy-regularized training reduces\nexplanation drift without sacrificing performance. TriGuard advances the\nfrontier in robust, interpretable model evaluation."}
{"id": "2506.14220", "pdf": "https://arxiv.org/pdf/2506.14220", "abs": "https://arxiv.org/abs/2506.14220", "authors": ["Kangkang Lu", "Yanhua Yu", "Zhiyong Huang", "Tat-Seng Chua"], "title": "Can Large Language Models Improve Spectral Graph Neural Networks?", "categories": ["cs.LG"], "comment": null, "summary": "Spectral Graph Neural Networks (SGNNs) have attracted significant attention\ndue to their ability to approximate arbitrary filters. They typically rely on\nsupervision from downstream tasks to adaptively learn appropriate filters.\nHowever, under label-scarce conditions, SGNNs may learn suboptimal filters,\nleading to degraded performance. Meanwhile, the remarkable success of Large\nLanguage Models (LLMs) has inspired growing interest in exploring their\npotential within the GNN domain. This naturally raises an important question:\n\\textit{Can LLMs help overcome the limitations of SGNNs and enhance their\nperformance?} In this paper, we propose a novel approach that leverages LLMs to\nestimate the homophily of a given graph. The estimated homophily is then used\nto adaptively guide the design of polynomial spectral filters, thereby\nimproving the expressiveness and adaptability of SGNNs across diverse graph\nstructures. Specifically, we introduce a lightweight pipeline in which the LLM\ngenerates homophily-aware priors, which are injected into the filter\ncoefficients to better align with the underlying graph topology. Extensive\nexperiments on benchmark datasets demonstrate that our LLM-driven SGNN\nframework consistently outperforms existing baselines under both homophilic and\nheterophilic settings, with minimal computational and monetary overhead."}
{"id": "2506.14251", "pdf": "https://arxiv.org/pdf/2506.14251", "abs": "https://arxiv.org/abs/2506.14251", "authors": ["Xiyu Zhao", "Qimei Cui", "Weicai Li", "Wei Ni", "Ekram Hossain", "Quan Z. Sheng", "Xiaofeng Tao", "Ping Zhang"], "title": "Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a\nbalance between personalization and generalization by conducting federated\nlearning (FL) to guide personalized learning (PL). While FL is unaffected by\npersonalized model training, in Ditto, PL depends on the outcome of the FL.\nHowever, the clients' concern about their privacy and consequent perturbation\nof their local models can affect the convergence and (performance) fairness of\nPL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension\nof Ditto under the protection of differential privacy (DP), and analyzes the\ntrade-off among its privacy guarantee, model convergence, and performance\ndistribution fairness. We also analyze the convergence upper bound of the\npersonalized models under DP-Ditto and derive the optimal number of global\naggregations given a privacy budget. Further, we analyze the performance\nfairness of the personalized models, and reveal the feasibility of optimizing\nDP-Ditto jointly for convergence and fairness. Experiments validate our\nanalysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of\nthe state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by\nover 32.71% in fairness and 9.66% in accuracy."}
{"id": "2506.14261", "pdf": "https://arxiv.org/pdf/2506.14261", "abs": "https://arxiv.org/abs/2506.14261", "authors": ["Rohan Gupta", "Erik Jenner"], "title": "RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?", "categories": ["cs.LG"], "comment": null, "summary": "Latent-space monitors aim to detect undesirable behaviours in large language\nmodels by leveraging internal model representations rather than relying solely\non black-box outputs. These methods have shown promise in identifying\nbehaviours such as deception and unsafe completions, but a critical open\nquestion remains: can LLMs learn to evade such monitors? To study this, we\nintroduce RL-Obfuscation, in which LLMs are finetuned via reinforcement\nlearning to bypass latent-space monitors while maintaining coherent\ngenerations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters\nand evaluate evasion success against a suite of monitors. We find that\ntoken-level latent-space monitors are highly vulnerable to this attack. More\nholistic monitors, such as max-pooling or attention-based probes, remain\nrobust. Moreover, we show that adversarial policies trained to evade a single\nstatic monitor generalise to unseen monitors of the same type. Finally, we\nstudy how the policy learned by RL bypasses these monitors and find that the\nmodel can also learn to repurpose tokens to mean something different\ninternally."}
{"id": "2506.14262", "pdf": "https://arxiv.org/pdf/2506.14262", "abs": "https://arxiv.org/abs/2506.14262", "authors": ["Mohammad Emtiyaz Khan"], "title": "Knowledge Adaptation as Posterior Correction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Adaptation is the holy grail of intelligence, but even the best AI models\n(like GPT) lack the adaptivity of toddlers. So the question remains: how can\nmachines adapt quickly? Despite a lot of progress on model adaptation to\nfacilitate continual and federated learning, as well as model merging, editing,\nunlearning, etc., little is known about the mechanisms by which machines can\nnaturally learn to adapt in a similar way as humans and animals. Here, we show\nthat all such adaptation methods can be seen as different ways of `correcting'\nthe approximate posteriors. More accurate posteriors lead to smaller\ncorrections, which in turn imply quicker adaptation. The result is obtained by\nusing a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023)\nwhere interference created during adaptation is characterized by the\nnatural-gradient mismatch over the past data. We present many examples to\ndemonstrate the use of posterior-correction as a natural mechanism for the\nmachines to learn to adapt quickly."}
{"id": "2506.14263", "pdf": "https://arxiv.org/pdf/2506.14263", "abs": "https://arxiv.org/abs/2506.14263", "authors": ["Qingyu Song", "Wei Lin", "Juncheng Wang", "Hong Xu"], "title": "Towards Robust Learning to Optimize with Theoretical Guarantees", "categories": ["cs.LG", "math.OC"], "comment": "Published in CVPR 2024, 55 pages, 17 figures, this version fixed some\n  typo", "summary": "Learning to optimize (L2O) is an emerging technique to solve mathematical\noptimization problems with learning-based methods. Although with great success\nin many real-world scenarios such as wireless communications, computer\nnetworks, and electronic design, existing L2O works lack theoretical\ndemonstration of their performance and robustness in out-of-distribution (OOD)\nscenarios. We address this gap by providing comprehensive proofs. First, we\nprove a sufficient condition for a robust L2O model with homogeneous\nconvergence rates over all In-Distribution (InD) instances. We assume an L2O\nmodel achieves robustness for an InD scenario. Based on our proposed\nmethodology of aligning OOD problems to InD problems, we also demonstrate that\nthe L2O model's convergence rate in OOD scenarios will deteriorate by an\nequation of the L2O model's input features. Moreover, we propose an L2O model\nwith a concise gradient-only feature construction and a novel gradient-based\nhistory modeling method. Numerical simulation demonstrates that our proposed\nmodel outperforms the state-of-the-art baseline in both InD and OOD scenarios\nand achieves up to 10 $\\times$ convergence speedup. The code of our method can\nbe found from https://github.com/NetX-lab/GoMathL2O-Official."}
{"id": "2506.14280", "pdf": "https://arxiv.org/pdf/2506.14280", "abs": "https://arxiv.org/abs/2506.14280", "authors": ["Bai Cong", "Nico Daheim", "Yuesong Shen", "Rio Yokota", "Mohammad Emtiyaz Khan", "Thomas Möllenhoff"], "title": "Improving LoRA with Variational Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "16 pages, 4 figures", "summary": "Bayesian methods have recently been used to improve LoRA finetuning and,\nalthough they improve calibration, their effect on other metrics (such as\naccuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian\nmethods also increase computational overheads and require additional tricks for\nthem to work well. Here, we fix these issues by using a recently proposed\nvariational algorithm called IVON. We show that IVON is easy to implement and\nhas similar costs to AdamW, and yet it can also drastically improve many\nmetrics by using a simple posterior pruning technique. We present extensive\nresults on billion-scale LLMs (Llama and Qwen series) going way beyond the\nscale of existing applications of IVON. For example, we finetune a Llama-3.2-3B\nmodel on a set of commonsense reasoning tasks and improve accuracy over AdamW\nby 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian\nmethods like Laplace-LoRA and BLoB. Overall, our results show that variational\nlearning with IVON can effectively improve LoRA finetuning."}
{"id": "2506.14291", "pdf": "https://arxiv.org/pdf/2506.14291", "abs": "https://arxiv.org/abs/2506.14291", "authors": ["Ben Finkelshtein", "İsmail İlkan Ceylan", "Michael Bronstein", "Ron Levie"], "title": "Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models", "categories": ["cs.LG", "cs.SI", "stat.ML"], "comment": null, "summary": "Graph machine learning architectures are typically tailored to specific tasks\non specific datasets, which hinders their broader applicability. This has led\nto a new quest in graph machine learning: how to build graph foundation models\ncapable of generalizing across arbitrary graphs and features? In this work, we\npresent a recipe for designing graph foundation models for node-level tasks\nfrom first principles. The key ingredient underpinning our study is a\nsystematic investigation of the symmetries that a graph foundation model must\nrespect. In a nutshell, we argue that label permutation-equivariance alongside\nfeature permutation-invariance are necessary in addition to the common node\npermutation-equivariance on each local neighborhood of the graph. To this end,\nwe first characterize the space of linear transformations that are equivariant\nto permutations of nodes and labels, and invariant to permutations of features.\nWe then prove that the resulting network is a universal approximator on\nmultisets that respect the aforementioned symmetries. Our recipe uses such\nlayers on the multiset of features induced by the local neighborhood of the\ngraph to obtain a class of graph foundation models for node property\nprediction. We validate our approach through extensive experiments on 29\nreal-world node classification datasets, demonstrating both strong zero-shot\nempirical performance and consistent improvement as the number of training\ngraphs increases."}
{"id": "2506.14306", "pdf": "https://arxiv.org/pdf/2506.14306", "abs": "https://arxiv.org/abs/2506.14306", "authors": ["Ata Yalcin", "Asli Umay Ozturk", "Yigit Sever", "Viktoria Pauw", "Stephan Hachinger", "Ismail Hakki Toroslu", "Pinar Karagoz"], "title": "Fair for a few: Improving Fairness in Doubly Imbalanced Datasets", "categories": ["cs.LG", "cs.CY"], "comment": "33 pages, 3 figures, submitted to AI Review", "summary": "Fairness has been identified as an important aspect of Machine Learning and\nArtificial Intelligence solutions for decision making. Recent literature offers\na variety of approaches for debiasing, however many of them fall short when the\ndata collection is imbalanced. In this paper, we focus on a particular case,\nfairness in doubly imbalanced datasets, such that the data collection is\nimbalanced both for the label and the groups in the sensitive attribute.\nFirstly, we present an exploratory analysis to illustrate limitations in\ndebiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution\nis proposed for finding the most suitable sampling and distribution for label\nand sensitive attribute, in terms of fairness and classification accuracy"}
{"id": "2506.14375", "pdf": "https://arxiv.org/pdf/2506.14375", "abs": "https://arxiv.org/abs/2506.14375", "authors": ["Muhammad Hamza Yousuf", "Jason Li", "Sahar Vahdati", "Raphael Theilen", "Jakob Wittenstein", "Jens Lehmann"], "title": "IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards", "categories": ["cs.LG", "cs.AI"], "comment": "under review, PAIS track @ ECAI 2025", "summary": "Invasive mechanical ventilation (MV) is a life-sustaining therapy for\ncritically ill patients in the intensive care unit (ICU). However, optimizing\nits settings remains a complex and error-prone process due to patient-specific\nvariability. While Offline Reinforcement Learning (RL) shows promise for MV\ncontrol, current stateof-the-art (SOTA) methods struggle with the hybrid\n(continuous and discrete) nature of MV actions. Discretizing the action space\nlimits available actions due to exponential growth in combinations and\nintroduces distribution shifts that can compromise safety. In this paper, we\npropose optimizations that build upon prior work in action space reduction to\naddress the challenges of discrete action spaces. We also adapt SOTA offline RL\nalgorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby\navoiding the pitfalls of discretization. Additionally, we introduce a\nclinically grounded reward function based on ventilator-free days and\nphysiological targets, which provides a more meaningful optimization objective\ncompared to traditional sparse mortality-based rewards. Our findings\ndemonstrate that AI-assisted MV optimization may enhance patient safety and\nenable individualized lung support, representing a significant advancement\ntoward intelligent, data-driven critical care solutions."}
{"id": "2506.14386", "pdf": "https://arxiv.org/pdf/2506.14386", "abs": "https://arxiv.org/abs/2506.14386", "authors": ["Christian H. X. Ali Mehmeti-Göpel", "Michael Wand"], "title": "ResNets Are Deeper Than You Think", "categories": ["cs.LG", "cs.AI"], "comment": "NeurIPS 2025 Submission", "summary": "Residual connections remain ubiquitous in modern neural network architectures\nnearly a decade after their introduction. Their widespread adoption is often\ncredited to their dramatically improved trainability: residual networks train\nfaster, more stably, and achieve higher accuracy than their feedforward\ncounterparts. While numerous techniques, ranging from improved initialization\nto advanced learning rate schedules, have been proposed to close the\nperformance gap between residual and feedforward networks, this gap has\npersisted. In this work, we propose an alternative explanation: residual\nnetworks do not merely reparameterize feedforward networks, but instead inhabit\na different function space. We design a controlled post-training comparison to\nisolate generalization performance from trainability; we find that\nvariable-depth architectures, similar to ResNets, consistently outperform\nfixed-depth networks, even when optimization is unlikely to make a difference.\nThese results suggest that residual connections confer performance advantages\nbeyond optimization, pointing instead to a deeper inductive bias aligned with\nthe structure of natural data."}
{"id": "2506.14390", "pdf": "https://arxiv.org/pdf/2506.14390", "abs": "https://arxiv.org/abs/2506.14390", "authors": ["Conrad Orglmeister", "Erik Bochinski", "Volker Eiselein", "Elvira Fleig"], "title": "Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computer Safety, Reliability and Security - SAFECOMP 2024\n  Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at\n  https://doi.org/10.1007/978-3-031-68738-9_29", "summary": "Understanding the decision-making and trusting the reliability of Deep\nMachine Learning Models is crucial for adopting such methods to safety-relevant\napplications. We extend self-explainable Prototypical Variational models with\nautoencoder-based out-of-distribution (OOD) detection: A Variational\nAutoencoder is applied to learn a meaningful latent space which can be used for\ndistance-based classification, likelihood estimation for OOD detection, and\nreconstruction. The In-Distribution (ID) region is defined by a Gaussian\nmixture distribution with learned prototypes representing the center of each\nmode. Furthermore, a novel restriction loss is introduced that promotes a\ncompact ID region in the latent space without collapsing it into single points.\nThe reconstructive capabilities of the Autoencoder ensure the explainability of\nthe prototypes and the ID region of the classifier, further aiding the\ndiscrimination of OOD samples. Extensive evaluations on common OOD detection\nbenchmarks as well as a large-scale dataset from a real-world railway\napplication demonstrate the usefulness of the approach, outperforming previous\nmethods."}
{"id": "2506.14391", "pdf": "https://arxiv.org/pdf/2506.14391", "abs": "https://arxiv.org/abs/2506.14391", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Geyong Min", "Man Luo"], "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficient traffic signal control (TSC) is essential for mitigating urban\ncongestion, yet existing reinforcement learning (RL) methods face challenges in\nscaling to large networks while maintaining global coordination. Centralized RL\nsuffers from scalability issues, while decentralized approaches often lack\nunified objectives, resulting in limited network-level efficiency. In this\npaper, we propose HiLight, a hierarchical reinforcement learning framework with\nglobal adversarial guidance for large-scale TSC. HiLight consists of a\nhigh-level Meta-Policy, which partitions the traffic network into subregions\nand generates sub-goals using a Transformer-LSTM architecture, and a low-level\nSub-Policy, which controls individual intersections with global awareness. To\nimprove the alignment between global planning and local execution, we introduce\nan adversarial training mechanism, where the Meta-Policy generates challenging\nyet informative sub-goals, and the Sub-Policy learns to surpass these targets,\nleading to more effective coordination. We evaluate HiLight across both\nsynthetic and real-world benchmarks, and additionally construct a large-scale\nManhattan network with diverse traffic conditions, including peak transitions,\nadverse weather, and holiday surges. Experimental results show that HiLight\nexhibits significant advantages in large-scale scenarios and remains\ncompetitive across standard benchmarks of varying sizes."}
{"id": "2506.14400", "pdf": "https://arxiv.org/pdf/2506.14400", "abs": "https://arxiv.org/abs/2506.14400", "authors": ["Roland Roller", "Michael Hahn", "Ajay Madhavan Ravichandran", "Bilgin Osmanodja", "Florian Oetke", "Zeineb Sassi", "Aljoscha Burchardt", "Klaus Netter", "Klemens Budde", "Anne Herrmann", "Tobias Strapatsas", "Peter Dabrock", "Sebastian Möller"], "title": "One Size Fits None: Rethinking Fairness in Medical AI", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted at the 6th Workshop on Gender Bias in Natural Language\n  Processing at ACL 2025", "summary": "Machine learning (ML) models are increasingly used to support clinical\ndecision-making. However, real-world medical datasets are often noisy,\nincomplete, and imbalanced, leading to performance disparities across patient\nsubgroups. These differences raise fairness concerns, particularly when they\nreinforce existing disadvantages for marginalized groups. In this work, we\nanalyze several medical prediction tasks and demonstrate how model performance\nvaries with patient characteristics. While ML models may demonstrate good\noverall performance, we argue that subgroup-level evaluation is essential\nbefore integrating them into clinical workflows. By conducting a performance\nanalysis at the subgroup level, differences can be clearly identified-allowing,\non the one hand, for performance disparities to be considered in clinical\npractice, and on the other hand, for these insights to inform the responsible\ndevelopment of more effective models. Thereby, our work contributes to a\npractical discussion around the subgroup-sensitive development and deployment\nof medical ML models and the interconnectedness of fairness and transparency."}
{"id": "2506.14411", "pdf": "https://arxiv.org/pdf/2506.14411", "abs": "https://arxiv.org/abs/2506.14411", "authors": ["John Wikman", "Alexandre Proutiere", "David Broman"], "title": "Adaptive Reinforcement Learning for Unobservable Random Delays", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "In standard Reinforcement Learning (RL) settings, the interaction between the\nagent and the environment is typically modeled as a Markov Decision Process\n(MDP), which assumes that the agent observes the system state instantaneously,\nselects an action without delay, and executes it immediately. In real-world\ndynamic environments, such as cyber-physical systems, this assumption often\nbreaks down due to delays in the interaction between the agent and the system.\nThese delays can vary stochastically over time and are typically unobservable,\nmeaning they are unknown when deciding on an action. Existing methods deal with\nthis uncertainty conservatively by assuming a known fixed upper bound on the\ndelay, even if the delay is often much lower. In this work, we introduce the\ninteraction layer, a general framework that enables agents to adaptively and\nseamlessly handle unobservable and time-varying delays. Specifically, the agent\ngenerates a matrix of possible future actions to handle both unpredictable\ndelays and lost action packets sent over networks. Building on this framework,\nwe develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA),\nwhich dynamically adjusts to delay patterns. Our method significantly\noutperforms state-of-the-art approaches across a wide range of locomotion\nbenchmark environments."}
{"id": "2506.14420", "pdf": "https://arxiv.org/pdf/2506.14420", "abs": "https://arxiv.org/abs/2506.14420", "authors": ["Ting Xiao", "Jiakun Zheng", "Rushuai Yang", "Kang Xu", "Qiaosheng Zhang", "Peng Liu", "Chenjia Bai"], "title": "Unsupervised Skill Discovery through Skill Regions Differentiation", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors\nthat can accelerate the learning of downstream tasks. Previous methods\ntypically focus on entropy-based exploration or empowerment-driven skill\nlearning. However, entropy-based exploration struggles in large-scale state\nspaces (e.g., images), and empowerment-based methods with Mutual Information\n(MI) estimations have limitations in state exploration. To address these\nchallenges, we propose a novel skill discovery objective that maximizes the\ndeviation of the state density of one skill from the explored regions of other\nskills, encouraging inter-skill state diversity similar to the initial MI\nobjective. For state-density estimation, we construct a novel conditional\nautoencoder with soft modularization for different skill policies in\nhigh-dimensional space. Meanwhile, to incentivize intra-skill exploration, we\nformulate an intrinsic reward based on the learned autoencoder that resembles\ncount-based exploration in a compact latent space. Through extensive\nexperiments in challenging state and image-based tasks, we find our method\nlearns meaningful skills and achieves superior performance in various\ndownstream tasks."}
{"id": "2506.14436", "pdf": "https://arxiv.org/pdf/2506.14436", "abs": "https://arxiv.org/abs/2506.14436", "authors": ["Shen Yuan", "Yin Zheng", "Taifeng Wang", "Binbin Liu", "Hongteng Xu"], "title": "MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation", "categories": ["cs.LG"], "comment": "23 pages, 6 figures", "summary": "Adapting large-scale foundation models in multi-task scenarios often suffers\nfrom task conflict and oblivion. To mitigate such issues, we propose a novel\n''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant\nmulti-task adaptation method. Given a weight matrix of a pre-trained model, our\nmethod applies SVD to it and introduces a learnable router to adjust its\nsingular values based on tasks and samples. Accordingly, the weight matrix\nbecomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert\ncorresponds to the outer product of a left singular vector and the\ncorresponding right one. We can improve the model capacity by imposing a\nlearnable orthogonal transform on the right singular vectors. Unlike low-rank\nadaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'\northogonality and maintains the column space of the original weight matrix.\nThese two properties make the adapted model resistant to the conflicts among\nthe new tasks and the oblivion of its original tasks, respectively. Experiments\non various datasets demonstrate that MoORE outperforms existing multi-task\nadaptation methods consistently, showing its superiority in terms of conflict-\nand oblivion-resistance. The code of the experiments is available at\nhttps://github.com/DaShenZi721/MoORE."}
{"id": "2506.14438", "pdf": "https://arxiv.org/pdf/2506.14438", "abs": "https://arxiv.org/abs/2506.14438", "authors": ["Pol Arévalo", "Alexis Molina", "Álvaro Ciudad"], "title": "sHGCN: Simplified hyperbolic graph convolutional neural networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hyperbolic geometry has emerged as a powerful tool for modeling complex,\nstructured data, particularly where hierarchical or tree-like relationships are\npresent. By enabling embeddings with lower distortion, hyperbolic neural\nnetworks offer promising alternatives to Euclidean-based models for capturing\nintricate data structures. Despite these advantages, they often face\nperformance challenges, particularly in computational efficiency and tasks\nrequiring high precision. In this work, we address these limitations by\nsimplifying key operations within hyperbolic neural networks, achieving notable\nimprovements in both runtime and performance. Our findings demonstrate that\nstreamlined hyperbolic operations can lead to substantial gains in\ncomputational speed and predictive accuracy, making hyperbolic neural networks\na more viable choice for a broader range of applications."}
{"id": "2506.14439", "pdf": "https://arxiv.org/pdf/2506.14439", "abs": "https://arxiv.org/abs/2506.14439", "authors": ["Rikiya Takehi", "Masahiro Asami", "Kosuke Kawakami", "Yuta Saito"], "title": "A General Framework for Off-Policy Learning with Partially-Observed Reward", "categories": ["cs.LG", "62L05, 68T05", "I.2.6; G.3"], "comment": "10 pages, 5 figures. Published as a conference paper at ICLR 2025", "summary": "Off-policy learning (OPL) in contextual bandits aims to learn a\ndecision-making policy that maximizes the target rewards by using only\nhistorical interaction data collected under previously developed policies.\nUnfortunately, when rewards are only partially observed, the effectiveness of\nOPL degrades severely. Well-known examples of such partial rewards include\nexplicit ratings in content recommendations, conversion signals on e-commerce\nplatforms that are partial due to delay, and the issue of censoring in medical\nproblems. One possible solution to deal with such partial rewards is to use\nsecondary rewards, such as dwelling time, clicks, and medical indicators, which\nare more densely observed. However, relying solely on such secondary rewards\ncan also lead to poor policy learning since they may not align with the target\nreward. Thus, this work studies a new and general problem of OPL where the goal\nis to learn a policy that maximizes the expected target reward by leveraging\ndensely observed secondary rewards as supplemental data. We then propose a new\nmethod called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR),\nwhich effectively uses the secondary rewards in addition to the\npartially-observed target reward to achieve effective OPL despite the\nchallenging scenario. We also discuss a case where we aim to optimize not only\nthe expected target reward but also the expected secondary rewards to some\nextent; counter-intuitively, we will show that leveraging the two objectives is\nin fact advantageous also for the optimization of only the target reward. Along\nwith statistical analysis of our proposed methods, empirical evaluations on\nboth synthetic and real-world data show that HyPeR outperforms existing methods\nin various scenarios."}
{"id": "2506.14449", "pdf": "https://arxiv.org/pdf/2506.14449", "abs": "https://arxiv.org/abs/2506.14449", "authors": ["Lucas Kreiss", "Amey Chaware", "Maryam Roohian", "Sarah Lemire", "Oana-Maria Thoma", "Birgitta Carlé", "Maximilian Waldner", "Sebastian Schürmann", "Oliver Friedrich", "Roarke Horstmeyer"], "title": "Detecting immune cells with label-free two-photon autofluorescence and deep learning", "categories": ["cs.LG", "physics.optics"], "comment": null, "summary": "Label-free imaging has gained broad interest because of its potential to omit\nelaborate staining procedures which is especially relevant for in vivo use.\nLabel-free multiphoton microscopy (MPM), for instance, exploits two-photon\nexcitation of natural autofluorescence (AF) from native, metabolic proteins,\nmaking it ideal for in vivo endomicroscopy. Deep learning (DL) models have been\nwidely used in other optical imaging technologies to predict specific target\nannotations and thereby digitally augment the specificity of these label-free\nimages. However, this computational specificity has only rarely been\nimplemented for MPM. In this work, we used a data set of label-free MPM images\nfrom a series of different immune cell types (5,075 individual cells for binary\nclassification in mixed samples and 3,424 cells for a multi-class\nclassification task) and trained a convolutional neural network (CNN) to\nclassify cell types based on this label-free AF as input. A low-complexity\nsqueezeNet architecture was able to achieve reliable immune cell classification\nresults (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples;\n0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class\nclassification in isolated samples). Perturbation tests confirmed that the\nmodel is not confused by extracellular environment and that both input AF\nchannels (NADH and FAD) are about equally important to the classification. In\nthe future, such predictive DL models could directly detect specific immune\ncells in unstained images and thus, computationally improve the specificity of\nlabel-free MPM which would have great potential for in vivo endomicroscopy."}
{"id": "2506.14457", "pdf": "https://arxiv.org/pdf/2506.14457", "abs": "https://arxiv.org/abs/2506.14457", "authors": ["Freya Behrens", "Lenka Zdeborová"], "title": "Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge", "categories": ["cs.LG"], "comment": "9 pages, 21 figures", "summary": "Dataset distillation aims to compress training data into fewer examples via a\nteacher, from which a student can learn effectively. While its success is often\nattributed to structure in the data, modern neural networks also memorize\nspecific facts, but if and how such memorized information is can transferred in\ndistillation settings remains less understood. In this work, we show that\nstudents trained on soft labels from teachers can achieve non-trivial accuracy\non held-out memorized data they never directly observed. This effect persists\non structured data when the teacher has not generalized.To analyze it in\nisolation, we consider finite random i.i.d. datasets where generalization is a\npriori impossible and a successful teacher fit implies pure memorization.\nStill, students can learn non-trivial information about the held-out data, in\nsome cases up to perfect accuracy. In those settings, enough soft labels are\navailable to recover the teacher functionally - the student matches the\nteacher's predictions on all possible inputs, including the held-out memorized\ndata. We show that these phenomena strongly depend on the temperature with\nwhich the logits are smoothed, but persist across varying network capacities,\narchitectures and dataset compositions."}
{"id": "2506.14459", "pdf": "https://arxiv.org/pdf/2506.14459", "abs": "https://arxiv.org/abs/2506.14459", "authors": ["Md. Mortuza Ahmmed", "Abdullah Al Noman", "Mahin Montasir Afif", "K. M. Tahsin Kabir", "Md. Mostafizur Rahman", "Mufti Mahmud"], "title": "A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals", "categories": ["cs.LG"], "comment": null, "summary": "Depression is a significant mental health concern, particularly in\nprofessional environments where work-related stress, financial pressure, and\nlifestyle imbalances contribute to deteriorating well-being. Despite increasing\nawareness, researchers and practitioners face critical challenges in developing\naccurate and generalizable predictive models for mental health disorders.\nTraditional classification approaches often struggle with the complexity of\ndepression, as it is influenced by multifaceted, interdependent factors,\nincluding occupational stress, sleep patterns, and job satisfaction. This study\naddresses these challenges by proposing a stacking-based ensemble learning\napproach to improve the predictive accuracy of depression classification among\nprofessionals. The Depression Professional Dataset has been collected from\nKaggle. The dataset comprises demographic, occupational, and lifestyle\nattributes that influence mental well-being. Our stacking model integrates\nmultiple base learners with a logistic regression-mediated model, effectively\ncapturing diverse learning patterns. The experimental results demonstrate that\nthe proposed model achieves high predictive performance, with an accuracy of\n99.64% on training data and 98.75% on testing data, with precision, recall, and\nF1-score all exceeding 98%. These findings highlight the effectiveness of\nensemble learning in mental health analytics and underscore its potential for\nearly detection and intervention strategies."}
{"id": "2506.14460", "pdf": "https://arxiv.org/pdf/2506.14460", "abs": "https://arxiv.org/abs/2506.14460", "authors": ["Junbin Qiu", "Zhengpeng Xie", "Xiangda Yan", "Yongjie Yang", "Yao Shu"], "title": "Zeroth-Order Optimization is Secretly Single-Step Policy Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing\nfunctions where explicit gradients are unavailable or expensive to compute.\nHowever, the underlying mechanisms of popular ZOO methods, particularly those\nemploying randomized finite differences, and their connection to other\noptimization paradigms like Reinforcement Learning (RL) are not fully\nelucidated. This paper establishes a fundamental and previously unrecognized\nconnection: ZOO with finite differences is equivalent to a specific instance of\nsingle-step Policy Optimization (PO). We formally unveil that the implicitly\nsmoothed objective function optimized by common ZOO algorithms is identical to\na single-step PO objective. Furthermore, we show that widely used ZOO gradient\nestimators, are mathematically equivalent to the REINFORCE gradient estimator\nwith a specific baseline function, revealing the variance-reducing mechanism in\nZOO from a PO perspective.Built on this unified framework, we propose ZoAR\n(Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO\nalgorithm incorporating PO-inspired variance reduction techniques: an averaged\nbaseline from recent evaluations and query reuse analogous to experience\nreplay. Our theoretical analysis further substantiates these techniques reduce\nvariance and enhance convergence. Extensive empirical studies validate our\ntheory and demonstrate that ZoAR significantly outperforms other methods in\nterms of convergence speed and final performance. Overall, our work provides a\nnew theoretical lens for understanding ZOO and offers practical algorithmic\nimprovements derived from its connection to PO."}
{"id": "2506.14472", "pdf": "https://arxiv.org/pdf/2506.14472", "abs": "https://arxiv.org/abs/2506.14472", "authors": ["Fabien Bernier", "Maxime Cordy", "Yves Le Traon"], "title": "Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks", "categories": ["cs.LG", "cs.AI"], "comment": "ECML PKDD 2025", "summary": "Accurate electrical consumption forecasting is crucial for efficient energy\nmanagement and resource allocation. While traditional time series forecasting\nrelies on historical patterns and temporal dependencies, incorporating external\nfactors -- such as weather indicators -- has shown significant potential for\nimproving prediction accuracy in complex real-world applications. However, the\ninclusion of these additional features often degrades the performance of global\npredictive models trained on entire populations, despite improving individual\nhousehold-level models. To address this challenge, we found that a hypernetwork\narchitecture can effectively leverage external factors to enhance the accuracy\nof global electrical consumption forecasting models, by specifically adjusting\nthe model weights to each consumer.\n  We collected a comprehensive dataset spanning two years, comprising\nconsumption data from over 6000 luxembourgish households and corresponding\nexternal factors such as weather indicators, holidays, and major local events.\nBy comparing various forecasting models, we demonstrate that a hypernetwork\napproach outperforms existing methods when associated to external factors,\nreducing forecasting errors and achieving the best accuracy while maintaining\nthe benefits of a global model."}
{"id": "2506.14515", "pdf": "https://arxiv.org/pdf/2506.14515", "abs": "https://arxiv.org/abs/2506.14515", "authors": ["Prabhav Sanga", "Jaskaran Singh", "Arun K. Dubey"], "title": "Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ICML MUGen'25", "summary": "As machine learning systems increasingly rely on data subject to privacy\nregulation, selectively unlearning specific information from trained models has\nbecome essential. In image classification, this involves removing the influence\nof particular training samples, semantic classes, or visual styles without full\nretraining. We introduce \\textbf{Forget-Aligned Model Reconstruction (FAMR)}, a\ntheoretically grounded and computationally efficient framework for post-hoc\nunlearning in deep image classifiers. FAMR frames forgetting as a constrained\noptimization problem that minimizes a uniform-prediction loss on the forget set\nwhile anchoring model parameters to their original values via an $\\ell_2$\npenalty. A theoretical analysis links FAMR's solution to\ninfluence-function-based retraining approximations, with bounds on parameter\nand output deviation. Empirical results on class forgetting tasks using\nCIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong\nperformance retention and minimal computational overhead. The framework\ngeneralizes naturally to concept and style erasure, offering a scalable and\ncertifiable route to efficient post-hoc forgetting in vision models."}
{"id": "2506.14518", "pdf": "https://arxiv.org/pdf/2506.14518", "abs": "https://arxiv.org/abs/2506.14518", "authors": ["Elif Yılmaz", "Christos Dimitrakakis"], "title": "Two-Player Zero-Sum Games with Bandit Feedback", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "We study a two-player zero-sum game (TPZSG) in which the row player aims to\nmaximize their payoff against an adversarial column player, under an unknown\npayoff matrix estimated through bandit feedback. We propose and analyze two\nalgorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and\nETC-TPZSG-AE, which improves upon it by incorporating an action pair\nelimination (AE) strategy that leverages the $\\varepsilon$-Nash Equilibrium\nproperty to efficiently select the optimal action pair. Our objective is to\ndemonstrate the applicability of ETC in a TPZSG setting by focusing on learning\npure strategy Nash Equilibrium. A key contribution of our work is a derivation\nof instance-dependent upper bounds on the expected regret for both algorithms,\nhas received limited attention in the literature on zero-sum games.\nParticularly, after $T$ rounds, we achieve an instance-dependent regret upper\nbounds of $O(\\Delta + \\sqrt{T})$ for ETC-TPZSG and $O(\\frac{\\log (T\n\\Delta^2)}{\\Delta})$ for ETC-TPZSG-AE, where $\\Delta$ denotes the suboptimality\ngap. Therefore, our results indicate that ETC-based algorithms perform\neffectively in adversarial game settings, achieving regret bounds comparable to\nexisting methods while providing insights through instance-dependent analysis."}
{"id": "2506.14521", "pdf": "https://arxiv.org/pdf/2506.14521", "abs": "https://arxiv.org/abs/2506.14521", "authors": ["Korbinian Pfab", "Marcel Rothering"], "title": "Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction", "categories": ["cs.LG", "I.2"], "comment": "Submitted and accepted to IEEE COMPSAC 2025", "summary": "Are current artificial intelligence (AI) research methodologies ready to\ncreate successful, productive, and profitable AI applications? This work\npresents a case study on an industrial AI use case called false call reduction\nfor automated optical inspection to demonstrate the shortcomings of current\nbest practices. We identify seven weaknesses prevalent in related peer-reviewed\nwork and experimentally show their consequences. We show that the best-practice\nmethodology would fail for this use case. We argue amongst others for the\nnecessity of requirement-aware metrics to ensure achieving business objectives,\nclear definitions of success criteria, and a thorough analysis of temporal\ndynamics in experimental datasets. Our work encourages researchers to\ncritically assess their methodologies for more successful applied AI research."}
{"id": "2506.14529", "pdf": "https://arxiv.org/pdf/2506.14529", "abs": "https://arxiv.org/abs/2506.14529", "authors": ["Xiaohan Zheng", "Lanning Wei", "Yong Li", "Quanming Yao"], "title": "Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution", "categories": ["cs.LG"], "comment": null, "summary": "Effective decision-making on networks often relies on learning from\ngraph-structured data, where Graph Neural Networks (GNNs) play a central role,\nbut they take efforts to configure and tune. In this demo, we propose LLMNet,\nshowing how to design GNN automated through Large Language Models. Our system\ndevelops a set of agents that construct graph-related knowlege bases and then\nleverages Retrieval-Augmented Generation (RAG) to support automated\nconfiguration and refinement of GNN models through a knowledge-guided evolution\nprocess. These agents, equipped with specialized knowledge bases, extract\ninsights into tasks and graph structures by interacting with the knowledge\nbases. Empirical results show LLMNet excels in twelve datasets across three\ngraph learning tasks, validating its effectiveness of GNN model designing."}
{"id": "2506.14540", "pdf": "https://arxiv.org/pdf/2506.14540", "abs": "https://arxiv.org/abs/2506.14540", "authors": ["Gerardo A. Flores", "Alyssa H. Smith", "Julia A. Fukuyama", "Ashia C. Wilson"], "title": "Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine learning-based decision support systems are increasingly deployed in\nclinical settings, where probabilistic scoring functions are used to inform and\nprioritize patient management decisions. However, widely used scoring rules,\nsuch as accuracy and AUC-ROC, fail to adequately reflect key clinical\npriorities, including calibration, robustness to distributional shifts, and\nsensitivity to asymmetric error costs. In this work, we propose a principled\nyet practical evaluation framework for selecting calibrated thresholded\nclassifiers that explicitly accounts for the uncertainty in class prevalences\nand domain-specific cost asymmetries often found in clinical settings. Building\non the theory of proper scoring rules, particularly the Schervish\nrepresentation, we derive an adjusted variant of cross-entropy (log score) that\naverages cost-weighted performance over clinically relevant ranges of class\nbalance. The resulting evaluation is simple to apply, sensitive to clinical\ndeployment conditions, and designed to prioritize models that are both\ncalibrated and robust to real-world variations."}
{"id": "2506.14563", "pdf": "https://arxiv.org/pdf/2506.14563", "abs": "https://arxiv.org/abs/2506.14563", "authors": ["Jesse St. Amand", "Leonardo Gizzi", "Martin A. Giese"], "title": "Single-Example Learning in a Mixture of GPDMs with Latent Geometries", "categories": ["cs.LG"], "comment": "13 pages, 2 figures, 3 tables", "summary": "We present the Gaussian process dynamical mixture model (GPDMM) and show its\nutility in single-example learning of human motion data. The Gaussian process\ndynamical model (GPDM) is a form of the Gaussian process latent variable model\n(GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM\ncombines multiple GPDMs in a probabilistic mixture-of-experts framework,\nutilizing embedded geometric features to allow for diverse sequences to be\nencoded in a single latent space, enabling the categorization and generation of\neach sequence class. GPDMs and our mixture model are particularly advantageous\nin addressing the challenges of modeling human movement in scenarios where data\nis limited and model interpretability is vital, such as in patient-specific\nmedical applications like prosthesis control. We score the GPDMM on\nclassification accuracy and generative ability in single-example learning,\nshowcase model variations, and benchmark it against LSTMs, VAEs, and\ntransformers."}
{"id": "2506.14574", "pdf": "https://arxiv.org/pdf/2506.14574", "abs": "https://arxiv.org/abs/2506.14574", "authors": ["Mingkang Zhu", "Xi Chen", "Zhongdao Wang", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent advancements in reinforcement learning from human feedback have shown\nthat utilizing fine-grained token-level reward models can substantially enhance\nthe performance of Proximal Policy Optimization (PPO) in aligning large\nlanguage models. However, it is challenging to leverage such token-level reward\nas guidance for Direct Preference Optimization (DPO), since DPO is formulated\nas a sequence-level bandit problem. To address this challenge, this work\ndecomposes the sequence-level PPO into a sequence of token-level proximal\npolicy optimization problems and then frames the problem of token-level PPO\nwith token-level reward guidance, from which closed-form optimal token-level\npolicy and the corresponding token-level reward can be derived. Using the\nobtained reward and Bradley-Terry model, this work establishes a framework of\ncomputable loss functions with token-level reward guidance for DPO, and\nproposes a practical reward guidance based on the induced DPO reward. This\nformulation enables different tokens to exhibit varying degrees of deviation\nfrom reference policy based on their respective rewards. Experiment results\ndemonstrate that our method achieves substantial performance improvements over\nDPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on\nAlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at\nhttps://github.com/dvlab-research/TGDPO."}
{"id": "2506.14577", "pdf": "https://arxiv.org/pdf/2506.14577", "abs": "https://arxiv.org/abs/2506.14577", "authors": ["Abdul Rahman Jacob", "Avinash Kori", "Emanuele De Angelis", "Ben Glocker", "Maurizio Proietti", "Francesca Toni"], "title": "Object-Centric Neuro-Argumentative Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of Machine Learning Research, 2025 19th Conference on\n  Neurosymbolic Learning and Reasoning", "summary": "Over the last decade, as we rely more on deep learning technologies to make\ncritical decisions, concerns regarding their safety, reliability and\ninterpretability have emerged. We introduce a novel Neural Argumentative\nLearning (NAL) architecture that integrates Assumption-Based Argumentation\n(ABA) with deep learning for image analysis. Our architecture consists of\nneural and symbolic components. The former segments and encodes images into\nfacts using object-centric learning, while the latter applies ABA learning to\ndevelop ABA frameworks enabling predictions with images. Experiments on\nsynthetic data show that the NAL architecture can be competitive with a\nstate-of-the-art alternative."}
{"id": "2506.14587", "pdf": "https://arxiv.org/pdf/2506.14587", "abs": "https://arxiv.org/abs/2506.14587", "authors": ["Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification", "categories": ["cs.LG"], "comment": "20 pages", "summary": "Shortcut learning undermines model generalization to out-of-distribution\ndata. While the literature attributes shortcuts to biases in superficial\nfeatures, we show that imbalances in the semantic distribution of sample\nembeddings induce spurious semantic correlations, compromising model\nrobustness. To address this issue, we propose SCISSOR (Semantic Cluster\nIntervention for Suppressing ShORtcut), a Siamese network-based debiasing\napproach that remaps the semantic space by discouraging latent clusters\nexploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR\neliminates the need for data augmentation and rewriting. We evaluate SCISSOR on\n6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and\nGYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports\n+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,\nand +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models\nwith ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for\nBERT on NLP. Our study redefines the landscape of model generalization by\naddressing overlooked semantic biases, establishing SCISSOR as a foundational\nframework for mitigating shortcut learning and fostering more robust,\nbias-resistant AI systems."}
{"id": "2506.14597", "pdf": "https://arxiv.org/pdf/2506.14597", "abs": "https://arxiv.org/abs/2506.14597", "authors": ["Thomas Newman", "Christopher Nemeth", "Matthew Jones", "Philip Jonathan"], "title": "Deep Learning Surrogates for Real-Time Gas Emission Inversion", "categories": ["cs.LG", "stat.AP", "stat.ML"], "comment": "3 figures, 11 pages", "summary": "Real-time identification and quantification of greenhouse-gas emissions under\ntransient atmospheric conditions is a critical challenge in environmental\nmonitoring. We introduce a spatio-temporal inversion framework that embeds a\ndeep-learning surrogate of computational fluid dynamics (CFD) within a\nsequential Monte Carlo algorithm to perform Bayesian inference of both emission\nrate and source location in dynamic flow fields. By substituting costly\nnumerical solvers with a multilayer perceptron trained on high-fidelity CFD\noutputs, our surrogate captures spatial heterogeneity and temporal evolution of\ngas dispersion, while delivering near-real-time predictions. Validation on the\nChilbolton methane release dataset demonstrates comparable accuracy to full CFD\nsolvers and Gaussian plume models, yet achieves orders-of-magnitude faster\nruntimes. Further experiments under simulated obstructed-flow scenarios confirm\nrobustness in complex environments. This work reconciles physical fidelity with\ncomputational feasibility, offering a scalable solution for industrial\nemissions monitoring and other time-sensitive spatio-temporal inversion tasks\nin environmental and scientific modeling."}
{"id": "2506.14607", "pdf": "https://arxiv.org/pdf/2506.14607", "abs": "https://arxiv.org/abs/2506.14607", "authors": ["Ziyu Gong", "Jim Lim", "David I. Inouye"], "title": "Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization", "categories": ["cs.LG", "cs.CY"], "comment": "32 pages, 20 figures. Accepted to ICML 2025", "summary": "Distribution matching (DM) is a versatile domain-invariant representation\nlearning technique that has been applied to tasks such as fair classification,\ndomain adaptation, and domain translation. Non-parametric DM methods struggle\nwith scalability and adversarial DM approaches suffer from instability and mode\ncollapse. While likelihood-based methods are a promising alternative, they\noften impose unnecessary biases through fixed priors or require explicit\ndensity models (e.g., flows) that can be challenging to train. We address this\nlimitation by introducing a novel approach to training likelihood-based DM\nusing expressive score-based prior distributions. Our key insight is that\ngradient-based DM training only requires the prior's score function -- not its\ndensity -- allowing us to train the prior via denoising score matching. This\napproach eliminates biases from fixed priors (e.g., in VAEs), enabling more\neffective use of geometry-preserving regularization, while avoiding the\nchallenge of learning an explicit prior density model (e.g., a flow-based\nprior). Our method also demonstrates better stability and computational\nefficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore,\nexperiments demonstrate superior performance across multiple tasks,\nestablishing our score-based method as a stable and effective approach to\ndistribution matching. Source code available at\nhttps://github.com/inouye-lab/SAUB."}
{"id": "2506.14619", "pdf": "https://arxiv.org/pdf/2506.14619", "abs": "https://arxiv.org/abs/2506.14619", "authors": ["Paolo Ascia", "Elena Raponi", "Thomas Bäck", "Fabian Duddeck"], "title": "Feasibility-Driven Trust Region Bayesian Optimization", "categories": ["cs.LG"], "comment": "Accepted for publication at AutoML2025", "summary": "Bayesian optimization is a powerful tool for solving real-world optimization\ntasks under tight evaluation budgets, making it well-suited for applications\ninvolving costly simulations or experiments. However, many of these tasks are\nalso characterized by the presence of expensive constraints whose analytical\nformulation is unknown and often defined in high-dimensional spaces where\nfeasible regions are small, irregular, and difficult to identify. In such\ncases, a substantial portion of the optimization budget may be spent just\ntrying to locate the first feasible solution, limiting the effectiveness of\nexisting methods. In this work, we present a Feasibility-Driven Trust Region\nBayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust\nregion from which the next candidate solution is selected, using information\nfrom both the objective and constraint surrogate models. Our adaptive strategy\nallows the trust region to shift and resize significantly between iterations,\nenabling the optimizer to rapidly refocus its search and consistently\naccelerate the discovery of feasible and good-quality solutions. We empirically\ndemonstrate the effectiveness of FuRBO through extensive testing on the full\nBBOB-constrained COCO benchmark suite and other physics-inspired benchmarks,\ncomparing it against state-of-the-art baselines for constrained black-box\noptimization across varying levels of constraint severity and problem\ndimensionalities ranging from 2 to 60."}
{"id": "2506.14698", "pdf": "https://arxiv.org/pdf/2506.14698", "abs": "https://arxiv.org/abs/2506.14698", "authors": ["Sidney Bender", "Jan Herrmann", "Klaus-Robert Müller", "Grégoire Montavon"], "title": "Towards Desiderata-Driven Design of Visual Counterfactual Explainers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Visual counterfactual explainers (VCEs) are a straightforward and promising\napproach to enhancing the transparency of image classifiers. VCEs complement\nother types of explanations, such as feature attribution, by revealing the\nspecific data transformations to which a machine learning model responds most\nstrongly. In this paper, we argue that existing VCEs focus too narrowly on\noptimizing sample quality or change minimality; they fail to consider the more\nholistic desiderata for an explanation, such as fidelity, understandability,\nand sufficiency. To address this shortcoming, we explore new mechanisms for\ncounterfactual generation and investigate how they can help fulfill these\ndesiderata. We combine these mechanisms into a novel 'smooth counterfactual\nexplorer' (SCE) algorithm and demonstrate its effectiveness through systematic\nevaluations on synthetic and real data."}
{"id": "2506.14746", "pdf": "https://arxiv.org/pdf/2506.14746", "abs": "https://arxiv.org/abs/2506.14746", "authors": ["Nataly Brukhim", "Aldo Pacchiano", "Miroslav Dudik", "Robert Schapire"], "title": "On the Hardness of Bandit Learning", "categories": ["cs.LG", "stat.ML"], "comment": "13 main pages", "summary": "We study the task of bandit learning, also known as best-arm identification,\nunder the assumption that the true reward function f belongs to a known, but\narbitrary, function class F. We seek a general theory of bandit learnability,\nakin to the PAC framework for classification. Our investigation is guided by\nthe following two questions: (1) which classes F are learnable, and (2) how\nthey are learnable. For example, in the case of binary PAC classification,\nlearnability is fully determined by a combinatorial dimension - the VC\ndimension- and can be attained via a simple algorithmic principle, namely,\nempirical risk minimization (ERM). In contrast to classical learning-theoretic\nresults, our findings reveal limitations of learning in structured bandits,\noffering insights into the boundaries of bandit learnability. First, for the\nquestion of \"which\", we show that the paradigm of identifying the learnable\nclasses via a dimension-like quantity fails for bandit learning. We give a\nsimple proof demonstrating that no combinatorial dimension can characterize\nbandit learnability, even in finite classes, following a standard definition of\ndimension introduced by Ben-David et al. (2019). For the question of \"how\", we\nprove a computational hardness result: we construct a reward function class for\nwhich at most two queries are needed to find the optimal action, yet no\nalgorithm can do so in polynomial time unless RP=NP. We also prove that this\nclass admits efficient algorithms for standard algorithmic operations often\nconsidered in learning theory, such as an ERM. This implies that computational\nhardness is in this case inherent to the task of bandit learning. Beyond these\nresults, we investigate additional themes such as learning under noise,\ntrade-offs between noise models, and the relationship between query complexity\nand regret minimization."}
{"id": "2505.22963", "pdf": "https://arxiv.org/pdf/2505.22963", "abs": "https://arxiv.org/abs/2505.22963", "authors": ["Zhuoran Duan", "Guoshun Nan", "Rushan Li", "Zijun Wang", "Lihua Xiong", "Chaoying Yuan", "Guorong Liu", "Hui Xu", "Qimei Cui", "Xiaofeng Tao", "Tony Q. S. Quek"], "title": "Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "The upcoming 6G will fundamentally reshape mobile networks beyond\ncommunications, unlocking a multitude of applications that were once considered\nunimaginable. Meanwhile, security and resilience are especially highlighted in\nthe 6G design principles. However, safeguarding 6G networks will be quite\nchallenging due to various known and unknown threats from highly heterogeneous\nnetworks and diversified security requirements of distinct use cases, calling\nfor a comprehensive re-design of security architecture. This motivates us to\npropose ES3A (Entire Smart Service-based Security Architecture), a novel\nsecurity architecture for 6G networks. Specifically, we first discuss six\nhigh-level principles of our ES3A that include hierarchy, flexibility,\nscalability, resilience, endogeny, and trust and privacy. With these goals in\nmind, we then introduce three guidelines from a deployment perspective,\nenvisioning our ES3A that offers service-based security, end-to-end protection,\nand smart security automation for 6G networks. Our architecture consists of\nthree layers and three domains. It relies on a two-stage orchestration\nmechanism to tailor smart security strategies for customized protection in\nhigh-dynamic 6G networks, thereby addressing the aforementioned challenges.\nFinally, we prototype the proposed ES3A on a real-world radio system based on\nSoftware-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.\nWe also provide a case to show the superiority of our architecture."}
{"id": "2506.13780", "pdf": "https://arxiv.org/pdf/2506.13780", "abs": "https://arxiv.org/abs/2506.13780", "authors": ["Sedat Porikli", "Vedat Porikli"], "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop\n  on Experimental Model Auditing via Controllable Synthesis (EMACS) and\n  Workshop on Demographic Diversity in Computer Vision (DemoDiv)", "summary": "Text-to-Image (T2I) models have transformed visual content creation,\nproducing highly realistic images from natural language prompts. However,\nconcerns persist around their potential to replicate and magnify existing\nsocietal biases. To investigate these issues, we curated a diverse set of\nprompts spanning thematic categories such as occupations, traits, actions,\nideologies, emotions, family roles, place descriptions, spirituality, and life\nevents. For each of the 160 unique topics, we crafted multiple prompt\nvariations to reflect a wide range of meanings and perspectives. Using Stable\nDiffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original\ncheckpoints, we generated over 16,000 images under consistent settings.\nAdditionally, we collected 8,000 comparison images from Google Image Search.\nAll outputs were filtered to exclude abstract, distorted, or nonsensical\nresults. Our analysis reveals significant disparities in the representation of\ngender, race, age, somatotype, and other human-centric factors across generated\nimages. These disparities often mirror and reinforce harmful stereotypes\nembedded in societal narratives. We discuss the implications of these findings\nand emphasize the need for more inclusive datasets and development practices to\nfoster fairness in generative visual systems."}
{"id": "2506.13783", "pdf": "https://arxiv.org/pdf/2506.13783", "abs": "https://arxiv.org/abs/2506.13783", "authors": ["Soyeon Choi", "Kangwook Lee", "Oliver Sng", "Joshua M. Ackerman"], "title": "Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents", "categories": ["physics.soc-ph", "cs.LG"], "comment": "8 pages", "summary": "How does the threat of infectious disease influence sociality among\ngenerative agents? We used generative agent-based modeling (GABM), powered by\nlarge language models, to experimentally test hypotheses about the behavioral\nimmune system. Across three simulation runs, generative agents who read news\nabout an infectious disease outbreak showed significantly reduced social\nengagement compared to agents who received no such news, including lower\nattendance at a social gathering, fewer visits to third places (e.g., cafe,\nstore, park), and fewer conversations throughout the town. In interview\nresponses, agents explicitly attributed their behavioral changes to\ndisease-avoidance motivations. A validity check further indicated that they\ncould distinguish between infectious and noninfectious diseases, selectively\nreducing social engagement only when there was a risk of infection. Our\nfindings highlight the potential of GABM as an experimental tool for exploring\ncomplex human social dynamics at scale."}
{"id": "2506.13787", "pdf": "https://arxiv.org/pdf/2506.13787", "abs": "https://arxiv.org/abs/2506.13787", "authors": ["Yanjun Dai", "Haoyang Feng", "Yuan Gao"], "title": "Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "While online advertising is highly dependent on implicit interaction networks\nof anonymous users for engagement inference, and for the selection and\noptimization of delivery strategies, existing graph models seldom can capture\nthe multi-scale temporal, semantic and higher-order dependency features of\nthese interaction networks, thus it's hard to describe the complicated patterns\nof the anonymous behavior. In this paper, we propose Decoupled\nTemporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main\ncontributions. Above all, we introduce temporal edge decomposition, which\ndivides each interaction into three types of channels: short-term burst,\ndiurnal cycle and long-range memory, and conducts feature extraction using the\nconvolution kernel of parallel dilated residuals; Furthermore, our model builds\na hierarchical heterogeneous aggregation, where user-user, user-advertisement,\nadvertisement-advertisement subgraphs are combined through the meta-path\nconditional Transformer encoder, where the noise structure is dynamically\ntamped down via the synergy of cross-channel self-attention and gating\nrelationship selector. Thirdly, the contrast regularity of feedback perception\nis formulated, the consistency of various time slices is maximized, the entropy\nof control exposure information with dual-view target is maximized, the global\nprototype of dual-momentum queue distillation is presented, and the strategy\ngradient layer with light weight is combined with delaying transformation\nsignal to fine-tune the node representation for benefit-oriented. The AUC of\nDTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in\ncomparison with the best baseline model."}
{"id": "2506.13792", "pdf": "https://arxiv.org/pdf/2506.13792", "abs": "https://arxiv.org/abs/2506.13792", "authors": ["Gonçalo Hora de Carvalho", "Lazar S. Popov", "Sander Kaatee", "Kristinn R. Thórisson", "Tangrui Li", "Pétur Húni Björnsson", "Jilles S. Dibangoye"], "title": "ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "We introduce ICE-ID, a novel benchmark dataset for historical identity\nresolution, comprising 220 years (1703-1920) of Icelandic census records.\nICE-ID spans multiple generations of longitudinal data, capturing name\nvariations, demographic changes, and rich genealogical links. To the best of\nour knowledge, this is the first large-scale, open tabular dataset specifically\ndesigned to study long-term person-entity matching in a real-world population.\nWe define identity resolution tasks (within and across census waves) with\nclearly documented metrics and splits. We evaluate a range of methods:\nhandcrafted rule-based matchers, a ML ensemble as well as LLMs for structured\ndata (e.g. transformer-based tabular networks) against a novel approach to\ntabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose\nAI framework designed to reason with limited knowledge and resources. Its core\nis Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that\nNARS is suprisingly simple and competitive with other standard approaches,\nachieving SOTA at our task. By releasing ICE-ID and our code, we enable\nreproducible benchmarking of identity resolution approaches in longitudinal\nsettings and hope that ICE-ID opens new avenues for cross-disciplinary research\nin data linkage and historical analytics."}
{"id": "2506.13803", "pdf": "https://arxiv.org/pdf/2506.13803", "abs": "https://arxiv.org/abs/2506.13803", "authors": ["Richard D. Lange", "Konrad P. Kording"], "title": "Causality in the human niche: lessons for machine learning", "categories": ["cs.AI", "cs.LG"], "comment": "23 pages, 2 figures", "summary": "Humans interpret the world around them in terms of cause and effect and\ncommunicate their understanding of the world to each other in causal terms.\nThese causal aspects of human cognition are thought to underlie humans' ability\nto generalize and learn efficiently in new domains, an area where current\nmachine learning systems are weak. Building human-like causal competency into\nmachine learning systems may facilitate the construction of effective and\ninterpretable AI. Indeed, the machine learning community has been importing\nideas on causality formalized by the Structural Causal Model (SCM) framework,\nwhich provides a rigorous formal language for many aspects of causality and has\nled to significant advances. However, the SCM framework fails to capture some\nsalient aspects of human causal cognition and has likewise not yet led to\nadvances in machine learning in certain critical areas where humans excel. We\ncontend that the problem of causality in the ``human niche'' -- for a social,\nautonomous, and goal-driven agent sensing and acting in the world in which\nhumans live -- is quite different from the kind of causality captured by SCMs.\nFor example, everyday objects come in similar types that have similar causal\nproperties, and so humans readily generalize knowledge of one type of object\n(cups) to another related type (bowls) by drawing causal analogies between\nobjects with similar properties, but such analogies are at best awkward to\nexpress in SCMs. We explore how such causal capabilities are adaptive in, and\nmotivated by, the human niche. By better appreciating properties of human\ncausal cognition and, crucially, how those properties are adaptive in the niche\nin which humans live, we hope that future work at the intersection of machine\nlearning and causality will leverage more human-like inductive biases to create\nmore capable, controllable, and interpretable systems."}
{"id": "2506.13814", "pdf": "https://arxiv.org/pdf/2506.13814", "abs": "https://arxiv.org/abs/2506.13814", "authors": ["Lufei Liu", "Tor M. Aamodt"], "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering", "categories": ["cs.GR", "cs.LG", "eess.IV"], "comment": "Published at ICML 2025", "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"}
{"id": "2506.13817", "pdf": "https://arxiv.org/pdf/2506.13817", "abs": "https://arxiv.org/abs/2506.13817", "authors": ["Saleem A. Al Dajani", "Abel Sanchez", "John R. Williams"], "title": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG", "cs.SE", "q-bio.QM"], "comment": "4 pages, 5 figures, Accepted by ICML 2025 FM4LS\n  https://openreview.net/forum?id=zNjXOZxEYB . Workshop on Multi-modal\n  Foundation Models and Large Language Models for Life Sciences (FM4LS)}, July\n  2025", "summary": "Generative AI foundation models offer transformative potential for processing\nstructured biological data, particularly in single-cell RNA sequencing, where\ndatasets are rapidly scaling toward billions of cells. We propose the use of\nagentic foundation models with real-time web search to automate the labeling of\nexperimental data, achieving up to 82.5% accuracy. This addresses a key\nbottleneck in supervised learning for structured omics data by increasing\nannotation throughput without manual curation and human error. Our approach\nenables the development of virtual cell foundation models capable of downstream\ntasks such as cell-typing and perturbation prediction. As data volume grows,\nthese models may surpass human performance in labeling, paving the way for\nreliable inference in large-scale perturbation screens. This application\ndemonstrates domain-specific innovation in health monitoring and diagnostics,\naligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network."}
{"id": "2506.13818", "pdf": "https://arxiv.org/pdf/2506.13818", "abs": "https://arxiv.org/abs/2506.13818", "authors": ["Marcelle Momha"], "title": "The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI", "categories": ["cs.CY", "cs.LG"], "comment": null, "summary": "Synthetic data, which is artificially generated and intelligently mimicking\nor supplementing the real-world data, is increasingly used. The proliferation\nof AI agents and the adoption of synthetic data create a synthetic mirror that\nconceptualizes a representation and potential distortion of reality, thus\ngenerating trust and accountability deficits. This paper explores the\nimplications for privacy and policymaking stemming from synthetic data\ngeneration, and the urgent need for new policy instruments and legal framework\nadaptation to ensure appropriate levels of trust and accountability for AI\nagents relying on synthetic data. Rather than creating entirely new policy or\nlegal regimes, the most practical approach involves targeted amendments to\nexisting frameworks, recognizing synthetic data as a distinct regulatory\ncategory with unique characteristics."}
{"id": "2506.13819", "pdf": "https://arxiv.org/pdf/2506.13819", "abs": "https://arxiv.org/abs/2506.13819", "authors": ["El Arbi Belfarsi", "Henry Flores", "Maria Valero"], "title": "Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Submitted to the IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI 2025)", "summary": "In this study, we present a dual-modal AI framework based on short-wave\ninfrared (SWIR) spectroscopy. The first modality employs a multi-wavelength\nSWIR imaging system coupled with convolutional neural networks (CNNs) to\ncapture spatial features linked to glucose absorption. The second modality uses\na compact photodiode voltage sensor and machine learning regressors (e.g.,\nrandom forest) on normalized optical signals. Both approaches were evaluated on\nsynthetic blood phantoms and skin-mimicking materials across physiological\nglucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage\nerror (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error\nGrid, while the photodiode system reached 86.4% Zone A accuracy. This framework\nconstitutes a state-of-the-art solution that balances clinical accuracy, cost\nefficiency, and wearable integration, paving the way for reliable continuous\nnon-invasive glucose monitoring."}
{"id": "2506.13835", "pdf": "https://arxiv.org/pdf/2506.13835", "abs": "https://arxiv.org/abs/2506.13835", "authors": ["Masakazu Inoue", "Motoshige Sato", "Kenichi Tomeoka", "Nathania Nah", "Eri Hatakeyama", "Kai Arulkumaran", "Ilya Horiguchi", "Shuntaro Sasai"], "title": "A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations", "categories": ["q-bio.QM", "cs.LG", "q-bio.NC"], "comment": "Accepted for presentation at Interspeech 2025. 5 pages, 4 figures, 2\n  tables", "summary": "Silent speech decoding, which performs unvocalized human speech recognition\nfrom electroencephalography/electromyography (EEG/EMG), increases accessibility\nfor speech-impaired humans. However, data collection is difficult and performed\nusing varying experimental setups, making it nontrivial to collect a large,\nhomogeneous dataset. In this study we introduce neural networks that can handle\nEEG/EMG with heterogeneous electrode placements and show strong performance in\nsilent speech decoding via multi-task training on large-scale EEG/EMG datasets.\nWe achieve improved word classification accuracy in both healthy participants\n(95.3%), and a speech-impaired patient (54.5%), substantially outperforming\nmodels trained on single-subject data (70.1% and 13.2%). Moreover, our models\nalso show gains in cross-language calibration performance. This increase in\naccuracy suggests the feasibility of developing practical silent speech\ndecoding systems, particularly for speech-impaired patients."}
{"id": "2506.13846", "pdf": "https://arxiv.org/pdf/2506.13846", "abs": "https://arxiv.org/abs/2506.13846", "authors": ["Runtao Liu", "Jiahao Zhan", "Yingqing He", "Chen Wei", "Alan Yuille", "Qifeng Chen"], "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "An effective reward model plays a pivotal role in reinforcement learning for\npost-training enhancement of visual generative models. However, current\napproaches of reward modeling suffer from implementation complexity due to\ntheir reliance on extensive human-annotated preference data or meticulously\nengineered quality dimensions that are often incomplete and\nengineering-intensive. Inspired by adversarial training in generative\nadversarial networks (GANs), this paper proposes GAN-RM, an efficient reward\nmodeling framework that eliminates manual preference annotation and explicit\nquality dimension engineering. Our method trains the reward model through\ndiscrimination between a small set of representative, unpaired target\nsamples(denoted as Preference Proxy Data) and model-generated ordinary outputs,\nrequiring only a few hundred target samples. Comprehensive experiments\ndemonstrate our GAN-RM's effectiveness across multiple key applications\nincluding test-time scaling implemented as Best-of-N sample filtering,\npost-training approaches like Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO)."}
{"id": "2506.13865", "pdf": "https://arxiv.org/pdf/2506.13865", "abs": "https://arxiv.org/abs/2506.13865", "authors": ["Kasidit Srimahajariyapong", "Supanut Thanasilp", "Thiparat Chotibut"], "title": "Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "comment": "15+7 pages, 7+5 figures", "summary": "Variational quantum algorithms (VQAs) promise near-term quantum advantage,\nyet parametrized quantum states commonly built from the digital gate-based\napproach often suffer from scalability issues such as barren plateaus, where\nthe loss landscape becomes flat. We study an analog VQA ans\\\"atze composed of\n$M$ quenches of a disordered Ising chain, whose dynamics is native to several\nquantum simulation platforms. By tuning the disorder strength we place each\nquench in either a thermalized phase or a many-body-localized (MBL) phase and\nanalyse (i) the ans\\\"atze's expressivity and (ii) the scaling of loss variance.\nNumerics shows that both phases reach maximal expressivity at large $M$, but\nbarren plateaus emerge at far smaller $M$ in the thermalized phase than in the\nMBL phase. Exploiting this gap, we propose an MBL initialisation strategy:\ninitialise the ans\\\"atze in the MBL regime at intermediate quench $M$, enabling\nan initial trainability while retaining sufficient expressivity for subsequent\noptimization. The results link quantum phases of matter and VQA trainability,\nand provide practical guidelines for scaling analog-hardware VQAs."}
{"id": "2506.13900", "pdf": "https://arxiv.org/pdf/2506.13900", "abs": "https://arxiv.org/abs/2506.13900", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Arthur Charpentier"], "title": "Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Cooperative game theory has become a cornerstone of post-hoc interpretability\nin machine learning, largely through the use of Shapley values. Yet, despite\ntheir widespread adoption, Shapley-based methods often rest on axiomatic\njustifications whose relevance to feature attribution remains debatable. In\nthis paper, we revisit cooperative game theory from an interpretability\nperspective and argue for a broader and more principled use of its tools. We\nhighlight two general families of efficient allocations, the Weber and Harsanyi\nsets, that extend beyond Shapley values and offer richer interpretative\nflexibility. We present an accessible overview of these allocation schemes,\nclarify the distinction between value functions and aggregation rules, and\nintroduce a three-step blueprint for constructing reliable and\ntheoretically-grounded feature attributions. Our goal is to move beyond fixed\naxioms and provide the XAI community with a coherent framework to design\nattribution methods that are both meaningful and robust to shifting\nmethodological trends."}
{"id": "2506.13904", "pdf": "https://arxiv.org/pdf/2506.13904", "abs": "https://arxiv.org/abs/2506.13904", "authors": ["Ivania Donoso-Guzmán", "Kristýna Sirka Kacafírková", "Maxwell Szymanski", "An Jacobs", "Denis Parra", "Katrien Verbert"], "title": "A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite promising developments in Explainable Artificial Intelligence, the\npractical value of XAI methods remains under-explored and insufficiently\nvalidated in real-world settings. Robust and context-aware evaluation is\nessential, not only to produce understandable explanations but also to ensure\ntheir trustworthiness and usability for intended users, but tends to be\noverlooked because of no clear guidelines on how to design an evaluation with\nusers.\n  This study addresses this gap with two main goals: (1) to develop a framework\nof well-defined, atomic properties that characterise the user experience of XAI\nin healthcare; and (2) to provide clear, context-sensitive guidelines for\ndefining evaluation strategies based on system characteristics.\n  We conducted a systematic review of 82 user studies, sourced from five\ndatabases, all situated within healthcare settings and focused on evaluating\nAI-generated explanations. The analysis was guided by a predefined coding\nscheme informed by an existing evaluation framework, complemented by inductive\ncodes developed iteratively.\n  The review yields three key contributions: (1) a synthesis of current\nevaluation practices, highlighting a growing focus on human-centred approaches\nin healthcare XAI; (2) insights into the interrelations among explanation\nproperties; and (3) an updated framework and a set of actionable guidelines to\nsupport interdisciplinary teams in designing and implementing effective\nevaluation strategies for XAI systems tailored to specific application\ncontexts."}
{"id": "2506.13912", "pdf": "https://arxiv.org/pdf/2506.13912", "abs": "https://arxiv.org/abs/2506.13912", "authors": ["Atul Anand Gopalakrishnan", "Jakir Hossain", "Tuğrulcan Elmas", "Ahmet Erdem Sarıyüce"], "title": "Density-aware Walks for Coordinated Campaign Detection", "categories": ["cs.SI", "cs.LG"], "comment": "16 Pages. Accepted at ECML-PKDD 2025", "summary": "Coordinated campaigns frequently exploit social media platforms by\nartificially amplifying topics, making inauthentic trends appear organic, and\nmisleading users into engagement. Distinguishing these coordinated efforts from\ngenuine public discourse remains a significant challenge due to the\nsophisticated nature of such attacks. Our work focuses on detecting coordinated\ncampaigns by modeling the problem as a graph classification task. We leverage\nthe recently introduced Large Engagement Networks (LEN) dataset, which contains\nover 300 networks capturing engagement patterns from both fake and authentic\ntrends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were\nconstructed by collecting interactions related to campaigns that stemmed from\nephemeral astroturfing. Established graph neural networks (GNNs) struggle to\naccurately classify campaign graphs, highlighting the challenges posed by LEN\ndue to the large size of its networks. To address this, we introduce a new\ngraph classification method that leverages the density of local network\nstructures. We propose a random weighted walk (RWW) approach in which node\ntransitions are biased by local density measures such as degree, core number,\nor truss number. These RWWs are encoded using the Skip-gram model, producing\ndensity-aware structural embeddings for the nodes. Training message-passing\nneural networks (MPNNs) on these density-aware embeddings yields superior\nresults compared to the simpler node features available in the dataset, with\nnearly a 12\\% and 5\\% improvement in accuracy for binary and multiclass\nclassification, respectively. Our findings demonstrate that incorporating\ndensity-aware structural encoding with MPNNs provides a robust framework for\nidentifying coordinated inauthentic behavior on social media networks such as\nTwitter."}
{"id": "2506.13946", "pdf": "https://arxiv.org/pdf/2506.13946", "abs": "https://arxiv.org/abs/2506.13946", "authors": ["Nikola Sandrić"], "title": "Rademacher learning rates for iterated random functions", "categories": ["stat.ML", "cs.LG", "math.PR", "68W40, 68T10, 60J05"], "comment": null, "summary": "Most existing literature on supervised machine learning assumes that the\ntraining dataset is drawn from an i.i.d. sample. However, many real-world\nproblems exhibit temporal dependence and strong correlations between the\nmarginal distributions of the data-generating process, suggesting that the\ni.i.d. assumption is often unrealistic. In such cases, models naturally include\ntime-series processes with mixing properties, as well as irreducible and\naperiodic ergodic Markov chains. Moreover, the learning rates typically\nobtained in these settings are independent of the data distribution, which can\nlead to restrictive choices of hypothesis classes and suboptimal sample\ncomplexities for the learning algorithm. In this article, we consider the case\nwhere the training dataset is generated by an iterated random function (i.e.,\nan iteratively defined time-homogeneous Markov chain) that is not necessarily\nirreducible or aperiodic. Under the assumption that the governing function is\ncontractive with respect to its first argument and subject to certain\nregularity conditions on the hypothesis class, we first establish a uniform\nconvergence result for the corresponding sample error. We then demonstrate the\nlearnability of the approximate empirical risk minimization algorithm and\nderive its learning rate bound. Both rates are data-distribution dependent,\nexpressed in terms of the Rademacher complexities of the underlying hypothesis\nclass, allowing them to more accurately reflect the properties of the\ndata-generating distribution."}
{"id": "2506.13947", "pdf": "https://arxiv.org/pdf/2506.13947", "abs": "https://arxiv.org/abs/2506.13947", "authors": ["Kazuto Fukuchi"], "title": "Meta Optimality for Demographic Parity Constrained Regression via Post-Processing", "categories": ["stat.ML", "cs.LG"], "comment": "ICML2025", "summary": "We address the regression problem under the constraint of demographic parity,\na commonly used fairness definition. Recent studies have revealed fair minimax\noptimal regression algorithms, the most accurate algorithms that adhere to the\nfairness constraint. However, these analyses are tightly coupled with specific\ndata generation models. In this paper, we provide meta-theorems that can be\napplied to various situations to validate the fair minimax optimality of the\ncorresponding regression algorithms. Furthermore, we demonstrate that fair\nminimax optimal regression can be achieved through post-processing methods,\nallowing researchers and practitioners to focus on improving conventional\nregression techniques, which can then be efficiently adapted for fair\nregression."}
{"id": "2506.13950", "pdf": "https://arxiv.org/pdf/2506.13950", "abs": "https://arxiv.org/abs/2506.13950", "authors": ["Dimitrios G. Patsatzis", "Nikolaos Kazantzis", "Ioannis G. Kevrekidis", "Constantinos Siettos"], "title": "A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems", "categories": ["math.NA", "cs.LG", "cs.NA", "math.DS", "37M10, 37M21, 37N30, 37N35, 65P99, 65D15, 68T07", "G.1.0; F.1.1; I.2.6"], "comment": "36 pages (31 pages of main text and Appendix, 5 of Supplement), 8\n  Figures (6 in the main text and Appendix and 2 in the Supplement)", "summary": "We propose a hybrid machine learning scheme to learn -- in physics-informed\nand numerical analysis-informed fashion -- invariant manifolds (IM) of discrete\nmaps for constructing reduced-order models (ROMs) for dynamical systems. The\nproposed scheme combines polynomial series with shallow neural networks,\nexploiting the complementary strengths of both approaches. Polynomials enable\nan efficient and accurate modeling of ROMs with guaranteed local exponential\nconvergence rate around the fixed point, where, under certain assumptions, the\nIM is demonstrated to be analytic. Neural networks provide approximations to\nmore complex structures beyond the reach of the polynomials' convergence. We\nevaluate the efficiency of the proposed scheme using three benchmark examples,\nexamining convergence behavior, numerical approximation accuracy, and\ncomputational training cost. Additionally, we compare the IM approximations\nobtained solely with neural networks and with polynomial expansions. We\ndemonstrate that the proposed hybrid scheme outperforms both pure polynomial\napproximations (power series, Legendre and Chebyshev polynomials) and\nstandalone shallow neural network approximations in terms of numerical\napproximation accuracy."}
{"id": "2506.13955", "pdf": "https://arxiv.org/pdf/2506.13955", "abs": "https://arxiv.org/abs/2506.13955", "authors": ["Matthew Lau", "Tian-Yi Zhou", "Xiangchi Yuan", "Jizhou Chen", "Wenke Lee", "Xiaoming Huo"], "title": "Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies", "categories": ["stat.ML", "cs.CR", "cs.LG", "stat.AP"], "comment": null, "summary": "Anomaly detection (AD) is a critical task across domains such as\ncybersecurity and healthcare. In the unsupervised setting, an effective and\ntheoretically-grounded principle is to train classifiers to distinguish normal\ndata from (synthetic) anomalies. We extend this principle to semi-supervised\nAD, where training data also include a limited labeled subset of anomalies\npossibly present in test time. We propose a theoretically-grounded and\nempirically effective framework for semi-supervised AD that combines known and\nsynthetic anomalies during training. To analyze semi-supervised AD, we\nintroduce the first mathematical formulation of semi-supervised AD, which\ngeneralizes unsupervised AD. Here, we show that synthetic anomalies enable (i)\nbetter anomaly modeling in low-density regions and (ii) optimal convergence\nguarantees for neural network classifiers -- the first theoretical result for\nsemi-supervised AD. We empirically validate our framework on five diverse\nbenchmarks, observing consistent performance gains. These improvements also\nextend beyond our theoretical framework to other classification-based AD\nmethods, validating the generalizability of the synthetic anomaly principle in\nAD."}
{"id": "2506.13963", "pdf": "https://arxiv.org/pdf/2506.13963", "abs": "https://arxiv.org/abs/2506.13963", "authors": ["Julian R. Rice", "Karthik Balaguru", "Fadia Ticona Rollano", "John Wilson", "Brent Daniel", "David Judi", "Ning Sun", "L. Ruby Leung"], "title": "Projecting U.S. coastal storm surge risks and impacts with deep learning", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs),\nyet assessing its current and future risk is difficult due to the phenomenon's\nrarity and physical complexity. Recent advances in artificial intelligence\napplications to natural hazard modeling suggest a new avenue for addressing\nthis problem. We utilize a deep learning storm surge model to efficiently\nestimate coastal surge risk in the United States from 900,000 synthetic TC\nevents, accounting for projected changes in TC behavior and sea levels. The\nderived historical 100-year surge (the event with a 1% yearly exceedance\nprobability) agrees well with historical observations and other modeling\ntechniques. When coupled with an inundation model, we find that heightened TC\nintensities and sea levels by the end of the century result in a 50% increase\nin population at risk. Key findings include markedly heightened risk in\nFlorida, and critical thresholds identified in Georgia and South Carolina."}
{"id": "2506.13964", "pdf": "https://arxiv.org/pdf/2506.13964", "abs": "https://arxiv.org/abs/2506.13964", "authors": ["Yusdivia Molina-Román", "David Gómez-Ortiz", "Ernestina Menasalvas-Ruiz", "José Gerardo Tamez-Peña", "Alejandro Santos-Díaz"], "title": "Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography", "categories": ["eess.IV", "cs.LG"], "comment": "6 pages, 4 figures", "summary": "Mammographic breast density classification is essential for cancer risk\nassessment but remains challenging due to subjective interpretation and\ninter-observer variability. This study compares multimodal and CNN-based\nmethods for automated classification using the BI-RADS system, evaluating\nBioMedCLIP and ConvNeXt across three learning scenarios: zero-shot\nclassification, linear probing with textual descriptions, and fine-tuning with\nnumerical labels. Results show that zero-shot classification achieved modest\nperformance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP\nlinear probe. Although linear probing demonstrated potential with pretrained\nembeddings, it was less effective than full fine-tuning. These findings suggest\nthat despite the promise of multimodal learning, CNN-based models with\nend-to-end fine-tuning provide stronger performance for specialized medical\nimaging. The study underscores the need for more detailed textual\nrepresentations and domain-specific adaptations in future radiology\napplications."}
{"id": "2506.13971", "pdf": "https://arxiv.org/pdf/2506.13971", "abs": "https://arxiv.org/abs/2506.13971", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior.\nHowever, the subjective moments of negative experience, where the conversation\nloses fluidity or enjoyment remain understudied. These moments are infrequent\nin naturalistic data, and thus training a supervised learning (SL) model\nrequires costly manual data annotation. We applied semi-supervised learning\n(SSL) to leverage targeted labeled and unlabeled clips for training multimodal\n(audio, facial, text) deep features to predict non-fluid or unenjoyable moments\nin holdout videoconference sessions. The modality-fused co-training SSL\nachieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by\nup to 4% with the same amount of labeled data. Remarkably, the best SSL model\nwith just 8% labeled data matched 96% of the SL model's full-data performance.\nThis shows an annotation-efficient framework for modeling videoconference\nexperience."}
{"id": "2506.13984", "pdf": "https://arxiv.org/pdf/2506.13984", "abs": "https://arxiv.org/abs/2506.13984", "authors": ["Andrzej Cichocki"], "title": "Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we develop a wide class Mirror Descent (MD) algorithms, which\nplay a key role in machine learning. For this purpose we formulated the\nconstrained optimization problem, in which we exploits the Bregman divergence\nwith the Tempesta multi-parametric deformation logarithm as a link function.\nThis link function called also mirror function defines the mapping between the\nprimal and dual spaces and is associated with a very-wide (in fact,\ntheoretically infinite) class of generalized trace-form entropies. In order to\nderive novel MD updates, we estimate generalized exponential function, which\nclosely approximates the inverse of the multi-parametric Tempesta generalized\nlogarithm. The shape and properties of the Tempesta logarithm and its\ninverse-deformed exponential functions can be tuned by several hyperparameters.\nBy learning these hyperparameters, we can adapt to distribution or geometry of\ntraining data, and we can adjust them to achieve desired properties of MD\nalgorithms. The concept of applying multi-parametric logarithms allow us to\ngenerate a new wide and flexible family of MD and mirror-less MD updates."}
{"id": "2506.13989", "pdf": "https://arxiv.org/pdf/2506.13989", "abs": "https://arxiv.org/abs/2506.13989", "authors": ["Johan Östman", "Edvin Callisen", "Anton Chen", "Kristiina Ausmees", "Emanuel Gårdh", "Jovan Zamac", "Jolanta Goldsteine", "Hugo Wefer", "Simon Whelan", "Markus Reimegård"], "title": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering", "categories": ["cs.SI", "cs.AI", "cs.DB", "cs.LG"], "comment": "21 figures, 22 pages", "summary": "Money laundering enables organized crime by allowing illicit funds to enter\nthe legitimate economy. Although trillions of dollars are laundered each year,\nonly a small fraction is ever uncovered. This stems from a range of factors,\nincluding deliberate evasion by launderers, the rarity of confirmed cases, and\nthe limited visibility each financial institution has into the global\ntransaction network. While several synthetic datasets are available, they fail\nto model the structural and behavioral complexity of real-world money\nlaundering. In particular, they often overlook partial observability, sparse\nand uncertain labels, strategic behavior, temporal dynamics, class imbalance,\nand network-level dependencies. To address these limitations, we present\nAMLGentex, an open-source suite for generating realistic, configurable\ntransaction data and benchmarking detection methods. It enables systematic\nevaluation of anti-money laundering (AML) systems in a controlled environment\nthat captures key real-world challenges. We demonstrate how the framework can\nbe used to rigorously evaluate methods under conditions that reflect the\ncomplexity of practical AML scenarios."}
{"id": "2506.13993", "pdf": "https://arxiv.org/pdf/2506.13993", "abs": "https://arxiv.org/abs/2506.13993", "authors": ["Michelangelo Conserva", "Alex Wilson", "Charlotte Stanton", "Vishal Batchu", "Varun Gulshan"], "title": "Mapping Farmed Landscapes from Remote Sensing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity."}
{"id": "2506.14006", "pdf": "https://arxiv.org/pdf/2506.14006", "abs": "https://arxiv.org/abs/2506.14006", "authors": ["Alexei V. Tkachenko", "Bortolo Matteo Mognetti", "Sergei Maslov"], "title": "Evolutionary chemical learning in dimerization networks", "categories": ["cond-mat.stat-mech", "cond-mat.dis-nn", "cs.LG", "nlin.AO", "physics.data-an", "q-bio.MN"], "comment": "7 pages, 5 figures + SI", "summary": "We present a novel framework for chemical learning based on Competitive\nDimerization Networks (CDNs) - systems in which multiple molecular species,\ne.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show\nthat these networks can be trained in vitro through directed evolution,\nenabling the implementation of complex learning tasks such as multiclass\nclassification without digital hardware or explicit parameter tuning. Each\nmolecular species functions analogously to a neuron, with binding affinities\nacting as tunable synaptic weights. A training protocol involving mutation,\nselection, and amplification of DNA-based components allows CDNs to robustly\ndiscriminate among noisy input patterns. The resulting classifiers exhibit\nstrong output contrast and high mutual information between input and output,\nespecially when guided by a contrast-enhancing loss function. Comparative\nanalysis with in silico gradient descent training reveals closely correlated\nperformance. These results establish CDNs as a promising platform for analog\nphysical computation, bridging synthetic biology and machine learning, and\nadvancing the development of adaptive, energy-efficient molecular computing\nsystems."}
{"id": "2506.14022", "pdf": "https://arxiv.org/pdf/2506.14022", "abs": "https://arxiv.org/abs/2506.14022", "authors": ["Jacob B. Landsberg", "Elizabeth A. Barnes", "Matthew Newman"], "title": "AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction", "categories": ["physics.ao-ph", "cs.LG"], "comment": "23 pages, 12 figures", "summary": "Subseasonal-to-seasonal forecasting is crucial for public health, disaster\npreparedness, and agriculture, and yet it remains a particularly challenging\ntimescale to predict. We explore the use of an interpretable AI-informed model\nanalog forecasting approach, previously employed on longer timescales, to\nimprove S2S predictions. Using an artificial neural network, we learn a mask of\nweights to optimize analog selection and showcase its versatility across three\nvaried prediction tasks: 1) classification of Week 3-4 Southern California\nsummer temperatures; 2) regional regression of Month 1 midwestern U.S. summer\ntemperatures; and 3) classification of Month 1-2 North Atlantic wintertime\nupper atmospheric winds. The AI-informed analogs outperform traditional analog\nforecasting approaches, as well as climatology and persistence baselines, for\ndeterministic and probabilistic skill metrics on both climate model and\nreanalysis data. We find the analog ensembles built using the AI-informed\napproach also produce better predictions of temperature extremes and improve\nrepresentation of forecast uncertainty. Finally, by using an interpretable-AI\nframework, we analyze the learned masks of weights to better understand S2S\nsources of predictability."}
{"id": "2506.14034", "pdf": "https://arxiv.org/pdf/2506.14034", "abs": "https://arxiv.org/abs/2506.14034", "authors": ["Brian Tsan", "Abylay Amanbayev", "Asoke Datta", "Florin Rusu"], "title": "Sketched Sum-Product Networks for Joins", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Sketches have shown high accuracy in multi-way join cardinality estimation, a\ncritical problem in cost-based query optimization. Accurately estimating the\ncardinality of a join operation -- analogous to its computational cost --\nallows the optimization of query execution costs in relational database\nsystems. However, although sketches have shown high efficacy in query\noptimization, they are typically constructed specifically for predefined\nselections in queries that are assumed to be given a priori, hindering their\napplicability to new queries. As a more general solution, we propose for\nSum-Product Networks to dynamically approximate sketches on-the-fly.\nSum-Product Networks can decompose and model multivariate distributions, such\nas relations, as linear combinations of multiple univariate distributions. By\nrepresenting these univariate distributions as sketches, Sum-Product Networks\ncan combine them element-wise to efficiently approximate the sketch of any\nquery selection. These approximate sketches can then be applied to join\ncardinality estimation. In particular, we implement the Fast-AGMS and Bound\nSketch methods, which have successfully been used in prior work, despite their\ncostly construction. By accurately approximating them instead, our work\nprovides a practical alternative to apply these sketches to query optimization."}
{"id": "2506.14051", "pdf": "https://arxiv.org/pdf/2506.14051", "abs": "https://arxiv.org/abs/2506.14051", "authors": ["Jiyuan Tan", "Jose Blanchet", "Vasilis Syrgkanis"], "title": "Estimation of Treatment Effects in Extreme and Unobserved Data", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Causal effect estimation seeks to determine the impact of an intervention\nfrom observational data. However, the existing causal inference literature\nprimarily addresses treatment effects on frequently occurring events. But what\nif we are interested in estimating the effects of a policy intervention whose\nbenefits, while potentially important, can only be observed and measured in\nrare yet impactful events, such as extreme climate events? The standard causal\ninference methodology is not designed for this type of inference since the\nevents of interest may be scarce in the observed data and some degree of\nextrapolation is necessary. Extreme Value Theory (EVT) provides methodologies\nfor analyzing statistical phenomena in such extreme regimes. We introduce a\nnovel framework for assessing treatment effects in extreme data to capture the\ncausal effect at the occurrence of rare events of interest. In particular, we\nemploy the theory of multivariate regular variation to model extremities. We\ndevelop a consistent estimator for extreme treatment effects and present a\nrigorous non-asymptotic analysis of its performance. We illustrate the\nperformance of our estimator using both synthetic and semi-synthetic data."}
{"id": "2506.14110", "pdf": "https://arxiv.org/pdf/2506.14110", "abs": "https://arxiv.org/abs/2506.14110", "authors": ["Steve Hanneke", "Mingyue Xu"], "title": "Universal Rates of ERM for Agnostic Learning", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2025", "summary": "The universal learning framework has been developed to obtain guarantees on\nthe learning rates that hold for any fixed distribution, which can be much\nfaster than the ones uniformly hold over all the distributions. Given that the\nEmpirical Risk Minimization (ERM) principle being fundamental in the PAC theory\nand ubiquitous in practical machine learning, the recent work of\narXiv:2412.02810 studied the universal rates of ERM for binary classification\nunder the realizable setting. However, the assumption of realizability is too\nrestrictive to hold in practice. Indeed, the majority of the literature on\nuniversal learning has focused on the realizable case, leaving the\nnon-realizable case barely explored.\n  In this paper, we consider the problem of universal learning by ERM for\nbinary classification under the agnostic setting, where the ''learning curve\"\nreflects the decay of the excess risk as the sample size increases. We explore\nthe possibilities of agnostic universal rates and reveal a compact trichotomy:\nthere are three possible agnostic universal rates of ERM, being either\n$e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete\ncharacterization of which concept classes fall into each of these categories.\nMoreover, we also establish complete characterizations for the target-dependent\nuniversal rates as well as the Bayes-dependent universal rates."}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111", "abs": "https://arxiv.org/abs/2506.14111", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "title": "Essential-Web v1.0: 24T tokens of organized web data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"}
{"id": "2506.14123", "pdf": "https://arxiv.org/pdf/2506.14123", "abs": "https://arxiv.org/abs/2506.14123", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "title": "Sampling from Your Language Model One Byte at a Time", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals."}
{"id": "2506.14186", "pdf": "https://arxiv.org/pdf/2506.14186", "abs": "https://arxiv.org/abs/2506.14186", "authors": ["Anselm Paulus", "A. René Geist", "Pierre Schumacher", "Vít Musil", "Georg Martius"], "title": "Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "I.2.9; I.2.6; I.6.4; G.1.6"], "comment": null, "summary": "Contact forces pose a major challenge for gradient-based optimization of\nrobot dynamics as they introduce jumps in the system's velocities.\nPenalty-based simulators, such as MuJoCo, simplify gradient computation by\nsoftening the contact forces. However, realistically simulating hard contacts\nrequires very stiff contact settings, which leads to incorrect gradients when\nusing automatic differentiation. On the other hand, using non-stiff settings\nstrongly increases the sim-to-real gap. We analyze the contact computation of\npenalty-based simulators to identify the causes of gradient errors. Then, we\npropose DiffMJX, which combines adaptive integration with MuJoCo XLA, to\nnotably improve gradient quality in the presence of hard contacts. Finally, we\naddress a key limitation of contact gradients: they vanish when objects do not\ntouch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism\nthat enables the simulator to generate informative contact gradients even\nbefore objects are in contact. To preserve physical realism, we apply CFD only\nin the backward pass using a straight-through trick, allowing us to compute\nuseful gradients without modifying the forward simulation."}
{"id": "2506.14198", "pdf": "https://arxiv.org/pdf/2506.14198", "abs": "https://arxiv.org/abs/2506.14198", "authors": ["Jeremy A. Collins", "Loránd Cheng", "Kunal Aneja", "Albert Wilcox", "Benjamin Joffe", "Animesh Garg"], "title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Action-labeled data for robotics is scarce and expensive, limiting the\ngeneralization of learned policies. In contrast, vast amounts of action-free\nvideo data are readily available, but translating these observations into\neffective policies remains a challenge. We introduce AMPLIFY, a novel framework\nthat leverages large-scale video data by encoding visual dynamics into compact,\ndiscrete motion tokens derived from keypoint trajectories. Our modular approach\nseparates visual motion prediction from action inference, decoupling the\nchallenges of learning what motion defines a task from how robots can perform\nit. We train a forward dynamics model on abundant action-free videos and an\ninverse dynamics model on a limited set of action-labeled examples, allowing\nfor independent scaling. Extensive evaluations demonstrate that the learned\ndynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x\nbetter pixel prediction accuracy compared to prior approaches, and broadly\nuseful. In downstream policy learning, our dynamics predictions enable a\n1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by\nlearning from action-free human videos, and the first generalization to LIBERO\ntasks from zero in-distribution action data. Beyond robotic control, we find\nthe dynamics learned by AMPLIFY to be a versatile latent world model, enhancing\nvideo prediction quality. Our results present a novel paradigm leveraging\nheterogeneous data sources to build efficient, generalizable world models. More\ninformation can be found at https://amplify-robotics.github.io/."}
{"id": "2506.14239", "pdf": "https://arxiv.org/pdf/2506.14239", "abs": "https://arxiv.org/abs/2506.14239", "authors": ["Louis Vervoort", "Vitaly Nikolaev"], "title": "Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by Journal for General Philosophy of Science", "summary": "We propose a test for abstract causal reasoning in AI, based on scholarship\nin the philosophy of causation, in particular on the neuron diagrams\npopularized by D. Lewis. We illustrate the test on advanced Large Language\nModels (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already\ncapable of correctly identifying causes in cases that are hotly debated in the\nliterature. In order to assess the results of these LLMs and future dedicated\nAI, we propose a definition of cause in neuron diagrams with a wider validity\nthan published hitherto, which challenges the widespread view that such a\ndefinition is elusive. We submit that these results are an illustration of how\nfuture philosophical research might evolve: as an interplay between human and\nartificial expertise."}
{"id": "2506.14270", "pdf": "https://arxiv.org/pdf/2506.14270", "abs": "https://arxiv.org/abs/2506.14270", "authors": ["Gijs Vermariën", "Thomas G. Bisbas", "Serena Viti", "Yue Zhao", "Xuefei Tang", "Rahul Ravichandran"], "title": "NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions", "categories": ["astro-ph.GA", "cs.LG"], "comment": "Accepted for publication in Machine Learning: Science and Technology.\n  Focus on ML and the Physical Sciences, Mach. Learn.: Sci. Technol (2025)", "summary": "Computational astrochemical models are essential for helping us interpret and\nunderstand the observations of different astrophysical environments. In the age\nof high-resolution telescopes such as JWST and ALMA, the substructure of many\nobjects can be resolved, raising the need for astrochemical modeling at these\nsmaller scales, meaning that the simulations of these objects need to include\nboth the physics and chemistry to accurately model the observations. The\ncomputational cost of the simulations coupling both the three-dimensional\nhydrodynamics and chemistry is enormous, creating an opportunity for surrogate\nmodels that can effectively substitute the chemical solver. In this work we\npresent surrogate models that can replace the original chemical code, namely\nLatent Augmented Neural Ordinary Differential Equations. We train these\nsurrogate architectures on three datasets of increasing physical complexity,\nwith the last dataset derived directly from a three-dimensional simulation of a\nmolecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show\nthat these surrogate models can provide speedup and reproduce the original\nobservable column density maps of the dataset. This enables the rapid inference\nof the chemistry (on the GPU), allowing for the faster statistical inference of\nobservations or increasing the resolution in hydrodynamical simulations of\nastrophysical environments."}
{"id": "2506.14276", "pdf": "https://arxiv.org/pdf/2506.14276", "abs": "https://arxiv.org/abs/2506.14276", "authors": ["Jack Cole", "Mohamed Osman"], "title": "Don't throw the baby out with the bathwater: How and why deep learning for ARC", "categories": ["cs.AI", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable\nchallenge for AI systems. Despite the typically low performance on ARC, the\ndeep learning paradigm remains the most effective known strategy for generating\nskillful (state-of-the-art) neural networks (NN) across varied modalities and\ntasks in vision, language etc. The deep learning paradigm has proven to be able\nto train these skillful neural networks and learn the abstractions needed in\nthese diverse domains. Our work doubles down on that and continues to leverage\nthis paradigm by incorporating on-the-fly NN training at test time. We\ndemonstrate that fully committing to deep learning's capacity to acquire novel\nabstractions yields state-of-the-art performance on ARC. Specifically, we treat\nboth the neural network and the optimizer (rather than just a pre-trained\nnetwork) as integral components of the inference process, fostering\ngeneralization to unseen tasks. Concretely, we propose a methodology for\ntraining on ARC, starting from pretrained LLMs, and enhancing their ARC\nreasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment\nInference Reverse-Augmentation and Vote (AIRV) as effective test-time\ntechniques. We are the first to propose and show deep learning can be used\neffectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a\nfurther 300% boost with TTFT. An early version of this approach secured first\nplace in the 2023 ARCathon competition, while the final version achieved the\ncurrent best score on the ARC private test-set (58%). Our findings highlight\nthe key ingredients of a robust reasoning system in unfamiliar domains,\nunderscoring the central mechanisms that improve broad perceptual reasoning."}
{"id": "2506.14287", "pdf": "https://arxiv.org/pdf/2506.14287", "abs": "https://arxiv.org/abs/2506.14287", "authors": ["Yanwei Wang"], "title": "Steering Robots with Inference-Time Interactions", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "MIT Robotics PhD Thesis", "summary": "Imitation learning has driven the development of generalist policies capable\nof autonomously solving multiple tasks. However, when a pretrained policy makes\nerrors during deployment, there are limited mechanisms for users to correct its\nbehavior. While collecting additional data for finetuning can address such\nissues, doing so for each downstream use case is inefficient at deployment. My\nresearch proposes an alternative: keeping pretrained policies frozen as a fixed\nskill repertoire while allowing user interactions to guide behavior generation\ntoward user preferences at inference time. By making pretrained policies\nsteerable, users can help correct policy errors when the model struggles to\ngeneralize-without needing to finetune the policy. Specifically, I propose (1)\ninference-time steering, which leverages user interactions to switch between\ndiscrete skills, and (2) task and motion imitation, which enables user\ninteractions to edit continuous motions while satisfying task constraints\ndefined by discrete symbolic plans. These frameworks correct misaligned policy\npredictions without requiring additional training, maximizing the utility of\npretrained models while achieving inference-time user objectives."}
{"id": "2506.14293", "pdf": "https://arxiv.org/pdf/2506.14293", "abs": "https://arxiv.org/abs/2506.14293", "authors": ["Tawsif Ahmed", "Andrej Radonjic", "Gollam Rabby"], "title": "SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music\nand song. To the best of our knowledge, there are no open-source high-quality\ndataset representing popular and well-known songs for generative music modeling\ntasks such as text-music, music-captioning, singing-voice synthesis, melody\nreconstruction and cross-model retrieval. Past contributions focused on\nisolated and constrained factors whose core perspective was to create synthetic\nor re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily\nlarge-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another\nfocus for the community. Unfortunately, adoption of these datasets has been\nbelow substantial in the generative music community as these datasets fail to\nreflect real-world music and its flavour. Our dataset changes this narrative\nand provides a dataset that is constructed using actual popular music and\nworld-renowned artists."}
{"id": "2506.14322", "pdf": "https://arxiv.org/pdf/2506.14322", "abs": "https://arxiv.org/abs/2506.14322", "authors": ["Avigail Cohen Rimon", "Mirela Ben-Chen", "Or Litany"], "title": "FRIDU: Functional Map Refinement with Guided Image Diffusion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to SGP 2025 (Symposium on Geometry Processing)", "summary": "We propose a novel approach for refining a given correspondence map between\ntwo shapes. A correspondence map represented as a functional map, namely a\nchange of basis matrix, can be additionally treated as a 2D image. With this\nperspective, we train an image diffusion model directly in the space of\nfunctional maps, enabling it to generate accurate maps conditioned on an\ninaccurate initial map. The training is done purely in the functional space,\nand thus is highly efficient. At inference time, we use the pointwise map\ncorresponding to the current functional map as guidance during the diffusion\nprocess. The guidance can additionally encourage different functional map\nobjectives, such as orthogonality and commutativity with the Laplace-Beltrami\noperator. We show that our approach is competitive with state-of-the-art\nmethods of map refinement and that guided diffusion models provide a promising\npathway to functional map processing."}
{"id": "2506.14329", "pdf": "https://arxiv.org/pdf/2506.14329", "abs": "https://arxiv.org/abs/2506.14329", "authors": ["Rickmer Schulte", "David Rügamer", "Thomas Nagler"], "title": "Adjustment for Confounding using Pre-Trained Representations", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "comment": "Accepted at ICML 2025", "summary": "There is growing interest in extending average treatment effect (ATE)\nestimation to incorporate non-tabular data, such as images and text, which may\nact as sources of confounding. Neglecting these effects risks biased results\nand flawed scientific conclusions. However, incorporating non-tabular data\nnecessitates sophisticated feature extractors, often in combination with ideas\nof transfer learning. In this work, we investigate how latent features from\npre-trained neural networks can be leveraged to adjust for sources of\nconfounding. We formalize conditions under which these latent features enable\nvalid adjustment and statistical inference in ATE estimation, demonstrating\nresults along the example of double machine learning. We discuss critical\nchallenges inherent to latent feature learning and downstream parameter\nestimation arising from the high dimensionality and non-identifiability of\nrepresentations. Common structural assumptions for obtaining fast convergence\nrates with additive or sparse linear models are shown to be unrealistic for\nlatent features. We argue, however, that neural networks are largely\ninsensitive to these issues. In particular, we show that neural networks can\nachieve fast convergence rates by adapting to intrinsic notions of sparsity and\ndimension of the learning problem."}
{"id": "2506.14374", "pdf": "https://arxiv.org/pdf/2506.14374", "abs": "https://arxiv.org/abs/2506.14374", "authors": ["Wai Man Si", "Mingjie Li", "Michael Backes", "Yang Zhang"], "title": "Excessive Reasoning Attack on Reasoning LLMs", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Recent reasoning large language models (LLMs), such as OpenAI o1 and\nDeepSeek-R1, exhibit strong performance on complex tasks through test-time\ninference scaling. However, prior studies have shown that these models often\nincur significant computational costs due to excessive reasoning, such as\nfrequent switching between reasoning trajectories (e.g., underthinking) or\nredundant reasoning on simple questions (e.g., overthinking). In this work, we\nexpose a novel threat: adversarial inputs can be crafted to exploit excessive\nreasoning behaviors and substantially increase computational overhead without\ncompromising model utility. Therefore, we propose a novel loss framework\nconsisting of three components: (1) Priority Cross-Entropy Loss, a modification\nof the standard cross-entropy objective that emphasizes key tokens by\nleveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss,\nwhich encourages the model to initiate additional reasoning paths during\ninference; and (3) Delayed Termination Loss, which is designed to extend the\nreasoning process and defer the generation of final outputs. We optimize and\nevaluate our attack for the GSM8K and ORCA datasets on\nDeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results\ndemonstrate a 3x to 9x increase in reasoning length with comparable utility\nperformance. Furthermore, our crafted adversarial inputs exhibit\ntransferability, inducing computational overhead in o3-mini, o1-mini,\nDeepSeek-R1, and QWQ models."}
{"id": "2506.14412", "pdf": "https://arxiv.org/pdf/2506.14412", "abs": "https://arxiv.org/abs/2506.14412", "authors": ["Tim Cofala", "Oleh Astappiev", "William Xion", "Hailay Teklehaymanot"], "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "4 pages, 5 figures. Report for SIGIR 2025 LiveRAG Challenge", "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR\n2025 LiveRAG Challenge."}
{"id": "2506.14435", "pdf": "https://arxiv.org/pdf/2506.14435", "abs": "https://arxiv.org/abs/2506.14435", "authors": ["Hongyu Wang", "Jiayu Xu", "Ruiping Wang", "Yan Feng", "Yitao Zhai", "Peng Pei", "Xunliang Cai", "Xilin Chen"], "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices."}
{"id": "2506.14440", "pdf": "https://arxiv.org/pdf/2506.14440", "abs": "https://arxiv.org/abs/2506.14440", "authors": ["David E. Hernandez", "Jose Chang", "Torbjörn E. M. Nordling"], "title": "Model compression using knowledge distillation with integrated gradients", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T05, 68T07", "I.2.6; I.4.2; I.4.9"], "comment": "49 pages, 12 figures", "summary": "Model compression is critical for deploying deep learning models on\nresource-constrained devices. We introduce a novel method enhancing knowledge\ndistillation with integrated gradients (IG) as a data augmentation strategy.\nOur approach overlays IG maps onto input images during training, providing\nstudent models with deeper insights into teacher models' decision-making\nprocesses. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented\nknowledge distillation achieves 92.6% testing accuracy with a 4.1x compression\nfactor-a significant 1.1 percentage point improvement ($p<0.001$) over\nnon-distilled models (91.5%). This compression reduces inference time from 140\nms to 13 ms. Our method precomputes IG maps before training, transforming\nsubstantial runtime costs into a one-time preprocessing step. Our comprehensive\nexperiments include: (1) comparisons with attention transfer, revealing\ncomplementary benefits when combined with our approach; (2) Monte Carlo\nsimulations confirming statistical robustness; (3) systematic evaluation of\ncompression factor versus accuracy trade-offs across a wide range (2.2x-1122x);\nand (4) validation on an ImageNet subset aligned with CIFAR-10 classes,\ndemonstrating generalisability beyond the initial dataset. These extensive\nablation studies confirm that IG-based knowledge distillation consistently\noutperforms conventional approaches across varied architectures and compression\nratios. Our results establish this framework as a viable compression technique\nfor real-world deployment on edge devices while maintaining competitive\naccuracy."}
{"id": "2506.14464", "pdf": "https://arxiv.org/pdf/2506.14464", "abs": "https://arxiv.org/abs/2506.14464", "authors": ["Maximilian Baronig", "Yeganeh Bahariasl", "Ozan Özdenizci", "Robert Legenstein"], "title": "A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Recurrent spiking neural networks (RSNNs) can be implemented very efficiently\nin neuromorphic systems. Nevertheless, training of these models with powerful\ngradient-based learning algorithms is mostly performed on standard digital\nhardware using Backpropagation through time (BPTT). However, BPTT has\nsubstantial limitations. It does not permit online training and its memory\nconsumption scales linearly with the number of computation steps. In contrast,\nlearning methods using forward propagation of gradients operate in an online\nmanner with a memory consumption independent of the number of time steps. These\nmethods enable SNNs to learn from continuous, infinite-length input sequences.\nYet, slow execution speed on conventional hardware as well as inferior\nperformance has hindered their widespread application. In this work, we\nintroduce HYbrid PRopagation (HYPR) that combines the efficiency of\nparallelization with approximate online forward learning. Our algorithm yields\nhigh-throughput online learning through parallelization, paired with constant,\ni.e., sequence length independent, memory demands. HYPR enables parallelization\nof parameter update computation over the sub sequences for RSNNs consisting of\nalmost arbitrary non-linear spiking neuron models. We apply HYPR to networks of\nspiking neurons with oscillatory subthreshold dynamics. We find that this type\nof neuron model is particularly well trainable by HYPR, resulting in an\nunprecedentedly low task performance gap between approximate forward gradient\nlearning and BPTT."}
{"id": "2506.14473", "pdf": "https://arxiv.org/pdf/2506.14473", "abs": "https://arxiv.org/abs/2506.14473", "authors": ["Zhijing Wan", "Zhixiang Wang", "Zheng Wang", "Xin Xu", "Shin'ichi Satoh"], "title": "Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages, 10 figures, accepted by ICML 2025", "summary": "One-shot subset selection serves as an effective tool to reduce deep learning\ntraining costs by identifying an informative data subset based on the\ninformation extracted by an information extractor (IE). Traditional IEs,\ntypically pre-trained on the target dataset, are inherently dataset-dependent.\nFoundation models (FMs) offer a promising alternative, potentially mitigating\nthis limitation. This work investigates two key questions: (1) Can FM-based\nsubset selection outperform traditional IE-based methods across diverse\ndatasets? (2) Do all FMs perform equally well as IEs for subset selection?\nExtensive experiments uncovered surprising insights: FMs consistently\noutperform traditional IEs on fine-grained datasets, whereas their advantage\ndiminishes on coarse-grained datasets with noisy labels. Motivated by these\nfinding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a\nmethod tailored for fine-grained image datasets. RAM-APL leverages multiple FMs\nto enhance subset selection by exploiting their complementary strengths. Our\napproach achieves state-of-the-art performance on fine-grained datasets,\nincluding Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011."}
{"id": "2506.14479", "pdf": "https://arxiv.org/pdf/2506.14479", "abs": "https://arxiv.org/abs/2506.14479", "authors": ["Wonyoung Kim"], "title": "Adaptive Data Augmentation for Thompson Sampling", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In linear contextual bandits, the objective is to select actions that\nmaximize cumulative rewards, modeled as a linear function with unknown\nparameters. Although Thompson Sampling performs well empirically, it does not\nachieve optimal regret bounds. This paper proposes a nearly minimax optimal\nThompson Sampling for linear contextual bandits by developing a novel estimator\nwith the adaptive augmentation and coupling of the hypothetical samples that\nare designed for efficient parameter learning. The proposed estimator\naccurately predicts rewards for all arms without relying on assumptions for the\ncontext distribution. Empirical results show robust performance and significant\nimprovement over existing methods."}
{"id": "2506.14488", "pdf": "https://arxiv.org/pdf/2506.14488", "abs": "https://arxiv.org/abs/2506.14488", "authors": ["Dong Xu", "Zhangfan Yang", "Ka-chun Wong", "Zexuan Zhu", "Jiangqiang Li", "Junkai Ji"], "title": "Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion", "categories": ["q-bio.BM", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Breakthroughs in high-accuracy protein structure prediction, such as\nAlphaFold, have established receptor-based molecule design as a critical driver\nfor rapid early-phase drug discovery. However, most approaches still struggle\nto balance pocket-specific geometric fit with strict valence and synthetic\nconstraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion\ntermed READ is introduced, which is the first to merge molecular\nRetrieval-Augmented Generation with an SE(3)-equivariant diffusion model.\nSpecifically, a contrastively pre-trained encoder aligns atom-level\nrepresentations during training, then retrieves graph embeddings of\npocket-matched scaffolds to guide each reverse-diffusion step at inference.\nThis single mechanism can inject real-world chemical priors exactly where\nneeded, producing valid, diverse, and shape-complementary ligands. Experimental\nresults demonstrate that READ can achieve very competitive performance in\nCBGBench, surpassing state-of-the-art generative models and even native\nligands. That suggests retrieval and diffusion can be co-optimized for faster,\nmore reliable structure-based drug design."}
{"id": "2506.14530", "pdf": "https://arxiv.org/pdf/2506.14530", "abs": "https://arxiv.org/abs/2506.14530", "authors": ["Anastasis Kratsios", "Tin Sum Cheng", "Aurelien Lucchi", "Haitz Sáez de Ocáriz Borde"], "title": "Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters", "categories": ["stat.ML", "cs.AI", "cs.LG", "cs.NE", "math.ST", "stat.TH"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted\nparameter-efficient fine-tuning (PEFT) technique for foundation models. Recent\nwork has highlighted an inherent asymmetry in the initialization of LoRA's\nlow-rank factors, which has been present since its inception and was presumably\nderived experimentally. This paper focuses on providing a comprehensive\ntheoretical characterization of asymmetric LoRA with frozen random factors.\nFirst, while existing research provides upper-bound generalization guarantees\nbased on averages over multiple experiments, the behaviour of a single\nfine-tuning run with specific random factors remains an open question. We\naddress this by investigating the concentration of the typical LoRA\ngeneralization gap around its mean. Our main upper bound reveals a sample\ncomplexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$ with\nhigh probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we\nalso determine the fundamental limits in terms of sample efficiency,\nestablishing a matching lower bound of\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$. By more closely reflecting the\npractical scenario of a single fine-tuning run, our findings offer crucial\ninsights into the reliability and practicality of asymmetric LoRA."}
{"id": "2506.14560", "pdf": "https://arxiv.org/pdf/2506.14560", "abs": "https://arxiv.org/abs/2506.14560", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA)\nrisk by enabling early detection and disease monitoring. Recent machine\nlearning methods have improved risk estimation (i.e., predicting the likelihood\nof disease progression) and predictive modelling (i.e., the forecasting of\nfuture outcomes based on current data) using medical images, but clinical\nadoption remains limited due to their lack of interpretability. Existing\napproaches that generate future images for risk estimation are complex and\nimpractical. Additionally, previous methods fail to localize anatomical knee\nlandmarks, limiting interpretability. We address these gaps with a new\ninterpretable machine learning method to estimate the risk of knee OA\nprogression via multi-task predictive modelling that classifies future knee OA\nseverity and predicts anatomical knee landmarks from efficiently generated\nhigh-quality future images. Such image generation is achieved by leveraging a\ndiffusion model in a class-conditioned latent space to forecast disease\nprogression, offering a visual representation of how particular health\nconditions may evolve. Applied to the Osteoarthritis Initiative dataset, our\napproach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71\nin predicting knee OA progression while offering ~9% faster inference time."}
{"id": "2506.14562", "pdf": "https://arxiv.org/pdf/2506.14562", "abs": "https://arxiv.org/abs/2506.14562", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines."}
{"id": "2506.14571", "pdf": "https://arxiv.org/pdf/2506.14571", "abs": "https://arxiv.org/abs/2506.14571", "authors": ["Venkatakrishnan Vaidyanathapuram Krishnan", "Nathaniel Condit-Schultz"], "title": "The Perception of Phase Intercept Distortion and its Application in Data Augmentation", "categories": ["eess.SP", "cs.LG", "eess.AS"], "comment": "Submitted to the IEEE Workshop on Applications of Signal Processing\n  to Audio and Acoustics (WASPAA) 2025", "summary": "Phase distortion refers to the alteration of the phase relationships between\nfrequencies in a signal, which can be perceptible. In this paper, we discuss a\nspecial case of phase distortion known as phase-intercept distortion, which is\ncreated by a frequency-independent phase shift. We hypothesize that, though\nthis form of distortion changes a signal's waveform significantly, the\ndistortion is imperceptible. Human-subject experiment results are reported\nwhich are consistent with this hypothesis. Furthermore, we discuss how the\nimperceptibility of phase-intercept distortion can be useful for machine\nlearning, specifically for data augmentation. We conducted multiple experiments\nusing phase-intercept distortion as a novel approach to data augmentation, and\nobtained improved results for audio machine learning tasks."}
{"id": "2506.14582", "pdf": "https://arxiv.org/pdf/2506.14582", "abs": "https://arxiv.org/abs/2506.14582", "authors": ["Kaleel Mahmood", "Caleb Manicke", "Ethan Rathbun", "Aayushi Verma", "Sohaib Ahmad", "Nicholas Stamatakis", "Laurent Michel", "Benjamin Fuller"], "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "18 Pages. Author version of article to appear at CCS 2025", "summary": "We show the security risk associated with using machine learning classifiers\nin United States election tabulators. The central classification task in\nelection tabulation is deciding whether a mark does or does not appear on a\nbubble associated to an alternative in a contest on the ballot. Barretto et al.\n(E-Vote-ID 2021) reported that convolutional neural networks are a viable\noption in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To\ndemonstrate and analyze the hypothetical vulnerability of machine learning\nmodels on election tabulators, we first introduce four new ballot datasets.\nSecond, we train and test a variety of different models on our new datasets.\nThese models include support vector machines, convolutional neural networks (a\nbasic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,\nusing our new datasets and trained models, we demonstrate that traditional\nwhite box attacks are ineffective in the voting domain due to gradient masking.\nOur analyses further reveal that gradient masking is a product of numerical\ninstability. We use a modified difference of logits ratio loss to overcome this\nissue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct\nattacks with the adversarial examples generated using our new methods. In\ntraditional adversarial machine learning, a high (50% or greater) attack\nsuccess rate is ideal. However, for certain elections, even a 5% attack success\nrate can flip the outcome of a race. We show such an impact is possible in the\nphysical domain. We thoroughly discuss attack realism, and the challenges and\npracticality associated with printing and scanning ballot adversarial examples."}
{"id": "2506.14603", "pdf": "https://arxiv.org/pdf/2506.14603", "abs": "https://arxiv.org/abs/2506.14603", "authors": ["Amirmojtaba Sabour", "Sanja Fidler", "Karsten Kreis"], "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/", "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis."}
{"id": "2506.14605", "pdf": "https://arxiv.org/pdf/2506.14605", "abs": "https://arxiv.org/abs/2506.14605", "authors": ["Giacomo Meanti", "Thomas Ryckeboer", "Michael Arbel", "Julien Mairal"], "title": "Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Code available at https://github.com/inria-thoth/ddm4ip", "summary": "This work addresses image restoration tasks through the lens of inverse\nproblems using unpaired datasets. In contrast to traditional approaches --\nwhich typically assume full knowledge of the forward model or access to paired\ndegraded and ground-truth images -- the proposed method operates under minimal\nassumptions and relies only on small, unpaired datasets. This makes it\nparticularly well-suited for real-world scenarios, where the forward model is\noften unknown or misspecified, and collecting paired data is costly or\ninfeasible. The method leverages conditional flow matching to model the\ndistribution of degraded observations, while simultaneously learning the\nforward model via a distribution-matching loss that arises naturally from the\nframework. Empirically, it outperforms both single-image blind and unsupervised\napproaches on deblurring and non-uniform point spread function (PSF)\ncalibration tasks. It also matches state-of-the-art performance on blind\nsuper-resolution. We also showcase the effectiveness of our method with a proof\nof concept for lens calibration: a real-world application traditionally\nrequiring time-consuming experiments and specialized equipment. In contrast,\nour approach achieves this with minimal data acquisition effort."}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch."}
{"id": "2506.14641", "pdf": "https://arxiv.org/pdf/2506.14641", "abs": "https://arxiv.org/abs/2506.14641", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars."}
{"id": "2506.14652", "pdf": "https://arxiv.org/pdf/2506.14652", "abs": "https://arxiv.org/abs/2506.14652", "authors": ["Alexandra Olteanu", "Su Lin Blodgett", "Agathe Balayn", "Angelina Wang", "Fernando Diaz", "Flavio du Pin Calmon", "Margaret Mitchell", "Michael Ekstrand", "Reuben Binns", "Solon Barocas"], "title": "Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "20 pages, 1 figure, 1 table", "summary": "In AI research and practice, rigor remains largely understood in terms of\nmethodological rigor -- such as whether mathematical, statistical, or\ncomputational methods are correctly applied. We argue that this narrow\nconception of rigor has contributed to the concerns raised by the responsible\nAI community, including overblown claims about AI capabilities. Our position is\nthat a broader conception of what rigorous AI research and practice should\nentail is needed. We believe such a conception -- in addition to a more\nexpansive understanding of (1) methodological rigor -- should include aspects\nrelated to (2) what background knowledge informs what to work on (epistemic\nrigor); (3) how disciplinary, community, or personal norms, standards, or\nbeliefs influence the work (normative rigor); (4) how clearly articulated the\ntheoretical constructs under use are (conceptual rigor); (5) what is reported\nand how (reporting rigor); and (6) how well-supported the inferences from\nexisting evidence are (interpretative rigor). In doing so, we also aim to\nprovide useful language and a framework for much-needed dialogue about the AI\ncommunity's work by researchers, policymakers, journalists, and other\nstakeholders."}
{"id": "2506.14665", "pdf": "https://arxiv.org/pdf/2506.14665", "abs": "https://arxiv.org/abs/2506.14665", "authors": ["Giulia Luise", "Chin-Wei Huang", "Thijs Vogels", "Derk P. Kooi", "Sebastian Ehlert", "Stephanie Lanius", "Klaas J. H. Giesbertz", "Amir Karton", "Deniz Gunceler", "Megan Stanley", "Wessel P. Bruinsma", "Lin Huang", "Xinran Wei", "José Garrido Torres", "Abylay Katbashev", "Bálint Máté", "Sékou-Oumar Kaba", "Roberto Sordillo", "Yingrong Chen", "David B. Williams-Young", "Christopher M. Bishop", "Jan Hermann", "Rianne van den Berg", "Paola Gori-Giorgi"], "title": "Accurate and scalable exchange-correlation with deep learning", "categories": ["physics.chem-ph", "cs.AI", "cs.CE", "cs.LG", "physics.comp-ph"], "comment": "Main: 13 pages plus references, 11 figures and tables. Supplementary\n  information: 19 pages, 12 figures and tables", "summary": "Density Functional Theory (DFT) is the most widely used electronic structure\nmethod for predicting the properties of molecules and materials. Although DFT\nis, in principle, an exact reformulation of the Schr\\\"odinger equation,\npractical applications rely on approximations to the unknown\nexchange-correlation (XC) functional. Most existing XC functionals are\nconstructed using a limited set of increasingly complex, hand-crafted features\nthat improve accuracy at the expense of computational efficiency. Yet, no\ncurrent approximation achieves the accuracy and generality for predictive\nmodeling of laboratory experiments at chemical accuracy -- typically defined as\nerrors below 1 kcal/mol. In this work, we present Skala, a modern deep\nlearning-based XC functional that bypasses expensive hand-designed features by\nlearning representations directly from data. Skala achieves chemical accuracy\nfor atomization energies of small molecules while retaining the computational\nefficiency typical of semi-local DFT. This performance is enabled by training\non an unprecedented volume of high-accuracy reference data generated using\ncomputationally intensive wavefunction-based methods. Notably, Skala\nsystematically improves with additional training data covering diverse\nchemistry. By incorporating a modest amount of additional high-accuracy data\ntailored to chemistry beyond atomization energies, Skala achieves accuracy\ncompetitive with the best-performing hybrid functionals across general main\ngroup chemistry, at the cost of semi-local DFT. As the training dataset\ncontinues to expand, Skala is poised to further enhance the predictive power of\nfirst-principles simulations."}
{"id": "2506.14673", "pdf": "https://arxiv.org/pdf/2506.14673", "abs": "https://arxiv.org/abs/2506.14673", "authors": ["Mikael Møller Høgsgaard", "Andrea Paudice"], "title": "Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The Median of Means (MoM) is a mean estimator that has gained popularity in\nthe context of heavy-tailed data. In this work, we analyze its performance in\nthe task of simultaneously estimating the mean of each function in a class\n$\\mathcal{F}$ when the data distribution possesses only the first $p$ moments\nfor $p \\in (1,2]$. We prove a new sample complexity bound using a novel\nsymmetrization technique that may be of independent interest. Additionally, we\npresent applications of our result to $k$-means clustering with unbounded\ninputs and linear regression with general losses, improving upon existing\nworks."}
{"id": "2506.14702", "pdf": "https://arxiv.org/pdf/2506.14702", "abs": "https://arxiv.org/abs/2506.14702", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet Üstün", "Sara Hooker"], "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations."}
{"id": "2506.14753", "pdf": "https://arxiv.org/pdf/2506.14753", "abs": "https://arxiv.org/abs/2506.14753", "authors": ["Qinchan", "Li", "Kenneth Chen", "Changyue", "Su", "Wittawat Jitkrittum", "Qi Sun", "Patsorn Sangkloy"], "title": "Cost-Aware Routing for Efficient Text-To-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models are well known for their ability to generate a high-fidelity\nimage for an input prompt through an iterative denoising process.\nUnfortunately, the high fidelity also comes at a high computational cost due\nthe inherently sequential generative process. In this work, we seek to\noptimally balance quality and computational cost, and propose a framework to\nallow the amount of computation to vary for each prompt, depending on its\ncomplexity. Each prompt is automatically routed to the most appropriate\ntext-to-image generation function, which may correspond to a distinct number of\ndenoising steps of a diffusion model, or a disparate, independent text-to-image\nmodel. Unlike uniform cost reduction techniques (e.g., distillation, model\nquantization), our approach achieves the optimal trade-off by learning to\nreserve expensive choices (e.g., 100+ denoising steps) only for a few complex\nprompts, and employ more economical choices (e.g., small distilled model) for\nless sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB\nthat by learning to route to nine already-trained text-to-image models, our\napproach is able to deliver an average quality that is higher than that\nachievable by any of these models alone."}
{"id": "2506.14762", "pdf": "https://arxiv.org/pdf/2506.14762", "abs": "https://arxiv.org/abs/2506.14762", "authors": ["Chengyuan Zhang", "Cathy Wu", "Lijun Sun"], "title": "Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior", "categories": ["stat.AP", "cs.LG", "cs.RO"], "comment": null, "summary": "Accurate and interpretable car-following models are essential for traffic\nsimulation and autonomous vehicle development. However, classical models like\nthe Intelligent Driver Model (IDM) are fundamentally limited by their\nparsimonious and single-regime structure. They fail to capture the multi-modal\nnature of human driving, where a single driving state (e.g., speed, relative\nspeed, and gap) can elicit many different driver actions. This forces the model\nto average across distinct behaviors, reducing its fidelity and making its\nparameters difficult to interpret. To overcome this, we introduce a\nregime-switching framework that allows driving behavior to be governed by\ndifferent IDM parameter sets, each corresponding to an interpretable behavioral\nmode. This design enables the model to dynamically switch between interpretable\nbehavioral modes, rather than averaging across diverse driving contexts. We\ninstantiate the framework using a Factorial Hidden Markov Model with IDM\ndynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes\n(e.g., aggressive acceleration, steady-state following) from external traffic\nscenarios (e.g., free-flow, congestion, stop-and-go) through two independent\nlatent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC)\nis used to jointly estimate the regime-specific parameters, transition\ndynamics, and latent state trajectories. Experiments on the HighD dataset\ndemonstrate that FHMM-IDM uncovers interpretable structure in human driving,\neffectively disentangling internal driver actions from contextual traffic\nconditions and revealing dynamic regime-switching patterns. This framework\nprovides a tractable and principled solution to modeling context-dependent\ndriving behavior under uncertainty, offering improvements in the fidelity of\ntraffic simulations, the efficacy of safety analyses, and the development of\nmore human-centric ADAS."}
{"id": "2506.14767", "pdf": "https://arxiv.org/pdf/2506.14767", "abs": "https://arxiv.org/abs/2506.14767", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm."}
