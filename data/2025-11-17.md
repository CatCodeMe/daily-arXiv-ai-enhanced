<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 77]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CL](#cs.CL) [Total: 15]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.OC](#math.OC) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu,Wei Pan,Xun Jian,Zeqi Ma,Tianjie Li,Yang Shen,Chengzhi Han,Yudong Zhao,Zhanhuai Li*

Main category: cs.DB

TL;DR: ResBench是一个评估数据库韧性的基准测试框架，通过模拟和注入不利事件，从八个维度量化数据库在真实世界逆境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据库基准测试主要关注理想运行环境下的性能，但真实场景中数据库会面临各种不利事件，需要从综合角度量化其应对能力。

Method: 通过清晰的层次解耦实现测试过程的自动化、标准化和可视化，模拟不利事件并在正常事务处理期间注入，使用模块收集多个指标进行评估。

Result: 从吞吐量、延迟、稳定性、抵抗性、恢复性、干扰期、适应能力和指标偏差八个维度评估数据库韧性，结果通过用户友好的图形界面展示。

Conclusion: ResBench提供了一个全面的数据库韧性评估框架，能够有效模拟真实世界中的不利事件并量化数据库的应对能力。

Abstract: Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets.

</details>


### [2] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 提出了一种将知识补全阶段集成到图数据库-图机器学习应用中的创新架构，通过揭示隐藏知识来改善数据集行为和指标


<details>
  <summary>Details</summary>
Motivation: 当前图数据库-图机器学习应用在分析数据时存在关键缺陷，特别是在知识图谱的知识补全方面，忽略了数据集中可能包含的宝贵隐藏知识，这可能导致图机器学习模型的错误解释

Method: 引入可扩展的传递关系，通过衰减函数建模信息在网络中的传播，实现跨多个节点的确定性知识流

Result: 实验结果表明该方法能从根本上重塑拓扑结构和整体数据集动态

Conclusion: 这种新的图数据库-图机器学习架构对于生成更好的模型和释放基于图的数据分析的全部潜力至关重要

Abstract: Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.
  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.
  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 提出了FengHuang平台，一种解耦的AI基础设施，通过多级共享内存架构解决GPU中心架构在推理工作负载中的内存和通信扩展限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统GPU中心架构在AI推理工作负载中面临内存容量、带宽和互连扩展的限制，需要新的基础设施设计来克服这些挑战。

Method: 采用多级共享内存架构，结合高速本地内存和集中式解耦远程内存，通过主动张量分页和张量操作的近内存计算来增强性能。

Result: 模拟显示FengHuang可实现高达93%的本地内存容量减少、50%的GPU计算节省，以及比传统GPU扩展快16倍到70倍的GPU间通信。在GPT-3、Grok-1和QWEN3-235B等负载下，可减少50%的GPU使用同时保持终端用户性能。

Conclusion: FengHuang提供了一个可扩展、灵活且经济高效的AI推理基础设施解决方案，作为机架级AI基础设施扩展方案提供了最佳平衡，其开放异构设计消除了供应商锁定并增强了供应链灵活性。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [4] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester是一个基于多智能体LLM的框架，用于自动化生成HPC软件的单元测试，特别针对OpenMP和MPI并行编程模型，通过协作工作流生成可编译且功能正确的测试用例。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理HPC应用中的非确定性行为和同步问题，需要更有效的自动化测试解决方案来确保并行软件的可靠性。

Method: 采用多智能体LLM框架，包含专门的配方智能体和测试智能体，通过迭代式批评循环协作生成和优化测试用例，针对并行执行构造、复杂通信模式和层次化并行性。

Result: HPCAgentTester能够生成可编译且功能正确的OpenMP和MPI原语测试，有效识别传统技术常遗漏的细微bug，相比独立LLM显著提高了测试编译率和正确性。

Conclusion: 该框架为并行软件系统提供了更强大和可扩展的可靠性保障解决方案，在HPC单元测试自动化方面具有重要价值。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


### [5] [UFO$^3$: Weaving the Digital Agent Galaxy](https://arxiv.org/abs/2511.11332)
*Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.DC

TL;DR: UFO$^3$是一个跨设备任务编排系统，通过将异构设备统一为单一编排结构，将用户请求建模为可变的TaskConstellation（分布式DAG），实现异步执行、自适应恢复和动态优化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的代理大多局限于单一操作系统或设备，使得跨设备工作流程脆弱且主要依赖手动操作，需要解决跨设备协作的挑战。

Method: 使用TaskConstellation模型，包含原子子任务（TaskStars）和显式控制数据依赖（TaskStarLines），通过Constellation Orchestrator异步安全执行任务，Agent Interaction Protocol提供持久低延迟通道。

Result: 在NebulaBench基准测试中，UFO$^3$实现83.3%子任务完成率、70.9%任务成功率，平均并行宽度1.72，端到端延迟降低31%，在故障注入实验中表现出优雅降级和恢复能力。

Conclusion: UFO$^3$实现了跨异构设备的准确、高效、弹性任务编排，将孤立代理统一为连贯的自适应计算结构，扩展了泛在计算领域。

Abstract: Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.
  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.

</details>


### [6] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: 提出了一种新的算法来克服物理系统模拟中域分解方法的性能限制，在64个Cerebras CS-3系统集群上实现了每秒160万时间步的模拟速度和84 PFLOP/s的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的域分解方法在网络计算环境中无法提供高模拟速率或高利用率，特别是在Exascale系统中这些工作负载只能达到峰值性能的一小部分。

Method: 引入了新颖的算法来克服这些限制，并在64个Cerebras CS-3系统集群上应用浅水方程模拟小行星撞击后的海啸，分辨率达到460米。

Result: 实现了超过160万时间步/秒的模拟速度和84 PFLOP/s的性能，在单节点和集群环境中都能达到峰值性能的90%。

Conclusion: 该算法能够显著提高物理系统模拟的性能和效率，在行星尺度的高分辨率模拟中表现出色。

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [7] [Discounted Cuts: A Stackelberg Approach to Network Disruption](https://arxiv.org/abs/2511.10804)
*Pål Grønås Drange,Fedor V. Fomin,Petr Golovach,Danil Sagunov*

Main category: cs.DS

TL;DR: 本文研究Stackelberg版本的Most Vital Links问题，提出折扣割模型，在一般图中多数变体是NP完全问题，但在有界亏格图上所有折扣割问题都可在多项式时间内求解。


<details>
  <summary>Details</summary>
Motivation: 研究攻击者与防御者之间的Stackelberg博弈，攻击者策略性地移除网络中的边来最大化破坏流量，防御者随后重新路由剩余流量。旨在连接人工智能、算法博弈论和运筹学领域。

Method: 引入折扣割数学模型，其中割的成本通过排除其k个最昂贵边来评估。开发统一算法框架分析各种形式的折扣割问题，包括最小化或最大化割成本，排除k个最贵或最便宜边。

Result: 在一般图中多数折扣割变体为NP完全问题，但在有界亏格图上所有折扣割问题都可在多项式时间内求解，这类图包括许多现实世界网络如交通和基础设施网络。

Conclusion: 折扣割模型推广了Most Vital Links问题并揭示了新的算法和复杂性理论特性，为相关领域研究建立了桥梁。

Abstract: We study a Stackelberg variant of the classical Most Vital Links problem, modeled as a one-round adversarial game between an attacker and a defender. The attacker strategically removes up to $k$ edges from a flow network to maximally disrupt flow between a source $s$ and a sink $t$, after which the defender optimally reroutes the remaining flow. To capture this attacker--defender interaction, we introduce a new mathematical model of discounted cuts, in which the cost of a cut is evaluated by excluding its $k$ most expensive edges. This model generalizes the Most Vital Links problem and uncovers novel algorithmic and complexity-theoretic properties.
  We develop a unified algorithmic framework for analyzing various forms of discounted cut problems, including minimizing or maximizing the cost of a cut under discount mechanisms that exclude either the $k$ most expensive or the $k$ cheapest edges. While most variants are NP-complete on general graphs, our main result establishes polynomial-time solvability for all discounted cut problems in our framework when the input is restricted to bounded-genus graphs, a relevant class that includes many real-world networks such as transportation and infrastructure networks. With this work, we aim to open collaborative bridges between artificial intelligence, algorithmic game theory, and operations research.

</details>


### [8] [Beating Meet-in-the-Middle for Subset Balancing Problems](https://arxiv.org/abs/2511.10823)
*Tim Randolph,Karol Węgrzycki*

Main category: cs.DS

TL;DR: 本文提出了针对子集平衡问题的首个突破"中间相遇"障碍的最坏情况算法，运行时间为O(|C|^{(0.5-ε)n})，改进了先前仅在平均情况下突破该障碍的结果。


<details>
  <summary>Details</summary>
Motivation: 子集平衡问题推广了子集和、划分和等子集和问题，但现有算法在系数集C = {-d,...,d}等情况下无法突破O(|C|^{n/2})的时间复杂度界限。本文旨在为这些系数集开发最坏情况下的快速精确算法。

Method: 采用Howgrave-Graham和Joux的表示技术，结合三种新技术：(1)在最坏情况输入下实现良好"混合"的策略；(2)为不含0的系数集创建灵活输入表示；(3)从包含"伪解对"的向量集中快速恢复兼容解对。

Result: 对于C = {-d,...,d}和C = {-d,...,d}\{0}等系数集，提出了运行时间为O(|C|^{(0.5-ε)n})的算法，首次在最坏情况下突破了中间相遇障碍。同时显著改进了等子集和问题的最佳精确算法。

Conclusion: 成功将表示技术从平均情况扩展到最坏情况输入，为多个系数集提供了突破中间相遇障碍的算法，但C = {-2,-1,1,2}和C = {-1,1}两种情况仍需进一步研究。

Abstract: We consider exact algorithms for Subset Balancing, a family of related problems that generalizes Subset Sum, Partition, and Equal Subset Sum. Specifically, given as input an integer vector $\vec{x} \in \mathbb{Z}^n$ and a constant-size coefficient set $C \subset \mathbb{Z}$, we seek a nonzero solution vector $\vec{c} \in C^n$ satisfying $\vec{c} \cdot \vec{x} = 0$.
  For $C = \{-d,\ldots,d\}$, $d > 1$ and $C = \{-d,\ldots,d\}\setminus\{0\}$, $d > 2$, we present algorithms that run in time $O(|C|^{(0.5 - ε)n})$ for a constant $ε> 0$ that depends only on $C$. These are the first algorithms that break the $O(|C|^{n/2})$-time ``Meet-in-the-Middle barrier'' for these coefficient sets in the worst case. This improves on the result of Chen, Jin, Randolph and Servedio (SODA 2022), who broke the Meet-in-the-Middle barrier on these coefficient sets in the average-case setting. We also improve the best exact algorithm for Equal Subset Sum (Subset Balancing with $C = \{-1,0,1\}$), due to Mucha, Nederlof, Pawlewicz, and Węgrzycki (ESA 2019), by an exponential margin. This positively answers an open question of Jin, Williams, and Zhang (ESA 2025). Our results leave two natural cases in which we cannot yet break the Meet-in-the-Middle barrier: $C = \{-2, -1, 1, 2\}$ and $C = \{-1, 1\}$ (Partition).
  Our results bring the representation technique of Howgrave-Graham and Joux (CRYPTO 2010) from average-case to worst-case inputs for many $C$. This requires a variety of new techniques: we present strategies for (1) achieving good ``mixing'' with worst-case inputs, (2) creating flexible input representations for coefficient sets without 0, and (3) quickly recovering compatible solution pairs from sets of vectors containing ``pseudosolution pairs''. These techniques may find application to other algorithmic problems on integer sums or be of independent interest.

</details>


### [9] [A number-theoretic conjecture implying faster algorithms for polynomial factorization and integer factorization](https://arxiv.org/abs/2511.10851)
*Chris Umans,Siki Wang*

Main category: cs.DS

TL;DR: 本文提出了一种新的多项式因式分解策略，旨在突破现有的3/2指数障碍，通过数论猜想实现更高效的算法。


<details>
  <summary>Details</summary>
Motivation: 当前有限域上单变量多项式因式分解的最快算法时间复杂度为O(n^{3/2+o(1)} polylog q)，且3/2指数被认为是采用baby-steps-giant-steps策略算法的固有障碍。

Method: 提出新策略，基于数论猜想：存在大小为n^β的集合S和T，其中元素为不超过exp(n^α)的正整数，使得每个i∈[n]都能整除某个s-t（s∈S，t∈T）。

Result: 证明如果α,β<1/2且集合S,T具有特定结构，则可以改进多项式因式分解的3/2指数。最优情况α=β=1/3将实现4/3指数算法。

Conclusion: 该策略有潜力突破3/2障碍，且第二个重要结果是能将整数确定性因式分解算法的最优指数从1/5降至1/6。

Abstract: The fastest known algorithm for factoring a degree $n$ univariate polynomial over a finite field $\mathbb{F}_q$ runs in time $O(n^{3/2 + o(1)}\text{polylog } q)$, and there is a reason to believe that the $3/2$ exponent represents a ''barrier'' inherent in algorithms that employ a so-called baby-steps-giant-steps strategy. In this paper, we propose a new strategy with the potential to overcome the $3/2$ barrier. In doing so we are led to a number-theoretic conjecture, one form of which is that there are sets $S, T$ of cardinality $n^β$, consisting of positive integers of magnitude at most $\exp(n^α)$, such that every integer $i \in [n]$ divides $s-t$ for some $s \in S, t \in T$. Achieving $α+ β\le 1 + o(1)$ is trivial; we show that achieving $α, β< 1/2$ (together with an assumption that $S, T$ are structured) implies an improvement to the exponent 3/2 for univariate polynomial factorization. Achieving $α= β= 1/3$ is best-possible and would imply an exponent 4/3 algorithm for univariate polynomial factorization. Interestingly, a second consequence would be a reduction of the current-best exponent for deterministic (exponential) algorithms for factoring integers, from $1/5$ to $1/6$.

</details>


### [10] [Cycle Basis Algorithms for Reducing Maximum Edge Participation](https://arxiv.org/abs/2511.10961)
*Fan Wang,Sandy Irani*

Main category: cs.DS

TL;DR: 本文研究构建具有低最大边参与度的图循环基问题，这在量子容错中至关重要，因为它直接影响晶格手术过程的开销。作者提出了负载感知启发式方法，改进了随机正则图和小量子码图上的性能，并分析了简化模型以建立边参与度的下界。


<details>
  <summary>Details</summary>
Motivation: 研究图循环基的最大边参与度问题，因为该指标在量子容错中起着关键作用，直接影响用于实现几乎通用量子门集的晶格手术过程的开销。

Method: 基于Freedman和Hastings的递归算法，引入了一族负载感知启发式方法，自适应选择顶点和边以最小化循环基构建过程中的边参与度。

Result: 在随机正则图和源自小量子码的图上改进了经验性能，分析表明启发式方法的最大负载增长约为(log n)^2量级。

Conclusion: 仔细的循环基构造可以在容错量子系统设计中产生显著的实际效益，该问题在理论上也具有重要意义，因为它本质上等同于图的基数（所有循环基中可能的最小最大边参与度）。

Abstract: We study the problem of constructing cycle bases of graphs with low maximum edge participation, defined as the maximum number of basis cycles that share any single edge. This quantity, though less studied than total weight or length, plays a critical role in quantum fault tolerance because it directly impacts the overhead of lattice surgery procedures used to implement an almost universal quantum gate set. Building on a recursive algorithm of Freedman and Hastings, we introduce a family of load-aware heuristics that adaptively select vertices and edges to minimize edge participation throughout the cycle basis construction. Our approach improves empirical performance on random regular graphs and on graphs derived from small quantum codes. We further analyze a simplified balls-into-bins process to establish lower bounds on edge participation. While the model differs from the cycle basis algorithm on real graphs, it captures what can be proven for our heuristics without using complex graph theoretic properties related to the distribution of cycles in the graph. Our analysis suggests that the maximum load of our heuristics grows on the order of (log n)^2. Our results indicate that careful cycle basis construction can yield significant practical benefits in the design of fault-tolerant quantum systems. This question also carries theoretical interest, as it is essentially identical to the basis number of a graph, defined as the minimum possible maximum edge participation over all cycle bases.

</details>


### [11] [R-enum Revisited: Speedup and Extension for Context-Sensitive Repeats and Net Frequencies](https://arxiv.org/abs/2511.11057)
*Kotaro Kimura,Tomohiro I*

Main category: cs.DS

TL;DR: 本文改进了r-enum算法，将时间复杂度从O(n log log_w(n/r))提升到O(n)，扩展了算法功能以计算近超极大重复等上下文敏感重复，并研究了见证NSMRs的净出现次数。


<details>
  <summary>Details</summary>
Motivation: Nishimoto和Tabei在2021年提出的r-enum算法虽然能在压缩工作空间内枚举特征子串，但其时间复杂度仍有改进空间。本文旨在进一步优化该算法的时间复杂度，并扩展其功能以处理更多类型的重复模式。

Method: 基于运行长度编码的BWT(RLBWT)，改进r-enum算法的时间复杂度，扩展算法以计算近超极大重复(NSMRs)、超极大重复和上下文多样性，并构建支持查询的数据结构。

Result: 成功将r-enum算法的时间复杂度从O(n log log_w(n/r))改进到O(n)，能够计算所有NSMRs及其净频率/出现次数，构建了支持查询的数据结构，并证明了净出现次数的总数量小于2r。

Conclusion: 本文显著改进了r-enum算法的性能，扩展了其应用范围，为字符串分析提供了更高效的压缩空间算法，同时获得了关于最小唯一子串数量的新上界2r。

Abstract: Nishimoto and Tabei [CPM, 2021] proposed r-enum, an algorithm to enumerate various characteristic substrings, including maximal repeats, in a string $T$ of length $n$ in $O(r)$ words of compressed working space, where $r \le n$ is the number of runs in the Burrows-Wheeler transform (BWT) of $T$. Given the run-length encoded BWT (RLBWT) of $T$, r-enum runs in $O(n\log\log_{w}(n/r))$ time in addition to the time linear to the number of output strings, where $w=Θ(\log n)$ is the word size. In this paper, we improve the $O(n\log\log_{w}(n/r))$ term to $O(n)$. We also extend r-enum to compute other context-sensitive repeats such as near-supermaximal repeats (NSMRs) and supermaximal repeats, and the context diversity for every maximal repeat in the same complexities. Furthermore, we study the occurrences that witness NSMRs, which have recently attracted attention under the name of net occurrences: An occurrence of a repeat is called a net occurrence if it is not covered by another repeat, and the net frequency of a repeat is the number of its net occurrences. With this terminology, an NSMR is defined to be a repeat with a positive net frequency. Given the RLBWT of $T$, we show how to compute the set $S^{nsmr}$ of all NSMRs in $T$ together with their net frequency/occurrences in $O(n)$ time and $O(r)$ space. We also show that an $O(r)$-space data structure can be built from the RLBWT to support queries of computing the net frequency of any query pattern $P$ in $O(|P|)$ time. The data structure is built in $O(r)$ space and in $O(n)$ time with high probability or deterministic $O(n+|S^{nsmr}|\log\log\min(σ,|S^{nsmr}|))$ time, where $σ\le r$ is the alphabet size of $T$. To achieve this, we prove that the total number of net occurrences is less than $2r$. We also get a new upper bound $2r$ of the number of minimal unique substrings in $T$, which may be of independent interest.

</details>


### [12] [An Efficient Algorithm for Minimizing Ordered Norms in Fractional Load Balancing](https://arxiv.org/abs/2511.11237)
*Daniel Blankenburg,Antonia Ellerbrock,Thomas Kesselheim,Jens Vygen*

Main category: cs.DS

TL;DR: 提出了一种随机算法，用于最小化负载向量的有序范数，通过资源定价机制和动态成本预算实现(1+ε)近似解，调用次数为O((n+d)(ε⁻²+loglog d)log(n+d))。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要针对ℓ∞范数，无法扩展到任意有序范数，需要开发新的证明技术来处理更一般的范数优化问题。

Method: 使用基于跟随正则化领导范式的资源定价机制，通过有序范数的平滑近似，结合动态成本预算和非均匀采样来控制步长。

Result: 算法以高概率在指定次数的oracle调用内找到(1+ε)近似解，解决了现有方法无法处理任意有序范数的问题。

Conclusion: 该方法通过新颖的资源定价和动态预算机制，成功扩展了有序范数最小化问题的求解范围，相关稳定性分析具有独立研究价值。

Abstract: We study the problem of minimizing an ordered norm of a load vector (indexed by a set of $d$ resources), where a finite number $n$ of customers $c$ contribute to the load of each resource by choosing a solution $x_c$ in a convex set $X_c \subseteq \mathbb{R}^d_{\geq 0}$; so we minimize $||\sum_{c}x_c||$ for some fixed ordered norm $||\cdot||$. We devise a randomized algorithm that computes a $(1+\varepsilon)$-approximate solution to this problem and makes, with high probability, $\mathcal{O}((n+d) (\varepsilon^{-2}+\log\log d)\log (n+d))$ calls to oracles that minimize linear functions (with non-negative coefficients) over $X_c$. While this has been known for the $\ell_{\infty}$ norm via the multiplicative weights update method, existing proof techniques do not extend to arbitrary ordered norms. Our algorithm uses a resource price mechanism that is motivated by the follow-the-regularized-leader paradigm, and is expressed by smooth approximations of ordered norms. We need and show that these have non-trivial stability properties, which may be of independent interest. For each customer, we define dynamic cost budgets, which evolve throughout the algorithm, to determine the allowed step sizes. This leads to non-uniform updates and may even reject certain oracle solutions. Using non-uniform sampling together with a martingale argument, we can guarantee sufficient expected progress in each iteration, and thus bound the total number of oracle calls with high probability.

</details>


### [13] [Improved Differentially Private Algorithms for Rank Aggregation](https://arxiv.org/abs/2511.11319)
*Quentin Hillebrand,Pasin Manurangsi,Vorapong Suppakitpaisarn,Phanu Vajanopath*

Main category: cs.DS

TL;DR: 本文改进了Kemeny排序聚合问题的差分隐私PTAS算法，并首次研究了footrule排序聚合问题的差分隐私算法，获得了近似最优解。


<details>
  <summary>Details</summary>
Motivation: 排序聚合需要将多个用户的物品排序合并成一个代表性排序。Alabi等人的工作提出了Kemeny排序聚合问题的差分隐私近似算法，但存在较大的加性误差。本文旨在改进这些算法并扩展研究范围。

Method: 提出了改进的差分隐私PTAS算法，减小了加性误差。同时首次研究了footrule排序聚合问题，给出了近似最优的差分隐私算法。

Result: 在中心模型中获得了更小加性误差的DP PTAS算法；footrule排序聚合问题获得了近似最优解；作为推论，得到了Kemeny排序聚合问题的2-近似算法，加性误差与Alabi等人的5-近似算法相同。

Conclusion: 本文显著改进了Kemeny排序聚合问题的差分隐私算法性能，并开辟了footrule排序聚合问题的差分隐私研究新方向，为排序聚合的隐私保护提供了更优的解决方案。

Abstract: Rank aggregation is a task of combining the rankings of items from multiple users into a single ranking that best represents the users' rankings. Alabi et al. (AAAI'22) presents differentially-private (DP) polynomial-time approximation schemes (PTASes) and $5$-approximation algorithms with certain additive errors for the Kemeny rank aggregation problem in both central and local models. In this paper, we present improved DP PTASes with smaller additive error in the central model. Furthermore, we are first to study the footrule rank aggregation problem under DP. We give a near-optimal algorithm for this problem; as a corollary, this leads to 2-approximation algorithms with the same additive error as the $5$-approximation algorithms of Alabi et al. for the Kemeny rank aggregation problem in both central and local models.

</details>


### [14] [Learning and Testing Convex Functions](https://arxiv.org/abs/2511.11498)
*Renato Ferreira Pinto,Cassandra Marcussen,Elchanan Mossel,Shivam Nadimpalli*

Main category: cs.DS

TL;DR: 本文研究了高斯空间下实值凸函数的学习和测试问题，提出了在Lipschitz平滑性假设下的学习算法和测试方法，并给出了样本复杂度的上下界。


<details>
  <summary>Details</summary>
Motivation: 虽然函数凸性在数学、统计学和计算机科学中被广泛研究，但其可学习性和可测试性主要在离散或受限设置中研究，且通常使用不适合实值函数的汉明距离。本文旨在在高斯测度下研究这些问题。

Method: 采用Lipschitz平滑性假设，提出了针对Lipschitz凸函数的不可知论学习算法和容忍性测试方法，包括单边测试器和双边测试器。

Result: 学习算法使用n^O(1/ε²)样本达到误差ε，并在CSQ模型中给出了n^poly(1/ε)的下界；测试器具有相同的样本复杂度，单边测试器使用O(√n/ε)^n样本。

Conclusion: 在高斯空间下，Lipschitz凸函数可以在多项式样本复杂度下学习和测试，平滑性假设是必要的，即使在单维情况下也是如此。

Abstract: We consider the problems of \emph{learning} and \emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.
  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:
  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\varepsilon$ using $n^{O(1/\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\mathrm{poly}(1/\varepsilon)}$ samples in the \emph{correlational statistical query (CSQ)} model.
  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\sqrt{n}/\varepsilon)^n$ samples.

</details>


### [15] [Faster MAX-CUT on Bounded Threshold Rank Graphs](https://arxiv.org/abs/2511.11499)
*Prashanti Anderson,Samuel B. Hopkins,Amit Rajaraman,David Steurer*

Main category: cs.DS

TL;DR: 提出了针对有界阈值秩图上的2CSP近似新算法，运行时间在1/ε上是多项式，在标签扩展图的阈值秩上是指数级，输入规模接近线性


<details>
  <summary>Details</summary>
Motivation: 解决有界阈值秩图上2CSP问题的现有算法运行时间在阈值秩和1/ε上都是指数级，需要更高效的近似算法

Method: 结合子空间枚举和半定规划的新颖方法，利用标签扩展图与基图阈值秩间的新比较不等式

Result: 获得了首个在1/ε上多项式时间运行的MAX-CUT (1+O(ε))近似算法，将运行时间从输入规模的多项式改进为接近线性

Conclusion: 该算法为有界阈值秩图上的2CSP问题提供了高效的近似解决方案，显著提升了现有算法的性能

Abstract: We design new algorithms for approximating 2CSPs on graphs with bounded threshold rank, that is, whose normalized adjacency matrix has few eigenvalues larger than $\varepsilon$, smaller than $-\varepsilon$, or both. Unlike on worst-case graphs, 2CSPs on bounded threshold rank graphs can be $(1+O(\varepsilon))$-approximated efficiently. Prior approximation algorithms for this problem run in time exponential in the threshold rank and $1/\varepsilon$. Our algorithm has running time which is polynomial in $1/\varepsilon$ and exponential in the threshold rank of the label-extended graph, and near-linear in the input size. As a consequence, we obtain the first $(1+O(\varepsilon))$ approximation for MAX-CUT on bounded threshold rank graphs running in $\mathrm{poly}(1/\varepsilon)$ time. We also improve the state-of-the-art running time for 2CSPs on bounded threshold-rank graphs from polynomial in input size to near-linear via a new comparison inequality between the threshold rank of the label-extended graph and base graph. Our algorithm is a simple yet novel combination of subspace enumeration and semidefinite programming.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码审查的看法，发现尽管同行代码审查对提高研究软件质量至关重要，但RSEs在采用过程中面临独特挑战，需要通过结构化流程、改进工具和针对性培训来提升采纳率和效果。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科研发现至关重要，但不断变化的需求、复杂输入和遗留依赖阻碍了软件质量和可维护性。虽然同行代码审查可以提高软件质量，但其在研究软件工程师中的采用情况尚未被探索。

Method: 通过调查问卷收集研究软件工程师对同行代码审查的看法，调查设计与先前研究保持一致以进行对比分析，同时包含针对RSEs的额外问题。

Result: 收到61份有效回复，发现结果与先前研究一致，同时揭示了RSEs相比更广泛开发者群体面临的独特挑战和实践差异。

Conclusion: 同行代码审查对提高研究软件质量、可维护性和可靠性至关重要。尽管RSEs面临独特挑战，但通过结构化流程、改进工具和针对性培训可以增强同行审查在研究软件开发中的采纳和效果。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [17] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了一种基于LLM和人工参与的补丁有效性评估方法，通过生成评估标准并人工优化，显著降低了手动标注成本，同时保持了与人工评估的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动程序修复评估方法依赖执行测试，无法准确判断补丁有效性，而人工标注成本高昂。需要一种既能减少人工成本又能保证评估准确性的方法。

Method: 采用人机协作方法：首先用LLM为每个bug生成评估标准，然后人工审核优化该标准，最后用LLM基于优化后的标准判断补丁有效性。

Result: 在人工评估一致的补丁上，该方法与人工共识的Cohen's kappa达到0.75，召回率0.94，精确率0.80。在包含分歧补丁的完整数据集上，Cohen's kappa为0.57，召回率0.93，精确率0.65。

Conclusion: 该方法能显著降低人工标注成本，在补丁有效性评估中与人工评估具有良好一致性，但在处理存在分歧的补丁时仍有改进空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [18] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 使用大型语言模型和一致性检查开发软件监控器，用于检测运行时控制流异常，在铁路系统案例中达到84.775%的控制流覆盖率和96.610%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统复杂性增加，设计时验证无法完全保证运行时行为，需要检测控制流异常来确保系统可靠性。

Method: 提出结合LLM和一致性检查的方法：利用LLM连接设计模型和实现代码，自动化源代码插装；通过一致性检查分析事件日志来检测控制流异常。

Result: 在ERTMS/ETCS铁路系统案例中，LLM源代码插装达到84.775%控制流覆盖率，异常检测达到96.610% F1分数和93.515% AUC。

Conclusion: 结合领域知识指导LLM进行源代码插装可获得可靠软件日志，通过一致性检查实现有效的控制流异常检测。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [19] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文首次系统研究了LLM驱动的代码代理在多行缺陷修复任务上的表现，发现修复准确率从25.8%到93.3%不等，且随着缺陷分散度和复杂度的增加而下降。高表现代理具有更好的语义一致性，而低表现代理常引入新的测试失败。


<details>
  <summary>Details</summary>
Motivation: 传统程序修复主要关注单行缺陷，而忽略了现实系统中普遍存在的多行缺陷。修复多行缺陷需要在多个不连续的代码区域进行协调编辑，面临更大挑战。

Method: 在Hunk4J数据集的372个多行缺陷上评估了四种LLM代码代理（Claude Code、Codex、Gemini-cli和Qwen Code），分析了1,488个修复轨迹，使用细粒度指标捕捉定位、修复准确性、回归行为和操作动态。开发了Maple工具为代理提供仓库级上下文。

Result: 修复准确率范围从25.8%（Qwen Code）到93.3%（Claude Code），随着缺陷分散度和复杂度的增加而持续下降。高表现代理实现了积极的回归减少，而低表现代理常引入新的测试失败。失败修复消耗更多资源（39%-343%更多token）和更长的执行时间（43%-427%）。Maple通过增强定位将Gemini-cli的修复准确率提高了30%。

Conclusion: 本研究通过细粒度指标和轨迹级分析，超越了准确性评估，解释了代码代理在多行修复过程中如何定位、推理和行动，为理解LLM在复杂程序修复任务中的表现提供了系统见解。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [20] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文研究了如何利用大型语言模型处理工业过程自动化领域的专业编程语言，通过少量样本提示方法解决简单问题，同时保护敏感公司数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用编程语言，而工业自动化领域使用的专业语言在LLM中的支持不足，且涉及敏感数据保护问题。

Method: 采用少量样本提示方法，在不进行大规模领域特定训练的情况下，利用现有LLM处理专业编程语言问题。

Result: 少量样本提示方法足以解决专业语言中的简单问题，且可在本地部署确保数据安全。

Conclusion: 企业无需投入大量资源训练领域特定模型，通过少量样本提示即可有效利用LLM处理工业自动化专业语言问题，同时满足数据保护需求。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [21] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD是一个多维度的软件质量数据集，整合了450个成熟开源项目的700多个质量指标，涵盖方法、类、文件和项目级别，支持软件质量研究的综合分析。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限的维度，如代码异味、技术债务或重构活动，限制了跨时间和质量维度的综合分析。

Method: 通过集成9个最先进的静态分析工具（SonarQube、CodeScene、PMD等），从450个开源项目中提取多维度质量指标，并整合版本控制、问题跟踪和漏洞数据。

Result: 创建了包含63,586个项目版本的SQuaD数据集，统一了700多个独特指标，涵盖方法、类、文件和项目级别，支持可维护性、技术债务、软件演化等研究。

Conclusion: SQuaD为软件质量研究提供了前所未有的规模支持，并提出了自动化数据集更新和跨项目质量建模等新兴研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [22] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER是一个检测智能合约可重用组件逻辑级使用违规的自动化系统，通过多特征提取、LLM知识构建、RAG驱动的检测器和冲突检查器，在3个数据集上达到81.55%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件(SCRs)在加速业务合约开发中发挥重要作用，但逻辑级使用违规问题日益严重。这种违规发生在SCR遵守使用规则但与具体业务逻辑不匹配时，导致重大漏洞，需要深度语义理解来检测。

Method: 1) 复合特征提取方法生成三种互补特征表示；2) LLM驱动的知识构建框架，使用理解导向提示和领域特定工具提取逻辑级使用并构建SCR知识库；3) RAG驱动的检测器，结合快速检索策略和综合分析；4) 逻辑级使用违规分析引擎，集成基于相似性的检查器和基于快照的推理冲突检查器。

Result: 在3个真实数据集上的评估显示，SCRUTINEER在检测SCR逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是首个自动化检测SCR逻辑级使用违规的实用系统，通过综合方法实现了准确稳健的检测，为解决智能合约开发中的关键安全问题提供了有效工具。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [23] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 提出了CertiA360工具，将敏捷方法的灵活性与航空航天安全关键系统开发的严格认证要求（如DO-178C）相结合，通过自动化变更管理和需求追踪来确保合规性。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等安全关键系统开发中，敏捷方法的灵活性与严格的认证标准（如DO-178C）存在冲突，需要找到既能保持敏捷性又能满足合规要求的方法。

Method: 开发CertiA360工具，通过自动化需求成熟度管理、变更追踪和与监管目标对齐，在航空航天行业专家的密切合作下进行设计和验证。

Result: 反馈显示CertiA360的自动化功能可以减少人工工作量，在响应需求变更的同时确保DO-178C合规性，虽然工具尚未通过DO-330认证。

Conclusion: 适当定制的敏捷方法不仅能够与安全系统开发和认证要求共存，还能在高度监管领域（如航空航天）提高效率。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [24] [Millimeter-Wave UAV Channel Model with Height-Dependent Path Loss and Shadowing in Urban Scenarios](https://arxiv.org/abs/2511.10763)
*Abdul Saboor,Evgenii Vinogradov*

Main category: cs.NI

TL;DR: 该论文提出了一个基于无人机高度的毫米波信道模型，研究城市几何布局对LoS概率和大尺度衰落的影响，发现即使建筑参数相同，空间组织仍会轻微影响路径损耗指数。


<details>
  <summary>Details</summary>
Motivation: 无人机作为空中基站可扩展6G毫米波覆盖，但空对地信道高度依赖无人机高度和城市几何布局，需要研究这些因素对信道特性的具体影响。

Method: 使用MATLAB射线追踪在26GHz频段模拟约10,000个城市场景，分析四种具有相同建筑参数但空间组织不同的城市布局，提取基于高度的LoS概率和路径损耗特性。

Result: NLoS路径损耗指数在高海拔时降至2.5-3，LoS路径损耗指数保持在2附近，阴影衰落随高度减小；几何布局即使建筑参数固定也会导致路径损耗指数±0.2的变化。

Conclusion: 提出的统一模型与射线追踪统计结果吻合良好，为复杂城市场景中的空中基站规划提供了实用的高度相关大尺度衰落模型。

Abstract: Uncrewed Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) are expected to extend 6G millimeter-Wave (mmWave) coverage and improve link reliability in urban areas. However, UAV-based Air-to-Ground (A2G) channels are highly dependent on height and urban geometry. This paper proposes an ABS height-dependent mmWave channel model and investigates whether urban geometry, beyond the standard built-up parameters, significantly affects LoS probability (PLoS) and Large-Scale Fading (LSF). Using MATLAB ray tracing at 26 GHz, we simulate approximately 10K city realizations for four urban layouts that share identical built-up parameters but differ in their spatial organization. We extract elevation-based PLoS using a sigmoid model and derive height-dependent Path-Loss Exponents (PLEs) and shadow-fading trends using exponential fits. Results show that PLE for Non-Line-of-Sight (NLoS) decreases toward 2.5-3 at high altitudes, Line-of-Sight (LoS) PLE remains near 2, and shadow fading reduces with height. We also find that geometric layout introduces a modest but consistent change in PLE (+/- 0.2), even when built-up parameters are fixed. The proposed unified model aligns well with ray-tracing statistics and offers a practical, height-dependent LSF model suitable for ABS planning in complex urban scenarios.

</details>


### [25] [Advancing IoT System Dependability: A Deep Dive into Management and Operation Plane Separation](https://arxiv.org/abs/2511.11204)
*Luoyao Hao,Shuo Zhang,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 提出通过分离管理和操作平面来增强大规模物联网系统的可靠性，管理平面强制执行总体策略，而操作工作流程保持不变。


<details>
  <summary>Details</summary>
Motivation: 增强大规模物联网系统的可靠性，支持多方管理实体（监管机构、制造商等）参与，同时保持现有操作流程不变。

Method: 设计身份无关的策略框架，使用灵活的描述符而非固定标识符，允许主动部署具有适应性的总体策略。

Result: 在三个数据集上的评估表明，该框架能够实现接近最优的表达能力和可靠的策略执行。

Conclusion: 所提出的管理平面框架能够有效增强物联网系统的可靠性，支持多方参与的管理模式，并具有良好的适应性。

Abstract: We propose to enhance the dependability of large-scale IoT systems by separating the management and operation plane. We innovate the management plane to enforce overarching policies, such as safety norms, operation standards, and energy restrictions, and integrate multi-faceted management entities, including regulatory agencies and manufacturers, while the current IoT operational workflow remains unchanged. Central to the management plane is a meticulously designed, identity-independent policy framework that employs flexible descriptors rather than fixed identifiers, allowing for proactive deployment of overarching policies with adaptability to system changes. Our evaluation across three datasets indicates that the proposed framework can achieve near-optimal expressiveness and dependable policy enforcement.

</details>


### [26] [Use Cases, Metrics, and Challenges of Nomadic Non-Public Networks for the 6G Standardization](https://arxiv.org/abs/2511.11217)
*Daniel Lindenschmitt,Michael Gundall,Ainur Daurembekova,Marcos Rates Crippa,Mohammad Asif Habibi,Bin Han,Philipp Rosemann,Dennis Krummacker,Benedikt Veith,Hans D. Schotten*

Main category: cs.NI

TL;DR: 本文探讨了从5G非公共网络(NPNs)向游牧非公共网络(NNPNs)的演进，分析了NNPNs在6G系统中的架构、资源分配、无线回传等关键技术，以及其在应急响应、交通等领域的应用，同时指出了相关的技术、监管和安全挑战。


<details>
  <summary>Details</summary>
Motivation: 传统静态基础设施存在局限性，需要发展动态自组织网络来支持6G系统。NNPNs能够扩展移动性和适应性，克服传统网络的限制，在应急响应、交通、农业等领域发挥重要作用。

Method: 通过分析网络架构、动态资源分配和无线回传等关键技术，定义关键性能指标(KPIs)来评估NNPN应用，并建立基于移动性和操作需求的分类框架。

Result: 识别了NNPNs的具体应用场景，建立了评估标准，揭示了NNPNs在提升连接性方面的潜力，特别是在传统网络受限的环境中。

Conclusion: NNPNs是6G网络的重要组成部分，但面临架构、监管和安全挑战。需要制定适应性政策和网络架构来最大化NNPNs在下一代通信系统中的效益，这些发现有助于标准化进程。

Abstract: Wireless communication is evolving with the adoption of dynamic and self-organizing networks. They are expected to play a crucial role in shaping sixth-generation (6G) systems and the ongoing standardization process. The concept of non-public networks (NPNs) introduced in fifth-generation (5G) will be enhanced by nomadic non-public networks (NNPNs), extending mobility and adaptability beyond fixed locations. These networks help overcome the limitations of traditional static infrastructures, making them applicable to areas such as emergency response, transportation, agriculture, and others. This paper examines the transition from NPNs to NNPNs, highlighting key technical aspects such as network architecture, dynamic resource allocation, and wireless backhauling. Several use cases illustrate how NNPNs improve connectivity in environments where traditional networks are limited. Additionally, the study defines Key Performance Indicators (KPIs) to evaluate NNPN applications and establishes a framework for categorizing them based on mobility and operational requirements. Despite their advantages, NNPNs introduce architectural, regulatory, and security challenges such as new approaches for handovers, spectrum policies or cross-border functionality, and trust mechanisms to maintain reliable operations. By identifying use cases, defining evaluation criteria, and addressing technical and regulatory challenges, this paper provides insights into integrating NNPNs into future 6G networks. These findings contribute to ongoing standardization efforts and emphasize the need for adaptable policies and network architectures to maximize the benefits of NNPNs in next-generation communication systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices](https://arxiv.org/abs/2511.10680)
*Jean-Philippe Lignier*

Main category: cs.LG

TL;DR: LAD-BNet是一种针对边缘设备优化的实时能源预测神经网络，在Google Coral TPU上实现快速推理，相比传统方法在精度和速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决智能电网和智能建筑中边缘设备实时能源预测的挑战，需要兼顾预测精度和计算效率。

Method: 提出LAD-BNet架构，结合显式时间滞后分支和扩张卷积的时序卷积网络，同时捕捉短期和长期依赖关系。

Result: 在10分钟分辨率的真实能耗数据上，1小时预测的MAPE为14.49%，Edge TPU推理时间仅18ms，比CPU快8-12倍，内存占用180MB。

Conclusion: 该模型为实时能源优化、需求管理和运营规划等工业应用提供了可行的边缘计算解决方案。

Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.

</details>


### [28] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: 本文提出LT-Soups方法，解决参数高效微调在长尾数据分布中头尾类性能权衡问题，通过两阶段模型融合框架在不同不平衡程度下实现更好的性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现实数据集通常呈现长尾分布，现有参数高效微调方法虽然能保持尾类性能，但会牺牲头类准确率。头尾类比是影响这种权衡的关键但被忽视的因素。

Method: 提出LT-Soups两阶段框架：第一阶段在平衡子集上微调模型并平均以减少头类偏差；第二阶段仅在完整数据集上微调分类器以恢复头类准确率。

Result: 在六个基准数据集上的实验表明，LT-Soups相比参数高效微调和传统模型融合方法，在广泛的不平衡程度范围内实现了更优的性能权衡。

Conclusion: LT-Soups能够有效解决长尾数据分布中的头尾类性能权衡问题，在不同不平衡程度下都表现出色，为参数高效微调在长尾场景中的应用提供了新思路。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [29] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 本研究探讨了数据质量问题（缺失值、噪声属性、异常值和标签错误）对信用风险评估机器学习模型预测准确性的影响，通过引入受控数据损坏评估了10种常用模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在信用风险评估中的应用日益增多，但其有效性很大程度上依赖于输入数据的质量，需要研究数据质量问题对模型性能的影响。

Method: 使用开源数据集，通过Pucktrick库引入受控数据损坏，评估包括随机森林、SVM和逻辑回归在内的10种常用模型的鲁棒性。

Result: 实验表明，模型的鲁棒性因数据退化的性质和严重程度而存在显著差异。

Conclusion: 所提出的方法和配套工具为从业者增强数据管道鲁棒性提供了实用支持，并为研究人员在数据为中心的AI背景下进行进一步实验提供了灵活框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [30] [Differentiable Sparse Identification of Lagrangian Dynamics](https://arxiv.org/abs/2511.10706)
*Zitong Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种新颖的可微分稀疏识别框架，用于从噪声数据中发现拉格朗日力学系统的控制方程，解决了现有方法对噪声敏感和数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的控制方程发现在非线性动力学中仍具挑战性。现有稀疏回归方法在处理有理函数和复杂机械系统中的噪声时存在困难，而拉格朗日形式主义虽然提供了更简洁的系统动力学表示，但现有识别方法受测量噪声和数据可用性的显著影响。

Method: 通过三个关键贡献：1）首次将三次B样条近似集成到拉格朗日系统识别中；2）结合已知物理约束的鲁棒方程发现机制；3）基于B样条基函数的递归导数计算方案，有效约束高阶导数并降低二阶动力系统的噪声敏感性。

Result: 所提方法在复杂机械系统中表现出优越性能，与基线方法相比，能够从噪声数据中更准确可靠地提取物理规律。

Conclusion: 该方法为解决拉格朗日系统识别中的噪声敏感性和数据限制问题提供了有效解决方案，特别适用于复杂机械系统。

Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.

</details>


### [31] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: BREP ReFT通过截断训练数据优化推理前缀生成、干预早期推理阶段防止错误累积、约束干预向量幅度避免干扰数值编码，显著提升了ReFT在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: ReFT方法在数学推理任务上表现不佳，主要问题在于早期推理阶段无法生成有效的前缀，并且会干扰数值编码导致错误累积。

Method: 提出BREP ReFT方法：1）截断训练数据优化推理前缀生成；2）干预早期推理阶段防止错误累积；3）约束干预向量幅度避免干扰数值编码。

Result: 在多种模型架构上的实验表明，BREP在数学推理任务上优于标准ReFT和基于权重的PEFT方法，具有更好的效果、效率和泛化能力。

Conclusion: BREP ReFT通过针对性地解决ReFT在数学推理中的关键问题，显著提升了性能，为参数高效微调提供了新的有效方法。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [32] [EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence](https://arxiv.org/abs/2511.10834)
*Ansel Kaplan Erol,Seungjun Lee,Divya Mahajan*

Main category: cs.LG

TL;DR: EarthSight是一个分布式卫星图像智能框架，通过多任务推理、地面站查询调度和动态过滤器排序，显著降低卫星图像分析的延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像处理需要将所有图像下传到地面站分析，导致数小时到数天的延迟。现有解决方案将每颗卫星视为独立计算节点，存在冗余推理和资源浪费问题。

Method: 1) 卫星上多任务推理使用共享主干网络；2) 地面站查询调度器聚合用户请求并预测优先级；3) 动态过滤器排序整合模型选择性、准确性和执行成本，早期拒绝低价值图像。

Result: 与最先进基线相比，EarthSight将每张图像的平均计算时间减少1.9倍，并将90%分位数端到端延迟从51分钟降低到21分钟。

Conclusion: EarthSight通过结合地面站的全局上下文和轨道上的资源感知自适应决策，使星座能够在严格的下行链路带宽和机载功率预算内执行可扩展、低延迟的图像分析。

Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.

</details>


### [33] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出了生成模型学习中不确定性量化的问题，讨论了使用集成方法进行精度-召回曲线分析的研究方向，并在合成数据集上验证了该方法在捕捉模型近似不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估方法主要关注学习分布与目标分布的接近程度，但忽视了这些测量中的固有不确定性。本文旨在解决生成模型分布近似能力的不确定性量化这一关键但研究不足的问题。

Method: 提出了使用基于集成的精度-召回曲线方法，通过聚合多个模型的预测来量化不确定性，从而系统比较不同模型架构的不确定性特征。

Result: 在合成数据集上的初步实验表明，聚合精度-召回曲线能够有效捕捉模型近似不确定性，为不同模型架构的系统比较提供了依据。

Conclusion: 不确定性量化是生成模型可靠性的重要方面，基于集成的精度-召回曲线方法为解决这一问题提供了有前景的研究方向，有助于更全面地评估生成模型的性能。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [34] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 该论文研究了边缘推理中的级联赌博机模型，分析了四种决策策略的理论遗憾保证，发现LCB和Thompson Sampling因持续自适应而获得常数遗憾，优于固定排序策略。


<details>
  <summary>Details</summary>
Motivation: 受边缘推理挑战的驱动，研究在推理模型具有相关准确率和错误概率情况下的级联赌博机模型。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)和Thompson Sampling，并提供理论遗憾保证分析。

Result: LCB和Thompson Sampling通过持续更新决策获得常数O(1)遗憾，而Explore-then-Commit和Action Elimination因固定排序限制而获得次优遗憾。仿真验证了理论发现。

Conclusion: 在不确定性下的边缘推理中，自适应能力对于高效推理至关重要，LCB和Thompson Sampling因持续自适应而表现最优。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [35] [Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning](https://arxiv.org/abs/2511.10713)
*Jun Masaki,Ariaki Higashi,Naoko Shinagawa,Kazuhiko Hirata,Yuichi Kurita,Akira Furui*

Main category: cs.LG

TL;DR: 提出了一种基于深度神经网络的自动化FIM评分估计方法，使用不同于标准FIM评估动作的简单练习来评估患者的身体独立性，减轻了传统评估的负担。


<details>
  <summary>Details</summary>
Motivation: 传统FIM评估对患者和医疗专业人员都造成了显著负担，需要开发自动化的评估方法来减轻这种负担。

Method: 采用深度神经网络架构，整合了时空图卷积网络(ST-GCN)、双向长短期记忆网络(BiLSTM)和注意力机制，通过简单练习动作来估计FIM运动项目评分。

Result: 在277名康复患者的研究中，该方法成功区分了完全独立患者和需要协助的患者，在不同FIM项目上达到70.09-78.79%的平衡准确率，并识别出可作为特定FIM评估项目可靠预测因子的特定运动模式。

Conclusion: 该方法能够有效自动化FIM评分估计，减轻评估负担，同时识别出关键的运动模式特征，为康复评估提供了新的技术途径。

Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.

</details>


### [36] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 提出了一种基于迹估计的矩阵自由方法，用于快速计算神经正切核(NTK)的迹、Frobenius范数、有效秩和对齐度，相比传统方法可获得多个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 计算完整的NTK矩阵对于循环架构等复杂模型通常不可行，需要一种高效的近似计算方法。

Method: 使用Hutch++迹估计器和单边估计器（仅需前向或反向自动微分），通过随机化方法快速分析经验NTK。

Result: 矩阵自由随机化方法可实现多个数量级的加速，单边估计器在低样本情况下表现优于Hutch++，特别是在模型状态与参数数量差距较大时。

Conclusion: 矩阵自由随机化方法能够显著加速NTK的分析和应用，为复杂模型的理论研究提供了实用工具。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [37] [SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems](https://arxiv.org/abs/2511.11111)
*Xin Wang,Pietro Lodi Rizzini,Sourav Medya,Zhiling Lan*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和大型语言模型的替代模型，用于准确预测Dragonfly网络中的应用程序运行时间，支持高效的混合仿真。


<details>
  <summary>Details</summary>
Motivation: Dragonfly网络在高性能计算中是领先的互连结构，但共享网络链路上的工作负载干扰是一个主要挑战。并行离散事件仿真虽然常用但计算成本高，不适合大规模或实时场景。

Method: 开发了结合图神经网络和大型语言模型的替代模型，从端口级路由器数据中捕捉空间和时间模式。

Result: 该模型在运行时间预测方面优于现有的统计和机器学习基线方法。

Conclusion: 该方法能够实现准确的运行时间预测，支持Dragonfly网络的高效混合仿真。

Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.

</details>


### [38] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 提出了两种新的线性预测聚类全局优化方法，通过利用可分离性的理论特性推导出具有可证明误差界的近似最优解，显著提高了计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有贪婪优化方法缺乏全局最优性，在非可分聚类设置中表现不佳；而混合整数规划方法虽然能保证全局最优但计算效率低下。需要开发既保证最优性又具有良好可扩展性的方法。

Method: 基于约束优化范式，利用可分离性的理论特性推导出具有可证明误差界的近似最优解，将问题简化为二次伪布尔优化问题，显著降低混合整数规划公式的复杂度。

Result: 在合成和真实数据集上的比较分析表明，新方法始终获得近似最优解，回归误差显著低于贪婪优化方法，同时比现有混合整数规划方法具有更好的可扩展性。

Conclusion: 提出的方法在保持全局最优性的同时显著提高了计算效率，为线性预测聚类提供了实用且可扩展的解决方案。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [39] [A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication](https://arxiv.org/abs/2511.11560)
*Angelo Rodio,Giovanni Neglia,Zheng Chen,Erik G. Larsson*

Main category: cs.LG

TL;DR: 本文对半去中心化联邦学习中的两种模型分发策略（S2S和S2A）进行了理论分析和实验比较，揭示了在不同数据异质性程度下两种策略的优劣，并提出了实际部署的设计指南。


<details>
  <summary>Details</summary>
Motivation: 半去中心化联邦学习中，设备主要依赖设备间通信，但偶尔与中央服务器交互。服务器可以将聚合模型仅分享给采样客户端（S2S）或广播给所有客户端（S2A）。这两种策略在实践中有重要意义，但缺乏严格的理论和实证比较。

Method: 在统一的收敛框架下分析S2S和S2A策略，考虑关键系统参数：采样率、服务器聚合频率和网络连接性。通过理论分析和实验验证进行比较。

Result: 分析和实验结果揭示了不同的性能机制，其中一种策略优于另一种主要取决于设备间数据异质性的程度。在特定条件下，S2S或S2A策略会表现出更好的性能。

Conclusion: 研究结果为实际半去中心化联邦学习部署提供了具体的设计指南，帮助根据数据异质性程度选择合适的模型分发策略。

Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.

</details>


### [40] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 研究变压器模型预测Collatz序列长步长，发现模型准确率随编码基数变化，最高达99.7%（基数24和32），最低仅25-37%（基数3和11）。所有模型都学习相同的模式：按输入模2^p的余数分类学习，准确率接近完美，反映模型学习预测与递增循环长度相关的输入。


<details>
  <summary>Details</summary>
Motivation: 使用数学问题作为工具来理解、解释和改进语言模型，探讨模型学习复杂算术函数的能力和机制。

Method: 使用变压器模型预测Collatz序列的长步长，分析不同编码基数对模型性能的影响，研究模型学习模式和错误类型。

Result: 模型准确率随编码基数变化显著，最高99.7%，最低25%。模型按输入模2^p的余数分类学习，准确率接近完美。90%以上的错误是由于正确计算但错误估计循环长度。

Conclusion: 学习复杂算术函数的难点在于理解计算的控制结构（循环长度）。使用数学问题作为分析工具的方法可广泛应用于其他问题，有望取得丰硕成果。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [41] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 研究评估了基于Transformer的神经算子在迁移学习中的表现，证明其能够有效在不同PDE问题间传递知识。


<details>
  <summary>Details</summary>
Motivation: 神经算子的训练计算成本高昂，现有研究通过下游学习（在简单问题上预训练，在复杂问题上微调）来解决这个问题。

Method: 在通用迁移学习设置中评估基于Transformer的神经算子，测试其在多种PDE问题上的性能，包括参数外推、新变量引入和多方程数据集迁移。

Result: 结果表明，先进的神经算子架构能够有效地在PDE问题间传递知识。

Conclusion: 基于Transformer的神经算子在迁移学习环境中具有良好表现，能够跨PDE问题有效传递知识。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [42] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 本文提出了一个变分量子核框架，在8个高维真实世界数据集上验证了量子核方法相对于经典RBF核的性能优势。


<details>
  <summary>Details</summary>
Motivation: 量子核方法在真实世界高维数据上的实际优势尚未得到验证，当前研究主要局限于低维或合成数据集。

Method: 开发了使用资源高效ansätze的变分量子核框架，并引入了参数缩放技术来加速收敛。

Result: 在8个具有挑战性的真实世界高维数据集上的经典模拟结果表明，所提出的量子核比标准经典核（如RBF核）具有明显的性能优势。

Conclusion: 适当设计的量子核可以作为多功能、高性能的工具，为现实世界机器学习中的量子增强应用奠定基础，但需要进一步研究来全面评估实际量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [43] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: SurfaceBench是首个用于符号曲面发现的综合基准，包含183个任务，涵盖15类符号复杂度，采用显式、隐式和参数化方程表示形式，通过几何感知指标评估方程发现质量。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法主要关注标量函数，忽略领域基础，依赖脆弱的字符串匹配指标，无法捕捉科学等价性。需要建立能够评估几何重建准确性的基准。

Method: 构建包含183个任务的基准数据集，涵盖不同符号复杂度和表示形式，使用合成采样的三维数据，结合符号检查和几何感知指标（如Chamfer和Hausdorff距离）进行评估。

Result: 实验表明，最先进的框架虽然在特定函数族上偶尔成功，但难以在表示类型和曲面复杂度之间泛化。

Conclusion: SurfaceBench建立了一个具有挑战性的诊断测试平台，连接符号推理与几何重建，为组合泛化、数据驱动科学归纳和几何感知推理的进展提供原则性基准。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [44] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出了一种区分外在和内在幻觉的评估框架，并利用注意力机制改进幻觉检测性能，发现基于采样的方法适合检测外在幻觉，而基于注意力的方法更适合内在幻觉。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型，需要更有效的检测策略。

Method: 使用基于注意力的不确定性量化算法，提出新颖的注意力聚合策略，构建区分外在和内在幻觉的评估框架。

Result: 基于采样的方法如语义熵能有效检测外在幻觉但无法处理内在幻觉，而基于输入token注意力聚合的方法更适合内在幻觉检测。

Conclusion: 注意力机制是量化模型不确定性的重要信号，为根据幻觉性质调整检测策略提供了新方向。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [45] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath提出了一种通过可逆神经流学习控制路径几何形状的新方法，用于处理稀疏和不规则采样的时间序列建模问题，相比固定插值方法在分类准确率上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程的性能高度依赖于从离散观测构建的控制路径选择，固定插值方案强加了简化的几何假设，往往不能准确表示底层数据流形，特别是在高缺失率情况下。

Method: FlowPath通过可逆神经流学习控制路径的几何形状，构建连续且数据自适应的流形，通过可逆性约束强制执行信息保持和良好行为的变换。

Result: 在18个基准数据集和真实案例研究中的实证评估表明，FlowPath相比使用固定插值或不可逆架构的基线方法，在分类准确率上持续取得统计显著的改进。

Conclusion: 结果强调了不仅需要建模路径上的动态，还需要建模路径本身的几何形状，为从不规则时间序列学习提供了鲁棒且可泛化的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [46] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 该论文提出了一种利用行为策略收集离线数据的方法，通过降低回报估计的方差来提高强化学习算法的样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。研究发现精心设计的行为策略可以收集离线数据以获得方差更低的回报估计，这挑战了传统认为在线收集数据方差最优的观点。

Method: 将离线策略评估的关键见解扩展到在线强化学习设置中，使用单一行为策略收集数据用于策略改进，通过重要性加权样本进行去偏和方差管理。在实验中扩展了两种策略梯度方法。

Result: 实验结果表明，该方法在多种环境中表现出更好的样本效率和性能。

Conclusion: 利用精心设计的行为策略收集离线数据可以显著降低回报估计的方差，从而提高强化学习算法的效率和稳定性。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [47] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 提出了STAMP适配器，将通用时间序列基础模型应用于EEG数据，性能媲美专用EEG基础模型


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对EEG专用基础模型与通用时间序列基础模型在EEG任务上的对比分析，需要开发轻量级适配器来利用通用模型处理EEG数据

Method: 设计空间-时间适配器STAMP，利用通用TSFM的单变量嵌入，隐式建模EEG数据的时空特征，支持多头池化

Result: 在8个临床EEG分类基准数据集上表现优异，性能与最先进的EEG专用基础模型相当，且参数轻量、输入灵活

Conclusion: STAMP适配器为使用通用时间序列基础模型处理EEG数据提供了轻量、灵活的解决方案，无需专门训练EEG专用模型

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [48] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种精确的代码选择算法，通过向LLM提出两种新型查询（成对成员资格和成对等价性）来选择最佳程序，在四个流行代码数据集上平均比最先进算法提升13.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法可能无法识别正确程序，因为它们可能错误识别非等价程序，或者依赖LLM并假设它总能正确确定每个输入的输出。

Method: 提出ExPairT-LLM算法，通过向LLM提出两种新型查询（成对成员资格和成对等价性）来选择程序，这些查询对LLM更简单，并通过锦标赛机制识别正确程序，对某些LLM错误具有鲁棒性。

Result: 在四个流行代码数据集上，ExPairT-LLM的pass@1（成功率）平均比最先进的代码选择算法高出+13.0%，最高可达+27.1%。同时将执行复杂推理的LLMs的pass@1提高了+24.0%。

Conclusion: ExPairT-LLM通过引入更简单的查询类型和锦标赛机制，有效提升了代码选择的准确性和鲁棒性，显著优于现有方法。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [49] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 该论文提出了一种利用公共数据指导私有零阶优化算法梯度近似的方法，在保持隐私的同时显著提升了模型效用和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私机器学习算法（如DP-SGD）存在计算和内存成本高的问题，而零阶方法虽然易于隐私化，但效用相对较低且应用领域有限。

Method: 提出了PAZO框架，利用公共数据来指导和改进私有零阶算法的梯度近似，包含一系列具有最小开销的公共数据辅助零阶优化器。

Result: PAZO在视觉和文本任务中实现了优越的隐私/效用权衡，在高度隐私保护机制下优于最佳的一阶基线方法，同时提供高达16倍的运行速度提升。

Conclusion: 利用公共数据辅助的零阶优化方法能够有效解决差分隐私机器学习中的计算效率和效用平衡问题，特别是在高隐私保护需求场景下表现优异。

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [50] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在训练数据不平衡问题，特别是在Golang语言中单元测试生成任务上的表现不足。


<details>
  <summary>Details</summary>
Motivation: 解决代码LLM训练数据不平衡问题，特别是开源代码数据过多而软件工程任务（如单元测试生成）数据不足的问题，尤其是在低资源语言如Golang中。

Method: 引入GO UT Bench基准数据集，包含5264对代码和单元测试，来自10个不同领域的Golang仓库。在两个LLM家族（专家混合模型和密集解码器）上进行微调评估。

Result: 微调后的模型在超过75%的基准任务上优于其基础模型。

Conclusion: GO UT Bench作为微调数据集有效提升了代码LLM在单元测试生成任务上的性能，特别是在Golang语言环境中。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [51] [Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations](https://arxiv.org/abs/2511.10872)
*Shuyuan Zhang,Zihan Wang,Xiao-Wen Chang,Doina Precup*

Main category: cs.LG

TL;DR: 提出G4RL方法，通过图编码器-解码器评估未见状态，解决图引导分层强化学习中样本效率低和子目标表示差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图引导分层强化学习方法依赖领域知识构建图，或动态构建图但难以充分利用图信息传递给新访问状态，存在样本效率低和子目标表示差的问题。

Method: 开发图编码器-解码器评估未见状态，G4RL方法可集成到任何现有GCHRL方法中，在具有对称和可逆转换的环境中使用。

Result: 经验结果表明，利用图编码器-解码器的高层和低层内在奖励显著提升了最先进GCHRL方法的性能，计算成本增加很小。

Conclusion: G4RL方法有效解决了图引导分层强化学习中的关键挑战，在密集和稀疏奖励环境中都能显著提升性能。

Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.

</details>


### [52] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种物理信息深度学习框架，直接从运动学数据估计肌肉激活和力，无需标注数据即可获得生理一致的预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高且缺乏高质量的多关节标注数据集，需要开发时间高效且无需标注数据的肌肉激活和力估计方法。

Method: 使用新颖的多关节交叉注意力模块（MJCA）和双向门控循环单元（BiGRU）层来捕捉关节间协调，通过物理信息损失函数嵌入多关节动力学、关节间耦合和外部力相互作用。

Result: 在两个数据集上的实验验证表明，该方法性能可与传统监督方法相媲美，且MJCA模块显著增强了关节间协调建模。

Conclusion: PI-MJCA-BiGRU框架能够实现时间高效的推理，提供生理一致的肌肉激活和力估计，无需地面真值标签。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [53] [Multi-View Polymer Representations for the Open Polymer Prediction](https://arxiv.org/abs/2511.10893)
*Wonjin Jung,Yongseok Choi*

Main category: cs.LG

TL;DR: 提出了一种多视角聚合物属性预测方法，通过集成四种不同表示方法的预测结果，在NeurIPS 2025 Open Polymer Prediction Challenge中排名第9。


<details>
  <summary>Details</summary>
Motivation: 利用互补的表示方法来提高聚合物属性预测的准确性，通过多视角设计整合不同数据表示的优势。

Method: 集成四种表示方法：RDKit/Morgan描述符、图神经网络、3D信息表示和预训练SMILES语言模型，采用均匀集成策略平均各方法的预测结果，使用10折交叉验证训练和SMILES测试时增强进行评估。

Result: 在2241个团队中排名第9，提交的集成模型在公开测试集上MAE为0.057，在私有测试集上MAE为0.082。

Conclusion: 多视角集成方法在聚合物属性预测任务中表现优异，证明了利用互补表示的有效性。

Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.

</details>


### [54] [Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters](https://arxiv.org/abs/2511.10898)
*Chenghao Duan,Chuanyi Ji*

Main category: cs.LG

TL;DR: 提出一种基于图注意力网络(GAT)的方法来预测恶劣天气导致的停电持续时间，通过无监督预训练和半监督学习，在四个飓风影响的数据集上表现优异，准确率超过93%。


<details>
  <summary>Details</summary>
Motivation: 自然灾害导致的大规模停电造成巨大经济和社会影响，准确预测停电恢复时间对电网韧性至关重要。现有方法面临空间依赖性、空间异质性和事件数据有限三大挑战。

Method: 使用图注意力网络(GAT)，结合无监督预训练和半监督学习框架，利用地理空间和天气数据来估计停电持续时间。

Result: 在影响美国东南部8个州501个县的四个主要飓风数据上，模型准确率超过93%，比XGBoost、随机森林、GCN和简单GAT方法提升2%-15%。

Conclusion: 提出的GAT方法在预测恶劣天气导致的停电持续时间方面表现出色，显著优于现有方法，为解决电网韧性预测问题提供了有效解决方案。

Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.

</details>


### [55] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出了SPP-FGC算法，通过本地结构图作为隐私保护知识共享媒介，解决了联邦聚类中性能与隐私的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中传统方法面临的困境：传输嵌入表示会泄露敏感数据，而仅共享抽象聚类原型会导致模型准确性下降。

Method: 基于客户端-服务器架构，客户端构建捕捉数据内在关系的私有结构图，服务器安全聚合和对齐这些图形成全局图，从中推导统一聚类结构。提供SPP-FGC（一次性方法）和SPP-FGC+（迭代方法）两种模式。

Result: 在广泛实验中实现了最先进性能，相比联邦基线将聚类准确性提高了高达10%（NMI），同时保持可证明的隐私保证。

Conclusion: SPP-FGC框架通过结构图作为知识共享媒介，成功解决了联邦聚类中性能与隐私的权衡问题，为不同需求提供了高效和强大的解决方案。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [56] [GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning](https://arxiv.org/abs/2511.10936)
*Ying Song,Balaji Palanisamy*

Main category: cs.LG

TL;DR: GraphToxin是首个针对图遗忘的图重建攻击方法，通过曲率匹配模块实现细粒度指导，能够恢复被删除的个体信息、个人链接和连接中的敏感内容，现有防御机制对其基本无效。


<details>
  <summary>Details</summary>
Motivation: 图遗忘虽然能响应"被遗忘权"要求删除敏感信息，但多方参与创造了新的攻击面，残留痕迹仍可能存在于遗忘后的图神经网络中，攻击者可以利用这些漏洞恢复被删除的样本。

Method: 提出GraphToxin攻击方法，引入新颖的曲率匹配模块为完整的遗忘图恢复提供细粒度指导，支持白盒和黑盒设置下的多节点移除，并提出了包含随机和最坏情况节点移除的全面评估框架。

Result: 实验证明GraphToxin能成功颠覆图遗忘的监管保证，不仅能恢复被删除个体的信息和链接，还能恢复其连接的敏感内容，现有防御机制对其基本无效甚至可能增强攻击效果。

Conclusion: GraphToxin带来的严重隐私风险凸显了开发更有效和鲁棒防御策略的紧迫性，需要进行最坏情况分析来系统评估图遗忘方法对图重建攻击的脆弱性。

Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.

</details>


### [57] [Flow matching-based generative models for MIMO channel estimation](https://arxiv.org/abs/2511.10941)
*Wenkai Liu,Nan Ma,Jianqiao Chen,Xiaoxuan Qi,Yuhang Ma*

Main category: cs.LG

TL;DR: 提出基于流匹配的生成模型用于MIMO信道估计，通过直线轨迹演化路径和速度场训练，显著减少采样开销并提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的信道估计方法虽然精度高，但采样速度慢是主要挑战，需要解决采样效率问题。

Method: 在流匹配框架下构建从噪声信道分布到真实信道分布的条件概率路径，推导仅依赖噪声统计的速度场，训练生成模型，使用ODE欧拉求解器进行快速噪声信道增强。

Result: 相比基于分数匹配的扩散模型方案，显著减少了采样开销，在不同信道条件下实现了优越的信道估计精度。

Conclusion: 提出的基于流匹配的信道估计方案在保持高精度的同时，有效解决了扩散模型采样速度慢的问题，具有实际应用价值。

Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.

</details>


### [58] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 提出一种新的可控模型融合方法，通过直接修正模型最终表示的最优线性变换，替代昂贵的离线多目标优化，实现线性复杂度、即时生成帕累托最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法面临参数干扰问题，且现有可控融合方法采用编译-查询范式，离线优化成本高、复杂度随任务数指数增长。

Method: 将视角从参数空间优化转向直接修正模型最终表示，将该修正建模为最优线性变换，获得闭式解，单步计算即可生成偏好感知模型。

Result: 实验结果显示该方法生成的帕累托前沿更优，偏好对齐更精确，计算成本大幅降低。

Conclusion: 该方法通过表示空间的最优线性变换，实现了高效、可控的模型融合，显著优于现有方法。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [59] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 本文提出了无监督鲁棒域自适应（URDA）新范式，解决了传统UDA方法忽视对抗攻击鲁棒性的问题，并开发了DART算法来同时保证迁移性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法强调迁移能力但忽视对抗攻击鲁棒性，而标准对抗训练在UDA中效果不佳。本文旨在解决UDA中的鲁棒性问题，探索对抗训练在UDA中失效的原因，并建立新的理论框架。

Method: 提出URDA新范式，揭示UDA+VAT中的内在纠缠问题，并开发DART算法：先预训练任意UDA模型，然后通过解纠缠蒸馏进行瞬时鲁棒化后训练步骤。

Result: 在四个基准数据集上的实验表明，DART能有效增强鲁棒性同时保持域适应性，验证了URDA范式和理论的有效性。

Conclusion: 本文首次建立了URDA范式和理论，提出的DART算法简单有效，能同时实现迁移性和鲁棒性，为无监督域自适应的鲁棒性研究提供了新方向。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [60] [Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing](https://arxiv.org/abs/2511.11046)
*Brian Godwin Lim*

Main category: cs.LG

TL;DR: 提出了邻域上下文消息传递（NCMP）框架，通过整合更广泛的邻域上下文信息来增强经典图神经网络的消息传递能力，并开发了SINC-GCN模型进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递GNN仅考虑中心节点与单个邻居节点的特征，未能利用整个邻域的丰富上下文信息，限制了学习复杂关系的能力。

Method: 基于注意力变体的关键属性形式化邻域上下文概念，提出NCMP框架，并开发了SINC-GCN模型来参数化和操作化NCMP。

Result: 在合成二元节点分类问题上的初步分析验证了所提GNN架构的表达能力和效率。

Conclusion: NCMP框架为增强经典GNN的图表示能力提供了一条实用路径。

Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.

</details>


### [61] [Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning](https://arxiv.org/abs/2511.11081)
*Jun Hu,Shangheng Chen,Yufei He,Yuan Li,Bryan Hooi,Bingsheng He*

Main category: cs.LG

TL;DR: 提出了Echoless-LP方法，通过分区聚焦无回声传播(PFEP)解决基于标签预计算的异质图神经网络中的训练标签泄漏问题，同时保持内存效率和兼容性。


<details>
  <summary>Details</summary>
Motivation: 传统端到端HGNNs在大规模图上训练效率低，基于预计算的HGNNs存在训练标签泄漏问题（回声效应），现有缓解策略在大型图上内存效率低或与高级消息传递方法不兼容。

Method: 提出Echoless-LP方法，使用PFEP将目标节点分区，每个分区中的节点只从其他分区的邻居收集标签信息，避免回声效应。还引入非对称分区方案(APS)和PostAdjust机制处理分区造成的信息损失和分布偏移。

Result: 在公共数据集上的实验表明，Echoless-LP相比基线方法实现了更优的性能，同时保持了内存效率。

Conclusion: Echoless-LP有效解决了基于标签预计算的HGNNs中的训练标签泄漏问题，为大规模异质图学习提供了高效且兼容的解决方案。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.

</details>


### [62] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: 本文提出了ScaPT（可扩展群体训练）框架，通过元代理和互信息正则化器实现高效的大规模群体训练，在Hanabi游戏中验证了其优于现有方法的零样本协调性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的训练方法受限于计算资源，主要在小规模群体中优化多样性，忽视了扩大群体规模带来的性能提升潜力。

Method: 提出ScaPT框架，包含两个关键组件：通过选择性共享参数实现群体的元代理，以及保证群体多样性的互信息正则化器。

Result: 在Hanabi游戏中与代表性框架进行对比评估，证实了ScaPT的优越性能。

Conclusion: ScaPT能够有效解决零样本协调中的群体规模扩展问题，提供更好的协调性能。

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [63] [Sheaf Cohomology of Linear Predictive Coding Networks](https://arxiv.org/abs/2511.11092)
*Jeffrey Seely*

Main category: cs.LG

TL;DR: 该论文将线性预测编码网络建模为胞腔层，揭示了网络拓扑结构与学习动态之间的深层联系，特别关注反馈回路导致的内部矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 预测编码网络使用局部优化替代全局反向传播，但反馈拓扑可能引入与监督无关的内部矛盾，导致学习停滞。需要理论工具来分析这些网络配置问题。

Method: 使用胞腔层理论框架：将激活映射为层上链，预测误差作为边缘误差，PC推理是层拉普拉斯算子下的扩散过程。利用层上同调分析不可约误差模式，通过Hodge分解识别内部矛盾。

Result: 发现反馈回路会创建内部矛盾，引入与监督无关的预测误差。层上同调能够表征推理无法消除的误差模式，Hodge分解可确定这些矛盾何时导致学习停滞。

Conclusion: 层形式主义为识别有问题的网络配置提供了诊断工具，并为循环PC网络的有效权重初始化提供了设计原则。

Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.

</details>


### [64] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出了一种新的知识图谱嵌入初始化策略，利用KG模式和先前学习的嵌入来为新实体获取初始表示，从而提高预测性能、增强知识保留并加速知识获取。


<details>
  <summary>Details</summary>
Motivation: 知识图谱频繁更新，需要其嵌入能够适应这些变化。现有持续学习方法中，嵌入初始化对最终嵌入的准确性和训练时间有重要影响，特别是对于相对较小且频繁的更新。

Method: 利用知识图谱模式和先前学习的嵌入，基于实体所属的类别为新实体获取初始表示，可以无缝集成到现有的KGE持续学习方法中。

Result: 实验分析表明，该初始化策略提高了KGE的预测性能，增强了知识保留，并加速了知识获取，减少了增量学习新嵌入所需的训练轮次和时间。

Conclusion: 所提出的初始化策略在各类KGE学习模型中均表现出优势，能够有效增强新知识的获取并减少灾难性遗忘。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [65] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 提出并评估了多种在中等和高维数据集中计算效率高的稳健方法，用于检测银行账户余额中的点异常，应用于260万条匿名用户银行账户余额的每日记录。


<details>
  <summary>Details</summary>
Motivation: 检测银行账户余额中的点异常对金融机构至关重要，可以识别潜在的欺诈、操作问题或其他异常情况。稳健统计在标记异常值和提供不受污染观测影响的数据分布参数估计方面很有用，但在高维设置下通常效率较低且计算成本高。

Method: 提出并评估了多种稳健方法，这些方法在中等和高维数据集中可能具有计算效率，具有高崩溃点和低计算时间。

Result: 应用处理了约260万条匿名用户银行账户余额的每日记录。

Conclusion: 所提出的稳健方法在高维金融数据异常检测中具有实际应用价值，能够平衡计算效率和检测准确性。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [66] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习框架，用于印度四个主要城市的短期降水预测，结合CNN-ConvLSTM架构和多种可解释性分析方法，实现了准确预测并揭示了城市特定的降水模式。


<details>
  <summary>Details</summary>
Motivation: 深度学习降水预测模型通常作为黑箱运行，限制了其在现实天气预报中的应用。为了在保持准确性的同时增强透明度，需要开发可解释的深度学习框架。

Method: 采用混合时间分布式CNN-ConvLSTM架构，基于多年代ERA5再分析数据进行训练。为每个城市优化卷积滤波器数量：班加罗尔(32)、孟买和德里(64)、加尔各答(128)。使用排列重要性、Grad-CAM、时间遮挡和反事实扰动进行可解释性分析。

Result: 模型在四个城市均取得良好表现：班加罗尔RMSE 0.21 mm/天，孟买0.52 mm/天，德里0.48 mm/天，加尔各答1.80 mm/天。预测时间范围从班加罗尔的1天到加尔各答的5天不等。

Conclusion: 研究表明可解释AI能够为不同城市环境提供准确的降水预测和透明的洞察，识别出城市特定的变量依赖模式和预测时间范围差异。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [67] [Adaptive Symmetrization of the KL Divergence](https://arxiv.org/abs/2511.11159)
*Omri Ben-Dov,Luiz F. O. Chamon*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法来最小化Jeffreys散度，通过使用代理模型来辅助优化主模型的Jeffreys散度，将联合训练任务表述为约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中许多任务可以简化为从有限样本学习概率分布。虽然前向KL散度因其易处理性而被广泛使用，但其不对称性可能无法捕捉目标分布的所有特性。对称替代方案如Jeffreys散度计算困难，需要开发新方法。

Method: 使用代理模型来辅助优化主模型的Jeffreys散度，将联合训练任务表述为约束优化问题，获得一个在实际训练过程中调整模型优先级的实用算法。

Result: 该框架可以结合归一化流( NFs )和能量基模型( EBMs )的优势，应用于密度估计、图像生成和基于模拟的推理等任务。

Conclusion: 提出的方法能够有效最小化Jeffreys散度，为结合不同类型生成模型的优势提供了实用框架。

Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.

</details>


### [68] [Training Neural Networks at Any Scale](https://arxiv.org/abs/2511.11163)
*Thomas Pethick,Kimon Antonakopoulos,Antonio Silveti-Falls,Leena Chennuru Vankadara,Volkan Cevher*

Main category: cs.LG

TL;DR: 现代神经网络训练优化方法综述，重点关注效率和可扩展性，介绍了最先进的优化算法及其统一模板。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模和复杂度的增长，需要高效的优化方法来处理大规模训练问题，使算法能够适应不同规模的问题。

Method: 提出统一的算法模板，强调适应问题结构的重要性，并讨论如何使算法对问题规模不敏感。

Result: 系统性地介绍了现代优化方法，为实践者和研究者提供了理解和应用这些新发展的基础。

Conclusion: 本文为神经网络优化领域提供了全面的介绍，有助于推动高效可扩展优化方法的研究和应用。

Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.

</details>


### [69] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 使用机器学习方法改进热浪等气候极端事件预测，通过功率均值聚合集成预测显著提升分类器性能


<details>
  <summary>Details</summary>
Motivation: 解决气候极端事件（特别是热浪）预测的关键挑战，改进对表面气温超过局部分位数的预测能力

Method: 将问题构建为分类问题，使用基于机器学习的天气预测模型，应用功率均值非线性聚合方法来整合集成预测

Result: 功率聚合方法比典型均值预测在极端热事件预测中表现更好，且对更高极端事件的预测效果更佳

Conclusion: 功率均值聚合方法在气候极端事件预测中显示出潜力和适应性，其最优性能随所选分位数阈值而变化

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [70] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了一种结合稀疏识别和卡尔曼滤波的新方法SKF，用于实时学习非线性动力系统的时变参数模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以实时识别复杂非线性系统的时变参数，需要将稀疏驱动方法与实时滤波技术相结合。

Method: 将Sindy算法与卡尔曼滤波器结合，将未知系统参数视为状态变量，通过前瞻误差优化参数识别策略。

Result: 在参数漂移或切换的混沌Lorenz系统和真实飞行数据构建的飞机模型上验证了方法的有效性。

Conclusion: SKF方法能够实时推断复杂时变非线性模型，显著简化了稀疏水平、方差参数和切换时刻的估计。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [71] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出了一种名为DGIMVCM的动态深度图学习方法，用于解决不完整多视图聚类问题，通过动态图学习、掩码图重构损失和对比学习来提高聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的不完整多视图聚类方法存在两个主要问题：(1) 使用KNN构建静态图会引入噪声并降低图拓扑的鲁棒性；(2) 直接使用MSE损失作为图重构损失会在优化过程中产生大量梯度噪声。

Method: 首先构建缺失鲁棒的全局图，设计图卷积嵌入层提取主要特征和精炼的动态视图特定图结构，通过图结构对比学习识别视图间一致性；然后引入图自注意力编码器提取高层表示，使用掩码图重构损失优化；最后构建聚类模块并通过伪标签自监督训练机制优化。

Result: 在多个数据集上的广泛实验验证了DGIMVCM的有效性和优越性。

Conclusion: DGIMVCM方法通过动态图学习、掩码图重构损失和对比学习机制，有效解决了不完整多视图聚类中的图构建噪声和梯度噪声问题，取得了优异的聚类性能。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [72] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass：一种强化学习模型，用于在未知环境中高效定位LoRa标签，通过空间感知特征提取和策略蒸馏来应对域偏移和信号波动，在80km²测试区域中成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的定位方法容易受到域偏移和信号波动的影响，导致级联决策错误和显著的定位不准确性，需要开发更鲁棒的搜索方法。

Method: 提出LoRaCompass模型，包含空间感知特征提取器和策略蒸馏损失函数来学习鲁棒的空间表示，并引入受上置信界启发的探索函数来引导传感器向标签移动。

Result: 在超过80km²的多样化未见环境中验证，定位成功率超过90%（比现有方法提高40%），搜索路径长度与初始距离呈线性比例关系。

Conclusion: LoRaCompass能够实现鲁棒且高效的LoRa标签搜索，在域偏移和信号波动下仍能保持高成功率，搜索效率显著优于现有方法。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [73] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种零样本合成验证框架，利用生成式AI监控联邦学习模型性能并确定早期停止点，可减少高达74%的训练轮次，同时保持准确率在最优值的1%以内。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常预设固定的全局训练轮次，导致在达到最优性能后仍进行不必要的计算，或在模型无法达到有意义的性能时继续训练，造成计算资源浪费。

Method: 引入零样本合成验证框架，利用生成式AI监控模型性能并自适应地确定早期停止点，在接近最优轮次时停止训练。

Result: 在多标签胸部X光分类任务上的数值结果显示，该方法可减少高达74%的训练轮次，同时准确率保持在最优值的1%以内。

Conclusion: 该框架有效解决了联邦学习的计算效率问题，既节省了计算资源，又支持快速超参数调整。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [74] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提供了一个对Tsallis-INF多臂老虎机算法最佳世界保证的简化推导，避免了共轭函数的使用，采用在线凸优化工具，不优化边界常数以获得更简洁的证明。


<details>
  <summary>Details</summary>
Motivation: 为Tsallis-INF算法的最佳世界保证提供一个更简单、更直接的推导方法，避免复杂的数学工具，使证明更加清晰易懂。

Method: 使用现代在线凸优化工具，避免使用共轭函数，不优化边界常数，专注于推导过程的简洁性。

Result: 成功推导出Tsallis-INF算法在随机和对抗性老虎机问题中的最佳世界保证，提供了一个更加简洁的证明版本。

Conclusion: 通过简化推导过程，证明了Tsallis-INF算法的有效性，同时保持了证明的严谨性，为相关研究提供了更易理解的理论基础。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [75] [Sparse Methods for Vector Embeddings of TPC Data](https://arxiv.org/abs/2511.11221)
*Tyler Wheeler,Michelle P. Kuchera,Raghuram Ramanujan,Ryan Krupp,Chris Wrede,Saiprasad Ravishankar,Connor L. Cross,Hoi Yan Ian Heung,Andrew J. Jones,Benjamin Votaw*

Main category: cs.LG

TL;DR: 本文探索了在时间投影室(TPC)数据上使用稀疏卷积网络进行表示学习，发现即使随机权重的稀疏ResNet架构也能提供有用的事件嵌入向量，通过物理任务预训练可进一步提升嵌入质量。


<details>
  <summary>Details</summary>
Motivation: TPC是核物理实验中广泛使用的探测器，需要有效的方法来处理其产生的复杂数据。研究旨在开发通用的表示学习工具，能够跨不同TPC实验应用。

Method: 将原始pad级信号表示为稀疏张量，使用Minkowski Engine ResNet模型进行训练，在GADGET II TPC数据上进行预训练，并测试在AT-TPC数据上的泛化能力。

Result: 发现即使未经训练的稀疏ResNet模型也能提供有用的AT-TPC数据嵌入，在GADGET数据上训练后性能进一步提升，嵌入揭示了丰富的事件结构。

Conclusion: 稀疏卷积技术有潜力成为TPC实验中通用的表示学习工具，能够跨不同探测器应用。

Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.

</details>


### [76] [Neural Network-Powered Finger-Drawn Biometric Authentication](https://arxiv.org/abs/2511.11235)
*Maan Al Balkhi,Kordian Gontarska,Marko Harasic,Adrian Paschke*

Main category: cs.LG

TL;DR: 基于神经网络的手指绘制数字生物认证研究，在触摸屏设备上通过绘制简单数字(0-9)进行用户认证，比较了CNN和自编码器架构的性能。


<details>
  <summary>Details</summary>
Motivation: 探索在触摸屏设备上使用手指绘制数字作为生物认证方法的可行性，为移动应用提供安全且用户友好的认证解决方案。

Method: 使用20名参与者在个人触摸屏设备上绘制2000个手指数字，比较了改进的Inception-V1网络、轻量级浅层CNN，以及卷积和全连接自编码器用于异常检测。

Result: 两种CNN架构均达到约89%的认证准确率，浅层CNN参数更少；自编码器方法达到约75%准确率。

Conclusion: 手指绘制符号认证为触摸屏设备提供了可行、安全且用户友好的生物认证解决方案，可与现有基于图案的认证方法集成构建多层安全系统。

Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.

</details>


### [77] [Virtual Width Networks](https://arxiv.org/abs/2511.11238)
*Seed,Baisheng Li,Banggu Wu,Bole Ma,Bowen Xiao,Chaoyi Zhang,Cheng Li,Chengyi Wang,Chenyin Xu,Chi Zhang,Chong Hu,Daoguang Zan,Defa Zhu,Dongyu Xu,Du Li,Faming Wu,Fan Xia,Ge Zhang,Guang Shi,Haobin Chen,Hongyu Zhu,Hongzhi Huang,Huan Zhou,Huanzhang Dou,Jianhui Duan,Jianqiao Lu,Jianyu Jiang,Jiayi Xu,Jiecao Chen,Jin Chen,Jin Ma,Jing Su,Jingji Chen,Jun Wang,Jun Yuan,Juncai Liu,Jundong Zhou,Kai Hua,Kai Shen,Kai Xiang,Kaiyuan Chen,Kang Liu,Ke Shen,Liang Xiang,Lin Yan,Lishu Luo,Mengyao Zhang,Ming Ding,Mofan Zhang,Nianning Liang,Peng Li,Penghao Huang,Pengpeng Mu,Qi Huang,Qianli Ma,Qiyang Min,Qiying Yu,Renming Pang,Ru Zhang,Shen Yan,Shen Yan,Shixiong Zhao,Shuaishuai Cao,Shuang Wu,Siyan Chen,Siyu Li,Siyuan Qiao,Tao Sun,Tian Xin,Tiantian Fan,Ting Huang,Ting-Han Fan,Wei Jia,Wenqiang Zhang,Wenxuan Liu,Xiangzhong Wu,Xiaochen Zuo,Xiaoying Jia,Ximing Yang,Xin Liu,Xin Yu,Xingyan Bin,Xintong Hao,Xiongcai Luo,Xujing Li,Xun Zhou,Yanghua Peng,Yangrui Chen,Yi Lin,Yichong Leng,Yinghao Li,Yingshuan Song,Yiyuan Ma,Yong Shan,Yongan Xiang,Yonghui Wu,Yongtao Zhang,Yongzhen Yao,Yu Bao,Yuehang Yang,Yufeng Yuan,Yunshui Li,Yuqiao Xian,Yutao Zeng,Yuxuan Wang,Zehua Hong,Zehua Wang,Zengzhi Wang,Zeyu Yang,Zhengqiang Yin,Zhenyi Lu,Zhexi Zhang,Zhi Chen,Zhi Zhang,Zhiqi Lin,Zihao Huang,Zilin Xu,Ziyun Wei,Zuo Wang*

Main category: cs.LG

TL;DR: Virtual Width Networks (VWN) 框架通过解耦表示宽度和主干网络宽度，在不显著增加计算成本的情况下扩展表示空间，显著加速优化过程。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过增加隐藏层大小来获得更宽表示，但会带来二次计算成本增长。VWN旨在获得宽表示的好处而不产生高计算代价。

Method: VWN将表示宽度与主干网络宽度解耦，扩展嵌入空间同时保持主干计算几乎不变。

Result: 8倍扩展使下一个token预测优化加速超过2倍，下两个token预测加速3倍。随着训练进行，损失差距增大且收敛加速比增加。发现虚拟宽度与损失减少之间存在近似对数线性缩放关系。

Conclusion: VWN不仅token高效，而且随着规模扩大效果更显著。虚拟宽度缩放可作为大型模型效率的新维度进行探索。

Abstract: We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.

</details>


### [78] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个为Split Federated Learning设计的统一防御框架，能够端到端检测和恢复五种复杂的数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning虽然是一种新兴的隐私保护分布式学习范式，但仍然容易受到针对本地特征、标签、粉碎数据和模型权重的复杂数据投毒攻击。现有的防御方法主要从传统联邦学习改编而来，在SFL中效果较差。

Method: HealSplit包含三个关键组件：(1)拓扑感知检测模块，通过拓扑异常评分识别投毒样本；(2)生成式恢复管道，为检测到的异常合成语义一致的替代品；(3)对抗性多教师蒸馏框架，使用语义监督和异常感知信号训练学生模型。

Result: 在四个基准数据集上的广泛实验表明，HealSplit在十种最先进的防御方法中表现最佳，在各种攻击场景下实现了卓越的鲁棒性和防御效果。

Conclusion: HealSplit为Split Federated Learning提供了一个有效的统一防御框架，能够显著提升系统对复杂数据投毒攻击的抵御能力。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [79] [Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels](https://arxiv.org/abs/2511.11245)
*Hong Huang,Chengyu Yao,Haiming Chen,Hang Gao*

Main category: cs.LG

TL;DR: 提出了NASK（邻域感知星形核），一种用于属性图学习的新型图核方法，能够同时处理数值和分类属性，并整合多尺度邻域结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有的图核方法难以同时捕捉属性图中的异构属性语义和邻域信息，而属性图在社交网络、生物信息学等领域广泛存在。

Method: 使用Gower相似系数的指数变换来联合建模数值和分类特征，并通过Weisfeiler-Lehman迭代增强的星形子结构来整合多尺度邻域结构信息。

Result: 在11个属性图和4个大规模真实图基准测试中，NASK始终优于16个最先进的基线方法，包括9个图核和7个图神经网络。

Conclusion: NASK是一种正定的图核方法，与基于核的学习框架兼容，在属性图学习任务中表现出卓越性能。

Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.

</details>


### [80] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: 基于电子健康记录（EHR）的预测模型在识别癌症高风险人群方面比传统风险因素表现更优，能实现3-6倍的真实癌症病例富集，有望支持更精准和可扩展的早期检测策略。


<details>
  <summary>Details</summary>
Motivation: 当前癌症筛查指南仅覆盖少数癌症类型，且依赖年龄、吸烟史等狭窄标准识别高风险人群。EHR记录的大规模纵向患者健康信息可能通过检测细微的癌症前诊断信号，提供更有效的风险识别工具。

Method: 使用All of Us研究项目中865,000多名参与者的EHR、基因组和调查数据，系统评估EHR预测模型与传统风险因素（包括基因突变和癌症家族史）在八种主要癌症中的临床效用。

Result: 即使使用基线建模方法，EHR模型在识别为高风险的人群中，真实癌症病例的富集度比单独使用传统风险因素高3-6倍。EHR基础模型进一步改善了26种癌症类型的预测性能。

Conclusion: 基于EHR的预测建模具有临床潜力，能够支持更精确和可扩展的早期检测策略，超越传统风险因素在癌症筛查中的局限性。

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [81] [Fast and Expressive Multi-Token Prediction with Probabilistic Circuits](https://arxiv.org/abs/2511.11346)
*Andreas Grivas,Lorenzo Loconte,Emile van Krieken,Piotr Nawrot,Yu Zhao,Euan Wielewski,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: MTPC是一个基于概率电路的多令牌预测框架，通过探索不同电路架构来编码未来令牌的联合分布，在保持原始验证器LLM性能的同时显著加速生成。


<details>
  <summary>Details</summary>
Motivation: 现有的多令牌预测方法通常假设未来令牌独立，牺牲了表达能力。本文研究在概率电路框架下探索表达能力和延迟之间的权衡。

Method: 提出MTPC框架，通过选择不同电路架构来编码未来令牌的联合分布，泛化了经典模型如混合模型、隐马尔可夫模型和张量网络。结合推测解码，对现有字节级LLM进行改造。

Result: 实验表明，与假设独立性的MTP相比，MTPC结合推测解码能显著加速生成，同时保证原始验证器LLM的性能。

Conclusion: MTPC框架在表达能力和延迟之间提供了最优权衡，通过探索不同的参数化方式如PC架构和部分层共享，实现了高效的生成加速。

Abstract: Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.

</details>


### [82] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 开发了一个多保真度机器学习力场框架，用于锂离子电池正极材料，通过同时利用低保真度非磁性和高保真度磁性计算数据集来提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池正极材料的机器学习力场开发相对有限，主要由于正极材料复杂的电子结构特性和高质量计算数据集的稀缺。

Method: 构建多保真度机器学习力场框架，同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。

Result: 在锂锰铁磷酸盐正极材料系统上的测试证明了这种多保真度方法的有效性。

Conclusion: 该工作有助于以较低的训练数据集成本实现正极材料的高精度机器学习力场训练，为机器学习力场在正极材料计算模拟中的应用提供了新视角。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [83] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文分析了在边缘设备内存限制下，使用内存高效零阶优化(MeZO)相比传统反向传播(BP)训练的优势，证明MeZO能在内存受限情况下支持更大模型，但需要更长的训练时间。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下支持不同智能任务的适应，传统BP训练需要存储层激活和优化器状态，限制了可部署模型的最大规模。

Method: 使用内存高效零阶优化(MeZO)，仅通过前向评估估计梯度，无需存储中间激活或优化器状态，从而在片上内存中容纳更大模型。

Result: 理论分析和数值验证表明，在设备内存约束下，MeZO相比BP能支持更大模型规模，且在有足够训练时间的情况下表现出精度优势。

Conclusion: MeZO是边缘设备内存受限环境下有效的微调方法，通过牺牲训练时间换取模型规模扩展能力，为边缘AI系统提供了可行的适应方案。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [84] [When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering](https://arxiv.org/abs/2511.11380)
*Jiangkai Long,Yanran Zhu,Chang Tang,Kun Sun,Yuanyuan Liu,Xuesong Yan*

Main category: cs.LG

TL;DR: SemST是一个语义引导的空间转录组学数据聚类框架，利用大语言模型将基因符号转化为生物语义嵌入，并与图神经网络捕获的空间关系融合，通过细粒度语义调制模块动态注入高阶生物知识。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型将基因视为孤立数值特征，忽略了基因符号中丰富的生物语义信息，阻碍了对关键生物特征的深入理解。

Method: 使用大语言模型将每个组织点中的基因集转化为生物语义嵌入，与图神经网络捕获的空间邻域关系融合，并引入细粒度语义调制模块进行特征校准。

Result: 在公共空间转录组学数据集上的实验表明，SemST实现了最先进的聚类性能，且FSM模块具有即插即用的通用性，能持续提升其他基线方法的性能。

Conclusion: SemST通过融合生物语义和空间结构，为空间转录组学数据分析提供了更深入的生物学理解，其FSM模块展示了良好的通用性和有效性。

Abstract: Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.

</details>


### [85] [Robust inverse material design with physical guarantees using the Voigt-Reuss Net](https://arxiv.org/abs/2511.11388)
*Sanath Keshav,Felix Fritzen*

Main category: cs.LG

TL;DR: 提出了一种具有严格物理保证的谱归一化替代模型，用于机械均质化的正向和逆向问题。该方法基于Voigt-Reuss界限，通过Cholesky类算子学习特征值在[0,1]范围内的对称正半定表示，确保预测结果在Löwner意义下位于界限之间。


<details>
  <summary>Details</summary>
Motivation: 解决机械均质化中正向预测和逆向设计的物理一致性问题，确保模型预测满足严格的物理约束和界限条件。

Method: 利用Voigt-Reuss界限的差异，通过Cholesky类算子学习对称正半定表示；在3D线性弹性中使用全连接Voigt-Reuss网络，在2D平面应变中结合谱归一化、可微分渲染器和CNN。

Result: 在3D线性弹性中，各向同性投影恢复接近完美保真度（R²≥0.998），张量级相对Frobenius误差中值≈1.7%，均值≈3.4%。在2D平面应变中，所有分量R²>0.99，准确跟踪渗透诱导的特征值跳跃，并能稳健推广到分布外图像。

Conclusion: Voigt-Reuss网络统一了准确、物理可接受的正向预测与大规模、约束一致的逆向设计，适用于椭圆算子和耦合物理场景。

Abstract: We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.

</details>


### [86] [SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming](https://arxiv.org/abs/2511.11391)
*Yeyue Cai,Jianhua Mo,Meixia Tao*

Main category: cs.LG

TL;DR: 提出了一种基于端到端深度学习的方案，同时设计彩虹波束并估计用户位置，通过将移相器和真时延系数作为可训练变量来合成任务导向的波束，最大化定位精度。


<details>
  <summary>Details</summary>
Motivation: 相时阵列结合移相器和真时延器已成为宽带感知和定位中生成频率相关彩虹波束的成本效益架构，需要更高效的定位方案。

Method: 使用端到端深度学习网络，将PS和TTD系数作为可训练变量来设计任务导向波束，通过轻量级全连接模块从用户反馈的最大量化接收功率及其对应子载波索引中恢复用户的角距坐标。

Result: 与现有分析和学习方案相比，该方法将开销降低了一个数量级，并持续提供更低的二维定位误差。

Conclusion: 所提出的端到端深度学习方案在彩虹波束设计和用户定位方面表现出色，显著降低了开销并提高了定位精度。

Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.

</details>


### [87] [Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning](https://arxiv.org/abs/2511.11402)
*Amit Jain,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.LG

TL;DR: 提出基于Transformer的强化学习框架，统一多阶段航天器轨迹优化，通过单一策略架构处理发射、上升、级间分离和入轨等不同任务阶段。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法需要为不同任务阶段分别训练策略，限制了适应性并增加了操作复杂性，需要能够跨动态不同阶段的自适应策略。

Method: 基于近端策略优化(PPO)，用Transformer编码器-解码器结构替换传统循环网络，集成门控Transformer-XL(GTrXL)架构，在关键操作期间维持跨秒到分钟时间尺度的连贯记忆。

Result: 在单阶段基准测试中达到接近最优性能，在多阶段航点导航变体中表现良好，在复杂多阶段火箭上升问题中有效学习跨动态不同阶段的连贯控制策略。

Conclusion: Transformer框架不仅能在简单情况下匹配解析解，还能有效学习跨动态不同阶段的连贯控制策略，为可扩展的自主任务规划奠定基础，减少对阶段特定控制器的依赖，同时保持与安全关键验证协议的兼容性。

Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.

</details>


### [88] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 该论文提出使用多标定方法来处理基于不完美预测的图匹配问题，通过构造多标定预测器，使得基于该预测器的最优匹配能够与在原始预测器上应用最佳决策规则的性能相竞争。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，基于贝叶斯最优预测器的完美信息场景不现实，需要处理不完美预测器导致的误差，并寻找能够补偿这些误差的次优决策规则。

Method: 提出多标定方法，要求预测器在受保护上下文集合的每个元素上无偏。给定匹配算法类C和任意边权重预测器γ，构造特定的多标定预测器γ̂。

Result: 基于多标定预测器γ̂的最优匹配能够与在原始预测器γ上应用C类中最佳决策规则的性能相竞争。

Conclusion: 多标定方法为解决基于不完美预测的图匹配问题提供了有效途径，并给出了样本复杂度界限作为理论支撑。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [89] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 本文展示了使用可微分编程解决声学逆问题的实用方法，包括边界导纳估计和共振阻尼的形状优化，通过自动微分和有限差分相结合实现了高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统声学逆问题求解需要手动推导伴随方程，过程复杂且效率低。本文旨在利用现代可微分软件栈实现快速原型开发，简化优化流程。

Method: 使用JAX-FEM进行自动微分实现边界导纳估计，结合PyTorch3D进行网格操作，采用随机有限差分方法进行形状优化，将物理驱动的边界优化与几何驱动的内部网格适应分离。

Result: 边界导纳估计达到3位精度，形状优化在目标频率下实现48.1%的能量减少，相比标准有限差分方法减少了30倍的FEM求解次数。

Conclusion: 现代可微分软件栈能够快速开发基于物理的逆问题优化工作流，自动微分适用于参数估计，有限差分与自动微分结合适用于几何设计。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [90] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输(OT)的流匹配生成模型后训练量化方法，能够在2-3位精度下保持生成质量，优于传统量化方案。


<details>
  <summary>Details</summary>
Motivation: 流匹配生成模型虽然具有高效的无模拟训练和确定性采样优势，但在实际部署中面临高精度参数需求带来的挑战，需要有效的量化方法来压缩模型。

Method: 采用基于最优传输的后训练量化方法，最小化量化权重与原始权重之间的2-Wasserstein距离，并与均匀量化、分段量化和对数量化方案进行系统比较。

Result: 在五个不同复杂度的基准数据集上的实验结果表明，OT量化在2-3位精度下能够保持视觉生成质量和潜在空间稳定性，而其他方法在此精度下失效。

Conclusion: 基于OT的量化是一种有原则且有效的方法，可用于压缩流匹配生成模型，适用于边缘和嵌入式AI应用。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [91] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级合并和知识仲裁实现有界遗忘，在恶意软件检测和二进制摘要任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖完整重训练或数据回放，在数据敏感的安全场景中不可行，且难以平衡新旧知识整合与干扰最小化。

Method: 通过合并预训练和新微调模型作为新旧知识教师，采用低秩稀疏更新限制参数变化到独立子空间，并通过基于模型置信度的知识仲裁动态平衡教师贡献。

Result: 在时间漂移的恶意软件检测中，保持分数从20.2%提升至38.6%；在二进制摘要中，BLEU分数达到先前工作的两倍，并在跨表示泛化中超越所有基线。

Conclusion: RETROFIT有效解决了安全分析中持续学习的挑战，实现了有界遗忘和有效知识转移，无需历史数据即可维持模型性能。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [92] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一个后训练框架，通过联合优化时间步和逐层精度来加速扩散模型的推理，无需重新训练，实现高达6.25倍的模型压缩和2.8倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但由于需要大量去噪步骤和繁重的矩阵运算，推理成本很高。需要一种无需训练的方法来减少延迟和内存使用。

Method: DiffPro结合三个部分：基于流形感知的敏感度指标分配权重位数、动态激活量化以稳定跨时间步的激活、以及基于师生漂移的预算时间步选择器。

Result: 实验显示DiffPro实现了高达6.25倍的模型压缩、减少50%的时间步数、2.8倍的推理加速，同时Delta FID <= 10，在标准基准测试中表现出实际效率提升。

Conclusion: DiffPro将步骤减少和精度规划统一到一个可部署的预算计划中，为实时节能的扩散推理提供了实用解决方案。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [93] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 提出FairReweighing预处理算法，基于密度估计确保回归模型满足分离标准，在保持高准确性的同时改善公平性


<details>
  <summary>Details</summary>
Motivation: AI软件在公共部门和工业应用中缺乏透明度，引发了对种族、性别、年龄等群体公平性的担忧。目前大多数公平性研究集中在二元分类任务上，回归问题的公平性研究相对不足

Method: 采用基于互信息的指标评估分离违规，提出FairReweighing预处理算法，基于密度估计重新加权数据，确保模型满足分离标准

Result: 理论上证明在数据独立性假设下，FairReweighing能保证训练数据的分离；实证显示在合成和真实数据上，该算法在改善分离的同时保持高准确性，优于现有最先进的回归公平性解决方案

Conclusion: FairReweighing算法有效解决了回归问题中的公平性挑战，为AI软件在敏感应用中的公平部署提供了实用解决方案

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [94] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 重新审视多步预测中的递归策略和直接策略的偏差-方差权衡，通过理论分析和实验证明传统认知可能不准确，特别是在非线性模型中递归策略可能同时具有更低偏差和更高方差。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略具有高偏差低方差，直接策略具有低偏差高方差，但这一直觉可能不准确，特别是在非线性预测场景中需要重新审视。

Method: 通过将多步预测误差分解为不可约噪声、结构近似误差和估计方差三个部分，对线性预测器和非线性预测器进行理论分析，并在ETTm1数据集上使用多层感知机进行实验验证。

Result: 对于线性预测器，结构近似误差为零；对于非线性预测器，递归策略通过重复组合可以增加模型表达能力，其估计方差可以表示为一步方差乘以基于雅可比矩阵的放大因子。

Conclusion: 选择递归策略还是直接策略应基于模型非线性程度和噪声特性，而非传统的偏差-方差直觉，为实际应用提供了更准确的指导。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [95] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 提出了一种基于纯机器学习的从运动捕捉数据合成雷达频谱图的方法，使用基于Transformer的模型将MoCap数据转换为多普勒雷达频谱图。


<details>
  <summary>Details</summary>
Motivation: 解决雷达数据稀缺问题，利用更丰富的运动捕捉数据来增强雷达数据集，为高级应用训练提供支持，同时相比基于物理的方法计算量更小。

Method: 将MoCap到频谱图的转换建模为窗口化的序列到序列任务，使用Transformer模型联合捕捉MoCap标记的空间关系和帧间的时间动态。

Result: 实验表明该方法能生成视觉和定量上合理的多普勒雷达频谱图，具有良好的泛化能力，模型具备将多部位运动转换为多普勒特征的能力。

Conclusion: 这是使用Transformer进行时间序列信号处理的有趣示例，特别适用于边缘计算和物联网雷达，能够有效增强稀缺雷达数据集，计算效率高于物理方法。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [96] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估适应性的方法，通过输入变换和均匀质量分箱来更准确地评估预测集的适应性，并基于此提出了新的自适应预测集算法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在分箱不平衡问题，导致对覆盖率和集合大小的估计不准确，需要更可靠的适应性评估指标。

Method: 使用输入变换按难度排序样本，然后进行均匀质量分箱，提出两个新指标来评估适应性，并开发了基于分组条件共形预测的新算法。

Result: 实验表明新指标与期望的适应性属性相关性更强，新算法在图像分类和医疗任务上都优于现有方法。

Conclusion: 提出的分箱方法和评估指标能更准确地评估预测集的适应性，基于此的新算法在多个任务上表现优异。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [97] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 提出了一种数据高效的轻量级U-Net分割管道，仅用10张标注SEM图像训练，在反应堆压力容器钢的碳化物分割中达到0.98的Dice系数，显著优于传统方法(0.85)，并将标注工作量减少一个数量级。


<details>
  <summary>Details</summary>
Motivation: 反应堆压力容器钢中碳化物对机械性能至关重要，但SEM图像中碳化物与基体的灰度值重叠使简单阈值分割无效，需要更有效的分割方法。

Method: 使用轻量级U-Net架构(3070万参数)，仅需10张标注的扫描电镜图像进行训练，实现数据高效的分割。

Result: 模型达到0.98的Dice-Sørensen系数，显著优于传统图像分析方法(0.85)，同时将标注工作量相比最先进的数据高效分割模型减少一个数量级。

Conclusion: 该方法实现了快速自动化的碳化物量化，可推广到其他钢种，展示了数据高效深度学习在反应堆压力容器钢分析中的潜力。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [98] [Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models](https://arxiv.org/abs/2511.11490)
*Joan Font-Quer Roset,Devina Mohan,Anna Scaife*

Main category: cs.LG

TL;DR: 使用基于分数的扩散模型估计Radio Galaxy Zoo数据集的固有维度，发现分布外源具有更高的固有维度值，RGZ的整体固有维度超过自然图像数据集。


<details>
  <summary>Details</summary>
Motivation: 研究RGZ数据集的固有维度如何随贝叶斯神经网络能量分数变化，能量分数衡量射电源与MiraBest子集的相似度。

Method: 使用基于分数的扩散模型估计RGZ数据集的固有维度，分析其与BNN能量分数、Fanaroff-Riley形态类别和信噪比的关系。

Result: 分布外源表现出更高的固有维度值；RGZ整体固有维度高于自然图像数据集；FR I和FR II类之间无关系；信噪比与固有维度呈弱负相关趋势。

Conclusion: RGZ数据集可利用固有维度与能量分数的关系来定量研究和改进各种自监督学习算法的表示学习。

Abstract: In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.

</details>


### [99] [Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation](https://arxiv.org/abs/2511.11500)
*Mohamad Amin Mohamadi,Tianhao Wang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文提出强化犹豫（RH）方法，通过三元奖励机制训练语言模型在不确定时主动弃权，并引入级联推理策略来优化模型协调。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型缺乏可信赖智能的基本要求：知道何时不回答。尽管在基准测试中表现优异，但这些模型会产生自信的幻觉，即使错误答案可能带来灾难性后果。

Method: 提出强化犹豫（RH）：在可验证奖励的强化学习（RLVR）中使用三元奖励（+1正确，0弃权，-λ错误）替代二元奖励。通过控制λ参数在逻辑谜题上进行实验，并引入级联和自级联两种推理策略。

Result: 在GSM8K、MedQA和GPQA上的评估显示，前沿模型几乎从不弃权。RH方法能够产生帕累托前沿，不同惩罚参数训练出不同风险偏好的模型。级联策略在降低计算成本的同时优于多数投票。

Conclusion: 弃权应作为首要训练目标，将"我不知道"从失败转变为协调信号，使模型通过对其局限性的校准诚实来赢得信任。

Abstract: Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$λ$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $λ$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.

</details>


### [100] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective通过修改模型架构跳过连接，使MoE模型的计算与通信重叠，在16B到109B参数的大模型中保持准确率，同时显著加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: 阻塞通信是分布式环境中高效运行MoE模型的主要障碍，需要解决计算与通信的同步问题。

Method: 修改现代模型架构，跳过模型中的连接，通过自蒸馏等技术转换大型模型，并实现通信与计算的显式重叠。

Result: 成功转换了从16B到109B参数的一系列最先进模型，在保持与原始开源版本相当准确率的同时实现了通信重叠。例如Llama 4 Scout（109B）在广泛下游评估中平均准确率仅比原始版本低1%。

Conclusion: FarSkip-Collective方法能够有效解决MoE模型的通信阻塞问题，在保持模型能力的同时显著提升训练和推理效率。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [101] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文研究了多组公平聚类问题，提出了近似算法解决NP难问题，并改进了公平相关聚类和公平共识聚类的近似保证。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法在处理具有多个保护属性（如年龄、种族、性别等）的数据时，经常无法公平代表边缘化群体，这主要由训练数据中的偏见引起。需要增强聚类结果的公平性，最好通过最小修改实现。

Method: 将最接近公平聚类问题推广到任意数量组的情况，证明该问题在组大小相等时也是NP难的，并提出近线性时间近似算法。利用这些算法进一步改进了公平相关聚类和公平共识聚类的近似保证。

Result: 提出了处理任意大小多组的最接近公平聚类的近似算法，改进了公平相关聚类的最先进结果，并为多组公平共识聚类问题提供了首个近似算法。

Conclusion: 成功解决了Chakraborty等人提出的两个开放性问题，为多组公平聚类提供了有效的算法解决方案，推动了公平机器学习领域的发展。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [102] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 本文展示了自注意力动力学与多智能体Oja流的关系，将单头自注意力系统的均衡点分为四类：共识、二分共识、聚类和多边形均衡，并分析了这些均衡与价值矩阵特征向量的关系。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力动力学与多智能体Oja流之间的关系，深入理解transformer中注意力机制的均衡特性和稳定性。

Method: 将自注意力动力学建模为连续时间多智能体系统，分析其与Oja流的关系，并对均衡点进行分类和稳定性分析。

Result: 发现自注意力动力学中存在四种均衡类型，前三种类型通常共存且具有渐近稳定性，这些均衡通常与价值矩阵的特征向量对齐，特别是主特征向量。

Conclusion: 自注意力动力学与多智能体Oja流密切相关，其均衡结构复杂但具有明确的分类，这为理解transformer的注意力机制提供了理论基础。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [103] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA通过稀疏注意力机制降低长上下文处理的计算成本，但缺乏理论理解和高效GPU实现。本文开发统计模型分析MoBA机制，发现路由器准确区分相关块是关键，提出信号噪声比连接架构参数与检索精度。建议使用更小块大小和键上的短卷积来提高路由精度，并开发FlashMoBA实现高效GPU执行。


<details>
  <summary>Details</summary>
Motivation: MoBA作为高效处理长上下文的LLM构建块，其设计原则未被充分理解，且缺乏高效GPU实现，阻碍了实际应用。需要深入理解其工作机制并解决实现效率问题。

Method: 开发统计模型分析MoBA机制，推导信号噪声比连接架构参数与路由精度；提出使用更小块大小和键上的短卷积来提高性能；设计硬件感知的FlashMoBA CUDA内核实现高效执行。

Result: 改进的MoBA模型在从头训练的LLM中达到密集注意力基线的性能水平；FlashMoBA相比FlashAttention-2在小块情况下实现高达14.7倍加速。

Conclusion: 通过理论分析和硬件优化相结合，成功提升了MoBA的性能和效率，使其成为实用的长上下文处理解决方案。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [104] [MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising](https://arxiv.org/abs/2511.11305)
*Chenghan Fu,Daoze Zhang,Yukang Lin,Zhanheng Nie,Xiang Zhang,Jianyu Liu,Yueran Liu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: MOON是一个用于电商多模态表示学习的可持续迭代实践框架，已在淘宝搜索广告系统全链路部署，在CTR预测任务上实现了20%的在线提升，经历了三年五次迭代。


<details>
  <summary>Details</summary>
Motivation: 解决多模态表示学习目标与下游任务训练目标之间的不对齐问题，通过定义"交换率"量化中间指标改进对下游收益的转化效果。

Method: 采用"预训练、后训练、应用"三阶段训练范式，在数据处理、训练策略、模型架构和下游应用四个关键维度进行迭代优化，并系统研究多模态表示学习的缩放规律。

Result: 在淘宝搜索广告系统全链路部署，CTR预测任务实现20%的在线提升，这是三年来CTR预测任务的最大改进。

Conclusion: MOON框架成功解决了多模态表示学习与下游任务的集成问题，积累了宝贵的实践经验，为研究社区提供了有价值的参考。

Abstract: We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [105] [StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks](https://arxiv.org/abs/2511.11320)
*Jiaqi Lin,Yi Jiang,Abhronil Sengupta*

Main category: cs.ET

TL;DR: 提出了一种基于随机脉冲神经元的平衡传播框架，解决了传统EP方法在脉冲神经网络中存在的训练不稳定和可扩展性问题，在保持生物合理性的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有节能、稀疏和生物启发的优势，但使用BPTT训练缺乏生物合理性。现有的平衡传播方法主要基于确定性神经元，在处理脉冲动力学不连续性时复杂且难以扩展到复杂视觉任务。

Method: 将概率性脉冲神经元整合到EP范式中，通过随机性平滑优化景观、稳定训练，并在深度卷积脉冲收敛循环神经网络中实现可扩展学习。理论证明该随机EP动力学在平均场理论下近似确定性EP。

Result: 在视觉基准测试中，缩小了与BPTT训练的SNN和EP训练的非脉冲CRNN之间的性能差距，同时保持了局部性。

Conclusion: 随机EP是神经形态和片上学习的一个有前景的方向，为生物合理的脉冲神经网络训练提供了可行的解决方案。

Abstract: Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and biologically grounded alternative. However, existing EP frameworks, primarily based on deterministic neurons, either require complex mechanisms to handle discontinuities in spiking dynamics or fail to scale beyond simple visual tasks. Inspired by the stochastic nature of biological spiking mechanism and recent hardware trends, we propose a stochastic EP framework that integrates probabilistic spiking neurons into the EP paradigm. This formulation smoothens the optimization landscape, stabilizes training, and enables scalable learning in deep convolutional spiking convergent recurrent neural networks (CRNNs). We provide theoretical guarantees showing that the proposed stochastic EP dynamics approximate deterministic EP under mean-field theory, thereby inheriting its underlying theoretical guarantees. The proposed framework narrows the gap to both BPTT-trained SNNs and EP-trained non-spiking CRNNs in vision benchmarks while preserving locality, highlighting stochastic EP as a promising direction for neuromorphic and on-chip learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [106] [HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control](https://arxiv.org/abs/2511.11549)
*Shreya Meel,Sennur Ulukus*

Main category: cs.CR

TL;DR: 本文提出了HetDAPAC框架，通过将非敏感属性验证集中化处理，在保持敏感属性隐私的同时提高了系统速率。


<details>
  <summary>Details</summary>
Motivation: DAPAC系统对所有属性采用相同的隐私保护级别，但并非所有属性都是敏感的，这种统一处理会降低系统速率。需要根据属性的隐私敏感性差异来优化系统设计。

Method: 提出HetDAPAC框架，将N个属性中的N-D个非敏感属性验证集中到一个中央服务器处理，仅对D个敏感属性保留DAPAC的分布式验证架构。

Result: 第一个方案将速率从1/2K提高到1/(K+1)，但存在服务器下载不均衡问题；第二个方案实现了服务器间下载均衡，速率为(D+1)/(2KD)。

Conclusion: HetDAPAC通过区分敏感和非敏感属性，在保持敏感属性隐私的同时显著提高了系统性能，为属性验证系统提供了更灵活的隐私-性能权衡方案。

Abstract: Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\frac{1}{2K}$ to $\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\frac{D+1}{2KD}$.

</details>


### [107] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: 这篇论文系统分析了Wi-Fi CSI生物识别的安全属性，揭示了现有研究在评估方法和安全考虑方面的系统性不一致问题，并提出了统一评估框架来暴露隐藏的风险。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI作为生物识别模式虽然被多次提出，但缺乏对其安全属性、对抗性恢复力和方法一致性的统一理解，需要从安全角度进行系统化分析。

Method: 构建了统一评估框架，分析现有工作在感知基础设施、信号表示、特征管道、学习模型和评估方法等方面的差异，使用安全相关指标如每类EER、FCS和基尼系数来揭示风险集中问题。

Result: 发现了系统性不一致：依赖聚合准确度指标、有限的FAR/FRR/EER报告、缺乏每用户风险分析、很少考虑威胁模型或对抗可行性。实证分析揭示了传统报告方法隐藏的风险集中问题。

Conclusion: 阐明了当前CSI生物识别的安全边界，为严格评估、可重复实验和未来研究方向提供了指导方针，为安全社区提供了对Wi-Fi CSI生物识别作为认证原语适用性的结构化、证据驱动的重新评估。

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [108] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: AFLGopher是一种可行性感知的定向模糊测试工具，通过改进距离计算机制，显著提高了到达目标代码和触发已知漏洞的效率。


<details>
  <summary>Details</summary>
Motivation: 现有定向模糊测试工具的引导机制在距离计算时未考虑可行性，导致效率低下。

Method: 提出可行性感知的距离计算方法，包括基于有限轨迹预测所有分支可行性的分类方法，以及运行时可行性更新机制。

Result: AFLGopher在到达目标代码方面比现有工具快2.52-3.76倍，在触发已知漏洞方面快4.52-5.60倍。

Conclusion: 可行性感知的引导机制能显著提升定向模糊测试的效率。

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [109] [PATCHEVAL: A New Benchmark for Evaluating LLMs on Patching Real-World Vulnerabilities](https://arxiv.org/abs/2511.11019)
*Zichao Wei,Jun Zeng,Ming Wen,Zeliang Yu,Kai Cheng,Yiding Zhu,Jingyi Guo,Shiqi Zhou,Le Yin,Xiaodong Su,Zhechao Ma*

Main category: cs.CR

TL;DR: PATCHEVAL是一个多语言漏洞修复基准测试，涵盖Go、JavaScript和Python，包含1000个2015-2025年的CVE漏洞，其中230个配备运行时沙箱环境用于补丁验证。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞修复基准存在漏洞过时、语言覆盖有限、补丁验证不可靠和可复现性不足等问题，需要更全面的评估框架来测试LLM在自动漏洞修复中的能力。

Method: 构建包含1000个CVE漏洞的多语言数据集，覆盖65种CWE类型，其中230个漏洞配备运行时沙箱环境进行安全测试和功能测试验证。

Result: 系统评估了一系列最先进的LLM和代理，提供了深入的实证分析，为未来自动漏洞修复研究提供了关键见解。

Conclusion: PATCHEVAL基准克服了现有基准的局限性，为评估LLM在多语言漏洞修复中的能力提供了可靠框架，推动了自动漏洞修复技术的发展。

Abstract: Software vulnerabilities are increasing at an alarming rate. However, manual patching is both time-consuming and resource-intensive, while existing automated vulnerability repair (AVR) techniques remain limited in effectiveness. Recent advances in large language models (LLMs) have opened a new paradigm for AVR, demonstrating remarkable progress. To examine the capability of LLMs in AVR, several vulnerability benchmarks have been proposed recently. However, they still suffer from key limitations of outdated vulnerabilities, limited language coverage, unreliable patch validation, and insufficient reproducibility. To overcome these challenges, we introduce PATCHEVAL, a multilingual benchmark for Go, JavaScript, and Python, languages for which existing benchmarks remain unexplored. PATCHEVAL curates a dataset of 1,000 vulnerabilities drawn from CVEs reported between 2015 and 2025, covering 65 distinct CWEs. A subset of 230 CVEs is further equipped with runtime sandbox environments, enabling patch verification through both security tests and functionality tests. To provide a systematic comparison of LLM-based vulnerability repair, we evaluate a series of state-of-the-art LLMs and agents, presenting an in-depth analysis that empirically yields key insights to guide future research in AVR.

</details>


### [110] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer是一种针对长上下文LLM的提示注入防御方法，通过定位和净化潜在注入令牌来防止攻击，利用LLM的注意力机制识别并移除驱动指令跟随行为的关键令牌。


<details>
  <summary>Details</summary>
Motivation: 现有的提示注入防御方法主要针对短上下文设计，在长上下文场景下效果有限，因为注入指令只占长上下文的很小部分，防御难度大。

Method: PISanitizer首先让LLM故意遵循上下文中的任意指令，然后净化那些接收高注意力并驱动指令跟随行为的令牌，从而消除注入指令的影响。

Result: 评估显示PISanitizer能成功防止提示注入、保持实用性、优于现有防御方法、效率高，并对基于优化的强自适应攻击具有鲁棒性。

Conclusion: PISanitizer为攻击者制造了一个困境：注入指令越能有效迫使LLM遵循它，就越可能被PISanitizer净化，从而提供了一种有效的长上下文提示注入防御方案。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [111] [Adaptive Intrusion Detection for Evolving RPL IoT Attacks Using Incremental Learning](https://arxiv.org/abs/2511.11464)
*Sumeyye Bas,Kiymet Kaya,Elif Ak,Sule Gunduz Oguducu*

Main category: cs.CR

TL;DR: 本文研究在RPL物联网网络中应用增量学习进行入侵检测，评估了五种模型家族，证明增量学习能有效应对新型攻击并减轻灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: RPL协议在资源受限的IoT系统中易受路由层攻击，传统防御方法无法有效应对新型或零日攻击，需要重新训练，这在动态IoT环境中不实用。

Method: 系统评估了五种模型家族（包括集成模型和深度学习模型），采用增量学习策略，结合攻击特定分析、遗忘行为和时间效率评估。

Result: 增量学习不仅能够恢复对新攻击类的检测性能，还能减轻对已学威胁的灾难性遗忘，同时相比完全重新训练减少了训练时间。

Conclusion: 增量学习为RPL物联网网络提供了可扩展的路径，能够在不断演化的环境中维持弹性的入侵检测能力。

Abstract: The routing protocol for low-power and lossy networks (RPL) has become the de facto routing standard for resource-constrained IoT systems, but its lightweight design exposes critical vulnerabilities to a wide range of routing-layer attacks such as hello flood, decreased rank, and version number manipulation. Traditional countermeasures, including protocol-level modifications and machine learning classifiers, can achieve high accuracy against known threats, yet they fail when confronted with novel or zero-day attacks unless fully retrained, an approach that is impractical for dynamic IoT environments. In this paper, we investigate incremental learning as a practical and adaptive strategy for intrusion detection in RPL-based networks. We systematically evaluate five model families, including ensemble models and deep learning models. Our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats, all while reducing training time compared to full retraining. By combining five diverse models with attack-specific analysis, forgetting behavior, and time efficiency, this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving RPL-based IoT networks.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [112] [Faster Algorithms for Structured Matrix Multiplication via Flip Graph Search](https://arxiv.org/abs/2511.10786)
*Kirill Khoruzhii,Patrick Gelß,Sebastian Pokutta*

Main category: cs.SC

TL;DR: 提出了针对2×2到5×5结构化矩阵的显式低秩双线性非交换乘法方案，通过翻转图搜索发现，改进了递归算法的渐近复杂度乘性因子。


<details>
  <summary>Details</summary>
Motivation: 改进结构化矩阵乘法的渐近复杂度，特别是针对特定矩阵结构（如上三角、对称、斜对称等）的乘法运算，为递归算法提供更好的构建块。

Method: 使用翻转图搜索在有限域F2和F3上发现张量分解方案，然后提升到整数环Z或有理数域Q上。针对不同矩阵结构（一般、上三角、下三角、对称、斜对称）及其转置乘积设计方案。

Result: 获得了4×4矩阵的秩34方案：一般矩阵乘其转置使用10次递归调用，将因子从0.634改进到0.615；上三角矩阵乘一般矩阵使用12次递归调用，将因子从0.615改进到0.595。还发现了需要2的逆的Q域方案，包括秩5的2×2对称-对称乘法和秩14的3×3斜对称-一般乘法（优于AlphaTensor的15）。

Conclusion: 通过有限域上的翻转图搜索发现了改进结构化矩阵乘法渐近复杂度的显式低秩方案，为递归算法提供了更好的构建块，并在多个具体案例中实现了复杂度因子的改进。

Abstract: We give explicit low-rank bilinear non-commutative schemes for multiplying structured $n \times n$ matrices with $2 \leq n \leq 5$, which serve as building blocks for recursive algorithms with improved multiplicative factors in asymptotic complexity. Our schemes are discovered over $\mathbb{F}_2$ or $\mathbb{F}_3$ and lifted to $\mathbb{Z}$ or $\mathbb{Q}$. Using a flip graph search over tensor decompositions, we derive schemes for general, upper-triangular, lower-triangular, symmetric, and skew-symmetric inputs, as well as products of a structured matrix with its transpose. In particular, we obtain $4 \times 4$ rank-34 schemes: (i) multiplying a general matrix by its transpose using 10 recursive calls, improving the factor from 26/41 (0.634) to 8/13 (0.615); and (ii) multiplying an upper-triangular matrix by a general matrix using 12 recursive calls, improving the factor from 8/13 (0.615) to 22/37 (0.595). Additionally, using $\mathbb{F}_3$ flip graphs, we discover schemes over $\mathbb{Q}$ that fundamentally require the inverse of 2, including a $2 \times 2$ symmetric-symmetric multiplication of rank 5 and a $3 \times 3$ skew-symmetric-general multiplication of rank 14 (improving upon AlphaTensor's 15).

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [113] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: 使用摘要概念改进静态数据竞争检测，通过结合锁集、线程ID和线程连接摘要，将正确解决的任务数量提高了五倍以上


<details>
  <summary>Details</summary>
Motivation: 静态分析可以证明数据竞争的缺失，但需要更精确的方法来捕获冲突访问不会并行发生的条件

Method: 重新利用摘要概念（计算历史的摘要），在线程模块化局部跟踪语义中定义数据竞争，并将冲突排除条件表达为摘要，在Goblint静态分析器中实现摘要驱动的数据竞争检测

Result: 在SV-COMP基准测试套件上评估，结合锁集摘要与线程ID和线程连接摘要，相比单独使用锁集推理，正确解决的任务数量增加了五倍以上

Conclusion: 摘要驱动的数据竞争检测方法显著提高了静态分析在证明数据竞争缺失方面的能力

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过注意力机制自适应结合双曲空间、复数空间和欧几里得空间，解决了现有方法在处理不同关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理多样化关系类型时存在局限：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习注意力机制为每种关系类型动态选择最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在从1K论文到10M论文的知识图谱上评估，相比TransE、RotatE、DistMult等基线方法均有持续改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，推理效率为85ms/三元组。

Conclusion: HyperComplEx通过自适应维度分配实现近线性扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [115] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层搜索架构，在真实湿实验验证中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离子液体发现面临数据有限、模型精度差和工作流程碎片化等关键挑战，需要开发更有效的预测和设计方法。

Method: 构建LLM增强的多模态离子液体领域基础模型，采用分层搜索架构进行分子筛选和设计，使用新整理的全面离子液体数据集进行训练和评估。

Result: 模型表现出优越性能，能够有效进行离子液体修饰，在真实湿实验验证中成功处理具有挑战性的分布外任务。

Conclusion: AIonopedia能够加速真实世界的离子液体发现，展示了在实际应用中的有效性和泛化能力。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [116] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个能够根据积累的经验在推理时动态生成定制化策略的AI系统，通过元策略机制灵活调整所有策略组件，在多个挑战性基准测试中显著提升性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在推理时只能通过修改文本输入来调整，无法灵活改变采样参数、工具配置或系统范式，而能够灵活适应的系统又需要离线优化且部署后无法改变。

Method: 使用基于LLM的元策略生成完整计算策略，包含两个组件：Guide基于当前问题和结构化记忆生成候选策略，Consolidator整合执行反馈改进未来策略生成。

Result: 在五个挑战性基准测试中，EGuR相比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累性能持续提升。

Conclusion: EGuR通过动态策略生成实现了AI系统在推理时的灵活适应，显著提升了问题解决能力和效率，证明了经验引导推理的有效性。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [117] [CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding](https://arxiv.org/abs/2511.10935)
*Yifan Zhuang,Calvin Huang,Zepeng Yu,Yongjie Zou,Jiawei Ju*

Main category: cs.SD

TL;DR: 提出了一种融合EEG和EMG信号的新型跨被试多模态BCI解码框架，用于在可听和无声语音条件下分类四个普通话声调，使用轻量级模块实现了优越的分类性能。


<details>
  <summary>Details</summary>
Motivation: 普通话声调分类面临特殊挑战，因为声调变化即使音素相同也传达不同含义。EEG和EMG信号的整合有望提高解码性能，特别是在辅助言语障碍患者的BCI应用中。

Method: 提出跨被试多模态BCI解码框架，融合EEG和EMG信号，结合时空特征提取分支与交叉注意力融合机制，并采用域对抗训练提高跨被试泛化能力。仅使用20个EEG通道和5个EMG通道。

Result: 在所有条件下均优于最先进基线，平均分类准确率：可听语音87.83%，无声语音88.08%。跨被试评估中仍保持强劲性能：可听语音83.27%，无声语音85.10%。

Conclusion: 使用最少EEG-EMG通道的声调级解码是可行的，且可能跨被试泛化，有助于开发实用的BCI应用。

Abstract: Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [118] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 提出了一个从人类反馈中持续学习的文本到SQL框架，通过存储结构化记忆来改进查询执行准确性，在BIRD基准测试中显著提升了准确率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在从自然语言生成SQL查询时，难以处理数据库特定模式和隐含领域知识，需要持续学习机制来改进

Method: 设计了多种学习代理架构变体，通过接收自然语言反馈来精炼查询，并将揭示的知识提炼存储到结构化记忆中供未来任务重用

Result: 在BIRD基准测试开发集上，记忆增强代理特别是过程代理，通过利用人在环反馈实现了显著的准确率提升和错误减少

Conclusion: 将隐含的人类专业知识转化为可重用知识对于构建更具适应性、领域感知的文本到SQL系统至关重要，这些系统能够从人在环中持续学习

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [119] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath是一个大规模多领域生物医学实体链接数据集，整合了9个现有专家标注数据集，通过UMLS标准化、62个生物医学词汇表映射和完整本体路径丰富，解决了数据碎片化、可解释模型资源缺乏和语义盲评估指标限制等问题。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学命名实体识别和实体链接面临数据碎片化、缺乏可解释模型资源和语义盲评估指标限制等挑战，阻碍了该领域的进展。

Method: 基于9个现有专家标注的实体链接数据集构建MedPath，所有实体使用UMLS最新版本进行标准化，映射到62个其他生物医学词汇表，并在最多11个生物医学词汇表中丰富完整的本体路径。

Result: 创建了MedPath数据集，支持语义丰富和可解释的实体链接系统的训练和评估，以及下一代可互操作和可解释的临床NLP模型的开发。

Conclusion: MedPath直接开启了生物医学NLP的新研究前沿，促进了语义丰富和可解释实体链接系统的训练和评估，以及可互操作和可解释临床NLP模型的开发。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [120] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 工具增强语言模型在使用代码解释器工具时会产生工具诱导近视(TIM)，虽然能提高答案准确率，但会损害推理过程的连贯性和质量。


<details>
  <summary>Details</summary>
Motivation: 研究工具增强语言模型在获得工具能力提升的同时，是否保持了可信的推理过程，揭示工具使用对推理质量的影响。

Method: 使用PYMATH基准测试(1679个竞赛级数学问题)，开发多维度评估套件，分析工具使用频率与推理质量的关系，并提出基于偏好优化的框架来改进工具使用方式。

Result: 工具增强语言模型在最终答案准确率上提升19.3个百分点，但推理质量下降41.5%，工具使用越频繁推理越不连贯，约55%高风险案例存在TIM问题。

Conclusion: 工具使用会损害语言模型的推理质量，但通过偏好优化框架可以改善工具使用方式，同时提升答案准确率和推理深度。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [121] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 提出一种基于自动机遍历历史的新方法，用于增强基于自动机的结构化生成中的输出多样性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于生成结构化输出，但现有结构化生成方法虽然确保有效性，却缺乏输出多样性

Method: 利用自动机遍历历史来引导LLM朝向新颖的结构模式

Result: 评估显示该方法显著提高了结构和内容多样性，同时保持了相当的生成效率

Conclusion: 该方法在生成开源库测试用例的案例研究中展示了有效性，能够生成更多样化的测试用例

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [122] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 提出了一种结合离线数据增强和并行检索的架构，用于历史人物对话系统，解决了传统方法在深度和延迟之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现历史人物对话系统时面临关键权衡：简单检索增强生成产生浅层响应，而多阶段反思方法虽然深度更好但延迟过高。

Method: 通过离线数据增强将传记数据转化为1,774个丰富的第一人称记忆，带有情感语义元数据，采用两阶段并行检索实现0.52秒的提示生成。

Result: 评估显示该方法在GPT-4上与传统RAG相当，在较小模型（GPT-3.5、GPT-3）上显著优于传统RAG，特别适合资源受限部署。

Conclusion: 该架构不仅可作为对话接口，还能作为传记分析的研究工具，具有教育、博物馆和研究应用的实用价值，可推广到任何有丰富文本记录的历史人物。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [123] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 提出一种基于专利文档内部多视图的对比学习框架，通过利用专利不同章节（摘要、权利要求、背景等）作为互补视图来学习专利嵌入，解决了SimCSE风格dropout增强导致的语义凝聚力损失问题。


<details>
  <summary>Details</summary>
Motivation: 发现SimCSE风格的dropout增强在专利领域存在特定失效模式：产生过于均匀的嵌入，丢失语义凝聚力。需要利用专利文档固有的篇章结构来引入自然的语义多样性。

Method: 提出基于章节的增强方法，将专利的不同部分（摘要、权利要求、背景等）作为对比学习的互补视图，利用专利固有的语义和结构多样性来缓解过度分散问题。

Result: 在大规模基准测试中，这种完全自监督的方法在先有技术检索和分类任务上匹配或超越了基于引用和IPC监督的基线方法，同时避免了对脆弱或不完整标注的依赖。

Conclusion: 不同专利章节在不同任务中具有专门化作用：权利要求和摘要有利于检索，背景部分有助于分类，这凸显了利用专利固有篇章结构进行表示学习的价值。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [124] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出了一种贝叶斯方法来量化大语言模型(LLM)二进制评估指标中的统计不确定性，重点关注由概率性文本生成策略引起的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法往往忽视统计不确定性量化，而LLM的随机性文本生成特性使得评估结果具有内在不确定性。

Method: 采用贝叶斯方法，通过两个案例研究来应用该方法：1)评估对抗性输入上的拒绝率，2)评估LLM在开放式对话中的成对偏好。

Result: 贝叶斯方法能够为基于LLM系统的行为提供有用的不确定性量化，特别是在评估有害输出敏感性和模型偏好时。

Conclusion: 贝叶斯不确定性量化方法对于理解LLM评估结果的可靠性至关重要，特别是在处理概率性文本生成系统时。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [125] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 提出了一种自监督框架来提升防护模型的语义鲁棒性，通过使用释义集合和偏斜感知聚合策略来增强预测一致性，显著减少了语义变异性并改善了模型校准。


<details>
  <summary>Details</summary>
Motivation: 防护模型对表面语言变化的敏感性是其关键漏洞，即使语义保持不变的释义也会导致安全评分大幅波动，表明缺乏语义基础。

Method: 引入自监督框架，利用释义集合通过新颖的偏斜感知聚合策略来强制预测一致性，用于鲁棒目标计算。

Result: 在六个开源防护模型上测试，方法将跨释义的语义变异性降低了约58%，基准准确率平均提高了约2.5%，并能泛化到未见过的风格变化。鲁棒性训练将校准度提高了40%。

Conclusion: 应将语义一致性作为首要训练目标，该方法为构建更可靠的防护模型提供了可扩展的方案，揭示了模型校准与一致性之间的双向关系。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [126] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 提出了STaDS框架来评估LLMs的理解能力，发现当前模型在跨领域决策中表现不一致，存在准确性与决策依据不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs虽然预测准确率高，但正确性不等于真正理解。需要评估模型是否能像人类专家一样在不同领域做出基于正确决策因素的一致决策。

Method: 引入STaDS框架，通过结构化表格决策模拟评估LLMs在15个不同决策场景中的表现，从问题理解、知识预测和决策因素依赖三个方面综合评估理解能力。

Result: 对9个前沿LLMs的测试显示：(a)大多数模型在不同领域难以保持一致的强准确性；(b)模型可能准确但全局不忠实，预测依据与陈述理由经常不匹配。

Conclusion: 需要超越准确性的全局理解评估协议，开发新框架来提升LLMs的理解能力。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [127] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 本研究比较了传统机器学习和深度迁移学习在预测儿童人工耳蜗植入后语言发展方面的表现，发现基于双线性注意力融合策略的深度迁移学习模型在准确率、敏感性和特异性方面均优于传统机器学习。


<details>
  <summary>Details</summary>
Motivation: 人工耳蜗植入能显著改善重度感音神经性听力损失儿童的语言能力，但治疗效果存在较大个体差异，且无法通过植入年龄或残余听力可靠预测。

Method: 使用基于脑神经解剖特征的预测模型，比较传统机器学习和深度迁移学习算法，采用二分类模型（高vs低语言改善者），纳入278名来自三个中心的植入儿童。

Result: 深度迁移学习预测模型达到：准确率92.39%、敏感性91.22%、特异性93.56%、AUC 0.977，在所有结果指标上均优于传统机器学习模型。

Conclusion: 深度迁移学习通过直接捕获判别性和任务特定信息，显著提高了预测性能，支持开发适用于全球人工耳蜗项目的单一预测模型的可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [128] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 通过参数空间对齐解决任务算术中的负干扰问题，成功将推理技能转移到非推理模型中


<details>
  <summary>Details</summary>
Motivation: 任务算术在LLM间转移技能时经常因训练过程中的模型分歧而产生负干扰，需要解决这一问题

Method: 利用Transformer架构的置换、旋转和缩放对称性对齐模型参数空间，适配GQA和SwiGLU层，采用权重和激活两种对齐方法

Result: 在挑战性推理基准测试中，该方法始终优于标准任务算术，成功实现了高级推理技能向非推理模型的转移

Conclusion: 该方法为在演化LLM家族间合并和转移专门技能提供了有效途径，减少了冗余微调并增强了模型适应性

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [129] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360是一个开源的Python工具包，用于解释大型语言模型(LLM)的输出，重点关注用户提供的上下文或提示。它包含三种基于扰动和梯度的黑盒与白盒解释方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在更高风险应用中的普及，开发解释LLM输出的工具变得至关重要，无论是摘要、列表还是问题回答。

Method: ICX360实现了三种最新的解释工具，使用扰动(黑盒方法)和梯度(白盒方法)来解释LLM。工具包包含快速入门指南和详细教程，涵盖检索增强生成、自然语言生成和越狱等用例。

Result: 开发了ICX360工具包，可在GitHub上获取，提供了LLM解释的实用解决方案。

Conclusion: ICX360为解释LLM输出提供了全面的工具集，特别关注用户提供的上下文，有助于提高LLM在关键应用中的透明度和可信度。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [130] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 开发了CardioEmbed，一个基于Qwen3-Embedding-8B的心脏病学专业嵌入模型，在7本心脏病学教科书上训练，在心脏病特异性语义检索任务中达到99.60%的检索准确率，比现有最佳医学嵌入模型MedTE提升15.94个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学文本嵌入模型主要基于PubMed研究文献开发，但临床心脏病学实践更依赖教科书中的程序性知识和专业术语，这种研究实践差距限制了现有模型在临床心脏病学应用中的有效性。

Method: 基于Qwen3-Embedding-8B，使用对比学习在7本心脏病学教科书（约15万句去重后）上训练，采用InfoNCE损失和批次内负样本策略。

Result: 在心脏病特异性语义检索任务中达到99.60%检索准确率，比MedTE提升15.94个百分点；在MTEB医学基准测试中，BIOSSES得分为0.77 Spearman，SciFact得分为0.61 NDCG@10。

Conclusion: 在综合性临床教科书上进行领域专业化训练可以实现近乎完美的心脏病学检索性能，显著优于现有医学嵌入模型。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [131] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 提出了一种称为"重归一化"的即插即用、无需训练且轻量级的解决方案，通过移除文本嵌入模型中存在的系统性偏差来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 发现当前文本嵌入模型的输出存在一致的偏差，即每个嵌入向量都可以分解为真实嵌入加上一个几乎在所有句子中都相同的偏差项。

Method: 提出两种重归一化变体：直接从嵌入向量中减去偏差项，或减去嵌入向量在偏差项上的投影。理论分析表明后者效果更好。

Result: 在MMTEB基准测试中，重归一化显著提升了38个模型的性能：检索任务提升9.7σ，分类任务提升3.1σ，其他任务提升0.8σ。

Conclusion: 重归一化是一种有效且通用的方法，能够一致且显著地提升现有文本嵌入模型的性能，且理论预测与实验结果一致。

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [132] [PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases](https://arxiv.org/abs/2511.11141)
*Udo Schlegel,Franziska Weeber,Jian Lan,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出PRSM指标评估CLIP模型对改写查询的敏感性，发现在社会反事实数据集上，CLIP的改写鲁棒性因改写策略而异，且在男性和女性相关查询中存在细微但一致的差异。


<details>
  <summary>Details</summary>
Motivation: CLIP模型虽然在零样本和少样本任务上表现强劲，但其对语言变体（特别是改写）的鲁棒性尚未充分探索。改写鲁棒性对于可靠部署至关重要，尤其是在社会敏感环境中，不一致的表征可能放大人口统计偏见。

Method: 引入改写排名稳定性指标（PRSM），使用社会反事实数据集量化CLIP对改写查询的敏感性，评估不同改写策略下的稳定性，并分析改写鲁棒性与性别的交互作用。

Result: CLIP的鲁棒性随改写策略不同而变化，在男性和女性相关查询中观察到细微但一致的差异。

Conclusion: 研究揭示了CLIP模型在改写鲁棒性方面存在变化，这对多模态系统的公平性和公平部署具有重要意义。

Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [133] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: MMA-Sim是首个比特级精度的参考模型，揭示了10种GPU架构中矩阵乘法加速器的详细算术行为，通过目标测试和随机测试推导出9种算法来模拟浮点矩阵乘法，验证了与真实硬件的比特等效性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU中的矩阵乘法加速器由于不同的未文档化浮点矩阵乘法规范，可能导致数值不精确和不一致，影响深度神经网络训练的稳定性和可复现性。

Method: 结合目标测试和随机测试的方法，推导出9种算术算法来模拟MMAs的浮点矩阵乘法行为，构建比特级精度的参考模型MMA-Sim。

Result: 大规模验证确认MMA-Sim与真实硬件具有比特等效性，识别出影响DNN训练稳定性的算术行为以及可能导致显著错误的未文档化行为。

Conclusion: MMA-Sim成功揭示了MMAs的详细算术行为，为理解和解决DNN训练中的数值稳定性问题提供了重要工具。

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [134] [Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time](https://arxiv.org/abs/2511.10777)
*Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 本文提出两种在单比特压缩感知中实现亚线性运行时间的支持恢复方案：一种是通用精确支持恢复，另一种是通用近似支持恢复，以及概率精确支持恢复方案。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数单比特压缩感知支持恢复方法运行时间为Ω(n)，本文旨在设计运行时间为o(n)的亚线性算法。

Method: 提出了两种方案：(1) 通用支持恢复方案，包括精确恢复和ε-近似恢复；(2) 概率精确支持恢复方案，在亚线性区域运行。

Result: (1.i) 通用精确支持恢复：m=O(k²log(n/k)log n)测量数，运行时间D=O(km)；(1.ii) 通用近似支持恢复：m=O(kε⁻¹log(n/k)log n)测量数，运行时间D=O(ε⁻¹m)；(2) 概率精确支持恢复：m=O(k(log k/log log k)log n)测量数，运行时间O(m)。

Conclusion: 本文提出的方案在单比特压缩感知支持恢复问题上实现了亚线性运行时间，显著改进了现有方法的性能，特别是在运行时间方面取得了重要突破。

Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\log(n/k)\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [135] [Neural Local Wasserstein Regression](https://arxiv.org/abs/2511.10824)
*Inga Girshfeld,Xiaohui Chen*

Main category: stat.ML

TL;DR: 提出神经局部Wasserstein回归，一种用于分布对分布回归的非参数框架，通过局部定义的传输映射来建模Wasserstein空间中的回归关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局最优传输映射或切空间线性化，这在近似能力和多变量底层域几何结构方面存在限制。

Method: 基于2-Wasserstein距离的核权重在参考测度周围局部化估计器，神经网络参数化传输算子以适应复杂数据几何，使用DeepSets架构和Sinkhorn近似损失进行训练。

Result: 在Gaussian和混合模型的合成实验以及MNIST分布预测任务中，该方法有效捕捉了现有方法无法处理的非线性和高维分布关系。

Conclusion: 局部化视角扩展了可容许变换的类别，避免了全局映射假设和线性化结构的限制，能够灵活适应复杂数据几何。

Abstract: We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.

</details>


### [136] [Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data](https://arxiv.org/abs/2511.10919)
*Jialei Liu,Jun Liao,Kuangnan Fang*

Main category: stat.ML

TL;DR: 提出了一种新颖的迁移学习与模型平均框架，用于解决正无标记学习中的数据稀缺和隐私约束问题，通过整合来自异构数据源的信息而不直接共享数据。


<details>
  <summary>Details</summary>
Motivation: 正无标记学习在欺诈检测和医疗诊断等高风险领域面临独特挑战，主要由于缺乏明确标记的负样本，同时受到数据稀缺和隐私约束的限制。

Method: 为每种源域类型构建定制的逻辑回归模型，通过模型平均将知识迁移到PU目标域，使用交叉验证准则确定最优权重以最小化KL散度，并扩展到高维设置使用稀疏惩罚估计器。

Result: 广泛的模拟和真实信用风险数据分析表明，该方法在预测准确性和鲁棒性方面优于其他比较方法，特别是在有限标记数据和异构环境下。

Conclusion: 该方法为PU学习提供了一种有效的解决方案，能够整合异构数据源信息而不直接共享数据，在数据稀缺和隐私约束场景下表现出优越性能。

Abstract: Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.

</details>


### [137] [Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths](https://arxiv.org/abs/2511.11161)
*Yuzhen Zhao,Yating Liu,Marc Hoffmann*

Main category: stat.ML

TL;DR: 基于高频离散观测数据，提出神经网络估计器用于非参数估计扩散过程的漂移函数，在紧凑域上获得非渐近收敛率，对组合漂移函数建立显式收敛率，数值实验显示收敛率与输入维度无关且优于B样条方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于N个独立轨迹的高频离散观测数据，在紧凑域上非参数估计时间齐次扩散过程漂移函数的问题，特别是在高维设置下有效捕捉局部特征。

Method: 提出神经网络估计器，推导非渐近收敛率，分解为训练误差、近似误差和扩散相关项，对组合漂移函数建立显式收敛率。

Result: 数值实验显示，对于具有局部波动的双层组合结构漂移函数，经验收敛率与输入维度d无关，相比B样条方法获得更好的收敛率和局部特征捕捉能力，特别是在高维设置中。

Conclusion: 神经网络估计器在非参数估计扩散过程漂移函数方面优于传统方法，特别是在高维情况下能有效捕捉局部特征，收敛率与维度无关。

Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.

</details>


### [138] [Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint](https://arxiv.org/abs/2511.11294)
*Bertille Tierny,Arthur Charpentier,François Hu*

Main category: stat.ML

TL;DR: 提出了一个后处理框架，用于分解线性模型中的预测偏差，将偏差分为直接（敏感属性）和间接（相关特征）成分，从而透明地解释公平性干预如何影响模型系数。


<details>
  <summary>Details</summary>
Motivation: 线性模型在高风险决策中广泛应用，但当引入公平性约束时，这些约束如何影响模型系数以及预测偏差在特征间的分布仍然不透明。现有方法要么依赖不现实的假设，要么忽略了敏感属性的明确作用。

Method: 扩展了先前工作，提出了一个后处理框架，可应用于任何线性模型，分析性地描述人口均等约束如何重塑每个模型系数，包括敏感和非敏感特征。该方法无需重新训练模型。

Result: 在合成和真实数据集上的实验表明，该方法能捕捉到先前工作遗漏的公平性动态，为线性模型的责任部署提供了实用且可解释的工具。

Conclusion: 该框架为模型审计和缓解提供了可操作的见解，能够透明地解释公平性干预，并揭示偏差如何通过相关变量持续存在或转移。

Abstract: Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [139] [One-Shot Transfer Learning for Nonlinear PDEs with Perturbative PINNs](https://arxiv.org/abs/2511.11137)
*Samuel Auroy,Pavlos Protopapas*

Main category: math.NA

TL;DR: 提出结合微扰理论和一次性迁移学习的PINNs框架，用于求解非线性偏微分方程，通过分解为线性子问题实现快速适应新问题实例


<details>
  <summary>Details</summary>
Motivation: 解决传统PINNs在求解非线性PDEs时训练成本高、难以适应新问题实例的挑战，实现快速迁移学习

Method: 将非线性PDEs分解为线性子问题序列，使用多头PINN学习线性算子潜在表示，通过闭式解适应新问题实例而无需重新训练

Result: 在KPP-Fisher和波动方程上验证，误差约1e-3，适应新问题实例时间小于0.2秒，与传统求解器精度相当但迁移更快

Conclusion: 成功将一次性迁移学习从非线性ODEs扩展到PDEs，为导数相关非线性和高维PDEs的扩展提供了基础

Abstract: We propose a framework for solving nonlinear partial differential equations (PDEs) by combining perturbation theory with one-shot transfer learning in Physics-Informed Neural Networks (PINNs). Nonlinear PDEs with polynomial terms are decomposed into a sequence of linear subproblems, which are efficiently solved using a Multi-Head PINN. Once the latent representation of the linear operator is learned, solutions to new PDE instances with varying perturbations, forcing terms, or boundary/initial conditions can be obtained in closed form without retraining.
  We validate the method on KPP-Fisher and wave equations, achieving errors on the order of 1e-3 while adapting to new problem instances in under 0.2 seconds; comparable accuracy to classical solvers but with faster transfer. Sensitivity analyses show predictable error growth with epsilon and polynomial degree, clarifying the method's effective regime.
  Our contributions are: (i) extending one-shot transfer learning from nonlinear ODEs to PDEs, (ii) deriving a closed-form solution for adapting to new PDE instances, and (iii) demonstrating accuracy and efficiency on canonical nonlinear PDEs. We conclude by outlining extensions to derivative-dependent nonlinearities and higher-dimensional PDEs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [140] [Public Goods Games in Directed Networks with Constraints on Sharing](https://arxiv.org/abs/2511.11475)
*Argyrios Deligkas,Gregory Gutin,Mark Jones,Philip R. Neary,Anders Yeo*

Main category: cs.GT

TL;DR: 该论文研究公共物品博弈中玩家在定向网络中的购买和分享决策，分析纳什均衡的存在性、计算复杂性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 研究在定向网络中，当公共物品不可分割且分享能力有限时，玩家如何做出购买和分享决策，以及这种博弈的均衡特性。

Method: 建立公共物品博弈模型，玩家决定是否购买物品并选择分享给哪些邻居，分析纯策略和混合策略纳什均衡，研究不同网络结构和分享容量下的均衡特性。

Result: 建立了关于分享容量和网络结构的尖锐复杂性二分法，揭示了均衡存在性、计算复杂性和效率与这些参数的关系。

Conclusion: 该模型为理解有限分享能力下的公共物品博弈提供了理论框架，揭示了网络结构和分享容量对均衡特性的重要影响。

Abstract: In a public goods game, every player chooses whether or not to buy a good that all neighboring players will have access to. We consider a setting in which the good is indivisible, neighboring players are out-neighbors in a directed graph, and there is a capacity constraint on their number, k, that can benefit from the good. This means that each player makes a two-pronged decision: decide whether or not to buy and, conditional on buying, choose which k out-neighbors to share access. We examine both pure and mixed Nash equilibria in the model from the perspective of existence, computation, and efficiency. We perform a comprehensive study for these three dimensions with respect to both sharing capacity (k) and the network structure (the underlying directed graph), and establish sharp complexity dichotomies for each.

</details>


### [141] [Deviation Dynamics in Cardinal Hedonic Games](https://arxiv.org/abs/2511.11531)
*Valentin Zech,Martin Bullinger*

Main category: cs.GT

TL;DR: 该论文研究了基数享乐博弈中稳定分区的计算问题，证明了基于No-instances存在性的计算困难性，并提出了在可加可分享乐博弈中寻找个体理性和契约个体稳定分区的动态方法。


<details>
  <summary>Details</summary>
Motivation: 享乐博弈中稳定分区计算具有挑战性，因为存在稳定结果不存在的游戏实例，这些No-instances可用于证明计算困难性。

Method: 在基数享乐博弈的动态模型中提供元定理，涵盖可加可分、分数和修正分数享乐博弈，以及所有基于单智能体偏离的合理稳定性概念。

Result: 证明了基于No-instances存在性的偏离动态收敛可能性和必要性的判定困难性，发现契约个体稳定动态从单例分区出发可能在线性步数内收敛，但最坏情况下需要指数级步数。

Conclusion: 研究为享乐博弈中稳定分区的计算复杂性提供了理论基础，揭示了动态方法在寻找特定稳定分区方面的潜力和局限性。

Abstract: Computing stable partitions in hedonic games is a challenging task because there exist games in which stable outcomes do not exist. Even more, these No-instances can often be leveraged to prove computational hardness results. We make this impression rigorous in a dynamic model of cardinal hedonic games by providing meta theorems. These imply hardness of deciding about the possible or necessary convergence of deviation dynamics based on the mere existence of No-instances. Our results hold for additively separable, fractional, and modified fractional hedonic games (ASHGs, FHGs, and MFHGs). Moreover, they encompass essentially all reasonable stability notions based on single-agent deviations. In addition, we propose dynamics as a method to find individually rational and contractually individual stable (CIS) partitions in ASHGs. In particular, we find that CIS dynamics from the singleton partition possibly converge after a linear number of deviations but may require an exponential number of deviations in the worst case.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [142] [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)
*Jiaming Huang,Yi Gao,Fuchang Pan,Renjie Li,Wei Dong*

Main category: cs.CV

TL;DR: 提出SemanticNN语义编解码器，在资源受限的嵌入式设备上实现容错的设备-边缘协作推理，通过语义级正确性而非比特级正确性，显著减少特征传输量同时保持高推理精度。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备快速增长，在资源极度受限的嵌入式设备上集成AI面临计算和通信限制，传统基于比特级传输正确性的方法在动态信道条件下效率低下。

Method: 提出SemanticNN语义编解码器，包含BER感知解码器适应动态信道条件、软量化编码器学习紧凑表示、特征增强学习训练策略，以及XAI不对称补偿解决编码器-解码器能力不匹配问题。

Result: 在STM32平台上使用三种模型和六个数据集进行实验，在变化传输错误率下，SemanticNN显著减少特征传输量56.82-344.83倍，同时保持优越的推理精度。

Conclusion: SemanticNN通过语义级容错方法，在严格的计算和通信约束下实现了高效且鲁棒的设备-边缘协作推理卸载。

Abstract: With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.

</details>


### [143] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出一种可扩展的高效数据归因方法，通过将基于反学习的慢速归因方法蒸馏到特征嵌入空间，实现高效检索高影响力训练图像，比现有方法快2500-400000倍。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的数据归因方法计算资源消耗大，难以在实际应用中部署，需要开发高效的归因方法。

Method: 将基于反学习的慢速归因方法蒸馏到特征嵌入空间，结合高效索引和搜索方法，无需运行昂贵的归因算法即可找到高影响力训练图像。

Result: 在MSCOCO训练的中等规模模型和LAION训练的大规模Stable Diffusion模型上验证，性能优于或与现有方法相当，速度提升2500-400000倍。

Conclusion: 该方法为实现Stable Diffusion等真实世界模型的大规模数据归因应用迈出了重要一步。

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [144] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: 提出了一种基于LRP的CNN剪枝方法，能在少量数据情况下实现更高剪枝率同时保持更好的模型精度


<details>
  <summary>Details</summary>
Motivation: 现有基于LRP的剪枝方法虽然不需要微调，适合数据有限场景，但仍存在显著的精度下降问题，限制了实际应用

Method: 使用层间相关性传播(LRP)技术进行模型剪枝，针对少量数据场景优化

Result: 相比现有方法，在保持精度方面表现更好，实现了更高的剪枝率

Conclusion: 该方法在数据有限的情况下能够有效压缩CNN模型，在保持精度的同时实现更高的剪枝率

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [145] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: PROMISE是一个新颖的多模态框架，通过提示学习和分层对比学习处理缺失模态问题，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据常存在模态缺失问题，现有生成方法难以保持跨模态一致性，导致性能下降。

Method: 提出PROMISE框架，将多模态提示学习融入分层对比学习，采用提示注意力机制动态生成缺失模态的鲁棒表示。

Result: 在基准数据集上的广泛实验和消融研究显示，PROMISE相比现有最先进方法具有显著优越性能。

Conclusion: PROMISE有效解决了多模态数据中模态缺失问题，通过提示学习和分层对比学习实现了跨模态表示的鲁棒性和一致性。

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [146] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: VisMem是一个受人类认知记忆理论启发的视觉语言模型框架，通过短期和长期视觉记忆模块解决视觉处理瓶颈问题，在多个视觉基准测试中平均性能提升11.8%。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在复杂视觉任务中存在"视觉处理瓶颈"，容易在生成过程中失去视觉证据的接地性，缺乏情境化的视觉体验。

Method: 提出VisMem框架，包含动态潜在视觉记忆：短期模块用于细粒度感知保留，长期模块用于抽象语义整合，在推理过程中无缝调用这些记忆。

Result: 在理解、推理和生成等多样化视觉基准测试中，VisMem相比原始模型平均性能提升11.8%，优于所有对比方法。

Conclusion: VisMem建立了一个新的潜在空间记忆增强范式，能够同时保持感知保真度和语义一致性。

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [147] [PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI](https://arxiv.org/abs/2511.11048)
*Sun Jo,Seok Young Hong,JinHyun Kim,Seungmin Kang,Ahjin Choi,Don-Gwan An,Simon Song,Je Hyeong Hong*

Main category: cs.CV

TL;DR: PINGS-X是一个基于轴对齐时空高斯表示的4D流MRI超分辨率框架，显著减少训练时间的同时实现卓越的超分辨率精度


<details>
  <summary>Details</summary>
Motivation: 解决传统物理信息神经网络在4D流MRI超分辨率中训练缓慢、需要为每个患者单独训练的问题

Method: 采用轴对齐时空高斯表示，包含归一化高斯溅射、轴对齐高斯和合并程序三个创新点

Result: 在计算流体动力学和真实4D流MRI数据集上，PINGS-X显著减少训练时间并实现优越的超分辨率精度

Conclusion: PINGS-X为4D流MRI超分辨率提供了一种高效准确的解决方案，克服了现有方法的训练效率限制

Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.

</details>


### [148] [VIDEOP2R: Video Understanding from Perception to Reasoning](https://arxiv.org/abs/2511.11113)
*Yifan Jiang,Yueying Wang,Rui Zhao,Toufiq Parag,Zhimin Chen,Zhenyu Liao,Jayakrishnan Unnikrishnan*

Main category: cs.CV

TL;DR: VideoP2R是一个面向视频语言模型的过程感知强化微调框架，通过将感知和推理建模为独立过程来增强视频推理能力，在7个基准测试中的6个达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 将强化微调(RFT)框架扩展到大型视频语言模型(LVLMs)具有挑战性，需要专门的方法来处理视频推理任务。

Method: 采用两阶段框架：SFT阶段使用三步流水线生成高质量的过程感知链式思维数据集；RL阶段提出过程感知组相对策略优化算法，为感知和推理提供独立奖励。

Result: 在7个视频推理和理解基准测试中的6个达到最先进性能，消融研究证实了过程感知建模和PA-GRPO的有效性。

Conclusion: VideoP2R成功将RFT扩展到视频语言模型，证明了过程感知建模的重要性，且模型的感知输出对下游推理具有充分信息。

Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.

</details>


### [149] [Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA](https://arxiv.org/abs/2511.11169)
*Ayush Pandey,Jai Bardhan,Ishita Jain,Ramya S Hebbalaguppe,Rohan Raju Dhanakshirur,Lovekesh Vig*

Main category: cs.CV

TL;DR: AlignVQA是一个基于辩论的多智能体框架，通过专门的视觉语言模型生成候选答案，经过两阶段交互（通用智能体批判、提炼和聚合）来获得更准确的置信度估计，并使用可微分的校准感知损失函数来微调专门智能体。


<details>
  <summary>Details</summary>
Motivation: 在VQA和Agentic AI中，AI系统对其答案的置信度校准至关重要，特别是在医疗诊断和自主导航等高风险领域。现代VQA系统虽然准确性提高，但其置信度估计的可靠性仍未被充分研究，经常产生过度自信的响应。

Method: 提出AlignVQA辩论框架：1）多样化专门VLM使用不同提示策略生成候选答案；2）通用智能体进行两阶段交互（批判、提炼和聚合）；3）引入可微分校准感知损失函数aligncal，通过最小化校准误差上界来微调专门智能体。

Result: 在多个基准VQA数据集上的实证结果表明，该方法显著减少了校准差异，更校准的专门智能体产生更好对齐的置信度。

Conclusion: AlignVQA框架通过多智能体辩论和校准感知微调，有效提高了VQA系统的置信度校准质量，使其置信度估计更准确地反映真实预测性能。

Abstract: In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.

</details>


### [150] [Questioning the Stability of Visual Question Answering](https://arxiv.org/abs/2511.11206)
*Amir Rosenfeld,Neta Glazer,Ethan Fetaya*

Main category: cs.CV

TL;DR: 该研究首次大规模系统性地评估了视觉语言模型对良性视觉和文本扰动的鲁棒性，发现即使是最先进的模型也对微小扰动高度敏感，且稳定性与正确性高度相关。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型取得了显著进展，但其在微小但语义保持的输入变化下的可靠性仍未被充分理解，需要系统评估模型对良性扰动的鲁棒性。

Method: 通过像素级偏移、几何变换、缩放填充、文本改写和多语言重写等不改变语义的扰动方式，在多种模型和数据集上进行大规模测试。

Result: 现代VLM对微小扰动高度敏感，大量样本在至少一种扰动下改变预测答案；稳定性与正确性高度相关，稳定样本更可能被正确回答；小模型的稳定性模式可预测大模型的正确性。

Conclusion: 当前VLM存在根本性的脆弱性，需要超越对抗性扰动的鲁棒性评估，关注模型应可靠保持的不变性。

Abstract: Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.

</details>


### [151] [BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.11421)
*Lan Li,Tao Hu,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: BOFA是一个用于类增量学习的框架，通过在CLIP的跨模态桥接层进行参数适配，使用正交低秩融合防止遗忘，并结合跨模态混合原型提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 将CLIP应用于类增量学习面临两个挑战：适应下游任务需要额外可学习模块增加模型复杂度和遗忘风险；现有方法未能充分利用多模态表示的互补优势。

Method: BOFA框架将模型适配限制在CLIP现有的跨模态桥接层，不增加额外参数。采用正交低秩融合机制，将参数更新约束在数学构造的低秩"安全子空间"中，该子空间与过去任务特征正交。同时使用结合稳定文本原型和视觉对应物的跨模态混合原型。

Result: 在标准基准测试上的广泛实验表明，BOFA在准确性和效率方面优于现有方法。

Conclusion: BOFA通过桥接层正交融合适配，实现了无数据回放的稳定知识积累，在类增量学习中取得了优越的性能。

Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

</details>


### [152] [VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation](https://arxiv.org/abs/2511.11450)
*Maximilian Rokuss,Moritz Langenberg,Yannick Kirchhoff,Fabian Isensee,Benjamin Hamm,Constantin Ulrich,Sebastian Regnery,Lukas Bauer,Efthimios Katsigiannopulos,Tobias Norajitra,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: VoxTell是一个用于文本提示的3D医学图像分割的视觉语言模型，能够将自由形式的文本描述映射到3D分割掩码。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够理解从单词到完整临床句子的自由形式文本描述，并将其映射到3D医学图像分割掩码的模型，以支持更灵活和自然的医学图像分析。

Method: 使用多阶段视觉语言融合策略，在解码器层间对齐文本和视觉特征，训练于62,000+个CT、MRI和PET体积图像，涵盖1,000+个解剖和病理类别。

Result: 在未见数据集上实现了最先进的零样本性能，在不同模态上表现出色，能够泛化到相关未见类别，并展示了强大的跨模态迁移能力和对语言变化的鲁棒性。

Conclusion: VoxTell展示了在医学图像分割中结合视觉和语言理解的潜力，为临床实践提供了更直观和灵活的工具。

Abstract: We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell

</details>


### [153] [CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation](https://arxiv.org/abs/2511.11522)
*Luthira Abeykoon,Ved Patel,Gawthaman Senthilvelan,Darshan Kasundra*

Main category: cs.CV

TL;DR: CVChess是一个深度学习框架，可将棋盘图像转换为FEN符号，然后输入在线象棋引擎获取最佳下一步移动


<details>
  <summary>Details</summary>
Motivation: 疫情期间在线象棋观看人数激增，但物理象棋游戏缺乏相应的辅助工具，导致模拟和数字象棋体验之间存在差距

Method: 使用带残差层的卷积神经网络进行棋子识别，包括霍夫线变换边缘检测、投影变换获得俯视棋盘、分割为64个方格、使用残差CNN将棋子分类为13类

Result: 使用包含10,800张标注智能手机图像的Chess Recognition Dataset进行训练和评估，残差连接有助于保留低级视觉特征并实现更深层特征提取

Conclusion: 该系统能够将物理棋盘图像转换为FEN字符串，可输入象棋引擎生成最优移动

Abstract: Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [154] [Surrogate-Based Differentiable Pipeline for Shape Optimization](https://arxiv.org/abs/2511.10761)
*Andrin Rehmann,Nolan Black,Josiah Bjorgaard,Alessandro Angioi,Andrei Paleyes,Niklas Heim,Dion Häfner,Alexander Lavin*

Main category: cs.CE

TL;DR: 提出使用可微分代理模型替换CAE工作流中的非可微分组件，实现端到端可微分管道，支持基于梯度的形状优化。


<details>
  <summary>Details</summary>
Motivation: 传统CAE工作流中的网格划分、物理模拟等组件不可微分，限制了基于梯度优化方法在高维设计空间中的应用。

Method: 用3D U-Net全场代理模型替代网格划分和模拟步骤，训练其学习形状的符号距离场与感兴趣场之间的映射关系。

Result: 构建了端到端可微分管道，可在没有可微分求解器的情况下实现基于梯度的形状优化。

Conclusion: 该方法在伴随方法不可用或难以实现的情况下特别有用，为工程优化提供了新途径。

Abstract: Gradient-based optimization of engineering designs is limited by non-differentiable components in the typical computer-aided engineering (CAE) workflow, which calculates performance metrics from design parameters. While gradient-based methods could provide noticeable speed-ups in high-dimensional design spaces, codes for meshing, physical simulations, and other common components are not differentiable even if the math or physics underneath them is. We propose replacing non-differentiable pipeline components with surrogate models which are inherently differentiable. Using a toy example of aerodynamic shape optimization, we demonstrate an end-to-end differentiable pipeline where a 3D U-Net full-field surrogate replaces both meshing and simulation steps by training it on the mapping between the signed distance field (SDF) of the shape and the fields of interest. This approach enables gradient-based shape optimization without the need for differentiable solvers, which can be useful in situations where adjoint methods are unavailable and/or hard to implement.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [155] [Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation](https://arxiv.org/abs/2511.11311)
*Petros Koutsouvelis,Matej Gazda,Leroy Volmer,Sina Amirrajab,Kamil Barbierik,Branislav Setlak,Jakub Gazda,Peter Drotar*

Main category: eess.IV

TL;DR: 该研究提出了一种模态不变表示学习方法，用于脑部MRI的多模态数据，并在中风和癫痫病灶分割任务中评估其效果。结果表明，尽管实现了跨模态对齐，但病灶分割主要受益于保留细粒度的模态特定特征。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域正转向通过自监督学习进行大规模基础模型预训练。利用大量未标记的脑部MRI数据，这些模型可以学习解剖学先验，从而在各种神经影像任务中提高少样本性能。然而，大多数自监督学习框架是为自然图像设计的，它们在捕捉多模态MRI信息方面的适应性仍有待探索。

Method: 提出了一种模态不变表示学习设置，通过大规模预训练来学习跨模态的表示。该方法旨在对齐不同MRI模态的特征表示，同时评估其在病灶分割任务中的有效性。

Result: 实验结果表明，尽管成功实现了跨模态对齐，但病灶分割任务主要受益于保留细粒度的模态特定特征，而不是完全的模态不变性。

Conclusion: 模态不变表示学习在脑部MRI多模态数据中能够实现跨模态对齐，但对于病灶分割等需要精细解剖信息的任务，保留模态特定特征更为重要。模型检查点和代码已公开提供。

Abstract: The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [156] [Inferring response times of perceptual decisions with Poisson variational autoencoders](https://arxiv.org/abs/2511.11480)
*Hayden R. Johnson,Anastasia N. Krouglova,Hadi Vafaii,Jacob L. Yates,Pedro J. Gonçalves*

Main category: q-bio.NC

TL;DR: 该论文提出了一个基于贝叶斯解码和泊松变分自编码器的感知决策计算模型，能够同时生成选择决策和反应时间，重现了人类感知决策的关键特征。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络模型通常将决策视为瞬时读取，忽略了决策过程的时间动态特性。

Method: 使用泊松变分自编码器学习视觉刺激的无监督表示，结合任务优化的贝叶斯解码器和基于熵的停止规则，构建图像可计算的感知决策模型。

Result: 在MNIST数字分类任务中，模型成功重现了感知决策的关键经验特征：随机变异性、右偏反应时间分布、希克定律和速度-准确性权衡。

Conclusion: 该模型提供了一个原则性的图像可计算框架，能够同时解释感知决策中的选择行为和反应时间动态。

Abstract: Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [157] [Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility](https://arxiv.org/abs/2511.11564)
*Albert Tan,Mohsen Bayati,James Nordlund,Roman Istomin*

Main category: stat.ME

TL;DR: 该论文研究了二分系统中的随机实验，其中只有部分处理侧单元有资格接受处理，但所有单元继续交互产生干扰效应。论文定义了与完全部署对齐的估计量，并开发了结合暴露映射、广义倾向得分和机器学习的干扰感知集成估计器。


<details>
  <summary>Details</summary>
Motivation: 在二分系统实验中，只有部分处理单元有资格接受处理，但所有单元继续交互产生干扰效应，这给因果推断带来挑战。需要开发能够处理这种资格约束和干扰效应的估计方法。

Method: 提出了干扰感知集成估计器，结合暴露映射、广义倾向得分和灵活机器学习方法。引入了连接处理层和结果层估计量的投影方法，在满足线性可加边条件下实现精确映射。

Result: 在模拟实验中，提出的估计器能够以低偏差和方差恢复PTTE和STTE，并减少忽略干扰时可能产生的偏差。在两个实地实验中，该方法修正了预期干扰偏差的方向，并在一个案例中改变了主要决策指标的符号和显著性。

Conclusion: 该研究为二分系统中的资格约束实验提供了有效的因果推断框架，能够准确估计干扰效应，并在实际应用中修正传统方法可能导致的错误结论。

Abstract: We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [158] [Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer](https://arxiv.org/abs/2511.11452)
*Seth Alain Chang,Muhammad Mueez Amjad,Noorul Wahab,Ethar Alzaid,Nasir Rajpoot,Adam Shephard*

Main category: q-bio.QM

TL;DR: 多模态深度学习在计算病理学中表现出色，但研究发现并非所有模态组合都能提升性能。只有高性能模态的组合才能带来优势，而加入低性能模态反而会降低预测准确性。


<details>
  <summary>Details</summary>
Motivation: 检验多模态深度学习中的假设：组合模态是否必然提升性能，以及模态整合是否取决于各模态的预测质量。

Method: 在前列腺癌数据集上使用组织病理学、放射学和临床数据，预测生化复发时间，比较不同模态组合的性能。

Result: 高性能模态组合确实优于单模态方法，但将低性能模态与高性能模态整合会降低预测准确性。

Conclusion: 多模态获益需要基于性能的选择性整合，而非不加区分的模态组合，这对计算病理学和医学影像中的MDL设计具有重要意义。

Abstract: Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [159] [Heterogeneous CACC Coexistence: Simulation, Analysis, and Modeling](https://arxiv.org/abs/2511.11429)
*Lorenzo Ghiro,Marco Franceschini,Renato Lo Cigno,Michele Segata*

Main category: eess.SY

TL;DR: 本文研究了异质CACC车辆组成的混合车队性能，发现某些CACC组合能安全运行，而其他组合在安全、舒适性或效率方面存在关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设车队由同质CACC车辆组成，但市场竞争可能导致不同制造商采用不同的CACC算法，需要研究异质车辆能否安全协作及其性能表现。

Method: 通过仿真实验研究混合车队性能：首先研究孤立混合车队的微观安全影响，评估不同CACC组合在速度变化和紧急制动场景下的表现；其次在高密度环形道路场景中评估宏观安全、舒适性和交通吞吐量影响。

Result: 研究发现某些CACC组合能够稳健安全地运行，而其他组合在安全、舒适性或效率方面表现出关键限制。

Conclusion: 结果强调了需要谨慎的系统设计和开发异质车队建模的理论框架。

Abstract: The design of Cooperative Adaptive Cruise Control (CACC) algorithms for vehicle platooning has been extensively investigated, leading to a wide range of approaches with different requirements and performance. Most existing studies evaluate these algorithms under the assumption of homogeneous platoons, i.e., when all platoon members adopt the same CACC. However, market competition is likely to result in vehicles from different manufacturers implementing distinct CACCs. This raises fundamental questions about whether heterogeneous vehicles can safely cooperate within a platoon and what performance can be achieved. To date, these questions have received little attention, as heterogeneous platoons are difficult to model and analyze. In this work, we introduce the concept of mixed platoons, i.e., platoons made of vehicles running heterogeneous CACCs, and we study their performance through simulation-based experiments. We consider mixtures of three well-established CACCs from the literature. In the first part of the paper, we study a single mixed platoon in isolation to understand the microscopic effects on safety: we evaluate the performance of various CACC-mixtures across speed change and emergency braking scenarios. In the second part, we examine a high-density ring-road scenario to assess macroscopic impacts on safety, comfort, and traffic throughput, especially comparing throughput results with those obtained from vehicles controlled by a standard Adaptive Cruise Control (ACC) or by human drivers. Our findings highlight that some combinations of CACCs can operate robustly and safely, while others exhibit critical limitations in safety, comfort, or efficiency. These results emphasize the need for careful system design and the development of theoretical frameworks for modeling heterogeneous platoons.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [160] [TSP integrality gap via 2-edge-connected multisubgraph problem under coincident IP optima](https://arxiv.org/abs/2511.11215)
*Toshiaki Yamanaka*

Main category: math.OC

TL;DR: 本文提出了一个转移原则：当2ECM问题的整数最优解是唯一哈密顿环时，任何输出哈密顿环的α近似算法都能为TSP提供α近似。通过构建具有唯一哈密顿环最优解和半整数LP解的2ECM实例，可以证明TSP的整数性间隙不超过4/3。


<details>
  <summary>Details</summary>
Motivation: 解决度量旅行商问题线性规划松弛的整数性间隙这一长期开放问题。

Method: 引入转移原则和割边距唯一性框架，证明当2ECM问题的整数最优解是唯一哈密顿环时，2ECM的近似算法可直接转化为TSP的近似算法。

Result: 如果存在2ECM实例同时具有唯一哈密顿环整数最优解和半整数LP解，则TSP的整数性间隙不超过4/3。

Conclusion: 构建这样的2ECM实例仍然是一个开放问题，但提出的框架为证明TSP整数性间隙上限提供了新途径。

Abstract: Determining the integrality gap of the linear programming (LP) relaxation of the metric traveling salesman problem (TSP) remains a long-standing open problem. We introduce a transfer principle: when the integer optimum of the 2-edge-connected multisubgraph problem (2ECM) is a unique Hamiltonian cycle $T$, any $α$-approximation algorithm for 2ECM that outputs a Hamiltonian cycle yields an $α$-approximation for TSP. We further develop a cut-margin uniqueness framework that certifies $T$ as the unique integer optimum for both problems and is stable under $\ell_\infty$-bounded perturbations. We show that, if instances exist where the 2ECM has both a unique Hamiltonian cycle integer optimum and a half-integral LP solution, then the TSP integrality gap is at most 4/3 by the algorithm of Boyd et al. (SIAM Journal on Discrete Mathematics 36:1730--1747, 2022). Constructing such instances remains an open problem.

</details>


### [161] [Linear-Space Extragradient Methods for Fast, Large-Scale Optimal Transport](https://arxiv.org/abs/2511.11359)
*Matthew X. Burns,Jiaming Liang*

Main category: math.OC

TL;DR: 提出了一种仅需O(n)存储空间的对偶外梯度方法(DXG)，用于求解最优传输(OT)和熵正则化OT(EOT)问题，首次实现了O(n²ε⁻¹)复杂度且仅需O(n)内存。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模(E)OT一阶方法在获得最优收敛率时需要O(n²)存储空间来实现遍历原始平均，存在存储效率瓶颈。

Method: 利用原始-对偶外梯度方法(PDXG)完全在对偶空间中实现，仅需O(n)存储。证明了重新表述的OT问题正则化等价于EOT，并可扩展到熵正则化重心问题。

Result: DXG是首个在O(n)内存下实现O(n²ε⁻¹)复杂度求解ε-近似OT的算法。数值实验显示在非/弱正则化情况下具有良好的可扩展性。

Conclusion: 提出的对偶外梯度方法解决了(E)OT计算中的存储效率问题，为大规模应用提供了更高效的解决方案，但在某些问题类别中性能仍需改进。

Abstract: Optimal transport (OT) and its entropy-regularized form (EOT) have become increasingly prominent computational problems, with applications in machine learning and statistics. Recent years have seen a commensurate surge in first-order methods aiming to improve the complexity of large-scale (E)OT. However, there has been a consistent tradeoff: attaining state-of-the-art rates requires $\mathcal{O}(n^2)$ storage to enable ergodic primal averaging. In this work, we demonstrate that recently proposed primal-dual extragradient methods (PDXG) can be implemented entirely in the dual with $\mathcal{O}(n)$ storage. Additionally, we prove that regularizing the reformulated OT problem is equivalent to EOT with extensions to entropy-regularized barycenter problems, further widening the applications of the proposed method. The proposed dual-only extragradient method (DXG) is the first algorithm to achieve $\mathcal{O}(n^2\varepsilon^{-1})$ complexity for $\varepsilon$-approximate OT with $\mathcal{O}(n)$ memory. Numerical experiments demonstrate that the dual extragradient method scales favorably in non/weakly-regularized regimes compared to existing algorithms, though future work is needed to improve performance in certain problem classes.

</details>


### [162] [Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates](https://arxiv.org/abs/2511.11466)
*Dmitry Kovalev,Ekaterina Borodich*

Main category: math.OC

TL;DR: 本文为几种非欧几里得SGD方法（如SignSGD、Lion、Muon）提供了新的收敛性分析，证明它们能够利用Hessian和梯度噪声的上界稀疏性或低秩结构，匹配自适应优化算法的最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析无法解释非欧几里得SGD方法在实际训练深度神经网络中的优越性能，因为它们未能超越传统欧几里得SGD的收敛速度。

Method: 在结构化平滑性和梯度噪声假设下，开发了新的统一收敛分析框架。

Result: 分析表明非欧几里得SGD能够：(i) 利用Hessian和梯度噪声上界的稀疏性或低秩结构；(ii) 从外推法或动量方差减少等工具中获益；(iii) 匹配AdaGrad和Shampoo等复杂自适应算法的最优收敛率。

Conclusion: 该研究解决了非欧几里得SGD理论分析的重要开放问题，为其实际成功提供了理论依据，证明这些方法能够达到最先进优化算法的收敛性能。

Abstract: Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.

</details>
