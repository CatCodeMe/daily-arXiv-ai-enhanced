<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.CV](#cs.CV) [Total: 7]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [quant-ph](#quant-ph) [Total: 3]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows](https://arxiv.org/abs/2512.17429)
*Kyriakos Psarakis*

Main category: cs.DB

TL;DR: 该论文提出Stateflow编程模型和Styx分布式流数据流引擎，通过将云应用映射到流数据流执行模型，实现可编程、高性能、容错的串行化事务处理，简化云应用开发。


<details>
  <summary>Details</summary>
Motivation: 当前构建可扩展、一致的云应用需要跨云计算、分布式系统、数据库和软件工程的专业知识，限制了开发人员范围。论文旨在通过解决可编程性、高性能容错串行化事务和无服务器语义三大挑战，实现云应用开发的民主化。

Method: 1. 识别云应用与流数据流执行模型的强关联性；2. 开发T-Statefun作为Apache Flink Statefun的事务扩展；3. 提出Stateflow高级面向对象编程模型，将应用编译为状态数据流图；4. 构建Styx分布式流数据流引擎，提供确定性、多分区、串行化事务和强容错保证；5. 扩展Styx支持事务状态迁移以实现弹性伸缩。

Result: Stateflow编程模型显著减少样板代码，Styx引擎消除了显式事务失败处理，性能显著优于现有最先进系统，并支持动态工作负载下的弹性伸缩。

Conclusion: 通过将云应用映射到流数据流执行模型，结合Stateflow编程模型和Styx引擎，成功解决了云应用开发的可编程性、高性能事务处理和弹性伸缩三大挑战，实现了云应用开发的民主化。

Abstract: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.
  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.
  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.
  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 本文提出了一种针对ROS2中任意有向无环图（DAG）任务的新型调度方法，使用事件执行器实现固定作业级优先级调度，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型。


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要局限于简单的链式任务调度，缺乏对任意有向无环图（DAG）任务的分析能力。现有研究多集中于链式调度和临时响应时间分析，需要更通用的调度方法。

Method: 使用事件执行器实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型。需要特殊的事件队列实现和支持LIFO顺序消息传递的通信中间件。

Result: 该方法能够在单处理器系统上为任意ROS2图生成与传统固定优先级DAG任务调度器相同的调度结果，尽管不需要通常所需的优先级信息。缩小了实时系统理论与ROS2调度分析之间的差距。

Conclusion: 提出的方法扩展了ROS2调度能力，使其能够处理任意DAG任务，为ROS2应用提供了更强大的实时调度分析框架，推动了实时系统理论在ROS2中的实际应用。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [3] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成高性能计算代码方面的能力，特别是针对Mandelbrot集在不同并行范式下的C++实现


<details>
  <summary>Details</summary>
Motivation: 并行编程是高性能计算中最具挑战性的方面之一，需要深入了解同步、通信和内存模型。尽管现代C++标准和OpenMP、MPI等框架简化了并行化，但掌握这些范式仍然很复杂。大型语言模型在代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的效果尚不清楚。

Method: 系统评估包括ChatGPT 4和5、Claude和LLaMA在内的领先大型语言模型，在生成使用共享内存、基于指令和分布式内存范式的Mandelbrot集C++实现方面的能力。每个生成的程序都使用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现强劲。

Conclusion: 大型语言模型在高性能计算代码生成方面具有实际应用潜力，特别是ChatGPT系列模型在生成正确且可扩展的并行代码方面表现突出。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [4] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve：首个针对扩散大语言模型的高效服务系统，通过内存优化、计算调度和生成质量协同优化，解决扩散模型特有的内存占用危机和资源振荡问题，在消费级和服务器级GPU上实现1.6-1.8倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案具有并行解码优势，但现有研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。研究发现dLLM存在特有的"内存占用危机"，由单一logit张量和计算密集型"刷新"阶段与带宽密集型"重用"阶段之间的严重资源振荡驱动。

Method: 提出dLLM-Serve系统，包含三个核心技术：1) Logit-Aware Activation Budgeting：分解瞬态张量峰值；2) Phase-Multiplexed Scheduler：交错处理异构请求阶段；3) Head-Centric Sparse Attention：将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载（LiveBench、Burst、OSC）和GPU（RTX 4090、L40S）上评估，相比最先进基线：消费级RTX 4090吞吐量提升1.61-1.81倍，服务器级L40S提升1.60-1.74倍，高负载下尾部延迟降低近4倍。

Conclusion: dLLM-Serve建立了首个可扩展dLLM推理蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，为扩散大语言模型的生产部署提供了高效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [5] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一个可扩展的向量索引系统，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现了高精度、低延迟和高吞吐量的分布式近似最近邻搜索。


<details>
  <summary>Details</summary>
Motivation: 现有分布式索引设计在扩展到数十亿向量时难以平衡精度、延迟和吞吐量之间的权衡，需要新的索引设计来解决这一挑战。

Method: SPIRE采用两个关键设计：1) 识别平衡的分区粒度以避免读取成本爆炸；2) 引入精度保持的递归构建方法，构建具有可预测搜索成本和稳定精度的多级索引。

Result: 在46个节点上对最多80亿向量的实验中，SPIRE实现了高可扩展性，吞吐量比最先进系统提高了9.64倍。

Conclusion: SPIRE通过创新的分区粒度和递归构建方法，成功解决了大规模分布式向量索引在精度、延迟和吞吐量之间的权衡问题，为数十亿规模的近似最近邻搜索提供了高效解决方案。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [6] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云端的联邦系统HEAL数据平台，作为NIH HEAL计划数据的统一搜索、发现和分析入口


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生的多样化数据分散在多个NIH和第三方数据仓库中，需要一个统一平台来促进数据发现和二次利用

Method: 基于开源Gen3平台构建，使用框架服务（认证授权、数据对象标识符、元数据管理）和API与数据仓库互操作

Result: 平台已整合1000多项HEAL研究，每月数百用户使用，与19个数据仓库互操作，提供丰富元数据和云端计算环境

Conclusion: HEAL数据平台通过网状架构实现分散数据的统一发现，确保数据符合FAIR原则，最大化HEAL数据的价值

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [7] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型推理系统，通过GPU协作解码加速预处理，并通过逻辑解耦、资源共享消除阶段间阻塞，提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的三阶段流水线（多模态预处理、视觉编码、LLM推理）存在显著瓶颈：CPU解码主导首token延迟，视觉编码与LLM阶段异构导致资源利用不足和阶段间阻塞，限制了系统吞吐量和延迟性能。

Method: 提出FlashCodec（多GPU协作视频解码加速预处理）和UnifiedServe（逻辑解耦但物理共享GPU资源的视觉-文本联合优化）两个互补设计，通过精心编排执行阶段和最小化干扰来优化端到端流水线。

Result: 相比最先进系统，该框架可服务3.0倍更多请求或实现1.5倍更严格的SLO约束，同时达到4.4倍更高的吞吐量。

Conclusion: FlashCodec和UnifiedServe联合形成的端到端优化堆栈能有效解决MLLM推理系统的瓶颈问题，显著提升系统性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach](https://arxiv.org/abs/2512.16927)
*Xinyu Guan,Shaohua Zhang*

Main category: cs.DS

TL;DR: 本文提出了一种结合Ukkonen算法和新搜索技术的优化后缀树方法，在Reuters语料库和人类基因组等大规模数据集上实现了线性时间和空间效率，显著优于传统文本搜索算法。


<details>
  <summary>Details</summary>
Motivation: 传统文本搜索算法（如朴素搜索、KMP、Boyer-Moore）在处理现代大规模复杂数据集（如Reuters语料库和人类基因组序列）时存在效率不足的问题，需要更高效的算法来满足自然语言处理和生物信息学等领域的需求。

Method: 研究系统地分析了文本搜索算法，重点优化后缀树，采用了分割方法和Ukkonen算法。提出了一种新颖的优化方法，将Ukkonen算法与新的搜索技术相结合。

Result: 优化后的后缀树方法实现了线性时间和空间效率，在基因组序列模式识别等任务中达到100%准确率，在Reuters语料库和人类基因组数据集上显著优于朴素搜索、KMP和Boyer-Moore等传统方法。

Conclusion: 该研究不仅推进了文本搜索算法的学术知识，而且在自然语言处理和生物信息学等领域展示了显著的实用价值，因其优越的资源效率和可靠性。

Abstract: In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.

</details>


### [9] [New Theoretical Insights and Algorithmic Solutions for Reconstructing Score Sequences from Tournament Score Sets](https://arxiv.org/abs/2512.16961)
*Bowen Liu*

Main category: cs.DS

TL;DR: 提出基于Landau定理的必要充分条件和必要条件，用于从锦标赛图的得分集重建得分序列，并开发了三个实用算法


<details>
  <summary>Details</summary>
Motivation: 解决Reid猜想中锦标赛得分集重建问题，虽然Yao在1989年给出了算术证明，但缺乏多项式时间的构造算法

Method: 基于Landau定理提出必要充分条件和结构化集合条件，利用群论技术，开发了三个算法：多项式时间重建算法、可扩展重建算法、多项式时间网络构建方法

Result: 提出的条件为重建问题提供了理论框架，算法具有实际应用价值，多项式时间算法可用于验证Reid猜想

Conclusion: 该框架不仅解决了得分序列重建问题，还为处理类似约束问题提供了新视角，在体育分析、排名预测和机器学习中有实际应用

Abstract: The score set of a tournament is defined as the set of its distinct out-degrees. In 1978, Reid proposed the conjecture that for any set of nonnegative integers $D$, there exists a tournament $T$ with a degree set $D$. In 1989, Yao presented an arithmetical proof of the conjecture, but a general polynomial-time construction algorithm is not known. This paper proposes a necessary and sufficient condition and a separate necessary condition, based on the existing Landau's theorem for the problem of reconstructing score sequences from score sets of tournament graphs. The necessary condition introduces a structured set that enables the use of group-theoretic techniques, offering not only a framework for solving the reconstruction problem but also a new perspective for approaching similar problems. In particular, the same theoretical approach can be extended to reconstruct valid score sets given constraints on the frequency of distinct scores in tournaments. Based on these conditions, we have developed three algorithms that demonstrate the practical utility of our framework: a polynomial-time algorithm and a scalable algorithm for reconstructing score sequences, and a polynomial-time network-building method that finds all possible score sequences for a given score set. Moreover, the polynomial-time algorithm for reconstructing the score sequence of a tournament for a given score set can be used to verify Reid's conjecture. These algorithms have practical applications in sports analysis, ranking prediction, and machine learning tasks such as learning-to-rank models and data imputation, where the reconstruction of partial rankings or sequences is essential for recommendation systems and anomaly detection.

</details>


### [10] [Toward Optimal Approximations for Resource-Minimization for Fire Containment on Trees and Non-Uniform k-Center](https://arxiv.org/abs/2512.17049)
*Jannis Blauth,Christian Nöbel,Rico Zenklusen*

Main category: cs.DS

TL;DR: 该论文针对图上的火灾蔓延模型（RMFC）提出了最优2-近似算法和渐近PTAS，解决了文献中的开放问题，并将技术扩展到非均匀k中心问题（NUkC）。


<details>
  <summary>Details</summary>
Motivation: 火灾蔓延模型中，即使是在树上，基本的计算问题也是计算困难的。特别是资源最小化火灾遏制（RMFC）问题，现有研究在近似性方面存在显著差距，需要填补这一理论空白。

Method: 首先为RMFC的平滑变体设计PTAS，通过仔细的LP引导枚举程序实现。然后将这些新技术与额外要素结合，利用RMFC与NUkC之间的联系，扩展到非均匀k中心问题。

Result: 为RMFC提供了最优2-近似算法和渐近PTAS，解决了两个开放问题。同时为NUkC设计了首个在额外中心数量方面最优的近似算法。

Conclusion: 通过统一方法解决了RMFC的近似性差距，并将技术成功扩展到NUkC问题，为这两个计算困难问题提供了最优或接近最优的近似算法。

Abstract: One of the most elementary spreading models on graphs can be described by a fire spreading from a burning vertex in discrete time steps. At each step, all neighbors of burning vertices catch fire. A well-studied extension to model fire containment is to allow for fireproofing a number $B$ of non-burning vertices at each step. Interestingly, basic computational questions about this model are computationally hard even on trees. One of the most prominent such examples is Resource Minimization for Fire Containment (RMFC), which asks how small $B$ can be chosen so that a given subset of vertices will never catch fire. Despite recent progress on RMFC on trees, prior work left a significant gap in terms of its approximability. We close this gap by providing an optimal $2$-approximation and an asymptotic PTAS, resolving two open questions in the literature. Both results are obtained in a unified way, by first designing a PTAS for a smooth variant of RMFC, which is obtained through a careful LP-guided enumeration procedure.
  Moreover, we show that our new techniques, with several additional ingredients, carry over to the non-uniform $k$-center problem (NUkC), by exploiting a link between RMFC on trees and NUkC established by Chakrabarty, Goyal, and Krishnaswamy. This leads to the first approximation algorithm for NUkC that is optimal in terms of the number of additional centers that have to be opened.

</details>


### [11] [Optimal Verification of a Minimum-Weight Basis in an Uncertainty Matroid](https://arxiv.org/abs/2512.17116)
*Haya Diwan,Lisa Hellerstein,Nicole Megow,Jens Schlöter*

Main category: cs.DS

TL;DR: 提出了多项式时间算法验证带不确定性权重区域的拟阵最小权基，推广了先前仅处理开区间的工作，并应用于在线问题和学习增强变体


<details>
  <summary>Details</summary>
Motivation: 在可探索不确定性研究中，需要解决验证问题：给定所有查询答案，找到最小成本查询集以证明组合优化问题的最优解。先前工作仅处理开区间不确定性，需要扩展到更一般的不确定性区域（有限集、实区间、开闭区间并集）

Method: 开发多项式时间算法验证拟阵最小权基，其中每个权重位于给定不确定性区域内。引入新技术处理更一般的不确定性区域类型（有限集、实区间、开闭区间并集），超越了先前仅处理开区间的工作

Result: 算法成功验证了带不确定性权重区域的拟阵最小权基，并将验证问题的结构结果应用于相应自适应在线问题的承诺变体，给出了最优算法。算法还可应用于可探索不确定性下最小权基问题的两个学习增强变体

Conclusion: 提出了处理一般不确定性区域的拟阵最小权基验证算法，推广了先前工作，验证问题的结构洞察对在线问题和学习增强变体有重要影响，展示了算法的广泛应用性

Abstract: Research in explorable uncertainty addresses combinatorial optimization problems where there is partial information about the values of numeric input parameters, and exact values of these parameters can be determined by performing costly queries. The goal is to design an adaptive query strategy that minimizes the query cost incurred in computing an optimal solution. Solving such problems generally requires that we be able to solve the associated verification problem: given the answers to all queries in advance, find a minimum-cost set of queries that certifies an optimal solution to the combinatorial optimization problem. We present a polynomial-time algorithm for verifying a minimum-weight basis of a matroid, where each weight lies in a given uncertainty area. These areas may be finite sets, real intervals, or unions of open and closed intervals, strictly generalizing previous work by Erlebach and Hoffman which only handled the special case of open intervals. Our algorithm introduces new techniques to address the resulting challenges.
  Verification problems are of particular importance in the area of explorable uncertainty, as the structural insights and techniques used to solve the verification problem often heavily influence work on the corresponding online problem and its stochastic variant. In our case, we use structural results from the verification problem to give a best-possible algorithm for a promise variant of the corresponding adaptive online problem. Finally, we show that our algorithms can be applied to two learning-augmented variants of the minimum-weight basis problem under explorable uncertainty.

</details>


### [12] [LZ78 Substring Compression in Compressed Space](https://arxiv.org/abs/2512.17217)
*Hiroki Shibata,Dominik Köppl*

Main category: cs.DS

TL;DR: 研究在子串压缩模型中LZ78分解的索引问题，提出在压缩空间下工作的算法，相比最优时间复杂度仅有对数级减速


<details>
  <summary>Details</summary>
Motivation: LZ78分解是数据压缩中的重要技术，但现有研究主要集中在普通数据的分解上，对于如何索引数据以实现快速LZ78分解的研究较少。本文研究在子串压缩模型中的LZ78分解问题，允许索引数据并在查询时返回指定子串的分解结果。

Method: 在子串压缩模型中提出新算法，该算法在压缩空间下工作，能够计算任意子串的LZ78分解。算法设计实现了与最优时间复杂度相比仅有对数级减速的性能。

Result: 提出的算法能够在压缩空间下高效工作，计算LZ78分解的时间复杂度仅比最优时间多出对数因子，实现了在子串压缩模型中的高效索引和分解。

Conclusion: 该研究填补了LZ78分解在索引和子串压缩模型中的研究空白，提出的算法在保持压缩空间效率的同时，实现了接近最优的时间性能，为实际应用提供了有效的解决方案。

Abstract: The Lempel--Ziv 78 (LZ78) factorization is a well-studied technique for data compression. It and its derivatives are used in compression formats such as "compress" or "gif". Although most research focuses on the factorization of plain data, not much research has been conducted on indexing the data for fast LZ78 factorization. Here, we study the LZ78 factorization and its derivatives in the substring compression model, where we are allowed to index the data and return the factorization of a substring specified at query time. In that model, we propose an algorithm that works in compressed space, computing the factorization with a logarithmic slowdown compared to the optimal time complexity.

</details>


### [13] [Refining the Complexity Landscape of Speed Scaling: Hardness and Algorithms](https://arxiv.org/abs/2512.17663)
*Antonios Antoniadis,Denise Graafsma,Ruben Hoeksma,Maria Vlasiou*

Main category: cs.DS

TL;DR: 该论文解决了调度问题中四个重要变体的计算复杂性，证明了在特定条件下最小化总（加权）流时间加能耗的问题是NP难的，但在给定完成时间顺序时可在多项式时间内求解。


<details>
  <summary>Details</summary>
Motivation: 研究单速度可调处理器上作业调度的计算复杂性，旨在捕捉（加权）流时间与能耗之间的权衡关系。尽管文献中已有多种问题表述，但四个重要变体的计算复杂性一直未解决，被明确列为开放问题。

Method: 通过理论证明方法分析不同问题变体的计算复杂性。具体包括：证明在单位权重作业任意大小和任意权重作业单位大小两种情况下，最小化总（加权）流时间加能耗的问题是NP难的；同时证明当给定完成时间顺序时，相同问题变体可在多项式时间内求解。

Result: 1) 证明了最小化总（加权）流时间加能耗的问题在两种情况下是NP难的：单位权重作业任意大小，以及任意权重作业单位大小。2) 这些结果扩展到在能量预算约束下最小化总（加权）流时间的问题。3) 证明了当给定完成时间顺序时，相同问题变体可在多项式时间内求解。

Conclusion: 该研究最终解决了四个重要调度问题变体的计算复杂性，揭示了优先级顺序和完成时间顺序对问题复杂性的微妙差异，为速度可调处理器上的调度问题提供了完整的复杂性分类。

Abstract: We study the computational complexity of scheduling jobs on a single speed-scalable processor with the objective of capturing the trade-off between the (weighted) flow time and the energy consumption. This trade-off has been extensively explored in the literature through a number of problem formulations that differ in the specific job characteristics and the precise objective function. Nevertheless, the computational complexity of four important problem variants has remained unresolved and was explicitly identified as an open question in prior work. In this paper, we settle the complexity of these variants.
  More specifically, we prove that the problem of minimizing the objective of total (weighted) flow time plus energy is NP-hard for the cases of (i) unit-weight jobs with arbitrary sizes, and (ii)~arbitrary-weight jobs with unit sizes. These results extend to the objective of minimizing the total (weighted) flow time subject to an energy budget and hold even when the schedule is required to adhere to a given priority ordering.
  In contrast, we show that when a completion-time ordering is provided, the same problem variants become polynomial-time solvable. The latter result highlights the subtle differences between priority and completion orderings for the problem.

</details>


### [14] [Capacitated Partition Vertex Cover and Partition Edge Cover](https://arxiv.org/abs/2512.17844)
*Rajni Dabas,Samir Khuller,Emilie Rivkin*

Main category: cs.DS

TL;DR: 本文研究了超图中的容量划分顶点覆盖问题及其变体，提出了近似算法，并改进了加权划分边覆盖问题的精确多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究超图中的容量划分顶点覆盖问题，该问题推广了经典的顶点覆盖、部分顶点覆盖和划分顶点覆盖问题，具有实际应用价值。

Method: 针对软容量和硬容量两种变体，分别设计了近似算法：对于加权软容量C-PVC问题，提出(f+1)-近似算法；对于无权硬容量C-PVC问题，提出(f+ε)-近似算法。同时改进了加权划分边覆盖问题的精确算法。

Result: 获得了两个主要结果：1) 加权软容量C-PVC问题的(f+1)-近似算法，时间复杂度为n^{O(ω)}；2) 无权硬容量C-PVC问题的(f+ε)-近似算法，时间复杂度为n^{O(ω/ε)}。此外，将加权划分边覆盖问题的算法复杂度从O(ωn^3)改进到O(mn+n^2 log n)。

Conclusion: 本文为超图中的容量划分顶点覆盖问题提供了有效的近似算法，并显著改进了加权划分边覆盖问题的精确算法，在理论和算法设计上都有重要贡献。

Abstract: Our first focus is the Capacitated Partition Vertex Cover (C-PVC) problem in hypergraphs. In C-PVC, we are given a hypergraph with capacities on its vertices and a partition of the hyperedge set into $ω$ distinct groups. The objective is to select a minimum size subset of vertices that satisfies two main conditions: (1) in each group, the total number of covered hyperedges meets a specified threshold, and (2) the number of hyperedges assigned to any vertex respects its capacity constraint. A covered hyperedge is required to be assigned to a selected vertex that belongs to the hyperedge. This formulation generalizes classical Vertex Cover, Partial Vertex Cover, and Partition Vertex Cover.
  We investigate two primary variants: soft capacitated (multiple copies of a vertex are allowed) and hard capacitated (each vertex can be chosen at most once). Let $f$ denote the rank of the hypergraph. Our main contributions are: $(i)$ an $(f+1)$-approximation algorithm for the weighted soft-capacitated C-PVC problem, which runs in $n^{O(ω)}$ time, and $(ii)$ an $(f+ε)$-approximation algorithm for the unweighted hard-capacitated C-PVC problem, which runs in $n^{O(ω/ε)}$ time.
  We also study a natural generalization of the edge cover problem, the \emph{Weighted Partition Edge Cover} (W-PEC) problem, where each edge has an associated weight, and the vertex set is partitioned into groups. For each group, the goal is to cover at least a specified number of vertices using incident edges, while minimizing the total weight of the selected edges. We present the first exact polynomial-time algorithm for the weighted case, improving runtime from $O(ωn^3)$ to $O(mn+n^2 \log n)$ and simplifying the algorithmic structure over prior unweighted approaches.

</details>


### [15] [Prefix Trees Improve Memory Consumption in Large-Scale Continuous-Time Stochastic Models](https://arxiv.org/abs/2512.17892)
*Landon Taylor,Joshua Jeppson,Ahmed Irfan,Lukas Buecherl,Chris Myers,Zhen Zhang*

Main category: cs.DS

TL;DR: 该论文提出使用前缀树替代哈希表来存储化学反应网络等大型并发系统的状态，以解决内存限制问题，并通过BMC预处理优化变量排序来进一步提升内存效率。


<details>
  <summary>Details</summary>
Motivation: 化学反应网络等高度并发系统具有巨大的状态空间，现有基于哈希表的方法虽然平均时间复杂度恒定，但面临严重的内存限制问题，无法处理超大规模状态空间。

Method: 1. 使用前缀树存储状态而非哈希表，以减少内存占用；2. 采用有界模型检查预处理步骤来优化变量排序，进一步改善内存使用效率。

Result: 理论分析和基准测试表明，对于超大规模状态空间，前缀树比哈希表更具优势；初步评估显示BMC预处理能有效改善内存使用。

Conclusion: 前缀树存储方法能有效解决大型并发系统状态空间的内存限制问题，该方法虽然主要针对化学反应网络，但可推广到所有连续时间马尔可夫链模型。

Abstract: Highly-concurrent system models with vast state spaces like Chemical Reaction Networks (CRNs) that model biological and chemical systems pose a formidable challenge to cutting-edge formal analysis tools. Although many symbolic approaches have been presented, transient probability analysis of CRNs, modeled as Continuous-Time Markov Chains (CTMCs), requires explicit state representation. For that purpose, current cutting-edge methods use hash maps, which boast constant average time complexity and linear memory complexity. However, hash maps often suffer from severe memory limitations on models with immense state spaces. To address this, we propose using prefix trees to store states for large, highly concurrent models (particularly CRNs) for memory savings. We present theoretical analyses and benchmarks demonstrating the favorability of prefix trees over hash maps for very large state spaces. Additionally, we propose using a Bounded Model Checking (BMC) pre-processing step to impose a variable ordering to further improve memory usage along with preliminary evaluations suggesting its effectiveness. We remark that while our work is motivated primarily by the challenges posed by CRNs, it is generalizable to all CTMC models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization](https://arxiv.org/abs/2512.16956)
*Shravan Chaudhari,Rahul Thomas Jacob,Mononito Goswami,Jiajun Cao,Shihab Rashid,Christian Bock*

Main category: cs.SE

TL;DR: SpIDER是一种增强的密集检索方法，通过图探索和LLM推理来改进代码单元的语义检索性能


<details>
  <summary>Details</summary>
Motivation: 现有密集嵌入方法虽然优于BM25，但缺乏对代码库的探索，未能充分利用代码的图结构信息

Method: 提出SpIDER方法，结合图探索获取辅助上下文，并通过LLM推理增强密集嵌入检索

Result: SpIDER在多种编程语言上持续改进密集检索性能

Conclusion: 结合图探索和LLM推理的密集检索方法能有效提升代码单元检索效果

Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.

</details>


### [17] [Resilient Microservices: A Systematic Review of Recovery Patterns, Strategies, and Evaluation Frameworks](https://arxiv.org/abs/2512.16959)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 对2014-2025年间微服务恢复策略的PRISMA系统文献综述，识别了9个弹性主题，提出了恢复模式分类法、弹性评估评分清单和约束感知决策矩阵


<details>
  <summary>Details</summary>
Motivation: 微服务系统在现代分布式计算环境中至关重要，但仍易受部分故障、级联超时和不一致恢复行为的影响。现有关于弹性和恢复模式的调查大多是描述性的，缺乏系统证据综合或定量严谨性

Method: 采用PRISMA标准的系统文献综述方法，从IEEE Xplore、ACM Digital Library和Scopus数据库中筛选2014-2025年的实证研究。从412条初始记录中，通过透明的纳入、排除和质量评估标准，最终选择了26项高质量研究

Result: 识别了9个重复出现的弹性主题：断路器、带抖动和预算的重试、带补偿的sagas、幂等性、舱壁隔离、自适应背压、可观测性、混沌验证。提出了恢复模式分类法、弹性评估评分清单和约束感知决策矩阵

Conclusion: 将碎片化的弹性研究整合为结构化、可分析的证据基础，支持可重复评估和知情设计，有助于构建容错且性能感知的微服务系统

Abstract: Microservice based systems underpin modern distributed computing environments but remain vulnerable to partial failures, cascading timeouts, and inconsistent recovery behavior. Although numerous resilience and recovery patterns have been proposed, existing surveys are largely descriptive and lack systematic evidence synthesis or quantitative rigor. This paper presents a PRISMA aligned systematic literature review of empirical studies on microservice recovery strategies published between 2014 and 2025 across IEEE Xplore, ACM Digital Library, and Scopus. From an initial corpus of 412 records, 26 high quality studies were selected using transparent inclusion, exclusion, and quality assessment criteria. The review identifies nine recurring resilience themes encompassing circuit breakers, retries with jitter and budgets, sagas with compensation, idempotency, bulkheads, adaptive backpressure, observability, and chaos validation. As a data oriented contribution, the paper introduces a Recovery Pattern Taxonomy, a Resilience Evaluation Score checklist for standardized benchmarking, and a constraint aware decision matrix mapping latency, consistency, and cost trade offs to appropriate recovery mechanisms. The results consolidate fragmented resilience research into a structured and analyzable evidence base that supports reproducible evaluation and informed design of fault tolerant and performance aware microservice systems.

</details>


### [18] [Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences](https://arxiv.org/abs/2512.17280)
*Christof Lorenza,Nils Brinckmann,Jan Bumberger,Marc Hanisch,Tobias Kuhnert,Ulrich Loup,Rubankumar Moorthy,Florian Obsersteiner,David Schäfer,Thomas Schnicke*

Main category: cs.SE

TL;DR: 开发传感器管理系统（SMS）来管理复杂传感器系统的全生命周期元数据，提供用户友好平台，支持FAIR原则的数据管理。


<details>
  <summary>Details</summary>
Motivation: 环境观测数据需要一致且全面的元数据（包括时间分辨的上下文信息如部署变化、配置和维护操作）来获得可靠结论和洞察。

Method: 开发传感器管理系统（SMS），使用明确定义的术语（设备、平台、配置、站点等）描述传感器实体，增强属性信息，并记录系统相关操作的连续历史。

Result: SMS提供用户友好且功能丰富的平台，能够建模复杂传感器系统，管理全生命周期信息，并链接到后续系统和服务（如PID注册、受控词汇表），建立终端用户社区。

Conclusion: SMS作为数字生态系统的核心元素，促进传感器相关元数据更一致、可持续和符合FAIR原则的提供。

Abstract: Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.

</details>


### [19] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: Req2LTL是一个将自然语言软件需求自动转换为LTL形式化规范的模块化框架，通过分层中间表示OnionL结合LLM语义分解和确定性规则合成，在航空航天需求上达到88.4%语义准确率和100%语法正确率。


<details>
  <summary>Details</summary>
Motivation: 将自然语言软件需求自动转换为形式化规范是扩展形式化验证到工业环境的关键挑战。现有方法（基于规则和基于学习）在处理真实工业需求的复杂性、模糊性和逻辑深度方面存在显著限制，LLM虽然擅长语义提取但仍面临困难。

Method: 提出Req2LTL框架，通过分层中间表示OnionL桥接自然语言和线性时序逻辑。利用LLM进行语义分解，并结合确定性规则合成方法，确保语法有效性和语义保真度。

Result: 在真实航空航天需求上的综合评估显示，Req2LTL达到88.4%语义准确率和100%语法正确率，显著优于现有方法。

Conclusion: Req2LTL通过结合LLM语义能力和确定性规则合成，有效解决了自然语言需求到形式化规范的转换问题，为工业环境中的形式化验证提供了实用解决方案。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [20] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 首次对真实世界可信执行环境(TEE)应用的大规模实证研究，分析了241个使用Intel SGX和ARM TrustZone的开源项目，揭示了开发者的实际使用模式、安全实践和SDK可用性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管TEEs（如Intel SGX和ARM TrustZone）在安全计算领域越来越重要，但开发者如何在实践中使用这些技术尚不清楚。本研究旨在填补这一空白，通过分析真实项目来了解TEE的实际应用情况。

Method: 收集并分析了GitHub上241个使用Intel SGX和ARM TrustZone的开源项目。采用手动检查与定制静态分析脚本相结合的方法，从三个维度进行研究：1) 应用领域分类和时间趋势分析；2) TEE集成方式分析；3) 安全实践检查。

Result: 研究发现：1) 主要应用领域是物联网设备安全(30%)，与学术界关注的区块链和密码系统(7%)形成鲜明对比，AI模型保护(12%)是新兴领域；2) 32.4%的项目重新实现密码功能而非使用官方SDK API，表明SDK可用性和可移植性不足；3) 25.3%的项目存在不安全编码行为（如硬编码密钥、缺少输入验证），削弱了安全保证。

Conclusion: 研究揭示了TEE实际应用与学术关注点之间的差距，指出了当前SDK在可用性方面的不足，以及开发者安全实践的缺陷。这些发现对改进TEE SDK可用性和支持可信软件开发具有重要意义。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [21] [GraphCue for SDN Configuration Code Synthesis](https://arxiv.org/abs/2512.17371)
*Haomin Qi,Fengfei Yu,Chengbo Huang*

Main category: cs.SE

TL;DR: GraphCue：基于拓扑感知检索和代理在环的SDN自动配置框架，通过图神经网络嵌入网络拓扑，结合结构化提示和验证循环，在628个验证案例中达到88.2%通过率


<details>
  <summary>Details</summary>
Motivation: SDN（软件定义网络）配置复杂且容易出错，需要自动化解决方案来减少人工配置负担并提高配置准确性

Method: 1. 将每个配置案例抽象为JSON图，使用三层GCN进行图嵌入；2. 通过对比学习训练轻量级图神经网络；3. 检索最相似的已验证参考配置；4. 使用结构化提示约束代码生成；5. 通过验证器执行候选配置并反馈失败信息给代理形成闭环

Result: 在628个验证案例中，GraphCue在20次迭代内达到88.2%的通过率，95%的验证循环在9秒内完成。消融实验显示，没有检索或结构化提示时性能显著下降

Conclusion: 拓扑感知检索和基于约束的条件生成是GraphCue性能的关键驱动因素，该框架为SDN自动化配置提供了有效的解决方案

Abstract: We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.

</details>


### [22] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 论文提出了一个包含1000个Python任务的基准测试，每个任务平均有7个开发者指定的约束条件，用于评估LLM在代码生成中遵循约束的能力，而不仅仅是功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在真实世界代码生成中，仅关注功能正确性是不够的，开发者还需要模型遵循鲁棒性、格式化和安全性等约束。现有基准主要通过测试用例执行评估正确性，对模型遵循约束的可靠性评估有限。

Method: 通过四阶段人机协作流程构建包含1000个Python任务的基准，每个任务平均有7个开发者指定的约束，涵盖13个类别。约束经过精心设计，确保原子性、相关性和客观性。评估14个开源和闭源模型，使用互补的遵循度指标，并提出C2A Score综合衡量正确性和约束遵循。

Result: 结果显示部分遵循和严格遵循之间存在显著差距：强模型在部分遵循上超过90%，但严格遵循率仅为39-66%。这表明可信的代码生成不仅需要正确性，还需要对开发者意图的一致遵循。

Conclusion: 可信的代码生成需要模型不仅生成功能正确的代码，还要能一致地遵循开发者的约束和意图。当前的LLM在严格遵循约束方面仍有很大改进空间。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [23] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 该论文系统综述了2015-2025年间42项关于Apache Kafka的研究，识别出9种常用设计模式，分析了评测方法的不一致性，并提出了统一分类法和决策启发式。


<details>
  <summary>Details</summary>
Motivation: 尽管Apache Kafka已成为高吞吐量事件流处理的基础平台，但关于其可重用架构设计模式和可复现评测方法的研究在学术界和工业界中仍然分散，缺乏系统性整合。

Method: 对2015-2025年间42项同行评审研究进行结构化综合，识别重复出现的Kafka设计模式，分析协同使用趋势、领域特定部署和实证评测实践。

Result: 识别出9种重复出现的Kafka设计模式：日志压缩、CQRS总线、精确一次管道、变更数据捕获、流表连接、Saga编排、分层存储、多租户主题和事件溯源重放。研究发现配置披露、评测严谨性和可复现性方面存在显著不一致。

Conclusion: 通过提供统一分类法、模式基准矩阵和可操作的决策启发式，为架构师和研究人员设计可复现、高性能、容错的Kafka事件流系统提供实用指导。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [24] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++是一个自动化框架，从GitHub拉取请求生成仓库级编码任务，覆盖11种语言，提供可扩展的多语言基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试如SWE-bench存在局限性：手动策划、静态数据集、仅关注Python错误修复。需要自动化、多语言、覆盖更广任务类型的基准测试。

Method: 通过四个阶段自动化生成任务：程序化获取、环境合成、测试预言提取、质量保证。最后通过提示引导的轨迹合成将失败实例转化为训练轨迹。

Result: 初始基准包含11,133个实例，来自3,971个仓库和11种语言。在1,782个实例子集上，Claude Sonnet 4.5达到36.20% pass@10，GPT-5 34.57%，Gemini 2.5 Pro 24.92%，GPT-4o 16.89%。微调SWE-Bench++实例在SWE-bench多语言基准上显示可测量的改进。

Conclusion: SWE-Bench++为评估和改进仓库级代码生成提供了一个可扩展、多语言的基准测试框架，解决了现有基准的局限性。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [25] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 该研究探讨了大型语言模型在软件工程调查中的滥用问题，分析了AI生成响应如何威胁调查数据的真实性、有效性和研究完整性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，调查参与者可能使用生成工具伪造或操纵回答，这对软件工程调查的数据真实性、有效性和研究完整性构成了新威胁。

Method: 通过Prolific平台收集2025年两次调查部署的数据，分析参与者回答内容以识别异常或伪造响应。对疑似AI生成的响应子集进行定性模式检查、叙事特征分析和使用Scribbr AI检测器的自动检测。

Result: 在49个调查响应中发现了表明合成作者身份的重复结构模式，包括重复序列、统一措辞和表面个性化。这些虚假叙事模仿连贯推理同时隐藏伪造内容，破坏了构念、内部和外部有效性。

Conclusion: 数据真实性已成为软件工程调查中有效性的新兴维度。可靠的证据现在需要结合自动和解释性验证程序、透明报告和社区标准来检测和防止AI生成响应，从而保护软件工程调查的可信度。

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [26] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 首次大规模实证分析软件缺陷预测中五种数据质量问题（类别不平衡、类别重叠、无关特征、属性噪声、异常值）的共现现象及其对模型性能的影响，揭示了普遍共现模式、危害阈值和反直觉效应。


<details>
  <summary>Details</summary>
Motivation: 现有SDP研究通常孤立地分析单一数据质量问题，而现实世界中这些问题经常共现并相互作用。缺乏对这些共现问题的系统性理解，限制了SDP模型在实际应用中的有效性。

Method: 使用可解释提升机（EBM）和分层交互分析，在374个数据集和5个分类器上同时分析五种数据质量问题的直接效应和条件效应，采用默认超参数设置反映实际基线使用情况。

Result: 共现现象几乎普遍存在（即使最少见的属性噪声也在93%以上数据集中与其他问题共现）；类别重叠是最持续有害的问题；确定了类别重叠（0.20）、不平衡（0.65-0.70）、无关特征（0.94）的临界阈值；发现了反直觉模式（如低无关特征时异常值可提升性能）；揭示了性能-鲁棒性权衡。

Conclusion: 研究填补了SDP领域对数据质量问题共现现象理解的空白，强调需要超越孤立分析，提供对现实环境中数据质量问题如何影响模型性能的整体、数据感知的理解。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [27] [Why Is My Transaction Risky? Understanding Smart Contract Semantics and Interactions in the NFT Ecosystem](https://arxiv.org/abs/2512.17500)
*Yujing Chen,Xuanming Liu,Zhiyuan Wan,Zuobin Wang,David Lo,Difan Xie,Xiaohu Yang*

Main category: cs.SE

TL;DR: 该研究对NFT生态系统中的智能合约语义和交互进行了大规模实证分析，发现智能合约语义多样性有限，市场合约和代理注册合约在交易中交互最频繁，诈骗代币在字节码层面呈现收敛特征，并识别了与风险交易相关的特定交互模式。


<details>
  <summary>Details</summary>
Motivation: NFT生态系统存在诈骗代币等安全风险，先前研究从安全挑战、参与者行为等角度探索了NFT生态，但缺乏对智能合约在交易过程中的语义和交互的理解，以及诈骗代币风险如何通过合约语义和交互表现出来。本研究旨在填补这一空白。

Method: 对以太坊上近1亿笔交易（跨越2000万个区块）的精选数据集进行大规模实证研究，分析NFT生态系统中智能合约的语义和交互模式。

Result: 1. NFT生态系统中智能合约语义多样性有限，主要由代理合约、代币合约和DeFi合约主导；2. 市场合约和代理注册合约在交易中最频繁参与智能合约交互；3. 代币合约在字节码层面具有多样性，而诈骗代币在字节码层面呈现收敛特征；4. 某些智能合约交互模式在风险和非风险交易中都常见，而其他模式主要与风险交易相关。

Conclusion: 基于研究发现，为缓解区块链生态系统中的风险提供了建议，并概述了未来的研究方向。研究揭示了NFT生态系统中智能合约交互的语义特征和风险模式。

Abstract: The NFT ecosystem represents an interconnected, decentralized environment that encompasses the creation, distribution, and trading of Non-Fungible Tokens (NFTs), where key actors, such as marketplaces, sellers, and buyers, utilize smart contracts to facilitate secure, transparent, and trustless transactions. Scam tokens are deliberately created to mislead users and facilitate financial exploitation, posing significant risks in the NFT ecosystem. Prior work has explored the NFT ecosystem from various perspectives, including security challenges, actor behaviors, and risks from scams and wash trading, leaving a gap in understanding the semantics and interactions of smart contracts during transactions, and how the risks associated with scam tokens manifest in relation to the semantics and interactions of contracts. To bridge this gap, we conducted a large-scale empirical study on smart contract semantics and interactions in the NFT ecosystem, using a curated dataset of nearly 100 million transactions across 20 million blocks on Ethereum. We observe a limited semantic diversity among smart contracts in the NFT ecosystem, dominated by proxy, token, and DeFi contracts. Marketplace and proxy registry contracts are the most frequently involved in smart contract interactions during transactions, engaging with a broad spectrum of contracts in the ecosystem. Token contracts exhibit bytecode-level diversity, whereas scam tokens exhibit bytecode convergence. Certain interaction patterns between smart contracts are common to both risky and non-risky transactions, while others are predominantly associated with risky transactions. Based on our findings, we provide recommendations to mitigate risks in the blockchain ecosystem, and outline future research directions.

</details>


### [28] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: SGCR框架通过将LLM基于人工编写的规范，使用显式和隐式双路径架构，在代码审查中实现可靠、相关的反馈，工业部署中开发者采纳率达42%，相比基线LLM提升90.9%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在自动化代码审查中存在可靠性不足、缺乏上下文感知和控制能力的问题，阻碍了实际应用。需要一种方法将LLM的生成能力与软件工程对可靠性的严格要求相结合。

Method: 提出规范驱动的代码审查（SGCR）框架，采用双路径架构：显式路径确保确定性遵守从规范派生的预定义规则；隐式路径启发式地发现和验证超出这些规则的问题。

Result: 在HiThink Research的工业环境中部署，SGCR的建议达到了42%的开发者采纳率，相比基线LLM的22%有90.9%的相对提升。

Conclusion: 规范驱动是一种强大的范式，能够弥合LLM的生成能力与软件工程对严格可靠性需求之间的差距。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [29] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 提出SVS-TEST方法评估SBOM漏洞扫描工具的可靠性和成熟度，发现现有工具存在静默失败和误报问题


<details>
  <summary>Details</summary>
Motivation: SBOM漏洞扫描工具在现实应用中存在不一致和意外行为，导致误报和静默失败，需要系统评估方法

Method: 开发SVS-TEST方法和工具，使用16个精心设计的SBOM及其真实基准，评估7个真实世界SVS工具的能力、成熟度和失败条件

Result: 发现SVS工具在可靠性和错误处理方面存在显著差异，多个工具在有效输入SBOM上静默失败，造成虚假安全感

Conclusion: SVS-TEST可帮助组织和工具开发者监控SVS能力成熟度，所有结果已公开并提前披露给工具开发者

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


### [30] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: 本文探讨利用大语言模型自动化从文本规范生成硬件设计的行为场景，以支持硬件设计中的行为驱动开发


<details>
  <summary>Details</summary>
Motivation: 硬件和系统设计中的测试验证复杂度随系统规模增长而显著增加。行为驱动开发在软件工程中已被证明有效，但在硬件设计中尚未广泛应用，主要障碍是需要从文本规范手动推导精确的行为场景。

Method: 利用大语言模型技术，研究基于LLM的方法来自动化从文本规范生成行为场景，以支持硬件设计中的行为驱动开发。

Result: 未在摘要中明确说明具体实验结果，但提出了LLM技术为自动化行为场景生成提供了新的机会。

Conclusion: 大语言模型的进展为自动化硬件设计中的行为驱动开发提供了新的可能性，有望减少手动工作量并提高效率。

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [31] [Enhancing AIGC Service Efficiency with Adaptive Multi-Edge Collaboration in A Distributed System](https://arxiv.org/abs/2512.17158)
*Changfu Xu,Jianxiong Guo,Jiandian Zeng,Houming Qiu,Tian Wang,Xiaowen Chu,Jiannong Cao*

Main category: cs.NI

TL;DR: 提出AMCoEdge方法，通过自适应多服务器协作移动边缘计算来提升AIGC服务效率，减少处理延迟和失败率。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC服务通常采用集中式框架，导致高响应时间；当前协作MEC方法主要支持单服务器卸载或固定边缘服务器交互，限制了灵活性和资源利用率，无法满足AIGC服务变化的计算和网络需求。

Method: 提出AMCoEdge方法，通过自适应多边缘服务器选择和动态工作负载分配，充分利用所有边缘服务器的计算和网络资源；设计基于深度强化学习的在线分布式算法，具有近似线性时间复杂度。

Result: 仿真结果显示，相比现有方法至少减少11.04%的任务卸载完成时间和44.86%的失败率；原型系统实现显示，服务延迟比三种代表性方法降低9.23%-31.98%。

Conclusion: AMCoEdge方法通过自适应多服务器协作MEC有效提升了AIGC服务效率，减少了处理延迟，具有实际应用价值。

Abstract: The Artificial Intelligence Generated Content (AIGC) technique has gained significant traction for producing diverse content. However, existing AIGC services typically operate within a centralized framework, resulting in high response times. To address this issue, we integrate collaborative Mobile Edge Computing (MEC) technology to reduce processing delays for AIGC services. Current collaborative MEC methods primarily support single-server offloading or facilitate interactions among fixed Edge Servers (ESs), limiting flexibility and resource utilization across all ESs to meet the varying computing and networking requirements of AIGC services. We propose AMCoEdge, an adaptive multi-server collaborative MEC approach to enhancing AIGC service efficiency. The AMCoEdge fully utilizes the computing and networking resources across all ESs through adaptive multi-ES selection and dynamic workload allocation, thereby minimizing the offloading make-span of AIGC services. Our design features an online distributed algorithm based on deep reinforcement learning, accompanied by theoretical analyses that confirm an approximate linear time complexity. Simulation results show that our method outperforms state-of-the-art baselines, achieving at least an 11.04% reduction in task offloading make-span and a 44.86% decrease in failure rate. Additionally, we develop a distributed prototype system to implement and evaluate our AMCoEdge method for real AIGC service execution, demonstrating service delays that are 9.23% - 31.98% lower than the three representative methods.

</details>


### [32] [Timely Information Updating for Mobile Devices Without and With ML Advice](https://arxiv.org/abs/2512.17381)
*Yu-Pin Hsu,Yi-Hsuan Tseng*

Main category: cs.NI

TL;DR: 该论文研究移动设备监控物理过程并向接入点发送状态更新的信息更新系统，提出在线算法解决信息及时性与更新成本之间的权衡，并引入机器学习建议开发增强算法。


<details>
  <summary>Details</summary>
Motivation: 移动设备监控物理过程并向接入点发送状态更新时，存在信息及时性与更新成本之间的基本权衡。需要设计在线算法来处理操作时长、信息陈旧度、更新成本和更新机会可用性等多种不确定性。

Method: 提出在线算法决定何时传输更新，仅使用可用观测。算法能对抗同时操纵多种不确定性源的对手。进一步引入可靠性未知的机器学习建议，开发ML增强算法，即使对手能破坏ML建议也能达到最优一致性-鲁棒性权衡。

Result: 算法渐近达到最优竞争比，最优竞争比与更新成本范围呈线性关系，但不受其他不确定性影响。最优竞争在线算法对ML建议呈现阈值式响应：要么完全信任，要么完全忽略，部分信任无法在不大幅降低鲁棒性的情况下改善一致性。随机环境中的仿真验证了理论发现。

Conclusion: 该研究为信息更新系统提供了对抗多种不确定性的在线算法框架，并展示了机器学习建议在对抗环境中的最优使用策略，为实际系统设计提供了理论指导。

Abstract: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.

</details>


### [33] [Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility](https://arxiv.org/abs/2512.17538)
*Zibin Lin,Shengli Zhang,Guofu Liao,Dacheng Tao,Taotao Wang*

Main category: cs.NI

TL;DR: BAID提出一个结合生物识别、区块链和零知识证明的身份基础设施，为AI代理建立可验证的用户-代码绑定，解决自主AI系统的可追溯问责问题。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理缺乏可追溯的问责机制，导致系统要么降级为工具使用，要么面临现实世界滥用的风险。传统基于密钥的身份验证无法保证操作者的物理身份和代理代码的完整性。

Method: BAID整合三种正交机制：1) 通过生物识别的本地绑定；2) 去中心化的链上身份管理；3) 基于zkVM的代码级认证协议，利用递归证明将程序二进制作为身份标识。

Result: 实现了完整的原型系统，证明了基于区块链的身份管理和基于zkVM的认证协议的实际可行性，为操作者身份、代理配置完整性和完整执行来源提供了密码学保证。

Conclusion: BAID通过建立可验证的用户-代码绑定，有效防止未经授权的操作和代码替换，为自主AI系统提供了可追溯的问责机制。

Abstract: Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID integrates three orthogonal mechanisms: local binding via biometric authentication, decentralized on-chain identity management, and a novel zkVM-based Code-Level Authentication protocol. By leveraging recursive proofs to treat the program binary as the identity, this protocol provides cryptographic guarantees for operator identity, agent configuration integrity, and complete execution provenance, thereby effectively preventing unauthorized operation and code substitution. We implement and evaluate a complete prototype system, demonstrating the practical feasibility of blockchain-based identity management and zkVM-based authentication protocol.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: 提出Dion2方法简化Muon优化器的正交化步骤，通过采样部分行或列来减少计算和通信成本，提高可扩展性


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然具有强大的经验性能和理论基础，但其正交化步骤的超线性成本随着规模增加而带来越来越大的开销。已有工作尝试减少进入正交化步骤的矩阵大小，但需要更简单的方法

Method: 提出Dion2方法，在每次迭代中采样一部分行或列，只对这些采样部分进行正交化。这种采样过程使更新变得稀疏，减少了计算和通信成本

Result: Dion2相比先前方法更简单，能有效减少Muon优化器的计算和通信开销，提高了算法的可扩展性

Conclusion: Dion2提供了一种简单有效的方法来缓解Muon优化器的正交化开销问题，通过稀疏采样策略改善了算法在大规模场景下的可扩展性

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [35] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 开发低成本双模式神经肌肉控制系统，结合EEG和EMG信号实现假肢手臂的多自由度实时控制，总成本约240美元。


<details>
  <summary>Details</summary>
Motivation: 传统低成本上肢假肢缺乏直观控制系统，限制了截肢者在资源匮乏环境下的功能性和可及性。

Method: 集成NeuroSky MindWave Mobile 2采集EEG信号和MyoWare 2.0采集EMG信号，通过两个ESP32微控制器分别处理。EEG使用轻量级分类模型检测眨眼事件控制手部开合，EMG采用阈值检测实现肘部三状态控制。

Result: 成功构建功能原型，EEG控制四个手指舵机，EMG控制两个肘部舵机，实现实时多自由度控制，总成本约240美元。

Conclusion: 该系统展示了低成本、生物直观的假肢控制可行路径，适用于资源匮乏地区和全球健康应用，未来可改进3D打印外壳、降低延迟和提升扭矩。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [36] [QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification](https://arxiv.org/abs/2512.16960)
*Bikash K. Behera,Giuseppe Sergioli,Robert Giuntini*

Main category: cs.LG

TL;DR: 量子启发机器学习中Kernel Trick与Pretty Good Measurement方法的统一比较，实验显示PGM和KPGM分类器在量子SMOTE场景下优于经典随机森林，PGM stereo编码表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量子启发机器学习利用量子理论数学框架增强经典算法，但不同方法（核技巧与PGM）缺乏统一比较。本文旨在理论结合实验对比这些范式，分析它们在量子SMOTE场景下的性能差异。

Method: 提出统一理论框架比较核技巧与PGM方法，包括核化PGM和直接PGM分类器。使用量子SMOTE变体生成合成过采样数据，实验评估不同编码方式（stereo/amplitude）和量子副本数对性能的影响。

Result: PGM和KPGM分类器均优于经典随机森林基线。PGM stereo编码（n_copies=2）获得最高准确率0.8512和F1分数0.8234。KPGM表现更稳定，在stereo和amplitude编码下分别达到0.8511和0.8483。量子副本数增加提升性能。

Conclusion: 量子启发分类器在召回率和平衡性能上有实际优势：PGM受益于编码特定增强，KPGM在采样策略间更稳健。研究为不同数据特性和计算约束下的方法选择提供实用指导。

Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.

</details>


### [37] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 提出"压缩即路由"新架构理念，通过Transformer自编码器实现64倍序列压缩，利用重构误差作为内在分布指纹来自动调度专家模块，无需显式门控网络。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型的三个主要挑战：上下文长度限制、高推理成本、持续学习中的灾难性遗忘。现有MoE架构依赖显式训练的路由分类器，增加了系统复杂性且在处理混合域输入时缺乏可解释性。

Method: 提出"压缩即路由"架构理念，训练87M参数的端到端Transformer自编码器，实现512个token压缩为8个潜在向量的64倍序列长度压缩。利用重构误差作为内在分布指纹来自动调度专家模块。

Result: 压缩器表现出极端的领域判别能力：在域内（代码）验证集上重构准确率达99.47%；在半分布外领域（Wiki文本）降至47.76%；在完全分布外领域（随机序列）仅0.57%。这种系统性性能差异验证了重构误差作为内在分布指纹的有效性。

Conclusion: 重构误差可作为内在分布指纹，使专家模块能基于重构残差自动调度而无需显式门控网络。该架构为处理超长上下文提供了新的"VRAM压缩"视角，为下一代可扩展模块化神经网络提供了新的研究方向。

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [38] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 基于XGBoost的轻量级梯度提升框架，利用METAR地面观测数据和物理引导特征工程，实现低能见度和降水事件的短期预报，在11个国际机场的盲测中比传统TAF预报性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前航空天气预报依赖计算密集的数值天气预报和人工发布的TAF产品，存在保守偏差和时间分辨率有限的问题，需要更高效、准确的短期预报方法保障航空安全和运行效率。

Method: 采用XGBoost梯度提升框架，仅使用METAR地面观测数据，通过基于热力学原理的物理引导特征工程增强特征，在11个不同气候区域的国际机场（2000-2024年数据）进行训练和评估。

Result: 在3小时战术预报盲测中，自动化模型相比传统TAF预报显著提高了检测率，召回率提升2.5-4.0倍，同时减少了误报。SHAP分析显示模型能够隐式重建局地物理驱动因素。

Conclusion: 该轻量级物理引导机器学习框架能够有效捕捉局地物理过程，无需人工配置，在短期航空天气预报中表现出色，为运行态势感知提供了可解释的自动化解决方案。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [39] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 论文提出turn-PPO方法，针对多轮任务中的强化学习优化，相比传统token级MDP，采用轮次级MDP公式，在多轮交互任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法在多轮长视野推理任务中存在局限性，需要更稳定有效的优势估计策略来提升LLM智能体在真实环境中的交互训练效果。

Method: 首先探索PPO作为替代方案，发现比GRPO更稳健。然后提出turn-PPO变体，采用轮次级MDP公式而非传统的token级MDP，专门针对多轮场景优化。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO无论是否包含长推理组件都表现出有效性，优于传统方法。

Conclusion: turn-PPO为多轮交互任务中的强化学习提供了更有效的优化方法，通过轮次级MDP公式解决了长视野推理的挑战。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [40] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: GB-DQN使用梯度提升集成方法解决深度强化学习中的非平稳环境问题，通过增量残差学习适应环境变化，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境（动态或奖励变化）会破坏已学习的价值函数，导致灾难性遗忘，这是深度强化学习面临的根本挑战。

Method: 提出梯度提升深度Q网络（GB-DQN），采用自适应集成方法，通过增量残差学习处理模型漂移。不是重新训练单个Q网络，而是构建加法集成，每个新学习器训练来近似当前集成在漂移后的Bellman残差。

Result: 理论证明每个提升步骤减少经验Bellman残差，集成在标准假设下收敛到漂移后最优价值函数。实验表明在多种控制任务中，相比DQN和常见非平稳基线方法，具有更快的恢复速度、更好的稳定性和更强的鲁棒性。

Conclusion: GB-DQN通过梯度提升集成有效应对非平稳环境挑战，提供理论保证和实证优势，是处理深度强化学习中环境变化的有效方法。

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [41] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 提出SFBD-OMNI框架，利用大量噪声样本和少量干净样本恢复真实分布，通过单边熵最优传输和EM算法解决任意测量模型下的分布恢复问题


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取完全观测样本成本高昂，而部分噪声观测相对容易收集。需要开发能够利用大量噪声样本恢复真实分布的方法

Method: 将分布恢复任务建模为单边熵最优传输问题，采用EM类算法求解。引入测试准则判断分布可恢复性，提出SFBD-OMNI框架，将SFBD推广到任意测量模型

Result: 在基准数据集和多样化测量设置下的实验表明，该方法在定性和定量性能上均有显著提升

Conclusion: SFBD-OMNI框架能够有效利用大量噪声样本和少量干净样本恢复真实分布，适用于任意测量模型，具有广泛的应用前景

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [42] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 本文提出DTDR方法，通过动态检索工具依赖关系，显著提升LLM函数调用代理的性能


<details>
  <summary>Details</summary>
Motivation: 现有检索方法依赖静态有限输入，无法捕捉多步骤工具依赖关系和演化任务上下文，导致引入不相关工具误导代理，降低效率和准确性

Method: 提出动态工具依赖检索（DTDR），基于初始查询和演化执行上下文进行条件检索，从函数调用演示中建模工具依赖关系，实现自适应检索

Result: 在多个数据集和LLM骨干网络上评估，DTDR相比最先进静态检索方法，函数调用成功率提升23%到104%

Conclusion: 动态工具检索能显著改善函数调用代理的性能，同时探索了将检索工具集成到提示中的策略

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [43] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 本文证明了k最近邻分类器在完备可分度量空间中弱普遍一致性的充要条件是该空间具有Nagata意义下的σ有限维性，从而完成了三个等价条件的证明闭环。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是完成k最近邻分类器理论中一个长期悬而未决的问题：建立完备可分度量空间中k最近邻分类器弱普遍一致性与空间几何性质之间的完整等价关系。此前已有部分结果，但关键方向(1)⇒(3)尚未证明，且系列文章中存在错误论断需要修正。

Method: 采用理论证明方法，通过数学分析技术证明了k最近邻分类器的弱普遍一致性蕴含空间的σ有限维性（Nagata意义下）。该方法补全了已有结果中的缺失环节，并纠正了先前文章中的错误主张。

Result: 成功证明了(1)⇒(3)的蕴含关系，从而完成了三个条件的等价性证明：(1)k最近邻分类器弱普遍一致，(2)强Lebesgue-Besicovitch微分性质对所有局部有限Borel测度成立，(3)空间具有Nagata意义下的σ有限维性。

Conclusion: 本文最终确立了k最近邻分类器理论中三个重要条件的完全等价性，解决了该领域的一个核心问题，为度量空间中分类器的一致收敛性提供了完整的几何刻画，并修正了先前文献中的错误。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [44] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出BEAMoE方法，通过低秩补偿实现带宽高效的MoE模型推理，在保持精度的同时减少数据传输


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏激活扩展容量，但给内存和带宽带来压力。现有的卸载方法虽然缓解了GPU内存问题，但token级路由导致不规则数据传输，使推理受限于I/O。静态均匀量化虽然减少了流量，但在激进压缩下会因忽略专家异构性而降低精度。

Method: 提出带宽高效的自适应混合专家方法，通过低秩补偿进行路由器引导的精度恢复。在推理时，传输紧凑的低秩因子给每个token的Top-n专家（n<k），并对这些专家应用补偿，同时保持其他专家为低比特。该方法与GPU和GPU-NDP系统的卸载功能集成。

Result: 该方法在带宽-精度权衡方面表现优异，并提高了吞吐量

Conclusion: BEAMoE方法通过低秩补偿机制有效解决了MoE模型推理中的带宽限制问题，在保持模型精度的同时显著减少了数据传输需求

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [45] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法减少ST-GNN在边缘计算中的通信开销，同时引入SEPA新指标评估交通事件预测能力


<details>
  <summary>Details</summary>
Motivation: ST-GNN在智能交通系统中处理分布式传感器数据时，相邻边缘节点间重复传输重叠节点特征导致通信开销过大

Method: 提出自适应剪枝算法动态过滤冗余邻居特征，基于近期模型性能调整剪枝率；引入SEPA指标专门评估交通减速和恢复事件的预测能力

Result: 在PeMS-BAY和PeMSD7-M数据集上验证，自适应剪枝在保持预测精度的同时显著降低通信成本，SEPA指标能更好揭示空间连接性对动态交通预测的价值

Conclusion: 自适应剪枝算法可在不牺牲关键交通事件响应能力的前提下减少通信开销，SEPA指标比标准误差指标更能评估动态交通预测性能

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [46] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 训练模型在故意包含错误的推理轨迹上，可以提升其检测和恢复错误的能力，而不损害标准解题能力


<details>
  <summary>Details</summary>
Motivation: 链式思维提示已成为大语言模型数学推理的核心方法，但模型对早期错误非常脆弱：单个算术错误或不合理推断通常会导致最终答案错误。研究是否可以通过训练模型识别有缺陷的推理轨迹来提升其错误恢复能力

Method: 使用MATH-lighteval竞赛级问题生成包含单一控制错误（计算错误或推理错误）的CoT前缀，使用GRPO和二元最终答案奖励对Qwen3-4B进行微调

Result: Mixed-CoT-RL模型在干净问题上与标准RL表现相当（41% vs 41%），但在包含错误推理的问题上显著优于标准RL（24% vs 19%）。仅使用干净数据训练的RL会降低鲁棒性（19% vs 20%基线）

Conclusion: 在训练中暴露有缺陷的推理轨迹可以改善错误恢复行为而不牺牲准确性，为LLMs中更鲁棒的数学推理提供了路径。混合训练效果最佳，推理错误训练比单纯计算错误训练带来更大的鲁棒性提升

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [47] [How to Square Tensor Networks and Circuits Without Squaring Them](https://arxiv.org/abs/2512.17090)
*Lorenzo Loconte,Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 平方张量网络及其扩展形式平方电路作为分布估计器具有表达能力强且支持闭式边缘化的特点，但平方操作增加了计算配分函数和边缘化的复杂度。本文通过参数化平方电路来克服边缘化计算开销，实现高效边缘化。


<details>
  <summary>Details</summary>
Motivation: 平方张量网络和平方电路作为分布估计器虽然表达能力强且支持闭式边缘化，但平方操作在计算配分函数和边缘化变量时引入了额外的计算复杂度，这限制了它们在机器学习中的应用。现有张量网络的规范形式通过酉矩阵参数化可以简化边缘化计算，但这些规范形式不适用于电路，因为电路可以表示不直接映射到已知张量网络的分解。

Method: 受规范形式中的正交性和电路中确定性可实现可处理最大化的启发，本文展示了如何参数化平方电路以克服其边缘化开销。提出的参数化方法即使在不同于张量网络的分解中也能实现高效边缘化，这些分解被编码为电路，其结构原本会使边缘化计算变得困难。

Result: 实验结果表明，在分布估计任务中，本文提出的平方电路条件不会损失表达能力，同时实现了更高效的学习。参数化方法成功解锁了在原本计算困难的电路结构中实现高效边缘化的能力。

Conclusion: 本文通过参数化平方电路解决了平方操作带来的边缘化计算复杂度问题，提出的方法在保持表达能力的条件下实现了高效边缘化，扩展了平方电路在机器学习中的应用范围。

Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.

</details>


### [48] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出融合强化学习与MPC规划的新方法，通过自适应采样机制提升规划性能和数据效率


<details>
  <summary>Details</summary>
Motivation: 解决具有层次结构的规划问题，需要结合强化学习的灵活性和MPC规划的最优性，但现有方法未能紧密耦合这两种范式

Method: 提出紧密耦合强化学习与MPPI采样的自适应规划框架：用强化学习动作指导MPPI采样，同时自适应聚合MPPI样本来改进价值估计

Result: 在赛车驾驶、改进Acrobot和带障碍的Lunar Lander等多个领域验证，相比现有方法成功率最高提升72%，收敛速度加快2.1倍

Conclusion: 该方法能有效处理复杂规划问题，具有更好的数据效率和性能，可适应不同应用场景

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [49] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: UniCoMTE是一个模型无关的框架，用于为多元时间序列分类器生成反事实解释，提高深度学习模型的可解释性，在医疗等高风险领域特别有用。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在复杂时间序列分类中表现出色，但其黑盒特性限制了在医疗等高风险领域的信任和采用，需要提高模型的可解释性。

Method: UniCoMTE通过修改输入样本并评估其对模型预测的影响，识别对模型预测影响最大的时间特征。该框架与多种模型架构兼容，可直接处理原始时间序列输入。

Result: 在ECG时间序列分类器上的评估显示，UniCoMTE生成的解释比现有方法（LIME和SHAP）更简洁、稳定且与人类认知一致，在清晰度和适用性方面表现更优。

Conclusion: UniCoMTE通过将模型预测与有意义的信号模式联系起来，推进了深度学习模型在现实世界时间序列应用中的可解释性。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [50] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 提出基于可微分快速故障仿真模型(DFFSM)的光伏阵列故障量化方法，使用梯度优化实现高效、准确的故障参数识别


<details>
  <summary>Details</summary>
Motivation: 现有光伏故障量化方法存在效率低和可解释性差的问题，需要开发更高效、准确的故障诊断方法

Method: 构建可微分快速故障仿真模型(DFFSM)精确模拟多故障下的I-V特性，提供故障参数的解析梯度，开发基于Adahessian优化器的梯度故障参数识别(GFPI)方法

Result: 在仿真和实测I-V曲线上验证，GFPI方法对不同故障类型（局部阴影、短路、串联电阻退化）均实现高量化精度，I-V重构误差低于3%

Conclusion: 可微分物理仿真器在光伏系统故障诊断中具有可行性和有效性，为智能维护提供高效准确的量化工具

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [51] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: Atom是一个在移动设备上高效执行视频-语言多阶段管道的系统，通过模块化分解和重用核心组件来减少冗余加载和实现并行执行。


<details>
  <summary>Details</summary>
Motivation: 当前视频-语言模型在移动设备上执行多阶段管道（如检索、字幕生成、组装）时存在效率问题，包括重复模型加载和碎片化执行，导致延迟高、资源浪费。

Method: 将十亿参数模型分解为可重用的模块（如视觉编码器和语言解码器），在不同子任务（字幕生成、推理、索引）中重用这些模块，消除重复加载并实现并行执行。

Result: 在商用智能手机上，Atom比非重用基线快27-33%，性能下降很小（检索任务Recall@1下降≤2.3，字幕生成CIDEr下降≤1.5）。

Conclusion: Atom为边缘设备上的高效视频-语言理解提供了一种实用、可扩展的方法，通过重用设计显著提升执行效率，同时保持性能。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [52] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，在训练时维护分解的动量和曲率统计信息，然后复用这些信息进行几何感知的模型组合，避免重复计算。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程在训练时计算曲率信息，然后丢弃，再为模型合并重新计算类似信息，浪费计算资源且丢弃了有价值的轨迹数据。需要统一训练和模型合并的框架。

Method: 引入统一框架，在训练期间维护分解的动量和曲率统计信息，然后复用这些信息进行几何感知的模型组合。该方法内存效率高，能累积任务显著性分数，无需后验Fisher计算即可实现曲率感知的合并。

Result: 在自然语言理解基准测试中，曲率感知的参数选择在所有稀疏度水平上都优于仅基于幅度的基线方法，多任务合并也优于强基线。框架展现出秩不变收敛性和优于现有低秩优化器的超参数鲁棒性。

Conclusion: 通过将优化轨迹视为可重用资产而非丢弃，该方法消除了冗余计算，同时实现了更原则性的模型组合，为训练和模型合并提供了统一的高效解决方案。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [53] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 首个针对古尼泊尔语的端到端手写文本识别系统，采用编码器-解码器架构，在低资源条件下实现了4.9%的字符错误率。


<details>
  <summary>Details</summary>
Motivation: 古尼泊尔语是一种具有历史意义但资源匮乏的语言，缺乏有效的手写文本识别系统，需要开发专门的技术来处理这种低资源历史文字。

Method: 采用行级转录方法，系统探索编码器-解码器架构和数据中心化技术，实现解码策略并分析标记级混淆以理解模型行为。

Result: 最佳模型实现了4.9%的字符错误率，虽然评估数据集保密，但发布了训练代码、模型配置和评估脚本以支持进一步研究。

Conclusion: 成功开发了首个古尼泊尔语端到端手写文本识别系统，为低资源历史文字识别提供了可行方案，并开源代码促进该领域研究。

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [54] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 该研究评估了CheXagent模型在含否定词的医学影像检索任务中的表现，通过微调方法改进CLIP模型处理否定语言的能力，并分析了模型内部行为变化。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在医学影像任务中应用广泛，但存在处理否定短语能力不足的问题，这在医学诊断场景中尤为关键，可能影响模型的可靠性。

Method: 使用Stanford AIMI CheXagent模型评估含否定词与不含否定词的胸部X光图像检索能力，基于先前工作提出的微调方法改进模型，并通过token归因、t-SNE投影和注意力头消融分析模型内部行为。

Result: 微调后CLIP模型处理否定语言的能力得到改善，但正例提示的准确率略有下降；通过内部行为分析揭示了不同微调方法如何重塑文本编码器对否定临床语言的表示。

Conclusion: 研究深化了对CLIP模型内部行为的理解，通过临床相关语言的微调提高了模型处理否定的能力，有助于提升医学AI设备的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [55] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: DiffeoMorph：一种端到端可微分框架，通过注意力SE(3)-等变图神经网络学习形态发生协议，指导智能体群体形成目标3D形状，使用3D Zernike多项式形状匹配损失和SO(3)不变性对齐。


<details>
  <summary>Details</summary>
Motivation: 生物系统通过相同智能体的集体行为形成复杂三维结构，这种分布式控制如何产生精确全局模式是发育生物学、分布式机器人、可编程物质和多智能体学习中的核心问题。需要一种可学习的方法指导智能体群体形态发生。

Method: 引入DiffeoMorph端到端可微分框架，每个智能体使用基于注意力的SE(3)-等变图神经网络更新位置和内部状态。使用基于3D Zernike多项式的形状匹配损失，将预测和目标形状作为连续空间分布比较。通过内层优化单位四元数对齐、外层更新智能体模型的双层问题实现SO(3)不变性，使用隐式微分计算对齐步骤的梯度。

Result: 通过系统基准测试确立了形状匹配损失相对于其他标准距离度量的优势。演示了DiffeoMorph能够仅使用最小空间线索形成从简单椭球体到复杂形态的各种形状。

Conclusion: DiffeoMorph为学习分布式形态发生协议提供了有效的可微分框架，通过3D Zernike多项式形状匹配和SO(3)不变性对齐实现了精确的3D形状形成，在生物启发式多智能体系统中有广泛应用前景。

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [56] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: 提出GPA方法，改进Nesterov方法的原始平均形式，解决单工作者DiLoCo和Schedule-Free等平均优化器的局限性，实现更平滑的迭代平均，减少内存开销和超参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有平均优化器如单工作者DiLoCo和Schedule-Free虽然能提升基础优化器性能，但存在局限性：DiLoCo的周期性平均引入双循环结构，增加内存需求和超参数数量；Schedule-Free维护过去权重的均匀平均。需要一种更高效、更简单的平均策略。

Method: GPA通过解耦Nesterov原始平均公式中的插值常数，实现每步平滑平均迭代，消除双循环结构。该方法只需一个额外缓冲区，简化超参数调优，并能匹配或超越基础优化器的收敛保证。

Result: 在Llama-160M模型上，GPA相比AdamW基线验证损失达到速度提升24.22%；在ImageNet ViT任务中，小批量和大批量设置下分别获得12%和27%的速度提升。理论证明GPA能匹配或超越基础优化器的收敛保证。

Conclusion: GPA是一种有效的平均优化器扩展，能克服现有方法的局限性，提供更好的性能、更简单的实现和更低的内存开销，同时保持理论收敛保证。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [57] [Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing](https://arxiv.org/abs/2512.17161)
*Liad Lea Didi,Kobi Cohen*

Main category: cs.LG

TL;DR: 提出SMILE算法，在通信受限的无线网络中实现分布式频谱共享，通过结合多臂赌博机学习和图约束协调，达到全局稳定且干扰感知的信道分配。


<details>
  <summary>Details</summary>
Motivation: 研究多认知通信实体（如小区、子网络或认知无线电用户）在干扰图建模的通信受限无线网络中的分布式频谱接入与共享问题。目标是实现全局稳定且干扰感知的信道分配，特别是在信道作为未知的、随时间变化的马尔可夫过程演化的随机环境中。

Method: 开发SMILE（Stable Multi-matching with Interference-aware LEarning）算法，将多臂赌博机学习与图约束协调相结合。该算法使小区能够分布式地平衡对未知信道的探索和对已学习信息的利用，同时保证通信效率。

Result: 证明SMILE收敛到最优稳定分配，并实现相对于完全了解期望效用的理想情况的对数遗憾。仿真验证了理论保证，并展示了SMILE在不同频谱共享场景下的鲁棒性、可扩展性和效率。

Conclusion: SMILE算法成功解决了随机、时变环境中的信道分配问题，首次在随机环境中建立了全局Gale-Shapley稳定性，为通信受限无线网络中的分布式频谱共享提供了有效的解决方案。

Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.

</details>


### [58] [BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions](https://arxiv.org/abs/2512.17198)
*Shao-Ting Chiu,Ioannis G. Kevrekidis,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: BumpNet是一种基于网格无关基函数展开的稀疏神经网络框架，用于PDE数值解和算子学习，通过可训练的sigmoid基函数实现高效训练和动态剪枝。


<details>
  <summary>Details</summary>
Motivation: 提出一种能够结合现代神经网络训练技术、实现模型简约性和h-自适应的PDE求解框架，克服传统径向基函数网络的局限性。

Method: 基于sigmoid激活函数构建可训练基函数（形状、位置、振幅均可训练），通过动态剪枝实现模型简约性和自适应，并与PINNs、EDNNs、DeepONet等现有架构结合。

Result: 提出了三种具体架构：Bump-PINNs用于一般PDE求解，Bump-EDNN用于时间演化PDE，Bump-DeepONet用于PDE算子学习，实验证明其高效性和准确性。

Conclusion: BumpNet是一个通用的稀疏神经网络框架，能够有效结合现代训练技术，通过可训练的sigmoid基函数和动态剪枝机制，在PDE数值解和算子学习中表现出优越性能。

Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.

</details>


### [59] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: DM-KRR方法通过扩散映射核结合动态感知验证策略，为复杂动力系统的长期预测提供了简单而强大的基准，在精度和数据效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出复杂的非线性动力学，长期预测困难。现有数据驱动模型在几何结构未知或表示不佳时性能下降，需要一种能适应系统内在几何结构的方法。

Method: 提出扩散映射核岭回归（DM-KRR）方法，结合扩散映射导出的数据驱动核和动态感知验证策略，无需显式流形重建或吸引子建模，自动适应系统不变集的内在几何结构。

Result: 在包括光滑流形、混沌吸引子和高维时空流在内的广泛系统中，DM-KRR在精度和数据效率上一致优于最先进的随机特征、神经网络和算子学习方法。

Conclusion: 长期预测能力不仅取决于模型表达能力，更关键的是通过动态一致的模型选择尊重数据中的几何约束。DM-KRR的简单性、几何感知能力和强大性能为复杂动力系统的可靠高效学习指明了方向。

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [60] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 该研究系统评估了五种时间序列预测模型在电动汽车充电需求预测中的表现，涵盖不同时间尺度（分钟、小时、天）和空间聚合水平（单个充电站到城市级），使用四个真实数据集进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及对电网管理带来的挑战，准确预测充电需求变得日益重要。然而，现有研究缺乏对不同预测方法在多种时间尺度和空间聚合水平下的系统性比较，特别是在多样化城市环境中的评估。

Method: 研究采用五种时间序列预测模型（包括传统统计方法、机器学习和深度学习方法），在四个公开真实数据集上，针对短、中、长期时间尺度（分钟、小时、天）和不同空间聚合水平（单个充电站、区域级、城市级）进行系统性评估。

Result: 这是首个使用多个真实数据集，在如此广泛的时间尺度和空间聚合水平上系统评估电动汽车充电需求预测的研究。研究为不同应用场景下的模型选择提供了实证依据。

Conclusion: 该研究填补了电动汽车充电需求预测领域系统性评估的空白，为电网管理和充电基础设施规划提供了重要的方法学参考，有助于根据具体的时间尺度和空间需求选择合适的预测模型。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [61] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: SHARP-QoS：一种基于双曲卷积和自适应特征共享的统一联合QoS预测框架，通过EMA损失平衡解决多QoS参数预测中的负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界QoS数据极其稀疏、噪声大且具有层次依赖关系，现有方法要么单独预测每个QoS参数导致计算成本高泛化差，要么联合预测时因数值范围不一致导致负迁移和表示学习不足。

Method: 1) 使用双曲卷积在庞加莱球中提取QoS和上下文结构的层次特征；2) 自适应特征共享机制允许信息性QoS和上下文信号的特征交换，门控特征融合模块支持动态特征选择；3) EMA损失平衡策略实现稳定联合优化。

Result: 在包含2、3、4个QoS参数的三个数据集上评估，SHARP-QoS优于单任务和多任务基线模型，能有效处理稀疏性、异常值鲁棒性和冷启动问题，同时保持适度的计算开销。

Conclusion: SHARP-QoS通过层次特征提取、自适应特征共享和损失平衡策略，为可靠的联合QoS预测提供了有效解决方案，解决了现有方法的局限性。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [62] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量(GBSM)，用于测量任意马尔可夫决策过程(MDP)对之间的状态相似性，具有严格的数学性质，并在多MDP场景中提供了更紧的理论边界和更好的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量(BSM)在单个MDP内分析状态相似性很有效，但在多个MDP之间的应用存在挑战。先前工作尝试将BSM扩展到MDP对，但缺乏完善的数学性质限制了进一步的理论分析。

Method: 正式建立了广义双模拟度量(GBSM)，用于测量任意MDP对之间的状态相似性。严格证明了三个基本度量性质：GBSM对称性、MDP间三角不等式和相同空间上的距离界限。

Result: 利用这些性质，理论分析了跨MDP的策略迁移、状态聚合和基于采样的估计，获得了比标准BSM更严格的理论边界。GBSM提供了闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了理论发现。

Conclusion: GBSM为多MDP场景中的状态相似性测量提供了坚实的理论基础，具有严格的数学性质和更优的理论性能，在多任务强化学习中具有应用价值。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [63] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 提出R-EMID信息论指标量化角色扮演模型性能退化，建立理论框架分析分布偏移影响，并通过协同进化强化学习提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 角色扮演模型在真实应用中表现不佳，现有方法无法细粒度诊断用户、角色和对话组合偏移对泛化的影响，缺乏形式化框架来表征RPM的泛化行为。

Method: 1) 提出R-EMID信息论指标量化性能退化；2) 推导R-EMID上界预测最坏情况泛化性能；3) 提出协同进化强化学习框架自适应建模用户、角色和对话上下文关系，提升对话响应生成概率估计。

Result: 评估发现用户偏移在所有偏移中风险最高，强化学习是增强RPM泛化的最有效方法。R-EMID能够有效量化RPM性能退化并揭示各种偏移的贡献。

Conclusion: R-EMID为RPM泛化提供了可解释的量化框架，协同进化强化学习能有效提升模型对分布偏移的适应性，用户偏移是需要重点关注的风险因素。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [64] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: MINPO是一个统一的神经网络框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学，通过学习非局部算子及其逆来直接求解积分-微分方程。


<details>
  <summary>Details</summary>
Motivation: 传统求解积分-微分方程的方法需要反复计算卷积积分，计算成本随核复杂度和维度快速增加。现有的神经求解器只能加速特定计算，无法泛化到不同的非局部结构。

Method: MINPO使用KANs或MLPs作为编码器，学习非局部算子及其逆的神经表示，然后显式重构未知解场。通过轻量级的非局部一致性损失项来保证学习算子与重构解之间的一致性。

Result: 与经典方法和最先进的基于MLPs的神经策略（如A-PINN、fPINN）及其KAN变体（A-PIKAN、fPIKAN）相比，MINPO在准确性上表现出色，并展示了在处理不同核类型、不同核维度和重复核积分计算需求方面的鲁棒性。

Conclusion: MINPO超越了特定问题的公式，为受非局部算子控制的系统提供了一个统一的框架，能够自然捕获并高效解决由广泛IDE谱及其子集（包括分数PDE）控制的非局部时空依赖关系。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [65] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD是一个半监督学习框架，通过结合深度表示学习、图标签传播和最优传输理论，在仅有不到三分之一标注数据的情况下实现近乎完美的阿尔茨海默病诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断面临临床评估昂贵且侵入性强的问题，导致神经影像数据集中只有小部分有真实标签。需要开发能够利用大量未标注数据的半监督方法。

Method: 提出MATCH-AD框架，整合深度表示学习、基于图的标签传播和最优传输理论。利用神经影像数据的流形结构将诊断信息从有限标注样本传播到大量未标注样本，并使用Wasserstein距离量化认知状态间的疾病进展。

Result: 在NACC近5000名受试者数据集上评估，尽管只有不到三分之一的真实标签，MATCH-AD实现了近乎完美的诊断准确率，kappa值显示几乎完美的一致性，显著优于所有基线方法。即使在严重标签稀缺情况下仍保持临床实用性。

Conclusion: 原则性半监督学习能够释放全球积累的部分标注神经影像数据的诊断潜力，显著减少标注负担，同时保持适合临床部署的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [66] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: M2RU是一种混合信号架构，实现了minion循环单元，用于边缘平台上的高效时序处理和片上持续学习，相比数字CMOS设计能效提升29倍。


<details>
  <summary>Details</summary>
Motivation: 边缘平台上的持续学习面临挑战，因为循环网络依赖能耗高的训练过程和频繁的数据传输，这在嵌入式部署中不切实际。

Method: 提出M2RU混合信号架构，集成加权比特流技术（使多比特数字输入无需高分辨率转换即可在交叉阵列中处理）和经验回放机制（稳定域偏移下的学习）。

Result: 达到15 GOPS/48.62 mW（312 GOPS/瓦），在序列MNIST和CIFAR-10任务上保持与软件基线5%以内的精度，相比CMOS数字设计能效提升29倍，持续学习工作负载下预期运行寿命12.2年。

Conclusion: M2RU为边缘级时序智能中的实时适应提供了一个可扩展且高能效的平台。

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [67] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: 该论文提出了一个基于图论的固有可解释性标准，使用结构局部解释和全局重组，区分了"可解释"与"已解释"模型，并以临床使用的PREDICT心血管风险模型为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能领域缺乏一致的固有可解释性定义和测试标准，现有工作要么依赖指标度量，要么诉诸直觉判断。需要建立一个全局适用的、严谨的固有可解释性标准。

Method: 提出基于图论的固有可解释性标准：1) 使用图论表示和分解模型结构进行局部解释；2) 将局部解释重组为全局解释；3) 形成可验证的假设-证据结构的注释；4) 区分"可解释"（允许解释）和"已解释"（具有已验证解释）模型。

Result: 该标准与现有固有可解释性直觉一致，能够解释为何大型回归模型可能不可解释而稀疏神经网络却可解释。成功为新西兰临床使用的心血管疾病风险模型PREDICT提供了完整解释，证明其具有固有可解释性。

Conclusion: 该工作为可解释性研究提供了结构化框架，为监管机构提供了灵活而严谨的合规测试标准，有助于推动可解释人工智能的标准化和实际应用。

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [68] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 论文通过因果机制验证，证明上下文学习可分解为任务图式和绑定两个可分离机制，前者通过MLP补丁100%转移，后者通过残差流补丁62%转移，揭示了ICL的双过程理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究将上下文学习视为单一机制（检索式、梯度下降式或纯贝叶斯式），缺乏对其内部机制的因果性理解。本文旨在通过因果机制验证，探究ICL是否由可分离的神经机制组成。

Method: 在9个模型（7个Transformer家族和Mamba，370M-13B参数）上进行激活补丁实验，通过双分离设计验证任务图式和绑定的可分离性，并分析架构通用性和先验知识影响。

Result: 1. 双分离证明：任务图式通过晚期MLP补丁100%转移，绑定通过残差流补丁62%转移；2. 先验-图式权衡：图式依赖与先验知识负相关；3. 架构通用性：机制在所有测试架构中均存在，包括非Transformer的Mamba。

Conclusion: ICL由可分离的任务图式和绑定机制组成，而非单一机制。模型在先验知识缺失时依赖图式，而先验知识通过注意力误路由（72.7%近因偏差）而非直接输出竞争（0%）产生干扰。这解释了任意映射成功而事实覆盖失败的原因，揭示了注意力而非输出层是真正瓶颈。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [69] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 提出LLM-SGA框架和ARHOCD检测器，通过样本生成与聚合增强对抗鲁棒性，结合集成学习、动态权重分配和对抗训练，在有害内容检测中同时提升泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体有害内容检测模型易受对抗攻击，现有方法难以同时实现高泛化能力和准确性，需要开发更鲁棒的检测框架。

Method: 1) 提出LLM-SGA框架，利用文本对抗攻击的关键不变性增强泛化能力；2) 实例化ARHOCD检测器，包含：多基检测器集成、基于样本可预测性和检测器能力的动态权重分配（贝叶斯更新）、迭代优化的对抗训练策略。

Result: 在仇恨言论、谣言和极端主义内容三个数据集上评估，ARHOCD在对抗条件下表现出强泛化能力并提高了检测准确率。

Conclusion: ARHOCD通过创新的框架设计和组件，成功解决了对抗鲁棒性研究中泛化能力和准确性难以兼顾的问题，为有害内容检测提供了有效解决方案。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [70] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 论文揭示奖励模型和LLM-as-a-Judge系统存在漏洞：低困惑度的控制令牌序列可以翻转二元评估结果，导致高误判率，并提出AdvJudge-Zero方法发现这些令牌，以及通过对抗训练降低误判率。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和LLM-as-a-Judge系统在现代后训练流程（如RLHF、DPO、RLAIF）中至关重要，它们提供标量反馈和二元决策来指导模型选择和基于RL的微调。然而这些评判系统存在一个反复出现的漏洞，可能导致奖励黑客攻击风险。

Method: 提出AdvJudge-Zero方法，利用模型的下一令牌分布和束搜索探索从头发现多样化的控制令牌序列。分析显示这些令牌诱导的隐藏状态扰动集中在低秩"软模式"中，与评判的拒绝方向反对齐。最后通过基于LoRA的对抗训练在控制令牌增强的小样本集上减少误判。

Result: 实验表明，这些控制令牌在大型开源权重和专用评判模型对数学和推理基准的错误答案评分时，会导致极高的误判率。对抗训练可以显著减少这些误判，同时保持评估质量。

Conclusion: LLM评判系统存在实际的安全漏洞，低困惑度控制令牌可以操纵评估结果。提出的AdvJudge-Zero方法能有效发现这些漏洞，而对抗训练是缓解这一问题的可行方案，对后训练流程的安全性具有重要意义。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [71] [DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference](https://arxiv.org/abs/2512.17398)
*Yonathan Bornfeld,Shai Avidan*

Main category: cs.LG

TL;DR: 提出一种新的私有推理激活模块，通过原型通道减少DReLU计算量，显著提升私有推理效率


<details>
  <summary>Details</summary>
Motivation: 私有推理中ReLU计算是主要瓶颈，现有方法致力于减少ReLU数量，但仍有优化空间

Method: 设计新激活模块：仅对原型通道执行DReLU操作，复制通道复用原型通道的DReLU结果，并扩展到跨层应用

Result: 在ResNet类型网络中大幅减少DReLU操作，解决扩展XOR问题仅需1个非线性和2个神经元，在分类和图像分割任务中达到SOTA

Conclusion: 提出的原型通道方法能有效减少私有推理中的DReLU计算量，提升效率同时保持性能

Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.

</details>


### [72] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 该论文提出了一个用于医疗影像模型子组性能差异分析的统计工具箱，解决了样本量差异、多重比较校正等统计挑战，并在皮肤病变和胸部X光数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前医疗影像机器学习模型分析中，按患者和记录属性进行分层性能分析已成为标准做法，但进行严谨的统计分析存在挑战：需要选择合适的性能指标、处理不同样本量和基础率、确定指标不确定性、校正多重比较，以及在组合分析中找到最"有趣"的子组。

Method: 开发了一个统计工具箱，专门针对医疗影像应用设计，包含：1）适用于不同样本量和基础率的性能指标选择；2）指标不确定性的确定方法；3）多重比较校正机制；4）组合分析中寻找最相关子组的算法。

Result: 在两个案例研究中验证了工具箱的有效性：1）在ISIC2020数据集上进行皮肤病变恶性分类；2）在MIMIC-CXR数据集上进行胸部X光疾病分类。工具箱能够帮助实践者严谨地评估模型在不同子组中的性能差异。

Conclusion: 该统计工具箱为医疗影像机器学习模型提供了严谨的子组性能差异分析框架，解决了现有分析中的统计挑战，使实践者能够轻松而系统地识别模型潜在的性能差异问题。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [73] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 提出一个基于多智能体强化学习的模型，用于模拟和分析长期电力市场机制对电力系统脱碳的影响。


<details>
  <summary>Details</summary>
Motivation: 电力系统是实现碳中和经济转型的关键，但缺乏先进工具来支持政策制定者设计和评估长期电力市场机制。现有工具难以捕捉市场参与者在脱碳过程中的动态响应和适应行为。

Method: 采用多智能体强化学习框架，发电公司作为利润最大化智能体在电力批发市场中进行投资决策。使用独立的近端策略优化算法，并通过广泛的超参数搜索确保分散式训练产生符合竞争行为的结果。

Result: 模型应用于意大利电力系统的简化版本，测试了不同竞争水平、市场设计和政策情景。结果显示市场设计对电力部门脱碳和避免价格波动具有关键作用。

Conclusion: 该框架能够评估多个政策和市场机制同时交互的长期电力市场，捕捉市场参与者对脱碳路径的响应和适应，为政策制定者提供重要工具支持。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [74] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: WG-KV提出了一种轻量级的KV缓存管理机制，通过学习预测token效用，在写入缓存前过滤低效用状态，显著降低内存使用并加速长上下文推理。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理面临二次注意力复杂度和线性KV缓存增长的瓶颈。现有方法通过后验选择或驱逐来缓解，但忽视了根本低效性：不加区分地写入持久内存。

Method: 将KV缓存管理形式化为三个因果原语：KV准入、选择和驱逐。通过Write-Gated KV实例化KV准入，这是一个轻量级机制，学习在token进入缓存前预测其效用，结合紧凑全局缓存和滑动局部缓存。

Result: 在Llama模型上，内存使用减少46-57%，预填充速度提升3.03-3.45倍，解码速度提升1.89-2.56倍，精度损失可忽略，且与FlashAttention和分页KV系统兼容。

Conclusion: 学习"写什么"是高效长上下文推理的原则性和实用方法，Write-Gated KV通过早期过滤低效用状态，在保持精度的同时显著提升效率。

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [75] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Lite-STGNN是一个轻量级的时空图神经网络，用于长期多元预测，结合了分解式时间建模和可学习的稀疏图结构，在保持高效的同时实现了最先进的预测精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决长期多元时间序列预测中传统方法（特别是基于Transformer的方法）参数量大、训练慢的问题，同时希望保持高精度并增强模型的可解释性。

Method: 1. 时间模块：采用趋势-季节性分解方法进行时间建模
2. 空间模块：使用低秩Top-K邻接学习和保守的逐视野门控进行消息传递
3. 整体架构：将空间修正集成到强大的线性基线模型中

Result: 1. 在四个基准数据集上，对于长达720步的预测视野，达到了最先进的准确率
2. 参数效率高，训练速度显著快于基于Transformer的方法
3. 消融研究显示：空间模块比时间基线提升4.6%，Top-K增强局部性3.3%，学习的邻接矩阵揭示了领域特定的交互动态

Conclusion: Lite-STGNN为长期多元时间序列预测提供了一个紧凑、可解释且高效的框架，在保持高性能的同时显著降低了计算复杂度。

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [76] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: 使用BiLSTM-VAE和BiLSTM-Transformer深度学习模型作为ANSYS蠕变模拟的替代模型，实现秒级预测，相比传统模拟节省大量计算时间。


<details>
  <summary>Details</summary>
Motivation: Inconel 625等高温合金的蠕变行为对航空航天和能源系统组件的长期可靠性至关重要，但传统的有限元模拟（如ANSYS）计算成本高昂，单次10,000小时模拟需要30-40分钟，限制了设计优化和结构健康监测的效率。

Method: 使用ANSYS基于Norton定律生成单轴应力（50-150 MPa）和温度（700-1000°C）条件下的蠕变应变数据，训练两种深度学习架构：1）BiLSTM变分自编码器（VAE）用于不确定性感知和生成式预测；2）BiLSTM-Transformer混合模型，利用自注意力机制捕捉长期时间依赖关系。

Result: 两种替代模型均表现良好：BiLSTM-VAE提供稳定可靠的蠕变应变预测并具有概率输出能力；BiLSTM-Transformer在整个时间范围内实现高确定性精度。性能评估使用RMSE、MAE和R²指标。延迟测试显示显著加速：ANSYS模拟需要30-40分钟，而替代模型在几秒内即可完成预测。

Conclusion: 提出的深度学习框架为高温合金蠕变评估提供了快速准确的解决方案，实现了从分钟级到秒级的计算速度提升，支持设计优化和结构健康监测应用，并为高温合金应用提供了可扩展的解决方案。

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [77] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: SafeBench-Seq是一个用于蛋白质序列危害筛查的基准测试和基线分类器，基于公开数据构建，采用同源性控制评估，可在普通CPU上运行。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计的基础模型存在生物安全风险，但社区缺乏一个简单、可复现的序列级危害筛查基准，该基准需要在同源性控制下评估且能在普通CPU上运行。

Method: 使用公开数据（SafeProtein危害数据和UniProt良性数据）构建基准，采用可解释特征（全局物理化学描述符和氨基酸组成）。通过≤40%同源性聚类数据集并进行聚类级留出验证，评估分类器性能并量化概率质量。

Result: 随机分割会显著高估鲁棒性；经过校准的线性模型表现出较好的校准性，而树集成模型保留稍高的Brier/ECE分数。SafeBench-Seq仅需CPU、可复现，且仅发布元数据。

Conclusion: SafeBench-Seq为蛋白质序列危害筛查提供了一个简单、可复现的基准测试框架，支持严格的评估而无需分发危险序列，有助于生物安全风险评估。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [78] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: CFF算法通过层间协作机制改进Forward-Forward算法，解决层间隔离问题，提升深层架构收敛效率


<details>
  <summary>Details</summary>
Motivation: 传统Forward-Forward算法存在层间隔离问题，各层独立优化goodness函数，缺乏集体学习动态，限制了表征协调和深层架构的收敛效率

Method: 提出Collaborative Forward-Forward学习框架，包含两种协作范式：固定耦合的F-CFF和可学习协作参数的A-CFF，通过加权所有层的贡献实现协调特征学习

Result: 在MNIST和Fashion-MNIST数据集上相比基线Forward-Forward实现有显著性能提升

Conclusion: 层间协作是Forward-Forward学习的基本增强机制，对神经形态计算架构和能源受限AI系统有直接应用价值

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [79] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 本文提出了针对解耦黑盒约束问题的贝叶斯优化新变体，基于知识梯度采集函数，能够智能选择评估相关约束而非全部约束。


<details>
  <summary>Details</summary>
Motivation: 在具有解耦黑盒约束的昂贵全局优化问题中，通常只有少数约束在最优解处是有效的，但现有方法往往评估所有约束，造成不必要的计算成本。

Method: 提出了基于知识梯度采集函数的贝叶斯优化新变体，专门处理解耦约束问题。该方法能够智能识别哪些约束可能在最优解处有效，从而只评估相关约束而非全部约束。

Result: 通过实证基准测试，证明了所提方法相对于现有最先进方法的优越性，在解耦约束优化问题上表现更佳。

Conclusion: 针对解耦黑盒约束问题提出的新贝叶斯优化方法能够有效减少不必要的约束评估，提高优化效率，在复杂约束优化任务中具有实际应用价值。

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [80] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: GreedySnake是一种新的SSD卸载训练系统，采用垂直调度策略，相比水平调度能实现更高的训练吞吐量，特别适合小批量训练，接近屋顶线模型的理论极限。


<details>
  <summary>Details</summary>
Motivation: SSD卸载训练是降低LLM训练成本的有效方法，但现有系统采用水平调度（按微批次顺序执行）存在效率问题，特别是在小批量情况下无法充分利用硬件资源。

Method: 提出GreedySnake系统：1）采用垂直调度策略，先执行一个层的所有微批次再进入下一层；2）将部分优化步骤与下一次迭代的前向传播重叠，缓解I/O瓶颈。

Result: 在A100 GPU上的实验显示，相比ZeRO-Infinity，GreedySnake在GPT-65B上实现1.96x（1 GPU）和1.93x（4 GPUs）的吞吐量提升，在GPT-175B上实现2.53x（1 GPU）的提升。

Conclusion: GreedySnake通过垂直调度和优化步骤重叠，显著提升了SSD卸载训练的效率，使系统更接近屋顶线模型的理论性能，为大规模LLM训练提供了更经济的解决方案。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [81] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: 开发用于静态和单事件动态网络的图表示学习新算法，基于潜在空间模型，创建结构感知的网络表示，实现统一的嵌入学习过程


<details>
  <summary>Details</summary>
Motivation: 当前图表示学习方法需要启发式或多阶段处理，缺乏统一的框架。需要开发能够自然捕捉网络特性（如同质性、传递性、平衡理论）并支持多种图分析任务的综合方法

Method: 基于潜在距离模型的潜在空间模型方法，创建结构感知的网络表示，实现统一的图表示学习过程，无需后处理步骤

Result: 开发了能够生成层次化网络结构表达、社区特征识别、极端节点发现和时间网络影响动态量化的方法，提供统一的网络嵌入框架

Conclusion: 提出的方法为静态和动态网络提供了统一的图表示学习框架，能够全面捕捉网络结构特性并支持多样化的图分析任务，消除了传统多阶段处理的需求

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [82] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL框架在真实世界自动驾驶场景中验证有效，能改善奖励-安全权衡，提升成功率并降低成本，同时增强对观测噪声的鲁棒性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在自动驾驶中面临性能优化与安全要求的根本矛盾，保守策略限制驾驶效率，而激进探索则带来安全风险。SRPL框架通过预测未来约束违规模型来解决这一挑战，但需要验证其在真实世界自动驾驶场景中的有效性。

Method: 在Waymo Open Motion Dataset和NuPlan数据集上进行系统实验，评估SRPL框架在真实世界自动驾驶场景中的表现。研究包括奖励-安全权衡分析、成功率统计检验、成本降低效果评估，以及对观测噪声鲁棒性和跨数据集零样本泛化能力的测试。

Result: SRPL能显著改善奖励-安全权衡，成功率提升效果显著（效应量r=0.65-0.86），成本降低效果明显（效应量r=0.70-0.83），所有改进p<0.05。预测安全表征对提升观测噪声鲁棒性起关键作用，在跨数据集零样本评估中，SRPL增强的智能体展现出比非SRPL方法更好的泛化能力。

Conclusion: 预测安全表征能有效增强自动驾驶中的安全强化学习，但SRPL的有效性依赖于底层策略优化器和数据集分布。研究结果证明了预测安全表征在真实世界自动驾驶场景中的潜力。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [83] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 该论文研究了在数据分散且无法共享的医疗领域，通过异步协作（仅共享已训练模型）结合模型拼接技术，实现竞争性性能表现的方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据分散且无法共享的领域，联邦学习需要同步训练，而异步协作（仅共享已训练模型）的可行性需要验证。

Method: 采用多目标视角，将各参与方数据性能独立看待，通过模型拼接技术结合中间表示，使用精心放置的拼接层来融合独立训练的模型。

Result: 单个参与方训练模型在自身数据上性能相似，但在其他方数据上性能较差；集成模型泛化更好但自身数据性能下降；拼接方法能恢复竞争性性能同时保持改进的泛化能力。

Conclusion: 异步协作结合模型拼接技术可以产生竞争性结果，为数据分散且无法共享的场景提供了可行的解决方案。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [84] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: 该论文提出了分布式参数神经网络(DiPaNet)的统一框架，将有限和无限维神经网络架构通过均匀化/离散化联系起来，并推导了神经元数量和隐藏层数趋于无穷时的近似误差。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在隐藏层神经元数量和层数趋于无穷时的极限情况，建立连续神经网络表示与现有架构的统一理论框架。

Method: 首先推导单隐藏层的积分无限宽度神经网络表示，扩展到具有有限积分隐藏层的深度残差连续神经网络；然后通过离散化技术形式化神经ODE与深度残差网络的近似误差；最后将两种方法合并为分布式参数神经网络(DiPaNet)的统一齐次表示。

Result: 证明了大多数现有的有限和无限维神经网络架构可以通过DiPaNet表示通过均匀化/离散化联系起来，建立了统一的确定性框架，适用于一般的一致连续矩阵权重函数。

Conclusion: DiPaNet框架为神经网络提供了统一的数学表示，连接了离散和连续架构，讨论了与神经场的异同，并提出了进一步推广和应用的可能性。

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [85] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 该论文提出了一种分析算法在噪声、扰动和与其他动态系统互连环境下收敛性的统一框架，通过Lyapunov理论量化扰动影响，并应用于分布式学习、机器学习泛化和隐私保护等多个场景。


<details>
  <summary>Details</summary>
Motivation: 算法在复杂物理、社会和工程系统中运行时，经常暴露于扰动、噪声以及与其他动态系统的互连中。现有分析通常假设算法在孤立环境中运行，缺乏对扰动影响的系统性量化方法。

Method: 利用Lyapunov逆定理，推导出量化扰动影响的关键不等式，将孤立环境下的收敛保证扩展到存在扰动的场景，建立稳定性边界和收敛速率。

Result: 开发了一个统一的分析工具，能够评估扰动对算法性能的影响，并在分布式学习的通信约束、机器学习泛化的敏感性、以及隐私保护的噪声注入等多个应用中验证了该方法的有效性。

Conclusion: 该研究提供了一个系统性的框架，用于分析算法在噪声、扰动和系统互连环境下的性能，为算法鲁棒性分析提供了统一的理论工具。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [86] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: 提出了一种结合硬优先和易优先的混合训练策略，通过交替训练算法提升PINNs在求解PDE时的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs训练策略存在局限性：硬优先方法（受有限元方法启发）和易优先方法各有优缺点，在不同类型PDE上表现不一致，存在显著的权衡问题。

Method: 开发了一种混合策略，结合硬优先和易优先的优势，采用交替训练算法，在具有陡峭梯度、非线性和高维度的PDE上实现稳定训练。

Result: 在多种PDE问题上取得一致高精度，相对L2误差主要在O(10^-5)到O(10^-6)范围内，显著超越基线方法，且在不同问题间表现出更好的可靠性。

Conclusion: 该工作为设计混合训练策略提供了新见解，能够增强PINNs的性能和鲁棒性，解决现有方法在不同PDE类型上表现不一致的问题。

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [87] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: SCOPE：一种新的规范性过程监控方法，通过反向归纳和因果学习，直接从观测数据中学习对齐的顺序干预推荐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的规范性过程监控方法在处理顺序干预时存在局限：要么只关注单次干预，要么将多次干预视为独立事件，忽略了时间上的交互作用。而考虑这些依赖关系的方法又依赖于模拟或数据增强来训练强化学习代理，这会造成现实差距和偏差。

Method: SCOPE采用反向归纳法来估计每个候选干预行动的效果，将其影响从最终决策点向后传播到第一个决策点。通过利用因果学习器，该方法可以直接使用观测数据，而不需要构建过程近似来进行强化学习。

Result: 在现有合成数据集和新半合成数据集上的实验表明，SCOPE在优化关键绩效指标方面始终优于最先进的规范性过程监控技术。新的半合成设置基于真实事件日志，为未来的顺序规范性过程监控研究提供了可重复的基准。

Conclusion: SCOPE通过直接利用观测数据学习对齐的顺序干预推荐，解决了现有规范性过程监控方法在处理顺序干预时的局限性，为组织提供了更有效的业务流程优化工具。

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [88] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO是一个混合训练框架，通过在每个训练实例中交错使用监督微调（SFT）和强化学习（RL），解决了传统两阶段管道（SFT然后RL）的不一致问题，从而提升大语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段训练管道（先SFT后RL）存在关键不一致性：SFT强制刚性模仿，抑制了探索并导致遗忘，限制了RL的改进潜力。需要一种更高效的训练框架来统一外部监督和自我探索。

Method: TRAPO框架在每个训练实例中交错优化SFT损失（在专家前缀上）和RL损失（在模型自身补全上）。引入信任区域SFT（TrSFT）来稳定训练，最小化信任区域内的前向KL散度，但在区域外衰减优化。还包含自适应前缀选择机制，根据测量效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO一致超越了标准SFT、RL、SFT-then-RL管道以及最近的最先进方法，为推理增强的LLMs建立了强大的新范式。

Conclusion: TRAPO通过统一监督学习和强化学习，解决了传统两阶段训练的不一致问题，为提升大语言模型的复杂推理能力提供了更有效的训练框架。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [89] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: 使用神经网络重建医疗辐射场中散射辐射空间分布的研究，通过蒙特卡洛模拟生成三个复杂度递增的数据集，评估不同神经网络架构的性能


<details>
  <summary>Details</summary>
Motivation: 为介入放射学和心脏病学等医疗辐射场中的辐射防护剂量学，开发能够准确估计散射辐射空间分布的方法，以改善辐射防护

Method: 基于Geant4的蒙特卡洛模拟生成三个复杂度递增的合成数据集，使用卷积神经网络和全连接神经网络架构进行训练和评估，比较不同设计决策对重建辐射通量和能谱分布的效果

Result: 研究确定了适用于重建医疗辐射场空间分布的有效神经网络设计决策，所有数据集和训练流程已作为开源资源发布

Conclusion: 神经网络能够有效重建医疗辐射场中的散射辐射分布，为辐射防护剂量学提供了新的技术途径，开源数据集和工具促进了该领域的进一步研究

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [90] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出"多谐级联"深度学习架构，通过多谐样条包序列实现非线性函数逼近，保持全局光滑性和概率解释，使用全局线性系统训练替代梯度下降


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构缺乏理论支撑和概率解释，梯度下降训练存在收敛慢、过拟合等问题，需要一种既保持理论一致性又能高效训练的替代方法

Method: 设计多谐级联架构：由多谐样条包序列组成，每层基于随机函数理论和无差别原理严格推导；提出全局线性系统训练方法：在每个批次上求解关于固定节点"星座"处函数值的全局线性系统，实现所有层同步更新

Result: 在MNIST数据集上展示了快速学习且不过拟合的性能；训练方法可扩展性强，所有计算简化为可在GPU上高效执行的2D矩阵操作

Conclusion: 多谐级联架构提供了具有严格理论基础的深度学习替代方案，其训练方法避免了梯度下降的局限性，保持了概率解释和理论一致性，具有良好的可扩展性和实际性能

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [91] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO是一个端到端的单细胞转录组特征选择框架，通过联合学习离散基因子集和预测任务，在单个可微分架构中实现紧凑且信息丰富的基因选择。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法多为多阶段流程或依赖事后特征归因，导致选择与预测弱耦合。需要开发端到端框架，使预测任务直接指导基因选择，同时选出的基因子集又能塑造预测表示。

Method: YOTO采用端到端可微分架构，通过封闭反馈循环联合学习离散基因子集和预测任务。模型强制稀疏性，仅让选中的基因参与推理，无需训练额外下游分类器。通过多任务学习设计，模型学习跨相关目标的共享表示。

Result: 在两个代表性单细胞RNA-seq数据集上评估，YOTO始终优于最先进的基线方法。结果表明稀疏、端到端、多任务的基因子集选择能提高预测性能，并产生紧凑且有意义的基因子集。

Conclusion: YOTO框架通过端到端联合学习实现了紧凑且信息丰富的基因选择，改进了预测性能，推动了生物标志物发现和单细胞分析的发展。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [92] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 提出了联邦SARSA（FedSARSA）的理论分析，在线性函数逼近和本地训练下建立了收敛保证，首次给出了异构环境中的样本和通信复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习中的异构性（本地转移和奖励的差异）对算法收敛性带来挑战，需要理论分析来理解FedSARSA在异构环境中的性能。

Method: 开发了新的多步误差展开分析工具，用于单智能体SARSA；建立了FedSARSA在异构环境中的收敛理论，量化了异构性影响，并分析了多轮本地更新的效果。

Result: 证明了FedSARSA在异构环境中收敛，获得了样本和通信复杂度界限；展示了FedSARSA在智能体数量上实现线性加速（除马尔可夫采样的高阶项外）；数值实验验证了理论发现。

Conclusion: FedSARSA在异构联邦强化学习环境中具有理论保证的收敛性，能够有效利用多个智能体实现加速，为联邦强化学习的理论分析提供了新工具和见解。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [93] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: 提出空间感知Transformer，通过可学习协方差核将地统计归纳偏置注入自注意力机制，结合物理先验与数据驱动残差，实现高维时空过程建模


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程计算复杂度高，无法处理大规模传感器网络；现代Transformer缺乏几何归纳偏置，将空间传感器视为排列不变token，无法理解距离概念。需要结合地统计学的概率严谨性与深度学习的灵活高容量表示

Method: 提出空间感知Transformer混合架构，通过可学习协方差核将地统计归纳偏置直接注入自注意力机制。将注意力结构分解为平稳物理先验和非平稳数据驱动残差，施加软拓扑约束，优先考虑空间邻近交互同时保留复杂动态建模能力

Result: 在合成高斯随机场和真实交通基准测试中，该方法优于最先进的图神经网络。网络通过反向传播成功恢复底层过程的真实空间衰减参数（"深度变差函数"现象）。统计验证表明该方法不仅预测精度更高，而且提供良好校准的概率预测

Conclusion: 该方法有效弥合了物理感知建模与数据驱动学习之间的差距，将地统计学的理论一致性与深度学习的表达能力相结合，为大规模时空过程建模提供了新范式

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [94] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA：一种基于拉普拉斯近似的权重空间正则化方法，应用于LoRA微调，通过估计参数置信度并约束高曲率方向的更新，在保持预训练知识的同时实现高效目标域学习。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法（如LoRA）虽然能快速适应下游任务，但往往导致灾难性遗忘预训练模型的领域知识。需要解决微调过程中的知识遗忘问题。

Method: LaLoRA将拉普拉斯近似应用于LoRA权重，估计模型对每个参数的置信度，约束高曲率方向的参数更新，从而在保持轻量化的同时保护先验知识。

Result: 在Llama模型数学推理微调中，LaLoRA改善了学习-遗忘权衡，可通过正则化强度直接控制该权衡。分析了不同曲率近似方法、拉普拉斯近似数据影响和超参数鲁棒性。

Conclusion: LaLoRA通过拉普拉斯近似正则化LoRA权重，有效缓解灾难性遗忘问题，在保持参数高效的同时实现更好的知识保留与目标域学习平衡。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [95] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了ECHO基准测试，用于系统评估图神经网络在长距离信息传播方面的能力，包含三个合成图任务和两个真实化学数据集，揭示了当前GNN在长程交互建模上的局限性。


<details>
  <summary>Details</summary>
Motivation: 有效捕捉长程交互是图神经网络研究中的基本但未解决的挑战，对科学应用至关重要。当前缺乏系统评估GNN处理长距离图传播能力的基准测试。

Method: 设计了ECHO基准测试，包含三个合成图任务（单源最短路径、节点离心率、图直径），构建在具有信息瓶颈的挑战性拓扑上；以及两个真实化学数据集（ECHO-Charge和ECHO-Energy），基于密度泛函理论计算原子部分电荷和分子总能量。

Result: 对流行GNN架构的广泛基准测试揭示了明显的性能差距，强调了真正长程传播的困难，并识别了能够克服固有局限性的设计选择。

Conclusion: ECHO为评估长程信息传播设立了新标准，展示了AI for science中对长程交互建模的迫切需求，为GNN研究提供了重要的评估框架。

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [96] [Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments](https://arxiv.org/abs/2512.17771)
*Dong Chen,Zhengqing Hu,Shixing Zhao,Yibo Guo*

Main category: cs.LG

TL;DR: 提出Easy Adaptation (EA)方法，设计特定小模型(SSMs)来补充大语言模型未充分拟合的数据分布，无需访问大模型参数即可达到参数高效微调的性能，且资源需求极低。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调(PEFT)方法面临两大挑战：1) 资源成本高，在资源受限环境中不实用；2) 参数依赖性强，需要访问大模型参数，而许多领先模型已闭源仅通过API访问，且微调过程昂贵缓慢。小模型在特定分布上表现优异且资源需求极低。

Method: 提出Easy Adaptation (EA)方法，设计特定小模型(SSMs)来补充大语言模型未充分拟合的数据分布。SSMs专门针对下游任务的数据分布进行优化，与大模型协同工作，无需访问或修改大模型参数。

Result: 大量实验表明，EA在多样化任务上能够匹配PEFT的性能，且无需访问大语言模型参数，仅需极少的计算资源。

Conclusion: EA提供了一种高效、低成本的大模型适应方案，解决了PEFT方法的资源成本和参数依赖问题，特别适用于资源受限环境和闭源大模型场景。

Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.

</details>


### [97] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出可校准消歧损失(CDL)解决多示例部分标记学习(MIPL)中的校准问题，通过两种实例化方法同时提升分类准确性和校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIPL方法存在校准性能差的问题，影响分类器可靠性，需要一种同时提升分类准确性和校准性能的解决方案。

Method: 提出可校准消歧损失(CDL)，包含两种实例化：第一种基于候选标签集概率校准预测，第二种整合候选和非候选标签集概率。该损失可无缝集成到现有MIPL和PLL框架中。

Result: 在基准和真实数据集上的实验结果表明，CDL显著提升了分类和校准性能，理论分析证明了其相对于传统消歧损失的优越性。

Conclusion: CDL是一种有效的即插即用解决方案，能够同时改善MIPL和PLL框架的分类准确性和校准可靠性。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [98] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 本文研究发现ID嵌入和文本模态嵌入在序列推荐中具有互补性，提出通过独立训练和简单集成的方法来利用这种互补性，无需复杂融合架构即可达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前序列推荐模型对ID嵌入和模态嵌入的互补性缺乏理解。一些工作完全用模态嵌入替代ID嵌入，另一些则采用复杂的多阶段训练或对齐架构来联合使用两者。本文旨在研究ID和文本特征的互补性，并探索更简单有效的利用方式。

Method: 提出新的序列推荐方法：首先独立训练基于ID和基于文本的模型以保持互补性，然后通过简单的集成策略来利用这种互补性。该方法避免了复杂的融合架构和多阶段训练。

Result: 实验表明该方法优于多个竞争性序列推荐基线，证明ID和文本特征对于达到SOTA性能都是必要的，但不需要复杂的融合架构。

Conclusion: ID嵌入和文本模态嵌入在序列推荐中确实具有互补性，通过独立训练和简单集成即可有效利用这种互补性，无需复杂的融合策略。两种特征对于实现最佳性能都是必要的。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


### [99] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Wasserstein-Fisher-Rao几何的采样方法，通过引入显式修正项和加权随机微分方程，改善非凸或多模态分布下的采样效率。


<details>
  <summary>Details</summary>
Motivation: 基于分数的扩散模型在处理非凸或多模态分布时，混合速率会指数级恶化。传统Ornstein-Uhlenbeck型随机微分方程在非对数凹目标分布下采样效率低下，需要开发能改善探索能力的采样方案。

Method: 利用信息几何工具，通过Wasserstein-Fisher-Rao几何将样本空间传输与概率测度空间的垂直（反应）动力学耦合。引入显式修正项，并使用Feynman-Kac表示通过加权随机微分方程实现重加权机制。

Result: 提出了WFR-based采样动力学的初步但严谨的研究框架，阐明了其几何和算子理论结构，为未来理论和算法发展奠定了基础。

Conclusion: Wasserstein-Fisher-Rao几何为重加权机制提供了自然框架，能够改善非凸分布下的采样探索能力，为连续生成建模提供了有前景的新方向。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [100] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出RRFF-FEM方法，使用多元t分布随机特征和频率加权Tikhonov正则化，从噪声数据中学习算子，在保证精度的同时提高计算效率和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统核方法算子学习虽然准确且理论完备，但计算成本高（特别是大规模训练集时），且对噪声敏感。需要一种既能保持精度又能提高计算效率、增强噪声鲁棒性的方法。

Method: 提出正则化随机傅里叶特征方法（RRFF），结合有限元重构映射（RRFF-FEM）。使用多元t分布生成随机特征，采用频率加权Tikhonov正则化抑制高频噪声。理论分析显示当特征数N与训练样本数m满足N∝m log m时，系统条件良好。

Result: 理论证明：建立了随机特征矩阵极端奇异值的高概率界，给出了估计和泛化保证。实验验证：在平流、Burgers、达西流、Helmholtz、Navier-Stokes、结构力学等基准PDE问题上，RRFF和RRFF-FEM对噪声鲁棒，相比无正则化随机特征模型性能提升、训练时间减少，与核方法和神经算子相比保持竞争性精度。

Conclusion: RRFF-FEM方法有效解决了核方法算子学习中的计算效率和噪声敏感问题，通过正则化随机特征和有限元重构，在保持精度的同时显著提升了计算性能和鲁棒性，为从噪声数据中学习PDE解算子提供了实用解决方案。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作性定义，基于实践探究模型(PIM)，并构建了包含1000多个跨学科样本的SGI-Bench基准，用于系统评估LLM在科学研究任务中的表现。研究发现了现有模型的多个不足，并提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个连贯的科学通用智能(SGI)框架——即能够自主构思、调查和跨科学领域推理的能力。需要建立一个操作性定义和系统评估基准来推动真正参与科学发现的AI系统发展。

Method: 1. 基于实践探究模型(PIM: 深思、构思、行动、感知)定义SGI；2. 设计四个科学家对齐任务：深度研究、想法生成、干/湿实验、实验推理；3. 构建SGI-Bench基准，包含1000多个专家策划的跨学科样本，灵感来自《科学》杂志的125个重大问题；4. 引入测试时强化学习(TTRL)，在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示多个差距：深度研究任务中精确匹配率低(10-20%)；生成的想法缺乏可行性和细节；干实验代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理存在持续挑战。TTRL方法能在没有参考答案的情况下提升假设新颖性。

Conclusion: 基于PIM的定义、以工作流程为中心的基准和实证见解为真正参与科学发现的AI系统奠定了基础。研究揭示了当前LLM在科学任务中的局限性，并提出了改进方向，推动了科学通用智能的发展。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [102] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体工作流的计划感知自动上下文工程框架，通过压缩上下文提高准确性并降低计算成本


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步骤工作流中会产生快速扩展的上下文，现有压缩方法忽视了多步骤、计划感知的特性，需要新的解决方案来维持保真度、避免注意力稀释并降低推理成本

Method: 提出PAACE框架，包含PAACE-Syn（生成带压缩监督的合成工作流）和PAACE-FT（蒸馏的计划感知压缩器），通过任务相关性建模、计划结构分析、指令协同精炼和函数保留压缩来优化上下文

Result: 在AppWorld、OfficeBench和8-Objective QA基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载，蒸馏模型保留97%性能同时降低一个数量级的推理成本

Conclusion: PAACE为LLM智能体工作流提供了有效的计划感知上下文压缩方案，能够在保持性能的同时大幅降低计算开销，实现实用部署

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [103] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 本文将Rashomon效应从分类任务扩展到序列决策领域，定义了行为相同但内部结构不同的策略集合，并使用形式化验证方法进行验证，发现该效应在序列决策中存在，且Rashomon集合构建的集成策略具有更好的分布偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策中的随机性使得验证策略行为相同性比分类任务更复杂，需要新的验证方法。

Method: 将Rashomon效应定义扩展到序列决策，提出使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，以验证行为相同性。通过实验验证该效应的存在，并探索从Rashomon集合构建集成策略和宽松策略的方法。

Result: 实验证明Rashomon效应在序列决策中存在。从Rashomon集合构建的集成策略比单个策略对分布偏移具有更强的鲁棒性。从Rashomon集合导出的宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中确实存在，且具有实际应用价值。从Rashomon集合构建的集成策略能提高鲁棒性，宽松策略能降低验证成本，这为序列决策系统的设计和分析提供了新视角。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [104] [Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks](https://arxiv.org/abs/2512.17466)
*Irched Chafaa,Giacomo Bacci,Luca Sanguinetti*

Main category: eess.SY

TL;DR: 提出轻量级Transformer模型，联合预测AP聚类和功率分配，仅需用户和AP的空间坐标，无需信道估计，消除导频污染，实现线性可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型缺乏处理动态网络配置的灵活性，许多方法忽视导频污染且计算复杂度高，需要更高效、灵活的解决方案。

Method: 使用轻量级Transformer模型，仅基于用户设备和AP的空间坐标联合预测AP聚类和功率分配；采用定制化线性注意力机制高效捕捉用户-AP交互；在导频复用约束下分配用户到AP以消除导频污染。

Result: 模型能最大化最小频谱效率，提供接近最优的性能，在动态场景中确保适应性和可扩展性，数值结果验证了有效性。

Conclusion: 提出的轻量级Transformer模型解决了现有方法的局限性，实现了高效、灵活、可扩展的AP聚类和功率分配，无需信道估计开销。

Abstract: Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.

</details>


### [105] [Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy](https://arxiv.org/abs/2512.17899)
*Aditya Gahlawat,Ahmed Aboudonia,Sandeep Banik,Naira Hovakimyan,Nikolai Matni,Aaron D. Ames,Gioele Zardini,Alberto Speranzon*

Main category: eess.SY

TL;DR: 提出DRIP架构，结合TaSIL和ℓ₁-DRAC，解决模仿学习中分布偏移问题，实现可认证的自主系统控制


<details>
  <summary>Details</summary>
Motivation: 模仿学习对分布偏移敏感，特别是政策误差以及外生扰动和内生模型误差引起的分布偏移，需要可认证的解决方案

Method: 提出DRIP架构，分层控制架构整合TaSIL（处理政策误差）和ℓ₁-DRAC（处理不确定性和扰动），通过精心设计各层输入输出要求保证整个控制流程的可认证性

Result: 构建了可认证的模仿学习控制架构，能够处理多种分布偏移源，为整合学习组件和基于模型的决策提供了路径

Conclusion: DRIP架构为实现完全可认证的自主系统管道铺平了道路，通过分层控制方法整合学习组件和可认证的基于模型决策

Abstract: Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [106] [Learning vertical coordinates via automatic differentiation of a dynamical core](https://arxiv.org/abs/2512.17877)
*Tim Whittaker,Seth Taylor,Elsa Cardoso-Bihlo,Alejandro Di Luca,Alex Bihlo*

Main category: physics.ao-ph

TL;DR: 提出NEUVE坐标框架，通过可微分动力学核心中的可学习神经网络优化地形跟随坐标，减少数值误差和虚假运动


<details>
  <summary>Details</summary>
Motivation: 传统地形跟随坐标在陡峭地形上会将网格结构印刻到解中，产生虚假的水平/垂直运动，而标准坐标使用需要手动调参的启发式衰减函数

Method: 开发端到端可微分的二维非静力欧拉方程求解器，提出基于积分变换神经网络的NEUVE地形跟随坐标，使用自动微分计算精确几何度量项

Result: 学习到的坐标在非线性统计基准中将均方误差降低1.4-2倍，消除了陡峭地形上的虚假垂直速度条纹

Conclusion: NEUVE框架通过将坐标参数化为可学习组件，优化了网格结构，减少了数值误差，为大气模型中的坐标设计提供了新方法

Abstract: Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [107] [Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems](https://arxiv.org/abs/2512.17259)
*Abhivansh Gupta*

Main category: cs.MA

TL;DR: 提出基于可验证性的架构OPERA，通过运行时证明、审计代理和挑战响应协议来检测和修复LLM智能体的未对齐行为，将评估重点从未对齐可能性转向检测速度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能体变得更加自主和多模态，确保其可控、可审计且忠实于部署者意图变得至关重要。现有基准主要衡量未对齐行为的倾向，但需要转向更关注检测和修复能力。

Method: 提出可验证性优先架构：1) 使用密码学和符号方法集成运行时行为证明；2) 嵌入轻量级审计代理，通过约束推理持续验证意图与行为；3) 对高风险操作实施挑战-响应证明协议。同时引入OPERA基准套件和评估协议。

Result: 开发了OPERA基准套件，用于衡量：i) 未对齐行为的可检测性；ii) 在隐蔽策略下的检测时间；iii) 可验证性机制对抗提示和角色注入攻击的鲁棒性。

Conclusion: 该方法将评估重点从"未对齐可能性有多大"转向"未对齐行为能被多快、多可靠地检测和修复"，为LLM智能体的安全部署提供了更实用的可验证性框架。

Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [108] [Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number](https://arxiv.org/abs/2512.17885)
*Frederick A. Gent,Mordecai-Mark Mac Low,Maarit J. Korpi-Lagg,Touko Puro,Matthias Reinhardt*

Main category: astro-ph.GA

TL;DR: 该论文通过高分辨率模拟研究星系磁场的湍流分量，发现小尺度发电机产生的湍流磁场强度在仅中等磁普朗特数下就趋于渐近值，这有助于改进全球星系模型中的磁场参数化。


<details>
  <summary>Details</summary>
Motivation: 现有星系磁场模拟存在矛盾：大尺度和小尺度发电机联合模拟得到的平均磁场与观测一致，但湍流磁场远弱于观测；而小尺度发电机模拟得到的湍流磁场又比理论预期低一个数量级。需要理解这种差异并改进星系磁场模型。

Method: 使用GPU加速的Pencil Code和Astaroth进行高分辨率模拟，研究超新星驱动的星系发电机，包含加热和冷却过程，在周期性计算域中分析不同磁普朗特数下的磁场演化。

Result: 模拟显示小尺度发电机产生的湍流磁场强度在仅中等磁普朗特数（几百）下就趋于渐近值，远低于星际介质的实际物理值，这与先前等温可压缩流的研究结果一致。

Conclusion: 湍流磁场在相对较低的磁普朗特数下达到渐近行为，这一发现为全球星系模型中磁场参数化提供了关键特征，有助于更准确地模拟星系磁场结构。

Abstract: Magnetic fields are critical at many scales to galactic dynamics and structure, including multiphase pressure balance, dust processing, and star formation. Dynamo action determines their dynamical structure and strength. Simulations of combined large- and small-scale dynamos have successfully developed mean fields with strength and topology consistent with observations but with turbulent fields much weaker than observed, while simulations of small-scale dynamos with parameters relevant to the interstellar medium yield turbulent fields an order of magnitude below the values observed or expected theoretically. We use the Pencil Code accelerated on GPUs with Astaroth to perform high-resolution simulations of a supernova-driven galactic dynamo including heating and cooling in a periodic domain. Our models show that the strength of the turbulent field produced by the small-scale dynamo approaches an asymptote at only modest magnetic Prandtl numbers. This allows us to use these models to suggest the essential characteristics of this constituent of the magnetic field for inclusion in global galactic models. The asymptotic limit occurs already at magnetic Prandtl number of only a few hundred, many orders of magnitude below physical values in the the interstellar medium and consistent with previous findings for isothermal compressible flows.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [109] [Line Cover and Related Problems](https://arxiv.org/abs/2512.17268)
*Matthias Bentert,Fedor v. Fomin,Petr A. Golovach,Souvik Saha,Sanjay Seetharaman,Anannya Upasana*

Main category: cs.CG

TL;DR: 本文研究了线覆盖问题的两个推广：线聚类和超平面覆盖，以及更一般的投影聚类问题。在线性覆盖问题固定参数可解的情况下，线聚类是W[1]-难的，超平面覆盖是W[2]-难的。最后给出了投影聚类的算法。


<details>
  <summary>Details</summary>
Motivation: 研究经典线覆盖问题的自然推广，探索线聚类和超平面覆盖问题的计算复杂性，这些在机器学习、数据分析和计算几何中有重要应用。

Method: 使用参数化复杂性理论分析问题，通过证明W[1]-硬度和W[2]-硬度建立下界，同时设计投影聚类的算法，其时间复杂度为n^{O(dk(r+1))}。

Result: 线聚类是W[1]-难的，除非指数时间假设失败，否则不存在n^{o(k)}时间算法。超平面覆盖是W[2]-难的（仅参数化k）。投影聚类算法时间复杂度为n^{O(dk(r+1))}。

Conclusion: 线覆盖问题的不同推广在参数化复杂性上表现出显著差异：线聚类比线覆盖更难，超平面覆盖又比线聚类更难。投影聚类算法推广了k-means聚类算法。

Abstract: We study extensions of the classic \emph{Line Cover} problem, which asks whether a set of $n$ points in the plane can be covered using $k$ lines. Line Cover is known to be NP-hard, and we focus on two natural generalizations. The first is \textbf{Line Clustering}, where the goal is to find $k$ lines minimizing the sum of squared distances from the input points to their nearest line. The second is \textbf{Hyperplane Cover}, which asks whether $n$ points in $\mathbb{R}^d$ can be covered by $k$ hyperplanes.
  We also study the more general \textbf{Projective Clustering} problem, which unifies both settings and has applications in machine learning, data analysis, and computational geometry. In this problem, one seeks $k$ affine subspaces of dimension $r$ that minimize the sum of squared distances from the given points in $\mathbb{R}^d$ to the nearest subspace.
  Our results reveal notable differences in the parameterized complexity of these problems. While Line Cover is fixed-parameter tractable when parameterized by $k$, we show that Line Clustering is W[1]-hard with respect to $k$ and does not admit an algorithm with running time $n^{o(k)}$ unless the Exponential Time Hypothesis fails. Hyperplane Cover is NP-hard even for $d=2$, and prior work of Langerman and Morin [Discrete & Computational Geometry, 2005] showed that it is fixed-parameter tractable when parameterized by both $k$ and $d$. We complement this by proving that Hyperplane Cover is W[2]-hard when parameterized by $k$ alone.
  Finally, we present an algorithm for Projective Clustering running in $n^{O(dk(r+1))}$ time. This bound matches our lower bound for Line Clustering and generalizes the classic algorithm for $k$-Means Clustering ($r=0$) by Inaba, Katoh, and Imai [SoCG 1994].

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [110] [Disentangled representations via score-based variational autoencoders](https://arxiv.org/abs/2512.17127)
*Benjamin S. H. Lyo,Eero P. Simoncelli,Cristina Savin*

Main category: stat.ML

TL;DR: SAMI是一种无监督表示学习方法，结合了扩散模型和VAE的理论框架，通过分数引导学习可解释的表示，能够自动捕获数据中的有意义结构。


<details>
  <summary>Details</summary>
Motivation: 动机是将扩散模型中隐含的结构信息通过变分自编码器变得显式和可解释，利用扩散模型的分数引导来学习有意义的表示。

Method: 通过统一扩散模型和VAE的证据下界，形成原则性目标，通过底层扩散过程的分数引导学习表示。

Result: 在合成数据中恢复真实生成因子，从复杂自然图像中学习因子化语义潜在维度，视频序列编码的潜在轨迹更直，能从预训练扩散模型中提取有用表示。

Conclusion: 扩散模型中的隐式结构信息可以通过与变分自编码器的协同组合变得显式和可解释。

Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.

</details>


### [111] [Sharp Structure-Agnostic Lower Bounds for General Functional Estimation](https://arxiv.org/abs/2512.17341)
*Jikai Jin,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 本文系统研究了结构无关估计器的最优误差率，证明了双重稳健学习在ATE估计中达到最优结构无关误差率，并扩展分析到一般函数类，确立了去偏/双重机器学习的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统最优估计方法依赖强结构假设，在实际中可能被错误设定且部署复杂。这引发了对结构无关方法（不施加结构先验的去偏黑盒估计）的兴趣，理解这些方法的基本极限至关重要。

Method: 首先分析ATE估计，证明双重稳健学习达到最优结构无关误差率；然后扩展到依赖未知干扰函数的一般函数类，分析去偏/双重机器学习的最优性；区分双重稳健可达到和不可达到两种机制，推导一阶去偏的最优率。

Result: 1) 对于ATE估计，双重稳健学习达到最优结构无关误差率；2) 对于一般函数类，去偏/双重机器学习在两种机制下都是最优的；3) 推导了显式最优率，恢复现有结果并扩展到其他估计量。

Conclusion: 结果为广泛使用的一阶去偏方法提供了理论验证，并为在没有结构假设情况下寻求最优方法的实践者提供了指导。本文推广并包含了作者先前在ATE下界方面的工作。

Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

</details>


### [112] [Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing](https://arxiv.org/abs/2512.17426)
*Xiaosi Gu,Ayaka Sakata,Tomoyuki Obuchi*

Main category: stat.ML

TL;DR: 本文提出了一种用于稀疏信号重建的1RSB-AMP算法，通过一阶复本对称破缺扩展近似消息传递，改进了传统RS-AMP的性能，特别是在算法发散的区域。


<details>
  <summary>Details</summary>
Motivation: 传统基于SCAD惩罚的稀疏信号重建中，复本对称（RS）AMP算法在某些参数区域会发散，且性能有限。需要开发更强大的算法来克服这些限制，接近贝叶斯最优性能。

Method: 从一阶复本对称破缺（1RSB）的信念传播出发，推导出1RSB-AMP的显式更新规则和相应的状态演化方程（1RSB-SE）。提出通过最小化发散区域大小（而非传统的零复杂度条件）来确定Parisi参数的新准则，并结合非凸控制协议。

Result: 1RSB-AMP与1RSB-SE在宏观层面高度一致，即使在RS-AMP发散的区域。新方法改进了完美重建的算法极限，数值实验证实了这一改进，虽然增益有限且仍略低于贝叶斯最优阈值。

Conclusion: 1RSB-AMP扩展为稀疏信号重建提供了比RS-AMP更强大的算法框架，通过新的Parisi参数确定准则改善了性能。虽然改进有限，但为理解1RSB相在该问题中的热力学性质提供了新见解。

Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.

</details>


### [113] [Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design](https://arxiv.org/abs/2512.17659)
*Madhav R. Muthyala,Farshud Sorourifar,Tianhong Tan,You Peng,Joel A. Paulson*

Main category: stat.ML

TL;DR: 提出"生成-优化"框架用于多目标分子设计，使用qPMHI采集函数进行批量选择，在可持续能源存储应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 分子设计需要满足多个相互冲突的目标，化学空间巨大且高保真模拟成本高。现有方法依赖连续潜在空间，存在架构纠缠和可扩展性挑战

Method: 提出模块化的"生成-优化"框架：每轮使用生成模型构建大量候选分子池，然后使用新颖的qPMHI采集函数选择最可能扩展帕累托前沿的批次候选

Result: 在合成基准测试和应用驱动任务中显著优于现有方法，在可持续能源存储案例中快速发现新型、多样且高性能的有机阴极材料

Conclusion: 提出的模块化框架解决了现有方法的局限性，qPMHI的加性分解特性实现了精确、可扩展的批量选择，为多目标分子发现提供了有效解决方案

Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.

</details>


### [114] [Imputation Uncertainty in Interpretable Machine Learning Methods](https://arxiv.org/abs/2512.17689)
*Pegah Golchian,Marvin N. Wright*

Main category: stat.ML

TL;DR: 比较不同插补方法对可解释机器学习方法置信区间覆盖率的影响，发现单次插补会低估方差，而多重插补在大多数情况下接近名义覆盖率


<details>
  <summary>Details</summary>
Motivation: 现实数据中经常出现缺失值，这会影响可解释机器学习方法的解释。现有研究考虑了偏差并显示不同插补方法可能导致模型解释不同，但忽略了额外的插补不确定性及其对方差和置信区间的影响

Method: 比较不同插补方法对三种可解释机器学习方法置信区间覆盖率的影响：排列特征重要性、部分依赖图和Shapley值

Result: 单次插补会导致方差低估，在大多数情况下只有多重插补接近名义覆盖率

Conclusion: 在处理缺失值时，需要考虑插补不确定性对可解释机器学习方法置信区间的影响，多重插补比单次插补更可靠

Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [115] [PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology](https://arxiv.org/abs/2512.17517)
*Siemen Brussee,Pieter A. Valkema,Jurre A. J. Weijer,Thom Doeleman,Anne M. R. Schrader,Jesper Kers*

Main category: cs.CV

TL;DR: PathBench-MIL是一个用于组织病理学中多实例学习的开源AutoML和基准测试框架，自动化MIL流程并提供可复现的模型评估


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学中多实例学习缺乏标准化、可复现的基准测试框架的问题，简化MIL流程构建和模型评估

Method: 开发开源AutoML框架，自动化端到端MIL流程（预处理、特征提取、MIL聚合），集成可视化工具、统一配置系统和模块化扩展性

Result: 创建了PathBench-MIL框架，支持数十种MIL模型和特征提取器的可复现基准测试，已在GitHub开源

Conclusion: PathBench-MIL为组织病理学MIL研究提供了标准化、可扩展的实验平台，促进快速实验和跨数据集任务比较

Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL

</details>


### [116] [CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency](https://arxiv.org/abs/2512.17213)
*Xiao Liang,Yuxuan An,Di Wang,Jiawei Hu,Zhicheng Jiao,Bin Jing,Quan Wang*

Main category: cs.CV

TL;DR: CheXPO-v2提出了一种新的医学视觉语言模型对齐框架，通过知识图谱一致性奖励机制实现过程监督，有效减少幻觉并提高临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型容易产生幻觉，影响临床可靠性。基于强化学习的对齐方法（如GRPO）依赖稀疏的结果奖励，导致模型"过度思考"——生成冗长、复杂且不可验证的思维链来证明答案，这种结果导向的方法掩盖了事实错误并带来安全风险。

Method: CheXPO-v2框架从结果监督转向过程监督，核心创新是知识图谱一致性奖励机制，通过实体关系匹配实现。方法将推理步骤明确解析为结构化的"疾病、关系、解剖部位"三元组，提供细粒度监督，在原子级别惩罚不连贯逻辑和幻觉。结合困难样本挖掘策略，显著提升性能。

Result: 在MIMIC-CXR-VQA等基准测试中，CheXPO-v2显著优于GRPO和最先进模型。仅使用5k样本就达到了新的最先进准确率，展示了卓越的数据效率，同时产生临床合理且可验证的推理。

Conclusion: CheXPO-v2通过过程监督和知识图谱一致性奖励机制，有效解决了医学视觉语言模型的幻觉问题，提高了临床可靠性和安全性，同时保持了数据效率。项目源代码已公开。

Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.

</details>


### [117] [MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation](https://arxiv.org/abs/2512.17450)
*Jon Muhovič,Janez Perš*

Main category: cs.CV

TL;DR: 提出MULTIAQUA多模态海事数据集，包含RGB、热成像、IR、LIDAR等同步校准数据，用于训练在恶劣能见度条件下仍能保持可靠性能的多模态方法


<details>
  <summary>Details</summary>
Motivation: 无人水面车辆在运行中会遇到各种视觉环境，有些条件（如恶劣天气、夜间）难以解析。虽然大多数情况可用彩色相机图像解决，但某些天气和光照条件需要额外信息。现有海事数据有限，需要扩展多模态数据资源。

Method: 提出MULTIAQUA多模态海事数据集，包含RGB、热成像、红外、激光雷达等多种传感器的同步、校准和标注数据。开发训练方法使多模态方法仅使用白天图像就能训练，从而在近乎完全黑暗的条件下仍保持可靠性能。

Result: 在困难的夜间测试集上评估了多种多模态方法，展示了所提数据集的价值。提出的训练方法使多模态方法能够以更稳健的方式训练，即使在近乎完全黑暗的条件下也能保持可靠性能。

Conclusion: MULTIAQUA数据集为开发监督方法提供了多模态数据资源，能够从不同模态中提取有用信息，确保在各种能见度条件下都能提供高质量的场景解析。提出的训练方法简化了数据采集、标注和训练过程，仅需白天图像就能训练出稳健的深度神经网络。

Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.

</details>


### [118] [TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis](https://arxiv.org/abs/2512.17488)
*Almustapha A. Wakili,Adamu Hussaini,Abubakar A. Musa,Woosub Jung,Wei Yu*

Main category: cs.CV

TL;DR: TwinSegNet：一种结合混合ViT-UNet模型与个性化数字孪生的隐私保护联邦学习框架，用于准确、实时的脑肿瘤分割，在异构MRI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤分割的深度学习方法依赖集中式数据收集，存在隐私问题且难以在不同机构间泛化。需要一种既能保护隐私又能实现个性化分割的解决方案。

Method: 提出TwinSegNet联邦学习框架，结合卷积编码器和Vision Transformer瓶颈来捕获局部和全局上下文。每个机构在私有数据上微调全局模型形成个性化数字孪生。

Result: 在九个异构MRI数据集（包括BraTS 2019-2021）上评估，TwinSegNet获得高达0.90%的Dice分数，敏感性和特异性超过90%，在非独立同分布客户端分布中表现稳健。

Conclusion: TwinSegNet在保护隐私的同时不牺牲性能，为多机构临床环境提供了可扩展的个性化分割方案，满足严格的数据保密要求。

Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.

</details>


### [119] [Visually Prompted Benchmarks Are Surprisingly Fragile](https://arxiv.org/abs/2512.17875)
*Haiwen Feng,Long Lian,Lisa Dunlap,Jiahao Shu,XuDong Wang,Renhao Wang,Trevor Darrell,Alane Suhr,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 现有视觉语言模型在视觉提示基准测试中表现脆弱，微小细节变化（如标记颜色、大小）就能显著改变模型排名，甚至让弱模型超越强模型。作者创建了VPBench基准来缓解这种不稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型时，需要测试模型独立分析视觉内容的能力，而不受文本先验影响。现有视觉提示基准测试存在脆弱性，微小细节变化就能显著影响模型表现和排名，这影响了基准测试的可靠性。

Method: 评估9个常用开源和闭源VLM在两个视觉提示任务上，分析视觉标记设计、数据集大小等细节对性能的影响。创建VPBench基准，包含16种视觉标记变体，以缓解不稳定性。

Result: 视觉标记的微小变化（如颜色从红变蓝）能完全改变模型排名；稍微增大标记大小能让InternVL3-8B与更大的专有模型如Gemini 2.5 Pro表现相当；JPEG压缩级别等低级推理选择也能改变模型排名。这些影响在视觉提示基准中比传统语义VLM评估大得多。

Conclusion: 视觉提示基准测试对实现细节非常敏感，这影响了评估的可靠性。VPBench通过包含多种视觉标记变体来缓解这种脆弱性，为更稳健的VLM评估提供了工具。

Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.

</details>


### [120] [RadarGen: Automotive Radar Point Cloud Generation from Cameras](https://arxiv.org/abs/2512.17897)
*Tomer Borreda,Fangqiang Ding,Sanja Fidler,Shengyu Huang,Or Litany*

Main category: cs.CV

TL;DR: RadarGen：基于扩散模型，从多视角相机图像生成逼真的汽车雷达点云，通过BEV表示和视觉线索引导实现跨模态生成仿真


<details>
  <summary>Details</summary>
Motivation: 现有的视觉数据集和仿真框架主要关注相机数据，缺乏高质量的雷达数据生成方法。为了构建统一的多模态生成仿真系统，需要能够从视觉输入生成物理上合理的雷达测量数据的方法。

Method: 1. 将雷达测量表示为鸟瞰图形式，编码空间结构、雷达截面积和速度属性
2. 采用高效的图像潜在扩散模型适应雷达领域
3. 从预训练基础模型中提取BEV对齐的深度、语义和运动线索来引导生成过程
4. 通过轻量级恢复步骤从生成的地图重建点云

Result: 在大规模驾驶数据上的评估表明，RadarGen能够捕捉特征性的雷达测量分布，并缩小了在真实数据上训练的感知模型的差距，标志着跨传感模态统一生成仿真的进展。

Conclusion: RadarGen通过从视觉输入生成物理上合理的雷达数据，为实现可扩展的多模态生成仿真提供了一条有前景的路径，使现有视觉数据集和仿真框架能够兼容雷达数据生成。

Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.

</details>


### [121] [Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting](https://arxiv.org/abs/2512.17908)
*Ananta R. Bhattarai,Helge Rhodin*

Main category: cs.CV

TL;DR: Re-Depth Anything：一个测试时自监督框架，通过融合Depth Anything V2和2D扩散模型先验，在无需标签的情况下提升单目深度估计的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型如Depth Anything V2在处理与训练分布差异较大的真实世界图像时表现不佳，存在领域差距问题。

Method: 采用测试时自监督框架，通过重新照明预测深度图并增强输入图像，利用形状从阴影（SfS）线索和分数蒸馏采样（SDS）替代传统光度重建。冻结编码器，仅更新中间嵌入并微调解码器以防止优化崩溃。

Result: 在多个基准测试中，Re-Depth Anything相比DA-V2在深度准确性和真实性方面取得显著提升。

Conclusion: 该方法展示了通过增强几何推理实现自监督的新途径，有效弥合了单目深度估计中的领域差距。

Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [122] [Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study](https://arxiv.org/abs/2512.17703)
*Shengdu Chai,Chen Lin,Xinyang Dong,Yuqiang Li,Wanli Ouyang,Lei Wang,X. C. Xie*

Main category: cond-mat.str-el

TL;DR: 通过量子蒙特卡洛和深度神经网络波函数方法，在130 GPa附近发现氢的高压相具有Cmcm空间群对称性，该结构在经典计算中不稳定，需要量子多体处理。


<details>
  <summary>Details</summary>
Motivation: 高压固态氢的晶体结构是基础科学中的重要未解问题，特别是130 GPa附近的破缺对称相，其电子和核自由度之间存在复杂耦合，需要重新审视。

Method: 开发基于深度神经网络波函数的第一性原理量子蒙特卡洛框架，在恒压系综中同时量子力学处理电子和原子核，测试结构稳定性至96个原子。

Result: 发现未报道的Cmcm空间群对称性基态结构候选，该结构定量匹配实验状态方程和X射线衍射图，且与现有拉曼和红外光谱数据兼容。静态DFT计算显示该结构在玻恩-奥本海默势能面上是动态不稳定的鞍点。

Conclusion: Cmcm结构需要完全量子多体处理才能稳定存在，这为高压氢相图提供了新见解，并呼吁进一步的实验验证。

Abstract: The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [123] [Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions](https://arxiv.org/abs/2512.17473)
*Atharva Awari,Nicolas Gillis,Arnaud Vandaele*

Main category: eess.SP

TL;DR: 提出基于ADMM的非线性矩阵分解算法，支持多种非线性函数和损失函数，在真实数据集上验证了其适用性、效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分解通常假设线性关系，但许多实际应用需要非线性建模。本文旨在开发一个通用的非线性矩阵分解框架，能够处理各种非线性函数和损失度量。

Method: 基于交替方向乘子法（ADMM）设计算法，支持多种非线性函数（如ReLU、平方函数、MinMax变换）和损失函数（最小二乘、ℓ1范数、KL散度）。

Result: 算法在真实数据集上展示了良好的性能，能够有效处理非负稀疏数据近似、概率电路表示和推荐系统等多种应用场景。

Conclusion: 提出的ADMM框架为非线性矩阵分解提供了灵活高效的解决方案，具有广泛的应用潜力，可扩展到其他非线性函数和度量标准。

Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [124] [Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function](https://arxiv.org/abs/2512.17245)
*Deriyan Senjaya,Stephen Ekaputra Limantoro*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出WT-RDF+方法，通过机器学习优化WT-RDF参数，提高非晶材料结构分析的精度，特别适用于Ge-Se系统。


<details>
  <summary>Details</summary>
Motivation: 非晶材料因其不规则、非周期结构而难以分析。WT-RDF方法虽然能预测RDF峰值和趋势，但在振幅精度方面存在局限，影响配位数等定量分析。

Method: 采用机器学习方法优化WT-RDF参数，开发出增强的WT-RDF+框架。该方法在二元Ge0.25Se0.75和三元Agx(Ge0.25Se0.75)100-x系统上进行验证。

Result: WT-RDF+显著提高了峰值预测精度，优于RBF和LSTM等基准ML模型，即使在仅使用25%二元数据集训练的情况下也能取得良好效果。

Conclusion: WT-RDF+是分析非晶材料结构的稳健可靠模型，特别适用于Ge-Se系统，支持相变薄膜的高效设计和开发，可用于下一代电子器件。

Abstract: Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [125] [Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification](https://arxiv.org/abs/2512.16964)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: PseudoColorViT-Alz：一种结合伪彩色增强与Vision Transformer的MRI图像分类框架，在阿尔茨海默病四分类任务上达到99.79%准确率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型难以从MRI扫描中有效提取阿尔茨海默病的细微结构变化特征，标准灰度MRI图像中的解剖纹理和对比度线索往往不够明显。

Method: 提出PseudoColorViT-Alz框架，通过伪彩色表示增强MRI图像，结合Vision Transformer的全局特征学习能力，放大解剖纹理和对比度线索。

Result: 在OASIS-1数据集四分类任务上达到99.79%准确率和100% AUC，超越2024-2025年CNN和Siamese网络方法（96.1%-99.68%准确率）。

Conclusion: 伪彩色增强与Vision Transformer结合能显著提升MRI阿尔茨海默病分类性能，为临床决策和早期检测提供了有前景的工具。

Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.

</details>


### [126] [Resource-efficient medical image classification for edge devices](https://arxiv.org/abs/2512.17515)
*Mahsa Lavaei,Zahra Abadi,Salar Beigzad,Alireza Maleki*

Main category: eess.IV

TL;DR: 该研究探索了通过模型量化技术实现医疗图像分类的资源高效方法，显著降低计算开销和内存需求，同时保持分类准确性，使AI驱动的医疗诊断能够在资源受限的边缘设备上部署。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类对准确及时诊断至关重要，但深度学习模型在资源受限的边缘设备上部署面临计算和内存限制的挑战，需要资源高效的方法。

Method: 采用模型量化技术，包括量化感知训练（QAT）和训练后量化（PTQ）方法，专门针对边缘设备进行优化，降低模型参数和激活的精度。

Result: 量化模型实现了模型大小和推理延迟的显著降低，能够在边缘硬件上实现实时处理，同时保持临床可接受的诊断准确性。

Conclusion: 该研究为在偏远和资源有限环境中部署AI驱动的医疗诊断提供了实用途径，增强了医疗技术的可访问性和可扩展性。

Abstract: Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.

</details>


### [127] [SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis](https://arxiv.org/abs/2512.17585)
*N. A. Adarsh Pritam,Jeba Shiney O,Sanyam Jain*

Main category: eess.IV

TL;DR: SkinGenBench研究预处理复杂度与生成模型选择对皮肤镜图像合成增强及黑色素瘤诊断的影响，发现生成架构选择比预处理复杂度更重要，StyleGAN2-ADA优于DDPMs，合成数据增强显著提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统评估预处理复杂度与生成模型选择如何影响皮肤镜图像的合成增强效果，以及这些合成图像对下游黑色素瘤诊断任务的实用价值。当前缺乏对这两个因素相互作用的系统性研究。

Method: 使用包含14,116张皮肤镜图像的数据集（来自HAM10000和MILK10K），评估两种代表性生成范式：StyleGAN2-ADA和DDPMs。在基础几何增强和高级伪影去除两种预处理流程下生成合成图像，使用FID、KID、IS等感知和分布度量评估图像质量，并通过特征空间分析和五个下游分类器的诊断性能评估合成图像的实用性。

Result: StyleGAN2-ADA在图像保真度方面表现更好，获得最低的FID（~65.5）和KID（~0.05），而扩散模型生成更高方差的样本但感知保真度较低。高级伪影去除仅带来边际改进，可能抑制了临床相关的纹理线索。合成数据增强显著提升黑色素瘤检测性能，使F1分数绝对提升8-15%，ViT-B/16达到F1~0.88和ROC-AUC~0.98，比未增强基线提升约14%。

Conclusion: 生成架构选择比预处理复杂度对图像保真度和诊断实用性影响更大；StyleGAN2-ADA优于DDPMs；高级预处理对生成质量和下游诊断增益有限；合成数据增强能显著提升黑色素瘤诊断性能，具有重要临床应用价值。

Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench

</details>


### [128] [Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data](https://arxiv.org/abs/2512.17759)
*Rahul Ravi,Ruizhe Li,Tarek Abdelfatah,Stephen Chan,Xin Chen*

Main category: eess.IV

TL;DR: 该研究开发了基于纵向CE-MRI和临床数据的机器学习模型，用于预测乳腺癌患者新辅助化疗后的病理完全缓解和5年无复发生存状态，通过图像配准方法显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 预测乳腺癌患者对新辅助化疗的治疗反应对于个性化治疗决策至关重要。传统方法难以准确预测病理完全缓解和长期生存结果，需要开发更有效的预测模型。

Method: 提出包含肿瘤分割、图像配准、特征提取和预测建模的完整框架。使用图像配准方法从不同时间点的原始肿瘤部位提取可比较的MRI特征，监测NACT过程中的肿瘤内变化。比较了四种特征提取器（一种影像组学和三种深度学习）与三种特征选择方法和四种机器学习模型的组合。

Result: 图像配准方法显著提升了预测模型性能。在PCR和RFS分类任务中，基于影像组学特征训练的Logistic回归模型表现最佳：PCR分类AUC 0.88，准确率0.85；RFS分类AUC 0.78，准确率0.72。

Conclusion: 图像配准方法显著改善了纵向特征学习在预测PCR和RFS方面的性能。影像组学特征提取器比预训练的深度学习特征提取器更有效，具有更高的性能和更好的可解释性。

Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.

</details>


### [129] [MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation](https://arxiv.org/abs/2512.17774)
*Saikat Roy,Yannick Kirchhoff,Constantin Ulrich,Maximillian Rokuss,Tassilo Wald,Fabian Isensee,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: MedNeXt-v2：一种用于3D医学图像分割的复合缩放3D ConvNeXt架构，通过改进的微架构和数据缩放实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有大规模监督预训练主要关注增加数据集规模，但忽略了主干网络是否在规模上成为有效的表示学习器。本文旨在填补这一空白，重新审视基于ConvNeXt的架构用于体积分割。

Method: 1. 基准测试现有主干网络，发现更强的从头开始性能可靠预测预训练后的下游性能；2. 引入MedNeXt-v2，采用3D全局响应归一化模块，使用深度、宽度和上下文缩放改进架构；3. 在18k CT体积上进行预训练，并在六个CT和MR基准测试（144个结构）上进行微调。

Result: MedNeXt-v2在六个具有挑战性的CT和MR基准测试上实现最先进性能，优于七个公开发布的预训练模型。发现：更强的骨干网络在相似数据上产生更好结果，表示缩放对病理分割有不成比例的益处，模态特定预训练在全微调后提供可忽略的益处。

Conclusion: MedNeXt-v2是3D医学图像分割中大规模监督表示学习的强大骨干网络。代码和预训练模型已在官方nnUNet仓库中提供。

Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [130] [A decomposition approach for large virtual network embedding](https://arxiv.org/abs/2512.17414)
*Amal Benhamiche,Pierre Fouilhoux,Lucas Létocart,Nancy Perrot,Alexis Schneider*

Main category: cs.DM

TL;DR: 提出基于列生成和分解的虚拟网络嵌入新方法，改进大规模实例的求解质量和下界


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法在资源稀缺时无法找到解或解质量差，需要更有效的VNE求解方法

Method: 提出新的整数线性规划模型，基于虚拟网络自动分区的分解方案，列生成方法，以及高效的Price-and-Branch启发式

Result: 相比现有方法能计算更好的下界，对大规模实例提供有效求解

Conclusion: 新方法显著改进虚拟网络嵌入问题的求解质量和效率

Abstract: Virtual Network Embedding (VNE) is the core combinatorial problem of Network Slicing, a 5G technology which enables telecommunication operators to propose diverse service-dedicated virtual networks, embedded onto a common substrate network. VNE asks for a minimum-cost mapping of a virtual network on a substrate network, encompassing simultaneous node placement and edge routing decisions. On a benchmark of large virtual networks with realistic topologies we compiled, the state-of-the art heuristics often provide expensive solutions, or even fail to find a solution when resources are sparse. We introduce a new integer linear formulation together with a decomposition scheme based on an automatic partition of the virtual network. This results in a column generation approach whose pricing problems are also VNE problems. This method allows to compute better lower bounds than state-of-the-art methods. Finally, we devise an efficient Price-and-Branch heuristic for large instances.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [131] [Do Foundational Audio Encoders Understand Music Structure?](https://arxiv.org/abs/2512.17209)
*Keisuke Toyama,Zhi Zhong,Akira Takahashi,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 本文系统评估了11种预训练基础音频编码器在音乐结构分析任务上的表现，发现基于音乐数据、采用掩码语言建模自监督学习的编码器效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练基础音频编码器在音乐信息检索的其他任务上表现出色，但在音乐结构分析方面的应用仍未被充分探索。现有研究仅评估了少数编码器，且不清楚学习方法、训练数据和模型上下文长度等因素对音乐结构分析性能的影响。

Method: 对11种不同类型的预训练基础音频编码器进行全面的实验评估，系统分析学习方法（自监督学习vs监督学习）、训练数据（音乐数据vs通用音频数据）和模型上下文长度等因素对音乐结构分析性能的影响。

Result: 实验结果表明，基于音乐数据、采用掩码语言建模自监督学习方法训练的音频编码器在音乐结构分析任务上表现最为出色。这些编码器能够更好地捕捉音乐的结构特征。

Conclusion: 本研究明确了影响预训练基础音频编码器在音乐结构分析任务上性能的关键因素，为未来音乐结构分析研究提供了重要指导，特别是推荐使用基于音乐数据的掩码语言建模自监督学习方法。

Abstract: In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.

</details>


### [132] [LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection](https://arxiv.org/abs/2512.17281)
*Ioannis Stylianou,Achintya kr. Sarkar,Nauman Dawalatabad,James Glass,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: LibriVAD是一个大规模、可扩展的开源语音活动检测数据集，通过系统控制信噪比和静默-语音比来提升VAD模型在噪声和未见条件下的鲁棒性，实验显示Vision Transformer结合MFCC特征优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VAD研究面临的主要限制是缺乏大规模、系统控制且公开可用的数据集，特别是在噪声、多样化和未见声学条件下的鲁棒性检测。

Method: 基于LibriSpeech构建LibriVAD数据集，添加真实和合成噪声源，系统控制信噪比和静默-语音比；评估多种特征-模型组合，包括波形、MFCC、Gammatone滤波器组倒谱系数，并引入Vision Transformer架构用于VAD。

Result: Vision Transformer结合MFCC特征在可见、未见和分布外条件下均优于现有VAD模型；增加数据集规模和平衡静默-语音比能显著提升模型在分布外条件下的泛化能力。

Conclusion: LibriVAD数据集和Vision Transformer架构为VAD研究提供了新基准，公开数据集、训练模型和代码将促进可复现性和VAD研究进展。

Abstract: Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.

</details>


### [133] [When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems](https://arxiv.org/abs/2512.17562)
*Sujal Chondhekar,Vasanth Murukuri,Rushabh Vasani,Sanika Goyal,Rajshree Badami,Anushree Rana,Sanjana SN,Karthik Pandia,Sulabh Katiyar,Neha Jagadeesh,Sankalp Gulati*

Main category: cs.SD

TL;DR: 现代大规模ASR模型在噪声环境下使用语音增强预处理反而会降低识别性能，原始噪声音频的识别准确率更高


<details>
  <summary>Details</summary>
Motivation: 传统观点认为语音增强能提升噪声环境下的ASR性能，但现代大规模ASR模型在多样化噪声数据上训练后，这种假设需要重新验证，特别是在医疗转录等关键应用中

Method: 使用MetricGAN-plus-voicebank去噪方法，在4个先进ASR系统（OpenAI Whisper、NVIDIA Parakeet、Google Gemini Flash 2.0、Parrotlet-a）上，对500个医疗语音录音在9种噪声条件下进行系统评估，使用考虑领域特定归一化的语义WER（semWER）作为性能指标

Result: 反直觉发现：语音增强预处理在所有噪声条件和模型中都降低了ASR性能。原始噪声音频在所有40个测试配置（4个模型×10个条件）中都获得了更低的semWER，性能下降范围从1.1%到46.6%绝对semWER增加

Conclusion: 现代ASR模型具有足够的内部噪声鲁棒性，传统语音增强可能移除了对ASR关键的声音特征。在嘈杂临床环境中部署医疗转录系统时，使用噪声减少技术预处理音频不仅是计算浪费，还可能损害转录准确性

Abstract: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [134] [HydroGym: A Reinforcement Learning Platform for Fluid Dynamics](https://arxiv.org/abs/2512.17534)
*Christian Lagemann,Sajeda Mokbel,Miro Gondrum,Mario Rüttgers,Jared Callaham,Ludger Paehler,Samuel Ahnert,Nicholas Zolman,Kai Lagemann,Nikolaus Adams,Matthias Meinke,Wolfgang Schröder,Jean-Christophe Loiseau,Esther Lagemann,Steven L. Brunton*

Main category: physics.flu-dyn

TL;DR: HydroGym是一个用于流体控制研究的强化学习平台，包含42个验证环境，支持可微分和不可微分求解器，能够显著提升样本效率，并展示出良好的迁移学习能力。


<details>
  <summary>Details</summary>
Motivation: 流体控制在交通、能源、医疗等领域至关重要，但面临高维、非线性、多尺度等挑战。强化学习在复杂领域表现出色，但在流体控制应用中缺乏标准化基准平台和计算资源支持。

Method: 开发HydroGym平台，集成42个验证的流体控制环境（从层流到三维湍流），提供不可微分求解器用于传统RL，以及可微分求解器通过梯度增强优化提升样本效率。平台具有可扩展架构，支持添加新环境、代理模型和控制算法。

Result: RL智能体在不同配置下发现鲁棒的控制原理（如边界层操纵、声反馈干扰、尾流重组）。迁移学习研究表明，在某一雷诺数或几何形状下学习的控制器能高效适应新条件，训练周期减少约50%。

Conclusion: HydroGym为流体动力学、机器学习和控制领域的研究者提供了一个可扩展框架，通过标准化基准和高效求解器，推动流体控制科学和技术的发展。

Abstract: Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [Vidarc: Embodied Video Diffusion Model for Closed-loop Control](https://arxiv.org/abs/2512.17661)
*Yao Feng,Chendong Xiang,Xinyi Mao,Hengkai Tan,Zuyue Zhang,Shuhe Huang,Kaiwen Zheng,Haitian Liu,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: Vidarc是一种基于视频扩散的机器人控制方法，通过掩码逆动力学模型和自回归生成实现快速闭环控制，在数据稀缺环境下显著提升成功率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺环境下的机器人臂操作面临挑战，现有视频方法在闭环控制中存在高延迟和不足的接地问题，需要优化面向具体化身的控制性能。

Method: 提出Vidarc方法：1）基于视频扩散的自回归具身化方法；2）使用掩码逆动力学模型通过动作相关掩码接地视频预测；3）通过缓存自回归生成实现实时反馈闭环控制。

Result: 在100万跨化身片段上预训练后，Vidarc超越SOTA基线：1）实际部署中成功率至少提高15%；2）延迟降低91%；3）在未见过的机器人平台上展现强大的泛化和纠错能力。

Conclusion: Vidarc通过视频扩散和掩码逆动力学模型成功解决了数据稀缺环境下的机器人控制问题，实现了快速准确的闭环控制，并展示了强大的跨平台泛化能力。

Abstract: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [136] [Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL](https://arxiv.org/abs/2512.17053)
*Khushboo Thaker,Yony Bresler*

Main category: cs.CL

TL;DR: Struct-SQL：使用结构化推理蒸馏训练小型语言模型，在Text-to-SQL任务中比非结构化CoT蒸馏提升8.1%，主要减少语法错误


<details>
  <summary>Details</summary>
Motivation: 企业级Text-to-SQL系统面临成本、安全和性能的三难困境。现有方案要么使用昂贵专有的大语言模型，要么使用性能低的小语言模型。改进小模型通常依赖从大模型蒸馏非结构化思维链，但这种方法存在固有模糊性。作者假设结构化推理表示能提供更清晰可靠的教学信号。

Method: 提出Struct-SQL知识蒸馏框架，训练小语言模型模仿强大大语言模型。采用查询执行计划作为形式化蓝图来推导结构化推理，使用结构化思维链进行蒸馏。

Result: 使用结构化CoT蒸馏的小模型相比非结构化CoT蒸馏基线获得8.1%的绝对提升。详细错误分析显示，性能提升的关键因素是语法错误显著减少。

Conclusion: 使用结构化逻辑蓝图教导模型推理对小语言模型生成可靠SQL是有益的，结构化推理表示确实提供了更清晰的教学信号。

Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.

</details>


### [137] [A Women's Health Benchmark for Large Language Models](https://arxiv.org/abs/2512.17028)
*Victoria-Elisabeth Gruber,Razvan Marinescu,Diego Fajardo,Amin H. Nassar,Christopher Arkfeld,Alexandria Ludlow,Shama Patel,Mehrnoosh Samaei,Valerie Klug,Anna Huber,Marcel Gühner,Albert Botta i Orfila,Irene Lagoja,Kimya Tarr,Haleigh Larson,Mary Beth Howard*

Main category: cs.CL

TL;DR: 该研究创建了首个女性健康基准测试WHB，评估13个先进LLM在女性健康领域的表现，发现约60%的失败率，模型在紧急情况识别等方面存在严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着LLM成为数百万人获取健康信息的主要来源，其在女性健康领域的准确性尚未得到充分检验。需要专门评估LLM在女性健康方面的表现，以识别潜在风险和改进空间。

Method: 开发了女性健康基准测试WHB，包含96个经过严格验证的模型测试用例，涵盖5个医学专业、3种查询类型和8种错误类型。评估了13个最先进的LLM模型。

Result: 当前模型在女性健康基准测试中表现出约60%的失败率，性能在不同专业和错误类型间差异显著。所有模型在"紧急情况识别"方面普遍困难，而GPT-5等新模型在避免不适当建议方面有显著改进。

Conclusion: AI聊天机器人目前尚不能提供可靠的女性健康建议，需要进一步改进模型在女性健康领域的准确性和安全性。

Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.

</details>


### [138] [Perturb Your Data: Paraphrase-Guided Training Data Watermarking](https://arxiv.org/abs/2512.17075)
*Pranav Shetty,Mirazul Haque,Petr Babkin,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: SPECTRA是一种训练数据检测水印方法，通过LLM生成与原文评分匹配的改写文本，即使训练数据仅占语料库的0.001%也能可靠检测。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在从互联网抓取的大规模文本语料上进行训练，训练数据检测对于执行版权和数据许可变得至关重要。需要一种能在数据发布前部署、即使数据在训练语料中占比极低也能存活的检测方法。

Method: 使用LLM对文本进行改写，通过独立的评分模型为每个改写版本分配评分。选择评分与原文最接近的改写版本，以避免引入分布偏移。检测时比较可疑模型的token概率与评分模型的token概率。

Result: SPECTRA在检测训练数据与非训练数据时，实现了超过九个数量级的p值差距，优于所有基线方法。该方法即使在数据仅占训练语料0.001%的情况下也能可靠检测。

Conclusion: SPECTRA为数据所有者提供了一种可扩展的、在发布前部署的水印方法，能够在大规模LLM训练中存活，有效解决训练数据检测和版权保护问题。

Abstract: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.

</details>


### [139] [Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection](https://arxiv.org/abs/2512.17630)
*Menna Elgabry,Ali Hamdi*

Main category: cs.CL

TL;DR: 提出基於Condorcet陪審團定理的置信度加權、可信度感知集成框架，用於文本情感檢測，通過結合架構多樣的小型Transformer模型，在僅595M參數下超越7B參數的大型LLMs，達到93.5%的宏觀F1分數。


<details>
  <summary>Details</summary>
Motivation: 傳統集成方法通常依賴同質架構，缺乏多樣性。大型語言模型雖然強大但參數效率低，且在小樣本任務上表現不佳。需要一種既能保持錯誤多樣性又能動態調整模型貢獻的集成方法。

Method: 結合BERT、RoBERTa、DistilBERT、DeBERTa和ELECTRA五種架構多樣的小型Transformer模型，完全微調用於情感分類。採用雙重加權投票機制：全局可信度（驗證F1分數）和局部置信度（實例級概率）動態加權模型貢獻。最小化參數收斂以保持錯誤多樣性。

Result: 在DAIR-AI數據集上達到93.5%的宏觀F1分數，超越最先進基準，顯著優於Falcon、Mistral、Qwen和Phi等大型LLMs（即使經過LoRA適配）。僅595M總參數，比7B參數模型更參數高效和魯棒。

Conclusion: 精心設計的小型微調模型集成可以在專業NLP任務（如情感檢測）中超越大型LLMs，證明了參數效率和架構多樣性的重要性。Condorcet陪審團定理啟發的集成框架提供了一種有效的替代方案。

Abstract: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [140] [Application of machine learning to predict food processing level using Open Food Facts](https://arxiv.org/abs/2512.17169)
*Nalin Arora,Aviral Chauhan,Siddhant Rana,Mahansh Aditya,Sumit Bhagat,Aditya Kumar,Akash Kumar,Akanksh Semar,Ayush Vikram Singh,Ganesh Bagler*

Main category: q-bio.BM

TL;DR: 该研究首次大规模使用机器学习基于营养成分数据对食品加工水平（NOVA分类）进行分类，发现超加工食品与较差的营养质量、更高的碳足迹和更多过敏原相关。


<details>
  <summary>Details</summary>
Motivation: 超加工食品与肥胖、心血管疾病、2型糖尿病和心理健康问题等健康问题日益相关，但缺乏大规模的系统性分类方法。本研究旨在利用机器学习基于营养成分数据对食品加工水平进行自动分类。

Method: 使用Open Food Facts数据库中超过90万种产品的数据，基于营养成分浓度数据训练机器学习模型（包括LightGBM、随机森林和CatBoost）来预测NOVA加工水平分类。

Result: LightGBM模型表现最佳，在不同营养面板上达到80-85%的准确率，能有效区分最低加工和超加工食品。分析显示较高NOVA类别与较低的Nutri-Score（营养质量较差）、较高的碳足迹、较低的Eco-Score以及更多添加剂和过敏原（特别是麸质和牛奶）相关。

Conclusion: 这项研究利用最大的NOVA标记产品数据集，强调了食品加工对健康、环境和过敏风险的影响，并展示了机器学习在可扩展分类中的价值。研究还提供了一个用户友好的网络工具用于基于营养成分数据的NOVA预测。

Abstract: Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [141] [Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease](https://arxiv.org/abs/2512.17340)
*Carter H. Nakamoto,Lucia Lushi Chen,Agata Foryciarz,Sherri Rose*

Main category: stat.ME

TL;DR: 提出一个针对多群体公平回归的通用框架，通过不公平惩罚项处理多个受偏见影响的群体，特别针对二元结果的真阳性率差异惩罚，可高效实现为成本敏感分类问题


<details>
  <summary>Details</summary>
Motivation: 现有公平回归方法在处理多个受偏见影响的群体方面存在不足，医疗保健领域需要能够同时考虑多个群体公平性的回归方法

Method: 提出通用回归框架，包含多群体不公平惩罚项，针对二元结果使用真阳性率差异惩罚，通过转化为成本敏感分类问题实现高效计算，并引入新的评分函数自动选择惩罚权重

Result: 模拟实验显示该方法在公平性-准确性边界上优于现有方法，应用于慢性肾病研究中开发终末期肾病公平分类器，显著改善了多个种族和民族群体的公平性，且整体拟合度无明显损失

Conclusion: 该方法为医疗保健领域处理多群体公平性问题提供了有效解决方案，能够在保持整体性能的同时显著改善多个受偏见影响群体的公平性

Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [142] [STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting](https://arxiv.org/abs/2512.17667)
*Yifei Cheng,Yujia Zhu,Baiyang Li,Xinhao Deng,Yitong Cai,Yaochen Ren,Qingyun Liu*

Main category: cs.CR

TL;DR: STAR将网站指纹攻击重新定义为零样本跨模态检索问题，通过联合嵌入空间学习加密流量与网站逻辑配置文件的语义对齐，无需目标网站流量训练即可识别未知网站。


<details>
  <summary>Details</summary>
Motivation: 现代HTTPS机制（如ECH和加密DNS）虽然提高了隐私性，但仍易受网站指纹攻击。现有方法依赖监督学习，需要特定网站的标记流量数据，可扩展性差且无法处理未知网站。

Method: 将网站指纹攻击重新定义为零样本跨模态检索问题，提出STAR框架。使用双编码器架构学习加密流量轨迹和爬取时逻辑配置文件的联合嵌入空间，通过对比学习和一致性目标训练，结合结构感知增强技术。

Result: 在1600个未见网站上，STAR达到87.9%的top-1准确率和0.963的AUC，优于监督学习和少样本基线。每个网站仅需4个标记轨迹的适配器可将top-5准确率提升至98.8%。

Conclusion: 现代Web协议中存在固有的语义-流量对齐，语义泄露是加密HTTPS流量的主要隐私风险。STAR展示了零样本跨模态检索在网站指纹攻击中的有效性，为隐私保护和攻击检测提供了新视角。

Abstract: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.

</details>


### [143] [Sedna: Sharding transactions in multiple concurrent proposer blockchains](https://arxiv.org/abs/2512.17045)
*Alejandro Ranchal-Pedrosa,Benjamin Marsh,Lefteris Kokoris-Kogias,Alberto Sonnino*

Main category: cs.CR

TL;DR: Sedna是一个面向用户的协议，使用可验证的无速率编码替代简单的交易复制，在保持抗审查性和低延迟的同时显著降低带宽开销和MEV暴露风险。


<details>
  <summary>Details</summary>
Motivation: 现代区块链采用多提议者共识来解决单领导者瓶颈和审查抵抗问题，但用户如何向提议者传播交易仍存在问题。当前方案要么简单复制交易到多个提议者（牺牲吞吐量并暴露MEV），要么只针对少数提议者（导致弱审查抵抗和高延迟），形成了抗审查性、低延迟和合理成本之间的三难困境。

Method: Sedna使用可验证的无速率编码技术，用户将编址的符号包私下传递给提议者子集。一旦收集到足够符号完成解码，交易按确定性顺序执行。协议无需修改共识机制，支持渐进部署。

Result: Sedna保证活跃性和"解码前隐私"，显著减少MEV暴露。分析表明协议接近带宽开销的信息论下界，相比简单复制效率提高2-3倍。

Conclusion: Sedna通过创新的编码方案解决了多提议者区块链中的交易传播三难困境，在保持抗审查性和低延迟的同时显著提升了效率和隐私保护。

Abstract: Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).
  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.

</details>


### [144] [Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning](https://arxiv.org/abs/2512.17254)
*Baolei Zhang,Minghong Fang,Zhuqing Liu,Biao Yi,Peizhao Zhou,Yuan Wang,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: ABBR：一个实用的联邦学习框架，通过降维加速隐私计算，同时防御拜占庭攻击和隐私推理攻击，显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临拜占庭攻击（恶意客户端操纵模型）和隐私推理攻击（从模型推断私有数据）的双重威胁。现有防御方法计算和通信开销大，理论与实践存在差距。

Method: 提出ABBR框架：1）首次利用降维技术加速隐私保护联邦学习中复杂过滤规则的隐私计算；2）分析低维空间中向量过滤的精度损失；3）引入自适应调优策略，最小化绕过过滤的恶意模型对全局模型的影响。

Result: 在公开数据集上评估显示：ABBR运行速度显著更快，通信开销最小，同时保持了与基线方法几乎相同的拜占庭攻击抵御能力。

Conclusion: ABBR是一个实用的拜占庭鲁棒且隐私保护的联邦学习框架，通过降维技术有效解决了现有防御方法计算和通信开销大的问题，在保持安全性的同时提升了效率。

Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.

</details>


### [145] [AutoDFBench 1.0: A Benchmarking Framework for Digital Forensic Tool Testing and Generated Code Evaluation](https://arxiv.org/abs/2512.16965)
*Akila Wickramasekara,Tharusha Mihiranga,Aruna Withanage,Buddhima Weerasinghe,Frank Breitinger,John Sheppard,Mark Scanlon*

Main category: cs.CR

TL;DR: AutoDFBench 1.0是首个自动化数字取证工具测试框架，基于NIST CFTT标准，支持传统工具、脚本和AI生成代码的评估，提供标准化指标和可重复的基准测试。


<details>
  <summary>Details</summary>
Motivation: NIST CFTT项目已成为数字取证工具测试的事实标准，但缺乏自动化基准测试框架，导致验证不一致、工具比较困难、验证可重复性有限。

Method: 开发模块化基准测试框架AutoDFBench 1.0，集成CFTT定义的五个领域（字符串搜索、删除文件恢复、文件雕刻、Windows注册表恢复、SQLite数据恢复），包含63个测试用例和10,968个独特测试场景，通过RESTful API执行评估并生成结构化JSON输出。

Result: 框架使用标准化指标（精确率、召回率、F1分数）评估每个测试用例，平均F1分数构成AutoDFBench Score，已在CFTT数据集上验证，实现了公平、可重复的工具比较。

Conclusion: AutoDFBench 1.0建立了首个统一、自动化、可扩展的数字取证工具测试基准测试框架，支持工具供应商、研究人员、从业者和标准化机构进行透明、可重复、可比较的DF技术评估。

Abstract: The National Institute of Standards and Technology (NIST) Computer Forensic Tool Testing (CFTT) programme has become the de facto standard for providing digital forensic tool testing and validation. However to date, no comprehensive framework exists to automate benchmarking across the diverse forensic tasks included in the programme. This gap results in inconsistent validation, challenges in comparing tools, and limited validation reproducibility. This paper introduces AutoDFBench 1.0, a modular benchmarking framework that supports the evaluation of both conventional DF tools and scripts, as well as AI-generated code and agentic approaches. The framework integrates five areas defined by the CFTT programme: string search, deleted file recovery, file carving, Windows registry recovery, and SQLite data recovery. AutoDFBench 1.0 includes ground truth data comprising of 63 test cases and 10,968 unique test scenarios, and execute evaluations through a RESTful API that produces structured JSON outputs with standardised metrics, including precision, recall, and F1~score for each test case, and the average of these F1~scores becomes the AutoDFBench Score. The benchmarking framework is validated against CFTT's datasets. The framework enables fair and reproducible comparison across tools and forensic scripts, establishing the first unified, automated, and extensible benchmarking framework for digital forensic tool testing and validation. AutoDFBench 1.0 supports tool vendors, researchers, practitioners, and standardisation bodies by facilitating transparent, reproducible, and comparable assessments of DF technologies.

</details>


### [146] [MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval](https://arxiv.org/abs/2512.16962)
*Saksham Sahai Srivastava,Haoyu He*

Main category: cs.CR

TL;DR: 提出MemoryGraft攻击方法，通过向LLM智能体的长期记忆中植入恶意成功经验，利用其语义模仿启发式，实现持久的行为漂移攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体越来越多地依赖长期记忆和RAG来积累经验并改进未来性能，这种经验学习能力虽然增强了智能体自主性，但也引入了一个关键且未被探索的攻击面——智能体推理核心与其自身过去经验之间的信任边界。

Method: 提出MemoryGraft间接注入攻击，攻击者通过提供良性输入级别的工件，诱导智能体构建被污染的RAG存储，将少量恶意程序模板与良性经验一起持久化。当智能体后来遇到语义相似的任务时，通过词法和嵌入相似性的联合检索可靠地浮现这些嫁接的记忆，智能体采用嵌入的不安全模式。

Result: 在MetaGPT的DataInterpreter智能体（使用GPT-4o）上验证了MemoryGraft，发现少量被污染记录可以在良性工作负载上占据检索经验的大部分比例，将基于经验的自我改进转变为隐蔽且持久的攻击向量。

Conclusion: 揭示了LLM智能体经验学习机制中的新安全漏洞，需要重新思考智能体记忆系统的安全设计，以防止通过长期记忆进行的隐蔽攻击。

Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.

</details>


### [147] [Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors](https://arxiv.org/abs/2512.17146)
*Huixin Zhan*

Main category: cs.CR

TL;DR: SAGE是一个用于审计基因组基础模型对抗性漏洞的代理框架，通过软提示扰动和风险指标评估发现ESM2等先进模型存在可测量的性能下降。


<details>
  <summary>Details</summary>
Motivation: 基因组基础模型（如ESM）在变异效应预测方面表现出色，但其在对抗性操纵下的安全性和鲁棒性尚未得到充分探索，需要系统性的安全评估框架。

Method: 提出SAGE代理框架，通过可解释的自动化风险审计循环：注入软提示扰动、监控模型行为、计算AUROC/AUPR等风险指标、生成结构化报告和基于大语言模型的解释。

Result: 发现即使是ESM2等最先进的基因组基础模型也对目标软提示攻击敏感，导致可测量的性能下降，揭示了先前隐藏的漏洞。

Conclusion: 基因组基础模型存在关键的安全漏洞，代理风险审计对于保护临床变异解释等生物医学应用的安全至关重要。

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.

</details>


### [148] [AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs](https://arxiv.org/abs/2512.17251)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: AlignDP是一种混合隐私锁，通过在数据接口处分离稀有和非稀有字段来阻止知识转移，稀有字段使用PAC不可区分性保护，非稀有字段使用RAPPOR本地差分隐私，全局聚合器管理组合和预算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临提取、蒸馏和未经授权的微调风险，现有防御措施如水印或监控都是在泄漏后采取行动，需要一种能在数据接口处阻止知识转移的主动防御机制。

Method: 设计两层级隐私保护：1) 稀有字段使用PAC不可区分性提供有效的零epsilon本地差分隐私保护；2) 非稀有字段使用RAPPOR本地差分隐私提供无偏频率估计；3) 全局聚合器强制执行组合和预算管理。

Result: 理论分析证明了PAC扩展到全局聚合的局限性，给出了RAPPOR估计的界限，并分析了效用权衡。玩具模拟验证了可行性：稀有类别保持隐藏，频繁类别能以小误差恢复。

Conclusion: AlignDP提供了一种在数据接口处阻止知识转移的混合隐私锁方法，通过分离稀有和非稀有字段的保护策略，在保护隐私的同时保持数据效用。

Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.

</details>


### [149] [MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification](https://arxiv.org/abs/2512.17594)
*Tosin Ige,Christopher Kiekintveld,Aritran Piplai,Asif Rahman,Olukunle Kolade,Sasidhar Kunapuli*

Main category: cs.CR

TL;DR: MADOOD：一种基于高斯判别分析和聚类驱动的两阶段深度学习框架，用于鲁棒的恶意软件OOD检测与分类，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 恶意软件分类中的分布外检测面临重大挑战，因为多态和变形恶意软件变种导致显著的类内变异。现有基于深度学习的恶意软件检测器大多依赖封闭世界假设，无法充分建模类内变异，在面对未见恶意软件家族时性能下降。

Method: 提出两阶段聚类驱动框架：第一阶段使用基于高斯判别分析的类条件球形决策边界建模恶意软件家族嵌入，通过Z分数距离分析识别异常样本；第二阶段通过深度神经网络整合聚类预测、精炼嵌入和监督分类器输出以提升最终分类精度。

Result: 在包含25个已知家族和多个新型OOD变种的基准恶意软件数据集上评估，MADOOD显著优于最先进的OOD检测方法，在未见恶意软件家族上AUC最高达0.911。

Conclusion: 该框架为现实世界恶意软件检测和异常识别提供了可扩展、可解释且统计原理清晰的解决方案，适用于不断演变的网络安全环境。

Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [150] [Quantum-enhanced Information Retrieval from Reflective Intelligent Surfaces](https://arxiv.org/abs/2512.17199)
*Shiqian Guo,Tingxiang Ji,Jianqing Liu*

Main category: quant-ph

TL;DR: 提出一种结合时间分辨量子接收器和多模探测信号的新方法，用于从被动可重构智能表面提取大字母信息，超越经典标准量子极限


<details>
  <summary>Details</summary>
Motivation: 传统被动背向散射系统受限于经典无线接收器的基本限制，无法在不牺牲能效或通信距离的情况下提高数据速率，这阻碍了该技术的广泛应用

Method: 采用时间分辨量子接收器结合多模探测信号，从被动可重构智能表面提取大字母调制信息，无需依赖复杂或脆弱的量子资源如纠缠

Result: 仿真结果显示该技术超越经典标准量子极限，支持调制大小达M=2^8，同时将探测能量减半或将通信距离提高1.41倍

Conclusion: 提出的自适应量子接收器在不依赖复杂量子资源的情况下实现了显著的量子优势，为被动背向散射系统提供了超越经典限制的解决方案

Abstract: Information retrieval from passive backscatter systems is widely used in digital applications with tight energy budgets, short communication distances, and low data rates. Due to the fundamental limits of classical wireless receivers, the achievable data rate cannot be increased without compromising either energy efficiency or communication range, thereby hindering the broader adoption of this technology. In this work, we present a novel time-resolving quantum receiver combined with a multi-mode probing signal to extract large-alphabet information modulated by a passive reconfigurable intelligent surface (RIS). The adaptive nature of the proposed receiver yields significant quantum advantages over classical receivers without relying on complex or fragile quantum resources such as entanglement. Simulation results show that the proposed technique surpasses the classical standard quantum limit (SQL) for modulation sizes up to M = 2^8, meanwhile halving the probing energy or increasing the communication distance by a factor of 1.41.

</details>


### [151] [Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines](https://arxiv.org/abs/2512.17660)
*João Marcos Cavalcanti de Albuquerque Neto,Gustavo Castro do Amaral,Guilherme Penello Temporão*

Main category: quant-ph

TL;DR: 量子辅助受限玻尔兹曼机在信用卡欺诈检测中表现优于经典方法，即使在当前噪声量子退火器上也能实现


<details>
  <summary>Details</summary>
Motivation: 随着量子计算处理效率和可用性的提高，新兴量子计算平台的应用案例变得具有经济意义。作者希望评估量子计算辅助的受限玻尔兹曼机在真实金融欺诈检测场景中的性能

Method: 使用量子计算辅助的受限玻尔兹曼机方法，在真实量子硬件和模拟器上运行，测试数据集包含巴西金融科技公司Stone提供的1.45亿笔交易记录，用于信用卡欺诈检测

Result: 量子辅助RBM方法在大多数性能指标上能够实现优于经典方法的性能，即使使用当前有噪声的量子退火器也能取得良好结果

Conclusion: 这项研究为在金融系统中实施量子辅助RBM进行一般故障检测铺平了道路，展示了量子计算在金融欺诈检测等实际应用中的潜力

Abstract: Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.

</details>


### [152] [Domain-Aware Quantum Circuit for QML](https://arxiv.org/abs/2512.17800)
*Gurinder Singh,Thaddeus Pellegrini,Kenneth M. Merz,*

Main category: quant-ph

TL;DR: 提出了一种基于领域知识的量子电路（DAQC），利用图像先验通过非重叠DCT式之字形窗口进行局部保持编码和纠缠，在NISQ设备上实现高性能量子机器学习图像分类。


<details>
  <summary>Details</summary>
Motivation: 在噪声中等规模量子（NISQ）设备上设计既具有表达力、可训练性，又能抵抗硬件噪声的参数化量子电路（PQCs）是量子机器学习面临的核心挑战。需要解决深度引起的和全局纠缠的贫瘠高原效应问题。

Method: 提出领域感知量子电路（DAQC），利用图像先验指导局部保持编码：1）使用非重叠DCT式之字形窗口进行编码；2）采用交错编码-纠缠-训练循环；3）在相邻像素对应的量子比特之间应用纠缠，与设备连接性对齐；4）通过分阶段、局部保持的信息流扩展有效感受野，避免深度全局混合。

Result: 在MNIST、FashionMNIST和PneumoniaMNIST数据集上评估，在真实量子硬件上：1）性能与强经典基线（ResNet-18/50、DenseNet-121、EfficientNet-B0）相当；2）显著优于量子电路搜索（QCS）基线；3）仅使用量子特征提取器和线性经典读出层，在QML图像分类任务中达到目前最佳报告性能。

Conclusion: DAQC通过利用图像先验指导局部保持编码和纠缠，有效解决了NISQ设备上量子机器学习的挑战，在有限深度和量子比特条件下实现了高性能图像分类，为量子特征提取器设计提供了新思路。

Abstract: Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [153] [Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning](https://arxiv.org/abs/2512.17185)
*Sandeep Neela*

Main category: q-fin.RM

TL;DR: SRR框架使用多层图建模金融市场，通过结构网络特征提供系统性风险的早期预警信号，优于传统特征模型。


<details>
  <summary>Details</summary>
Motivation: 金融危机源于跨部门、市场和投资者行为的结构性脆弱性积累，传统方法难以预测这种系统性转变，因为它们来自市场参与者之间的动态交互而非孤立的价格变动。

Method: 提出Systemic Risk Radar (SRR)框架，将金融市场建模为多层图来检测系统性脆弱性和崩溃状态转变的早期迹象。使用基于相关性的实例化，比较了快照GNN、简化时间GNN原型与逻辑回归、随机森林等基线模型。

Result: 在互联网泡沫、全球金融危机和COVID-19冲击三个重大危机中评估，结果显示结构网络信息相比纯特征模型能提供更有用的早期预警信号。图衍生特征能捕捉压力事件期间市场结构的有意义变化。

Conclusion: 研究结果表明SRR框架的有效性，并为进一步扩展提供了方向：添加更多图层（行业/因子暴露、情绪）和更具表达力的时间架构（LSTM/GRU或Transformer编码器）以更好地处理不同类型的危机。

Abstract: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.
  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.
  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [154] [Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest](https://arxiv.org/abs/2512.17277)
*Saeed Ebrahimi,Weijie Jiang,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: Pinterest提出针对冷启动物品推荐问题的四项轻量级解决方案，包括残差连接、分数正则化、流形混合等技术，仅增加5%参数，却提升10%新鲜内容参与度


<details>
  <summary>Details</summary>
Motivation: Pinterest作为视觉发现平台，推荐系统对用户体验至关重要。冷启动物品在训练数据中出现频率低，导致模型预测效果差，而现有研究在工业规模平台上的解决方案有限，需要高效且轻量的方法

Method: 1) 设计轻量级方案，总参数仅增加5%；2) 为非历史特征引入残差连接，提升其重要性；3) 加入分数正则化项，缓解冷启动物品得分偏低问题；4) 应用流形混合技术，解决标签稀疏性

Result: 方法组合实施后，Pinterest新鲜内容参与度提升10%，未对整体参与度和成本产生负面影响，已部署服务超过5.7亿用户

Conclusion: 通过系统性地解决冷启动物品推荐的核心挑战，Pinterest开发了一套高效、轻量的解决方案，在工业规模下显著提升了推荐系统对新鲜内容的推荐效果

Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.

</details>


### [155] [Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application](https://arxiv.org/abs/2512.17462)
*Olivier Jeunen,Schaun Wheeler*

Main category: cs.IR

TL;DR: 代理式个性化营销在金融服务应用中减少退订21%，提高提前报税行为


<details>
  <summary>Details</summary>
Motivation: 评估代理式个性化方法在金融服务客户沟通系统中的行为和留存效果，特别是在税务申报期间

Method: 通过为期两个月的随机对照试验，比较代理式消息方法与常规规则式营销系统，关注退订行为和转化时机

Result: 代理式消息使退订事件相对减少21%（±0.01），并在国家截止日期前增加了提前申报行为

Conclusion: 自适应、用户级决策系统能够调节参与强度，同时改善长期留存指标

Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [156] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: Torrent是一种分布式DMA架构，通过逻辑链式传输实现高效的点对多点数据移动，无需修改NoC硬件和互连协议，在性能和能效方面显著优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 现代SoC中计算能力与片上通信带宽之间的差距日益扩大，成为AI等数据并行工作负载的关键瓶颈。虽然点对多点数据传输（如组播）对高性能至关重要，但标准互连协议缺乏原生组播支持。现有的P2MP解决方案（如支持组播的NoC）会给网络硬件带来额外开销，需要修改互连协议，从而影响可扩展性和兼容性。

Method: Torrent采用分布式DMA架构，通过Chainwrite机制在NoC上形成逻辑链进行P2MP数据传输，数据像链表一样遍历目标节点。该方法保持了每个数据传输的点对点特性，同时支持向无限数量目的地的灵活传输。开发了两种调度算法来确定基于NoC拓扑的最优链顺序，以优化性能和能耗。

Result: RTL和FPGA原型评估显示，Torrent在网络层组播方面具有显著的性能、灵活性和可扩展性优势。相比单播基线，Torrent实现了最高7.88倍的加速。16nm工艺ASIC合成确认了架构的极小面积开销（1.2%）和功耗开销（2.3%）。Chainwrite机制使Torrent能够以每个目的地82个周期和207um²的面积开销实现可扩展的P2MP数据传输。

Conclusion: Torrent提供了一种无需修改NoC硬件和互连协议的高效P2MP数据传输解决方案，通过逻辑链式传输机制实现了优异的性能、灵活性和可扩展性，同时保持极低的硬件开销。

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>
