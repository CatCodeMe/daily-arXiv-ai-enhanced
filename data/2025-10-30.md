<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 77]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.RO](#cs.RO) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ODataX: A Progressive Evolution of the Open Data Protocol](https://arxiv.org/abs/2510.24761)
*Anirudh Ganesh,Nitin Sood*

Main category: cs.DB

TL;DR: ODataX是一个改进的OData协议版本，在保持向后兼容性的同时，通过简化查询语法、内置性能防护和增强缓存机制来解决OData采用障碍。


<details>
  <summary>Details</summary>
Motivation: OData协议虽然功能强大且成熟，但主要局限于微软和SAP等企业环境，未能广泛采用。本文旨在分析阻碍OData广泛采用的关键障碍。

Method: 引入ODataX协议，在保持与OData v4向后兼容的同时，通过渐进式复杂性披露（简化查询语法）、内置性能防护（查询成本估算）和增强缓存机制来改进协议。

Result: ODataX协议设计解决了OData的采用障碍，提供了更简化的查询语法和性能保障机制。

Conclusion: ODataX旨在弥合企业级查询标准化与现代Web开发实践所需的简单性之间的差距，促进OData协议更广泛的采用。

Abstract: The Open Data Protocol (OData) provides a standardized approach for building
and consuming RESTful APIs with rich query capabilities. Despite its power and
maturity, OData adoption remains confined primarily to enterprise environments,
particularly within Microsoft and SAP ecosystems. This paper analyzes the key
barriers preventing wider OData adoption and introduces ODataX, an evolved
version of the protocol designed to address these limitations. ODataX maintains
backward compatibility with OData v4 while introducing progressive complexity
disclosure through simplified query syntax, built-in performance guardrails via
query cost estimation, and enhanced caching mechanisms. This work aims to
bridge the gap between enterprise-grade query standardization and the
simplicity demanded by modern web development practices.

</details>


### [2] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: StorageXTuner是一个基于LLM代理的存储系统自动调优框架，通过四个代理分离关注点，实现异构存储引擎的高效配置优化。


<details>
  <summary>Details</summary>
Motivation: 存储系统自动配置困难，参数空间大且工作负载、部署环境和版本差异大。现有启发式和ML调优器通常系统特定，需要手动适配，且在系统变化时性能下降。

Method: 设计四个代理：执行器（沙盒化基准测试）、提取器（性能摘要）、搜索器（基于洞察的配置探索）和反射器（洞察生成与管理）。采用洞察驱动的树搜索和分层内存，结合轻量级检查器防止不安全操作。

Result: 在RocksDB、LevelDB、CacheLib和MySQL InnoDB上测试，相比默认设置和ELMo-Tune，吞吐量最高提升575%和111%，p99延迟降低88%和56%，收敛所需试验次数更少。

Conclusion: StorageXTuner通过LLM代理驱动的框架实现了跨存储系统的自动调优，显著提升性能并减少调优成本。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


### [3] [Time-varying Vector Field Compression with Preserved Critical Point Trajectories](https://arxiv.org/abs/2510.25143)
*Mingze Xia,Yuxiao Li,Pu Jiao,Bei Wang,Xin Liang,Hanqi Guo*

Main category: cs.DB

TL;DR: 提出一种高效的有损压缩框架，能够精确保留时变向量场中的所有临界点轨迹，解决了现有压缩方法会扭曲这一关键特征的问题。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和观测产生大量时变向量场数据，存储和传输困难。有损压缩是减少数据量的有效方法，但现有方法会扭曲临界点轨迹这一关键特征。

Method: 扩展了空间临界点保留理论到时空临界点轨迹保留，开发压缩框架实现该功能；提出半拉格朗日预测器利用平流主导区域的时空相关性，与传统Lorenzo预测器结合提高压缩效率。

Result: 在四个真实科学数据集上测试，方法提供高达124.48倍压缩比，同时有效保留所有临界点轨迹，比最佳无损压缩器高56.07倍。

Conclusion: 该方法在保持临界点轨迹完整性的同时实现了高压缩比，现有有损压缩器在类似压缩比下无法保留所有临界点轨迹。

Abstract: Scientific simulations and observations are producing vast amounts of
time-varying vector field data, making it hard to store them for archival
purposes and transmit them for analysis. Lossy compression is considered a
promising approach to reducing these data because lossless compression yields
low compression ratios that barely mitigate the problem. However, directly
applying existing lossy compression methods to timevarying vector fields may
introduce undesired distortions in critical-point trajectories, a crucial
feature that encodes key properties of the vector field. In this work, we
propose an efficient lossy compression framework that exactly preserves all
critical-point trajectories in time-varying vector fields. Our contributions
are threefold. First, we extend the theory for preserving critical points in
space to preserving critical-point trajectories in space-time, and develop a
compression framework to realize the functionality. Second, we propose a
semi-Lagrange predictor to exploit the spatiotemporal correlations in
advectiondominated regions, and combine it with the traditional Lorenzo
predictor for improved compression efficiency. Third, we evaluate our method
against state-of-the-art lossy and lossless compressors using four real-world
scientific datasets. Experimental results demonstrate that the proposed method
delivers up to 124.48X compression ratios while effectively preserving all
critical-point trajectories. This compression ratio is up to 56.07X higher than
that of the best lossless compressors, and none of the existing lossy
compressors can preserve all critical-point trajectories at similar compression
ratios.

</details>


### [4] [DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries](https://arxiv.org/abs/2510.25401)
*Jiahao Lou,Quan Yu,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 提出解耦存储架构解决图索引更新效率问题，通过三阶段查询机制和增量页面级拓扑重排序策略，在保持查询性能的同时显著提升更新速度。


<details>
  <summary>Details</summary>
Motivation: 传统耦合存储方法在图索引更新时效率低下，会产生大量冗余向量读写和无效I/O，需要解决这一瓶颈。

Method: 采用解耦存储架构，设计三阶段查询机制利用多PQ压缩向量过滤无效I/O和计算，以及增量页面级拓扑重排序策略将新节点插入到最相似邻居所在的页面。

Result: 解耦架构使插入速度提升10.05倍，删除速度提升6.89倍；三阶段查询和增量重排序使查询效率相比传统耦合架构提升2.66倍。

Conclusion: 解耦存储架构结合优化策略能同时显著提升图索引的更新效率和查询性能，解决了传统耦合存储的瓶颈问题。

Abstract: On-disk graph-based indexes are widely used in approximate nearest neighbor
(ANN) search systems for large-scale, high-dimensional vectors. However,
traditional coupled storage methods, which store vectors within the index, are
inefficient for index updates. Coupled storage incurs excessive redundant
vector reads and writes when updating the graph topology, leading to
significant invalid I/O. To address this issue, we propose a decoupled storage
architecture. While a decoupled architecture reduces query performance. To
overcome this limitation, we design two tailored strategies: (i) a three-stage
query mechanism that leverages multiple PQ compressed vectors to filter invalid
I/O and computations, and (ii) an incremental page-level topological reordering
strategy that incrementally inserts new nodes into pages containing their most
similar neighbors to mitigate read amplification. Together, these techniques
substantially reduce both I/O and computational overhead during ANN search.
Experimental results show that the decoupled architecture improves update speed
by 10.05x for insertions and 6.89x for deletions, while the three-stage query
and incremental reordering enhance query efficiency by 2.66x compared to the
traditional coupled architecture.

</details>


### [5] [One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans](https://arxiv.org/abs/2510.25684)
*Yujun He,Hangdong Zhao,Simon Frisk,Yifei Yang,Kevin Kristensen,Paraschos Koutris,Xiangyao Yu*

Main category: cs.DB

TL;DR: SplitJoin是一个通过引入split作为一等查询操作符来优化多连接查询处理的框架，通过分区输入表并采用不同查询计划来减少中间结果大小。


<details>
  <summary>Details</summary>
Motivation: 在多连接查询处理中最小化中间结果至关重要。虽然Yannakakis算法对无环查询提供强大保证，但环状查询仍然是一个开放挑战。

Method: 通过将输入表分区为重和轻部分，SplitJoin允许不同数据分区使用不同的查询计划，利用现有二元连接引擎减少中间结果大小。系统探索了基于split的优化设计空间，包括阈值选择、拆分策略和拆分后的连接顺序。

Result: 在DuckDB上，SplitJoin完成了43个社交网络查询（原生29个），平均实现2.1倍更快运行时间和7.9倍更小的中间结果（分别高达13.6倍和74倍）；在Umbra上，完成了45个查询（原生35个），平均实现1.3倍加速和1.2倍更小的中间结果（分别高达6.1倍和2.1倍）。

Conclusion: SplitJoin框架通过引入split操作符有效解决了环状查询的中间结果最小化问题，在现有数据库系统上实现了显著的性能提升。

Abstract: Minimizing intermediate results is critical for efficient multi-join query
processing. Although the seminal Yannakakis algorithm offers strong guarantees
for acyclic queries, cyclic queries remain an open challenge. In this paper, we
propose SplitJoin, a framework that introduces split as a first-class query
operator. By partitioning input tables into heavy and light parts, SplitJoin
allows different data partitions to use distinct query plans, with the goal of
reducing intermediate sizes using existing binary join engines. We
systematically explore the design space for split-based optimizations,
including threshold selection, split strategies, and join ordering after
splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves
substantial improvements: on DuckDB, SplitJoin completes 43 social network
queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller
intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it
completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller
intermediates on average (up to 6.1x and 2.1x, respectively).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree是首个将WMO FM-301标准从单次雷达体扫扩展到时间分辨、分析就绪档案的数据集级框架，通过开源架构将操作雷达档案转化为FAIR兼容的云优化数据集。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据是科学价值最高但结构利用不足的地球观测数据集之一，现有雷达档案分散、供应商特定且不符合FAIR原则，阻碍了大规模研究、可重现性和云原生计算。

Method: 基于FM-301/CfRadial 2.1标准，使用xarray DataTree将雷达体扫组织为层次化、元数据丰富的结构，并序列化为Zarr格式，结合Icechunk实现ACID兼容存储和版本控制。

Result: 在准垂直剖面和降水累积工作流等案例研究中展示了显著的性能提升，所有工具和数据集通过Raw2Zarr仓库开源发布。

Conclusion: 这项工作为雷达数据管理、高性能地球科学和AI就绪天气基础设施提供了可重现和可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [7] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出了一种多分辨率模型融合方法，通过结合低分辨率训练和原始分辨率微调来显著减少神经网络训练时间，在保持模型精度的同时将训练时间减少高达47%。


<details>
  <summary>Details</summary>
Motivation: 神经网络在科学研究中应用日益广泛，但训练过程耗时严重，特别是处理大尺寸高维数据时。需要开发能降低计算成本的高效训练方法。

Method: 多分辨率模型融合方法：先训练降分辨率数据的模型，然后与原始分辨率数据进行融合精调。通过加速每个融合阶段的模型收敛来减少总训练时间。

Result: 在两个真实科学应用（CosmoFlow和Neuron Inverter）中，该方法分别将训练时间减少了47%和44%，同时模型精度未受影响。

Conclusion: 多分辨率模型融合方法能显著减少端到端训练时间，同时保持模型精度，为处理大尺寸高维数据的神经网络训练提供了有效解决方案。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [8] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 提出了ER-Mapping和NI-Balancer两种技术来优化晶圆级芯片上的MoE模型性能，通过平衡通信压力和隐藏专家迁移开销，显著提升了计算和通信效率。


<details>
  <summary>Details</summary>
Motivation: MoE模型依赖专家并行性来缓解内存瓶颈，但在GPU集群中跨节点通信开销大。晶圆级芯片提供了高性能网络但受限于网格拓扑，导致通信不平衡和性能损失，且缺乏片上磁盘导致专家迁移开销高。

Method: 1. ER-Mapping：共同设计注意力层和MoE层的映射，平衡通信压力；2. NI-Balancer：将完整专家迁移分解为多个步骤，交替利用两个层的冷链接来隐藏迁移开销。

Result: ER-Mapping实现通信减少高达62%，NI-Balancer在MoE计算和通信方面分别带来54%和22%的改进。相比SOTA NVL72超级节点，WSC平台平均提供39%更高的每设备MoE性能。

Conclusion: 通过ER-Mapping和NI-Balancer的协同设计，晶圆级芯片能够充分发挥其在大规模MoE模型中的潜力，显著提升性能表现。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [9] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 提出了一种保护隐私的多阶段AI训练方法，通过模拟知识图谱设计模型，在联邦学习框架中训练，仅返回聚合性能指标，确保敏感医疗数据不离开医院环境。


<details>
  <summary>Details</summary>
Motivation: 临床数据整合对个性化医疗发展至关重要，但受到GDPR等隐私法规严格限制，特别是在罕见病小队列研究中。需要开发既能利用高质量结构化数据又能保护患者隐私的AI训练方法。

Method: 四阶段方法：1) 在模拟临床知识图谱上设计模型结构；2) 在FeatureCloud联邦学习框架中准备模型；3) 在医院环境中使用真实数据进行训练；4) 仅返回聚合性能指标进行评估。

Result: 在TUM.ai Makeathon 2024挑战中成功验证，50名学生无需访问真实数据即可开发患者分类和诊断模型。

Conclusion: 通过联邦学习框架部署安全算法是实现医疗领域隐私保护AI的实用方法，能够在不暴露敏感数据的情况下进行模型开发和评估。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [10] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 本章提出了数据密集型工作负载的分类方法，并概述了在大规模分布式系统中调度这些工作负载的常用方法，同时介绍了文献中的新策略并指出了开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大数据爆炸式增长，工作负载变得更加复杂和计算密集，这些应用在分布式互连资源上处理，规模越来越大。数据密集型应用具有不同的并行度，需要有效利用数据局部性，并可能施加多种服务质量要求，如时间约束、容错能力以及能效等目标。

Method: 提出了数据密集型工作负载的分类方法，并概述了在大规模分布式系统中调度这些工作负载的常用方法。

Result: 介绍了文献中提出的新策略，并对数据密集型工作负载调度进行了系统性的分类和总结。

Conclusion: 指出了数据密集型工作负载调度面临的开放挑战和未来研究方向。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [11] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 本文研究了移动智能体在分布式网络中的聚集问题，特别关注当智能体标签可能重复时的情况。论文给出了可聚集团队的特征描述，设计了多项式时间复杂度的聚集算法，并证明了所需共同知识量的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统的确定性聚集算法通常假设智能体具有互不相同的标签来打破对称性。但实际应用中标签可能重复，因此需要研究在标签可能重复的情况下如何实现确定性聚集。

Method: 提出了一个完整的可聚集团队特征描述方法，设计了一个在多项式时间内聚集所有可聚集团队的算法，该算法仅需要O(log log log μ)比特的共同知识，其中μ是标签的最大重复次数。

Result: 获得了可聚集团队的完整特征描述，设计了时间复杂度为poly(n,logλ)的聚集算法，并证明了所需共同知识量的几乎最优性。

Conclusion: 即使在智能体标签可能重复的情况下，仍然可以实现确定性聚集，且所需共同知识量很小。该工作还提供了首个无需共同知识即可在多项式时间内聚集任意规模团队（当所有标签互异时）的确定性算法。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [12] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming是一个支持精确一次处理的流处理系统，通过窗口化无冲突复制数据类型(Windowed CRDTs)实现可扩展的全局聚合计算，采用去中心化协调机制显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有流处理系统在全局聚合计算时存在可扩展性瓶颈，要么在单个任务实例中计算，要么使用静态聚合树，导致延迟高、故障恢复时延尖峰大。需要解决集中式协调带来的性能限制。

Method: 提出确定性编程模型，使用窗口化无冲突复制数据类型(Windowed CRDTs)作为共享复制状态的新抽象，支持去中心化故障恢复算法。

Result: 在全局聚合工作负载上，相比现有系统延迟降低5倍，吞吐量提高2倍，故障场景下延迟减少11倍。

Conclusion: 证明了去中心化协调与确定性编程的有效性，以及Windowed CRDTs在全局聚合计算中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Reviving Thorup's Shortcut Conjecture](https://arxiv.org/abs/2510.24954)
*Aaron Bernstein,Henry Fleischmann,George Z. Li,Bernhard Haeupler,Maximilian Probst Gutenberg,Gary Hoppenworth,Seth Pettie,Thatchaphol Saranurak,Yonggang Jiang,Leon Schiller*

Main category: cs.DS

TL;DR: 本文重新审视了Thorup关于可达性捷径的猜想，发现虽然原始猜想被Hesse证伪，但允许添加Steiner顶点后，所有已知的下界都被打破。同时排除了厚度限制下的理想捷径，并提出了新的困难实例候选。


<details>
  <summary>Details</summary>
Motivation: 重新审视Thorup关于图可达性捷径的猜想，虽然原始形式被证伪，但允许添加Steiner顶点可能实现理想的尺寸-直径权衡。

Method: 通过允许添加新的捷径边和Steiner顶点，分析捷径的厚度限制，并构造候选困难实例。

Result: 1) 允许Steiner顶点时打破所有已知捷径下界；2) 排除了厚度t=o(log n/log log n)的理想捷径；3) 提出了新的候选困难实例。

Conclusion: Thorup猜想的精神在允许Steiner顶点时可能成立，为并行算法提供了有前景的应用，可能显著改进现有算法的深度。

Abstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of
reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally
asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add
$m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$
so that $\text{dist}_{G_+}(u,v) \leq m^{o(1)}$ for all $(u,v)\in E^*$, where
$G_+=(V,E\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03],
followed by significant efforts in the last few years to optimize the lower
bounds.
  In this paper we observe that although Hesse refuted the letter of Thorup's
conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not
refute the spirit of the conjecture, which should allow $G_+$ to contain both
new (shortcut) edges and new Steiner vertices. Our results are as follows.
  (1) On the positive side, we present explicit attacks that break all known
shortcut lower bounds when Steiner vertices are allowed.
  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size,
$m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\log n/\log \log n)$,
meaning no path can contain $t$ consecutive Steiner vertices.
  (3) We propose a candidate hard instance as the next step toward resolving
the revised version of Thorup's conjecture.
  Finally, we show promising implications. Almost-optimal parallel algorithms
for computing a generalization of the shortcut that approximately preserves
distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$
depth for exact shortcut paths and exact maximum flow. The state-of-the-art
algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\v{n}, Haeupler,
Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.

</details>


### [14] [Hedgegraph Polymatroids](https://arxiv.org/abs/2510.25043)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Weihang Wang,Weihao Zhu*

Main category: cs.DS

TL;DR: 本文提出了两种基于划分的hedgegraph连通性度量方法，通过多拟阵理论解决了hedgegraph中割函数非子模性带来的算法挑战。


<details>
  <summary>Details</summary>
Motivation: Hedgegraph通过将超边分组到hedge中，能够建模超边之间的依赖关系，但割函数不满足子模性，这给连通性算法带来了障碍。

Method: 引入两种基于划分的连通性度量，研究hedgegraph相关的多拟阵结构，从多拟阵角度分析连通性问题。

Result: 多拟阵视角带来了新的可处理性结果，并推广了图和超图中的经典结果。

Conclusion: 多拟阵方法为hedgegraph连通性问题提供了有效的理论框架和算法解决方案。

Abstract: Graphs and hypergraphs combine expressive modeling power with algorithmic
efficiency for a wide range of applications. Hedgegraphs generalize hypergraphs
further by grouping hyperedges under a color/hedge. This allows hedgegraphs to
model dependencies between hyperedges and leads to several applications.
However, it poses algorithmic challenges. In particular, the cut function is
not submodular, which has been a barrier to algorithms for connectivity. In
this work, we introduce two alternative partition-based measures of
connectivity in hedgegraphs and study their structural and algorithmic aspects.
Instead of the cut function, we investigate a polymatroid associated with
hedgegraphs. The polymatroidal lens leads to new tractability results as well
as insightful generalizations of classical results on graphs and hypergraphs.

</details>


### [15] [$\{s,t\}$-Separating Principal Partition Sequence of Submodular Functions](https://arxiv.org/abs/2510.25664)
*Kristóf Bérczi,Karthekeyan Chandrasekaran,Tamás Király,Daniel P. Szabo*

Main category: cs.DS

TL;DR: 本文提出了子模函数的{s,t}-分离主划分序列理论，证明了其存在性并设计了多项式时间构造算法。该理论应用于两个问题：(1)单调和正模函数的{s,t}-分离子模k划分问题的近似算法；(2)超图定向问题中同时满足强连通度至少k和(s,t)-连通度至少ℓ的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 受两个应用问题驱动：一是{s,t}-分离子模k划分问题的近似算法需求，二是超图定向问题中同时满足多种连通性约束的需求。Narayanan和Fujishige提出的主划分序列理论虽然应用广泛，但需要扩展到{s,t}-分离场景。

Method: 定义了子模函数的{s,t}-分离主划分序列，证明了其存在性，并设计了多项式时间构造算法。

Result: 成功构建了{s,t}-分离主划分序列理论，并应用于两个具体问题：为单调和正模函数的{s,t}-分离子模k划分问题提供了近似算法；为超图定向问题设计了多项式时间算法，能同时满足强连通度和(s,t)-连通度约束。

Conclusion: 提出的{s,t}-分离主划分序列理论扩展了传统主划分序列理论，为子模函数在分离约束下的结构分析提供了新工具，并在两个重要应用问题上取得了算法突破。

Abstract: Narayanan and Fujishige showed the existence of the principal partition
sequence of a submodular function, a structure with numerous applications in
areas such as clustering, fast algorithms, and approximation algorithms. In
this work, motivated by two applications, we develop a theory of
$\{s,t\}$-separating principal partition sequence of a submodular function. We
define this sequence, show its existence, and design a polynomial-time
algorithm to construct it. We show two applications: (1) approximation
algorithm for the $\{s,t\}$-separating submodular $k$-partitioning problem for
monotone and posimodular functions and (2) polynomial-time algorithm for the
hypergraph orientation problem of finding an orientation that simultaneously
has strong connectivity at least $k$ and $(s,t)$-connectivity at least $\ell$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: RepoAlign-Bench是首个针对变更请求驱动的仓库级代码检索基准，包含52k标注实例。ReflectCode采用对抗性反射增强的双塔架构，通过LLM引导的反射动态整合语法模式、函数依赖和语义扩展意图，在Top-5准确率和召回率上分别提升12.2%和7.1%。


<details>
  <summary>Details</summary>
Motivation: 现代代码库复杂性增加，需要能够理解跨组件变更意图的检索系统，而传统函数级搜索范式缺乏这种能力。虽然近期研究改进了自然语言查询与代码片段的对齐，但针对特定变更请求检索上下文相关代码仍未被充分探索。

Method: 提出ReflectCode，一种对抗性反射增强的双塔架构，包含解耦的代码编码器和文档编码器组件。通过大型语言模型引导的反射动态整合语法模式、函数依赖和语义扩展意图。

Result: ReflectCode在Top-5准确率上提升12.2%，在召回率上提升7.1%，优于现有最先进基线方法。

Conclusion: 该研究为上下文感知的代码检索开辟了新方向，从函数中心匹配转向整体仓库级推理。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [17] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出Compiler.next，一种基于搜索的编译器，通过将人类意图转换为工作软件，实现AI原生软件系统的无缝演进，降低非专家的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助软件工程工具存在认知过载、工具集成效率低和AI副驾驶能力有限等问题，需要新的范式来应对软件工程3.0时代的挑战。

Method: 设计基于搜索的编译器架构，通过动态优化认知架构及其组件（提示、基础模型配置、系统参数），在多个目标（准确性、成本、延迟）之间寻找最优权衡。

Result: 提出了Compiler.next的架构设计，并制定了解决意图编译核心挑战的路线图，包括开发质量编程构造、有效搜索启发式、可重复性和编译器间互操作性。

Conclusion: 该愿景为完全自动化、搜索驱动的软件开发奠定了基础，促进更快的创新和更高效的AI驱动系统，是民主化软件开发的关键基石。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [18] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 提出LLM脚本语言(LSL)的概念，旨在通过领域特定语言来规范和控制大语言模型的交互，提高AI应用的可靠性、鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用虽然令人印象深刻，但其不可靠性和产生幻觉内容的倾向阻碍了实际应用。需要软件工程工具来约束LLM输出，提供更强的保证。

Method: 开发领域特定语言(DSL)来编写与LLM的交互脚本，通过LSL控制LLM输出、强制交互结构化，并与验证、验证和可解释性集成。

Result: 提出了LSL的愿景框架，使LLM交互可编程化，并与训练或实现解耦。

Conclusion: LSL可能是改进基于AI应用的关键，能够解决当前LLM软件在可靠性、鲁棒性和可信度方面的局限性。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [19] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct是一个扩展AI辅助自动验证能力的框架，能够从单函数验证扩展到复杂数据结构模块验证，在Verus中实现了99.2%的函数验证成功率。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助验证主要针对单个函数，无法处理复杂数据结构模块的验证需求，且LLM经常误解Verus的注解语法和验证语义。

Method: 使用规划器模块系统生成抽象、类型不变量、规范和证明代码，在提示中嵌入语法指导，并包含修复阶段自动纠正注解错误。

Result: 在11个Rust数据结构模块评估中，成功验证了10个模块，共验证128/129个函数（99.2%成功率）。

Conclusion: 该框架代表了向自动AI辅助形式验证目标迈出的重要一步。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [20] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文提出了Human-AI RE Synergy Model (HARE-SM)框架，将AI驱动的分析与人类监督相结合，以改进需求工程中的需求获取、分析和验证过程。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程依赖劳动密集型手动流程，容易出错且复杂。AI技术（特别是大语言模型、自然语言处理和生成式AI）提供了变革性解决方案，但同时也带来了算法偏见、缺乏可解释性和伦理问题等挑战。

Method: 采用多阶段研究方法，包括准备需求工程数据集、微调AI模型、设计协作式人机工作流程，并提出了HARE-SM概念框架和早期原型实现。

Result: 提出了一个强调通过透明度、可解释性和偏见缓解来实现伦理AI使用的概念框架，为在协作环境中应用智能数据科学技术处理半结构化和非结构化需求工程数据建立了研究议程和实际设计方向。

Conclusion: HARE-SM框架通过整合AI驱动分析和人类监督，为解决AI在需求工程中应用的挑战提供了有效途径，为未来需求工程的发展奠定了理论基础和实践指导。

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [21] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: BeTaL是一个利用LLM自动化设计动态基准测试的框架，通过参数化基准模板和LLM推理来获得目标属性（如难度和真实性），在成本效益高的方式下创建符合期望难度的基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统的手工静态基准测试很快会饱和，而动态基准测试虽然能随模型发展而演进，但创建和持续更新成本高昂。需要一种自动化方法来设计动态基准测试。

Method: BeTaL框架通过参数化基准模板的关键设计选择，使用LLM在参数空间中推理以获得目标属性，从而自动化动态基准设计过程。

Result: BeTaL创建的基准测试更接近期望难度，平均偏差在5.3%到13.2%之间，比基线方法提高了2-4倍。成功创建了两个新基准测试并扩展了τ-bench基准。

Conclusion: BeTaL提供了一种成本效益高的自动化方法来解决动态基准测试设计的挑战，显著提高了基准测试与期望难度的匹配度。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [22] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 提出了一种基于代码属性图和图变换的框架，用于增强现有代码抄袭检测系统对抗重构式混淆攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 编程教育中的抄袭检测面临日益复杂的混淆技术挑战，特别是自动重构式攻击。现有的教育用抄袭检测系统对基本混淆有抵抗力，但难以应对保持程序行为的结构性修改。

Method: 使用代码属性图和图变换技术构建可扩展框架，增强现有最先进的检测器对抗重构式混淆的能力。

Result: 在真实学生提交代码上的综合评估显示，该框架在检测经过算法和AI混淆攻击的抄袭代码方面有显著改进。

Conclusion: 该框架能有效提升代码抄袭检测系统对抗重构式混淆攻击的性能。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [23] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: Adapt是一个基于LLM的动态证明精化框架，通过LLM引导的决策器根据证明助手状态和错误证明上下文动态选择最佳精化策略，显著提升了定理证明成功率。


<details>
  <summary>Details</summary>
Motivation: 现有证明精化方法采用固定策略，无法根据具体证明问题动态选择有效策略，限制了性能。需要一种能够自适应选择精化策略的方法来克服这一限制。

Method: 引入Adapt框架，利用LLM引导的决策器根据证明助手状态和错误证明上下文动态选择合适的精化策略。

Result: 在两个基准测试中，Adapt显著优于四个现有方法，分别证明了16.63%和18.58%更多的定理。在五个不同LLM上都表现出良好的泛化能力。

Conclusion: Adapt框架通过动态策略选择有效提升了定理证明性能，证明了自适应精化策略的重要性。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [24] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix是一种检测和自动修复REST API误用的方法，通过识别不符合规范的代码片段，结合API规范生成提示，利用大语言模型生成修正代码。


<details>
  <summary>Details</summary>
Motivation: 开发者在测试阶段才能发现REST API规范违反问题，错误信息缺乏有效诊断细节，调试过程需要反复试错。

Method: 识别不符合规范的代码片段，将其与相关API规范整合到提示中，利用大语言模型生成修正代码。

Result: dcFix能够准确检测API误用，并且在性能上优于基线方法（在提示中不包含代码片段不符合REST API规范信息的方法）。

Conclusion: dcFix方法有效解决了REST API误用的检测和自动修复问题，显著提升了调试效率。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [25] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: KUMIC框架通过检索机制和思维链优化，提升大语言模型在多意图代码注释生成中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统代码注释生成仅提供通用概述，无法满足开发者对实现细节和用户对使用说明等不同意图的需求，需要多意图注释生成。

Method: 基于上下文学习，KUMIC设计检索机制获取高代码-注释一致性的示例，利用思维链引导LLM关注与特定意图对齐的代码语句，构建从代码到意图特定语句再到注释的映射知识链。

Result: 在BLEU、METEOR、ROUGE-L和SBERT指标上分别比最先进基线方法提升14.49%、22.41%、20.72%和12.94%。

Conclusion: KUMIC通过优化知识利用和构建映射知识链，有效解决了LLM在多意图注释生成中构建正确关系的问题，显著提升了生成质量。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [26] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出了TECS/Rust-OE框架，通过利用调用流和实时OS的独占控制机制，在保持内存安全的同时优化性能，解决了原有TECS/Rust框架因过度独占控制导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统日益复杂，需要选择合适的编程语言确保系统可靠性。现有TECS/Rust框架使用静态可变变量表示系统结构，但为确保线程安全而应用的过度独占控制导致了性能下降。

Method: 提出TECS/Rust-OE框架，利用调用流和实时OS的独占控制机制，自动生成基于组件描述的Rust代码，在保持内存安全的同时优化性能。

Result: 评估显示，优化的独占控制减少了开销，生成的代码具有高可重用性。

Conclusion: TECS/Rust-OE框架成功解决了性能问题，同时保持了内存安全和代码可重用性，为复杂嵌入式系统开发提供了有效解决方案。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [27] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出TECS/Rust框架，将Rust的内存安全特性集成到嵌入式系统组件框架TECS中，以解决C语言内存安全问题，同时保持组件化开发的灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统复杂度增加，基于组件的开发(CBD)成为解决方案，但CBD通常使用C语言存在内存安全问题。需要结合Rust的内存安全特性来改进CBD框架。

Method: 开发TECS/Rust框架，利用Rust的编译时内存安全特性（生命周期和借用检查），为TECS组件框架提供Rust支持，自动生成Rust代码，并与实时操作系统高效集成。

Result: 生成的代码占实际代码的很大比例，与无框架开发的代码相比执行时间差异极小，表明引入的开销可以忽略不计。

Conclusion: TECS/Rust框架成功将Rust的内存安全特性集成到嵌入式系统CBD中，在保证内存安全的同时保持了性能效率。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [28] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 本研究比较了基于属性的测试(PBT)和基于示例的测试(EBT)在检测LLM生成代码边缘案例方面的效果，发现两者结合能达到81.25%的错误检测率，优于单独使用任一方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中生成代码的普及，确保LLM生成代码的质量变得重要。传统的基于示例测试方法经常遗漏边缘案例，因此需要研究更有效的测试方法。

Method: 分析了16个HumanEval问题，使用Claude-4-sonnet生成PBT和EBT测试代码，比较两种方法在检测标准解决方案在扩展测试用例中失败时的效果。

Result: 单独使用PBT或EBT的错误检测率均为68.75%，但两者结合后检测率提升至81.25%。PBT擅长检测性能问题和通过广泛输入空间探索边缘案例，而EBT擅长检测特定边界条件和特殊模式。

Conclusion: 结合PBT和EBT的混合方法可以提高LLM生成代码的可靠性，为LLM基于代码生成的测试策略提供指导。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [29] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus是一个AI辅助的形式验证系统，通过模块化重构和分解-重组工作流，显著提高了代码验证的成功率。


<details>
  <summary>Details</summary>
Motivation: 形式验证对构建可靠软件系统至关重要，但需要专业知识且成本高昂。现代AI系统虽然能识别数学证明模式，但如何有效整合到形式验证过程中仍是一个挑战。

Method: 通过分解复杂程序逻辑为可验证的小组件，验证后重新组合构建原始程序的证明。使用结构化分解将复杂引理分解为可验证的子引理，并在自动化工具不足时允许用户提供自然语言指导。

Result: 在定制数据集中成功验证了86%的任务（基线为68%）。随着规范复杂度的增加，成功率从30%提升到69%；集成复杂程序的证明大纲后，从25%提升到87%。

Conclusion: 临时应用模块化重构显著提高了AI验证单个组件的有效性，证明了AI辅助形式验证的可行性。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [30] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 通过分析Stack Overflow上AI代理开发者的讨论，识别出7大类77个技术挑战，涵盖运行时集成、依赖管理、编排复杂性和评估可靠性等问题，并量化了问题的流行度和难度。


<details>
  <summary>Details</summary>
Motivation: AI代理系统在研究和产业中迅速流行，但开发者在构建、部署和维护这些新兴系统时面临持续且未被充分探索的挑战。

Method: 在Stack Overflow平台上研究开发者讨论，通过标签扩展和过滤构建挑战分类法，应用LDA-MALLET进行主题建模，并手动验证和标记结果主题。

Result: 识别出7个主要领域的重复性问题，包含77个不同的技术挑战，量化了主题流行度和难度，映射了代理开发中使用的工具和编程语言，并跟踪了2021-2025年的演变。

Conclusion: 研究结果为从业者、研究人员和教育工作者提供了关于代理可靠性和开发者支持的具体指导。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [31] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: 对ICSE 2024和ASE 2024会议上86篇LLM相关研究进行分析，发现只有18篇提供了研究构件并使用OpenAI模型。在这18篇中，仅5篇适合复现，但都无法完全复现结果，其中2篇部分可复现，3篇不可复现。


<details>
  <summary>Details</summary>
Motivation: 理解当前LLM研究的可复现性程度，识别阻碍可复现性的因素，为改进实证研究提供建议。

Method: 分析ICSE 2024和ASE 2024会议上86篇LLM相关研究，对其中18篇提供研究构件并使用OpenAI模型的研究进行复现尝试。

Result: 18篇研究中仅5篇适合复现，但都无法完全复现结果：2篇部分可复现，3篇不可复现。

Conclusion: 需要更严格的研究构件评估和更稳健的研究设计，以确保未来出版物的可复现价值。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [32] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL是一个能量感知的模糊测试框架，通过将能耗考虑纳入模糊测试启发式算法，在保持覆盖率的同时减少自动化测试的环境影响。


<details>
  <summary>Details</summary>
Motivation: 传统灰盒模糊测试方法如AFL++主要关注覆盖率最大化，没有考虑探索不同执行路径的能耗成本。持续模糊测试活动消耗大量计算资源并产生显著的碳足迹。

Method: GreenAFL引入了两个关键修改：能量感知的语料库最小化（在减少初始语料库时考虑能耗）和能量引导的启发式算法（将变异导向高覆盖率、低能耗的输入）。

Result: 消融研究表明，当使用至少一个修改时，能够实现最高的覆盖率和最低的能耗。

Conclusion: 将能耗考虑纳入模糊测试启发式算法可以在保持测试效果的同时显著减少环境足迹，为可持续软件测试提供了可行方案。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [33] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: LOCALIZE是一个低代码、配置优先的无线电定位框架，通过声明式配置和标准化工作流实现可复现的机器学习实验，减少编码工作量并保持可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在无线电定位中面临的三个关键问题：低编码工作量、默认可复现性（包括版本控制代码、数据和配置）以及内置可扩展性，现有工具很少能同时满足这三个要求。

Method: 采用低代码、配置优先的方法，实验通过人类可读的配置声明，工作流编排器运行从数据准备到报告的标准化流水线，所有工件（数据集、模型、指标和报告）都进行版本控制。

Result: 与普通Jupyter笔记本基线相比，该框架减少了编写工作量，同时保持了相当的运行时间和内存行为。使用蓝牙低功耗数据集的实验表明，随着数据量从1倍增加到10倍，编排开销保持有界。

Conclusion: LOCALIZE框架使基于机器学习的可复现定位实验变得实用、易于访问且可扩展。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [34] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench是一个环境配置诊断基准，通过过程级轨迹评估来诊断LLM代理在环境设置中的细粒度能力，包括规划、错误诊断、反馈修复等环节。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估端到端的构建/测试成功率，无法揭示代理在环境配置过程中成功或失败的具体原因和位置，限制了软件工程代理能力的深入分析。

Method: 通过自动注入真实的README错误构建任务实例，在Docker中进行可扩展的高质量验证，结合过程级分析和端到端可执行性来评估代理能力。

Result: 评估显示，虽然代理能够定位错误，但在将反馈转化为有效修正方面存在困难，这限制了端到端的性能表现。

Conclusion: Enconda-bench是首个为环境配置提供过程级内部能力评估的框架，为改进软件工程代理提供了可操作的见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [35] [Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty](https://arxiv.org/abs/2510.24869)
*Mehrshad Eskandarpour,Hossein Soleimani*

Main category: cs.NI

TL;DR: 基于PPO的深度强化学习框架用于5G密集网络中的自主负载均衡，通过调整CIO值来优化用户-小区关联，在多目标奖励下平衡效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在密集、高动态的5G无线接入网络中，高效的移动性管理和负载均衡对维持服务质量至关重要。

Method: 使用基于PPO的深度强化学习框架，将控制问题建模为马尔可夫决策过程，通过周期性调整CIO值来引导用户-小区关联，采用多目标奖励函数捕获关键性能指标。

Result: 在500+训练轮次和压力测试中，PPO策略持续改善KPI趋势（更高吞吐量和公平性，更低延迟、抖动、丢包率和切换次数），并表现出快速稳定的收敛。

Conclusion: PPO的裁剪策略更新和基于优势的训练为下一代RAN负载均衡提供了稳健、可部署的控制方案，使用纯Python工具链实现。

Abstract: Efficient mobility management and load balancing are critical to sustaining
Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We
present a deep reinforcement learning framework based on Proximal Policy
Optimization (PPO) for autonomous, QoS-aware load balancing implemented
end-to-end in a lightweight, pure-Python simulation environment. The control
problem is formulated as a Markov Decision Process in which the agent
periodically adjusts Cell Individual Offset (CIO) values to steer user-cell
associations. A multi-objective reward captures key performance indicators
(aggregate throughput, latency, jitter, packet loss rate, Jain's fairness
index, and handover count), so the learned policy explicitly balances
efficiency and stability under user mobility and noisy observations. The PPO
agent uses an actor-critic neural network trained from trajectories generated
by the Python simulator with configurable mobility (e.g., Gauss-Markov) and
stochastic measurement noise. Across 500+ training episodes and stress tests
with increasing user density, the PPO policy consistently improves KPI trends
(higher throughput and fairness, lower delay, jitter, packet loss, and
handovers) and exhibits rapid, stable convergence. Comparative evaluations show
that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based
CDQL baseline across all KPIs while maintaining smoother learning dynamics and
stronger generalization as load increases. These results indicate that PPO's
clipped policy updates and advantage-based training yield robust, deployable
control for next-generation RAN load balancing using an entirely Python-based
toolchain.

</details>


### [36] [Performance Evaluation of Multimedia Traffic in Cloud Storage Services over Wi-Fi and LTE Networks](https://arxiv.org/abs/2510.25079)
*Albert Espinal,V. Sanchez Padilla,Yesenia Cevallos*

Main category: cs.NI

TL;DR: 评估Dropbox、Google Drive和OneDrive在Wi-Fi和LTE网络下多媒体文件上传的性能，Google Drive表现最稳定，Dropbox带宽利用率高但LTE延迟大，OneDrive对移动网络波动敏感。


<details>
  <summary>Details</summary>
Motivation: 了解不同云存储服务在不同网络条件下的性能表现，为优化云存储应用提供参考。

Method: 使用Wireshark捕获流量，分析延迟、抖动、带宽和丢包率等关键指标，并采用双泊松函数建模数据包大小分布。

Result: Google Drive在两种网络中表现最一致，延迟低且抖动小；Dropbox带宽利用率高但LTE延迟较大；OneDrive行为多变，对网络波动敏感。Wi-Fi连接更稳定，LTE性能因平台实现而异。

Conclusion: Wi-Fi为多媒体传输提供更稳定的连接，云存储服务性能受网络类型和平台实现影响，建议使用更大数据集和异构接入网络进行进一步分析。

Abstract: The performance of Dropbox, Google Drive, and OneDrive cloud storage services
was evaluated under Wi-Fi and LTE network conditions during multimedia file
uploads. Traffic was captured using Wireshark, and key metrics (including
delay, jitter, bandwidth, and packet loss) were analyzed. Google Drive
maintained the most consistent performance across both types of networks,
showing low latency and reduced jitter. Dropbox showed efficient bandwidth
utilization, but experienced a longer delay over LTE, attributed to a greater
number of intermediate hops. OneDrive presented variable behavior, with
elevated packet rates and increased sensitivity to fluctuations in the mobile
network. A bimodal distribution of packet sizes was observed and modeled using
a dual Poisson function. In general, Wi-Fi connections provided greater
stability for multimedia transfers, while LTE performance varied depending on
platform-specific implementations. The results contribute to a better
understanding of traffic behavior in cloud-based storage applications and
suggest further analysis with larger datasets and heterogeneous access
networks.

</details>


### [37] [Learning-Based vs Human-Derived Congestion Control: An In-Depth Experimental Study](https://arxiv.org/abs/2510.25105)
*Mihai Mazilu,Luca Giacomoni,George Parisis*

Main category: cs.NI

TL;DR: 本文对基于学习的拥塞控制算法进行了系统性研究，通过与TCP Cubic和BBR等传统算法对比，揭示了现有学习型CC方法的优势和根本局限性。


<details>
  <summary>Details</summary>
Motivation: 随着通信技术、应用和流量负载的快速变化，传统人工设计的静态拥塞控制算法面临严峻挑战。基于学习的CC方法尚处于早期阶段，需要深入研究其局限性并推动可部署解决方案的开发。

Method: 建立了可复现的系统性评估方法，直接对比学习型CC方法与广泛部署的传统算法（TCP Cubic和BBR v3），进行大规模实验分析。

Result: 研究发现：将公平性直接嵌入奖励函数有效但泛化性不足；RL方法能获取全部可用带宽并保持低延迟；现有最新学习型CC方法在带宽和延迟动态变化时表现不佳，但对非拥塞性丢包具有抵抗性。

Conclusion: 基于学习的拥塞控制研究需要更多关注泛化能力和动态环境适应性，同时强调透明性和可复现性对研究机器学习生成策略的重要性。

Abstract: Learning-based congestion control (CC), including Reinforcement-Learning,
promises efficient CC in a fast-changing networking landscape, where evolving
communication technologies, applications and traffic workloads pose severe
challenges to human-derived, static CC algorithms. Learning-based CC is in its
early days and substantial research is required to understand existing
limitations, identify research challenges and, eventually, yield deployable
solutions for real-world networks. In this paper, we extend our prior work and
present a reproducible and systematic study of learning-based CC with the aim
to highlight strengths and uncover fundamental limitations of the
state-of-the-art. We directly contrast said approaches with widely deployed,
human-derived CC algorithms, namely TCP Cubic and BBR (version 3). We identify
challenges in evaluating learning-based CC, establish a methodology for
studying said approaches and perform large-scale experimentation with
learning-based CC approaches that are publicly available. We show that
embedding fairness directly into reward functions is effective; however, the
fairness properties do not generalise into unseen conditions. We then show that
RL learning-based approaches existing approaches can acquire all available
bandwidth while largely maintaining low latency. Finally, we highlight that
existing the latest learning-based CC approaches under-perform when the
available bandwidth and end-to-end latency dynamically change while remaining
resistant to non-congestive loss. As with our initial study, our
experimentation codebase and datasets are publicly available with the aim to
galvanise the research community towards transparency and reproducibility,
which have been recognised as crucial for researching and evaluating
machine-generated policies.

</details>


### [38] [ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks](https://arxiv.org/abs/2510.25145)
*Giancarlo Maldonado Cardenas,Diana C. Gonzalez,Judy C. Guevara,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 提出基于机器学习的随机接入信道前导码碰撞早期检测机制，在多种通信场景下神经网络模型表现最佳，准确率超过98%，通过量化优化后推理时间从2500ms降至0.3ms。


<details>
  <summary>Details</summary>
Motivation: 大规模机器类通信场景中随机接入信道的前导码碰撞是主要瓶颈，需要高效的早期碰撞检测机制来提升蜂窝物联网部署性能。

Method: 使用MATLAB模拟真实信道条件生成标注数据集，评估九种经典分类器（包括树集成、支持向量机和神经网络），并在四种不同通信场景下进行测试，应用训练后量化技术。

Result: 神经网络在分布内评估中达到超过98%的平衡准确率，分布外评估中维持95%准确率。全整数量化将推理时间从2500ms大幅降低至0.3ms，精度损失可忽略。

Conclusion: 该解决方案结合了高检测精度和低延迟推理，适用于真实网络中可扩展的实时蜂窝物联网应用。

Abstract: Preamble collision in the random access channel (RACH) is a major bottleneck
in massive machine-type communication (mMTC) scenarios, typical of cellular IoT
(CIoT) deployments. This work proposes a machine learning-based mechanism for
early collision detection during the random access (RA) procedure. A labeled
dataset was generated using the RA procedure messages exchanged between the
users and the base station under realistic channel conditions, simulated in
MATLAB. We evaluate nine classic classifiers -- including tree ensembles,
support vector machines, and neural networks -- across four communication
scenarios, varying both channel characteristics (e.g., Doppler spread,
multipath) and the cell coverage radius, to emulate realistic propagation,
mobility, and spatial conditions. The neural network outperformed all other
models, achieving over 98\% balanced accuracy in the in-distribution evaluation
(train and test drawn from the same dataset) and sustaining 95\% under
out-of-distribution evaluation (train/test from different datasets). To enable
deployment on typical base station hardware, we apply post-training
quantization. Full integer quantization reduced inference time from 2500 ms to
as low as 0.3 ms with negligible accuracy loss. The proposed solution combines
high detection accuracy with low-latency inference, making it suitable for
scalable, real-time CIoT applications found in real networks.

</details>


### [39] [Adaptive Design of mmWave Initial Access Codebooks using Reinforcement Learning](https://arxiv.org/abs/2510.25271)
*Sabrine Aroua,Christos Anastasios Bovolis,Bo Göransson,Anastasios Giovanidis,Mathieu Leconte,Apostolos Destounis*

Main category: cs.NI

TL;DR: 提出了一种基于强化学习的自适应SSB码本设计框架，在专家知识基础上动态调整波束配置，相比静态专家配置平均提升10.8%的用户连接率。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络中SSB码本由专家精心设计，但在动态或异构环境中缺乏灵活性，限制了整体性能。需要能够适应不同用户分布的智能码本设计方法。

Method: 采用混合强化学习框架，基于专家设计的SSB波束池，通过实时反馈自适应选择或组合波束，无需用户位置信息且满足实际波束约束。

Result: 仿真结果显示，相比静态专家配置，该方法平均提升10.8%的用户连接率。

Conclusion: 结合专家知识与数据驱动优化，能够实现更智能、灵活和鲁棒的波束管理，为下一代无线网络提供有效解决方案。

Abstract: Initial access (IA) is the process by which user equipment (UE) establishes
its first connection with a base station. In 5G systems, particularly at
millimeter-wave frequencies, IA integrates beam management to support highly
directional transmissions. The base station employs a codebook of beams for the
transmission of Synchronization Signal Blocks (SSBs), which are periodically
swept to detect and connect users. The design of this SSB codebook is critical
for ensuring reliable, wide-area coverage. In current networks, SSB codebooks
are meticulously engineered by domain experts. While these expert-defined
codebooks provide a robust baseline, they lack flexibility in dynamic or
heterogeneous environments where user distributions vary, limiting their
overall effectiveness. This paper proposes a hybrid Reinforcement Learning (RL)
framework for adaptive SSB codebook design. Building on top of expert
knowledge, the RL agent leverages a pool of expert-designed SSB beams and
learns to adaptively select or combine them based on real-time feedback. This
enables the agent to dynamically tailor codebooks to the actual environment,
without requiring explicit user location information, while always respecting
practical beam constraints. Simulation results demonstrate that, on average,
the proposed approach improves user connectivity by 10.8$\%$ compared to static
expert configurations. These findings highlight the potential of combining
expert knowledge with data-driven optimization to achieve more intelligent,
flexible, and resilient beam management in next-generation wireless networks.

</details>


### [40] [TCP ROCCET: An RTT-Oriented CUBIC Congestion Control Extension for 5G and Beyond Networks](https://arxiv.org/abs/2510.25281)
*Lukas Prause,Mark Akselrod*

Main category: cs.NI

TL;DR: TCP ROCCET是TCP CUBIC的延迟感知扩展，通过结合RTT和丢包来检测网络拥塞，在5G蜂窝网络中能有效降低延迟和缓冲区膨胀，同时保持与BBRv3相当的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统基于丢包的TCP拥塞控制算法（如CUBIC）在现代蜂窝网络中面临挑战，由于RLC层大缓冲区导致缺乏丢包信号，造成缓冲区膨胀问题。现有算法如BBR虽表现更好但仍会过度填充缓冲区且存在公平性问题。

Method: 提出TCP ROCCET作为TCP CUBIC的延迟感知扩展，除了响应丢包外，还基于往返时间（RTT）来检测网络拥塞。在真实5G网络中进行评估，包括静止和移动场景。

Result: TCP ROCCET相比标准CUBIC能减少延迟和缓冲区膨胀，无需特定网络架构。与BBRv3相比，提供相似吞吐量但保持更低整体延迟。在变化网络条件下表现出改进的拥塞响应能力。

Conclusion: TCP ROCCET通过结合延迟和丢包信号，有效解决了蜂窝网络中TCP CUBIC的性能问题，在保持吞吐量的同时显著降低了延迟和缓冲区膨胀。

Abstract: The behavior of loss-based TCP congestion control algorithms like TCP CUBIC
continues to be a challenge in modern cellular networks. Due to the large RLC
layer buffers required to deal with short-term changes in channel capacity, the
behavior of both the Slow Start and congestion avoidance phases may be heavily
impacted by the lack of packet losses and the resulting bufferbloat. While
existing congestion control algorithms like TCP BBR do tend to perform better
even in the presence of large bottleneck buffers, they still tend to fill the
buffer more than necessary and can have fairness issues when compared to
loss-based algorithms.
  In this paper, we analyze the issues with the use of loss-based congestion
control algorithms by analyzing TCP CUBIC, which is currently the most popular
variant. To mitigate the issues experienced by TCP CUBIC in cellular networks,
we introduce TCP ROCCET, a latency-based extension of TCP CUBIC that responds
to network congestion based on round-trip time in addition to packet loss.
  Our findings show that TCP ROCCET can reduce latency and bufferbloat compared
to the standard CUBIC implementation, without requiring a specific network
architecture. Compared to TCP BBRv3, ROCCET offers similar throughput while
maintaining lower overall latency. The evaluation was conducted in real 5G
networks, including both stationary and mobile scenarios, confirming ROCCET's
improved response to network congestion under varying conditions.

</details>


### [41] [Energy consumption assessment of a Virtual Reality Remote Rendering application over 5G networks](https://arxiv.org/abs/2510.25357)
*Roberto Viola,Mikel Irazola,José Ramón Juárez,Minh Nguyen,Alexander Zoubarev,Alexander Futasz,Louay Bassbouss,Amr A. AbdelNabi,Javier Fernández Hidalgo*

Main category: cs.NI

TL;DR: 本文研究了在真实5G测试环境中VR应用远程渲染的能耗影响，提出了硬件和软件两种能耗监测方案，评估了不同配置下VR远程渲染器的能耗表现。


<details>
  <summary>Details</summary>
Motivation: 远程渲染让轻量设备能够访问高性能图形内容，但引发了远程计算节点、5G核心网、无线接入网和用户设备等网络组件能耗的担忧。

Method: 提出并评估了硬件和软件两种互补的能耗监测方案，使用基于MoQ协议的VR远程渲染器作为测试案例，在不同多媒体和网络配置下测量能耗。

Result: 研究结果揭示了在5G环境中运行真实VR应用时能耗与性能之间的权衡关系。

Conclusion: 该研究为理解VR远程渲染在5G环境中的能耗特性提供了关键见解，有助于优化能耗与性能的平衡。

Abstract: This paper investigates the energy implications of remote rendering for
Virtual Reality (VR) applications within a real 5G testbed. Remote rendering
enables lightweight devices to access high-performance graphical content by
offloading computationally intensive tasks to Cloud-native Network Functions
(CNFs) running on remote servers. However, this approach raises concerns
regarding energy consumption across the various network components involved,
including the remote computing node, the 5G Core, the Radio Access Network
(RAN), and the User Equipment (UE). This work proposes and evaluates two
complementary energy monitoring solutions, one hardware-based and one
software-based, to measure energy consumption at different system levels. A VR
remote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ)
protocol, is used as test case for assessing its energy footprint under
different multimedia and network configurations. The results provide critical
insights into the trade-off between energy consumption and performance of a
real-world VR application running in a 5G environment.

</details>


### [42] [Evaluating Learning Congestion control Schemes for LEO Constellations](https://arxiv.org/abs/2510.25498)
*Mihai Mazilu,Aiden Valentine,George Parisis*

Main category: cs.NI

TL;DR: 本文首次通过仿真驱动的评估方法，系统分析了LEO卫星网络中多种拥塞控制方案的性能，揭示了现有方案在动态网络环境下的局限性，并为设计LEO专用传输协议提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星网络由于频繁切换、快速变化的往返时延和非拥塞性丢包等特性，给拥塞控制带来了独特挑战，需要对这些方案进行系统性评估。

Method: 结合LeoEM框架的轨道动力学和Mininet微基准测试，评估了三类代表性拥塞控制算法：基于丢包的、基于模型的和基于学习的算法，涵盖单流和多流场景以及主动队列管理的交互。

Result: 发现：1）感知切换的丢包方案能回收带宽但增加延迟；2）BBRv3保持高吞吐但响应慢；3）RL方案在动态条件下表现差但抗非拥塞丢包强；4）RTT不对称和多瓶颈会严重降低公平性；5）AQM能恢复公平性和提升效率。

Conclusion: 现有拥塞控制方案在LEO网络中存在严重局限性，需要设计专门针对LEO特性的数据传输协议，AQM在瓶颈处能有效改善性能。

Abstract: Low Earth Orbit (LEO) satellite networks introduce unique congestion control
(CC) challenges due to frequent handovers, rapidly changing round-trip times
(RTTs), and non-congestive loss. This paper presents the first comprehensive,
emulation-driven evaluation of CC schemes in LEO networks, combining realistic
orbital dynamics via the LeoEM framework with targeted Mininet
micro-benchmarks. We evaluated representative CC algorithms from three classes,
loss-based (Cubic, SaTCP), model-based (BBRv3), and learning-based (Vivace,
Sage, Astraea), across diverse single-flow and multi-flow scenarios, including
interactions with active queue management (AQM). Our findings reveal that: (1)
handover-aware loss-based schemes can reclaim bandwidth but at the cost of
increased latency; (2) BBRv3 sustains high throughput with modest delay
penalties, yet reacts slowly to abrupt RTT changes; (3) RL-based schemes
severely underperform under dynamic conditions, despite being notably resistant
to non-congestive loss; (4) fairness degrades significantly with RTT asymmetry
and multiple bottlenecks, especially in human-designed CC schemes; and (5) AQM
at bottlenecks can restore fairness and boost efficiency. These results expose
critical limitations in current CC schemes and provide insight for designing
LEO-specific data transport protocols.

</details>


### [43] [Device to Device Pairs Sharding based on Distance](https://arxiv.org/abs/2510.25552)
*K Prajwal,Tharun K,Navaneeth P,Ishwar Mandal,Kiran M*

Main category: cs.NI

TL;DR: 该论文开发了一个基于距离标准的设备到设备(D2D)通信模型，使用K-means聚类方法对用户设备进行分组，实现不通过基站的直接通信。


<details>
  <summary>Details</summary>
Motivation: 传统蜂窝系统中设备必须通过基站通信，而用户对多媒体数据交换、快速服务、高质量语音通话等需求不断增长，需要提高网络容量。有限的频谱资源需要灵活使用来应对日益增长的需求。

Method: 使用K-means聚类方法基于距离标准对用户设备和蜂窝用户进行分片，建立不涉及eNB的设备到设备通信模型。

Result: 开发了一个D2D通信模型，允许两个终端用户之间直接通信，不通过演进型节点基站。

Conclusion: 设备到设备通信是满足下一代网络高数据速率需求的新兴促进者，能够提高网络容量和频谱利用效率。

Abstract: In the conventional cellular system, devices are not allowed to communicate
directly with each other in the licensed cellular bandwidth and all
communications take place through the base stations. The users requirements has
led the technology to become fast and faster. Multimedia rich data exchange,
fast service, high quality voice calls, newer and more demanding applications,
information at fingertips, everything requires technology and communication
between devices. A constant need to increase network capacity for meeting the
users growing demands has led to the growth of cellular communication networks
from the first generation(1G) to the fifth generation(5G). There will be crores
of connected devices in the coming future . A large number of connections are
going to be heterogeneous, demanding lesser delays, higher data rates, superior
throughput and enhanced system capacity. The available spectrum resources are
limited and has to be flexibly used by mobile network operators to cope with
the rising demands. An emerging facilitator of the upcoming high data rate
demanding next-generation networks are device-to-device(D2D) communication.
This paper has developed a model that establishes Device-to-Device (D2D)
communication between two end-users without involving the eNB (evolved Node B).
We have sharded the UEs and CUs based on the criteria of DISTANCE. To do so, we
used the K-means clustering method.

</details>


### [44] [Deep Reinforcement Learning-Based Cooperative Rate Splitting for Satellite-to-Underground Communication Networks](https://arxiv.org/abs/2510.25562)
*Kaiqiang Lin,Kangchun Zhao,Yijie Mao*

Main category: cs.NI

TL;DR: 提出了一种用于卫星-地下网络的协作速率分割传输框架，通过地上中继转发公共流到地下设备，并使用深度强化学习优化功率分配、消息分割和时隙调度，以最大化地下设备的最小可达速率。


<details>
  <summary>Details</summary>
Motivation: 卫星到地下网络的可靠下行通信面临严重挑战，主要由于地下土壤和空气-土壤界面的折射导致信号严重衰减。

Method: 提出协作速率分割传输框架，使用地上中继解码并转发公共流到地下设备；采用基于近端策略优化算法的深度强化学习解决方案，集成分布感知动作建模和多分支行动者网络，联合优化功率分配、消息分割和时隙调度。

Result: 在地下管道监测场景的仿真结果显示，该方法在各种地下设备数量和地下条件下，相比传统基准策略实现了平均最大最小速率增益超过167%。

Conclusion: 所提出的协作速率分割辅助传输框架和深度强化学习优化方法能有效解决卫星-地下网络中的下行通信挑战，显著提升地下设备的最小可达速率。

Abstract: Reliable downlink communication in satellite-to-underground networks remains
challenging due to severe signal attenuation caused by underground soil and
refraction in the air-soil interface. To address this, we propose a novel
cooperative rate-splitting (CRS)-aided transmission framework, where an
aboveground relay decodes and forwards the common stream to underground devices
(UDs). Based on this framework, we formulate a max-min fairness optimization
problem that jointly optimizes power allocation, message splitting, and time
slot scheduling to maximize the minimum achievable rate across UDs. To solve
this high-dimensional non-convex problem under uncertain channels, we develop a
deep reinforcement learning solution framework based on the proximal policy
optimization (PPO) algorithm that integrates distribution-aware action modeling
and a multi-branch actor network. Simulation results under a realistic
underground pipeline monitoring scenario demonstrate that the proposed approach
achieves average max-min rate gains exceeding $167\%$ over conventional
benchmark strategies across various numbers of UDs and underground conditions.

</details>


### [45] [MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization](https://arxiv.org/abs/2510.25705)
*Elif Ebru Ohri,Qi Liao,Anastasios Giovanidis,Francesca Fossati,Nour-El-Houda Yellas*

Main category: cs.NI

TL;DR: MetaLore是一个基于深度强化学习的框架，用于在元宇宙或数字孪生环境中联合分配通信和计算资源，通过引入新的信息年龄指标来优化同步性能。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实和虚拟现实的发展，物理世界与数字世界之间的无缝同步仍然是一个关键挑战，特别是在实时应用中，延迟会影响用户体验。

Method: 提出MetaLore框架，使用深度强化学习动态分配通信带宽和计算资源，引入Age of Request Information (AoRI) 和 Age of Sensor Information (AoSI) 两个新指标，并集成到奖励函数中。

Result: DRL解决方案通过使用网络侧两个队列长度的小型任务导向观察空间，实现了与完全枚举暴力解决方案相当的性能，并能有效自主适应动态流量条件。

Conclusion: MetaLore框架能够有效优化元宇宙环境中的资源分配，提高同步质量，同时保证端到端延迟要求。

Abstract: As augmented and virtual reality evolve, achieving seamless synchronization
between physical and digital realms remains a critical challenge, especially
for real-time applications where delays affect the user experience. This paper
presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for
joint communication and computational resource allocation in Metaverse or
digital twin environments. MetaLore dynamically shares the communication
bandwidth and computational resources among sensors and mobile devices to
optimize synchronization, while offering high throughput performance. Special
treatment is given in satisfying end-to-end delay guarantees. A key
contribution is the introduction of two novel Age of Information (AoI) metrics:
Age of Request Information (AoRI) and Age of Sensor Information (AoSI),
integrated into the reward function to enhance synchronization quality. An open
source simulator has been extended to incorporate and evaluate the approach.
The DRL solution is shown to achieve the performance of full-enumeration
brute-force solutions by making use of a small, task-oriented observation space
of two queue lengths at the network side. This allows the DRL approach the
flexibility to effectively and autonomously adapt to dynamic traffic
conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo是一个利用群体智能和分布式成对排名共识的新型AI推理协议，通过声誉加权的异构模型协作显著提升推理性能，在多个基准测试中表现优于多数投票方法，同时具备抗Sybil攻击和恶意参与者过滤能力。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和训练规模收益递减，需要能够水平和垂直扩展的推理层来满足需求，通过分布式群体智能实现高质量AI推理的民主化访问。

Method: 采用群体推理方法：基于成对排名的Bradley-Terry风格聚合模型，结合链上声誉系统和能力证明共识机制，通过声誉加权实现异构模型间的协作。

Result: 在GPQA Diamond基准上达到85.90%准确率，比相同模型集的多数投票方法(68.69%)提升17.21个百分点(+25.1%相对提升)；在对抗性提示注入攻击下仅退化0.12%，而单体模型基线退化6.20%。

Conclusion: 该协议为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化访问，同时不牺牲可靠性或安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [47] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 该论文分析了弱到强泛化现象，即强模型在弱模型监督下训练能超越教师模型。研究聚焦于从线性CNN到两层ReLU CNN的泛化机制，识别了数据稀缺和数据丰富两种机制，并揭示了良性过拟合、有害过拟合和标签校正等关键机制。


<details>
  <summary>Details</summary>
Motivation: 现有关于弱到强泛化的理论研究大多局限于抽象框架或线性/随机特征模型，缺乏对实际CNN架构的深入分析。本文旨在填补这一空白，为弱到强泛化提供更具体的理论解释。

Method: 使用结构化数据（包含不同难度的标签相关信号和标签无关噪声），分析当强模型在预训练弱模型标注的数据上使用梯度下降训练时的动态过程。研究从线性CNN到两层ReLU CNN的泛化。

Result: 识别了两种机制：数据稀缺机制中，泛化通过良性过拟合或有害过拟合发生，取决于数据量；数据丰富机制中，泛化在早期阶段通过标签校正实现，但过度训练会降低性能。

Conclusion: 弱到强泛化在不同数据机制下表现出不同的动态特性，为理解这一现象提供了更全面的理论框架，并揭示了训练过程中的关键转折点。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [48] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应性景观，计算20个生物学相关特征来表征景观地形，帮助解释和比较适应性预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试缺乏关于基础适应性景观的地形信息，这阻碍了模型性能的解释和比较。

Method: GraphFLA从DNA、RNA、蛋白质等不同模态的突变数据构建适应性景观，计算20个特征来表征4个基本方面的景观地形。

Result: 应用于5,300多个景观，展示了其在解释和比较数十个适应性预测模型性能方面的实用性，并发布了155个组合完整的经验适应性景观。

Conclusion: GraphFLA提供了一个强大的框架来分析和比较适应性景观，有助于理解影响模型准确性的因素和不同模型的各自优势。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [49] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 该论文研究了在ESP32-S3微控制器上使用压缩CNN模型和低功耗广域网进行环境监测，通过设备端推理和仅传输结果，将整体能耗降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境挑战日益严峻，需要设计能够在偏远地区长期运行的节能物联网设备，特别是涉及图像数据的应用。数据传输能耗高，在微控制器上直接进行推理可以减少消息大小，延长物联网节点寿命。

Method: 在ESP32-S3上评估使用常见低功耗广域网和针对特定领域数据集训练的压缩CNN模型，采用训练后量化技术压缩模型。

Result: 设备端执行CNN推理并仅传输结果，相比发送原始图像数据，整体能耗降低高达5倍。量化模型相比非量化模型精度仅下降几个百分点。

Conclusion: 这些发现支持开发具有减少碳足迹的物联网应用，通过嵌入式机器学习在环境监测场景中实现自主运行。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [50] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 论文发现OOD泛化基准中ID与OOD准确率的正相关性可能是异质OOD样本聚合的假象，通过OODSelect方法识别出语义一致的OOD子集，其中高ID准确率反而预测低OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 现有OOD基准中ID与OOD准确率的正相关性常被解释为虚假相关性在实践中很少见，但作者怀疑这种模式可能是聚合异质样本造成的假象。

Method: 使用基于梯度的OODSelect方法识别语义一致的OOD子集，在这些子集中验证ID与OOD准确率的关系。

Result: 在广泛使用的分布偏移基准中，OODSelect发现了占标准OOD集一半以上的子集，其中高ID准确率预测低OOD准确率，与整体趋势相反。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，需要更细粒度的分析方法来理解模型在分布偏移下的行为。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [51] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应多任务EEG分类器，使用GRU-TCN网络结合深度Q网络实时调整决策阈值，用于卒中快速分诊，在卒中类型分类上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确、床旁可部署的工具；EEG有潜力但在首次接触时使用不足。

Method: 将32通道EEG信号转换为功率谱密度特征，使用GRU-TCN网络预测卒中类型、半球偏侧化和严重程度，并应用深度Q网络实时调整决策阈值。

Result: 基线GRU-TCN在卒中类型分类上达到89.3%准确率，严重程度96.9%，偏侧化96.7%；加入DQN阈值自适应后，卒中类型准确率提升至98.0%。

Conclusion: 自适应阈值调整可将操作点转移到临床偏好的敏感度-特异度权衡，同时集成的头皮图和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [52] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 本文系统研究了低秩伪逆在噪声环境下的谱范数鲁棒性，推导了尖锐的非渐近扰动界，揭示了误差如何随特征间隙、谱衰减和噪声对齐而缩放。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵经常受到噪声影响，但低秩逆近似的谱范数鲁棒性仍缺乏深入理解，需要系统研究噪声环境下低秩逆近似的稳定性。

Method: 引入轮廓积分技术的新颖应用，针对非全纯函数f(z)=1/z进行分析，推导出改进的扰动界，相比经典全逆界提升高达√n倍。

Result: 在温和的噪声假设下，获得了尖锐的非渐近扰动界，经验验证表明新界能准确跟踪真实扰动误差，而基于经典结果的估计往往显著高估。

Conclusion: 研究结果为噪声计算环境中低秩逆近似提供了实用的、频谱感知的保证，改进了对低秩伪逆鲁棒性的理解。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [53] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，在主题一致性、困惑度和下游分类任务上优于传统LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性化和可解释性。

Method: nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成，并开发了随机变分期望最大化算法来联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类任务上持续优于LDA和Dirichlet-Multinomial回归。

Conclusion: 结果表明，在存在辅助信息的场景下，将神经表示学习与概率主题建模相结合具有显著优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [54] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵在谱范数下的新扰动界，改进了经典的Eckart-Young-Mirsky定理，并应用于差分隐私PCA，解决了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声或测量误差如何影响低秩近似，特别是在谱范数下。差分隐私低秩近似中，需要在保护隐私的同时保持数据的top-p结构。现有工作多分析Frobenius范数误差，但可能高估或低估真实子空间失真。

Method: 使用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱泛函类。在温和的特征值间隙和范数条件下，建立了高概率谱范数扰动界。

Result: 我们的界限对‖(A+E)_p - A_p‖给出了尖锐估计，改进因子可达√n。经验结果证实，我们的界限在不同扰动机制下紧密跟踪实际谱误差。

Conclusion: 我们建立了改进的谱范数扰动界，为差分隐私PCA提供了改进的效用保证，解决了文献中的开放问题，并且我们的分析方法可扩展到更广泛的谱泛函。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [55] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一个快速准确的冰盖建模模拟器，将Kolmogorov-Arnold网络作为特征校准器置于图卷积网络之前，通过可学习的一维扭曲和线性混合步骤改善特征条件化和非线性编码。


<details>
  <summary>Details</summary>
Motivation: 提高冰盖数值模型模拟器的性能，通过KAN前端改善特征条件化，在不增加消息传递深度的情况下提升非线性编码能力。

Method: 使用KAN作为GCN的特征校准器，应用可学习的一维扭曲和线性混合步骤，在Pine Island Glacier的36个融化率模拟数据集上进行训练和测试。

Result: 在2-5层架构中，KAN-GCN匹配或超越了纯GCN和MLP-GCN基线的准确性。在较粗网格上通过用节点级变换替换边级消息传递层提高了推理吞吐量。

Conclusion: KAN优先设计为大型瞬态场景扫描提供了有利的准确性与效率权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [56] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出了一种分布式机器学习的计算资源优化算法，通过协同优化数据分配和CPU资源分配，在时变网络中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 随着AI研究的快速发展，对快速、计算高效且可扩展的解决方案需求日益增长，需要优化分布式机器学习的计算资源分配。

Method: 采用协同优化方法，在时变信息共享网络中同时优化数据处理和计算资源分配，支持对数尺度量化通信，确保算法始终可行。

Result: 与现有CPU调度方案相比，该算法将成本最优性差距改善了50%以上，证明了向最优情况的收敛性。

Conclusion: 该算法为分布式机器学习提供了有效的计算资源优化解决方案，在保持训练性能的同时显著提升了资源利用效率。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [57] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出了WBT-BGRL框架，通过加权三元组损失增强自举学习，用于二部图的归纳链接预测


<details>
  <summary>Details</summary>
Motivation: 二部图链接预测在推荐系统和故障检测中很重要，但现有方法在归纳、加权和二部图场景中的效果未经验证，对比方法存在负采样效率低和偏差问题

Method: 使用双GCN编码器的二部图架构，通过加权三元组损失增强自举学习，是非对比性框架

Result: 在真实数据集（工业和电商）上表现出竞争力，特别是在预训练阶段应用加权时效果更好

Conclusion: 加权非对比学习对二部图的归纳链接预测具有重要价值

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [58] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现LLMs生成的思维链中许多推理步骤并不真正影响最终预测，提出了True Thinking Score来量化每个步骤的因果影响，发现只有少量步骤真正驱动模型预测，并识别了TrueThinking方向来操控推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成的思维链常被视为模型内部思考过程的忠实反映，用于监控不安全意图，但研究发现许多推理步骤实际上并不真正贡献于最终预测，这影响了LLM推理的效率和思维链的可信度。

Method: 提出了True Thinking Score来测量每个推理步骤对最终预测的因果影响，识别了TrueThinking方向，并通过沿该方向或反方向的操控来强制模型执行或忽略特定推理步骤。

Result: 在AIME数据集上，只有平均2.3%的推理步骤具有高TTS值（≥0.7），表明大部分步骤是装饰性的。通过操控TrueThinking方向可以改变模型对验证步骤的内部推理，从而影响最终结果。

Conclusion: LLMs经常在思维链中表达推理步骤但实际上并不在内部执行它们，这既削弱了LLM推理的效率，也损害了思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [59] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型存在文化敏感神经元，这些神经元对特定文化背景的输入表现出选择性敏感，在文化多样性视觉问答中起重要作用。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现优异，但在处理文化相关输入时仍有困难，需要理解模型如何处理文化背景信息。

Method: 使用CVQA基准识别文化选择性神经元，通过消融实验验证其重要性，并提出新的基于边际的选择器CAS来识别文化敏感神经元。

Result: 实验发现存在文化敏感神经元，消融这些神经元会显著影响对应文化问题的性能，而对其他文化影响很小。CAS方法优于现有的基于概率和熵的方法。

Conclusion: 文化敏感神经元倾向于聚集在某些解码层中，这为理解多模态表征的内部组织提供了新视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [60] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 该论文通过算法-硬件协同设计，同时提升机器学习的效率和可靠性，包括模型压缩、近似贝叶斯推理、数字/模拟硬件优化，以及概率光子计算等创新方法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习在嵌入式等资源受限平台上的计算需求日益增长，需要既高效又能在分布偏移下提供可靠预测的解决方案。贝叶斯神经网络能量化不确定性但计算开销大。

Method: 采用算法和硬件效率的联合优化：Galen系统进行自动分层压缩；模拟加速器建模设备缺陷并扩展噪声训练；开发解析和集成近似替代昂贵采样；概率光子计算利用模拟噪声作为熵源。

Result: 实现了资源高效且鲁棒的推理，为传统和贝叶斯神经网络提供了可行的解决方案，显著提升了在嵌入式平台上的部署效率。

Conclusion: 通过算法-硬件协同设计，效率和可靠性可以共同推进，为下一代可信赖、高能效的机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [61] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文发现现代语言模型具有低秩结构，可以利用这种结构通过不相关提示的线性组合来生成目标提示的响应。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的固有低维结构，理解其内在表示特性。

Method: 将语言模型视为序列概率模型，分析其logits矩阵的低秩特性，并利用线性组合方法进行生成。

Result: 实证表明多种现代语言模型都表现出低秩结构，且可以利用这种结构进行有效生成。

Conclusion: 语言模型的低秩结构是一个普遍现象，为模型分析和生成提供了新的理论框架和实用方法。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [62] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出了一个基于排名条件的委员会框架，通过为每个构象排名分配深度神经网络委员会，实现了构象不确定性与认知不确定性的分离，在SARS-CoV-2抗体对接中表现优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习辅助定向进化方法通常依赖单一构象或单一委员会来处理所有构象，无法有效分离构象不确定性与认知不确定性，限制了抗体适应性景观探索的效率。

Method: 引入排名条件委员会框架，利用排名构象为每个排名分配深度神经网络委员会，从而在原理上分离认知不确定性和构象不确定性。

Result: 在SARS-CoV-2抗体对接任务中验证了该方法的有效性，相比基线策略取得了显著改进。

Conclusion: 该方法为治疗性抗体发现提供了可扩展的途径，并直接解决了构象不确定性建模的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [63] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出基于博弈论的端到端特征选择框架，通过评估特征的协同交互和边际贡献来确定特征重要性，显著降低计算成本同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据量指数增长导致机器学习模型训练计算成本激增，许多特征对模型性能无正面贡献却消耗大量计算资源。

Method: 基于合作博弈论构建特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定重要性，包含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明该方法在保持预测性能的同时实现了显著的计算量减少。

Conclusion: 该框架为大规模机器学习计算挑战提供了高效解决方案，源代码已开源。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [64] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种用于离线强化学习的风险感知扩散策略，通过似然比检验在采样时提供统计风险控制，在保持训练不变的情况下改进回报与分布外性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在采样时通常使用缺乏统计风险概念的启发式方法，需要一种能够提供用户可解释风险预算的风险感知采样规则。

Method: 将每个去噪步骤视为无条件先验和状态条件策略头之间的序贯假设检验，累积对数似然比并用逻辑控制器门控条件均值，阈值τ在校准后满足用户指定的Type-I水平α。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion相比强Q引导基线改进了回报与分布外性能的权衡，同时满足期望的α水平。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性的、校准的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [65] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一种结合实时癫痫检测和预测的新方法，使用多种机器学习算法在CHB-MIT头皮EEG数据库上进行评估，其中逻辑回归在检测方面表现最佳，LSTM在预测方面达到89.26%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅在癫痫发作后检测，限制了早期干预机会。本研究旨在开发能够检测和预测癫痫发作的系统，从被动管理转向主动预防。

Method: 使用CHB-MIT头皮EEG数据库（969小时记录，173次癫痫发作），采用K近邻、逻辑回归、随机森林、支持向量机进行癫痫检测，使用LSTM网络进行癫痫预测。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和SVM准确率94.0%但召回率为0%；LSTM预测准确率89.26%。

Conclusion: 该研究展示了开发实时监测工具的潜力，能够检测和预测癫痫发作，实现从被动管理到主动预防的转变，帮助患者采取预防措施降低风险。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [66] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应轨迹分割和选项发现来改进分层强化学习。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中具有理论优势，但实践中面临自主发现语义子目标和学习最优选项终止边界的挑战。

Method: 集成自监督Transformer变化点检测模块，利用内在信号生成启发式伪标签推断环境动态的潜在变化，并通过三种方式利用推断的变化点：稳定终止函数梯度、通过分段行为克隆预训练内部选项策略、在CPD定义的状态分区上施加选项间差异惩罚。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本效率更高和更鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [67] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统解构了矩阵白化优化器，发现其性能优势主要来自方差适应而非谱归一化，方差适应版本始终优于符号下降对应方法。


<details>
  <summary>Details</summary>
Motivation: 研究各种近似矩阵白化变换的优化器，旨在分离解释性能的关键组件，理解为什么矩阵白化方法比元素级方法表现更好。

Method: 通过实验系统解构矩阵白化优化器，比较不同变体的性能，特别关注谱归一化和方差适应两个组件的作用。

Result: 所有矩阵白化方法都可靠地优于Adam等元素级方法；性能提升主要来自方差适应而非准确的谱归一化；低秩方差估计器能有效降低内存成本且不损失性能。

Conclusion: 矩阵白化优化器的成功关键在于方差适应组件，而非谱下降方向；方差适应策略是解释性能差距的被忽视因素。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [68] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，能够将多区域神经记录分解为共享和私有潜在子空间，通过新颖的对齐和解缠损失来分离网络级动态与区域特定活动。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，分离共享网络级动态与区域特定活动是一个核心挑战，需要开发能够有效分解这些成分的方法。

Method: SPIRE采用深度多编码器自编码器架构，引入对齐和解缠损失来因子化记录，仅使用基线数据进行训练，能够稳健地恢复跨区域结构。

Result: 在具有真实潜在变量的合成基准测试中，SPIRE在非线性扭曲和时间错位下优于经典概率模型。在颅内深部脑刺激记录中，SPIRE显示共享潜在变量可靠地编码刺激特异性特征，这些特征在不同位点和频率间具有泛化性。

Conclusion: SPIRE为分析刺激下的多区域神经动态提供了一个实用、可复现的工具，能够揭示外部扰动如何重组跨区域结构。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [69] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 该研究系统评估了基于放射组学的机器学习模型在MRI序列分布偏移下的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持高F1分数，而使用所有特征的模型性能下降40%。数据增强显著改善了不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 基于放射组学的机器学习模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响，需要系统评估其鲁棒性。

Method: 使用16个水果的体模，评估了：(1)5种MRI序列的协议变化；(2)分割变化（完整、部分、旋转）；(3)观察者间变异性。训练XGBoost分类器，比较协议不变特征与序列特定特征。

Result: 使用协议不变特征训练的模型在分布偏移下F1分数>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据增强使预期校准误差降低35%，温度缩放效果有限。

Conclusion: 协议感知特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [70] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计任务的有向无环混合图距离度量方法，用于评估因果发现方法在潜在混杂下的表现。


<details>
  <summary>Details</summary>
Motivation: 因果发现领域缺乏在潜在混杂情况下有效评估发现图质量的度量方法，现有方法难以衡量真实进展。

Method: 使用基于固定识别的符号验证器，量化图差异如何扭曲不同处理-结果对的因果效应估计量。

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较。

Conclusion: 提出的图距离度量为评估因果发现方法在潜在混杂下的性能提供了新的有效工具。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [71] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出DWMGrad优化算法，通过动态指导机制更新动量和学习率，解决传统优化算法在处理复杂模型和非凸优化问题时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习研究中，SGD和Adam等优化算法在处理学习效率波动、复杂模型需求和非凸优化问题时存在明显不足，主要源于算法在应对复杂数据结构和模型时的局限性。

Method: 基于传统方法，引入依赖历史数据的动态指导机制，动态更新动量和学习率，使优化器能灵活调整对历史信息的依赖，适应不同训练场景。

Result: 通过大量实验验证，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法能够更好地适应变化的环境和任务复杂性，在深度学习优化方面表现出优越性能。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [72] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数，相比现有方法具有更好的准确性和可扩展性，使集成黑盒函数的新架构探索成为可能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不增加可训练参数的情况下通过集成黑盒函数来提升模型性能，为结合模拟和数字设备扩展网络规模提供路径。

Method: 提出Bounded Numerical Differentiation (BOND)方法，这是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数。

Result: 实验表明，固定未训练网络形式的黑盒函数可以在不增加可训练参数的情况下提升模型性能，且无需对黑盒函数本身进行大量优化。

Conclusion: 利用固定不可训练模块扩展模型容量具有潜力，为结合模拟和数字设备扩展网络规模提供了可行路径。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [73] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化来平衡模型适应性和稳定性，有效应对用户偏好随时间变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法主要关注保留先前任务的性能，但这忽视了推荐系统的特殊性：目标不是预测过去的偏好，过时的偏好甚至可能在当前兴趣显著变化时损害性能。

Method: 提出PESO方法，引入近端正则化器，将当前适配器锚定到其最近的冻结状态，使模型能够灵活平衡适应性和保留性，更好地捕捉最近的用户行为。

Result: 理论上证明这种近端设计在LoRA子空间中提供数据感知、方向性指导；实证上PESO持续优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过近端正则化有效解决了推荐系统中持续学习的独特挑战，在平衡模型适应性和稳定性方面表现出色。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [74] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB是一个多视图信息瓶颈框架，通过将图分解为特征、结构和扩散视图来缓解GNN中的复杂性偏见，使用对比学习和条件信息瓶颈来平衡任务效用和公平性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理关系数据时会放大训练数据偏见，传播歧视性属性和结构不平衡到不公平结果中。现有公平性方法通常将偏见视为单一来源，忽略了不同属性和结构的影响，导致公平性和效用的次优权衡。

Method: 提出FairMIB框架：1）将图分解为特征、结构和扩散视图；2）使用对比学习最大化跨视图互信息以学习无偏见表示；3）集成多视角条件信息瓶颈目标来最小化与敏感属性的互信息；4）在扩散视图中引入逆概率加权邻接校正以减少偏见传播。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。

Conclusion: FairMIB通过多视图分解和信息瓶颈方法有效缓解了GNN中的偏见传播问题，在保持任务效用的同时显著提升了公平性。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [75] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文研究表明，在某些情况下，训练和测试数据分布不匹配可能是有益的，即使各组件之间没有相关性或迁移学习，测试性能仍可能因训练比例不匹配而提升。


<details>
  <summary>Details</summary>
Motivation: 探讨训练和测试数据分布比例不匹配时的性能影响，挑战传统认为分布匹配总是最优的观点。

Method: 分析混合分布场景，识别最优训练比例，并扩展到组合技能分布不同的情况。

Result: 发现在多种场景下，分布偏移可以带来益处，测试性能可能因训练比例不匹配而提升。

Conclusion: 分布偏移在某些情况下具有潜在益处，这为机器学习训练策略提供了新的视角。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [76] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出神经微分流形(NDM)架构，将神经网络重新概念化为微分流形，通过几何结构增强泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络使用欧几里得参数空间，缺乏几何结构。NDM旨在将几何结构显式融入网络设计，提供内在正则化并增强可解释性。

Method: 三层架构：坐标层实现平滑图表转换，几何层通过辅助子网络动态生成流形度量，演化层通过双目标损失函数优化任务性能和几何简洁性。

Result: 该框架支持与学习流形几何对齐的自然梯度下降优化，为内部表示提供清晰的几何意义，增强泛化性和鲁棒性。

Conclusion: 神经微分流形代表了向几何结构化、可解释且高效深度学习系统的根本性转变，尽管仍面临计算挑战。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [77] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出了一个统一的对抗学习双层模型，从数据扰动角度解释聚类模型中的对抗攻击机制，并分析了δ度量的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型结构复杂，对抗攻击机制尚未得到很好解释，攻击效果衡量方法也不明确，需要研究对抗学习的基本原理。

Method: 提出统一的对抗学习双层模型，从数据扰动视角分析聚类模型的对抗攻击，研究δ度量在聚类模型对抗学习中的适用性。

Result: 发现当数据扰动较小时聚类模型具有鲁棒性，当扰动较大时聚类结果会改变导致攻击；δ度量在聚类模型对抗学习中具有良好定义。

Conclusion: 建立了对抗学习的统一框架，阐明了聚类模型中对抗攻击的数据扰动机制，验证了δ度量在衡量攻击效果方面的有效性。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [78] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出一种基于数据驱动的降维方法，专门用于处理双曲波传播的物理数据，使用低秩神经表示(LRNR)架构，能够从数据中学习高效的波传播低维表示。


<details>
  <summary>Details</summary>
Motivation: 针对物理数据中双曲波传播的高维特性，需要开发能够有效降维并保持物理意义的方法，理论证明这类波存在高效表示的可能性。

Method: 在超网络框架中使用低秩神经表示(LRNR)架构，结合深度学习技术直接从数据中学习波传播的低维表示，利用低秩张量表示。

Result: 训练后的LRNR自然产生低秩张量表示，揭示了波传播的新分解方式，每个分解模式对应可解释的物理特征，并支持通过压缩方案进行高效推理。

Conclusion: LRNR架构能够有效学习波传播的低维表示，不仅提供物理可解释性，还具备高效推理能力，适合在性能要求高的场景中部署。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [79] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络耦合在单一目标下，通过token级交叉注意力层实现邻居间细粒度消息传递，在友谊预测和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常同时具有序列性和关系性，但现有序列建模和图建模方法往往忽略其中一个模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构：1）将序列编码器与GNN在单一目标下耦合，允许梯度在两者间流动；2）引入TOKENXATTN层，通过token级交叉注意力在邻居序列的事件间传递消息。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）两个场景中，BRIDGE在排序和分类指标上持续优于静态GNN、时序图方法和纯序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过统一的端到端架构和细粒度token级消息传递，有效提升了关系序列数据的建模性能。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [80] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 提出一个统一框架，将数据增强与因果推断结合，证明当结果生成机制对数据增强不变时，数据增强可视为对治疗生成机制的干预，有助于减少隐藏混杂因素带来的因果效应估计偏差。


<details>
  <summary>Details</summary>
Motivation: 在存在未观测混杂因素时，通常使用工具变量(IVs)，但IVs在许多应用中不如数据增强(DA)容易获得。本文旨在探索DA在超越i.i.d.设置下的应用，用于干预间的泛化。

Method: 通过适当正则化基于IV的估计器，引入IV-like(IVL)回归概念，将参数化DA建模为IVL回归问题，并通过组合使用模拟最坏情况下的DA应用。

Result: 理论和模拟实验表明，该方法能改善因果估计和泛化任务的性能，超越简单DA的效果。真实数据实验也支持了这一观点。

Conclusion: 数据增强可作为工具变量的替代方案，在因果推断中有效减少混杂偏差，提高跨干预的泛化性能，特别是在工具变量不可得的情况下。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [81] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种Lipschitz感知的线性嫁接方法，通过将非线性激活函数替换为线性函数来消除近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经网络验证中存在近似误差问题，这些误差阻碍了获得紧的局部Lipschitz常数，而紧的Lipschitz常数对于认证鲁棒性至关重要。线性嫁接可以消除近似误差，但缺乏理论解释。

Method: 提出Lipschitz感知的线性嫁接方法，将非线性激活函数替换为线性函数，消除主导的近似误差源，从而获得更紧的局部Lipschitz常数。

Result: 实验表明，在线性嫁接影响较大的激活函数上应用该方法，能够收紧l∞局部Lipschitz常数，并提高认证鲁棒性，即使没有进行认证训练。

Conclusion: 线性嫁接通过消除近似误差来收紧局部Lipschitz常数，是提高认证鲁棒性的有效方法，理论分析为此提供了支持。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [82] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决最优停电问题，通过利用不同实例间的共享模式，在保证解质量的同时显著提高求解速度。


<details>
  <summary>Details</summary>
Motivation: 为缓解野火点火风险，电力公司需要在高风险区域断电。最优停电问题是计算复杂的混合整数线性规划问题，需要在操作环境中快速频繁求解。由于不同实例具有共同结构，这促使使用机器学习方法来利用跨实例的共享模式。

Method: 开发了机器学习引导的框架，扩展了现有的ML引导MILP求解方法，同时整合了关于通电和断电线路数量的领域知识。

Result: 在基于加州的大规模合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量解。

Conclusion: 机器学习引导的方法能够有效解决最优停电问题，在保证解质量的同时显著提高求解效率，适用于需要快速决策的电力系统操作环境。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [83] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选可泛化的时间步，避免对不确定和异常时间步的过拟合。


<details>
  <summary>Details</summary>
Motivation: 深度模型在时间序列预测中容易因噪声和异常而产生严重过拟合，传统方法对所有时间步进行统一优化会导致模型学习不确定和异常模式。

Method: 引入选择性学习框架，使用不确定性掩码（基于残差熵）和异常掩码（基于残差下界估计）来筛选时间步，仅对可泛化时间步计算MSE损失。

Result: 在8个真实数据集上的实验表明，该方法显著提升了主流深度模型的预测性能，其中Informer的MSE降低了37.4%，TimesNet降低8.4%，iTransformer降低6.5%。

Conclusion: 选择性学习策略能有效缓解深度时间序列预测中的过拟合问题，通过关注可泛化时间步来提升模型性能。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [84] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的成本敏感多类PU学习方法，通过为正向和推断负向损失分配不同的数据依赖权重，实现无偏风险估计，在多个数据集上表现出优于基线的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多类PU学习(MPU)面临挑战，许多现有方法无法确保无偏风险估计，限制了性能和稳定性。现实应用中标注可靠负样本困难或成本高，需要有效的MPU解决方案。

Method: 在经验风险最小化框架下，为正向和从未标注混合数据中推断的负向损失分量分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器。

Result: 在8个公共数据集上的广泛实验表明，在不同类别先验和类别数量下，相比强基线方法在准确性和稳定性方面都取得了持续提升。

Conclusion: 提出的自适应损失加权方法为多类PU学习提供了有效的无偏风险估计框架，在多个数据集上验证了其优越性能。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [85] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 本文提出了Bulk-Space-Filtration-Accelerator (BSFA)框架，通过差异化缩放损失Hessian不同特征子空间中的参数更新来加速深度学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明深度学习优化中存在根本性二分法：沿损失Hessian顶部特征方向的参数更新虽然幅度大但对损失减少贡献小，而正交分量（Bulk-space）的更新幅度小但驱动大部分学习进展。

Method: BSFA框架通过差异缩放不同子空间的更新组件来加速训练，使用PCA对历史更新进行高效子空间估计，并采用逐块策略在参数块基础上应用估计。

Result: BSFA在各种任务中表现出加速效果，在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比原始AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，通过有效利用损失Hessian的子空间结构来同时增强训练稳定性和收敛速度。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [86] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 本文提出了两种改进贝叶斯网络结构采样效率的技术：高效实现基本移动操作，以及通过预处理修剪可能的父集来加速求和计算。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构推断通常通过马尔可夫链采样进行，但现有方法在采样效率和计算成本方面存在改进空间。

Method: 1. 高效实现添加、删除或反转单条边的基本移动操作；2. 设计预处理方法修剪可能的父集，以近似保持求和结果。

Result: 实证研究表明，这些技术相比先前方法能带来显著的效率提升。

Conclusion: 提出的两种技术能有效提高贝叶斯网络结构采样的计算效率，为贝叶斯推断提供更实用的工具。

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [87] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原理的IBNorm归一化方法，通过有界压缩操作在保持训练稳定性的同时增强表征的信息量，在语言和视觉模型中均优于传统归一化方法。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法如BatchNorm、LayerNorm等主要关注方差控制，但没有有效控制表征如何捕获任务相关信息。

Method: 基于信息瓶颈原理设计IBNorm，引入有界压缩操作，在保持预测信息的同时抑制无关变异。

Result: 在LLaMA、GPT-2等大语言模型和ResNet、ViT等视觉模型中，IBNorm均优于传统归一化方法，互信息分析证实了更好的信息瓶颈行为。

Conclusion: IBNorm通过信息瓶颈原理实现了更有效的表征学习，在理论和实验上都优于方差中心的归一化方法。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [88] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 该论文通过敏感性分析的统一视角解决深度学习模型的不稳定性和脆弱性问题，结合Lipschitz网络、随机平滑和曲率正则化来提升模型的泛化能力、对抗鲁棒性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对输入的小变化敏感，且优化过程受尖锐损失函数影响，导致不稳定性和脆弱性。论文旨在通过敏感性分析统一解决这些问题。

Method: 采用Lipschitz网络约束输入扰动敏感性，引入基于损失函数曲率的正则化技术，并探索随机平滑方法。开发了高效谱范数计算、新型Lipschitz约束层和改进的认证程序。

Result: 构建了一个统一框架，其中Lipschitz连续性、随机平滑和曲率正则化相互作用，有效解决了稳定性方面的基本挑战。

Conclusion: 通过结合理论分析和实用方法，论文为深度学习模型的稳定性问题提供了系统性的解决方案，贡献了理论分析和实用方法论。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [89] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现。该框架通过两级架构学习PDE的基本符号组件及其组合，能够嵌入已知物理定律保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 复杂时空动态建模是科学中的重大挑战，传统PDE难以从第一原理推导，现有数据驱动方法存在物理不一致性和数据需求大的问题，需要能够系统整合部分物理知识的新方法。

Method: 采用两级分层架构：第一级学习PDE的基本符号组件，第二级学习其组合。基于自适应傅里叶神经算子构建，能够捕捉非局部依赖和高阶算子。通过计算图直接嵌入已知物理定律。

Result: 该框架能够有效减少学习复杂度，保证物理一致性，提高数据效率，并支持通过符号回归可解释地发现潜在控制方程。

Conclusion: 分层物理嵌入学习框架为复杂时空系统的建模和物理定律发现提供了新的有效途径，能够系统整合先验知识并实现可解释的发现过程。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [90] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了一种多目标强化学习算法，在最大化期望回报的同时，使策略在目标状态上的边际状态分布更加分散，实现均匀访问多个目标状态。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法主要关注最大化期望回报，可能导致策略过度利用少数奖励源。在许多自然场景中，需要学习既能最大化回报又能均匀访问多个目标状态的策略，这一方面相对未被充分探索。

Method: 提出了一种新颖算法，通过优化自定义的强化学习奖励来学习高回报的策略混合，使边际状态分布在目标状态集上分散。算法基于当前策略混合为采样的轨迹计算奖励，并使用离线强化学习算法更新策略混合。

Result: 证明了算法的性能保证，显示了优化自然目标函数（捕获期望回报和边际状态分布在目标状态上的分散度）的有效收敛边界。在合成MDP和标准RL环境中进行了实验验证。

Conclusion: 该算法能够有效解决多目标强化学习问题，在保持高期望回报的同时实现目标状态的均匀访问，为需要分散访问多个奖励源的场景提供了实用解决方案。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [91] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出了一种基于循环矩阵和对角矩阵乘积的新型可逆线性层，显著降低了参数复杂度和计算复杂度，并在此基础上构建了CDFlow模型。


<details>
  <summary>Details</summary>
Motivation: 设计在保持高效雅可比行列式和逆矩阵计算的同时增强表达能力的线性层，是归一化流模型面临的关键挑战。

Method: 使用循环矩阵和对角矩阵的乘积分解来构建可逆线性层，利用快速傅里叶变换实现高效计算。

Result: CDFlow在自然图像数据集上实现了强大的密度估计，能有效建模具有周期性结构的数据，并显著加速了归一化流的关键操作。

Conclusion: 该方法为可扩展生成建模提供了实用优势，在保持表达能力的同时大幅降低了计算复杂度。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [92] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 该论文提出了解决信息级联流行度预测中三个关键问题的方法：时间泄漏、特征贫乏数据集和计算效率低。通过时间有序分割策略、Taoke电商数据集和CasTemp轻量级框架，在无泄漏评估下实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测存在三个关键局限：时间泄漏导致评估不真实、特征贫乏数据集限制实际应用、图方法计算效率低下。

Method: 提出时间有序分割策略防止未来信息泄漏；构建Taoke电商数据集包含丰富属性和购买转化信号；开发CasTemp框架使用时间游走、Jaccard邻居选择和GRU编码。

Result: CasTemp在四个数据集上实现最优性能，计算速度提升数个数量级，特别擅长预测第二阶段流行度转化。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个维度的问题，实现了无泄漏评估下的高效准确级联预测。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [93] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文分析了超图上的半监督学习，提出了高阶超图学习方法，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但超图半监督学习的理论基础有限，需要研究其渐近一致性和正则化方法。

Method: 提出了高阶超图学习方法，通过骨架图的拉普拉斯算子幂进行多尺度平滑正则化。

Result: 理论分析表明该方法收敛到高阶Sobolev半范数，在标准基准测试中表现优异。

Conclusion: 该研究为超图学习提供了理论保证，提出的方法在理论和实验上都取得了良好效果。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [94] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 该论文提出在知识图谱嵌入模型中使用模型融合方法（特别是加权平均）来替代传统的集成学习方法，以减少计算开销并提高链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算延迟和内存开销增加。模型融合方法提供了有前景的替代方案，无需训练多个模型。

Method: 提出两种加权平均方法：1）从训练周期开始维护模型参数的运行平均值；2）仅在验证集上泛化性能提升时选择性地更新集成模型参数的运行平均值。

Result: 在链接预测任务中，所提出的加权平均方法相比最先进的基准集成方法持续提升了性能，在字面增强KGE模型和多跳查询回答任务中也表现良好。

Conclusion: 加权平均方法能够在减少计算开销的同时，在各种评估设置中一致地提高知识图谱嵌入模型的性能。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [95] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一种基于损失函数从初始非凸性向最优解附近凸性转换假设的两阶段优化算法，通过检测转换点分别使用Adam和共轭梯度法，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习中损失函数经常存在非凸区域，导致普遍使用Adam等非凸优化方法。但局部最小值周围环境通常是凸的，可以利用二阶方法获得超线性收敛。假设实际任务中损失函数会从初始非凸性向最优解附近的凸性转换。

Method: 设计两阶段优化算法：1) 通过观察梯度范数与损失的关系检测凸性转换点；2) 在非凸区域使用Adam，在凸区域使用共轭梯度法。

Result: 计算实验证实这种简单凸性结构足够频繁出现，可以被实际利用来显著改善收敛性和准确性。

Conclusion: 提出的框架成功利用了损失函数从非凸到凸的转换特性，通过自适应切换优化算法实现了收敛速度和精度的显著提升。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [96] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 本文提出生物学信息机器学习(BIML)，作为物理信息机器学习(PIML)的扩展，专门针对生物学领域的独特挑战进行优化。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习在物理领域成功整合了机制理解，但生物学建模面临多面性先验知识、异质噪声数据、部分可观测性和复杂高维网络等独特挑战，需要专门的方法论。

Method: 提出BIML框架，通过四个基础支柱实现：不确定性量化、情境化、约束潜结构推断和可扩展性，利用基础模型和大型语言模型作为关键赋能工具。

Result: BIML将PIML的方法重新工具化，使其能够在更软、概率形式的先验知识下运行，更好地适应生物学的实际需求。

Conclusion: BIML为PIML启发的创新提供了具体路线图，将其引导到具有高度科学和社会相关性的挑战中，建议构建BIML生态系统。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [97] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 该论文研究了学习函数空间之间映射（算子）的问题，提出了多算子学习和单算子学习两种机制，并开发了新的网络架构MNO和MONet，建立了通用逼近理论并推导了缩放规律。


<details>
  <summary>Details</summary>
Motivation: 机器学习中大多数问题关注有限维空间之间的映射学习，但科学应用需要逼近函数空间之间的算子。本文旨在为学习算子集合提供理论和实践基础。

Method: 引入两种新架构MNO和MONet用于多算子学习；对于单算子学习，开发了平衡子网络架构复杂度的框架；建立了连续、可积和Lipschitz算子三种设置下的通用逼近结果。

Result: 理论分析表明新架构具有强表达能力，推导了网络规模与逼近精度的缩放规律；在参数化PDE基准测试中验证了所提架构的高效性。

Conclusion: 这项工作为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础，证明了所提架构在表达能力和计算效率方面的优势。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [98] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在大规模合成数据集上微调LLM，使其具备连续黑盒优化能力，无需参数调优即可超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法需要针对每个应用领域进行精细的参数调优，而现有的大语言模型在连续黑盒优化任务上能力有限。

Method: 通过从多样化的BO参数化生成的大规模合成数据集上微调大语言模型，利用LLM的预训练能力实现跨优化任务的泛化。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器，展示了LLM在高级数值推理方面的能力。

Conclusion: GPTOpt为无需参数调优的全局优化提供了一个灵活框架，证明了LLM在连续黑盒优化任务上的潜力。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [99] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了效用校准框架，通过特定效用函数评估多类分类器的校准误差，统一并重新解释现有校准指标，支持更鲁棒的校准评估。


<details>
  <summary>Details</summary>
Motivation: 现有多类校准评估方法要么关注特定预测方面（如top-class置信度、类级校准），要么使用计算复杂的变分公式，需要可扩展的校准评估方法。

Method: 提出效用校准框架，通过定义与最终用户目标相关的效用函数来测量校准误差，能够统一现有校准指标并支持更丰富的下游效用评估。

Result: 该框架能够统一和重新解释现有校准指标，特别是支持更鲁棒的top-class和类级校准指标，并能评估更丰富的下游效用类别。

Conclusion: 效用校准提供了一个通用框架，用于可扩展地评估多类校准，超越了二值化方法，支持更广泛的用户决策标准评估。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [100] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 提出了梯度权重对齐(GWA)指标，通过量化每个样本梯度与模型权重之间的一致性来跟踪泛化性能，无需验证集即可预测最优早停、比较模型和识别重要训练样本。


<details>
  <summary>Details</summary>
Motivation: 在深度学习领域，需要鲁棒的验证指标来检测过拟合、监控训练动态。研究训练数据与模型权重之间的交互是否能产生既能跟踪泛化又能归因性能的指标。

Method: 引入梯度权重对齐(GWA)，量化每个样本梯度与模型权重之间的一致性。有效学习对应一致的对齐，而错位表示泛化性能恶化。该指标可在训练期间高效计算。

Result: 大量实验表明，GWA能准确预测最优早停点，支持有原则的模型比较，识别有影响力的训练样本，提供直接从训练数据进行的模型分析方法。

Conclusion: GWA提供了一种无需验证集的模型分析方法，能够有效监控训练动态、跟踪泛化性能，并为训练样本贡献提供归因。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [101] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 该论文提出原型神经符号架构来解决神经符号AI中的推理捷径问题，通过在极低数据量下学习正确的基本概念而非利用伪相关来满足符号约束。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号AI模型容易学习推理捷径，即通过伪相关而非正确概念来满足符号约束，这影响了模型的可靠性和安全性。

Method: 引入原型神经符号架构，基于原型学习理论，在训练时考虑输入与少量标注数据点的相似性，同时满足背景知识约束。

Result: 在rsbench基准测试中，包括合成任务（MNIST-EvenOdd和Kand-Logic）和真实高风险任务（BDD-OIA）上，该方法在极低监督下显著提高了学习正确概念的能力。

Conclusion: 原型接地是一种有效且标注效率高的策略，为安全可靠的神经符号学习开辟了新途径。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [102] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性循环神经网络的单变量时间序列基础模型，仅使用合成数据预训练，在零样本时间序列预测中实现了高效的长时程预测和顶级性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本时间序列预测基础模型在高效长时程预测和可复现性方面面临挑战，现有仅使用合成数据的方法在挑战性基准测试中表现不佳。

Method: 使用GatedDeltaProduct架构和状态编织技术，基于线性RNN构建完全可并行化训练的模型，无需窗口化或摘要技术。通过统一随机微分方程、高斯过程和音频合成的综合合成数据流水线进行预训练。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN实现了顶级竞争性能，超越了所有现有的仅合成数据方法，并超过了大多数使用真实数据训练的模型，同时通过完全可并行化训练和推理实现了更高的效率。

Conclusion: TempoPFN为时间序列预测提供了一个高效、可复现的基础模型解决方案，开源了完整的数据生成流水线和训练代码，为未来研究奠定了基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [103] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法预测员工倦怠风险，评估了KNN、随机森林和SVM三种算法，其中SVM表现最佳(R²=0.84)，并开发了交互式界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠综合征严重影响个人福祉和组织绩效，需要早期检测和预防策略。

Method: 使用HackerEarth员工倦怠挑战数据集，评估KNN、随机森林和SVM三种监督学习算法，通过30折交叉验证和R²指标评估性能。

Result: SVM模型表现最佳(R²=0.84)，在统计上显著优于KNN和随机森林，并开发了基于Streamlit的交互界面。

Conclusion: 机器学习在组织环境中支持早期倦怠检测和基于数据的心理健康策略方面具有潜力。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [104] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出了一种具有模型内在机制概念解释的新模型，强调概念解释的忠实性，概念在类别间共享，并能从任何层忠实追踪其对logit的贡献和输入可视化。


<details>
  <summary>Details</summary>
Motivation: 现有后处理概念解释方法不够忠实于模型，且对模型学习的概念做出限制性假设（如类别特异性、小空间范围或与人类期望对齐）。

Method: 提出新模型，具有模型内在机制概念解释，概念跨类别共享，可从任何层忠实追踪其对logit的贡献和输入可视化，并利用基础模型提出新的概念一致性度量C²-Score。

Result: 相比先前工作，提出的概念在数量上更一致，用户认为更可解释，同时保持竞争力的ImageNet性能。

Conclusion: 该方法在保持模型性能的同时，提供了更忠实和可解释的概念解释，解决了现有概念解释方法的局限性。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [105] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种基于核引导互信息的新目标函数，结合多头注意力机制，能够从多父节点的有向无环图数据中可证明地恢复图结构。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力机制的图结构学习方法主要局限于树状图，难以处理更一般的多父节点有向无环图，因为难以设计训练目标让不同注意力头分别学习不同的父子依赖关系。

Method: 引入基于f-散度的核引导互信息度量，结合多头注意力框架，每个头关联不同的边际转移核来建模多样化的父子依赖关系。

Result: 理论证明在K-父节点DAG生成的序列上，通过梯度上升训练单层多头transformer能在多项式时间内收敛到全局最优，且收敛时的注意力分数模式能够准确反映真实邻接矩阵。

Conclusion: 该方法为从多父节点DAG数据中可证明地恢复图结构提供了理论保证，实验验证了理论发现。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [106] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络架构，其中整个循环核心由参数化量子电路实现，并由经典前馈网络控制。该模型在多个序列学习任务中表现出与强经典基线竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个物理一致的量子循环神经网络，将量子计算的指数级状态空间优势与经典非线性控制相结合，实现高容量记忆和输入条件参数化。

Method: 使用参数化量子电路作为循环核心，量子态作为隐藏状态，通过中电路测量获取部分观测，结合经典前馈网络提供非线性控制，形成混合量子-经典架构。

Result: 在情感分析、MNIST、置换MNIST、复制记忆和语言建模等任务中，使用最多14个量子比特进行模拟，模型表现与强经典基线竞争，并在机器翻译中展示了软注意力机制的有效性。

Conclusion: 这是第一个基于量子操作并在广泛序列学习任务中实现与经典基线竞争性能的模型，为量子机器学习提供了有前景的方向。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [107] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 本研究将大气预报基础模型Aurora迁移到海洋领域，通过微调预测加那利上升流系统的海表温度，展示了跨领域深度学习模型在海洋预报中的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统数值海洋预报模型存在计算成本高和可扩展性限制的问题，需要探索更高效的预测方法。

Method: 采用分阶段微调过程，结合纬度加权误差指标和超参数优化，使用高分辨率海洋再分析数据对Aurora模型进行适配。

Result: 模型实现了0.119K的低RMSE和高异常相关系数（ACC≈0.997），能够捕捉大尺度SST结构，但在海岸区域细节预测方面存在挑战。

Conclusion: 这项工作证明了将不同领域预训练的深度学习模型用于海洋应用的可行性，为数据驱动的海洋预报领域做出贡献。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [108] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出了一个从随机PAC-Bayesian保证中提取单个假设保证的统一框架，解决了传统PAC-Bayes只能提供随机采样假设期望风险保证的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PAC-Bayes框架只能提供随机采样假设的期望风险保证，需要在测试时进行随机预测，这在实际部署单个确定性假设时不可用。

Method: 提出了一个统一框架，包括一般oracle边界、数值边界和多数投票特化，用于从随机PAC-Bayesian保证中提取单个假设的保证。

Result: 经验表明，该方法在确定性分类器的泛化边界方面始终优于流行基线（最高可达2倍）。

Conclusion: 该框架成功地将PAC-Bayesian保证扩展到单个确定性假设，解决了实际部署中的关键限制。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [109] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Orlicz几何结构的广义Sobolev IPM方法，通过Musielak正则化将复杂计算简化为单变量优化问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Sobolev IPM方法受限于L^p几何结构，无法融入其他结构先验。为了突破这一限制，需要开发能够容纳多样化几何先验的广义框架。

Method: 通过Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并利用图结构将问题简化为单变量优化。

Result: 提出的GSI-M方法比流行的Orlicz-Wasserstein计算快几个数量级，在文档分类和拓扑数据分析任务中表现出实际优势。

Conclusion: 基于Orlicz几何结构的广义Sobolev IPM框架成功克服了传统L^p结构的限制，通过Musielak正则化实现了高效计算，为概率度量比较提供了更灵活的工具。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [110] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出了一种基于SVD分解的结构化局部学习框架，通过在低秩流形上训练神经网络，减少可训练参数数量，同时保持与反向传播相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播的内存计算开销大，以及直接反馈对齐在深层网络中扩展性差的问题，寻找一种既高效又准确的训练方法。

Method: 将权重矩阵进行SVD分解，在分解后的组件上进行训练，使用包含交叉熵、子空间对齐和正交正则化的复合损失函数，构建与SVD结构匹配的反馈矩阵。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上实现了与反向传播相当的准确率，同时减少了可训练参数数量。

Conclusion: 低秩流形上的局部学习是替代全秩梯度训练的有原则且可扩展的方法。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [111] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核评分规则的不确定性度量框架，统一现有方法并提供设计新度量的原则性方法，核选择决定度量特性如尾部敏感性、鲁棒性和分布外响应。


<details>
  <summary>Details</summary>
Motivation: 回归任务在安全关键领域需要适当的不确定性量化，但现有文献主要关注分类问题。

Method: 基于适当评分规则（特别是核评分）构建总不确定性、偶然不确定性和认知不确定性的度量家族。

Result: 实验证明这些度量在下游任务中有效，并揭示了不同实现之间的权衡，包括鲁棒性和分布外检测性能。

Conclusion: 该框架统一了多个知名度量，提供了设计任务特定度量的具体指导原则，核特性与下游行为有明确对应关系。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [112] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了浮点(FP)和整数(INT)量化格式在不同粒度下的性能，发现FP在粗粒度量化中表现优异，但在细粒度(块级)量化中，MXINT8在算法精度和硬件效率上优于FP格式。对于4位格式，FP通常具有精度优势，但应用异常值缓解技术后NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越多采用低精度浮点格式处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，包括不同粒度(粗粒度和细粒度)和位宽(8位和4位)的比较，并引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题。

Result: 发现关键性能交叉点：FP在粗粒度量化中表现优异，但在细粒度量化中MXINT8在算法精度和硬件效率上优于FP格式；对于4位格式，FP通常有精度优势，但应用Hadamard旋转等异常值缓解技术后NVINT4可以超越NVFP4。

Conclusion: 挑战当前硬件发展轨迹，证明一刀切的FP方法不是最优的，主张细粒度INT格式(特别是MXINT8)为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [113] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是基于WGAN框架的改进版本，通过引入贝叶斯最优学习阈值(BOLT)原理，使用Lipschitz连续判别器隐式最小化不同于Wasserstein距离的度量距离，显著提升了训练稳定性。


<details>
  <summary>Details</summary>
Motivation: WGAN虽然解决了传统GAN训练不稳定的问题，但其使用的Wasserstein距离可能不是最优选择。作者希望找到更好的度量距离来进一步提升GAN的训练效果和稳定性。

Method: 在WGAN框架基础上引入BOLT原理，使用Lipschitz连续判别器来隐式最小化一种新的度量距离，而不是传统的Wasserstein距离。

Result: 在四个标准图像生成基准测试(CIFAR-10、CelebA-64、LSUN Bedroom-64、LSUN Church-64)上，BOLT-GAN始终优于WGAN，FID分数降低了10-60%。

Conclusion: BOLT是一个广泛适用的原理，可以有效增强GAN的训练效果，BOLT-GAN在训练稳定性和生成质量方面都优于WGAN。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [114] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文系统研究了VLA模型在动作微调过程中的表征保留问题，发现简单的动作微调会导致视觉表征退化，并提出了一种简单有效的对齐方法来缓解这种退化。


<details>
  <summary>Details</summary>
Motivation: 研究VLA模型在动作微调过程中原始视觉语言表征和知识的保留程度，因为当前尚不清楚这些预训练的VLM表征在适应动作模态时能保留多少。

Method: 通过探测VLA的隐藏表征和分析注意力图来表征和测量这些效应；设计对比任务和方法来比较VLA模型与对应VLM；评估多种视觉表征对齐策略并引入简单有效的方法。

Result: 发现简单的动作微调会导致视觉表征退化；提出的对齐方法能够缓解退化并提高对分布外场景的泛化能力。

Conclusion: 阐明了动作微调与VL表征退化之间的权衡关系，并提出了恢复继承VL能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [115] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出FedLap框架，用于联邦学习中的图结构数据，通过拉普拉斯平滑在谱域捕获节点间依赖关系，确保隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中图结构数据的问题，特别是互联子图场景，现有方法存在隐私风险或计算复杂度高的问题。

Method: 使用拉普拉斯平滑在谱域利用全局结构信息，有效捕获节点间依赖关系，同时保证隐私和可扩展性。

Result: 在基准数据集上的广泛实验表明，FedLap相比现有技术实现了竞争性或更优的性能。

Conclusion: FedLap是首个具有强隐私保证的子图联邦学习方案，在保持隐私的同时实现了良好的性能。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [116] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 该研究展示了循环神经网络(RNNs)如何模拟隐马尔可夫模型(HMMs)的离散状态动态，揭示了RNNs通过噪声维持的轨道动态和"踢神经元"机制来实现概率计算。


<details>
  <summary>Details</summary>
Motivation: 过去RNN研究主要关注简单、确定性行为，但自然行为往往更丰富、自发且具有随机性。HMMs揭示了自然行为被分割为离散潜在状态，这种动态与RNNs的连续状态空间看似矛盾。

Method: 首先训练RNNs复制HMMs的发射统计特性，然后逆向工程分析训练好的网络机制。在无输入时，RNN活动收敛到单一固定点；在随机输入驱动下，轨迹呈现噪声维持的闭合轨道动态。

Result: 训练后的RNNs发展出高度结构化的连接性，少量"踢神经元"负责在慢动态区域间发起快速确定性转换。网络进入随机共振状态，能够执行概率计算。该机制在多种HMM架构中通用。

Conclusion: RNNs可以通过模块化重用相同的动态模式来模拟复杂的离散潜在动态，这表明RNNs具有实现复杂概率计算的组合性原理。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [117] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: GNSS是一个基于图神经网络的动态结构模拟器，通过局部坐标系、符号感知损失函数和波长感知连接半径等创新设计，在波主导的动态结构模拟中实现了高精度和高效推理。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在计算流体力学中已有应用，但在结构问题特别是动态案例中研究较少，需要填补这一空白。

Method: 采用编码-处理-解码范式，使用节点固定局部坐标系避免速度计算误差，采用符号感知回归损失减少相位误差，使用波长感知连接半径优化图构建。

Result: 在50kHz汉宁调制脉冲激励的梁案例中，GNSS能准确再现数百个时间步的物理过程，并能泛化到未见过的载荷条件，而现有GNN方法无法收敛或提供有意义的预测。

Conclusion: 具有物理一致性更新规则的局部保持图神经网络是动态波主导结构模拟的竞争性替代方案，相比显式有限元基准实现了显著的推理加速同时保持时空保真度。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [118] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了卷积脉冲GRU（CS-GRU）单元，通过卷积操作保留局部结构，结合脉冲神经元的时间精度和GRU的门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时会丢失局部细节，现有方法如SpikGRU无法捕捉事件驱动时空数据中的细粒度局部依赖关系。

Method: 设计卷积脉冲GRU单元，利用卷积操作保持局部结构，同时整合脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进的GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率提高69%。

Conclusion: CS-GRU是一个多功能架构，在保持局部依赖关系的同时，显著提升了时序和时空数据处理的性能和效率。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [119] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出LieSolver方法，利用李对称性精确满足PDE约束，通过对称变换学习初始和边界数据，比PINNs更快更准确


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在求解初边值问题时无法精确满足PDE约束的问题，提高计算效率和预测可靠性

Method: 利用李对称变换构建模型，通过对称性精确满足PDE约束，从初始和边界数据学习解

Result: 相比物理信息神经网络(PINNs)，LieSolver在求解线性齐次PDE时速度更快、精度更高

Conclusion: 该方法显著提高了PDE约束问题的计算效率和预测可靠性，支持严格的误差估计

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [120] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个基于SparkDataFrames的机器学习预处理和探索性数据分析工具，通过JSON配置文件描述数据处理步骤，包含69个处理阶段，支持多种数据格式的预处理和特征工程。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习需求的增长，现有工具在处理多格式数据时存在开销大、可扩展性差的问题，无法很好地集成到大型处理管道中，需要开发一个轻量级、可扩展的预处理工具。

Method: 使用SparkDataFrames存储数据以确保可扩展性，采用通用JSON输入文件格式描述对DataFrame的分步更改，实现了输入输出、过滤、基础统计、特征工程和探索性数据分析等处理阶段。

Result: 实现了69个处理阶段，在6个不同数据集上验证了关键阶段的功能，能够独立处理平面文件中的多个字段并重新组合，展示了聚类分析和图数据库数据准备能力。

Conclusion: MLPrE提供了一个通用且可扩展的预处理和早期数据分析工具，填补了机器学习应用中对这类工具的关键需求，能够加速和简化大型工作流中的早期开发阶段。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [121] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 该论文通过设计一个合成分类任务，证明了传统多示例学习方法在处理相邻实例特征时的局限性，并量化了这些方法与最优贝叶斯估计器的性能差距。


<details>
  <summary>Details</summary>
Motivation: 传统多示例学习方法在处理医学影像时，将实例分开处理，忽略了相邻切片或补丁之间的上下文关系，而这些关系在实际应用中至关重要。

Method: 设计了一个合成分类任务，其中准确预测必须考虑相邻实例特征。量化了现有多示例学习方法与最优贝叶斯估计器的性能比较。

Result: 实证研究表明，即使是最新的相关多示例学习方法，在从数万个实例从头开始训练时，仍然难以达到最优泛化性能。

Conclusion: 当前的多示例学习方法在处理相邻实例的上下文关系方面仍有改进空间，需要开发更好的方法来捕获这些关键特征。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [122] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了神经随机流（NSF）及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意时间点的一步采样，相比传统数值方法可获得100倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统SDE建模方法需要昂贵的数值求解器在任意时间点之间采样，计算成本高。

Method: 使用带有架构约束的条件归一化流来学习SDE转移规律，保持随机流的数学特性。

Result: 在大时间间隔下可获得两个数量级的加速，同时在合成SDE模拟和真实世界跟踪与视频数据上保持与数值方法相当的分布准确性。

Conclusion: NSF方法在保持准确性的同时显著降低了任意时间点采样的计算成本，为SDE建模提供了高效解决方案。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [123] [Spectral functions in Minkowski quantum electrodynamics from neural reconstruction: Benchmarking against dispersive Dyson--Schwinger integral equations](https://arxiv.org/abs/2510.24728)
*Rodrigo Carmo Terin*

Main category: hep-ph

TL;DR: 提出了一种Minkowski时空的物理信息神经网络方法(M-PINN)来直接求解量子电动力学的Dyson-Schwinger积分方程，结合了色散求解器和神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 直接在Minkowski时空中求解Dyson-Schwinger方程，避免从欧几里得空间解析延拓的复杂性，为更现实的顶点和未淬火效应扩展铺平道路。

Method: 结合色散求解器(基于Lehmann表示和减除色散关系)和M-PINN学习费米子质量函数B(p²)，在相同截断和重整化配置下，损失函数整合DSE残差与多尺度正则化、单调性/平滑性惩罚。

Result: 基准测试显示从红外到紫外尺度在壳上和动量减除方案中都达到定量一致，在受控设置中重现色散解，同时保持计算紧凑和可微分性。

Conclusion: M-PINN方法成功地在Minkowski时空中求解DSE，为扩展包含现实顶点、未淬火效应和不确定性感知变体提供了可行途径。

Abstract: A Minkowskian physics-informed neural network approach (M--PINN) is
formulated to solve the Dyson--Schwinger integral equations (DSE) of quantum
electrodynamics (QED) directly in Minkowski spacetime. Our novel strategy
merges two complementary approaches: (i) a dispersive solver based on Lehmann
representations and subtracted dispersion relations, and (ii) a M--PINN that
learns the fermion mass function $B(p^2)$, under the same truncation and
renormalization configuration (quenched, rainbow, Landau gauge) with the loss
integrating the DSE residual with multi--scale regularization, and
monotonicity/smoothing penalties in the spacelike branch in the same way as in
our previous work in Euclidean space. The benchmarks show quantitative
agreement from the infrared (IR) to the ultraviolet (UV) scales in both
on-shell and momentum-subtraction schemes. In this controlled setting, our
M--PINN reproduces the dispersive solution whilst remaining computationally
compact and differentiable, paving the way for extensions with realistic
vertices, unquenching effects, and uncertainty-aware variants.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [124] [DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes](https://arxiv.org/abs/2510.24734)
*Qirui Hou,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: DrivingScene是一个在线前馈框架，仅使用两个连续环视图像就能重建4D动态驾驶场景，通过轻量级残差流网络预测动态物体的非刚性运动，在nuScenes数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 实时高保真重建动态驾驶场景面临复杂动态和稀疏视图的挑战，现有方法难以平衡质量和效率。

Method: 提出轻量级残差流网络，在学习的静态场景先验基础上预测每个相机的动态物体非刚性运动，通过场景流显式建模动态，并采用粗到细的训练范式避免端到端方法的不稳定性。

Result: 在nuScenes数据集上，该仅使用图像的方法能在线同时生成高质量深度、场景流和3D高斯点云，在动态重建和新视角合成方面显著优于最先进方法。

Conclusion: DrivingScene框架成功解决了动态驾驶场景重建中的质量与效率平衡问题，为实时应用提供了有效解决方案。

Abstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is
challenged by complex dynamics and sparse views, with prior methods struggling
to balance quality and efficiency. We propose DrivingScene, an online,
feed-forward framework that reconstructs 4D dynamic scenes from only two
consecutive surround-view images. Our key innovation is a lightweight residual
flow network that predicts the non-rigid motion of dynamic objects per camera
on top of a learned static scene prior, explicitly modeling dynamics via scene
flow. We also introduce a coarse-to-fine training paradigm that circumvents the
instabilities common to end-to-end approaches. Experiments on nuScenes dataset
show our image-only method simultaneously generates high-quality depth, scene
flow, and 3D Gaussian point clouds online, significantly outperforming
state-of-the-art methods in both dynamic reconstruction and novel view
synthesis.

</details>


### [125] [Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds](https://arxiv.org/abs/2510.24773)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 提出基于机器学习的移动激光扫描点云不确定性评估框架，使用随机森林和XGBoost模型，通过局部几何特征预测点级误差，无需高精度参考数据。


<details>
  <summary>Details</summary>
Motivation: 传统后向不确定性建模依赖高精度参考数据，成本高昂且难以大规模获取，需要一种不依赖参考数据的点云不确定性评估方法。

Method: 使用随机森林和XGBoost两种集成学习模型，基于局部几何特征与点级误差的关系进行训练，采用空间分区数据集避免数据泄露。

Result: 两种模型都能有效捕捉几何特征与不确定性的非线性关系，平均ROC-AUC值超过0.87，高程变化、点密度和局部结构复杂度是最重要的预测特征。

Conclusion: 该框架提供了数据驱动的不确定性评估视角，为大规模点云质量控制和误差分析提供了可扩展和适应性强的基础。

Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point
clouds is essential for ensuring the accuracy and credibility of downstream
applications such as 3D mapping, modeling, and change analysis. Traditional
backward uncertainty modeling heavily rely on high-precision reference data,
which are often costly or infeasible to obtain at large scales. To address this
issue, this study proposes a machine learning-based framework for point-level
uncertainty evaluation that learns the relationship between local geometric
features and point-level errors. The framework is implemented using two
ensemble learning models, Random Forest (RF) and XGBoost, which are trained and
validated on a spatially partitioned real-world dataset to avoid data leakage.
Experimental results demonstrate that both models can effectively capture the
nonlinear relationships between geometric characteristics and uncertainty,
achieving mean ROC-AUC values above 0.87. The analysis further reveals that
geometric features describing elevation variation, point density, and local
structural complexity play a dominant role in predicting uncertainty. The
proposed framework offers a data-driven perspective of uncertainty evaluation,
providing a scalable and adaptable foundation for future quality control and
error analysis of large-scale point clouds.

</details>


### [126] [The Underappreciated Power of Vision Models for Graph Structural Understanding](https://arxiv.org/abs/2510.24788)
*Xinjian Zhao,Wei Pang,Zhongkai Xue,Xiangru Jian,Lei Zhang,Yaoyao Xu,Xiaozhuang Song,Shu Wu,Tianshu Yu*

Main category: cs.CV

TL;DR: 该论文发现视觉模型在图结构理解方面具有与GNNs相当的性能，但在全局拓扑感知和尺度不变推理方面表现更优，提出了GraphAbstract基准来评估模型的全局图属性感知能力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉模型在图理解中的潜力，因为GNNs采用自下而上的消息传递机制，与人类视觉感知全局结构的方式存在根本差异，现有基准测试混淆了领域特征与拓扑理解。

Method: 引入GraphAbstract基准，评估模型识别组织原型、检测对称性、感知连接强度和识别关键元素等全局图属性的能力，比较视觉模型与GNNs的表现。

Result: 视觉模型在需要整体结构理解的任务上显著优于GNNs，且在不同图尺度下保持泛化能力，而GNNs在全局模式抽象方面存在困难，并随着图规模增大性能下降。

Conclusion: 视觉模型在图结构理解方面具有显著但未被充分利用的能力，特别是在需要全局拓扑意识和尺度不变推理的问题上，这为开发更有效的图基础模型开辟了新途径。

Abstract: Graph Neural Networks operate through bottom-up message-passing,
fundamentally differing from human visual perception, which intuitively
captures global structures first. We investigate the underappreciated potential
of vision models for graph understanding, finding they achieve performance
comparable to GNNs on established benchmarks while exhibiting distinctly
different learning patterns. These divergent behaviors, combined with
limitations of existing benchmarks that conflate domain features with
topological understanding, motivate our introduction of GraphAbstract. This
benchmark evaluates models' ability to perceive global graph properties as
humans do: recognizing organizational archetypes, detecting symmetry, sensing
connectivity strength, and identifying critical elements. Our results reveal
that vision models significantly outperform GNNs on tasks requiring holistic
structural understanding and maintain generalizability across varying graph
scales, while GNNs struggle with global pattern abstraction and degrade with
increasing graph size. This work demonstrates that vision models possess
remarkable yet underutilized capabilities for graph structural understanding,
particularly for problems requiring global topological awareness and
scale-invariant reasoning. These findings open new avenues to leverage this
underappreciated potential for developing more effective graph foundation
models for tasks dominated by holistic pattern recognition.

</details>


### [127] [A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data](https://arxiv.org/abs/2510.24791)
*Jingjun Bi,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出RSGSLM方法，结合图卷积网络、伪标签和拓扑平衡校正，在多视图图像数据上实现高效的半监督学习


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理缺乏明确图结构的图像数据和多视图数据时效率有限，且多视图数据的图结构整合仍具挑战性

Method: 结合线性特征变换和多视图图融合的GCN框架，动态整合伪标签到损失函数，校正类边界附近标记样本的权重，引入无监督平滑损失

Result: 在多视图基准图像数据集上的实验表明，RSGSLM超越了现有的半监督学习方法

Conclusion: RSGSLM通过优化图结构和伪标签整合，在多视图半监督学习场景中实现了优越性能

Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have
gained attention due to their effectiveness in reducing the need for extensive
data annotations. Pseudo-labeling uses predictions from unlabeled data to
improve model training, while graph-based methods are characterized by
processing data represented as graphs. However, the lack of clear graph
structures in images combined with the complexity of multi-view data limits the
efficiency of traditional and existing techniques. Moreover, the integration of
graph structures in multi-view data is still a challenge. In this paper, we
propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view
Data (RSGSLM). Our method addresses these challenges by (i) combining linear
feature transformation and multi-view graph fusion within a Graph Convolutional
Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the
GCN loss function to improve classification in multi-view data, and (iii)
correcting topological imbalances by adjusting the weights of labeled samples
near class boundaries. Additionally, (iv) we introduce an unsupervised
smoothing loss applicable to all samples. This combination optimizes
performance while maintaining computational efficiency. Experimental results on
multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing
semi-supervised learning approaches in multi-view contexts.

</details>


### [128] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu,Bo Wang,Pengpeng Zeng,Haonan Zhang,Ji Zhang,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 该论文首次对高效视觉-语言-动作模型进行全面综述，提出了统一的分类法，将现有技术分为高效模型设计、高效训练和高效数据收集三大支柱，旨在解决VLAs模型部署中的计算和数据需求挑战。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在具身智能领域展现出强大能力，但其底层大规模基础模型带来的巨大计算和数据需求严重阻碍了实际部署。迫切需要解决这些挑战以推动VLAs的实际应用。

Method: 提出了统一的分类法，系统性地组织该领域的研究工作，将现有技术分为三个核心支柱：高效模型设计（关注高效架构和模型压缩）、高效训练（减少模型学习过程中的计算负担）和高效数据收集（解决机器人数据获取和使用的瓶颈）。

Result: 通过对该框架内最先进方法的批判性回顾，为研究社区建立了基础参考，总结了代表性应用，明确了关键挑战，并为未来研究绘制了路线图。

Conclusion: 该调查为高效视觉-语言-动作模型领域提供了首个全面综述，建立了系统化的分类框架，识别了当前挑战并指明了未来研究方向，有助于推动VLAs模型的实际部署和应用。

Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/

</details>


### [129] [The Generation Phases of Flow Matching: a Denoising Perspective](https://arxiv.org/abs/2510.24830)
*Anne Gagneux,Ségolène Martin,Rémi Gribonval,Mathurin Massias*

Main category: cs.CV

TL;DR: 本文从去噪角度分析流匹配生成过程，通过建立流匹配模型与去噪器的形式化联系，揭示了生成过程的不同动态阶段及其对生成质量的影响。


<details>
  <summary>Details</summary>
Motivation: 流匹配虽然取得了显著成功，但其生成过程质量的影响因素仍不明确。作者希望通过去噪视角来实证探究流匹配的生成机制。

Method: 建立流匹配模型与去噪器的形式化联系，设计原则性和受控的扰动（噪声和漂移）来影响样本生成，分析生成过程的不同动态阶段。

Result: 揭示了生成过程中不同的动态阶段，能够精确表征去噪器在生成过程的哪个阶段成功或失败，以及这为什么重要。

Conclusion: 通过去噪视角为流匹配生成过程提供了新的见解，能够精确理解生成过程不同阶段的关键特征及其对最终生成质量的影响。

Abstract: Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.

</details>


### [130] [Understanding Multi-View Transformers](https://arxiv.org/abs/2510.24907)
*Michal Stary,Julien Gaubil,Ayush Tewari,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 该论文提出了一种分析和可视化多视图变换器（如DUSt3R）内部机制的方法，通过探测残差连接来理解其3D表示的形成过程。


<details>
  <summary>Details</summary>
Motivation: 多视图变换器以黑盒方式解决3D任务，其内部工作机制不明确，这限制了模型改进和在安全关键应用中的使用。

Method: 通过探测多视图变换器各层残差连接来分析和可视化3D表示，研究DUSt3R模型的潜在状态发展和各层作用。

Result: 研究发现DUSt3R估计的对应关系会随着重建几何的细化而改进，揭示了其与具有更强显式全局姿态归纳偏置方法的差异。

Conclusion: 该方法为理解多视图变换器的内部工作机制提供了新视角，有助于模型改进和安全应用。

Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .

</details>


### [131] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh,Jie Ji,Xiaolong Ma,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出M-SAM框架解决多模态学习中主导模态压制其他模态的问题，通过识别主导模态、调制损失函数和更新权重来平衡多模态学习


<details>
  <summary>Details</summary>
Motivation: 多模态学习中主导模态往往会压制其他模态，限制了模型的泛化能力

Method: M-SAM框架包含三个步骤：1) 使用Shapley值识别主导模态；2) 调制损失函数以增强主导模态的鲁棒性；3) 通过反向传播更新权重

Result: 在四个不同数据集上的实验表明，M-SAM优于最新的优化和梯度操作方法，显著平衡和改进了多模态学习

Conclusion: M-SAM能够确保主导模态的鲁棒学习，同时增强其他模态的贡献，使模型能够探索和利用互补特征来提升整体性能

Abstract: In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.

</details>


### [132] [Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models](https://arxiv.org/abs/2510.25051)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 提出了一种结合2D乳腺X光片视觉特征与临床元数据文本描述的多模态框架，通过卷积神经网络与语言表示的战略集成，在癌症检测和钙化识别方面优于单模态基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助诊断系统在临床部署中存在局限性，特别是在处理多模态数据的细微解释和需要先验临床历史方面存在困难。

Method: 使用创新的标记化模块，将2D乳腺X光片的视觉特征与易于获取的临床元数据和合成放射学报告的结构化文本描述相结合，战略性地集成卷积神经网络与语言表示。

Result: 在多国队列筛查乳腺X光片评估中，该多模态方法在癌症检测和钙化识别方面表现出优于单模态基线的性能，特别是处理高分辨率图像方面。

Conclusion: 该方法为开发临床可行的基于视觉语言模型的计算机辅助诊断系统建立了新范式，通过有效的融合机制充分利用影像数据和上下文患者信息。

Abstract: Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.

</details>


### [133] [A Study on Inference Latency for Vision Transformers on Mobile Devices](https://arxiv.org/abs/2510.25166)
*Zhuojin Li,Marco Paolieri,Leana Golubchik*

Main category: cs.CV

TL;DR: 本研究定量分析了190个真实世界视觉变换器(ViT)在移动设备上的性能特征，并与102个卷积神经网络(CNN)进行比较，开发了包含1000个合成ViT延迟测量的数据集，证明可以准确预测新ViT的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习技术在移动设备上的显著进步，特别是在计算机视觉领域，需要定量研究ViT在移动设备上的性能特征，并与CNN进行比较以了解影响ViT延迟的因素。

Method: 比较190个真实世界ViT与102个CNN的性能特征，开发包含1000个合成ViT延迟测量的数据集，涵盖两个机器学习框架和六个移动平台的代表性构建块和最先进架构。

Result: 研究揭示了影响ViT架构在移动设备上延迟的因素，并证明使用开发的数据集可以准确预测新ViT的推理延迟，满足实际应用需求。

Conclusion: 通过系统性的性能分析和数据集开发，为移动设备上ViT架构的优化和部署提供了重要见解，能够有效预测新ViT模型的推理延迟性能。

Abstract: Given the significant advances in machine learning techniques on mobile
devices, particularly in the domain of computer vision, in this work we
quantitatively study the performance characteristics of 190 real-world vision
transformers (ViTs) on mobile devices. Through a comparison with 102 real-world
convolutional neural networks (CNNs), we provide insights into the factors that
influence the latency of ViT architectures on mobile devices. Based on these
insights, we develop a dataset including measured latencies of 1000 synthetic
ViTs with representative building blocks and state-of-the-art architectures
from two machine learning frameworks and six mobile platforms. Using this
dataset, we show that inference latency of new ViTs can be predicted with
sufficient accuracy for real-world applications.

</details>


### [134] [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)
*Runxi Huang,Mingxuan Yu,Mingyu Tsoi,Xiaomin Ouyang*

Main category: cs.CV

TL;DR: MMEdge是一个基于流水线感知和编码的端侧多模态推理框架，通过将推理过程分解为细粒度单元实现增量计算，并引入时间聚合模块和自适应优化机制来降低延迟同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了感知动态与模型执行之间的紧密耦合以及复杂的模态间依赖关系，而边缘设备上的实时多模态推理对于自动驾驶、人机交互等应用至关重要。

Method: 提出流水线感知编码设计，将推理分解为细粒度单元；引入时间聚合模块捕获时序动态；采用自适应多模态配置优化器和跨模态推测跳过机制。

Result: 在两个公共多模态数据集和真实无人机测试平台上评估，MMEdge显著降低了端到端延迟，同时在不同系统和数据动态下保持高任务准确性。

Conclusion: MMEdge通过流水线设计和自适应优化，有效解决了边缘设备上多模态推理的延迟和准确性平衡问题，为实时应用提供了可行解决方案。

Abstract: Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.

</details>


### [135] [3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework](https://arxiv.org/abs/2510.25347)
*Ayman Abaid,Gianpiero Guidone,Sara Alsubai,Foziyah Alquahtani,Talha Iqbal,Ruth Sharif,Hesham Elzomor,Emiliano Bianchini,Naeif Almagal,Michael G. Madden,Faisal Sharif,Ihsan Ullah*

Main category: cs.CV

TL;DR: 提出基于放射组学的冠状动脉钙化评分方法，使用伪标签解决标注数据不足问题，在非对比CCTA扫描中显著优于预训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 解决冠状动脉钙化评分中标注数据有限的问题，探索无需专家分割的自动化方法。

Method: 使用放射组学管道和伪标签生成训练标签，比较CT-FM和RadImageNet预训练模型特征与传统放射组学特征。

Result: 放射组学模型在182名患者数据集上达到84%准确率，显著优于CNN嵌入特征(p<0.05)。

Conclusion: 放射组学方法在无专家标注情况下仍能有效进行冠状动脉钙化评分，性能优于深度学习基础模型。

Abstract: Coronary artery calcium (CAC) scoring plays a crucial role in the early
detection and risk stratification of coronary artery disease (CAD). In this
study, we focus on non-contrast coronary computed tomography angiography (CCTA)
scans, which are commonly used for early calcification detection in clinical
settings. To address the challenge of limited annotated data, we propose a
radiomics-based pipeline that leverages pseudo-labeling to generate training
labels, thereby eliminating the need for expert-defined segmentations.
Additionally, we explore the use of pretrained foundation models, specifically
CT-FM and RadImageNet, to extract image features, which are then used with
traditional classifiers. We compare the performance of these deep learning
features with that of radiomics features. Evaluation is conducted on a clinical
CCTA dataset comprising 182 patients, where individuals are classified into two
groups: zero versus non-zero calcium scores. We further investigate the impact
of training on non-contrast datasets versus combined contrast and non-contrast
datasets, with testing performed only on non contrast scans. Results show that
radiomics-based models significantly outperform CNN-derived embeddings from
foundation models (achieving 84% accuracy and p<0.05), despite the
unavailability of expert annotations.

</details>


### [136] [Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers](https://arxiv.org/abs/2510.25372)
*M Yashwanth,Sharannya Ghosh,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 提出了PEP-FedPT框架，通过类别上下文混合提示(CCMP)实现联邦学习中的视觉Transformer提示调优，平衡泛化与个性化需求。


<details>
  <summary>Details</summary>
Motivation: 传统全局提示调优在异构客户端上泛化能力差，而个性化调优容易过拟合本地数据且缺乏泛化性，需要一种统一框架来解决这一矛盾。

Method: 使用类别上下文混合提示(CCMP)，基于类别特定提示和全局共享提示，通过全局类别原型和客户端类别先验自适应组合提示，实现样本级个性化而无需存储客户端相关参数。

Result: 在CIFAR-100、TinyImageNet、DomainNet和iNaturalist数据集上的综合评估表明，PEP-FedPT在不同数据异构场景下始终优于最先进的基线方法。

Conclusion: PEP-FedPT为视觉Transformer的高效和可泛化联邦提示调优建立了坚实基础，成功实现了泛化与个性化的统一。

Abstract: Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.

</details>


### [137] [Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation](https://arxiv.org/abs/2510.25739)
*Zhi-Kai Chen,Jun-Peng Jiang,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: Hawk是一种利用图像空间结构的推测解码方法，在保持图像质量和多样性的同时，将自回归图像生成模型的推理速度提升了1.71倍。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型虽然能产生高保真图像，但推理速度慢。推测解码在文本生成中已证明有效，但在图像生成中应用不足，主要挑战包括更大的采样空间和二维空间结构利用不充分。

Method: 提出Hawk方法，利用图像的空间结构来指导推测模型进行更准确和高效的预测，解决草稿模型与目标模型输出对齐困难的问题。

Result: 在多个文本到图像基准测试中，相比标准自回归模型实现了1.71倍的加速，同时保持了图像保真度和多样性。

Conclusion: Hawk成功地将推测解码应用于图像生成领域，通过利用图像的空间结构特性，在保持生成质量的同时显著提升了推理效率。

Abstract: Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [138] [AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers](https://arxiv.org/abs/2510.24724)
*Nazmun Nahar,Ritesh Harshad Ruparel,Shariar Kabir,Sumaiya Tasnia Khan,Shyamasree Saha,Mamunur Rashid*

Main category: cs.HC

TL;DR: AmarDoctor是一个多语言语音交互数字健康应用，专门为孟加拉语使用者提供患者分诊和AI驱动的临床决策支持，填补了该人群在数字医疗获取方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数字健康平台主要服务于欧洲人群和语言，孟加拉语使用者等群体在数字医疗获取方面存在严重不足。AmarDoctor旨在解决这一服务缺口，通过数据驱动方法加强初级医疗服务。

Method: 采用双界面系统（患者端和医疗提供者端），支持三种主要孟加拉语方言。患者模块使用自适应提问算法评估症状并引导至合适的专科医生，集成语音交互AI助手克服数字素养障碍。临床医生界面整合AI决策支持，生成结构化临时诊断和治疗建议。

Result: 在185个临床病例评估中，AmarDoctor的top-1诊断准确率达到81.08%（医生平均为50.27%），专科推荐准确率达到91.35%（医生平均为62.6%）。

Conclusion: AmarDoctor在诊断准确性和专科推荐方面显著优于人类医生，证明了其在为服务不足人群提供高质量数字医疗服务方面的有效性。

Abstract: This study presents AmarDoctor, a multilingual voice-interactive digital
health app designed to provide comprehensive patient triage and AI-driven
clinical decision support for Bengali speakers, a population largely
underserved in access to digital healthcare. AmarDoctor adopts a data-driven
approach to strengthen primary care delivery and enable personalized health
management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health
have become popular in recent years, they mainly serve European demographics
and languages. AmarDoctor addresses this gap with a dual-interface system for
both patients and healthcare providers, supporting three major Bengali
dialects. At its core, the patient module uses an adaptive questioning
algorithm to assess symptoms and guide users toward the appropriate specialist.
To overcome digital literacy barriers, it integrates a voice-interactive AI
assistant that navigates users through the app services. Complementing this,
the clinician-facing interface incorporates AI-powered decision support that
enhances workflow efficiency by generating structured provisional diagnoses and
treatment recommendations. These outputs inform key services such as
e-prescriptions, video consultations, and medical record management. To
validate clinical accuracy, the system was evaluated against a gold-standard
set of 185 clinical vignettes developed by experienced physicians.
Effectiveness was further assessed by comparing AmarDoctor performance with
five independent physicians using the same vignette set. Results showed
AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus
physicians average of 50.27 percent) and a top specialty recommendation
precision of 91.35 percent (versus physicians average of 62.6 percent).

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [139] [The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks](https://arxiv.org/abs/2510.25112)
*Di Zhang*

Main category: cs.PL

TL;DR: 提出并发程序分析验证的新范式——奇点理论，将程序执行空间建模为分支拓扑空间，使用代数拓扑工具检测死锁和活锁。


<details>
  <summary>Details</summary>
Motivation: 为并发程序验证建立几何和拓扑基础，超越传统模型检测的局限性。

Method: 将并发程序执行空间建模为分支拓扑空间，程序状态为点，状态转移为路径；使用同伦和同调群等代数拓扑工具定义并发拓扑不变量。

Result: 能够系统性地检测和分类并发"奇点"（死锁作为吸引子，活锁作为不可收缩环），无需穷举遍历所有状态。

Conclusion: 奇点理论为并发程序验证提供了新的几何拓扑框架，有望克服传统方法的限制。

Abstract: This paper introduces a novel paradigm for the analysis and verification of
concurrent programs -- the Singularity Theory. We model the execution space of
a concurrent program as a branched topological space, where program states are
points and state transitions are paths. Within this framework, we characterize
deadlocks as attractors and livelocks as non-contractible loops in the
execution space. By employing tools from algebraic topology, particularly
homotopy and homology groups, we define a series of concurrent topological
invariants to systematically detect and classify these concurrent
"singularities" without exhaustively traversing all states. This work aims to
establish a geometric and topological foundation for concurrent program
verification, transcending the limitations of traditional model checking.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [140] [CFL-SparseMed: Communication-Efficient Federated Learning for Medical Imaging with Top-k Sparse Updates](https://arxiv.org/abs/2510.24776)
*Gousia Habib,Aniket Bhardwaj,Ritvik Sharma,Shoeib Amin Banday,Ishfaq Ahmad Malik*

Main category: eess.IV

TL;DR: 提出CFL-SparseMed联邦学习方法，使用Top-k稀疏化技术减少通信开销，解决医疗图像分类中的数据异构性和隐私问题


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类需要安全可靠的方法，但集中式模型面临数据和隐私问题。联邦学习虽然能保护隐私，但在异构非IID数据和大规模网络中面临通信成本高的问题

Method: 使用Top-k稀疏化技术，只传输前k个梯度值来减少通信开销，同时解决数据异构性问题

Result: 该方法在保持模型精度的同时有效降低了通信开销，提高了联邦学习效率，保护了隐私，并改善了非IID医疗图像环境下的诊断准确性

Conclusion: CFL-SparseMed为医疗图像分类提供了一种统一的解决方案，有效平衡了通信效率、数据异构性和模型准确性

Abstract: Secure and reliable medical image classification is crucial for effective
patient treatment, but centralized models face challenges due to data and
privacy concerns. Federated Learning (FL) enables privacy-preserving
collaborations but struggles with heterogeneous, non-IID data and high
communication costs, especially in large networks. We propose
\textbf{CFL-SparseMed}, an FL approach that uses Top-k Sparsification to reduce
communication overhead by transmitting only the top k gradients. This unified
solution effectively addresses data heterogeneity while maintaining model
accuracy. It enhances FL efficiency, preserves privacy, and improves diagnostic
accuracy and patient care in non-IID medical imaging settings. The
reproducibility source code is available on
\href{https://github.com/Aniket2241/APK_contruct}{Github}.

</details>


### [141] [Physics-Guided Conditional Diffusion Networks for Microwave Image Reconstruction](https://arxiv.org/abs/2510.25729)
*Shirin Chehelgami,Joe LoVetri,Vahab Khoshdel*

Main category: eess.IV

TL;DR: 提出基于条件潜在扩散模型的电磁逆散射问题求解框架，能够生成多个可能的介电常数分布，处理逆问题的不唯一性。


<details>
  <summary>Details</summary>
Motivation: 现有确定性机器学习方法只能产生单一重建结果，无法反映逆问题固有的非唯一性特征。

Method: 使用条件潜在扩散模型生成多个合理的介电常数分布，集成前向电磁求解器进行物理评估，选择散射场数据差异最小的解作为最终结果。

Result: 使用创新的合成数据集训练模型，获得高质量的介电常数重建结果，在形状识别方面具有优异保真度。

Conclusion: 混合生成物理框架为稳健的数据驱动微波成像提供了有前景的方向。

Abstract: A conditional latent-diffusion based framework for solving the
electromagnetic inverse scattering problem associated with microwave imaging is
introduced. This generative machine-learning model explicitly mirrors the
non-uniqueness of the ill-posed inverse problem. Unlike existing inverse
solvers utilizing deterministic machine learning techniques that produce a
single reconstruction, the proposed latent-diffusion model generates multiple
plausible permittivity maps conditioned on measured scattered-field data,
thereby generating several potential instances in the range-space of the
non-unique inverse mapping. A forward electromagnetic solver is integrated into
the reconstruction pipeline as a physics-based evaluation mechanism. The space
of candidate reconstructions form a distribution of possibilities consistent
with the conditioning data and the member of this space yielding the lowest
scattered-field data discrepancy between the predicted and measured scattered
fields is reported as the final solution. Synthetic and experimental labeled
datasets are used for training and evaluation of the model. An innovative
labeled synthetic dataset is created that exemplifies a varied set of
scattering features. Training of the model using this new dataset produces high
quality permittivity reconstructions achieving improved generalization with
excellent fidelity to shape recognition. The results highlight the potential of
hybrid generative physics frameworks as a promising direction for robust,
data-driven microwave imaging.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [142] [Sub-microsecond Transformers for Jet Tagging on FPGAs](https://arxiv.org/abs/2510.24784)
*Lauri Laatu,Chang Sun,Arianna Cox,Abhijith Gandrakota,Benedikt Maier,Jennifer Ngadiuba,Zhiqiang Que,Wayne Luk,Maria Spiropulu,Alexander Tapper*

Main category: physics.ins-det

TL;DR: 首个在FPGA上实现亚微秒级延迟的Transformer模型，用于高能物理中的喷注重建任务，在CERN大型强子对撞机中实现实时应用。


<details>
  <summary>Details</summary>
Motivation: Transformer在机器学习中表现出色，但其计算复杂度限制了在实时应用中的使用，特别是在对撞机实验的硬件触发系统中。

Method: 采用高粒度量化和分布式算术优化技术，将整个Transformer模型部署在单个FPGA上，并扩展hls4ml工具支持多头注意力和线性注意力。

Result: 实现了约100纳秒的延迟，性能优于其他基线模型，满足高吞吐量和低延迟要求。

Conclusion: 这项工作推动了高亮度大型强子对撞机下一代触发系统的发展，使Transformer能够用于高能物理及其他领域的实时应用。

Abstract: We present the first sub-microsecond transformer implementation on an FPGA
achieving competitive performance for state-of-the-art high-energy physics
benchmarks. Transformers have shown exceptional performance on multiple tasks
in modern machine learning applications, including jet tagging at the CERN
Large Hadron Collider (LHC). However, their computational complexity prohibits
use in real-time applications, such as the hardware trigger system of the
collider experiments up until now. In this work, we demonstrate the first
application of transformers for jet tagging on FPGAs, achieving
$\mathcal{O}(100)$ nanosecond latency with superior performance compared to
alternative baseline models. We leverage high-granularity quantization and
distributed arithmetic optimization to fit the entire transformer model on a
single FPGA, achieving the required throughput and latency. Furthermore, we add
multi-head attention and linear attention support to hls4ml, making our work
accessible to the broader fast machine learning community. This work advances
the next-generation trigger systems for the High Luminosity LHC, enabling the
use of transformers for real-time applications in high-energy physics and
beyond.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [143] [GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction](https://arxiv.org/abs/2510.25220)
*Zhijie Lin,Zhuofeng Li,Chenglei Dai,Wentian Bao,Shuai Lin,Enyun Yu,Haoxiang Zhang,Liang Zhao*

Main category: cs.IR

TL;DR: 提出GReF统一生成式高效重排序框架，解决传统两阶段重排序方法中生成器与评估器分离、自回归生成器推理效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 多阶段推荐系统中重排序对建模项目间相关性至关重要，但现有两阶段方法存在生成器与评估器分离阻碍端到端训练、自回归生成器推理效率低两大挑战。

Method: 提出Gen-Reranker自回归生成器，包含双向编码器和动态自回归解码器；通过项目曝光顺序预训练；使用Rerank-DPO进行后训练实现端到端优化；引入有序多令牌预测(OMTP)提高推理效率。

Result: 离线实验显示GReF优于最先进的重排序方法，延迟接近非自回归模型；已在快手3亿日活用户的视频应用中部署，显著提升在线推荐质量。

Conclusion: GReF框架成功解决了重排序中的端到端训练和推理效率问题，在实际大规模推荐系统中验证了有效性。

Abstract: In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.

</details>


### [144] [TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation](https://arxiv.org/abs/2510.25259)
*Yehjin Shin,Jeongwhan Choi,Seojin Kim,Noseong Park*

Main category: cs.IR

TL;DR: 提出了TV-Rec模型，使用时间变异的卷积滤波器替代固定核和自注意力机制，在序列推荐中更好地捕捉用户行为的时间变化模式，实现更高的表达能力和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的序列推荐模型通常将卷积滤波器与自注意力机制结合使用，因为固定的卷积滤波器难以捕捉全局交互。需要一种能够独立捕捉位置依赖时间变化的方法。

Method: 受图信号处理启发，使用时间变异的图滤波器来捕捉用户序列中位置依赖的时间变化模式，替代传统的固定核和自注意力机制。

Result: 在六个公共基准测试中，TV-Rec平均优于最先进基线方法7.49%，同时减少了计算量并加速了推理过程。

Conclusion: 时间变异卷积滤波器能够有效捕捉序列推荐中的复杂交互模式，无需自注意力机制即可实现优越性能，为序列推荐提供了新的有效方法。

Abstract: Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [145] [Timing Games in Responsive Consensus Protocols](https://arxiv.org/abs/2510.25144)
*Kaya Alpturer,Kushal Babel,Aditya Saraf*

Main category: cs.GT

TL;DR: 该论文通过引入动态区块奖励和投票机制来解决区块链共识协议中的时间博弈问题，激励验证者及时提出区块，实现响应式共识。


<details>
  <summary>Details</summary>
Motivation: 区块链应用中验证者通过策略性延迟提案来增加奖励，这破坏了共识协议的响应性。研究旨在解决这种时间博弈问题，使响应性在区块链协议中成为可能。

Method: 建立响应式共识协议的时间博弈模型，引入随轮次时间递减的动态区块奖励，并通过投票机制让其他验证者对当前领导者的轮次时间进行投票测量延迟。

Result: 通过精心设置协议参数，投票机制使验证者能够协调并达到合作均衡，通过更高的奖励率使所有参与者受益。

Conclusion: 响应性本身可以促进更快的区块提案，而不是因为时间博弈而无法实现。从静态到动态区块奖励的转变虽然增加了验证者效用对延迟的敏感性，但分析表明这种影响在理论和实际网络模拟中都是轻微的。

Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate
at the speed of the network -- is widely used in consensus protocol design to
optimize latency and throughput. However, blockchain applications incentivize
validators to play timing games by strategically delaying their proposals,
since increased block time correlates with greater rewards. Consequently, it
may appear that responsiveness (even under optimistic conditions) is impossible
in blockchain protocols. In this work, we develop a model of timing games in
responsive consensus protocols and find a prisoner's dilemma structure, where
cooperation (proposing promptly) is in the validators' best interest, but
individual incentives encourage validators to delay proposals selfishly. To
attain desirable equilibria, we introduce dynamic block rewards that decrease
with round time to explicitly incentivize faster proposals. Delays are measured
through a voting mechanism, where other validators vote on the current leader's
round time. By carefully setting the protocol parameters, the voting mechanism
allows validators to coordinate and reach the cooperative equilibrium,
benefiting all through a higher rate-of-reward. Thus, instead of responsiveness
being an unattainable property due to timing games, we show that responsiveness
itself can promote faster block proposals. One consequence of moving from a
static to dynamic block reward is that validator utilities become more
sensitive to latency, worsening the gap between the best- and worst-connected
validators. Our analysis shows, however, that this effect is minor in both
theoretical latency models and simulations based on real-world networks.

</details>


### [146] [Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games](https://arxiv.org/abs/2510.25080)
*Will Wolf*

Main category: cs.GT

TL;DR: 本文介绍了有界单向响应游戏(BORGs)这一新的游戏结构类别，并以修改版Monopoly Deal作为基准环境，展示了CFR算法在该领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索一种较少被研究但策略丰富的游戏结构——有界单向响应，这种动态发生在玩家行动短暂转移控制权给对手时，对手必须通过一个或多个连续移动满足固定条件才能结束回合。

Method: 引入修改版Monopoly Deal作为基准环境来隔离BORG动态，使用反事实遗憾最小化(CFR)算法，并开发了一个轻量级全栈研究平台，集成了环境、并行化CFR运行时和可玩网页界面。

Result: CFR算法成功收敛到该领域的有效策略，无需新的算法扩展。开发的研究平台可在单台工作站上运行，为探索有界单向响应设置中的状态表示和策略学习提供了实用基础。

Conclusion: BORGs代表了一类策略丰富的游戏结构，CFR算法能够有效处理这种动态，开发的平台为相关研究提供了可复现的实验基础。

Abstract: Card games are widely used to study sequential decision-making under
uncertainty, with real-world analogues in negotiation, finance, and
cybersecurity. Typically, these games fall into three categories based on the
flow of control: strictly-sequential (where players alternate single actions),
deterministic-response (where some actions trigger a fixed outcome), and
unbounded reciprocal-response (where alternating counterplays are permitted). A
less-explored but strategically rich structure exists: the bounded one-sided
response. This dynamic occurs when a player's action briefly transfers control
to the opponent, who must satisfy a fixed condition through one or more
sequential moves before the turn resolves. We term games featuring this
mechanism Bounded One-Sided Response Games (BORGs).
  We introduce a modified version of Monopoly Deal as a benchmark environment
that specifically isolates the BORG dynamic, where a Rent action forces the
opponent to sequentially choose payment assets. We demonstrate that the
gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully
converges on effective strategies for this domain without requiring novel
algorithmic extensions. To support efficient, reproducible experimentation, we
present a lightweight, full-stack research platform that unifies the
environment, a parallelized CFR runtime, and a human-playable web interface,
all runnable on a single workstation. This system provides a practical
foundation for exploring state representation and policy learning in bounded
one-sided response settings.
  The trained CFR agent and source code are available at
https://monopolydeal.ai.

</details>


### [147] [Learning-Augmented Online Bidding in Stochastic Settings](https://arxiv.org/abs/2510.25582)
*Spyros Angelopoulos,Bertrand Simon*

Main category: cs.GT

TL;DR: 本文研究了学习增强设置下的在线竞价问题，探讨了包含随机性的预测预言机或算法本身。


<details>
  <summary>Details</summary>
Motivation: 在线竞价是经典优化问题，在在线决策、可中断系统设计和近似算法分析中有广泛应用。现有研究主要关注不利用预测质量随机信息的预言机和确定性算法。

Method: 第一部分研究基于分布预测的竞价，寻找在算法一致性和鲁棒性之间提供最佳权衡的帕累托最优算法；第二部分研究随机竞价算法的能力和局限性，通过呈现一致性/鲁棒性权衡的上下界。

Result: 提出了在分布预测下提供最佳一致性-鲁棒性权衡的帕累托最优算法，并给出了随机竞价算法在一致性/鲁棒性权衡方面的上下界。

Conclusion: 该工作扩展了学习增强在线竞价的研究范围，首次系统性地考虑了预测质量中的随机性因素，为随机算法在在线竞价中的应用提供了理论分析框架。

Abstract: Online bidding is a classic optimization problem, with several applications
in online decision-making, the design of interruptible systems, and the
analysis of approximation algorithms. In this work, we study online bidding
under learning-augmented settings that incorporate stochasticity, in either the
prediction oracle or the algorithm itself. In the first part, we study bidding
under distributional predictions, and find Pareto-optimal algorithms that offer
the best-possible tradeoff between the consistency and the robustness of the
algorithm. In the second part, we study the power and limitations of randomized
bidding algorithms, by presenting upper and lower bounds on the
consistency/robustness tradeoffs. Previous works focused predominantly on
oracles that do not leverage stochastic information on the quality of the
prediction, and deterministic algorithms.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [148] [Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis](https://arxiv.org/abs/2510.25468)
*Emanuele De Angelis,Florian Frohn*

Main category: cs.LO

TL;DR: 这是HCVS 2025工作坊的后论文集，该工作坊于2025年7月22日在克罗地亚萨格勒布举行，作为CAV 2025的附属活动。


<details>
  <summary>Details</summary>
Motivation: 收集和整理HCVS 2025工作坊的研究成果，促进霍恩子句在验证和合成领域的学术交流。

Method: 通过工作坊形式汇集相关研究，并出版后论文集。

Result: 成功举办了第12届HCVS工作坊，并出版了包含最新研究成果的论文集。

Conclusion: 该论文集记录了HCVS 2025工作坊的学术成果，为霍恩子句在验证和合成领域的研究提供了重要参考。

Abstract: This volume contains the post-proceedings of the 12th Workshop on Horn
Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb,
Croatia, on July 22, 2025, as affiliated workshop of the 37th International
Conference on Computer Aided Verification (CAV 2025).

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [149] [Scaling flow-based approaches for topology sampling in $\mathrm{SU}(3)$ gauge theory](https://arxiv.org/abs/2510.25704)
*Claudio Bonanno,Andrea Bulgarelli,Elia Cellini,Alessandro Nada,Dario Panfalone,Davide Vadacchino,Lorenzo Verzichelli*

Main category: hep-lat

TL;DR: 提出了一种基于非平衡模拟的方法来缓解格点规范理论中接近连续极限时的拓扑冻结问题，通过开放边界条件和逐步引入周期性边界条件的非平衡蒙特卡洛方法，有效降低拓扑电荷的自相关性。


<details>
  <summary>Details</summary>
Motivation: 在格点规范理论中，当接近连续极限时会出现拓扑冻结现象，导致拓扑电荷的自相关性急剧增加，严重影响模拟效率。需要开发有效方法来缓解这一问题。

Method: 使用开放边界条件降低拓扑电荷的自相关性，然后通过非平衡蒙特卡洛方法逐步引入周期性边界条件来精确消除开放边界条件的非物理效应。还设计了定制化的随机归一化流来优化边界条件的演化过程。

Result: 在四维SU(3)杨-米尔斯理论中实现了对计算成本缩放的完全控制，在0.045 fm的格点间距下有效采样拓扑结构，定制化随机归一化流相比纯随机非平衡方法表现出更优越的性能。

Conclusion: 该方法为在连续极限下高效采样拓扑结构提供了清晰的策略，并为未来基于流的解决方案开辟了更高效的途径。

Abstract: We develop a methodology based on out-of-equilibrium simulations to mitigate
topological freezing when approaching the continuum limit of lattice gauge
theories. We reduce the autocorrelation of the topological charge employing
open boundary conditions, while removing exactly their unphysical effects using
a non-equilibrium Monte Carlo approach in which periodic boundary conditions
are gradually switched on. We perform a detailed analysis of the computational
costs of this strategy in the case of the four-dimensional $\mathrm{SU}(3)$
Yang-Mills theory. After achieving full control of the scaling, we outline a
clear strategy to sample topology efficiently in the continuum limit, which we
check at lattice spacings as small as $0.045$ fm. We also generalize this
approach by designing a customized Stochastic Normalizing Flow for evolutions
in the boundary conditions, obtaining superior performances with respect to the
purely stochastic non-equilibrium approach, and paving the way for more
efficient future flow-based solutions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [150] [Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation](https://arxiv.org/abs/2510.01225)
*Andrei Lazarev,Dmitrii Sedov*

Main category: cs.CE

TL;DR: 本文提出了一种基于大型语言模型（特别是Google Gemini Pro）的自动化金融摘要生成框架，通过数据提取、提示工程和LLM分析，能够高效处理海量非结构化数据并生成可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 信息爆炸式增长给研究人员和专业人士带来了巨大挑战，传统分析方法难以处理海量非结构化数据，需要一种高效的方法来帮助研究人员节省时间并了解当前趋势。

Method: 结合OpenAlex数据提取、策略性提示工程和LLM驱动分析，采用从数据获取、JSON构建到与Gemini交互的完整流程，最终自动生成PDF报告。

Result: 成功开发出能够自动生成全面金融摘要的系统，能够概括关键发现、识别新兴趋势，并将项目开源在GitHub上供进一步开发。

Conclusion: 该框架有效解决了传统分析方法的局限性，证明了LLMs在高效处理大数据和提供可操作见解方面的强大能力，为研究人员提供了实用的工具。

Abstract: The exponential growth of information presents a significant challenge for
researchers and professionals seeking to remain at the forefront of their
fields and this paper introduces an innovative framework for automatically
generating insightful financial digests using the power of Large Language
Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of
data extraction from OpenAlex, strategic prompt engineering, and LLM-driven
analysis, we demonstrate the automated example of creating a comprehensive
digests that generalize key findings, identify emerging trends. This approach
addresses the limitations of traditional analysis methods, enabling the
efficient processing of vast amounts of unstructured data and the delivery of
actionable insights in an easily digestible format. This paper describes how
LLMs work in simple words and how we can use their power to help researchers
and scholars save their time and stay informed about current trends. Our study
includes step-by-step process, from data acquisition and JSON construction to
interaction with Gemini and the automated generation of PDF reports, including
a link to the project's GitHub repository for broader accessibility and further
development.

</details>


### [151] [Re-evaluating sample efficiency in de novo molecule generation](https://arxiv.org/abs/2212.01385)
*Morgan Thomas,Noel M. O'Boyle,Andreas Bender,Chris De Graaf*

Main category: cs.CE

TL;DR: 本文改进了分子生成中的样本效率基准测试，考虑分子量和LogP等化学性质，重新评估生成模型，发现Augmented Hill-Climb方法在样本效率和生成分子化学质量方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决从头分子生成中的数据效率问题，特别是在结合深度生成模型与计算昂贵的分子评分函数时，需要提高样本效率并考虑生成分子的化学质量。

Method: 改进现有的样本效率基准测试方法，纳入分子量和LogP等化学性质考量，重新评估所有基准生成模型，并测试Augmented Hill-Climb方法。

Result: 考虑分子化学性质后，生成模型的排名重新排序；Augmented Hill-Climb方法在样本效率和生成分子化学质量方面表现最优。

Conclusion: 样本效率和化学质量的持续改进使得计算昂贵的评分函数能够在更现实的时间尺度上常规集成到分子设计中。

Abstract: De novo molecule generation can suffer from data inefficiency; requiring
large amounts of training data or many sampled data points to conduct objective
optimization. The latter is a particular disadvantage when combining deep
generative models with computationally expensive molecule scoring functions
(a.k.a. oracles) commonly used in computer-aided drug design. Recent works have
therefore focused on methods to improve sample efficiency in the context of de
novo molecule drug design, or to benchmark it. In this work, we discuss and
adapt a recent sample efficiency benchmark to better reflect realistic goals
also with respect to the quality of chemistry generated, which must always be
considered in the context of small-molecule drug design; we then re-evaluate
all benchmarked generative models. We find that accounting for molecular weight
and LogP with respect to the training data, and the diversity of chemistry
proposed, re-orders the ranking of generative models. In addition, we benchmark
a recently proposed method to improve sample efficiency (Augmented Hill-Climb)
and found it ranked top when considering both the sample efficiency and
chemistry of molecules generated. Continual improvements in sample efficiency
and chemical desirability enable more routine integration of computationally
expensive scoring functions on a more realistic timescale.

</details>


### [152] [Stiff Circuit System Modeling via Transformer](https://arxiv.org/abs/2510.24727)
*Weiman Yan,Yi-Chia Chang,Wanyu Zhao*

Main category: cs.CE

TL;DR: 提出了一种结合Crossformer和KANs的新方法，用于精确高效地建模刚性电路的瞬态行为，在ADC电路上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 刚性电路在传统建模框架下具有挑战性，需要更精确高效的建模方法。

Method: 使用Crossformer（时间序列预测的先进Transformer模型）结合Kolmogorov-Arnold Networks (KANs)，利用Crossformer的时间表示能力和KANs的增强特征提取。

Result: 在SPICE模拟生成的ADC电路数据集上验证，显著减少了训练时间和错误率。

Conclusion: 该方法在预测电路对各种输入条件的响应方面实现了更高的保真度，为刚性电路建模提供了有效解决方案。

Abstract: Accurate and efficient circuit behavior modeling is a cornerstone of modern
electronic design automation. Among different types of circuits, stiff circuits
are challenging to model using previous frameworks. In this work, we propose a
new approach using Crossformer, which is a current state-of-the-art Transformer
model for time-series prediction tasks, combined with Kolmogorov-Arnold
Networks (KANs), to model stiff circuit transient behavior. By leveraging the
Crossformer's temporal representation capabilities and the enhanced feature
extraction of KANs, our method achieves improved fidelity in predicting circuit
responses to a wide range of input conditions. Experimental evaluations on
datasets generated through SPICE simulations of analog-to-digital converter
(ADC) circuits demonstrate the effectiveness of our approach, with significant
reductions in training time and error rates.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [153] [PitchFlower: A flow-based neural audio codec with pitch controllability](https://arxiv.org/abs/2510.25566)
*Diego Torres,Axel Roebel,Nicolas Obin*

Main category: eess.AS

TL;DR: PitchFlower是一种基于流的神经音频编解码器，具有显式的音高可控性，通过扰动训练实现音高解耦，在保持高质量音频的同时提供精确的音高控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确控制音高同时保持高质量音频的神经音频编解码器，为语音属性解耦提供可扩展的框架。

Method: 在训练期间将F0轮廓展平并随机移位，同时提供真实F0作为条件；使用向量量化瓶颈防止音高恢复；基于流的解码器生成高质量音频。

Result: PitchFlower比WORLD实现更准确的音高控制和更高的音频质量，在可控性方面优于SiFiGAN，同时保持可比较的质量。

Conclusion: 该框架不仅实现了音高解耦，还为解耦其他语音属性提供了简单且可扩展的路径。

Abstract: We present PitchFlower, a flow-based neural audio codec with explicit pitch
controllability. Our approach enforces disentanglement through a simple
perturbation: during training, F0 contours are flattened and randomly shifted,
while the true F0 is provided as conditioning. A vector-quantization bottleneck
prevents pitch recovery, and a flow-based decoder generates high quality audio.
Experiments show that PitchFlower achieves more accurate pitch control than
WORLD at much higher audio quality, and outperforms SiFiGAN in controllability
while maintaining comparable quality. Beyond pitch, this framework provides a
simple and extensible path toward disentangling other speech attributes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [154] [Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees](https://arxiv.org/abs/2510.24754)
*Yuqicheng Zhu,Jingcheng Wu,Yizhen Wang,Hongkuan Zhou,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: stat.ML

TL;DR: 提出了UnKGCP框架，为不确定性知识图谱嵌入方法生成预测区间，保证以用户指定置信水平包含真实分数，区间长度反映预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性知识图谱嵌入方法只产生点估计，无法量化预测不确定性，限制了在需要理解预测置信度的高风险应用中的可靠性。

Method: 基于共形预测框架，提出了专门针对UnKGE方法的新颖非共形性度量，以及高效的区间构建程序。

Result: 提供了区间的理论保证并通过实验验证，在标准基准测试中区间尖锐且能有效捕捉预测不确定性。

Conclusion: UnKGCP框架能够可靠地量化不确定性知识图谱嵌入的预测不确定性，为高风险应用提供置信度保证。

Abstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector
representations that capture both structural and uncertainty information to
predict scores of unseen triples. However, existing methods produce only point
estimates, without quantifying predictive uncertainty-limiting their
reliability in high-stakes applications where understanding confidence in
predictions is crucial. To address this limitation, we propose \textsc{UnKGCP},
a framework that generates prediction intervals guaranteed to contain the true
score with a user-specified level of confidence. The length of the intervals
reflects the model's predictive uncertainty. \textsc{UnKGCP} builds on the
conformal prediction framework but introduces a novel nonconformity measure
tailored to UnKGE methods and an efficient procedure for interval construction.
We provide theoretical guarantees for the intervals and empirically verify
these guarantees. Extensive experiments on standard benchmarks across diverse
UnKGE methods further demonstrate that the intervals are sharp and effectively
capture predictive uncertainty.

</details>


### [155] [Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm](https://arxiv.org/abs/2510.24815)
*Clément Bénard*

Main category: stat.ML

TL;DR: 提出了TreeHFD算法，用于从数据样本中估计树集合的Hoeffding分解，解决了输入变量依赖时的可解释性问题


<details>
  <summary>Details</summary>
Motivation: 树集合在表格数据预测中表现优异，但其黑盒特性限制了在关键决策中的应用。Hoeffding分解是强大的可解释性方法，但在输入变量依赖时的实际估计仍存在问题

Method: 开发TreeHFD算法，通过分层正交约束来估计树集合的Hoeffding分解，确保正交性、稀疏性和因果变量选择

Result: TreeHFD在模拟和真实数据上表现出高性能，能够收敛并保持正交性、稀疏性等性质，且与广泛使用的TreeSHAP方法有强关联

Conclusion: TreeHFD有效解决了树集合的可解释性问题，为依赖变量场景提供了实用的Hoeffding分解估计方法

Abstract: Tree ensembles have demonstrated state-of-the-art predictive performance
across a wide range of problems involving tabular data. Nevertheless, the
black-box nature of tree ensembles is a strong limitation, especially for
applications with critical decisions at stake. The Hoeffding or ANOVA
functional decomposition is a powerful explainability method, as it breaks down
black-box models into a unique sum of lower-dimensional functions, provided
that input variables are independent. In standard learning settings, input
variables are often dependent, and the Hoeffding decomposition is generalized
through hierarchical orthogonality constraints. Such generalization leads to
unique and sparse decompositions with well-defined main effects and
interactions. However, the practical estimation of this decomposition from a
data sample is still an open problem. Therefore, we introduce the TreeHFD
algorithm to estimate the Hoeffding decomposition of a tree ensemble from a
data sample. We show the convergence of TreeHFD, along with the main properties
of orthogonality, sparsity, and causal variable selection. The high performance
of TreeHFD is demonstrated through experiments on both simulated and real data,
using our treehfd Python package (https://github.com/ThalesGroup/treehfd).
Besides, we empirically show that the widely used TreeSHAP method, based on
Shapley values, is strongly connected to the Hoeffding decomposition.

</details>


### [156] [Generative Bayesian Optimization: Generative Models as Acquisition Functions](https://arxiv.org/abs/2510.25240)
*Rafael Oliveira,Daniel M. Steinberg,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出一种将生成模型转化为批量贝叶斯优化候选解采样器的通用策略，通过直接偏好优化训练生成模型，无需构建代理模型即可实现大批量、高维度和组合设计的优化。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法在大批量、非连续设计空间和高维组合优化方面存在局限，需要一种能够直接利用生成模型进行高效采样的方法。

Method: 采用直接偏好优化(DPO)思想，使用观测数据直接计算简单效用值训练生成模型，形成与期望效用成比例的提案分布，避免构建代理模型。

Result: 理论证明生成模型在BO过程中渐进收敛到全局最优解，实验验证了该方法在高维大批量优化问题上的有效性。

Conclusion: 该方法为批量贝叶斯优化提供了一种通用且高效的生成模型框架，能够处理复杂的设计空间和大批量优化问题。

Abstract: We present a general strategy for turning generative models into candidate
solution samplers for batch Bayesian optimization (BO). The use of generative
models for BO enables large batch scaling as generative sampling, optimization
of non-continuous design spaces, and high-dimensional and combinatorial design.
Inspired by the success of direct preference optimization (DPO), we show that
one can train a generative model with noisy, simple utility values directly
computed from observations to then form proposal distributions whose densities
are proportional to the expected utility, i.e., BO's acquisition function
values. Furthermore, this approach is generalizable beyond preference-based
feedback to general types of reward signals and loss functions. This
perspective avoids the construction of surrogate (regression or classification)
models, common in previous methods that have used generative models for
black-box optimization. Theoretically, we show that the generative models
within the BO process approximately follow a sequence of distributions which
asymptotically concentrate at the global optima under certain conditions. We
also demonstrate this effect through experiments on challenging optimization
problems involving large batches in high dimensions.

</details>


### [157] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: 本文证明了在可逆马尔可夫链条件下，标准离策略TD(0)算法与线性函数逼近的收敛性，通过建立折扣因子的显式上界来保证收敛。


<details>
  <summary>Details</summary>
Motivation: 离策略学习与函数逼近结合可能导致算法发散，现有方法通过重要性采样等修改算法来确保收敛，但增加了复杂性。本文旨在分析标准算法在可逆马尔可夫链条件下的收敛性。

Method: 将Tsitsiklis和Van Roy的随机逼近框架从同策略情况扩展到离策略情况，限制在可逆马尔可夫链类中进行分析，建立了折扣因子的显式上界条件。

Result: 在可逆马尔可夫链条件下，标准离策略TD(0)算法以概率1收敛，并达到投影贝尔曼误差为零。通过一维随机游走和加权图上的随机游走等示例验证了结果。

Conclusion: 通过限制在可逆马尔可夫链类中，标准离策略TD(0)算法可以在不修改算法的情况下实现收敛，这为许多实际应用提供了理论保证。

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [158] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 提出了一种结合变分自编码器和混合效应回归的方法，用于在罕见疾病小样本研究中处理测量工具变化的问题，并通过统计推断评估治疗转换效果。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病治疗选择有限，患者经常转换治疗，但小样本量和测量工具变化使传统纵向建模方法难以应用。

Method: 使用变分自编码器将不同测量工具的观测值映射到共享的潜在空间，然后应用混合效应回归模型捕捉时间动态和治疗转换效应。

Result: 成功应用于脊髓性肌萎缩症患者数据，能够对齐不同测量工具的运动表现项目，并量化治疗转换效果。

Conclusion: 该方法展示了在联合潜在表示中建模解决小数据挑战的潜力，支持模型选择和治疗转换效果评估。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [159] [Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)
*Hugo Lavenant,Giacomo Zanella*

Main category: stat.ML

TL;DR: 本文研究了掩码扩散模型的计算效率与准确性权衡，提供了与数据维度无关的误差界限，并确定了基于数据分布信息特征的最优调度策略。


<details>
  <summary>Details</summary>
Motivation: 研究离散数据生成模型（如掩码扩散模型）在减少自回归模型计算成本时的计算效率与准确性权衡问题，探索如何通过优化调度策略来提升性能。

Method: 使用采样算法直接定义方法，不采用传统的时间反转扩散过程推导，从而获得简单透明的证明。分析非恒定调度大小的影响，并根据数据分布的信息特征确定最优调度策略。

Result: 建立了仅依赖于每次迭代生成的平均令牌数且与数据维度无关的相对熵误差界限，支持了掩码扩散模型的经验成功。确定了最优调度作为数据分布信息特征函数的形式。

Conclusion: 掩码扩散模型在计算效率与准确性之间存在可控的权衡关系，通过优化调度策略可以显著提升性能，且该方法具有理论保证和实际可行性。

Abstract: Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.

</details>


### [160] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出了一种基于累积和动态控制限的方法，用于持续监测机器学习模型的校准状态，能够检测传统过程监控和概念漂移应用中的校准偏差。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习模型在图像分类方面取得了显著进展，但如何评估和维持预测校准性是一个重要问题。现有方法主要关注校准评估和重新校准，但缺乏持续监测模型随时间可能失去校准的方法。

Method: 使用累积和控制图方法，结合动态控制限，能够监测概率预测和事件结果，无需访问机器学习模型内部结构。

Result: 该方法能够早期检测影响图像分类性能的操作环境变化，适用于任何需要随时间监测概率预测校准性的场景。

Conclusion: 提出的图表方法为持续监测机器学习模型校准提供了一种有效工具，特别适用于实际部署环境中检测校准偏差。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


### [161] [How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs](https://arxiv.org/abs/2510.25753)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 该论文研究了预训练Transformer在非线性任务上的上下文学习能力，分析了非线性MLP头如何增强ICL性能，并揭示了数据混合效应和特征学习的关键条件。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究通常使用简化的架构和数据模型，限制了其对现实场景的适用性。本研究旨在分析真实Transformer在非线性、多源异构数据下的上下文学习机制。

Method: 分析包含两层MLP的Transformer模型，第一层通过单步梯度训练，第二层完全优化。在高维渐近理论下，证明此类模型等价于结构化多项式预测器，利用高斯普适性和正交多项式理论。

Result: 非线性MLP显著提升了ICL性能，特别是在非线性任务上。识别了高质量数据源的关键特性（低噪声、结构化协方差），并发现只有当任务协方差具有足够结构时才会出现特征学习。

Conclusion: 该工作推进了Transformer上下文学习的理论基础，为架构设计和数据选择提供了可操作的见解，并通过多语言情感分析实验验证了理论发现的实际适用性。

Abstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.

</details>


### [162] [E-Scores for (In)Correctness Assessment of Generative Model Outputs](https://arxiv.org/abs/2510.25770)
*Guneet S. Dhillon,Javier González,Teodora Pandeva,Alicia Curth*

Main category: stat.ML

TL;DR: 本文提出使用e值来评估生成模型输出的错误性，替代传统的p值方法，避免p-hacking问题，同时保持统计保证。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成模型（特别是大语言模型）正确性的机制有限，基于p值的方法容易受到p-hacking的影响，无法在事后选择容忍度时保持统计保证。

Method: 利用符合预测框架，使用e值来补充生成模型输出，提供e分数作为错误性度量，允许用户在观察e分数后自适应选择容忍度。

Result: 实验证明该方法在评估数学事实性和属性约束满足等不同类型的正确性方面有效，同时实现了与之前方法相同的统计保证。

Conclusion: e分数方法为生成模型输出评估提供了更灵活和鲁棒的机制，解决了p-hacking问题，并支持自适应容忍度选择。

Abstract: While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [163] [Robust variable selection for spatial point processes observed with noise](https://arxiv.org/abs/2510.25550)
*Dominik Sturm,Ivo F. Sbalzarini*

Main category: stat.ME

TL;DR: 提出一种结合稀疏估计和噪声鲁棒模型选择的空间点过程变量选择方法，通过稳定性选择和最佳子集惩罚来提高噪声环境下的变量选择性能


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率空间数据通过遥感和自动图像分析变得越来越可用，识别影响事件定位的空间协变量对于理解潜在机制至关重要。然而，自动化采集技术的结果通常存在噪声，导致虚假位移和遗漏事件，这影响了稀疏点过程估计的可靠性

Method: 使用基于点过程子采样的稳定性选择，并结合非凸最佳子集惩罚来增强模型选择性能。研究了噪声对稀疏点过程估计的影响，包括泊松过程和托马斯过程

Result: 在广泛的模拟实验中，该方法在不同噪声场景下可靠地恢复了真实协变量，提高了选择准确性和稳定性。在林业数据集上的应用展示了该方法的实际效用

Conclusion: 该方法为噪声环境下的空间点过程模型提供了系统性的鲁棒变量选择框架，无需额外过程知识

Abstract: We propose a method for variable selection in the intensity function of
spatial point processes that combines sparsity-promoting estimation with
noise-robust model selection. As high-resolution spatial data becomes
increasingly available through remote sensing and automated image analysis,
identifying spatial covariates that influence the localization of events is
crucial to understand the underlying mechanism. However, results from automated
acquisition techniques are often noisy, for example due to measurement
uncertainties or detection errors, which leads to spurious displacements and
missed events. We study the impact of such noise on sparse point-process
estimation across different models, including Poisson and Thomas processes. To
improve noise robustness, we propose to use stability selection based on
point-process subsampling and to incorporate a non-convex best-subset penalty
to enhance model-selection performance. In extensive simulations, we
demonstrate that such an approach reliably recovers true covariates under
diverse noise scenarios and improves both selection accuracy and stability. We
then apply the proposed method to a forestry data set, analyzing the
distribution of trees in relation to elevation and soil nutrients in a tropical
rain forest. This shows the practical utility of the method, which provides a
systematic framework for robust variable selection in spatial point-process
models under noise, without requiring additional knowledge of the process.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [164] [Constructive Lyapunov Functions via Topology-Preserving Neural Networks](https://arxiv.org/abs/2510.24730)
*Jaehong Oh*

Main category: eess.SY

TL;DR: ONN算法在收敛速度、边缘效率和计算复杂度方面达到最优性能，在300万节点语义网络上比基线方法提升99.75%，ORTSF集成到Transformer中在WikiText-103上实现14.7%困惑度降低和2.3倍收敛加速。


<details>
  <summary>Details</summary>
Motivation: 将Massera的抽象存在定理转化为具体、可扩展的算法，为神经网络、机器人和分布式系统中的构造性稳定性分析开辟途径。

Method: 开发ONN算法，建立与最优控制、信息几何、拓扑数据分析、离散几何和范畴论的深层联系。

Result: 证明指数收敛（μ=3.2×10^-4）和拓扑保持，ORTSF集成到Transformer中实现显著性能提升。

Conclusion: 这项工作将抽象理论转化为具有可证明保证的具体算法，为多个领域提供了构造性稳定性分析的新途径。

Abstract: We prove that ONN achieves order-optimal performance on convergence rate
($\mu \propto \lambda_2$), edge efficiency ($E = N$ for minimal connectivity $k
= 2$), and computational complexity ($O(N d^2)$). Empirical validation on
3M-node semantic networks demonstrates 99.75\% improvement over baseline
methods, confirming exponential convergence ($\mu = 3.2 \times 10^{-4}$) and
topology preservation. ORTSF integration into transformers achieves 14.7\%
perplexity reduction and 2.3 faster convergence on WikiText-103. We establish
deep connections to optimal control (Hamilton-Jacobi-Bellman), information
geometry (Fisher-efficient natural gradient), topological data analysis
(persistent homology computation in $O(KN)$), discrete geometry (Ricci flow),
and category theory (adjoint functors). This work transforms Massera's abstract
existence theorem into a concrete, scalable algorithm with provable guarantees,
opening pathways for constructive stability analysis in neural networks,
robotics, and distributed systems.

</details>


### [165] [Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification](https://arxiv.org/abs/2510.24757)
*Ahmet Eren Sertbaş,Tufan Kumbasar*

Main category: eess.SY

TL;DR: 提出了一种稳定设计的LPV神经网络状态空间模型，通过Schur参数化保证稳定性，直接从数据中学习潜在状态和内部调度变量。


<details>
  <summary>Details</summary>
Motivation: 传统辨识方法难以在保持稳定性的同时捕捉潜在动态，需要一种能同时学习潜在状态和调度变量并保证稳定性的非线性系统建模方法。

Method: 结合编码器进行初始状态估计和状态空间表示网络构建调度依赖的系统矩阵，使用Schur参数化保证状态转移矩阵稳定性，训练框架整合多步预测损失和状态一致性正则化项。

Result: 在基准非线性系统上的评估表明，该模型持续匹配或超越经典子空间辨识方法和最近的基于梯度的方法。

Conclusion: 稳定性约束的神经LPV辨识为复杂非线性系统建模提供了一个可扩展且可靠的框架。

Abstract: Accurate modeling of nonlinear systems is essential for reliable control, yet
conventional identification methods often struggle to capture latent dynamics
while maintaining stability. We propose a \textit{stable-by-design LPV neural
network-based state-space} (NN-SS) model that simultaneously learns latent
states and internal scheduling variables directly from data. The
state-transition matrix, generated by a neural network using the learned
scheduling variables, is guaranteed to be stable through a Schur-based
parameterization. The architecture combines an encoder for initial state
estimation with a state-space representer network that constructs the full set
of scheduling-dependent system matrices. For training the NN-SS, we develop a
framework that integrates multi-step prediction losses with a state-consistency
regularization term, ensuring robustness against drift and improving
long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark
nonlinear systems, and the results demonstrate that the model consistently
matches or surpasses classical subspace identification methods and recent
gradient-based approaches. These findings highlight the potential of
stability-constrained neural LPV identification as a scalable and reliable
framework for modeling complex nonlinear systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [166] [scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration](https://arxiv.org/abs/2510.24987)
*Jianle Sun,Chaoqi Liang,Ran Wei,Peng Zheng,Lei Bai,Wanli Ouyang,Hongliang Yan,Peng Ye*

Main category: q-bio.QM

TL;DR: 提出scMRDR框架，用于无配对多组学单细胞数据整合，通过解耦模态共享和特定表示，结合多种正则化方法实现可扩展的跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 单细胞多组学数据整合面临挑战，现有方法依赖配对信息或计算全局耦合矩阵，限制了可扩展性和灵活性。

Method: 使用β-VAE架构解耦模态共享和特定表示，结合等距正则化、对抗性目标和掩码重建损失策略。

Result: 在基准数据集上表现出优异的批次校正、模态对齐和生物信号保留性能，能有效扩展到大规模数据集。

Conclusion: scMRDR为大规模多组学数据整合提供了强大灵活的解决方案，支持两个以上组学的整合。

Abstract: Advances in single-cell sequencing have enabled high-resolution profiling of
diverse molecular modalities, while integrating unpaired multi-omics
single-cell data remains challenging. Existing approaches either rely on pair
information or prior correspondences, or require computing a global pairwise
coupling matrix, limiting their scalability and flexibility. In this paper, we
introduce a scalable and flexible generative framework called single-cell
Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired
multi-omics integration. Specifically, we disentangle each cell's latent
representations into modality-shared and modality-specific components using a
well-designed $\beta$-VAE architecture, which are augmented with isometric
regularization to preserve intra-omics biological heterogeneity, adversarial
objective to encourage cross-modal alignment, and masked reconstruction loss
strategy to address the issue of missing features across modalities. Our method
achieves excellent performance on benchmark datasets in terms of batch
correction, modality alignment, and biological signal preservation. Crucially,
it scales effectively to large-level datasets and supports integration of more
than two omics, offering a powerful and flexible solution for large-scale
multi-omics data integration and downstream biological discovery.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [167] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文推导了多种基础差分隐私机制的紧致零集中差分隐私(zCDP)边界，包括拉普拉斯机制、离散拉普拉斯机制、k-随机响应和RAPPOR，证实了Wang(2022)关于拉普拉斯机制zCDP边界的猜想。


<details>
  <summary>Details</summary>
Motivation: 虽然存在从ε-DP到zCDP的最坏情况转换，但许多常用算法满足更强的保证。本文旨在为几种基础机制推导紧致的zCDP特征描述。

Method: 通过理论分析和证明，推导了拉普拉斯机制、离散拉普拉斯机制、k-随机响应(k≤6)和RAPPOR的紧致zCDP边界，并提供了有界范围机制的最坏情况zCDP边界。

Result: 证明了ε-DP拉普拉斯机制的紧致zCDP边界为ε + e^(-ε) - 1，证实了Wang(2022)的猜想。同时为其他机制提供了相应的紧致zCDP边界。

Conclusion: 本文为多种基础差分隐私机制提供了紧致的zCDP特征描述，填补了理论空白，有助于更准确地分析这些机制在zCDP框架下的隐私保护性能。

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>


### [168] [Is Protective DNS Blocking the Wild West?](https://arxiv.org/abs/2510.25352)
*David Plonka,Branden Palacio,Debbie Perouli*

Main category: cs.CR

TL;DR: 对研究教育网络中保护性DNS服务的性能进行被动测量研究，发现DNS阻止列表在名称、目标、透明度和来源方面存在混乱，难以比较，缺乏组织监督，在大规模运营中存在挑战和风险。


<details>
  <summary>Details</summary>
Motivation: 研究保护性DNS服务在研究教育网络中的实际表现，该网络服务于数百个成员机构，旨在了解基于免费DNS阻止列表的保护机制的有效性和可操作性。

Method: 使用免费可用的DNS阻止列表（包含被视为威胁的域名），对一周内观察到的数亿用户真实DNS查询进行测试，分析哪些查询会被阻止。

Result: 发现阻止列表在名称、目标、透明度和来源方面存在混乱，难以进行比较，这些保护性DNS的基础设施缺乏组织监督。

Conclusion: 保护性DNS的底层机制缺乏有序监管，在大规模运营中面临挑战和风险，需要更好的组织和标准化。

Abstract: We perform a passive measurement study investigating how a Protective DNS
service might perform in a Research & Education Network serving hundreds of
member institutions. Utilizing freely-available DNS blocklists consisting of
domain names deemed to be threats, we test hundreds of millions of users' real
DNS queries, observed over a week's time, to find which answers would be
blocked because they involve domain names that are potential threats. We find
the blocklists disorderly regarding their names, goals, transparency, and
provenance making them quite difficult to compare. Consequently, these
Protective DNS underpinnings lack organized oversight, presenting challenges
and risks in operation at scale.

</details>


### [169] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: 本文研究序列数据发布中的隐私风险，提出基于隐马尔可夫模型和强化学习的双向推理攻击模型，证明即使单个发布满足隐私保证，时序相关性仍可能导致敏感信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现实系统中数据通常以序列或连续方式发布，时序相关性可能让攻击者从多个发布中推断出单个发布中隐藏的敏感信息，而现有研究主要关注单次数据发布的隐私保护。

Method: 提出结合隐马尔可夫模型和强化学习的双向推理攻击模型，利用序列中前后观测来推断私有信息，并在轨迹数据场景中实例化该框架。

Result: 在Geolife、Porto Taxi和SynMob数据集上的实验表明，该模型显著优于独立处理每个发布的基线方法，揭示了序列数据发布的基本隐私风险。

Conclusion: 序列数据发布存在固有隐私风险，需要开发新的隐私保护框架来显式建模时序依赖性，如时间感知差分隐私或序列数据混淆策略。

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [170] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: RAGuard是一个检测框架，通过扩展检索范围、基于困惑度的过滤和文本相似性过滤来识别RAG系统中的中毒文本，有效防御数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然增强了LLMs的能力，但引入了数据投毒的安全风险，现有防御方法难以应对高级攻击。

Method: 采用三阶段方法：扩展检索范围增加干净文本比例；基于困惑度的分块过滤检测异常变化；文本相似性过滤标记高度相似文本。

Result: 在大规模数据集上的实验表明，RAGuard能有效检测和缓解投毒攻击，包括强自适应攻击。

Conclusion: RAGuard提供了一种非参数化方法，显著提升了RAG系统的安全性，能够可靠防御数据投毒威胁。

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [171] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: 该论文调查了Android系统中恶意软件威胁的检测方法，重点比较了机器学习技术在检测恶意威胁方面的性能表现。


<details>
  <summary>Details</summary>
Motivation: 全球恶意软件威胁日益增加，特别是Android系统上的加密勒索软件，对移动通信安全构成严重威胁，破坏用户体验并带来隐私风险。

Method: 采用机器学习技术进行恶意威胁检测，对Android应用程序数据集进行分析，并通过准确率等指标评估不同方法的性能。

Result: 研究发现大多数现有研究依赖用户反馈和评论，但这些信息可能被恶意操纵。机器学习方法在检测恶意威胁方面显示出潜力，但需要大量数据支持。

Conclusion: 开发专门、自动化的反恶意软件系统面临数据需求大的挑战，需要更稳健的机器学习方法来应对Android平台上的恶意威胁。

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [172] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文提出了针对机器学习模型反演攻击的防御方案L2FE-Hash，这是首个支持欧几里得距离比较器的模糊提取器，能有效抵御包括新提出的PIPE攻击在内的各种反演攻击。


<details>
  <summary>Details</summary>
Motivation: 模型反演攻击对使用机器学习模型的隐私敏感应用构成严重威胁，特别是在人脸认证系统中。现有防御方案缺乏系统性的安全特性定义，且现有的模糊提取器在ML人脸认证中并不安全。

Method: 首先形式化定义了防御模型反演攻击所需的安全特性，并将其与密码学中的模糊提取器概念关联。提出新的PIPE攻击来验证现有方案的不安全性，然后设计了L2FE-Hash模糊提取器，支持标准的欧几里得距离比较器。

Result: PIPE攻击对现有方案的攻击成功率超过89%。L2FE-Hash在理论分析中具有计算安全性保证，在实证测试中在人脸认证中表现出可用精度，能完全抵御包括PIPE在内的各种反演攻击。

Conclusion: L2FE-Hash是首个针对ML人脸认证系统设计的模糊提取器，提供了攻击无关的安全性，无需重新训练受保护的ML模型，能有效防御模型反演攻击。

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [173] [Decoding non-invasive brain activity with novel deep-learning approaches](https://arxiv.org/abs/2510.24733)
*Richard Csaky*

Main category: eess.SP

TL;DR: 该论文研究非侵入性脑电信号（EEG和MEG）的建模和解码，重点关注视觉刺激感知和内心言语的脑活动解码，并处理脑电记录中的变异性问题。


<details>
  <summary>Details</summary>
Motivation: 研究大脑在感知视觉刺激或进行内心言语时的活动，并提高此类刺激的解码性能，同时解决脑电记录中存在的个体内、个体间和数据集间的变异性问题。

Method: 分为方法学和实验两部分：方法学部分使用深度学习进行脑解码，包括个体层面的线性模型、群体解码的深度学习方法，以及基于卷积和Transformer架构的MEG数据预测模型；实验部分收集了包含EEG、MEG和OPM数据的高试验次数内心言语数据集。

Result: 基于Transformer的模型在生成接近真实脑数据的信号方面表现出色，提高了脑电生理学建模的准确性和可靠性；但内心言语的解码结果大多为阴性，表明内心言语解码具有挑战性。

Conclusion: 深度学习特别是Transformer架构在脑电信号建模方面具有潜力，但内心言语的解码仍然是一个困难的任务，需要进一步研究来解决这一挑战。

Abstract: This thesis delves into the world of non-invasive electrophysiological brain
signals like electroencephalography (EEG) and magnetoencephalography (MEG),
focusing on modelling and decoding such data. The research aims to investigate
what happens in the brain when we perceive visual stimuli or engage in covert
speech (inner speech) and enhance the decoding performance of such stimuli. The
thesis is divided into two main sections, methodological and experimental work.
A central concern in both sections is the large variability present in
electrophysiological recordings, whether it be within-subject or
between-subject variability, and to a certain extent between-dataset
variability. In the methodological sections, we explore the potential of deep
learning for brain decoding. We present advancements in decoding visual stimuli
using linear models at the individual subject level. We then explore how deep
learning techniques can be employed for group decoding, introducing new methods
to deal with between-subject variability. Finally, we also explores novel
forecasting models of MEG data based on convolutional and Transformer-based
architectures. In particular, Transformer-based models demonstrate superior
capabilities in generating signals that closely match real brain data, thereby
enhancing the accuracy and reliability of modelling the brain's
electrophysiology. In the experimental section, we present a unique dataset
containing high-trial inner speech EEG, MEG, and preliminary optically pumped
magnetometer (OPM) data. Our aim is to investigate different types of inner
speech and push decoding performance by collecting a high number of trials and
sessions from a few participants. However, the decoding results are found to be
mostly negative, underscoring the difficulty of decoding inner speech.

</details>


### [174] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: Cardi-GPT是一个基于深度学习和自然语言交互的ECG专家系统，使用16残差块CNN处理12导联心电图，在24种心脏疾病上达到0.6194加权准确率，并通过模糊化层和聊天机器人界面改善临床沟通。


<details>
  <summary>Details</summary>
Motivation: 传统ECG解读和临床沟通需要大量专业知识且具有挑战性，需要一种能简化ECG解释并增强临床沟通的解决方案。

Method: 采用16残差块CNN处理12导联ECG数据，加入模糊化层将数值输出转换为临床语言类别，集成聊天机器人界面进行诊断洞察探索和医疗提供者间沟通。

Result: 在跨越4个国家6家医院的多样化数据集上评估，表现优于基线模型，整体响应质量得分达73%（覆盖度、基础性和连贯性评估）。

Conclusion: Cardi-GPT通过弥合复杂ECG数据解读与可行临床洞察之间的差距，代表了心血管医疗的变革性创新，有望提高诊断准确性、临床工作流程和患者结局。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [175] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: StrikeWatch是一个紧凑型腕戴系统，通过IMU信号在设备上实时进行步态识别，特别针对检测脚跟与脚前掌着地模式，帮助跑者自我纠正有害步态。


<details>
  <summary>Details</summary>
Motivation: 跑步有益健康但不正确的步态模式会导致损伤，现有步态分析系统笨重且只能离线分析，腕戴设备更实用但实时步态识别面临IMU信号噪声、计算资源有限和依赖云端连接的挑战。

Method: 提出四种紧凑深度学习架构（1D-CNN、1D-SepCNN、LSTM和Transformer），在AMD Spartan-7和Lattice iCE40UP5K FPGA上优化能效推理，使用自定义硬件原型收集户外跑步标注数据集。

Result: 6位量化1D-SepCNN在iCE40UP5K上达到最高平均F1分数0.847，每次推理仅消耗0.350μJ，延迟0.140ms，320mAh电池可支持13.6天连续推理。

Conclusion: 研究揭示了模型复杂度和硬件效率之间的权衡，1D-SepCNN在紧凑腕戴设备上实现了高效的实时步态识别，为跑者提供实时反馈纠正有害步态。

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [176] [Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR](https://arxiv.org/abs/2510.24740)
*Nader Nemati*

Main category: eess.SP

TL;DR: 提出STAR（正弦时间-幅度重采样）作为心电图分类的节拍级数据增强方法，通过在连续R峰之间进行受控时间扭曲和幅度缩放，保留关键形态特征并提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 临床12导联心电图分类面临记录条件多样、病理重叠和标签不平衡等挑战，传统增强方法可能扭曲诊断关键形态，需要一种既能增加训练多样性又能保持临床可信度的增强技术。

Method: STAR在连续R峰之间的R-R段进行节拍级增强，应用受控时间扭曲和幅度缩放，保持P-QRS-T顺序不变，且不改变轨迹的头尾部分，可与常见1D SE-ResNet风格ECG编码器集成。

Result: STAR提供形态保真变异性，增强训练多样性而不破坏峰值或间隔；提高跨设备、站点和队列的稳定性；通过节拍级增强改善稀有类别学习，减少过拟合。

Conclusion: STAR为临床ECG分类提供了一种简单可控的数据增强方法，在形态可信度、操作简单性和跨源耐久性方面具有优势，并发布了完整的Python实现和透明训练流程。

Abstract: Clinical 12-lead ECG classification remains difficult because of diverse
recording conditions, overlapping pathologies, and pronounced label imbalance
hinder generalization, while unconstrained augmentations risk distorting
diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude
Resampling (STAR) is introduced as a beat-wise augmentation that operates
strictly between successive R-peaks to apply controlled time warping and
amplitude scaling to each R--R segment, preserving the canonical P--QRS--T
order and leaving the head and tail of the trace unchanged. STAR is designed
for practical pipelines and offers: (i) morphology-faithful variability that
broadens training diversity without corrupting peaks or intervals; (ii)
source-resilient training, improving stability across devices, sites, and
cohorts without dataset-specific tuning; (iii) model-agnostic integration with
common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on
rare classes via beat-level augmentation, reducing overfitting by resampling
informative beats instead of duplicating whole records. In contrast to global
crops, large shifts, or additive noise, STAR avoids transformations that
suppress or misalign clinical landmarks. A complete Python implementation and a
transparent training workflow are released, aligned with a source-aware,
stratified five-fold protocol over a multi-institutional 12-lead corpus,
thereby facilitating inspection and reuse. Taken together, STAR provides a
simple and controllable augmentation for clinical ECG classification where
trustworthy morphology, operational simplicity, and cross-source durability are
essential.

</details>


### [177] [EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification](https://arxiv.org/abs/2510.24748)
*Dong-Hyeon Kang,Ju-Hyeon Nam,Sang-Chul Lee*

Main category: eess.SP

TL;DR: EcoScaleNet是一种高效的卷积全尺度网络，通过分层设计和瓶颈卷积，在保持全感受野覆盖的同时显著降低计算成本，在长序列ECG分类中实现SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 解决12导联心电图分类中手动读取易出错、传统CNN难以选择适合长序列感受野大小的问题，以及Omni Scale CNN计算成本过高的问题。

Method: 提出分层变体EcoScaleNet，在每个阶段限制最大核长度，插入瓶颈卷积控制通道增长并融合多尺度特征，消除冗余计算。

Result: 在CODE 15% ECG数据集上，相比OS CNN减少90%参数和99%FLOPs，同时将宏平均F1分数提高2.4%。

Conclusion: EcoScaleNet以极低计算成本实现长序列ECG分类的SOTA精度，可在普通硬件上实时部署。

Abstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for
early detection of cardiac abnormalities, yet manual reading is error prone and
existing CNN based classifiers struggle to choose receptive field sizes that
generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN)
addresses this by enumerating prime sized kernels inspired by Goldbach
conjecture to cover every scale, but its exhaustive design explodes
computational cost and blocks deeper, wider models. We present Efficient
Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that
retains full receptive field coverage while eliminating redundancy. At each
stage, the maximum kernel length is capped to the scale still required after
down sampling, and bottleneck convolutions inserted before and after every Omni
Scale block curtail channel growth and fuse multi scale features. On the large
scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by
99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These
results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence
ECG classification at a fraction of the computational cost, enabling real time
deployment on commodity hardware. Our EcoScaleNet code is available in GitHub
Link.

</details>


### [178] [Continuous subsurface property retrieval from sparse radar observations using physics informed neural networks](https://arxiv.org/abs/2510.25648)
*Ishfaq Aziz,Mohamad Alipour*

Main category: eess.SP

TL;DR: 提出了一种物理信息机器学习框架，用于重建地下介电常数作为深度的连续函数，通过模拟和雷达实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统波反演方法通常假设少数离散均匀层，需要密集测量或对材料边界的强先验知识，限制了在真实连续变化场景中的可扩展性和准确性。

Method: 使用物理信息机器学习框架，将地下介电常数重建为深度的完全神经连续函数，训练同时满足测量数据和麦克斯韦方程。

Result: 与现场介电常数测量结果高度一致（R^2=0.93），对细微变化敏感（Δε_r=2），在两层系统中仅需三个战略布置的传感器即可恢复准确剖面。

Conclusion: 该方法将地下反演从边界驱动重新定义为连续属性估计，能够准确表征平滑介电常数变化，推进了使用低成本雷达系统的电磁成像。

Abstract: Estimating subsurface dielectric properties is essential for applications
ranging from environmental surveys of soils to nondestructive evaluation of
concrete in infrastructure. Conventional wave inversion methods typically
assume few discrete homogeneous layers and require dense measurements or strong
prior knowledge of material boundaries, limiting scalability and accuracy in
realistic settings where properties vary continuously. We present a physics
informed machine learning framework that reconstructs subsurface permittivity
as a fully neural, continuous function of depth, trained to satisfy both
measurement data and Maxwells equations. We validate the framework with both
simulations and custom built radar experiments on multilayered natural
materials. Results show close agreement with in-situ permittivity measurements
(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).
Parametric analysis reveals that accurate profiles can be recovered with as few
as three strategically placed sensors in two layer systems. This approach
reframes subsurface inversion from boundary-driven to continuous property
estimation, enabling accurate characterization of smooth permittivity
variations and advancing electromagnetic imaging using low cost radar systems.

</details>


### [179] [PyDPF: A Python Package for Differentiable Particle Filtering](https://arxiv.org/abs/2510.25693)
*John-Joseph Brady,Benjamin Cox,Víctor Elvira,Yunpeng Li*

Main category: eess.SP

TL;DR: 本文提供了一个基于PyTorch的统一API实现，实现了多种可微分粒子滤波器(DPFs)，使这些算法更易于访问并便于比较。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波器(PF)是时间序列分析中估计隐藏状态的有效方法，但标准粒子滤波器不可微分，无法直接应用梯度优化技术。最近提出的方法通过修改重采样步骤使粒子滤波器可微分，但缺乏统一的实现框架。

Method: 在PyTorch框架上构建统一API，实现多种可微分粒子滤波器(DPFs)，通过修改重采样步骤使粒子滤波过程可微分。

Result: 成功复现了多个现有研究的实验，展示了DPFs如何应用于解决状态空间建模中的常见挑战。

Conclusion: 该实现使可微分粒子滤波器算法更易于被广泛研究社区访问，并促进了算法间的直接比较。

Abstract: State-space models (SSMs) are a widely used tool in time series analysis. In
the complex systems that arise from real-world data, it is common to employ
particle filtering (PF), an efficient Monte Carlo method for estimating the
hidden state corresponding to a sequence of observations. Applying particle
filtering requires specifying both the parametric form and the parameters of
the system, which are often unknown and must be estimated. Gradient-based
optimisation techniques cannot be applied directly to standard particle
filters, as the filters themselves are not differentiable. However, several
recently proposed methods modify the resampling step to make particle filtering
differentiable. In this paper, we present an implementation of several such
differentiable particle filters (DPFs) with a unified API built on the popular
PyTorch framework. Our implementation makes these algorithms easily accessible
to a broader research community and facilitates straightforward comparison
between them. We validate our framework by reproducing experiments from several
existing studies and demonstrate how DPFs can be applied to address several
common challenges with state space modelling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [180] [Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems](https://arxiv.org/abs/2510.24909)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 开发了一个计算信任模型，将i*概念建模与多智能体系统的计算信任机制相结合，用于分析合作竞争环境中动态演化的信任关系。


<details>
  <summary>Details</summary>
Motivation: 解决需求工程中合作竞争环境下信任关系动态演化的建模问题，填补概念建模语言缺乏计算机制与计算信任模型缺乏需求工程背景之间的鸿沟。

Method: 开发双层信任系统（即时信任和声誉），采用非对称更新机制，合作逐步建立信任而违规急剧侵蚀信任，并构建从i*依赖网络到计算信任模型的结构化转换框架。

Result: 在78,125个参数配置下验证了负面偏见、滞后效应和累积损害放大的稳健涌现，雷诺-日产联盟案例研究（1999-2025）达到81.7%的验证准确率。

Conclusion: 成功构建了连接需求工程概念建模与计算信任分析的桥梁，能够有效模拟合作竞争环境中信任的动态演化过程。

Abstract: Requirements engineering increasingly occurs in multi-stakeholder
environments where organizations simultaneously cooperate and compete, creating
coopetitive relationships in which trust evolves dynamically based on observed
behavior over repeated interactions. While conceptual modeling languages like
i* represent trust relationships qualitatively, they lack computational
mechanisms for analyzing how trust changes with behavioral evidence.
Conversely, computational trust models from multi-agent systems provide
algorithmic updating but lack grounding in requirements engineering contexts
and conceptual models. This technical report bridges this gap by developing a
computational trust model that extends game-theoretic foundations for strategic
coopetition with dynamic trust evolution. We introduce trust as a two-layer
system with immediate trust responding to current behavior and reputation
tracking violation history. Trust evolves through asymmetric updating where
cooperation builds trust gradually while violations erode it sharply, creating
hysteresis effects and trust ceilings that constrain relationship recovery. We
develop a structured translation framework enabling requirements engineers to
instantiate computational trust models from i* dependency networks and
organizational contexts. Comprehensive experimental validation across 78,125
parameter configurations establishes robust emergence of negativity bias,
hysteresis effects, and cumulative damage amplification. Empirical validation
using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60
validation points (81.7%), successfully reproducing documented trust evolution
across five distinct relationship phases including crisis and recovery periods.
This technical report builds upon its foundational companion work in
arXiv:2510.18802.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [181] [Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling](https://arxiv.org/abs/2510.25001)
*Riddhi Pratim Ghosh,Ian Barnett*

Main category: stat.CO

TL;DR: 比较贝叶斯神经网络(BNNs)和混合密度网络(MDNs)在不确定性感知非线性回归中的表现，发现MDNs在捕捉多模态响应和自适应不确定性方面更有效，而BNNs在有限数据下提供更可解释的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究两种主要概率神经建模范式在不确定性感知非线性回归中的表现，为非线性系统中的不确定性建模提供指导。

Method: 建立统一的理论和实证框架，在理论方面推导收敛率和误差界限，在实证方面评估两种架构在合成非线性数据集和放射学基准上的表现。

Result: MDNs由于基于似然的性质实现更快的KL散度收敛，更有效地捕捉多模态响应和自适应不确定性；BNNs存在变分推断引起的额外近似偏差，但在有限数据下提供更可解释的认知不确定性。

Conclusion: 两种方法具有互补优势：后验基于的BNNs和似然基于的MDNs分别在不同方面表现出色，为不确定性感知建模提供了有价值的指导。

Abstract: This paper investigates two prominent probabilistic neural modeling
paradigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs)
for uncertainty-aware nonlinear regression. While BNNs incorporate epistemic
uncertainty by placing prior distributions over network parameters, MDNs
directly model the conditional output distribution, thereby capturing
multimodal and heteroscedastic data-generating mechanisms. We present a unified
theoretical and empirical framework comparing these approaches. On the
theoretical side, we derive convergence rates and error bounds under H\"older
smoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL)
divergence convergence due to their likelihood-based nature, whereas BNNs
exhibit additional approximation bias induced by variational inference.
Empirically, we evaluate both architectures on synthetic nonlinear datasets and
a radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and
qualitative results demonstrate that MDNs more effectively capture multimodal
responses and adaptive uncertainty, whereas BNNs provide more interpretable
epistemic uncertainty under limited data. Our findings clarify the
complementary strengths of posterior-based and likelihood-based probabilistic
learning, offering guidance for uncertainty-aware modeling in nonlinear
systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环结构因果模型中的反事实推断，关注shift-scale干预下的推理问题


<details>
  <summary>Details</summary>
Motivation: 传统反事实推断框架假设无环结构因果模型，但许多真实系统（如生物系统）包含反馈循环或循环依赖，违反了无环性假设

Method: 研究循环SCMs中的反事实推断，特别关注shift-scale干预（软性、策略式变化，重新缩放和/或移动变量的机制）

Result: 未在摘要中明确说明

Conclusion: 未在摘要中明确说明

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [183] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文提出了ProFees框架，使用LLM自动化医疗评估与管理(E/M)编码任务，在真实数据集上比商业系统准确率提升36%以上。


<details>
  <summary>Details</summary>
Motivation: 自动化E/M编码可以减轻医生文档负担、提高计费效率，最终改善患者护理质量，但现实世界的复杂性使这项任务具有挑战性。

Method: 提出了基于LLM的ProFees框架，专门处理E/M编码中的现实复杂性，包括多提示策略。

Result: 在专家策划的真实数据集上，ProFees比商业CPT E/M编码系统准确率提高36%以上，比最强的单提示基线提高近5%。

Conclusion: ProFees框架有效解决了E/M编码自动化中的现实复杂性，显著提高了编码准确性。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [184] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出RAVR框架，通过利用答案引导推理来增强语言模型的推理能力，解决了在模型能力不足时难以采样高质量推理路径的问题。


<details>
  <summary>Details</summary>
Motivation: 当语言模型对某些任务能力不足时，传统的强化学习难以采样到高质量的推理路径。受认知科学启发，作者发现"为什么这是答案"比"答案是什么"更容易回答，因为前者避免了开放式探索的认知负担。

Method: 提出RAVR框架，使用答案条件推理作为仅问题推理的变分替代，通过答案引导来推导高质量的推理路径。

Result: 在通用和数学领域的实验中，RAVR相比强基线模型取得了持续改进，减少了推理中的犹豫，加强了结论整合，并促进了问题特定策略的使用。

Conclusion: 答案引导的推理能够显著提高推理路径的期望效用，将难以处理的问题转化为可学习的问题，为增强语言模型推理能力提供了有效方法。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [185] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 该论文提出了一个双范式框架，将智能体AI系统分为符号/经典范式和神经/生成范式，通过系统文献综述分析了两种范式的理论基础、应用领域和伦理挑战，并指出未来发展方向是两者的有意整合。


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI快速发展带来的概念混淆问题，澄清现代神经系统与过时符号模型之间的区别，避免概念重构的误区。

Method: 采用PRISMA系统综述方法，分析了90项研究（2018-2025），从三个维度构建双范式框架：理论基础与架构原则、领域特定实施、范式特定伦理与治理挑战。

Result: 发现范式选择具有战略性：符号系统主导安全关键领域（如医疗），神经系统在适应性强的数据丰富环境中占优（如金融）；识别出符号系统治理模型不足和神经符号混合架构需求等关键研究空白。

Conclusion: 智能体AI的未来不在于单一范式的统治，而在于两种范式的有意整合，以创建既适应性强又可靠的系统，为未来研究、开发和政策提供概念工具包。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [186] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch是一个验证引导的推理框架，通过符号闭包计算、词典验证和自适应草图生成来减少推理链长度，在提高准确性的同时显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法如思维链提示和自一致性虽然能提高大语言模型的准确性，但生成长推理链会导致token消耗、计算成本和延迟大幅增加。

Method: 提出ProofSketch框架，整合符号闭包计算、词典验证和自适应草图生成，通过验证引导来优化推理过程。

Result: 实验证明ProofSketch在减少token使用的同时提高了准确性，表明该方法为高效可信推理提供了有前景的路径。

Conclusion: ProofSketch框架通过验证引导的推理方法，在保持甚至提高准确性的前提下，显著降低了推理过程的资源消耗。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [187] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 本研究开发了两种生成失语症患者语音转录本的方法，通过程序化编程和大型语言模型来缓解失语症研究中的数据稀缺问题，发现Mistral 7b Instruct模型在模拟失语症语言退化方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 失语症研究中，言语语言病理学家需要大量时间手动编码语音样本，而可用数据稀缺限制了自动化系统的发展。本研究旨在通过生成合成转录本来解决数据不足的问题。

Method: 构建了两种生成方法：程序化编程方法和使用Mistral 7b Instruct、Llama 3.1 8b Instruct LLMs的方法。通过词丢弃、填充词插入和错语替换等技术，生成四个严重程度级别的转录本。

Result: 与人工获取的转录本相比，Mistral 7b Instruct在捕捉失语症语言退化关键方面表现最佳，在NDW、词数和词长等指标上显示出更真实的定向变化。

Conclusion: 未来工作应创建更大的数据集，微调模型以更好地表示失语症特征，并让言语语言病理学家评估合成转录本的真实性和实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [188] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 该论文提出了Idea2Plan任务和基准，用于系统评估大语言模型从研究概念到研究计划的转换能力，发现GPT-5系列表现最佳但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究规划是科学发现和自主研究代理发展的关键能力，但目前缺乏对大语言模型研究规划能力的系统性理解。

Method: 构建了Idea2Plan Bench基准，包含200个ICML 2025论文的研究想法和评分标准，并提出了Idea2Plan JudgeEval来评估LLM评委的可靠性。

Result: 实验结果显示GPT-5和GPT-5-mini在基准上表现最强，但仍有显著的改进空间。

Conclusion: 该研究为理解LLMs的研究规划能力提供了新见解，并为未来进展奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [189] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 训练小型注意力变换器在符号化IOI任务上，发现单层双头模型即可完美解决任务，揭示了可解释的最小推理电路。


<details>
  <summary>Details</summary>
Motivation: 预训练模型复杂性掩盖了特定推理任务所需的最小机制，需要研究简化模型中的核心推理电路。

Method: 训练小型注意力变换器在符号化间接宾语识别任务上，通过残差流分解、谱分析和嵌入干预分析电路机制。

Result: 单层双头模型完美解决IOI任务，两个头分别专精于加性和对比性子电路；双层单头模型通过层间组合也能达到类似性能。

Conclusion: 任务特定训练能产生高度可解释的最小电路，为研究变换器推理计算基础提供了受控测试平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [190] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 该研究评估大型语言模型在生物医学文献中识别知识缺口的能力，包括明确声明和上下文推断的隐性知识缺口，并提出了TABI推理方案来结构化推理过程。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于识别未知领域，但现有研究主要关注明确知识缺口检测，本研究旨在扩展至推断隐性知识缺口这一新任务。

Method: 在四个数据集的近1500份文档上进行了实验，对比了OpenAI的闭源模型和Llama、Gemma 2等开源模型，提出了TABI（Toulmin-Abductive Bucketed Inference）推理方案来结构化隐性知识缺口的推理过程。

Result: LLMs在识别明确和隐性知识缺口方面表现出强大能力，开源和闭源模型都表现良好，较大模型通常表现更优。

Conclusion: LLMs具有系统性识别候选知识缺口的强大能力，可支持早期研究制定、政策制定和资助决策，但需要领域适应、人工验证和跨模型基准测试来确保稳健部署。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [191] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本文评估了生成式大语言模型在生物医学文本中共指消解任务中的表现，使用CRAFT语料库作为基准，通过四种提示实验比较了生成式与判别式方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的共指消解面临领域特定术语复杂、指称形式歧义高、共指表达间存在长距离依赖等独特挑战，需要专门评估大语言模型在此领域的表现。

Method: 使用CRAFT语料库作为基准，设计了四种提示实验：基础提示、上下文增强提示、缩写增强提示和实体词典增强提示，并与基于SpanBERT的判别式方法进行对比。

Result: LLaMA 8B和17B模型在实体增强提示下表现出更高的精确率和F1分数，表明轻量级提示工程能有效提升LLM在生物医学NLP任务中的实用性。

Conclusion: 虽然大语言模型在表面层次共指消解方面表现出色，但其性能仍对长距离上下文和指称歧义敏感，实体增强提示能显著提升模型在生物医学领域的表现。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [192] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 论文提出了一个评估语言模型推理效率的框架，通过逻辑编程视角来衡量模型避免不必要推理的能力，发现当前模型在面对无关信息时准确率显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要关注正确性而忽略效率，但人类推理需要有效识别和忽略无关信息。

Method: 提出基于逻辑编程的评估框架，将自然语言证明与逻辑程序的最短证明对齐，量化模型避免不必要推理的效率。

Result: 当前语言模型在面对无关公理时准确率显著下降，即使是最小且领域一致的干扰也会影响表现，生成的证明经常包含无关推理的绕路。

Conclusion: 语言模型在推理效率方面存在不足，需要改进其识别和忽略无关信息的能力以实现更高效的推理。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [193] [SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving](https://arxiv.org/abs/2510.24949)
*Anil Yildiz,Sarah M. Thornton,Carl Hildebrandt,Sreeja Roy-Singh,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 提出SCOUT轻量级替代模型，直接从智能体潜在传感器表征预测场景覆盖标签，避免昂贵的人类标注或计算密集型LVLM推理，实现高效可扩展的场景覆盖评估。


<details>
  <summary>Details</summary>
Motivation: 现有场景覆盖评估方法依赖昂贵的人工标注或计算密集型大视觉语言模型，成本高且效率低，难以大规模部署。

Method: 通过蒸馏过程训练SCOUT模型，学习近似LVLM生成的覆盖标签，利用预计算感知特征避免冗余计算，直接从智能体潜在传感器表征预测场景覆盖。

Result: 在真实自动驾驶导航场景数据集上评估，SCOUT在保持高精度的同时显著降低计算成本，为大规模覆盖分析提供有效实用替代方案。

Conclusion: SCOUT代表了自主系统中高效场景覆盖监督的重要进展，虽然性能依赖于LVLM生成训练标签的质量，但提供了可扩展的解决方案。

Abstract: Assessing scenario coverage is crucial for evaluating the robustness of
autonomous agents, yet existing methods rely on expensive human annotations or
computationally intensive Large Vision-Language Models (LVLMs). These
approaches are impractical for large-scale deployment due to cost and
efficiency constraints. To address these shortcomings, we propose SCOUT
(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate
model designed to predict scenario coverage labels directly from an agent's
latent sensor representations. SCOUT is trained through a distillation process,
learning to approximate LVLM-generated coverage labels while eliminating the
need for continuous LVLM inference or human annotation. By leveraging
precomputed perception features, SCOUT avoids redundant computations and
enables fast, scalable scenario coverage estimation. We evaluate our method
across a large dataset of real-life autonomous navigation scenarios,
demonstrating that it maintains high accuracy while significantly reducing
computational cost. Our results show that SCOUT provides an effective and
practical alternative for large-scale coverage analysis. While its performance
depends on the quality of LVLM-generated training labels, SCOUT represents a
major step toward efficient scenario coverage oversight in autonomous systems.

</details>


### [194] [Scalable predictive processing framework for multitask caregiving robots](https://arxiv.org/abs/2510.25053)
*Hayato Idei,Tamon Miyake,Tetsuya Ogata,Yuichi Yamashita*

Main category: cs.RO

TL;DR: 提出基于预测处理理论的分层多模态循环神经网络，能够直接处理3万维视觉-本体感觉输入，无需降维即可学习护理任务，展示了层次潜在动态自组织、视觉退化鲁棒性和多任务学习中的不对称干扰特性。


<details>
  <summary>Details</summary>
Motivation: 现有护理机器人系统多为任务特定且依赖手工预处理，缺乏跨场景泛化能力。受人类大脑分层预测处理理论的启发，旨在开发能够灵活适应多样化场景的自主护理机器人。

Method: 基于自由能原理的预测处理框架，构建分层多模态循环神经网络，直接整合3万维视觉-本体感觉输入，无需任务特定特征工程，学习刚性物体重新定位和柔性毛巾擦拭两种护理任务。

Result: 模型展现出层次潜在动态自组织、视觉退化鲁棒性和多任务学习中的不对称干扰特性。擦拭任务对重新定位任务影响很小，而学习重新定位任务仅轻微降低擦拭性能，整体保持鲁棒性。

Conclusion: 预测处理作为通用且可扩展的计算原理，为实现鲁棒、灵活和自主的护理机器人提供了方向，同时为理解人类大脑在不确定现实环境中实现灵活适应的能力提供了理论见解。

Abstract: The rapid aging of societies is intensifying demand for autonomous care
robots; however, most existing systems are task-specific and rely on
handcrafted preprocessing, limiting their ability to generalize across diverse
scenarios. A prevailing theory in cognitive neuroscience proposes that the
human brain operates through hierarchical predictive processing, which
underlies flexible cognition and behavior by integrating multimodal sensory
signals. Inspired by this principle, we introduce a hierarchical multimodal
recurrent neural network grounded in predictive processing under the
free-energy principle, capable of directly integrating over 30,000-dimensional
visuo-proprioceptive inputs without dimensionality reduction. The model was
able to learn two representative caregiving tasks, rigid-body repositioning and
flexible-towel wiping, without task-specific feature engineering. We
demonstrate three key properties: (i) self-organization of hierarchical latent
dynamics that regulate task transitions, capture variability in uncertainty,
and infer occluded states; (ii) robustness to degraded vision through
visuo-proprioceptive integration; and (iii) asymmetric interference in
multitask learning, where the more variable wiping task had little influence on
repositioning, whereas learning the repositioning task led to a modest
reduction in wiping performance, while the model maintained overall robustness.
Although the evaluation was limited to simulation, these results establish
predictive processing as a universal and scalable computational principle,
pointing toward robust, flexible, and autonomous caregiving robots while
offering theoretical insight into the human brain's ability to achieve flexible
adaptation in uncertain real-world environments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [195] [Nonlinear Dynamics In Optimization Landscape of Shallow Neural Networks with Tunable Leaky ReLU](https://arxiv.org/abs/2510.25060)
*Jingzhou Liu*

Main category: math.OC

TL;DR: 该论文研究了使用均方损失和泄漏ReLU激活的浅层神经网络非线性动力学。在Gaussian输入和等层宽度k条件下，建立了基于等变梯度度的理论框架来检测临界点的分岔，发现多模退化在临界数0处发生，且分岔与宽度无关，仅出现在非负α时。


<details>
  <summary>Details</summary>
Motivation: 研究浅层神经网络在训练过程中的非线性动力学行为，特别是临界点的分岔现象及其对称性，以理解网络训练的动态特性。

Method: 基于等变梯度度理论框架，分析在Gaussian输入和等层宽度条件下，泄漏ReLU激活参数α变化时临界点的分岔行为。

Result: 发现多模退化在临界数0处一致发生，与神经元数量k无关；分岔现象仅出现在非负α值，且与网络宽度无关；在工程范围α∈(0,1)内，全局最小值不会发生进一步的对称破缺不稳定性。

Conclusion: 建立了适用于任意k≥4的理论框架，揭示了泄漏ReLU神经网络中临界点分岔的基本特性，为理解神经网络训练动力学提供了理论支撑。

Abstract: In this work, we study the nonlinear dynamics of a shallow neural network
trained with mean-squared loss and leaky ReLU activation. Under Gaussian inputs
and equal layer width k, (1) we establish, based on the equivariant gradient
degree, a theoretical framework, applicable to any number of neurons k>= 4, to
detect bifurcation of critical points with associated symmetries from global
minimum as leaky parameter $\alpha$ varies. Typically, our analysis reveals
that a multi-mode degeneracy consistently occurs at the critical number 0,
independent of k. (2) As a by-product, we further show that such bifurcations
are width-independent, arise only for nonnegative $\alpha$ and that the global
minimum undergoes no further symmetry-breaking instability throughout the
engineering regime $\alpha$ in range (0,1). An explicit example with k=5 is
presented to illustrate the framework and exhibit the resulting bifurcation
together with their symmetries.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [196] [Sustainable NARMA-10 Benchmarking for Quantum Reservoir Computing](https://arxiv.org/abs/2510.25183)
*Avyay Kodali,Priyanshi Singh,Pranay Pandey,Krishna Bhatia,Shalini Devendrababu,Srinjoy Ganguly*

Main category: quant-ph

TL;DR: 比较量子储层计算(QRC)与经典模型(ESN、LSTM)及混合量子-经典架构(QLSTM)在NARMA-10任务上的性能，评估预测精度、计算成本和评估时间。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在时间序列预测中的潜力，特别是在资源受限环境下实现可持续AI应用。

Method: 使用量子储层计算(QRC)与经典模型(ESN、LSTM)及混合量子-经典架构(QLSTM)在非线性自回归移动平均任务(NARMA-10)上进行对比实验。

Result: QRC实现了具有竞争力的预测精度，同时在资源受限环境中展现出潜在的可持续性优势。

Conclusion: 量子储层计算在可持续时间序列AI应用中具有前景，特别是在计算资源有限的情况下。

Abstract: This study compares Quantum Reservoir Computing (QRC) with classical models
such as Echo State Networks (ESNs) and Long Short-Term Memory networks (LSTMs),
as well as hybrid quantum-classical architectures (QLSTM), for the nonlinear
autoregressive moving average task (NARMA-10). We evaluate forecasting accuracy
(NRMSE), computational cost, and evaluation time. Results show that QRC
achieves competitive accuracy while offering potential sustainability
advantages, particularly in resource-constrained settings, highlighting its
promise for sustainable time-series AI applications.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [197] [Energy Approach from $\varepsilon$-Graph to Continuum Diffusion Model with Connectivity Functional](https://arxiv.org/abs/2510.25114)
*Yahong Yang,Sun Lee,Jeff Calder,Wenrui Hao*

Main category: math.NA

TL;DR: 该论文推导了具有一般连通性泛函的ε图的能量基连续极限，证明了离散能量与连续对应之间的误差最多为O(ε)，且误差界在连通密度存在强局部波动时仍然有效。


<details>
  <summary>Details</summary>
Motivation: 研究ε图在连续极限下的能量行为，特别是当连通密度存在强局部波动时，建立离散与连续模型之间的精确误差界。

Method: 推导能量基连续极限，分析离散能量与连续对应之间的误差，并引入神经网络程序从边权重数据重建连通密度。

Result: 离散能量与连续对应之间的误差最多为O(ε)，误差界仅涉及连通密度的W^{1,1}范数，在强局部波动下仍有效。

Conclusion: 该连续极限模型可嵌入脑动力学框架，用学习得到的空间变化系数替代常扩散系数，产生与常规常扩散模型显著不同的动力学行为。

Abstract: We derive an energy-based continuum limit for $\varepsilon$-graphs endowed
with a general connectivity functional. We prove that the discrete energy and
its continuum counterpart differ by at most $O(\varepsilon)$; the prefactor
involves only the $W^{1,1}$-norm of the connectivity density as
$\varepsilon\to0$, so the error bound remains valid even when that density has
strong local fluctuations. As an application, we introduce a neural-network
procedure that reconstructs the connectivity density from edge-weight data and
then embeds the resulting continuum model into a brain-dynamics framework. In
this setting, the usual constant diffusion coefficient is replaced by the
spatially varying coefficient produced by the learned density, yielding
dynamics that differ significantly from those obtained with conventional
constant-diffusion models.

</details>


### [198] [Meshless solutions of PDE inverse problems on irregular geometries](https://arxiv.org/abs/2510.25752)
*James V. Roggeveen,Michael P. Brenner*

Main category: math.NA

TL;DR: 提出了一种在复杂时空域上求解非线性偏微分方程逆问题和优化问题的新方法，使用超矩形上的谱基参数化解，通过优化损失函数同时强制方程、边界条件和优化目标。


<details>
  <summary>Details</summary>
Motivation: 解决复杂空间域上非线性偏微分方程逆问题和优化问题的长期挑战，传统方法在这些问题上存在困难。

Method: 在包含真实域的超矩形上定义谱基参数化解，通过优化损失函数同时满足方程、边界条件和优化目标，借鉴PINNs思想。

Result: 经验证明该方法在多种方程上都能实现指数收敛，并能自然融入数据同化和高效求解PDE解的优化问题。

Conclusion: 该方法为复杂域上非线性PDE逆问题和优化问题提供了高效解决方案，具有指数收敛特性。

Abstract: Solving inverse and optimization problems over solutions of nonlinear partial
differential equations (PDEs) on complex spatial domains is a long-standing
challenge. Here we introduce a method that parameterizes the solution using
spectral bases on arbitrary spatiotemporal domains, whereby the basis is
defined on a hyperrectangle containing the true domain. We find the
coefficients of the basis expansion by solving an optimization problem whereby
both the equations, the boundary conditions and any optimization targets are
enforced by a loss function, building on a key idea from Physics-Informed
Neural Networks (PINNs). Since the representation of the function natively has
exponential convergence, so does the solution of the optimization problem, as
long as it can be solved efficiently. We find empirically that the optimization
protocols developed for machine learning find solutions with exponential
convergence on a wide range of equations. The method naturally allows for the
incorporation of data assimilation by including additional terms in the loss
function, and for the efficient solution of optimization problems over the PDE
solutions.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [199] [Conditional neural field for spatial dimension reduction of turbulence data: a comparison study](https://arxiv.org/abs/2510.25135)
*Junyi Guo,Pan Du,Xiantao Fan,Yahui Li,Jian-Xun Wang*

Main category: physics.flu-dyn

TL;DR: 本文研究了条件神经场(CNFs)在湍流空间降维中的应用，比较了三种条件机制和一种新的域分解CNF，在多个湍流数据集上评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 探索条件神经场作为网格无关、基于坐标的解码器，用于湍流流动的空间降维，并与传统方法进行基准比较。

Method: 使用三种条件机制：激活调制(FiLM)、低秩权重偏置调制(FP)和最后一层内积耦合，并引入域分解CNF来局部化复杂性。在统一的编码-解码框架和评估协议下进行测试。

Result: CNF-FP在训练和范围内测试中误差最低，CNF-FiLM在范围外场景中泛化能力最好。域分解显著提高了范围外精度，特别是在更复杂的数据集上。

Conclusion: 研究为在湍流压缩和重建中使用CNFs时选择条件机制、容量和域分解提供了严格的物理感知基础。

Abstract: We investigate conditional neural fields (CNFs), mesh-agnostic,
coordinate-based decoders conditioned on a low-dimensional latent, for spatial
dimensionality reduction of turbulent flows. CNFs are benchmarked against
Proper Orthogonal Decomposition and a convolutional autoencoder within a
unified encoding-decoding framework and a common evaluation protocol that
explicitly separates in-range (interpolative) from out-of-range (strict
extrapolative) testing beyond the training horizon, with identical
preprocessing, metrics, and fixed splits across all baselines. We examine three
conditioning mechanisms: (i) activation-only modulation (often termed FiLM),
(ii) low-rank weight and bias modulation (termed FP), and (iii) last-layer
inner-product coupling, and introduce a novel domain-decomposed CNF that
localizes complexities. Across representative turbulence datasets (WMLES
channel inflow, DNS channel inflow, and wall pressure fluctuations over
turbulent boundary layers), CNF-FP achieves the lowest training and in-range
testing errors, while CNF-FiLM generalizes best for out-of-range scenarios once
moderate latent capacity is available. Domain decomposition significantly
improves out-of-range accuracy, especially for the more demanding datasets. The
study provides a rigorous, physics-aware basis for selecting conditioning,
capacity, and domain decomposition when using CNFs for turbulence compression
and reconstruction.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [200] [EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation](https://arxiv.org/abs/2510.25132)
*Chao Song,Zhiyuan Liu,Han Huang,Liang Wang,Qiong Wang,Jianyu Shi,Hui Yu,Yihang Zhou,Yang Zhang*

Main category: q-bio.BM

TL;DR: EnzyControl是一种能够进行功能和底物特异性控制的酶骨架生成方法，通过EnzyAdapter组件和两阶段训练，在结构性和功能性指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决当前生成模型在结合数据、底物特异性控制和从头酶骨架生成方面的局限性。

Method: 基于EnzyBind数据集，提出EnzyControl方法，使用EnzyAdapter组件集成到预训练基序支架模型中，通过两阶段训练实现底物感知的酶骨架生成。

Result: 在EnzyBind和EnzyBench基准测试中，EnzyControl在结构性和功能性指标上表现最佳，设计性和催化效率分别比基线模型提高13%。

Conclusion: EnzyControl成功实现了底物特异性酶骨架生成，为计算蛋白质工程提供了有效工具。

Abstract: Designing enzyme backbones with substrate-specific functionality is a
critical challenge in computational protein engineering. Current generative
models excel in protein design but face limitations in binding data,
substrate-specific control, and flexibility for de novo enzyme backbone
generation. To address this, we introduce EnzyBind, a dataset with 11,100
experimentally validated enzyme-substrate pairs specifically curated from
PDBbind. Building on this, we propose EnzyControl, a method that enables
functional and substrate-specific control in enzyme backbone generation. Our
approach generates enzyme backbones conditioned on MSA-annotated catalytic
sites and their corresponding substrates, which are automatically extracted
from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,
a lightweight, modular component integrated into a pretrained motif-scaffolding
model, allowing it to become substrate-aware. A two-stage training paradigm
further refines the model's ability to generate accurate and functional enzyme
structures. Experiments show that our EnzyControl achieves the best performance
across structural and functional metrics on EnzyBind and EnzyBench benchmarks,
with particularly notable improvements of 13\% in designability and 13\% in
catalytic efficiency compared to the baseline models. The code is released at
https://github.com/Vecteur-libre/EnzyControl.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [201] [Fast Dimensionality Reduction from $\ell_2$ to $\ell_p$](https://arxiv.org/abs/2510.25541)
*Rafael Chiclana,Mark Iwen*

Main category: math.PR

TL;DR: 本文提出了一种从ℓ₂到ℓ_p的线性嵌入方法，当目标维度较小时运行时间更优，并证明了任何范数嵌入的维度下界。


<details>
  <summary>Details</summary>
Motivation: 近年来，在目标空间中通过ℓ₁范数保持欧几里得距离的嵌入方法受到关注，特别是在高维最近邻搜索等应用中。Dirksen等人的突破性工作建立了最优的ℓ₂→ℓ₁嵌入，但计算复杂度为O(d log d)。本文旨在推广这一方向，为任意p∈[1,2]提供更高效的嵌入方法。

Method: 基于Ailon和Liberty的构造，提出从ℓ₂到ℓ_p的简单线性嵌入方法。当k≤d^{1/4}时，运行时间降低到O(d log k)。

Result: 当目标维度k较小时，新方法比现有方法具有更优的运行时间。同时证明了对任意范数，嵌入到k维空间所需的维度下界为Ω(ε^{-2} log(ε²n)/log(1/ε))，与ℓ₂情况的最优界仅相差对数因子。

Conclusion: 本文提供了从ℓ₂到ℓ_p的高效线性嵌入方法，在目标维度较小时显著改进运行时间，并建立了任意范数嵌入的维度下界理论结果。

Abstract: The Johnson-Lindenstrauss (JL) lemma is a fundamental result in
dimensionality reduction, ensuring that any finite set $X \subseteq
\mathbb{R}^d$ can be embedded into a lower-dimensional space $\mathbb{R}^k$
while approximately preserving all pairwise Euclidean distances. In recent
years, embeddings that preserve Euclidean distances when measured via the
$\ell_1$ norm in the target space have received increasing attention due to
their relevance in applications such as nearest neighbor search in high
dimensions. A recent breakthrough by Dirksen, Mendelson, and Stollenwerk
established an optimal $\ell_2 \to \ell_1$ embedding with computational
complexity $O(d \log d)$. In this work, we generalize this direction and
propose a simple linear embedding from $\ell_2$ to $\ell_p$ for any $p \in
[1,2]$ based on a construction of Ailon and Liberty. Our method achieves a
reduced runtime of $O(d \log k)$ when $k \leq d^{1/4}$, improving upon prior
runtime results when the target dimension is small. Additionally, we show that
for \emph{any norm} $\|\cdot\|$ in the target space, any embedding of
$(\mathbb{R}^d, \|\cdot\|_2)$ into $(\mathbb{R}^k, \|\cdot\|)$ with distortion
$\varepsilon$ generally requires $k = \Omega\big(\varepsilon^{-2}
\log(\varepsilon^2 n)/\log(1/\varepsilon)\big)$, matching the optimal bound for
the $\ell_2$ case up to a logarithmic factor.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [202] [Large-Scale Network Embedding in Apache Spark](https://arxiv.org/abs/2106.10620)
*Wenqing Lin*

Main category: cs.SI

TL;DR: 提出了一种基于Apache Spark的高效分布式网络嵌入算法，通过递归图分割和并行计算处理大规模图，在速度和性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统网络嵌入方法难以高效处理大规模图，因为图计算成本高且中间结果过大，无法在单机上处理。

Method: 使用Apache Spark递归将大图分割成小规模子图，并行计算每个子图的网络嵌入，最后聚合所有子图输出得到节点嵌入。

Result: 能在几小时内处理数十亿边的大图，比现有方法快至少4倍，在链接预测和节点分类任务上分别提升4.25%和4.27%。

Conclusion: 该算法成功应用于腾讯在线游戏中的好友推荐和物品推荐，运行时间减少91.11%，评估指标提升12.80%。

Abstract: Network embedding has been widely used in social recommendation and network
analysis, such as recommendation systems and anomaly detection with graphs.
However, most of previous approaches cannot handle large graphs efficiently,
due to that (i) computation on graphs is often costly and (ii) the size of
graph or the intermediate results of vectors could be prohibitively large,
rendering it difficult to be processed on a single machine. In this paper, we
propose an efficient and effective distributed algorithm for network embedding
on large graphs using Apache Spark, which recursively partitions a graph into
several small-sized subgraphs to capture the internal and external structural
information of nodes, and then computes the network embedding for each subgraph
in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain
the embeddings of nodes in a linear cost. After that, we demonstrate in various
experiments that our proposed approach is able to handle graphs with billions
of edges within a few hours and is at least 4 times faster than the
state-of-the-art approaches. Besides, it achieves up to $4.25\%$ and $4.27\%$
improvements on link prediction and node classification tasks respectively. In
the end, we deploy the proposed algorithms in two online games of Tencent with
the applications of friend recommendation and item recommendation, which
improve the competitors by up to $91.11\%$ in running time and up to $12.80\%$
in the corresponding evaluation metrics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [203] [Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems](https://arxiv.org/abs/2510.25736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文研究了图复制数据库中的对称私有信息检索(SPIR)问题，通过开发算法将PIR方案转换为SPIR方案，为路径图和循环图建立了容量下界，并针对三顶点路径图确定了SPIR容量为1/2。


<details>
  <summary>Details</summary>
Motivation: 研究图复制数据库模型中的SPIR问题，其中数据库复制由简单图建模，每个服务器对应一个顶点，消息仅在有边连接的服务器间复制。目标是量化相比图复制公共随机性设置下SPIR容量的改进。

Method: 开发了一种算法，将一类PIR方案转换为相应的SPIR方案，从而为存在此类方案的图建立容量下界。该方法特别适用于路径图和循环图。

Result: 为路径图和循环图推导了比相应PIR容量更紧的容量上界。对于三顶点路径图的特殊情况，确定了SPIR容量为1/2。

Conclusion: 提出的转换算法为图复制数据库中的SPIR问题提供了有效的容量分析框架，在特定图结构下获得了精确的容量结果。

Abstract: We revisit the problem of symmetric private information retrieval (SPIR) in
settings where the database replication is modeled by a simple graph. Here,
each vertex corresponds to a server, and a message is replicated on two servers
if and only if there is an edge between them. To satisfy the requirement of
database privacy, we let all the servers share some common randomness,
independent of the messages. We aim to quantify the improvement in SPIR
capacity, i.e., the maximum ratio of the number of desired and downloaded
symbols, compared to the setting with graph-replicated common randomness.
Towards this, we develop an algorithm to convert a class of PIR schemes into
the corresponding SPIR schemes, thereby establishing a capacity lower bound on
graphs for which such schemes exist. This includes the class of path and cyclic
graphs for which we derive capacity upper bounds that are tighter than the
trivial bounds given by the respective PIR capacities. For the special case of
path graph with three vertices, we identify the SPIR capacity to be
$\frac{1}{2}$.

</details>


### [204] [Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications](https://arxiv.org/abs/2510.24763)
*Tingting Huang,Jundong Chen,Huanqiang Zeng,Guofa Cai,Georges Kaddoum*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的功率域非正交多址混沌移位键控系统，用于车载通信，通过DNN解调器消除混沌同步需求，提高频谱效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有多用户混沌通信系统存在频谱效率低、用户连接数有限、计算复杂度高等问题，特别是在车载通信场景下需要更安全高效的传输方案。

Method: 设计了基于深度神经网络的解调器，采用双域特征提取架构联合处理时域和频域信息，并集成到连续干扰消除框架中。

Result: 系统在频谱效率、能量效率、误码率、安全性和鲁棒性方面表现优异，同时保持较低的计算复杂度。

Conclusion: 该系统为安全车载通信提供了实用的解决方案，验证了其在实践中的可行性。

Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for
vehicular communication systems. Chaos-based modulation schemes have garnered
considerable interest due to their benefits in physical layer security.
However, most existing MU chaotic communication systems, particularly those
based on non-coherent detection, suffer from low spectral efficiency due to
reference signal transmission, and limited user connectivity under orthogonal
multiple access (OMA). While non-orthogonal schemes, such as sparse code
multiple access (SCMA)-based DCSK, have been explored, they face high
computational complexity and inflexible scalability due to their fixed codebook
designs. This paper proposes a deep learning-assisted power domain
non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for
vehicular communications. A deep neural network (DNN)-based demodulator is
designed to learn intrinsic chaotic signal characteristics during offline
training, thereby eliminating the need for chaotic synchronization or reference
signal transmission. The demodulator employs a dual-domain feature extraction
architecture that jointly processes the time-domain and frequency-domain
information of chaotic signals, enhancing feature learning under dynamic
channels. The DNN is integrated into the successive interference cancellation
(SIC) framework to mitigate error propagation issues. Theoretical analysis and
extensive simulations demonstrate that the proposed system achieves superior
performance in terms of spectral efficiency (SE), energy efficiency (EE), bit
error rate (BER), security, and robustness, while maintaining lower
computational complexity compared to traditional MU-DCSK and existing DL-aided
schemes. These advantages validate its practical viability for secure vehicular
communications.

</details>
